Using TensorFlow backend.
[2019-04-04 11:57:47,707] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v1', activation='linear', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-v1', eval_act_func='part4_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=100000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=5e-06, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=5000000, metric_func='part4_v1', model_dir='None', model_param=[12, 1], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_v1', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=10, test_env=['Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'], test_mode='Multiple', train_act_func='part4_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=10.0, weight_initer='glorot_uniform', window_len=20)
[2019-04-04 11:57:47,707] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-04 11:57:47.740873: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-04 11:58:04,235] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-04 11:58:04,235] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-v1', 'Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'] ...
[2019-04-04 11:58:04,258] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation worker starts!
[2019-04-04 11:58:04,281] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation worker starts!
[2019-04-04 11:58:04,305] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation worker starts!
[2019-04-04 11:58:04,305] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:04,306] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-04 11:58:04,371] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:04,372] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run1
[2019-04-04 11:58:05,307] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:05,308] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-04 11:58:05,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:05,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run1
[2019-04-04 11:58:06,309] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:06,310] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-04 11:58:06,386] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:06,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run1
[2019-04-04 11:58:07,311] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:07,312] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-04 11:58:07,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:07,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run1
[2019-04-04 11:58:08,313] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:08,314] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-04 11:58:08,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:08,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run1
[2019-04-04 11:58:08,690] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 11:58:08,691] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 11:58:08,691] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:08,692] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 11:58:08,692] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 11:58:08,692] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:08,692] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:08,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run1
[2019-04-04 11:58:08,708] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run1
[2019-04-04 11:58:08,721] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run1
[2019-04-04 11:58:09,315] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:09,315] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-04 11:58:09,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:09,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run1
[2019-04-04 11:58:10,316] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:10,317] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-04 11:58:10,431] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:10,432] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run1
[2019-04-04 11:58:11,318] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:11,319] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-04 11:58:11,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:11,443] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run1
[2019-04-04 11:58:12,320] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:12,321] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-04 11:58:12,431] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:12,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run1
[2019-04-04 11:58:13,321] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:13,322] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-04 11:58:13,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:13,450] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run1
[2019-04-04 11:58:14,322] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:14,323] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-04 11:58:14,488] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:14,490] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run1
[2019-04-04 11:58:15,324] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:15,325] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-04 11:58:15,490] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:15,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run1
[2019-04-04 11:58:16,326] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:16,327] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-04 11:58:16,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:16,466] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run1
[2019-04-04 11:58:17,327] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:17,341] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-04 11:58:17,444] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:17,446] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run1
[2019-04-04 11:58:18,346] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:18,346] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-04 11:58:18,472] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:18,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run1
[2019-04-04 11:58:19,347] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 11:58:19,348] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-04 11:58:19,462] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:58:19,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run1
[2019-04-04 11:59:33,575] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-04 11:59:33,575] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.25, 50.0, 0.0, 0.0, 24.5, 24.94584858243891, 0.284425279467566, 0.0, 1.0, 0.0]
[2019-04-04 11:59:33,576] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 11:59:33,576] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.12103107 0.04509398 0.18491459 0.05285948 0.06669068 0.44741437
 0.0819958 ], sampled 0.060658368649436745
[2019-04-04 12:00:13,158] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7438.4892 212395370.0133 784.8055
[2019-04-04 12:00:45,769] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7250.2045 246665026.2084 434.8041
[2019-04-04 12:00:49,359] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7344.4813 236786569.7592 761.7627
[2019-04-04 12:00:50,394] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 7344.481272393106, 236786569.75924128, 761.7626661944096, 7438.489236267711, 212395370.01333046, 784.8055198882013, 7250.204482300058, 246665026.20839772, 434.80409286973855]
[2019-04-04 12:00:55,844] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.19868575 0.0722286  0.11363201 0.11510098 0.10675025 0.34478614
 0.0488163 ], sum to 1.0000
[2019-04-04 12:00:55,890] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6824
[2019-04-04 12:00:56,133] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 20.0, 20.42430545238629, -0.7465705918426645, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 22800.0000, 
sim time next is 23400.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 21.0, 20.43872859501841, -0.7401766773099875, 0.0, 1.0, 95032.60323589637], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.25, 0.2032273829182009, 0.25327444089667084, 0.0, 1.0, 0.4525362058852208], 
reward next is 0.5475, 
noisyNet noise sample is [array([-0.39248797], dtype=float32), -0.1805619]. 
=============================================
[2019-04-04 12:00:57,257] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.18788867 0.07507616 0.13454084 0.08654279 0.07308599 0.40044552
 0.04242004], sum to 1.0000
[2019-04-04 12:00:57,257] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0094
[2019-04-04 12:00:57,540] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [7.7, 93.0, 52.66666666666667, 0.0, 20.0, 20.27117097530185, -0.7731973844198885, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 36600.0000, 
sim time next is 37200.0000, 
raw observation next is [7.7, 93.0, 56.33333333333333, 0.0, 19.0, 20.24989474289279, -0.7703952152258707, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.18777777777777777, 0.0, 0.08333333333333333, 0.18749122857439904, 0.24320159492470975, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.176726], dtype=float32), -1.6652054]. 
=============================================
[2019-04-04 12:01:12,942] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.17988554 0.03098725 0.12869439 0.07655898 0.11269046 0.43697986
 0.03420354], sum to 1.0000
[2019-04-04 12:01:12,942] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6500
[2019-04-04 12:01:13,137] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.3, 67.33333333333334, 0.0, 0.0, 19.5, 20.27732132380785, -0.7650840395849278, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 157800.0000, 
sim time next is 158400.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 20.5, 20.14823049000269, -0.7783201240966288, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.20833333333333334, 0.17901920750022407, 0.2405599586344571, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([0.07039382], dtype=float32), -1.810585]. 
=============================================
[2019-04-04 12:01:17,909] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.14592338 0.05571675 0.08206791 0.0750189  0.04061246 0.55179846
 0.04886216], sum to 1.0000
[2019-04-04 12:01:17,909] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9556
[2019-04-04 12:01:18,208] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.900000000000002, 75.33333333333334, 0.0, 0.0, 26.0, 23.02695738760211, -0.175327487232985, 0.0, 1.0, 44607.87714235745], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 181200.0000, 
sim time next is 181800.0000, 
raw observation next is [-8.9, 76.0, 0.0, 0.0, 26.0, 23.06806807310477, -0.1698044666357641, 0.0, 1.0, 44535.08881581151], 
processed observation next is [1.0, 0.08695652173913043, 0.21606648199445982, 0.76, 0.0, 0.0, 0.6666666666666666, 0.42233900609206404, 0.443398511121412, 0.0, 1.0, 0.21207185150386434], 
reward next is 0.7879, 
noisyNet noise sample is [array([-0.3250189], dtype=float32), 0.793831]. 
=============================================
[2019-04-04 12:01:20,052] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.24762084 0.05890006 0.09297982 0.08044364 0.0893454  0.39478692
 0.03592324], sum to 1.0000
[2019-04-04 12:01:20,052] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1174
[2019-04-04 12:01:20,405] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-8.033333333333333, 77.0, 71.50000000000001, 49.33333333333332, 20.5, 24.09655928546806, -0.08624746501743406, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 206400.0000, 
sim time next is 207000.0000, 
raw observation next is [-7.85, 76.5, 79.0, 0.0, 19.0, 24.14537148402334, -0.09097390659326765, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24515235457063714, 0.765, 0.2633333333333333, 0.0, 0.08333333333333333, 0.5121142903352783, 0.46967536446891084, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6042616], dtype=float32), -0.006229141]. 
=============================================
[2019-04-04 12:01:20,419] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[1.2730517]
 [1.4180228]
 [1.4439538]
 [1.4259158]
 [1.583425 ]], R is [[1.95958114]
 [2.93998528]
 [3.9105854 ]
 [4.87147951]
 [5.82276487]].
[2019-04-04 12:01:28,935] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.24760778 0.03188093 0.09514055 0.03938161 0.18097685 0.37959448
 0.02541778], sum to 1.0000
[2019-04-04 12:01:28,935] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4272
[2019-04-04 12:01:29,030] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.2, 80.5, 0.0, 0.0, 21.0, 22.27875424739086, -0.3692022689909613, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 257400.0000, 
sim time next is 258000.0000, 
raw observation next is [-4.3, 80.0, 0.0, 0.0, 21.5, 22.15571418991158, -0.3943605518447283, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.34349030470914127, 0.8, 0.0, 0.0, 0.2916666666666667, 0.3463095158259651, 0.3685464827184239, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39046085], dtype=float32), -0.94023705]. 
=============================================
[2019-04-04 12:01:29,104] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[2.0620916]
 [2.0870597]
 [2.0837078]
 [2.0630972]
 [2.3365088]], R is [[2.94773269]
 [3.91825533]
 [4.87907314]
 [5.83028269]
 [6.77197981]].
[2019-04-04 12:01:32,638] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7748: loss 36.5308
[2019-04-04 12:01:32,730] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7748: learning rate 0.0000
[2019-04-04 12:01:33,357] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7854: loss 35.7322
[2019-04-04 12:01:33,357] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7854: learning rate 0.0000
[2019-04-04 12:01:33,436] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7868: loss 42.0256
[2019-04-04 12:01:33,446] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7868: learning rate 0.0000
[2019-04-04 12:01:33,541] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 7887: loss 7.7007
[2019-04-04 12:01:33,541] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 7887: learning rate 0.0000
[2019-04-04 12:01:33,710] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7912: loss 27.0544
[2019-04-04 12:01:33,713] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7913: loss 29.1781
[2019-04-04 12:01:33,714] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7913: learning rate 0.0000
[2019-04-04 12:01:33,733] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7912: learning rate 0.0000
[2019-04-04 12:01:33,908] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 7945: loss 37.5014
[2019-04-04 12:01:33,929] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 7945: learning rate 0.0000
[2019-04-04 12:01:34,003] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7957: loss 32.5600
[2019-04-04 12:01:34,003] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7957: learning rate 0.0000
[2019-04-04 12:01:34,279] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 7997: loss 15.1689
[2019-04-04 12:01:34,304] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 7997: learning rate 0.0000
[2019-04-04 12:01:34,430] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 8026: loss 52.8193
[2019-04-04 12:01:34,430] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 8026: learning rate 0.0000
[2019-04-04 12:01:34,570] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8052: loss 19.1591
[2019-04-04 12:01:34,571] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8052: learning rate 0.0000
[2019-04-04 12:01:34,956] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 8115: loss 26.7570
[2019-04-04 12:01:34,956] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 8115: learning rate 0.0000
[2019-04-04 12:01:35,191] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8154: loss 38.1928
[2019-04-04 12:01:35,197] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8154: learning rate 0.0000
[2019-04-04 12:01:35,223] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8160: loss 47.2689
[2019-04-04 12:01:35,241] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8160: learning rate 0.0000
[2019-04-04 12:01:35,472] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8197: loss 11.0653
[2019-04-04 12:01:35,493] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8197: learning rate 0.0000
[2019-04-04 12:01:36,260] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 8323: loss 37.2753
[2019-04-04 12:01:36,261] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 8323: learning rate 0.0000
[2019-04-04 12:01:58,245] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.19902495 0.0491741  0.06455999 0.02403218 0.15269066 0.48994586
 0.02057222], sum to 1.0000
[2019-04-04 12:01:58,246] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8666
[2019-04-04 12:01:58,455] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.9166666666666667, 34.5, 44.0, 0.0, 26.0, 24.27920046692846, 0.1225542525746797, 1.0, 1.0, 199696.6382927024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 489000.0000, 
sim time next is 489600.0000, 
raw observation next is [1.1, 34.0, 38.0, 0.0, 26.0, 24.88973035875078, 0.184994404299276, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.34, 0.12666666666666668, 0.0, 0.6666666666666666, 0.5741441965625649, 0.5616648014330919, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34455693], dtype=float32), -1.0442451]. 
=============================================
[2019-04-04 12:01:59,600] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.1896919  0.05997165 0.06508128 0.02125021 0.14678009 0.48390353
 0.03332135], sum to 1.0000
[2019-04-04 12:01:59,600] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4274
[2019-04-04 12:01:59,662] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.533333333333333, 26.0, 127.8333333333333, 0.0, 26.0, 24.65913286028164, 0.127971600131772, 1.0, 1.0, 129299.3913163006], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 476400.0000, 
sim time next is 477000.0000, 
raw observation next is [-1.45, 26.5, 129.0, 0.0, 26.0, 25.10559065385341, 0.1724492368535326, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.422437673130194, 0.265, 0.43, 0.0, 0.6666666666666666, 0.5921325544877843, 0.5574830789511775, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4010268], dtype=float32), 0.61252576]. 
=============================================
[2019-04-04 12:01:59,769] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[3.9260275]
 [4.0621824]
 [4.091672 ]
 [4.0593915]
 [4.0416923]], R is [[5.09919167]
 [5.43248844]
 [5.47659731]
 [6.34253502]
 [7.14194584]].
[2019-04-04 12:02:05,315] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.17998704 0.06525448 0.05034069 0.04402961 0.11036202 0.52577186
 0.02425432], sum to 1.0000
[2019-04-04 12:02:05,315] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3713
[2019-04-04 12:02:05,482] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.65, 88.5, 0.0, 0.0, 25.0, 23.96231126988076, -0.00629670296545369, 0.0, 1.0, 57730.94110171865], 
current ob forecast is [], 
actual action is [25.5], 
sim time this is 523800.0000, 
sim time next is 524400.0000, 
raw observation next is [4.533333333333333, 88.33333333333334, 0.0, 0.0, 25.5, 23.91543311770852, -0.01024497166333275, 0.0, 1.0, 52160.16981785507], 
processed observation next is [0.0, 0.043478260869565216, 0.5881809787626964, 0.8833333333333334, 0.0, 0.0, 0.625, 0.49295275980904335, 0.4965850094455557, 0.0, 1.0, 0.2483817610374051], 
reward next is 0.7516, 
noisyNet noise sample is [array([2.6781118], dtype=float32), 0.014553738]. 
=============================================
[2019-04-04 12:02:09,417] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15239: loss 42.1930
[2019-04-04 12:02:09,419] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15239: learning rate 0.0000
[2019-04-04 12:02:10,560] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.21505441 0.03223032 0.06401622 0.02932585 0.23261689 0.4022969
 0.02445939], sum to 1.0000
[2019-04-04 12:02:10,560] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0745
[2019-04-04 12:02:10,749] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-2.8, 83.66666666666667, 0.0, 0.0, 24.0, 23.67383514172937, -0.0146505347079904, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 593400.0000, 
sim time next is 594000.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 22.0, 23.72043377019088, -0.02227555274725848, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.38504155124653744, 0.83, 0.0, 0.0, 0.3333333333333333, 0.4767028141825733, 0.49257481575091383, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0696009], dtype=float32), 1.9014567]. 
=============================================
[2019-04-04 12:02:10,808] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[3.4338322]
 [3.5381696]
 [3.6026719]
 [3.6236222]
 [3.4228556]], R is [[4.56214523]
 [5.51652384]
 [6.21909046]
 [6.84850216]
 [7.27566624]].
[2019-04-04 12:02:10,899] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 15565: loss 36.1503
[2019-04-04 12:02:10,899] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 15565: learning rate 0.0000
[2019-04-04 12:02:10,977] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15577: loss 22.2105
[2019-04-04 12:02:10,978] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15577: learning rate 0.0000
[2019-04-04 12:02:11,501] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 15692: loss 29.7318
[2019-04-04 12:02:11,502] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 15693: learning rate 0.0000
[2019-04-04 12:02:11,731] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15754: loss 36.5602
[2019-04-04 12:02:11,733] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15754: learning rate 0.0000
[2019-04-04 12:02:12,081] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15847: loss 35.4483
[2019-04-04 12:02:12,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15847: learning rate 0.0000
[2019-04-04 12:02:12,800] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 16030: loss 30.6061
[2019-04-04 12:02:12,800] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 16030: learning rate 0.0000
[2019-04-04 12:02:12,866] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16052: loss 32.5600
[2019-04-04 12:02:12,869] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16052: learning rate 0.0000
[2019-04-04 12:02:13,155] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16131: loss 27.7908
[2019-04-04 12:02:13,156] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16131: learning rate 0.0000
[2019-04-04 12:02:13,203] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16142: loss 28.5082
[2019-04-04 12:02:13,234] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16142: learning rate 0.0000
[2019-04-04 12:02:13,306] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 16162: loss 31.5812
[2019-04-04 12:02:13,329] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 16162: learning rate 0.0000
[2019-04-04 12:02:13,972] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16327: loss 28.7368
[2019-04-04 12:02:13,973] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16327: learning rate 0.0000
[2019-04-04 12:02:14,168] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16379: loss 39.3337
[2019-04-04 12:02:14,169] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16379: learning rate 0.0000
[2019-04-04 12:02:14,560] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 16482: loss 18.6858
[2019-04-04 12:02:14,563] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 16482: learning rate 0.0000
[2019-04-04 12:02:14,629] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16500: loss 35.5576
[2019-04-04 12:02:14,631] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16500: learning rate 0.0000
[2019-04-04 12:02:14,929] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 16586: loss 29.3434
[2019-04-04 12:02:14,931] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 16586: learning rate 0.0000
[2019-04-04 12:02:16,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.25265944 0.06832595 0.05102432 0.03585352 0.15232195 0.40382093
 0.03599388], sum to 1.0000
[2019-04-04 12:02:16,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8819
[2019-04-04 12:02:16,156] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.816666666666666, 65.0, 111.6666666666667, 17.0, 23.0, 24.63322700312457, 0.07093259660626562, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 641400.0000, 
sim time next is 642000.0000, 
raw observation next is [-3.733333333333333, 65.0, 105.8333333333333, 8.499999999999998, 24.0, 24.58752240945208, 0.05328632197701613, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.35918744228993543, 0.65, 0.3527777777777777, 0.009392265193370164, 0.5, 0.5489602007876734, 0.5177621073256721, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.134414], dtype=float32), -0.64104325]. 
=============================================
[2019-04-04 12:02:16,159] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[3.7642722]
 [3.432202 ]
 [3.588529 ]
 [4.0074463]
 [3.9442801]], R is [[4.81942177]
 [5.77122736]
 [6.71351528]
 [7.64638042]
 [8.56991673]].
[2019-04-04 12:02:36,022] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.1693076  0.02296962 0.02887496 0.01507475 0.32733876 0.42899743
 0.0074369 ], sum to 1.0000
[2019-04-04 12:02:36,023] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3298
[2019-04-04 12:02:36,162] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 86.0, 54.0, 0.0, 19.0, 23.28545197935275, -0.1802441646723182, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 831600.0000, 
sim time next is 832200.0000, 
raw observation next is [-3.9, 85.33333333333334, 52.33333333333333, 0.0, 19.5, 23.42608604548831, -0.1688681618463813, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.8533333333333334, 0.17444444444444443, 0.0, 0.125, 0.452173837124026, 0.44371061271787293, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.38920632], dtype=float32), -0.6273701]. 
=============================================
[2019-04-04 12:02:42,376] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.14562218 0.02086096 0.01564343 0.00946515 0.25908813 0.54349166
 0.00582842], sum to 1.0000
[2019-04-04 12:02:42,376] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2724
[2019-04-04 12:02:42,590] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.67675990483975, 0.1588572820402626, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 846000.0000, 
sim time next is 846600.0000, 
raw observation next is [-3.816666666666666, 85.5, 0.0, 0.0, 26.0, 24.88052984839657, 0.1703300046977299, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3568790397045245, 0.855, 0.0, 0.0, 0.6666666666666666, 0.5733774873663808, 0.5567766682325767, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0613976], dtype=float32), -0.032789662]. 
=============================================
[2019-04-04 12:02:42,684] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23108: loss 39.1549
[2019-04-04 12:02:42,684] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23108: learning rate 0.0000
[2019-04-04 12:02:43,682] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23350: loss 30.8017
[2019-04-04 12:02:43,691] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23350: learning rate 0.0000
[2019-04-04 12:02:44,365] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23552: loss 29.3092
[2019-04-04 12:02:44,365] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23552: learning rate 0.0000
[2019-04-04 12:02:44,648] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 23639: loss 26.4898
[2019-04-04 12:02:44,649] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 23639: learning rate 0.0000
[2019-04-04 12:02:45,001] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 23731: loss 27.5346
[2019-04-04 12:02:45,008] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 23731: learning rate 0.0000
[2019-04-04 12:02:45,131] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23772: loss 37.3528
[2019-04-04 12:02:45,132] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23772: learning rate 0.0000
[2019-04-04 12:02:45,401] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 23822: loss 34.6422
[2019-04-04 12:02:45,401] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 23822: learning rate 0.0000
[2019-04-04 12:02:45,595] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23867: loss 27.8866
[2019-04-04 12:02:45,597] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23867: learning rate 0.0000
[2019-04-04 12:02:46,052] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23961: loss 27.7776
[2019-04-04 12:02:46,063] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23961: learning rate 0.0000
[2019-04-04 12:02:46,236] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 24001: loss 25.0904
[2019-04-04 12:02:46,236] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 24001: learning rate 0.0000
[2019-04-04 12:02:47,202] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 24211: loss 27.0545
[2019-04-04 12:02:47,210] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 24211: learning rate 0.0000
[2019-04-04 12:02:47,905] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24370: loss 23.6417
[2019-04-04 12:02:47,905] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24370: learning rate 0.0000
[2019-04-04 12:02:47,952] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.15639332 0.02791657 0.01038887 0.00951087 0.50542265 0.2851576
 0.00521006], sum to 1.0000
[2019-04-04 12:02:47,952] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5622
[2019-04-04 12:02:48,147] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.8, 93.0, 97.33333333333333, 0.0, 26.0, 25.07184937273801, 0.2381609427931112, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 912000.0000, 
sim time next is 912600.0000, 
raw observation next is [3.8, 93.0, 96.0, 0.0, 26.0, 25.2711019982965, 0.2663947541669515, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.32, 0.0, 0.6666666666666666, 0.6059251665247084, 0.5887982513889839, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14779603], dtype=float32), -0.20251013]. 
=============================================
[2019-04-04 12:02:48,278] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 24428: loss 31.8154
[2019-04-04 12:02:48,282] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 24428: learning rate 0.0000
[2019-04-04 12:02:48,385] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24462: loss 30.2598
[2019-04-04 12:02:48,386] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24462: learning rate 0.0000
[2019-04-04 12:02:48,586] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24500: loss 26.9538
[2019-04-04 12:02:48,587] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24501: learning rate 0.0000
[2019-04-04 12:02:48,854] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 24571: loss 31.9720
[2019-04-04 12:02:48,854] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 24571: learning rate 0.0000
[2019-04-04 12:02:54,361] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.2518966  0.04274941 0.01473766 0.00994458 0.20408916 0.47293377
 0.00364876], sum to 1.0000
[2019-04-04 12:02:54,362] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4214
[2019-04-04 12:02:54,503] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.066666666666666, 83.0, 0.0, 0.0, 26.0, 24.92226451547095, 0.3280111066229367, 0.0, 1.0, 66362.25095387253], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 966000.0000, 
sim time next is 966600.0000, 
raw observation next is [8.25, 83.0, 0.0, 0.0, 26.0, 24.90968958684851, 0.3417273313079007, 0.0, 1.0, 48265.75662024835], 
processed observation next is [1.0, 0.17391304347826086, 0.6911357340720222, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5758074655707093, 0.613909110435967, 0.0, 1.0, 0.22983693628689691], 
reward next is 0.7702, 
noisyNet noise sample is [array([-0.3204973], dtype=float32), 0.36483738]. 
=============================================
[2019-04-04 12:03:03,271] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.13835962 0.03733251 0.01345138 0.0100207  0.34882244 0.44574326
 0.00627008], sum to 1.0000
[2019-04-04 12:03:03,271] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0281
[2019-04-04 12:03:03,412] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [10.33333333333333, 77.66666666666667, 0.0, 0.0, 26.0, 25.61914737887327, 0.6198478012050903, 0.0, 1.0, 39481.33201460761], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1128000.0000, 
sim time next is 1128600.0000, 
raw observation next is [10.25, 78.0, 0.0, 0.0, 24.0, 25.61296905806869, 0.6176785691474754, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.7465373961218837, 0.78, 0.0, 0.0, 0.5, 0.6344140881723908, 0.7058928563824919, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9191599], dtype=float32), 1.9258809]. 
=============================================
[2019-04-04 12:03:04,972] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 30358: loss 32.3409
[2019-04-04 12:03:04,973] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 30358: learning rate 0.0000
[2019-04-04 12:03:07,104] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31259: loss 54.5488
[2019-04-04 12:03:07,109] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31262: learning rate 0.0000
[2019-04-04 12:03:07,135] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31270: loss 27.9012
[2019-04-04 12:03:07,138] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 31270: learning rate 0.0000
[2019-04-04 12:03:07,521] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 31456: loss 35.0934
[2019-04-04 12:03:07,521] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 31456: learning rate 0.0000
[2019-04-04 12:03:07,704] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 31551: loss 27.2456
[2019-04-04 12:03:07,713] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 31553: learning rate 0.0000
[2019-04-04 12:03:07,977] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 31690: loss 40.2332
[2019-04-04 12:03:07,982] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 31693: learning rate 0.0000
[2019-04-04 12:03:08,145] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 31765: loss 35.9609
[2019-04-04 12:03:08,146] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 31765: learning rate 0.0000
[2019-04-04 12:03:08,196] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31791: loss 40.1698
[2019-04-04 12:03:08,201] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31792: learning rate 0.0000
[2019-04-04 12:03:08,247] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.17006525 0.04883239 0.01883409 0.02136612 0.5547922  0.1722348
 0.01387507], sum to 1.0000
[2019-04-04 12:03:08,247] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4511
[2019-04-04 12:03:08,257] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31816: loss 29.4390
[2019-04-04 12:03:08,258] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.96666666666667, 72.33333333333334, 0.0, 0.0, 23.0, 24.24909751494638, 0.2886808175058999, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 1201200.0000, 
sim time next is 1201800.0000, 
raw observation next is [16.78333333333333, 73.66666666666666, 0.0, 0.0, 23.5, 24.22694703613001, 0.2811241269323839, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.927516158818098, 0.7366666666666666, 0.0, 0.0, 0.4583333333333333, 0.5189122530108342, 0.5937080423107947, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9485295], dtype=float32), -0.5448257]. 
=============================================
[2019-04-04 12:03:08,260] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31817: learning rate 0.0000
[2019-04-04 12:03:08,781] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.1928923  0.0688509  0.01995013 0.01766996 0.57383525 0.10831282
 0.01848863], sum to 1.0000
[2019-04-04 12:03:08,783] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7831
[2019-04-04 12:03:08,940] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 65.0, 143.5, 0.0, 21.5, 24.85385926986914, 0.4486510464733575, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1173600.0000, 
sim time next is 1174200.0000, 
raw observation next is [18.3, 65.0, 138.3333333333333, 0.0, 22.0, 24.86538232919005, 0.4481899967251994, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.46111111111111097, 0.0, 0.3333333333333333, 0.572115194099171, 0.6493966655750665, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48889178], dtype=float32), 0.6646046]. 
=============================================
[2019-04-04 12:03:10,113] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32556: loss 37.7245
[2019-04-04 12:03:10,114] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32556: learning rate 0.0000
[2019-04-04 12:03:10,141] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 32564: loss 29.6933
[2019-04-04 12:03:10,154] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 32565: learning rate 0.0000
[2019-04-04 12:03:10,305] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 32636: loss 23.3598
[2019-04-04 12:03:10,311] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 32637: learning rate 0.0000
[2019-04-04 12:03:10,369] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 32677: loss 35.0161
[2019-04-04 12:03:10,370] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 32677: learning rate 0.0000
[2019-04-04 12:03:10,391] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32688: loss 22.8711
[2019-04-04 12:03:10,393] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32689: learning rate 0.0000
[2019-04-04 12:03:10,400] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 32694: loss 24.0329
[2019-04-04 12:03:10,406] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 32694: learning rate 0.0000
[2019-04-04 12:03:11,208] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 33045: loss 23.8088
[2019-04-04 12:03:11,209] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 33045: learning rate 0.0000
[2019-04-04 12:03:16,601] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.22194594 0.03672523 0.00570408 0.00472199 0.37371132 0.35414693
 0.00304455], sum to 1.0000
[2019-04-04 12:03:16,608] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7275
[2019-04-04 12:03:16,629] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.45209817442805, 0.5194929988543294, 0.0, 1.0, 38469.77235423022], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1317000.0000, 
sim time next is 1317600.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.45222234941602, 0.5064085667819748, 0.0, 1.0, 34903.57154448553], 
processed observation next is [1.0, 0.2608695652173913, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6210185291180016, 0.6688028555939916, 0.0, 1.0, 0.1662074835451692], 
reward next is 0.8338, 
noisyNet noise sample is [array([0.33678776], dtype=float32), 0.11751663]. 
=============================================
[2019-04-04 12:03:22,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.7951458e-02 6.6562584e-03 1.4257306e-03 1.4488969e-03 5.9396333e-01
 3.3808130e-01 4.7311070e-04], sum to 1.0000
[2019-04-04 12:03:22,982] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5844
[2019-04-04 12:03:23,033] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.2481594021554, 0.439671006478375, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1359000.0000, 
sim time next is 1359600.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.19126908460576, 0.4924369166864737, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5992724237171467, 0.6641456388954913, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.271617], dtype=float32), -1.6078783]. 
=============================================
[2019-04-04 12:03:30,719] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.20653513 0.01857713 0.00379687 0.00397202 0.57191116 0.19379814
 0.00140963], sum to 1.0000
[2019-04-04 12:03:30,719] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3291
[2019-04-04 12:03:30,732] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 20.0, 23.67439074336865, 0.06273205776814189, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 1488600.0000, 
sim time next is 1489200.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 20.5, 23.57825780798191, 0.0453880923419093, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.20833333333333334, 0.4648548173318258, 0.5151293641139697, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9698589], dtype=float32), 1.2145054]. 
=============================================
[2019-04-04 12:03:30,899] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 38883: loss 27.9659
[2019-04-04 12:03:30,900] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 38883: learning rate 0.0000
[2019-04-04 12:03:31,845] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.1352634  0.03413767 0.00280166 0.00301398 0.48159945 0.34113097
 0.00205287], sum to 1.0000
[2019-04-04 12:03:31,845] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9844
[2019-04-04 12:03:32,036] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 92.66666666666667, 0.0, 0.0, 24.0, 25.36483534550549, 0.4576634672993903, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1477200.0000, 
sim time next is 1477800.0000, 
raw observation next is [2.2, 93.0, 0.0, 0.0, 25.0, 25.42446663176048, 0.4458250684564668, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.93, 0.0, 0.0, 0.5833333333333334, 0.6187055526467068, 0.6486083561521556, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0739998], dtype=float32), 0.5200809]. 
=============================================
[2019-04-04 12:03:32,176] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39183: loss 4.1818
[2019-04-04 12:03:32,203] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39191: learning rate 0.0000
[2019-04-04 12:03:33,549] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39593: loss 26.9924
[2019-04-04 12:03:33,549] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39593: learning rate 0.0000
[2019-04-04 12:03:33,711] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 39640: loss 34.2811
[2019-04-04 12:03:33,711] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 39640: learning rate 0.0000
[2019-04-04 12:03:33,926] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 39704: loss 8.6246
[2019-04-04 12:03:33,927] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 39704: learning rate 0.0000
[2019-04-04 12:03:34,204] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39796: loss 31.6917
[2019-04-04 12:03:34,205] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39796: learning rate 0.0000
[2019-04-04 12:03:34,277] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39815: loss 23.5382
[2019-04-04 12:03:34,278] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39815: learning rate 0.0000
[2019-04-04 12:03:34,399] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 39851: loss 29.5007
[2019-04-04 12:03:34,401] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 39852: learning rate 0.0000
[2019-04-04 12:03:34,777] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 39983: loss 23.9021
[2019-04-04 12:03:34,777] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 39983: learning rate 0.0000
[2019-04-04 12:03:35,516] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40210: loss 18.5049
[2019-04-04 12:03:35,517] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40210: learning rate 0.0000
[2019-04-04 12:03:35,572] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 40232: loss 5.0752
[2019-04-04 12:03:35,587] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 40234: learning rate 0.0000
[2019-04-04 12:03:35,860] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 40332: loss 13.1250
[2019-04-04 12:03:35,860] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 40332: learning rate 0.0000
[2019-04-04 12:03:36,127] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40417: loss 21.8861
[2019-04-04 12:03:36,127] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40417: learning rate 0.0000
[2019-04-04 12:03:36,437] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 40525: loss 23.5653
[2019-04-04 12:03:36,437] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 40525: learning rate 0.0000
[2019-04-04 12:03:36,956] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 40703: loss 24.7018
[2019-04-04 12:03:36,957] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 40703: learning rate 0.0000
[2019-04-04 12:03:37,385] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40862: loss 35.9282
[2019-04-04 12:03:37,411] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40862: learning rate 0.0000
[2019-04-04 12:03:47,314] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3601147e-01 1.2209378e-02 1.4719759e-03 1.2143594e-03 5.2692831e-01
 3.2180530e-01 3.5921769e-04], sum to 1.0000
[2019-04-04 12:03:47,321] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6767
[2019-04-04 12:03:47,464] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 84.66666666666667, 99.0, 0.0, 26.0, 25.61771300532713, 0.4800318106108998, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1685400.0000, 
sim time next is 1686000.0000, 
raw observation next is [1.1, 85.33333333333334, 103.0, 0.0, 26.0, 25.62578273741343, 0.4698932196268375, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.8533333333333334, 0.3433333333333333, 0.0, 0.6666666666666666, 0.6354818947844526, 0.6566310732089459, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.15740669], dtype=float32), 0.34677687]. 
=============================================
[2019-04-04 12:03:47,477] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[17.630104]
 [17.629164]
 [17.507002]
 [17.509642]
 [17.4186  ]], R is [[18.33395195]
 [19.06165695]
 [19.78208542]
 [20.46522331]
 [21.10228729]].
[2019-04-04 12:03:53,180] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.10482487 0.03705198 0.0037164  0.00350755 0.65955776 0.18866193
 0.00267948], sum to 1.0000
[2019-04-04 12:03:53,180] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5100
[2019-04-04 12:03:53,203] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.533333333333333, 87.0, 0.0, 0.0, 26.0, 24.39474535347733, 0.2513102520894124, 0.0, 1.0, 45078.98464701084], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1752000.0000, 
sim time next is 1752600.0000, 
raw observation next is [-1.616666666666667, 87.0, 0.0, 0.0, 26.0, 24.39398652338655, 0.2487643321220333, 0.0, 1.0, 44724.59443779978], 
processed observation next is [0.0, 0.2608695652173913, 0.4178208679593721, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5328322102822124, 0.5829214440406778, 0.0, 1.0, 0.212974259227618], 
reward next is 0.7870, 
noisyNet noise sample is [array([1.0084182], dtype=float32), 0.31925547]. 
=============================================
[2019-04-04 12:03:57,890] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 47412: loss 10.4942
[2019-04-04 12:03:57,891] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 47412: learning rate 0.0000
[2019-04-04 12:03:58,196] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47474: loss 10.0904
[2019-04-04 12:03:58,196] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47474: learning rate 0.0000
[2019-04-04 12:03:59,045] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47647: loss 12.6183
[2019-04-04 12:03:59,046] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47647: learning rate 0.0000
[2019-04-04 12:03:59,581] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47763: loss 11.0102
[2019-04-04 12:03:59,598] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47763: learning rate 0.0000
[2019-04-04 12:03:59,612] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 47770: loss 17.1921
[2019-04-04 12:03:59,613] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 47770: learning rate 0.0000
[2019-04-04 12:03:59,615] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.6826110e-02 1.5943877e-02 1.9805827e-03 1.1272489e-03 7.7337492e-01
 1.3001639e-01 7.3094328e-04], sum to 1.0000
[2019-04-04 12:03:59,615] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9434
[2019-04-04 12:03:59,690] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.02869296510509, 0.2964263475041949, 0.0, 1.0, 18746.91179960846], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1796400.0000, 
sim time next is 1797000.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.03310320494908, 0.2886628305282427, 0.0, 1.0, 25935.22433727663], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5860919337457565, 0.5962209435094142, 0.0, 1.0, 0.12350106827274586], 
reward next is 0.8765, 
noisyNet noise sample is [array([2.082916], dtype=float32), -0.28379735]. 
=============================================
[2019-04-04 12:03:59,710] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[14.200955 ]
 [14.2198105]
 [14.137864 ]
 [14.000495 ]
 [14.034169 ]], R is [[14.81488323]
 [15.5774641 ]
 [16.29848289]
 [16.83637619]
 [17.18906593]].
[2019-04-04 12:04:00,122] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47880: loss 10.9302
[2019-04-04 12:04:00,122] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47880: learning rate 0.0000
[2019-04-04 12:04:00,388] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 47945: loss 20.3141
[2019-04-04 12:04:00,389] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 47945: learning rate 0.0000
[2019-04-04 12:04:00,903] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 48078: loss 14.5657
[2019-04-04 12:04:00,916] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 48078: learning rate 0.0000
[2019-04-04 12:04:01,281] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 48177: loss 11.6450
[2019-04-04 12:04:01,297] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 48178: learning rate 0.0000
[2019-04-04 12:04:01,430] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48228: loss 13.7353
[2019-04-04 12:04:01,431] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48228: learning rate 0.0000
[2019-04-04 12:04:01,441] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 48232: loss 11.6503
[2019-04-04 12:04:01,442] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 48232: learning rate 0.0000
[2019-04-04 12:04:02,043] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 48422: loss 11.8465
[2019-04-04 12:04:02,044] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 48422: learning rate 0.0000
[2019-04-04 12:04:02,529] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48565: loss 6.7210
[2019-04-04 12:04:02,530] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48565: learning rate 0.0000
[2019-04-04 12:04:02,823] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 48654: loss 11.3059
[2019-04-04 12:04:02,826] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 48654: learning rate 0.0000
[2019-04-04 12:04:02,945] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 48693: loss 16.2421
[2019-04-04 12:04:02,945] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 48693: learning rate 0.0000
[2019-04-04 12:04:02,951] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.08113131 0.02177011 0.00303027 0.00216793 0.6096192  0.2809367
 0.00134453], sum to 1.0000
[2019-04-04 12:04:02,953] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8343
[2019-04-04 12:04:03,041] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.600000000000001, 77.5, 129.3333333333333, 65.33333333333333, 26.0, 25.10322234372908, 0.2625506522294423, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1851000.0000, 
sim time next is 1851600.0000, 
raw observation next is [-5.6, 77.0, 124.6666666666667, 58.16666666666666, 26.0, 25.20715713539423, 0.2592442829650836, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.77, 0.4155555555555557, 0.06427255985267034, 0.6666666666666666, 0.6005964279495192, 0.5864147609883612, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6087703], dtype=float32), 0.020891627]. 
=============================================
[2019-04-04 12:04:03,405] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 48827: loss 14.5049
[2019-04-04 12:04:03,406] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 48827: learning rate 0.0000
[2019-04-04 12:04:05,825] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.12591718 0.03216031 0.00505782 0.00529553 0.62523973 0.20275259
 0.00357673], sum to 1.0000
[2019-04-04 12:04:05,825] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2052
[2019-04-04 12:04:05,851] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 22.96881504914493, -0.1699977735164335, 0.0, 1.0, 47515.30071744363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1842000.0000, 
sim time next is 1842600.0000, 
raw observation next is [-6.7, 78.0, 9.666666666666664, 0.0, 26.0, 22.94575163935632, -0.1738086392875953, 0.0, 1.0, 47470.74165043855], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.032222222222222215, 0.0, 0.6666666666666666, 0.41214596994636005, 0.4420637869041349, 0.0, 1.0, 0.22605115071637402], 
reward next is 0.7739, 
noisyNet noise sample is [array([1.8727486], dtype=float32), 0.7412413]. 
=============================================
[2019-04-04 12:04:09,081] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.0453792e-02 5.5457274e-03 1.7720647e-03 6.4052379e-04 8.6845344e-01
 8.2304932e-02 8.2950515e-04], sum to 1.0000
[2019-04-04 12:04:09,085] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2903
[2019-04-04 12:04:09,134] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.416666666666667, 85.5, 0.0, 0.0, 21.5, 24.24219562950616, 0.02702021683376766, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1885800.0000, 
sim time next is 1886400.0000, 
raw observation next is [-5.6, 86.0, 0.0, 0.0, 22.0, 24.06303385594756, -0.007860820397870782, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.86, 0.0, 0.0, 0.3333333333333333, 0.5052528213289632, 0.4973797265340431, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6276213], dtype=float32), 0.105916776]. 
=============================================
[2019-04-04 12:04:09,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0514551e-02 1.0363839e-02 1.1906396e-03 1.0316279e-03 8.2764447e-01
 1.1882565e-01 4.2918342e-04], sum to 1.0000
[2019-04-04 12:04:09,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2161
[2019-04-04 12:04:09,255] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.8, 80.33333333333334, 0.0, 0.0, 26.0, 24.4252694771799, 0.1107909053039526, 0.0, 1.0, 49317.45639382321], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1891200.0000, 
sim time next is 1891800.0000, 
raw observation next is [-5.9, 79.0, 0.0, 0.0, 26.0, 24.41622332269019, 0.1071086708954108, 0.0, 1.0, 46371.80644898847], 
processed observation next is [0.0, 0.9130434782608695, 0.2991689750692521, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5346852768908491, 0.5357028902984703, 0.0, 1.0, 0.22081812594756414], 
reward next is 0.7792, 
noisyNet noise sample is [array([0.7692355], dtype=float32), -1.2075588]. 
=============================================
[2019-04-04 12:04:13,490] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.23794995e-01 1.25983441e-02 1.60459324e-03 9.01994295e-04
 7.40190983e-01 1.20453611e-01 4.55460773e-04], sum to 1.0000
[2019-04-04 12:04:13,490] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3434
[2019-04-04 12:04:13,572] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.633333333333333, 84.83333333333334, 67.66666666666667, 373.0000000000001, 24.5, 25.54709676448278, 0.2429023641635257, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1933800.0000, 
sim time next is 1934400.0000, 
raw observation next is [-8.366666666666667, 83.66666666666667, 76.33333333333334, 461.0, 25.0, 25.61179244252537, 0.266482561648837, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.23084025854108958, 0.8366666666666667, 0.2544444444444445, 0.5093922651933702, 0.5833333333333334, 0.6343160368771142, 0.5888275205496124, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3324942], dtype=float32), -0.6552034]. 
=============================================
[2019-04-04 12:04:30,463] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 55041: loss 9.0801
[2019-04-04 12:04:30,465] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 55041: learning rate 0.0000
[2019-04-04 12:04:31,909] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55440: loss 15.4614
[2019-04-04 12:04:31,919] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55440: learning rate 0.0000
[2019-04-04 12:04:32,096] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55488: loss 7.1704
[2019-04-04 12:04:32,096] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55488: learning rate 0.0000
[2019-04-04 12:04:32,651] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55635: loss 8.1175
[2019-04-04 12:04:32,652] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55635: learning rate 0.0000
[2019-04-04 12:04:32,802] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 55674: loss 15.1890
[2019-04-04 12:04:32,818] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 55674: learning rate 0.0000
[2019-04-04 12:04:33,016] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 55728: loss 4.3964
[2019-04-04 12:04:33,023] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 55728: learning rate 0.0000
[2019-04-04 12:04:33,046] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 55733: loss 4.0782
[2019-04-04 12:04:33,050] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 55733: learning rate 0.0000
[2019-04-04 12:04:33,268] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55774: loss 2.3267
[2019-04-04 12:04:33,268] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55774: learning rate 0.0000
[2019-04-04 12:04:33,568] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 55841: loss 11.9933
[2019-04-04 12:04:33,570] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 55841: learning rate 0.0000
[2019-04-04 12:04:33,968] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 55935: loss 5.6214
[2019-04-04 12:04:33,969] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 55935: learning rate 0.0000
[2019-04-04 12:04:34,303] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56012: loss 2.1152
[2019-04-04 12:04:34,304] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56012: learning rate 0.0000
[2019-04-04 12:04:34,712] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 56107: loss 18.3698
[2019-04-04 12:04:34,748] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 56107: learning rate 0.0000
[2019-04-04 12:04:35,491] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56276: loss 7.6835
[2019-04-04 12:04:35,491] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56276: learning rate 0.0000
[2019-04-04 12:04:35,938] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 56371: loss 6.4487
[2019-04-04 12:04:35,939] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 56371: learning rate 0.0000
[2019-04-04 12:04:36,502] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 56485: loss 4.5101
[2019-04-04 12:04:36,506] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 56485: learning rate 0.0000
[2019-04-04 12:04:37,107] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56597: loss 4.6269
[2019-04-04 12:04:37,108] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56597: learning rate 0.0000
[2019-04-04 12:04:39,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9726835e-02 4.0434473e-03 8.5888802e-05 5.6676043e-05 9.1292328e-01
 4.3146592e-02 1.7400396e-05], sum to 1.0000
[2019-04-04 12:04:39,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1826
[2019-04-04 12:04:39,928] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.8, 66.66666666666666, 145.0, 0.0, 26.0, 26.30708212122071, 0.5281408626146898, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2122800.0000, 
sim time next is 2123400.0000, 
raw observation next is [-5.7, 67.33333333333334, 141.0, 0.0, 26.0, 26.37123412109732, 0.5307050667237873, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.30470914127423826, 0.6733333333333335, 0.47, 0.0, 0.6666666666666666, 0.6976028434247766, 0.676901688907929, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2127689], dtype=float32), 0.77341187]. 
=============================================
[2019-04-04 12:04:49,731] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4952769e-02 2.9867266e-03 2.2492777e-04 2.2715805e-04 9.0912765e-01
 6.2409569e-02 7.1175411e-05], sum to 1.0000
[2019-04-04 12:04:49,731] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5321
[2019-04-04 12:04:49,746] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.800000000000001, 76.16666666666667, 0.0, 0.0, 26.0, 24.30587902085671, 0.1477426871905814, 0.0, 1.0, 44228.91876387902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2250600.0000, 
sim time next is 2251200.0000, 
raw observation next is [-6.9, 77.33333333333334, 0.0, 0.0, 26.0, 24.24857776594133, 0.1414496609719275, 0.0, 1.0, 44176.72355151574], 
processed observation next is [1.0, 0.043478260869565216, 0.27146814404432135, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5207148138284442, 0.5471498869906425, 0.0, 1.0, 0.21036535024531303], 
reward next is 0.7896, 
noisyNet noise sample is [array([-0.9814459], dtype=float32), -0.36024624]. 
=============================================
[2019-04-04 12:04:50,591] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4910260e-02 2.4989946e-03 8.4630730e-05 6.3265492e-05 9.2196155e-01
 3.0427674e-02 5.3594398e-05], sum to 1.0000
[2019-04-04 12:04:50,592] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0949
[2019-04-04 12:04:50,637] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.5, 70.5, 0.0, 0.0, 26.0, 25.10949004877282, 0.3673764438116342, 0.0, 1.0, 44740.74056840398], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2238600.0000, 
sim time next is 2239200.0000, 
raw observation next is [-5.6, 71.0, 0.0, 0.0, 26.0, 25.0884920314915, 0.3635281007219286, 0.0, 1.0, 44621.41943832748], 
processed observation next is [1.0, 0.9565217391304348, 0.30747922437673136, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5907076692909584, 0.6211760335739762, 0.0, 1.0, 0.21248294970632134], 
reward next is 0.7875, 
noisyNet noise sample is [array([-0.87721837], dtype=float32), 0.3008788]. 
=============================================
[2019-04-04 12:05:07,142] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63573: loss 10.6590
[2019-04-04 12:05:07,143] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63573: learning rate 0.0000
[2019-04-04 12:05:07,206] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63584: loss 11.3116
[2019-04-04 12:05:07,213] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63584: learning rate 0.0000
[2019-04-04 12:05:07,361] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 63605: loss 14.4807
[2019-04-04 12:05:07,373] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 63605: learning rate 0.0000
[2019-04-04 12:05:07,750] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63674: loss 4.7112
[2019-04-04 12:05:07,781] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63674: learning rate 0.0000
[2019-04-04 12:05:08,003] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 63719: loss 17.8044
[2019-04-04 12:05:08,004] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 63719: learning rate 0.0000
[2019-04-04 12:05:08,149] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 63751: loss 7.2205
[2019-04-04 12:05:08,149] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 63751: learning rate 0.0000
[2019-04-04 12:05:09,126] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 63942: loss 4.8744
[2019-04-04 12:05:09,126] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 63942: learning rate 0.0000
[2019-04-04 12:05:09,262] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 63959: loss 4.7981
[2019-04-04 12:05:09,263] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 63959: learning rate 0.0000
[2019-04-04 12:05:09,593] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 64022: loss 3.7889
[2019-04-04 12:05:09,596] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 64022: learning rate 0.0000
[2019-04-04 12:05:09,783] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64053: loss 4.4684
[2019-04-04 12:05:09,784] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64053: learning rate 0.0000
[2019-04-04 12:05:09,975] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64095: loss 13.0481
[2019-04-04 12:05:09,978] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64095: learning rate 0.0000
[2019-04-04 12:05:10,488] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64216: loss 5.3550
[2019-04-04 12:05:10,488] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64216: learning rate 0.0000
[2019-04-04 12:05:11,427] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 64417: loss 4.0360
[2019-04-04 12:05:11,455] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 64417: learning rate 0.0000
[2019-04-04 12:05:12,230] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 64598: loss 4.8414
[2019-04-04 12:05:12,231] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 64600: learning rate 0.0000
[2019-04-04 12:05:12,621] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 64703: loss 5.3346
[2019-04-04 12:05:12,626] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 64704: learning rate 0.0000
[2019-04-04 12:05:12,918] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 64771: loss 5.2679
[2019-04-04 12:05:12,919] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 64771: learning rate 0.0000
[2019-04-04 12:05:40,821] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71212: loss 3.9534
[2019-04-04 12:05:40,871] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71212: learning rate 0.0000
[2019-04-04 12:05:41,512] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71341: loss 6.5232
[2019-04-04 12:05:41,531] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71341: learning rate 0.0000
[2019-04-04 12:05:41,874] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 71406: loss 7.1286
[2019-04-04 12:05:41,875] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 71406: learning rate 0.0000
[2019-04-04 12:05:43,033] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 71592: loss 8.8462
[2019-04-04 12:05:43,041] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 71592: learning rate 0.0000
[2019-04-04 12:05:43,182] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 71608: loss 4.4456
[2019-04-04 12:05:43,210] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 71608: learning rate 0.0000
[2019-04-04 12:05:43,401] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71646: loss 4.3582
[2019-04-04 12:05:43,401] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71646: learning rate 0.0000
[2019-04-04 12:05:43,564] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71667: loss 9.8053
[2019-04-04 12:05:43,605] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71667: learning rate 0.0000
[2019-04-04 12:05:44,221] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 71766: loss 4.9198
[2019-04-04 12:05:44,223] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 71766: learning rate 0.0000
[2019-04-04 12:05:44,694] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 71836: loss 4.4942
[2019-04-04 12:05:44,729] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 71836: learning rate 0.0000
[2019-04-04 12:05:45,511] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 71944: loss 5.7874
[2019-04-04 12:05:45,534] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 71944: learning rate 0.0000
[2019-04-04 12:05:46,778] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72145: loss 14.9075
[2019-04-04 12:05:46,781] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72145: learning rate 0.0000
[2019-04-04 12:05:47,210] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 72227: loss 3.5753
[2019-04-04 12:05:47,217] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 72230: learning rate 0.0000
[2019-04-04 12:05:47,212] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1802624e-02 3.5036425e-03 5.0915187e-05 3.1872365e-05 9.0843570e-01
 3.6140226e-02 3.5094530e-05], sum to 1.0000
[2019-04-04 12:05:47,265] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3792
[2019-04-04 12:05:47,363] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.16666666666667, 83.0, 0.0, 0.0, 26.0, 22.36736399664741, -0.2516356016374585, 0.0, 1.0, 45466.41246603589], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2703000.0000, 
sim time next is 2703600.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.36033774628662, -0.2606160483297565, 0.0, 1.0, 44556.26577339197], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.6666666666666666, 0.3633614788572184, 0.4131279838900812, 0.0, 1.0, 0.21217269415900938], 
reward next is 0.7878, 
noisyNet noise sample is [array([-1.0117884], dtype=float32), -0.22260179]. 
=============================================
[2019-04-04 12:05:48,638] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 72504: loss 6.0277
[2019-04-04 12:05:48,639] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 72504: learning rate 0.0000
[2019-04-04 12:05:49,003] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4500, global step 72587: loss 4.1965
[2019-04-04 12:05:49,003] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4500, global step 72587: learning rate 0.0000
[2019-04-04 12:05:49,580] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 72716: loss 10.2890
[2019-04-04 12:05:49,587] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 72716: learning rate 0.0000
[2019-04-04 12:05:49,900] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 72791: loss 5.0851
[2019-04-04 12:05:49,905] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 72791: learning rate 0.0000
[2019-04-04 12:06:19,515] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.24679774e-03 1.22336636e-03 6.95137214e-06 1.18507605e-05
 9.88941908e-01 3.56071000e-03 8.44776969e-06], sum to 1.0000
[2019-04-04 12:06:19,522] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5238
[2019-04-04 12:06:19,562] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 157.3333333333333, 727.3333333333334, 26.0, 25.07131546030471, 0.3973743815888571, 0.0, 1.0, 24069.90515125724], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2983800.0000, 
sim time next is 2984400.0000, 
raw observation next is [-3.0, 65.0, 145.5, 745.5, 26.0, 25.06980478829522, 0.399226206316848, 0.0, 1.0, 29650.95737263362], 
processed observation next is [0.0, 0.5652173913043478, 0.3795013850415513, 0.65, 0.485, 0.8237569060773481, 0.6666666666666666, 0.5891503990246015, 0.6330754021056161, 0.0, 1.0, 0.14119503510777914], 
reward next is 0.8588, 
noisyNet noise sample is [array([0.13682206], dtype=float32), 0.39821756]. 
=============================================
[2019-04-04 12:06:21,203] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79229: loss 3.5715
[2019-04-04 12:06:21,204] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79229: learning rate 0.0000
[2019-04-04 12:06:21,471] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 79249: loss 3.5293
[2019-04-04 12:06:21,473] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 79250: learning rate 0.0000
[2019-04-04 12:06:21,937] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 79326: loss 3.6068
[2019-04-04 12:06:21,938] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 79326: learning rate 0.0000
[2019-04-04 12:06:22,023] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9114252e-02 6.9001592e-03 3.1086329e-05 2.9749470e-05 9.4078994e-01
 1.3103213e-02 3.1482894e-05], sum to 1.0000
[2019-04-04 12:06:22,024] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4840
[2019-04-04 12:06:22,067] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 166.0, 78.0, 25.0, 24.99462464252637, 0.2807292748012349, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.5], 
sim time this is 2973600.0000, 
sim time next is 2974200.0000, 
raw observation next is [-3.833333333333333, 70.0, 170.0, 59.99999999999999, 25.5, 24.88663035289047, 0.2588911125648358, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3564173591874424, 0.7, 0.5666666666666667, 0.06629834254143646, 0.625, 0.5738858627408726, 0.586297037521612, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3573503], dtype=float32), -0.03390204]. 
=============================================
[2019-04-04 12:06:22,135] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 79367: loss 3.6395
[2019-04-04 12:06:22,135] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 79367: learning rate 0.0000
[2019-04-04 12:06:22,651] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79487: loss 3.6750
[2019-04-04 12:06:22,652] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79487: learning rate 0.0000
[2019-04-04 12:06:23,919] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 79811: loss 3.7000
[2019-04-04 12:06:23,925] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 79814: learning rate 0.0000
[2019-04-04 12:06:24,386] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 79938: loss 3.6164
[2019-04-04 12:06:24,386] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 79938: learning rate 0.0000
[2019-04-04 12:06:24,789] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80044: loss 3.6539
[2019-04-04 12:06:24,791] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80044: learning rate 0.0000
[2019-04-04 12:06:24,793] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2893923e-02 1.9088876e-03 7.6952711e-06 6.8272166e-06 9.7934204e-01
 5.8375373e-03 3.0731060e-06], sum to 1.0000
[2019-04-04 12:06:24,794] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3525
[2019-04-04 12:06:24,861] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 80059: loss 3.3970
[2019-04-04 12:06:24,862] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.333333333333333, 61.66666666666667, 108.5, 791.8333333333334, 26.0, 25.35427519751338, 0.4420731363587194, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2986800.0000, 
sim time next is 2987400.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 107.0, 783.6666666666666, 26.0, 25.31756433860159, 0.4347085451750614, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.4025854108956602, 0.6083333333333333, 0.3566666666666667, 0.8659300184162062, 0.6666666666666666, 0.609797028216799, 0.6449028483916871, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5578715], dtype=float32), 2.2455485]. 
=============================================
[2019-04-04 12:06:24,868] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 80059: learning rate 0.0000
[2019-04-04 12:06:25,532] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.8814524e-02 1.3134244e-02 1.9247370e-04 2.0060304e-04 8.7083125e-01
 4.6764560e-02 6.2267376e-05], sum to 1.0000
[2019-04-04 12:06:25,533] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8521
[2019-04-04 12:06:25,547] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.90823873011209, 0.02998881212630633, 0.0, 1.0, 40250.29099549349], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3041400.0000, 
sim time next is 3042000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.8823155774353, 0.02388531672118857, 0.0, 1.0, 40236.21227746744], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.490192964786275, 0.5079617722403962, 0.0, 1.0, 0.19160101084508305], 
reward next is 0.8084, 
noisyNet noise sample is [array([-0.19873685], dtype=float32), -0.60285026]. 
=============================================
[2019-04-04 12:06:25,581] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[23.770823]
 [23.752762]
 [23.741472]
 [23.732979]
 [23.734896]], R is [[24.38968468]
 [24.95412064]
 [25.51290894]
 [26.06617737]
 [26.61405945]].
[2019-04-04 12:06:25,653] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80295: loss 3.4472
[2019-04-04 12:06:25,653] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80295: learning rate 0.0000
[2019-04-04 12:06:25,936] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 80368: loss 3.5911
[2019-04-04 12:06:25,936] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 80368: learning rate 0.0000
[2019-04-04 12:06:26,033] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 80381: loss 3.6666
[2019-04-04 12:06:26,034] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 80381: learning rate 0.0000
[2019-04-04 12:06:26,989] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 80665: loss 3.4834
[2019-04-04 12:06:26,991] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 80665: learning rate 0.0000
[2019-04-04 12:06:27,132] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 80685: loss 3.4005
[2019-04-04 12:06:27,132] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 80685: learning rate 0.0000
[2019-04-04 12:06:27,416] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 80766: loss 3.5864
[2019-04-04 12:06:27,417] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 80766: learning rate 0.0000
[2019-04-04 12:06:27,635] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5000, global step 80813: loss 3.5005
[2019-04-04 12:06:27,636] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5000, global step 80813: learning rate 0.0000
[2019-04-04 12:06:38,354] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7351067e-04 1.1107177e-05 3.2197691e-09 3.5293719e-09 9.9956411e-01
 5.1220686e-05 2.1622115e-09], sum to 1.0000
[2019-04-04 12:06:38,356] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4184
[2019-04-04 12:06:38,379] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.933333333333334, 99.83333333333334, 89.66666666666666, 707.0, 26.0, 27.09975765758591, 0.9090669961041357, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3165000.0000, 
sim time next is 3165600.0000, 
raw observation next is [6.866666666666667, 99.66666666666667, 86.83333333333333, 693.0, 26.0, 26.29147566157762, 0.8502106110996973, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6528162511542014, 0.9966666666666667, 0.28944444444444445, 0.7657458563535912, 0.6666666666666666, 0.6909563051314684, 0.7834035370332324, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9634576], dtype=float32), -0.77825695]. 
=============================================
[2019-04-04 12:06:41,688] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 86964: loss 2.6775
[2019-04-04 12:06:41,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 86964: learning rate 0.0000
[2019-04-04 12:06:41,805] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 86998: loss 2.6568
[2019-04-04 12:06:41,806] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 86998: learning rate 0.0000
[2019-04-04 12:06:41,894] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87032: loss 4.3074
[2019-04-04 12:06:41,897] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87033: learning rate 0.0000
[2019-04-04 12:06:42,541] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 87298: loss 2.9837
[2019-04-04 12:06:42,541] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 87298: learning rate 0.0000
[2019-04-04 12:06:42,629] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 87341: loss 3.1462
[2019-04-04 12:06:42,629] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 87341: learning rate 0.0000
[2019-04-04 12:06:43,046] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 87526: loss 3.1750
[2019-04-04 12:06:43,047] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 87526: learning rate 0.0000
[2019-04-04 12:06:43,399] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87653: loss 3.0135
[2019-04-04 12:06:43,402] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87655: learning rate 0.0000
[2019-04-04 12:06:43,551] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6056662e-02 1.1032216e-03 2.4786398e-06 5.1031489e-06 9.6766961e-01
 1.5161423e-02 1.4678453e-06], sum to 1.0000
[2019-04-04 12:06:43,554] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5182
[2019-04-04 12:06:43,566] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.95655970889387, 0.3459695589899023, 0.0, 1.0, 43832.98471131865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3292200.0000, 
sim time next is 3292800.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.96802776200863, 0.3321373838285979, 0.0, 1.0, 43833.01533112016], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.580668980167386, 0.6107124612761993, 0.0, 1.0, 0.2087286444339055], 
reward next is 0.7913, 
noisyNet noise sample is [array([-0.07190438], dtype=float32), -0.50845397]. 
=============================================
[2019-04-04 12:06:43,885] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 87831: loss 3.1294
[2019-04-04 12:06:43,891] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 87831: learning rate 0.0000
[2019-04-04 12:06:44,099] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 87908: loss 3.1042
[2019-04-04 12:06:44,111] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 87908: learning rate 0.0000
[2019-04-04 12:06:44,573] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 88066: loss 3.1967
[2019-04-04 12:06:44,581] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 88066: learning rate 0.0000
[2019-04-04 12:06:45,275] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 88352: loss 3.0400
[2019-04-04 12:06:45,276] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 88352: learning rate 0.0000
[2019-04-04 12:06:45,347] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 88382: loss 3.0159
[2019-04-04 12:06:45,348] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 88382: learning rate 0.0000
[2019-04-04 12:06:45,871] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 88601: loss 2.9986
[2019-04-04 12:06:45,872] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 88601: learning rate 0.0000
[2019-04-04 12:06:46,304] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 88757: loss 2.9602
[2019-04-04 12:06:46,317] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 88760: learning rate 0.0000
[2019-04-04 12:06:46,427] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 88807: loss 2.5609
[2019-04-04 12:06:46,428] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 88807: learning rate 0.0000
[2019-04-04 12:06:46,673] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5500, global step 88912: loss 3.2982
[2019-04-04 12:06:46,674] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5500, global step 88912: learning rate 0.0000
[2019-04-04 12:06:49,615] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.3171480e-03 1.9319153e-04 3.3003897e-07 5.3143685e-07 9.9304527e-01
 4.4336930e-04 2.3703312e-07], sum to 1.0000
[2019-04-04 12:06:49,616] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1394
[2019-04-04 12:06:49,672] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.833333333333333, 60.0, 58.66666666666667, 317.0, 26.0, 25.52367422410871, 0.372548782147148, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3399000.0000, 
sim time next is 3399600.0000, 
raw observation next is [-1.666666666666667, 60.00000000000001, 72.83333333333333, 369.5, 26.0, 25.46170512142805, 0.4142741520952674, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4164358264081256, 0.6000000000000001, 0.24277777777777776, 0.40828729281767956, 0.6666666666666666, 0.6218087601190042, 0.6380913840317558, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30085582], dtype=float32), 0.89778394]. 
=============================================
[2019-04-04 12:06:51,998] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8313111e-03 1.8270040e-04 6.3017990e-07 1.1238394e-06 9.9334586e-01
 2.6379027e-03 5.1805358e-07], sum to 1.0000
[2019-04-04 12:06:52,000] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1685
[2019-04-04 12:06:52,031] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83873865439725, 0.2827908939579101, 0.0, 1.0, 41086.97217877777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3380400.0000, 
sim time next is 3381000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.82983611636828, 0.2770702436848537, 0.0, 1.0, 41093.06940848121], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5691530096973567, 0.5923567478949513, 0.0, 1.0, 0.1956812828975296], 
reward next is 0.8043, 
noisyNet noise sample is [array([1.2854615], dtype=float32), -0.23169352]. 
=============================================
[2019-04-04 12:06:52,049] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[33.794434]
 [34.03515 ]
 [34.391056]
 [34.508835]
 [34.534874]], R is [[34.09281158]
 [34.55623245]
 [35.01493073]
 [35.46894073]
 [35.91834641]].
[2019-04-04 12:06:52,098] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2874031e-04 6.1772582e-05 1.7735928e-08 2.1360384e-08 9.9911350e-01
 3.9594428e-04 3.8046384e-09], sum to 1.0000
[2019-04-04 12:06:52,098] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7732
[2019-04-04 12:06:52,114] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.16099401576238, 0.4202403266787191, 0.0, 1.0, 36044.73740606607], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3439200.0000, 
sim time next is 3439800.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.0568852535542, 0.4147405944897296, 1.0, 1.0, 84704.93357160893], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5880737711295166, 0.6382468648299099, 1.0, 1.0, 0.4033568265314711], 
reward next is 0.5966, 
noisyNet noise sample is [array([0.02254718], dtype=float32), -0.41834742]. 
=============================================
[2019-04-04 12:06:53,149] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.2485766e-03 2.1443103e-04 4.2858499e-08 7.1292440e-08 9.9332446e-01
 2.1245937e-04 2.6299910e-08], sum to 1.0000
[2019-04-04 12:06:53,150] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7164
[2019-04-04 12:06:53,168] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.49393415727337, 0.4645406721934712, 0.0, 1.0, 18753.39760808759], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3454800.0000, 
sim time next is 3455400.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.43972939192177, 0.4550179241403701, 0.0, 1.0, 50854.16057094983], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6199774493268141, 0.6516726413801234, 0.0, 1.0, 0.2421626693854754], 
reward next is 0.7578, 
noisyNet noise sample is [array([-0.4872735], dtype=float32), 0.48528135]. 
=============================================
[2019-04-04 12:06:53,874] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.72927650e-04 2.45642514e-05 1.24458790e-08 7.25192173e-09
 9.99202430e-01 1.00062505e-04 2.90623614e-09], sum to 1.0000
[2019-04-04 12:06:53,874] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5483
[2019-04-04 12:06:53,899] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 67.0, 36.5, 317.0, 26.0, 26.44769994931224, 0.4650900142275102, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3430800.0000, 
sim time next is 3431400.0000, 
raw observation next is [2.0, 67.0, 28.33333333333333, 251.6666666666666, 26.0, 26.3785231838343, 0.5950681447320273, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.09444444444444443, 0.27808471454880285, 0.6666666666666666, 0.698210265319525, 0.6983560482440091, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.96187866], dtype=float32), -1.1783065]. 
=============================================
[2019-04-04 12:07:00,263] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.7697587e-04 1.9810861e-05 7.5906863e-09 7.2309230e-09 9.9932182e-01
 8.1326914e-05 1.0990441e-08], sum to 1.0000
[2019-04-04 12:07:00,263] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3390
[2019-04-04 12:07:00,284] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 70.0, 0.0, 0.0, 26.0, 25.27182688185953, 0.4554780612027401, 0.0, 1.0, 141223.823008861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3537600.0000, 
sim time next is 3538200.0000, 
raw observation next is [-1.0, 68.0, 0.0, 0.0, 26.0, 25.20321137157486, 0.4618018700931414, 0.0, 1.0, 83718.43906412376], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.68, 0.0, 0.0, 0.6666666666666666, 0.600267614297905, 0.6539339566977138, 0.0, 1.0, 0.39865923363868455], 
reward next is 0.6013, 
noisyNet noise sample is [array([0.45692357], dtype=float32), -0.21837227]. 
=============================================
[2019-04-04 12:07:00,551] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 94957: loss 5.1664
[2019-04-04 12:07:00,554] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 94957: learning rate 0.0000
[2019-04-04 12:07:00,859] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95105: loss 4.9739
[2019-04-04 12:07:00,860] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95105: learning rate 0.0000
[2019-04-04 12:07:01,106] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 95195: loss 5.1721
[2019-04-04 12:07:01,107] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 95195: learning rate 0.0000
[2019-04-04 12:07:01,669] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 95428: loss 4.9340
[2019-04-04 12:07:01,679] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 95428: learning rate 0.0000
[2019-04-04 12:07:01,769] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 95472: loss 5.0905
[2019-04-04 12:07:01,770] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 95472: learning rate 0.0000
[2019-04-04 12:07:01,784] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 95482: loss 4.9171
[2019-04-04 12:07:01,785] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 95482: learning rate 0.0000
[2019-04-04 12:07:02,065] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2185866e-03 1.8855305e-04 6.3415314e-07 9.9883005e-07 9.9638331e-01
 1.2073779e-03 5.1467322e-07], sum to 1.0000
[2019-04-04 12:07:02,066] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5663
[2019-04-04 12:07:02,096] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.166666666666667, 43.33333333333333, 0.0, 0.0, 26.0, 25.47883775655748, 0.3956757724263357, 0.0, 1.0, 47303.02806313649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3618600.0000, 
sim time next is 3619200.0000, 
raw observation next is [-1.333333333333333, 44.66666666666667, 0.0, 0.0, 26.0, 25.44540210939557, 0.391648412301397, 0.0, 1.0, 56853.44669617689], 
processed observation next is [0.0, 0.9130434782608695, 0.42566943674976926, 0.4466666666666667, 0.0, 0.0, 0.6666666666666666, 0.6204501757829641, 0.6305494707671323, 0.0, 1.0, 0.2707306985532233], 
reward next is 0.7293, 
noisyNet noise sample is [array([-2.2412384], dtype=float32), -0.827001]. 
=============================================
[2019-04-04 12:07:02,418] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 95726: loss 5.0592
[2019-04-04 12:07:02,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 95726: learning rate 0.0000
[2019-04-04 12:07:02,842] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 95898: loss 5.0274
[2019-04-04 12:07:02,846] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 95900: learning rate 0.0000
[2019-04-04 12:07:03,220] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 96088: loss 4.7775
[2019-04-04 12:07:03,221] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 96088: learning rate 0.0000
[2019-04-04 12:07:03,501] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 96219: loss 4.6316
[2019-04-04 12:07:03,513] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 96220: learning rate 0.0000
[2019-04-04 12:07:04,038] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 96448: loss 4.9873
[2019-04-04 12:07:04,040] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 96448: learning rate 0.0000
[2019-04-04 12:07:04,260] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 96559: loss 4.8383
[2019-04-04 12:07:04,260] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 96559: learning rate 0.0000
[2019-04-04 12:07:04,315] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 96587: loss 4.8956
[2019-04-04 12:07:04,317] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 96588: learning rate 0.0000
[2019-04-04 12:07:05,554] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 97261: loss 4.7208
[2019-04-04 12:07:05,556] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 97263: learning rate 0.0000
[2019-04-04 12:07:05,604] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 97297: loss 4.7846
[2019-04-04 12:07:05,614] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 97298: learning rate 0.0000
[2019-04-04 12:07:05,620] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6000, global step 97302: loss 4.9240
[2019-04-04 12:07:05,621] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6000, global step 97302: learning rate 0.0000
[2019-04-04 12:07:08,243] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4792857e-03 7.8203542e-05 7.9909441e-08 1.3347085e-07 9.9702817e-01
 4.1412379e-04 3.6734075e-08], sum to 1.0000
[2019-04-04 12:07:08,244] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4791
[2019-04-04 12:07:08,262] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.10831785847398, 0.2777604390983919, 0.0, 1.0, 41755.41366716981], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3736800.0000, 
sim time next is 3737400.0000, 
raw observation next is [-3.166666666666667, 67.0, 0.0, 0.0, 26.0, 25.09278052648479, 0.2746062354389305, 0.0, 1.0, 41862.86044149951], 
processed observation next is [1.0, 0.2608695652173913, 0.3748845798707295, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5910650438737326, 0.5915354118129769, 0.0, 1.0, 0.199346954483331], 
reward next is 0.8007, 
noisyNet noise sample is [array([0.79365623], dtype=float32), -0.10316301]. 
=============================================
[2019-04-04 12:07:10,602] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 12:07:10,604] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 12:07:10,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:07:10,606] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 12:07:10,606] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 12:07:10,606] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:07:10,607] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:07:10,610] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run2
[2019-04-04 12:07:10,611] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run2
[2019-04-04 12:07:10,635] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run2
[2019-04-04 12:07:27,085] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.08366178], dtype=float32), 0.09162004]
[2019-04-04 12:07:27,085] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-9.5, 42.33333333333333, 78.0, 574.3333333333334, 26.0, 25.66320194668251, 0.4266233959528741, 1.0, 1.0, 183183.8592561689]
[2019-04-04 12:07:27,085] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 12:07:27,086] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.4306770e-04 4.2789117e-05 2.2555730e-08 2.4588983e-08 9.9922800e-01
 8.6073953e-05 2.5909642e-08], sampled 0.3195163467891482
[2019-04-04 12:07:43,820] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.08366178], dtype=float32), 0.09162004]
[2019-04-04 12:07:43,820] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-5.7, 61.5, 0.0, 0.0, 26.0, 24.91987680545366, 0.2962169794563815, 0.0, 1.0, 44290.37201233492]
[2019-04-04 12:07:43,820] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 12:07:43,821] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.6924731e-04 6.3250372e-05 5.2597496e-08 9.3247600e-08 9.9864274e-01
 3.2464543e-04 3.9225302e-08], sampled 0.5064517590496573
[2019-04-04 12:08:41,282] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.08366178], dtype=float32), 0.09162004]
[2019-04-04 12:08:41,282] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.280394829, 97.9677408, 50.61573457, 0.0, 26.0, 25.34177489332113, 0.3016022029221261, 1.0, 1.0, 0.0]
[2019-04-04 12:08:41,282] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 12:08:41,283] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.7919239e-04 2.0044143e-05 7.3049988e-09 1.6032491e-08 9.9943167e-01
 6.9099318e-05 6.7095196e-09], sampled 0.8107390674474213
[2019-04-04 12:09:07,899] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7356.2772 239341795.3600 1605.0776
[2019-04-04 12:09:18,222] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.08366178], dtype=float32), 0.09162004]
[2019-04-04 12:09:18,223] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.18753191, 27.16469539, 231.3289858, 747.80460685, 26.0, 25.46421304430237, 0.4760438669406334, 0.0, 1.0, 0.0]
[2019-04-04 12:09:18,223] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 12:09:18,223] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.00216612e-03 1.62829849e-04 1.31974389e-07 1.12371971e-07
 9.97495174e-01 3.39467515e-04 1.04459886e-07], sampled 0.8495493529195759
[2019-04-04 12:09:42,180] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.8901 263363085.2404 1559.0360
[2019-04-04 12:09:46,198] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7184.0213 275515519.3830 1229.3382
[2019-04-04 12:09:47,248] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 100000, evaluation results [100000.0, 7241.890070283628, 263363085.24043244, 1559.0360133906217, 7356.277164952633, 239341795.3599506, 1605.077568561636, 7184.021336271612, 275515519.3829613, 1229.3382325662633]
[2019-04-04 12:09:55,530] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 102610: loss 0.4490
[2019-04-04 12:09:55,539] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 102610: learning rate 0.0000
[2019-04-04 12:09:56,644] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 102968: loss 0.4420
[2019-04-04 12:09:56,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 102968: learning rate 0.0000
[2019-04-04 12:09:57,418] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 103214: loss 0.4194
[2019-04-04 12:09:57,420] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 103214: learning rate 0.0000
[2019-04-04 12:09:57,599] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 103278: loss 0.3758
[2019-04-04 12:09:57,600] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 103278: learning rate 0.0000
[2019-04-04 12:09:57,712] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 103325: loss 0.3999
[2019-04-04 12:09:57,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 103325: learning rate 0.0000
[2019-04-04 12:09:57,818] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 103355: loss 0.3470
[2019-04-04 12:09:57,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 103355: learning rate 0.0000
[2019-04-04 12:09:59,305] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 103786: loss 0.3249
[2019-04-04 12:09:59,306] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 103786: learning rate 0.0000
[2019-04-04 12:09:59,642] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 103908: loss 0.3596
[2019-04-04 12:09:59,651] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 103908: learning rate 0.0000
[2019-04-04 12:09:59,876] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 103999: loss 0.3880
[2019-04-04 12:09:59,877] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 103999: loss 0.3657
[2019-04-04 12:09:59,878] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 103999: learning rate 0.0000
[2019-04-04 12:09:59,877] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 103999: learning rate 0.0000
[2019-04-04 12:10:00,184] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 104100: loss 0.4053
[2019-04-04 12:10:00,189] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 104101: learning rate 0.0000
[2019-04-04 12:10:00,694] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 104289: loss 0.3677
[2019-04-04 12:10:00,695] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 104289: learning rate 0.0000
[2019-04-04 12:10:01,514] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 104513: loss 0.3591
[2019-04-04 12:10:01,521] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 104513: learning rate 0.0000
[2019-04-04 12:10:01,980] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.40351330e-04 5.50257973e-05 1.38045015e-08 3.00751850e-08
 9.98876512e-01 3.28082620e-04 9.00660702e-09], sum to 1.0000
[2019-04-04 12:10:01,981] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7703
[2019-04-04 12:10:02,017] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.22966816152907, 0.3536809817924521, 0.0, 1.0, 40957.52219379773], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3906000.0000, 
sim time next is 3906600.0000, 
raw observation next is [-4.333333333333334, 74.0, 0.0, 0.0, 26.0, 25.21000199737074, 0.3523492922704062, 0.0, 1.0, 41003.2291691865], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.74, 0.0, 0.0, 0.6666666666666666, 0.600833499780895, 0.6174497640901354, 0.0, 1.0, 0.19525347223422143], 
reward next is 0.8047, 
noisyNet noise sample is [array([0.77702737], dtype=float32), 0.36092812]. 
=============================================
[2019-04-04 12:10:03,104] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 104995: loss 0.3372
[2019-04-04 12:10:03,105] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 104995: learning rate 0.0000
[2019-04-04 12:10:03,353] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6500, global step 105064: loss 0.3595
[2019-04-04 12:10:03,355] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6500, global step 105064: learning rate 0.0000
[2019-04-04 12:10:03,664] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 105154: loss 0.2927
[2019-04-04 12:10:03,665] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 105154: learning rate 0.0000
[2019-04-04 12:10:04,706] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.6821005e-03 8.3287698e-05 4.0358284e-08 1.1075959e-07 9.9380481e-01
 4.2955810e-04 1.4808104e-07], sum to 1.0000
[2019-04-04 12:10:04,711] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1621
[2019-04-04 12:10:04,791] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.666666666666666, 60.0, 20.16666666666666, 213.5, 26.0, 25.4492173491247, 0.4333765437820103, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3915600.0000, 
sim time next is 3916200.0000, 
raw observation next is [-7.833333333333334, 59.0, 34.33333333333333, 264.0, 26.0, 25.65666734970478, 0.4368375811828531, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.2456140350877193, 0.59, 0.11444444444444443, 0.29171270718232045, 0.6666666666666666, 0.6380556124753983, 0.6456125270609511, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.165276], dtype=float32), 1.0072637]. 
=============================================
[2019-04-04 12:10:13,964] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.8699202e-03 3.6020592e-04 1.9573645e-07 6.9745573e-07 9.9401581e-01
 7.5292226e-04 2.3850527e-07], sum to 1.0000
[2019-04-04 12:10:13,964] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5981
[2019-04-04 12:10:13,982] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.333333333333334, 39.0, 0.0, 0.0, 26.0, 24.67471068791446, 0.1847473562344368, 0.0, 1.0, 40108.69323698217], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4083600.0000, 
sim time next is 4084200.0000, 
raw observation next is [-4.5, 39.5, 0.0, 0.0, 26.0, 24.64022324351639, 0.1813512518265445, 0.0, 1.0, 40075.61527553345], 
processed observation next is [1.0, 0.2608695652173913, 0.3379501385041552, 0.395, 0.0, 0.0, 0.6666666666666666, 0.5533519369596993, 0.5604504172755148, 0.0, 1.0, 0.19083626321682595], 
reward next is 0.8092, 
noisyNet noise sample is [array([-1.5104553], dtype=float32), -0.664702]. 
=============================================
[2019-04-04 12:10:17,443] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6099500e-03 4.9443654e-05 7.1500736e-08 5.4990746e-08 9.9809617e-01
 2.4426679e-04 2.9248401e-08], sum to 1.0000
[2019-04-04 12:10:17,455] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8650
[2019-04-04 12:10:17,529] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.333333333333334, 36.33333333333333, 15.33333333333333, 78.16666666666664, 26.0, 25.26448668531057, 0.3106435740026724, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4088400.0000, 
sim time next is 4089000.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 30.66666666666666, 156.3333333333333, 26.0, 25.35368182427648, 0.3240505272344591, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3471837488457987, 0.35166666666666674, 0.1022222222222222, 0.17274401473296497, 0.6666666666666666, 0.6128068186897068, 0.6080168424114863, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.96157324], dtype=float32), 1.5754087]. 
=============================================
[2019-04-04 12:10:17,537] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[42.899326]
 [42.272057]
 [42.149727]
 [41.94521 ]
 [41.905926]], R is [[43.5473938 ]
 [44.1119194 ]
 [44.67079926]
 [44.25996017]
 [44.627491  ]].
[2019-04-04 12:10:22,266] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 110753: loss 3.8596
[2019-04-04 12:10:22,266] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 110753: learning rate 0.0000
[2019-04-04 12:10:22,433] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 110813: loss 4.0461
[2019-04-04 12:10:22,433] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 110813: learning rate 0.0000
[2019-04-04 12:10:24,004] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 111258: loss 3.7420
[2019-04-04 12:10:24,005] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 111258: learning rate 0.0000
[2019-04-04 12:10:24,224] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 111346: loss 4.0894
[2019-04-04 12:10:24,225] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 111346: learning rate 0.0000
[2019-04-04 12:10:24,674] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 111480: loss 3.9054
[2019-04-04 12:10:24,675] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 111480: learning rate 0.0000
[2019-04-04 12:10:24,843] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 111526: loss 4.2156
[2019-04-04 12:10:24,844] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 111526: learning rate 0.0000
[2019-04-04 12:10:26,148] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 111879: loss 3.6601
[2019-04-04 12:10:26,152] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 111882: learning rate 0.0000
[2019-04-04 12:10:26,211] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 111897: loss 4.1307
[2019-04-04 12:10:26,211] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 111897: learning rate 0.0000
[2019-04-04 12:10:26,277] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 111913: loss 3.8031
[2019-04-04 12:10:26,278] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 111913: learning rate 0.0000
[2019-04-04 12:10:26,605] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 112025: loss 3.9970
[2019-04-04 12:10:26,607] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 112025: learning rate 0.0000
[2019-04-04 12:10:27,280] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 112226: loss 3.7280
[2019-04-04 12:10:27,318] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 112242: learning rate 0.0000
[2019-04-04 12:10:28,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3771660e-03 3.7175760e-04 4.8761797e-07 1.6589995e-06 9.9374616e-01
 5.0206308e-04 7.1714106e-07], sum to 1.0000
[2019-04-04 12:10:28,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6591
[2019-04-04 12:10:28,278] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.833333333333334, 49.16666666666667, 0.0, 0.0, 26.0, 24.66902822438583, 0.2186235786785909, 0.0, 1.0, 40050.8197489154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4171800.0000, 
sim time next is 4172400.0000, 
raw observation next is [-5.0, 49.0, 0.0, 0.0, 26.0, 24.62802110963899, 0.2096825483363974, 0.0, 1.0, 40164.85211383322], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.49, 0.0, 0.0, 0.6666666666666666, 0.5523350924699159, 0.5698941827787991, 0.0, 1.0, 0.19126120054206297], 
reward next is 0.8087, 
noisyNet noise sample is [array([2.7905064], dtype=float32), 0.2260774]. 
=============================================
[2019-04-04 12:10:28,283] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 112538: loss 3.8227
[2019-04-04 12:10:28,285] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 112539: learning rate 0.0000
[2019-04-04 12:10:28,307] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.38708134e-03 2.14644198e-04 7.77043141e-08 2.01530639e-07
 9.98127401e-01 2.70436169e-04 1.06888386e-07], sum to 1.0000
[2019-04-04 12:10:28,307] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8921
[2019-04-04 12:10:28,339] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.833333333333333, 45.5, 0.0, 0.0, 26.0, 25.42035160103647, 0.3461333783112583, 0.0, 1.0, 29985.22023926722], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4240200.0000, 
sim time next is 4240800.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.42520956387482, 0.3442504052495751, 0.0, 1.0, 31826.70461758783], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6187674636562349, 0.6147501350831918, 0.0, 1.0, 0.15155573627422778], 
reward next is 0.8484, 
noisyNet noise sample is [array([-0.22493395], dtype=float32), 0.7693442]. 
=============================================
[2019-04-04 12:10:28,398] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 112571: loss 3.9838
[2019-04-04 12:10:28,399] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 112571: learning rate 0.0000
[2019-04-04 12:10:31,017] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7000, global step 113357: loss 3.7002
[2019-04-04 12:10:31,018] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7000, global step 113357: learning rate 0.0000
[2019-04-04 12:10:31,186] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 113412: loss 3.5758
[2019-04-04 12:10:31,191] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 113415: loss 3.7592
[2019-04-04 12:10:31,205] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 113415: learning rate 0.0000
[2019-04-04 12:10:31,211] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 113415: learning rate 0.0000
[2019-04-04 12:10:38,438] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8346290e-03 1.4151614e-04 1.3470673e-08 2.3835250e-07 9.9777693e-01
 2.4671061e-04 9.8983826e-09], sum to 1.0000
[2019-04-04 12:10:38,438] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1882
[2019-04-04 12:10:38,449] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.8, 69.66666666666667, 0.0, 0.0, 26.0, 25.6315230749629, 0.3885331753941807, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4335600.0000, 
sim time next is 4336200.0000, 
raw observation next is [3.75, 69.5, 0.0, 0.0, 26.0, 25.58543543021333, 0.379013905875379, 0.0, 1.0, 18737.2843140094], 
processed observation next is [1.0, 0.17391304347826086, 0.5664819944598338, 0.695, 0.0, 0.0, 0.6666666666666666, 0.6321196191844441, 0.6263379686251264, 0.0, 1.0, 0.08922516340004477], 
reward next is 0.9108, 
noisyNet noise sample is [array([1.4126626], dtype=float32), -0.6397388]. 
=============================================
[2019-04-04 12:10:45,976] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.00908601e-05 1.33270888e-07 9.43567620e-12 3.30650438e-11
 9.99989510e-01 2.39071113e-07 1.10057415e-11], sum to 1.0000
[2019-04-04 12:10:45,980] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9115
[2019-04-04 12:10:46,077] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.97107573453691, 0.4542863467957316, 0.0, 1.0, 109608.4413539405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4477800.0000, 
sim time next is 4478400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.00723844128707, 0.4629078875117322, 1.0, 1.0, 46030.50216574733], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5839365367739223, 0.6543026291705774, 1.0, 1.0, 0.2191928674559397], 
reward next is 0.7808, 
noisyNet noise sample is [array([1.3222694], dtype=float32), 2.1071384]. 
=============================================
[2019-04-04 12:10:46,633] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.3857664e-04 7.8933681e-06 8.0397355e-10 5.1004077e-09 9.9923468e-01
 1.8871639e-05 4.1455012e-10], sum to 1.0000
[2019-04-04 12:10:46,633] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1387
[2019-04-04 12:10:46,657] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 26.0, 25.58033275549963, 0.4987946241849184, 0.0, 1.0, 29815.5345970113], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4432200.0000, 
sim time next is 4432800.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 26.0, 25.53412818779993, 0.5014668632134814, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6278440156499941, 0.6671556210711604, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5556142], dtype=float32), 1.7774309]. 
=============================================
[2019-04-04 12:10:47,588] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 118587: loss 0.7776
[2019-04-04 12:10:47,599] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 118589: learning rate 0.0000
[2019-04-04 12:10:47,975] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 118704: loss 0.9277
[2019-04-04 12:10:47,975] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 118704: learning rate 0.0000
[2019-04-04 12:10:49,617] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 119219: loss 0.9200
[2019-04-04 12:10:49,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 119219: learning rate 0.0000
[2019-04-04 12:10:49,644] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.4277359e-05 3.1538752e-07 9.7655070e-12 5.4462740e-11 9.9993384e-01
 1.5387502e-06 1.5156649e-11], sum to 1.0000
[2019-04-04 12:10:49,644] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8784
[2019-04-04 12:10:49,693] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.01084125933714, 0.4626374521008117, 0.0, 1.0, 198659.5442127378], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4479600.0000, 
sim time next is 4480200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.98140452143707, 0.4872105412740278, 0.0, 1.0, 178046.6688831921], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5817837101197559, 0.6624035137580092, 0.0, 1.0, 0.8478412803961528], 
reward next is 0.1522, 
noisyNet noise sample is [array([-0.46681505], dtype=float32), -1.5500891]. 
=============================================
[2019-04-04 12:10:49,864] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 119295: loss 0.7353
[2019-04-04 12:10:49,866] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 119295: learning rate 0.0000
[2019-04-04 12:10:50,789] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 119565: loss 1.0671
[2019-04-04 12:10:50,789] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 119565: learning rate 0.0000
[2019-04-04 12:10:51,000] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 119620: loss 0.8669
[2019-04-04 12:10:51,007] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 119620: learning rate 0.0000
[2019-04-04 12:10:51,169] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 119663: loss 0.9345
[2019-04-04 12:10:51,171] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 119663: learning rate 0.0000
[2019-04-04 12:10:51,510] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 119754: loss 1.1096
[2019-04-04 12:10:51,512] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 119754: learning rate 0.0000
[2019-04-04 12:10:52,385] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 119964: loss 0.8303
[2019-04-04 12:10:52,415] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 119964: learning rate 0.0000
[2019-04-04 12:10:53,123] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120137: loss 1.1036
[2019-04-04 12:10:53,127] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120140: learning rate 0.0000
[2019-04-04 12:10:53,634] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 120259: loss 1.0098
[2019-04-04 12:10:53,635] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 120259: learning rate 0.0000
[2019-04-04 12:10:54,170] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 120386: loss 1.0488
[2019-04-04 12:10:54,171] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 120386: learning rate 0.0000
[2019-04-04 12:10:55,065] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120606: loss 1.0807
[2019-04-04 12:10:55,065] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120606: learning rate 0.0000
[2019-04-04 12:10:57,272] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7500, global step 121191: loss 0.8985
[2019-04-04 12:10:57,274] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7500, global step 121191: learning rate 0.0000
[2019-04-04 12:10:57,942] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 121391: loss 1.0909
[2019-04-04 12:10:57,946] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 121392: learning rate 0.0000
[2019-04-04 12:10:58,122] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 121435: loss 0.9207
[2019-04-04 12:10:58,146] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 121435: learning rate 0.0000
[2019-04-04 12:10:58,406] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.1167539e-05 6.2403967e-07 4.1771801e-11 1.5519720e-10 9.9994683e-01
 1.3978845e-06 4.3764613e-11], sum to 1.0000
[2019-04-04 12:10:58,407] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4249
[2019-04-04 12:10:58,508] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 56.16666666666666, 0.0, 0.0, 26.0, 25.24343310665425, 0.523834640513887, 0.0, 1.0, 60779.13180259103], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4567800.0000, 
sim time next is 4568400.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 26.0, 25.39778207809852, 0.5475097209121789, 0.0, 1.0, 32843.55627485167], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6164818398415433, 0.6825032403040595, 0.0, 1.0, 0.15639788702310317], 
reward next is 0.8436, 
noisyNet noise sample is [array([-0.2430011], dtype=float32), -1.3017446]. 
=============================================
[2019-04-04 12:11:00,034] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0920494e-06 1.4602192e-07 3.6174403e-12 4.1550565e-12 9.9999666e-01
 1.0185207e-07 1.0803220e-12], sum to 1.0000
[2019-04-04 12:11:00,034] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9778
[2019-04-04 12:11:00,063] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.9, 49.66666666666666, 201.6666666666667, 520.6666666666666, 26.0, 26.97680861007981, 0.8263963259562818, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4632000.0000, 
sim time next is 4632600.0000, 
raw observation next is [4.95, 49.83333333333334, 200.3333333333333, 442.3333333333334, 26.0, 27.2040883663275, 0.8551713476336923, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5997229916897507, 0.4983333333333334, 0.6677777777777776, 0.48876611418047894, 0.6666666666666666, 0.7670073638606251, 0.7850571158778975, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2210506], dtype=float32), 0.33316994]. 
=============================================
[2019-04-04 12:11:14,550] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.0917143e-05 1.3768075e-05 6.7622086e-09 2.0787278e-08 9.9986279e-01
 3.2407464e-05 5.7639360e-09], sum to 1.0000
[2019-04-04 12:11:14,573] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4937
[2019-04-04 12:11:14,635] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 81.5, 0.0, 0.0, 26.0, 24.81746556163525, 0.3170481690079733, 0.0, 1.0, 40551.37375469938], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4761000.0000, 
sim time next is 4761600.0000, 
raw observation next is [-5.333333333333334, 85.0, 0.0, 0.0, 26.0, 24.80611127619262, 0.3125256384966617, 0.0, 1.0, 40532.76675247952], 
processed observation next is [0.0, 0.08695652173913043, 0.31486611265004616, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5671759396827184, 0.6041752128322205, 0.0, 1.0, 0.19301317501180723], 
reward next is 0.8070, 
noisyNet noise sample is [array([1.265818], dtype=float32), 1.8453164]. 
=============================================
[2019-04-04 12:11:16,496] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 126505: loss 4.3560
[2019-04-04 12:11:16,503] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 126507: learning rate 0.0000
[2019-04-04 12:11:18,424] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 126992: loss 4.2542
[2019-04-04 12:11:18,427] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 126992: learning rate 0.0000
[2019-04-04 12:11:18,834] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 127107: loss 4.3541
[2019-04-04 12:11:18,835] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 127107: learning rate 0.0000
[2019-04-04 12:11:20,585] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 127516: loss 4.2213
[2019-04-04 12:11:20,588] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 127516: learning rate 0.0000
[2019-04-04 12:11:20,800] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 127578: loss 4.2370
[2019-04-04 12:11:20,825] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 127578: learning rate 0.0000
[2019-04-04 12:11:20,951] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 127630: loss 4.1508
[2019-04-04 12:11:20,952] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 127630: learning rate 0.0000
[2019-04-04 12:11:21,287] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 127708: loss 4.2041
[2019-04-04 12:11:21,289] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 127708: learning rate 0.0000
[2019-04-04 12:11:21,809] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 127856: loss 4.2426
[2019-04-04 12:11:21,810] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 127856: learning rate 0.0000
[2019-04-04 12:11:21,829] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 127863: loss 4.2619
[2019-04-04 12:11:21,831] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 127863: learning rate 0.0000
[2019-04-04 12:11:22,504] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8746365e-05 2.1245303e-06 1.2243747e-09 5.7106098e-10 9.9991643e-01
 2.7253218e-06 7.1396034e-10], sum to 1.0000
[2019-04-04 12:11:22,504] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7472
[2019-04-04 12:11:22,544] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 37.0, 105.5, 729.5, 26.0, 25.18455770831127, 0.4440462933948575, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4807200.0000, 
sim time next is 4807800.0000, 
raw observation next is [3.0, 37.0, 97.0, 727.0, 26.0, 25.18408824703348, 0.4429338421164488, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.3233333333333333, 0.8033149171270718, 0.6666666666666666, 0.5986740205861233, 0.6476446140388162, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32400098], dtype=float32), 0.22800446]. 
=============================================
[2019-04-04 12:11:23,136] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 128229: loss 4.2818
[2019-04-04 12:11:23,136] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 128229: learning rate 0.0000
[2019-04-04 12:11:23,314] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 128278: loss 4.2832
[2019-04-04 12:11:23,316] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 128278: learning rate 0.0000
[2019-04-04 12:11:24,681] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 128670: loss 4.1856
[2019-04-04 12:11:24,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 128670: learning rate 0.0000
[2019-04-04 12:11:24,874] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 128714: loss 4.2023
[2019-04-04 12:11:24,875] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 128714: learning rate 0.0000
[2019-04-04 12:11:27,578] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8000, global step 129344: loss 4.1725
[2019-04-04 12:11:27,578] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8000, global step 129344: learning rate 0.0000
[2019-04-04 12:11:28,178] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 129486: loss 4.0402
[2019-04-04 12:11:28,184] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 129486: learning rate 0.0000
[2019-04-04 12:11:28,766] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 129638: loss 4.1935
[2019-04-04 12:11:28,777] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 129638: learning rate 0.0000
[2019-04-04 12:11:31,195] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.0748603e-05 2.3076673e-06 2.0708106e-09 1.1678910e-09 9.9989831e-01
 8.5448664e-06 1.4887521e-09], sum to 1.0000
[2019-04-04 12:11:31,195] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8060
[2019-04-04 12:11:31,222] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.5, 57.5, 0.0, 0.0, 26.0, 25.44703632394477, 0.3837806920820251, 0.0, 1.0, 18761.96156076576], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4836600.0000, 
sim time next is 4837200.0000, 
raw observation next is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 26.0, 25.40676058106308, 0.3765713614752872, 0.0, 1.0, 40997.4331565472], 
processed observation next is [0.0, 1.0, 0.4164358264081256, 0.5833333333333335, 0.0, 0.0, 0.6666666666666666, 0.6172300484219232, 0.625523787158429, 0.0, 1.0, 0.1952258721740343], 
reward next is 0.8048, 
noisyNet noise sample is [array([0.6616475], dtype=float32), 0.1580191]. 
=============================================
[2019-04-04 12:11:32,217] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.4930952e-05 1.8070384e-06 6.5984712e-10 5.3686955e-10 9.9994123e-01
 2.0556688e-06 2.6224589e-10], sum to 1.0000
[2019-04-04 12:11:32,218] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5848
[2019-04-04 12:11:32,330] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 45.0, 79.00000000000001, 273.6666666666667, 26.0, 25.17921173795246, 0.3494183373592439, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4899000.0000, 
sim time next is 4899600.0000, 
raw observation next is [3.0, 45.0, 67.5, 252.0, 26.0, 25.14286884131655, 0.3391269703283073, 0.0, 1.0, 18680.41167023054], 
processed observation next is [0.0, 0.7391304347826086, 0.5457063711911359, 0.45, 0.225, 0.27845303867403315, 0.6666666666666666, 0.5952390701097124, 0.6130423234427691, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.9170477], dtype=float32), -0.8194134]. 
=============================================
[2019-04-04 12:11:41,154] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0157123e-07 2.0880964e-09 1.1479020e-14 1.2708159e-13 9.9999976e-01
 2.0216031e-09 1.3220496e-13], sum to 1.0000
[2019-04-04 12:11:41,154] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9167
[2019-04-04 12:11:41,230] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.666666666666667, 25.83333333333334, 46.66666666666666, 416.3333333333333, 26.0, 27.30300402131247, 0.8570929350267186, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4986600.0000, 
sim time next is 4987200.0000, 
raw observation next is [7.333333333333334, 25.66666666666667, 40.33333333333333, 360.1666666666666, 26.0, 27.55142356180014, 0.8367093046511423, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6657433056325024, 0.2566666666666667, 0.13444444444444442, 0.3979742173112338, 0.6666666666666666, 0.7959519634833448, 0.7789031015503808, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1406739], dtype=float32), 0.7128139]. 
=============================================
[2019-04-04 12:11:46,130] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.73064505e-06 1.90501197e-07 5.92513582e-12 1.05589495e-11
 9.99992967e-01 7.52989564e-08 4.85912525e-12], sum to 1.0000
[2019-04-04 12:11:46,131] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8396
[2019-04-04 12:11:46,190] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.666666666666667, 45.0, 109.5, 670.5, 26.0, 26.53012287856823, 0.6247755987724696, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5044800.0000, 
sim time next is 5045400.0000, 
raw observation next is [2.0, 44.0, 112.0, 698.0, 26.0, 26.61361312353968, 0.6505562864102022, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.518005540166205, 0.44, 0.37333333333333335, 0.7712707182320442, 0.6666666666666666, 0.7178010936283066, 0.7168520954700673, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12886952], dtype=float32), -0.13100713]. 
=============================================
[2019-04-04 12:11:46,318] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:46,318] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:46,320] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run2
[2019-04-04 12:11:49,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:49,439] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:49,446] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run2
[2019-04-04 12:11:49,775] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:49,775] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:49,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run2
[2019-04-04 12:11:50,341] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:50,341] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:50,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run2
[2019-04-04 12:11:50,964] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:50,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:50,966] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run2
[2019-04-04 12:11:51,123] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:51,123] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:51,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run2
[2019-04-04 12:11:52,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:52,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:52,012] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run2
[2019-04-04 12:11:52,106] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:52,107] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:52,108] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run2
[2019-04-04 12:11:52,800] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:52,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:52,802] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run2
[2019-04-04 12:11:53,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:53,992] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:53,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run2
[2019-04-04 12:11:54,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:54,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:54,627] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run2
[2019-04-04 12:11:54,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:54,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:54,775] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run2
[2019-04-04 12:11:55,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:55,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:55,571] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run2
[2019-04-04 12:11:57,414] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:57,414] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:57,416] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run2
[2019-04-04 12:11:57,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:57,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:57,515] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run2
[2019-04-04 12:11:57,927] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:11:57,927] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:11:57,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run2
[2019-04-04 12:12:02,634] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7224770e-02 3.8323313e-02 3.6071232e-04 1.1663019e-03 9.0240383e-01
 2.0285422e-02 2.3568991e-04], sum to 1.0000
[2019-04-04 12:12:02,635] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6951
[2019-04-04 12:12:02,681] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.5, 19.67484360858283, -0.8958516709515095, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 6000.0000, 
sim time next is 6600.0000, 
raw observation next is [7.199999999999999, 96.0, 0.0, 0.0, 20.0, 19.80388254759382, -0.8885481225008798, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.16666666666666666, 0.15032354563281825, 0.20381729249970673, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05604228], dtype=float32), -0.6483162]. 
=============================================
[2019-04-04 12:12:13,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4573813e-04 1.2446219e-06 2.1689828e-10 6.8803241e-10 9.9985123e-01
 1.8239116e-06 2.1786269e-10], sum to 1.0000
[2019-04-04 12:12:13,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4064
[2019-04-04 12:12:13,886] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 68.0, 18.5, 4.5, 26.0, 24.7920254841628, 0.2291523846672735, 1.0, 1.0, 9408.014086507934], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 115200.0000, 
sim time next is 115800.0000, 
raw observation next is [-7.383333333333333, 66.83333333333334, 24.66666666666667, 6.000000000000001, 26.0, 24.9764957868622, 0.2318167414386088, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.25807940904893817, 0.6683333333333334, 0.08222222222222224, 0.006629834254143647, 0.6666666666666666, 0.5813746489051832, 0.577272247146203, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.5639467], dtype=float32), 2.7101042]. 
=============================================
[2019-04-04 12:12:50,293] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9010393e-07 5.6561378e-09 1.6245242e-13 3.3556356e-12 9.9999976e-01
 6.6807861e-09 3.1420377e-13], sum to 1.0000
[2019-04-04 12:12:50,293] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8846
[2019-04-04 12:12:50,357] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.866666666666667, 44.33333333333334, 31.66666666666666, 279.5, 26.0, 25.93310567771266, 0.2778599684109045, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 318000.0000, 
sim time next is 318600.0000, 
raw observation next is [-10.05, 45.5, 24.0, 240.0, 26.0, 25.7747479717133, 0.3835814030809784, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.18421052631578946, 0.455, 0.08, 0.26519337016574585, 0.6666666666666666, 0.6478956643094417, 0.6278604676936594, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54076004], dtype=float32), -0.044261962]. 
=============================================
[2019-04-04 12:13:00,860] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6370335e-06 2.8318595e-07 5.2042988e-12 3.1347890e-11 9.9999690e-01
 2.4821458e-07 3.2670761e-12], sum to 1.0000
[2019-04-04 12:13:00,860] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6773
[2019-04-04 12:13:00,940] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.9000000000000001, 56.66666666666667, 0.0, 0.0, 26.0, 25.74756589518888, 0.2436304738997274, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 494400.0000, 
sim time next is 495000.0000, 
raw observation next is [0.8, 63.5, 0.0, 0.0, 26.0, 25.42346549746241, 0.2177114875483182, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4847645429362882, 0.635, 0.0, 0.0, 0.6666666666666666, 0.6186221247885341, 0.5725704958494394, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.94733655], dtype=float32), 0.16889884]. 
=============================================
[2019-04-04 12:13:00,970] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[66.07463 ]
 [65.942055]
 [66.015114]
 [65.83119 ]
 [65.548515]], R is [[66.43993378]
 [66.77553558]
 [67.10778046]
 [67.43670654]
 [67.76233673]].
[2019-04-04 12:13:19,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7573135e-06 1.3569306e-07 4.6195131e-11 1.3903037e-10 9.9999475e-01
 3.9066910e-07 4.5779023e-11], sum to 1.0000
[2019-04-04 12:13:19,869] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2731
[2019-04-04 12:13:19,884] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 87.0, 0.0, 0.0, 26.0, 24.62043195644159, 0.1875091737833516, 0.0, 1.0, 42316.8099217337], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 604800.0000, 
sim time next is 605400.0000, 
raw observation next is [-3.483333333333333, 86.83333333333333, 0.0, 0.0, 26.0, 24.58885253240958, 0.1806771238979141, 0.0, 1.0, 42300.58516301093], 
processed observation next is [0.0, 0.0, 0.3661126500461681, 0.8683333333333333, 0.0, 0.0, 0.6666666666666666, 0.5490710443674649, 0.5602257079659714, 0.0, 1.0, 0.20143135791909966], 
reward next is 0.7986, 
noisyNet noise sample is [array([0.62850225], dtype=float32), 0.39851475]. 
=============================================
[2019-04-04 12:13:32,265] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0879800e-08 8.3518797e-10 1.2536787e-14 3.6645952e-14 9.9999988e-01
 8.8156749e-10 1.7182953e-14], sum to 1.0000
[2019-04-04 12:13:32,265] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4630
[2019-04-04 12:13:32,347] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 45.0, 76.5, 17.0, 26.0, 25.67063138731261, 0.2942879347338072, 1.0, 1.0, 23438.7860096869], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 748800.0000, 
sim time next is 749400.0000, 
raw observation next is [-0.9666666666666666, 46.5, 73.66666666666666, 12.33333333333333, 26.0, 25.25647386831899, 0.3331394800098507, 1.0, 1.0, 30817.45285411961], 
processed observation next is [1.0, 0.6956521739130435, 0.43582640812557716, 0.465, 0.24555555555555553, 0.013627992633517492, 0.6666666666666666, 0.6047061556932493, 0.6110464933366169, 1.0, 1.0, 0.14674977549580767], 
reward next is 0.8533, 
noisyNet noise sample is [array([0.46780813], dtype=float32), 1.0932126]. 
=============================================
[2019-04-04 12:13:40,780] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.3488034e-06 1.6530858e-07 4.1049802e-11 9.3712108e-11 9.9999416e-01
 3.2492153e-07 9.7057674e-11], sum to 1.0000
[2019-04-04 12:13:40,780] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2143
[2019-04-04 12:13:40,821] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.65054447446813, 0.1402517569249488, 0.0, 1.0, 41573.78653294971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 687600.0000, 
sim time next is 688200.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.60917921981412, 0.1329700189053568, 0.0, 1.0, 41513.82801797466], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.6666666666666666, 0.55076493498451, 0.5443233396351189, 0.0, 1.0, 0.19768489532368885], 
reward next is 0.8023, 
noisyNet noise sample is [array([-0.7417171], dtype=float32), -0.023861917]. 
=============================================
[2019-04-04 12:13:47,852] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.0608334e-06 3.0913921e-07 3.6613566e-11 1.0021899e-10 9.9999154e-01
 1.1078658e-07 9.2356314e-12], sum to 1.0000
[2019-04-04 12:13:47,852] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6865
[2019-04-04 12:13:48,031] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 68.33333333333334, 0.0, 0.0, 26.0, 23.44896065200896, -0.08713636807652465, 0.0, 1.0, 42001.87946182528], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 803400.0000, 
sim time next is 804000.0000, 
raw observation next is [-6.700000000000001, 69.66666666666667, 0.0, 0.0, 26.0, 23.43272667178057, -0.02002218568818333, 1.0, 1.0, 202414.411683902], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.45272722264838094, 0.49332593810393893, 1.0, 1.0, 0.9638781508757238], 
reward next is 0.0361, 
noisyNet noise sample is [array([1.8043243], dtype=float32), 1.5188494]. 
=============================================
[2019-04-04 12:13:48,098] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[58.092434]
 [58.068787]
 [58.035465]
 [58.04108 ]
 [58.04475 ]], R is [[58.11558533]
 [58.33442307]
 [58.55094528]
 [58.76523972]
 [58.97726059]].
[2019-04-04 12:14:11,367] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.15751955e-05 1.17364614e-07 8.21134881e-13 1.34125237e-11
 9.99988317e-01 3.55015111e-08 2.64904852e-12], sum to 1.0000
[2019-04-04 12:14:11,367] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3004
[2019-04-04 12:14:11,387] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.25, 83.0, 0.0, 0.0, 26.0, 25.35423787129927, 0.4364201088076314, 0.0, 1.0, 79699.27299084108], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 966600.0000, 
sim time next is 967200.0000, 
raw observation next is [8.433333333333334, 83.0, 0.0, 0.0, 26.0, 25.37725356284333, 0.4497890031410088, 0.0, 1.0, 52714.6218045596], 
processed observation next is [1.0, 0.17391304347826086, 0.6962142197599263, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6147711302369441, 0.6499296677136696, 0.0, 1.0, 0.25102200859314094], 
reward next is 0.7490, 
noisyNet noise sample is [array([-1.4368303], dtype=float32), -1.7337109]. 
=============================================
[2019-04-04 12:14:13,685] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6267857e-08 6.9897266e-10 1.7180585e-15 1.0594675e-14 1.0000000e+00
 7.9012358e-10 1.8861842e-15], sum to 1.0000
[2019-04-04 12:14:13,691] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8391
[2019-04-04 12:14:13,756] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 81.0, 81.66666666666667, 0.0, 26.0, 25.75015532111242, 0.5693201558853689, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1003800.0000, 
sim time next is 1004400.0000, 
raw observation next is [14.4, 81.0, 75.5, 0.0, 26.0, 24.30479817609039, 0.4828592368193718, 1.0, 1.0, 195414.5294687718], 
processed observation next is [1.0, 0.6521739130434783, 0.8614958448753465, 0.81, 0.25166666666666665, 0.0, 0.6666666666666666, 0.5253998480075325, 0.6609530789397906, 1.0, 1.0, 0.9305453784227229], 
reward next is 0.0695, 
noisyNet noise sample is [array([2.6555076], dtype=float32), 0.11925565]. 
=============================================
[2019-04-04 12:14:16,458] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3589013e-08 1.6108738e-10 1.6198201e-15 3.6487774e-14 1.0000000e+00
 1.4919560e-10 3.6005202e-15], sum to 1.0000
[2019-04-04 12:14:16,461] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2662
[2019-04-04 12:14:16,541] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.67955764624934, 0.5683818872314095, 0.0, 1.0, 60769.07598258115], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1035000.0000, 
sim time next is 1035600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.62378113556641, 0.5722525735491898, 0.0, 1.0, 71192.9313270984], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6353150946305343, 0.6907508578497299, 0.0, 1.0, 0.3390139587004686], 
reward next is 0.6610, 
noisyNet noise sample is [array([0.6064033], dtype=float32), -0.60945207]. 
=============================================
[2019-04-04 12:14:16,606] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6027076e-08 1.8154878e-10 1.8846449e-15 3.8765320e-14 1.0000000e+00
 1.7439838e-10 4.0331914e-15], sum to 1.0000
[2019-04-04 12:14:16,608] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8569
[2019-04-04 12:14:16,652] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.62378113556641, 0.5722525735491898, 0.0, 1.0, 71192.9313270984], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1035600.0000, 
sim time next is 1036200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.59938886194908, 0.5796528479731766, 0.0, 1.0, 57079.67210854337], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6332824051624234, 0.6932176159910588, 0.0, 1.0, 0.2718079624216351], 
reward next is 0.7282, 
noisyNet noise sample is [array([0.6064033], dtype=float32), -0.60945207]. 
=============================================
[2019-04-04 12:14:24,797] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3442446e-06 7.5184211e-08 6.0067051e-12 1.3485089e-11 9.9999762e-01
 4.8813778e-08 2.3268954e-12], sum to 1.0000
[2019-04-04 12:14:24,809] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3904
[2019-04-04 12:14:24,823] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.48054761043537, 0.5861218753019174, 0.0, 1.0, 46095.19590800802], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1286400.0000, 
sim time next is 1287000.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.45001786837983, 0.5865211564907539, 0.0, 1.0, 56250.55314131958], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6208348223649859, 0.6955070521635847, 0.0, 1.0, 0.26785977686342655], 
reward next is 0.7321, 
noisyNet noise sample is [array([-0.3017051], dtype=float32), -0.033167217]. 
=============================================
[2019-04-04 12:14:24,840] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[68.91308]
 [68.73545]
 [68.52685]
 [68.59744]
 [68.77192]], R is [[69.02557373]
 [69.11582184]
 [69.30343628]
 [69.61040497]
 [69.91429901]].
[2019-04-04 12:14:30,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5602989e-08 4.8550880e-10 3.6276524e-15 3.7576435e-14 1.0000000e+00
 2.0095157e-10 1.6047517e-15], sum to 1.0000
[2019-04-04 12:14:30,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8283
[2019-04-04 12:14:30,167] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 124.6666666666667, 0.0, 26.0, 26.11819821934268, 0.5827918487907792, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1338000.0000, 
sim time next is 1338600.0000, 
raw observation next is [1.1, 92.0, 122.3333333333333, 0.0, 26.0, 26.05881297314869, 0.5791567583092538, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.4077777777777777, 0.0, 0.6666666666666666, 0.6715677477623908, 0.6930522527697512, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4215902], dtype=float32), -0.36944613]. 
=============================================
[2019-04-04 12:14:50,665] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5871105e-09 2.0314667e-10 1.0158523e-15 5.0541838e-15 1.0000000e+00
 3.9767586e-10 6.9797140e-16], sum to 1.0000
[2019-04-04 12:14:50,665] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8819
[2019-04-04 12:14:50,760] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.183333333333333, 91.5, 0.0, 0.0, 26.0, 25.52563006730549, 0.5417824215380899, 0.0, 1.0, 40198.02822588547], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1461000.0000, 
sim time next is 1461600.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.52021423161152, 0.5393788645385545, 0.0, 1.0, 39372.69452971256], 
processed observation next is [1.0, 0.9565217391304348, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6266845193009599, 0.6797929548461848, 0.0, 1.0, 0.18748902157005978], 
reward next is 0.8125, 
noisyNet noise sample is [array([0.33809176], dtype=float32), 1.4946501]. 
=============================================
[2019-04-04 12:14:53,085] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.30452349e-07 1.09763345e-08 1.22667117e-14 6.80870779e-13
 9.99999881e-01 9.61020485e-10 8.12396686e-14], sum to 1.0000
[2019-04-04 12:14:53,085] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6614
[2019-04-04 12:14:53,132] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.283333333333333, 99.33333333333334, 0.0, 0.0, 26.0, 25.57648028043376, 0.4357567976440538, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1493400.0000, 
sim time next is 1494000.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 26.0, 25.42849150424978, 0.4161167550133784, 0.0, 1.0, 65653.79737488093], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6190409586874818, 0.6387055850044595, 0.0, 1.0, 0.31263713035657587], 
reward next is 0.6874, 
noisyNet noise sample is [array([-1.4696006], dtype=float32), 0.31015858]. 
=============================================
[2019-04-04 12:14:53,204] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[71.760895]
 [71.73143 ]
 [71.717285]
 [71.72402 ]
 [71.75202 ]], R is [[71.90496826]
 [72.18592072]
 [72.46406555]
 [72.73942566]
 [72.92266846]].
[2019-04-04 12:14:56,791] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9208657e-09 3.3359447e-11 2.7093656e-17 2.1816361e-15 1.0000000e+00
 1.2515998e-11 1.0356230e-16], sum to 1.0000
[2019-04-04 12:14:56,791] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9680
[2019-04-04 12:14:56,848] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.38333333333333, 50.66666666666666, 68.66666666666667, 12.33333333333333, 26.0, 27.28981512306213, 0.8451840512998329, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1612200.0000, 
sim time next is 1612800.0000, 
raw observation next is [13.3, 51.0, 64.0, 18.5, 26.0, 27.35813836570188, 0.8535207923393292, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8310249307479226, 0.51, 0.21333333333333335, 0.020441988950276244, 0.6666666666666666, 0.7798448638084899, 0.7845069307797764, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.87588006], dtype=float32), -0.26007584]. 
=============================================
[2019-04-04 12:15:00,767] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2507575e-08 1.8862403e-09 8.4260542e-15 1.1484635e-13 1.0000000e+00
 5.9842548e-10 5.0056229e-14], sum to 1.0000
[2019-04-04 12:15:00,767] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9791
[2019-04-04 12:15:00,805] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.683333333333334, 97.0, 0.0, 0.0, 26.0, 25.4682401610999, 0.5386419004092099, 0.0, 1.0, 79180.06510208291], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1662600.0000, 
sim time next is 1663200.0000, 
raw observation next is [5.5, 97.0, 0.0, 0.0, 26.0, 25.47898797120057, 0.5520959513710114, 0.0, 1.0, 48030.19100726821], 
processed observation next is [1.0, 0.2608695652173913, 0.6149584487534627, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6232489976000476, 0.6840319837903371, 0.0, 1.0, 0.22871519527270576], 
reward next is 0.7713, 
noisyNet noise sample is [array([0.43118498], dtype=float32), 0.16419022]. 
=============================================
[2019-04-04 12:15:04,836] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.6539266e-09 7.1274781e-10 5.7505617e-16 2.1871106e-15 1.0000000e+00
 5.6170674e-10 3.8332143e-16], sum to 1.0000
[2019-04-04 12:15:04,836] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2423
[2019-04-04 12:15:04,898] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 15.5, 0.0, 26.0, 25.7459693583419, 0.5234218192154431, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1702800.0000, 
sim time next is 1703400.0000, 
raw observation next is [1.1, 88.00000000000001, 10.66666666666666, 0.0, 26.0, 25.88819976480585, 0.5344059799659133, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.8800000000000001, 0.035555555555555535, 0.0, 0.6666666666666666, 0.6573499804004875, 0.6781353266553044, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6062018], dtype=float32), 0.0120034935]. 
=============================================
[2019-04-04 12:15:11,243] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.1327290e-09 2.9260917e-11 4.9186790e-16 4.5843829e-15 1.0000000e+00
 1.2310615e-10 3.0459792e-16], sum to 1.0000
[2019-04-04 12:15:11,243] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3590
[2019-04-04 12:15:11,298] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.72034663681862, 0.5568629689304455, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1713600.0000, 
sim time next is 1714200.0000, 
raw observation next is [1.0, 88.66666666666667, 0.0, 0.0, 26.0, 25.67982952945167, 0.5446346888212895, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.8866666666666667, 0.0, 0.0, 0.6666666666666666, 0.6399857941209725, 0.6815448962737632, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.371239], dtype=float32), -0.74519384]. 
=============================================
[2019-04-04 12:15:30,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.5477688e-07 5.3131646e-09 1.6886521e-12 3.0554103e-12 9.9999952e-01
 3.4998446e-09 3.8499832e-13], sum to 1.0000
[2019-04-04 12:15:30,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7728
[2019-04-04 12:15:30,346] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.566666666666666, 76.33333333333334, 0.0, 0.0, 26.0, 24.57210973273246, 0.1406645385441428, 0.0, 1.0, 44864.91415049986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1894800.0000, 
sim time next is 1895400.0000, 
raw observation next is [-6.75, 77.0, 0.0, 0.0, 26.0, 24.53235241961306, 0.1326262747836719, 0.0, 1.0, 44883.76250065576], 
processed observation next is [0.0, 0.9565217391304348, 0.275623268698061, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5443627016344216, 0.544208758261224, 0.0, 1.0, 0.21373220238407503], 
reward next is 0.7863, 
noisyNet noise sample is [array([1.114652], dtype=float32), -1.5517149]. 
=============================================
[2019-04-04 12:15:52,066] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0195654e-08 7.3806759e-11 2.7117969e-16 1.0931570e-14 1.0000000e+00
 6.4272837e-11 3.4126365e-16], sum to 1.0000
[2019-04-04 12:15:52,066] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2334
[2019-04-04 12:15:52,103] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666666, 70.16666666666667, 0.0, 0.0, 26.0, 25.41523387828481, 0.3803971607674866, 1.0, 1.0, 27581.76156181923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2229000.0000, 
sim time next is 2229600.0000, 
raw observation next is [-4.733333333333333, 70.33333333333334, 0.0, 0.0, 26.0, 25.24432452775536, 0.3543646827905398, 1.0, 1.0, 32414.26670119195], 
processed observation next is [1.0, 0.8260869565217391, 0.3314866112650046, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.60369371064628, 0.6181215609301799, 1.0, 1.0, 0.15435365095805692], 
reward next is 0.8456, 
noisyNet noise sample is [array([1.7464167], dtype=float32), -0.24668632]. 
=============================================
[2019-04-04 12:15:57,305] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0009004e-08 8.4414011e-11 5.2370074e-15 3.3301036e-14 1.0000000e+00
 3.0872005e-10 4.4306587e-15], sum to 1.0000
[2019-04-04 12:15:57,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3015
[2019-04-04 12:15:57,377] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666666, 70.16666666666667, 0.0, 0.0, 26.0, 25.4152339076553, 0.3803971752631143, 1.0, 1.0, 27581.76621902059], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2229000.0000, 
sim time next is 2229600.0000, 
raw observation next is [-4.733333333333333, 70.33333333333334, 0.0, 0.0, 26.0, 25.24432456152512, 0.3543647005284161, 1.0, 1.0, 32414.26876042696], 
processed observation next is [1.0, 0.8260869565217391, 0.3314866112650046, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.6036937134604266, 0.6181215668428054, 1.0, 1.0, 0.1543536607639379], 
reward next is 0.8456, 
noisyNet noise sample is [array([0.23351228], dtype=float32), -0.4771878]. 
=============================================
[2019-04-04 12:16:07,534] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.65145877e-06 1.07241114e-07 9.36074829e-12 9.50006046e-11
 9.99997258e-01 1.69008558e-08 2.90505207e-11], sum to 1.0000
[2019-04-04 12:16:07,555] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9690
[2019-04-04 12:16:07,591] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.10928352290853, 0.08090822159874607, 0.0, 1.0, 41374.74800580196], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2358000.0000, 
sim time next is 2358600.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.07386378667552, 0.07363289602751799, 0.0, 1.0, 41440.29258604657], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5061553155562933, 0.5245442986758393, 0.0, 1.0, 0.19733472660022175], 
reward next is 0.8027, 
noisyNet noise sample is [array([2.2390132], dtype=float32), -0.41893518]. 
=============================================
[2019-04-04 12:16:15,144] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 12:16:15,145] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 12:16:15,146] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:16:15,146] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 12:16:15,146] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 12:16:15,147] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:16:15,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run3
[2019-04-04 12:16:15,148] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:16:15,175] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run3
[2019-04-04 12:16:15,191] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run3
[2019-04-04 12:16:31,999] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13352315], dtype=float32), 0.14239958]
[2019-04-04 12:16:31,999] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-15.20033136, 49.69892825, 0.0, 0.0, 26.0, 25.29490077053212, 0.3461102146905071, 1.0, 1.0, 87333.52782082428]
[2019-04-04 12:16:32,000] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 12:16:32,001] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.9242558e-08 2.8216189e-09 1.3115564e-13 6.3664671e-13 1.0000000e+00
 1.9570821e-09 2.1639588e-13], sampled 0.9521237581363999
[2019-04-04 12:16:41,170] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13352315], dtype=float32), 0.14239958]
[2019-04-04 12:16:41,170] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.7333333333333334, 81.66666666666667, 95.83333333333333, 178.5, 26.0, 24.83795159185002, 0.2761540815764593, 0.0, 1.0, 60210.63050984275]
[2019-04-04 12:16:41,171] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 12:16:41,172] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.0236980e-07 9.4876533e-09 3.9713474e-13 2.5876786e-12 9.9999976e-01
 4.9169504e-09 5.6926431e-13], sampled 0.15891262009141427
[2019-04-04 12:16:56,007] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.13352315], dtype=float32), 0.14239958]
[2019-04-04 12:16:56,008] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [18.10144734833333, 68.262884245, 161.5865515633334, 0.0, 26.0, 25.02927971082834, 0.5031153531742273, 0.0, 0.0, 0.0]
[2019-04-04 12:16:56,008] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 12:16:56,009] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.7741513e-07 6.9179741e-08 3.1368402e-12 4.2696985e-11 9.9999928e-01
 1.0883288e-08 1.0986009e-11], sampled 0.1389423553256085
[2019-04-04 12:17:28,219] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.13352315], dtype=float32), 0.14239958]
[2019-04-04 12:17:28,220] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.3, 59.0, 0.0, 0.0, 26.0, 25.12082075585286, 0.3310478960849621, 0.0, 1.0, 38650.16455421322]
[2019-04-04 12:17:28,220] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 12:17:28,220] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.0643647e-07 3.5324530e-09 2.4215780e-13 1.1308894e-12 9.9999988e-01
 2.7369409e-09 2.7175742e-13], sampled 0.5653024667883052
[2019-04-04 12:18:09,042] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.3120 239964473.9670 1605.1648
[2019-04-04 12:18:47,848] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.3611 263474166.2365 1556.9761
[2019-04-04 12:18:51,662] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 12:18:52,684] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 200000, evaluation results [200000.0, 7241.361113159637, 263474166.23647976, 1556.9761225887328, 7353.3120287287975, 239964473.96695527, 1605.1647511176193, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 12:18:57,289] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7416388e-08 1.0240964e-09 3.7594909e-15 9.0976136e-14 1.0000000e+00
 5.4350974e-10 4.9190003e-15], sum to 1.0000
[2019-04-04 12:18:57,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2469
[2019-04-04 12:18:57,299] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.7, 47.33333333333334, 166.0, 53.66666666666666, 26.0, 25.76275868617748, 0.2987316452977382, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2544600.0000, 
sim time next is 2545200.0000, 
raw observation next is [-0.6, 47.0, 182.5, 58.0, 26.0, 25.71721555054361, 0.3009076432871773, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.47, 0.6083333333333333, 0.06408839779005525, 0.6666666666666666, 0.6431012958786342, 0.6003025477623924, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9769174], dtype=float32), 0.6599492]. 
=============================================
[2019-04-04 12:18:58,440] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3110620e-07 2.2371729e-08 2.7065149e-12 1.7873120e-11 9.9999940e-01
 1.5415846e-08 2.8967683e-12], sum to 1.0000
[2019-04-04 12:18:58,440] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6264
[2019-04-04 12:18:58,486] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.7333333333333335, 28.66666666666667, 0.0, 0.0, 26.0, 24.92754904433556, 0.2144893701068356, 0.0, 1.0, 28516.86283542786], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2485200.0000, 
sim time next is 2485800.0000, 
raw observation next is [0.55, 29.0, 0.0, 0.0, 26.0, 24.91554691463559, 0.2113482090199053, 0.0, 1.0, 41155.17259167504], 
processed observation next is [0.0, 0.782608695652174, 0.4778393351800555, 0.29, 0.0, 0.0, 0.6666666666666666, 0.5762955762196326, 0.570449403006635, 0.0, 1.0, 0.1959770123413097], 
reward next is 0.8040, 
noisyNet noise sample is [array([-1.4212724], dtype=float32), 0.12821978]. 
=============================================
[2019-04-04 12:18:59,747] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0874585e-07 3.4318077e-08 4.2331520e-12 2.5638235e-11 9.9999988e-01
 2.6112049e-08 3.1883192e-12], sum to 1.0000
[2019-04-04 12:18:59,747] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8547
[2019-04-04 12:18:59,774] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 38.0, 0.0, 0.0, 26.0, 24.97831634216246, 0.1891796210825729, 0.0, 1.0, 38901.00324613664], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2512800.0000, 
sim time next is 2513400.0000, 
raw observation next is [-1.7, 39.0, 0.0, 0.0, 26.0, 24.95770667701085, 0.1971652525296284, 0.0, 1.0, 38844.43274618866], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.39, 0.0, 0.0, 0.6666666666666666, 0.5798088897509043, 0.5657217508432094, 0.0, 1.0, 0.18497348926756504], 
reward next is 0.8150, 
noisyNet noise sample is [array([1.9633983], dtype=float32), -0.010333186]. 
=============================================
[2019-04-04 12:19:02,965] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3613055e-09 6.0903387e-11 5.9934578e-15 3.2331864e-14 1.0000000e+00
 3.0581537e-11 6.7034492e-15], sum to 1.0000
[2019-04-04 12:19:02,965] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8107
[2019-04-04 12:19:02,979] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.35, 57.5, 0.0, 0.0, 26.0, 25.33645001871273, 0.3447932031921468, 0.0, 1.0, 53785.4504771217], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2586600.0000, 
sim time next is 2587200.0000, 
raw observation next is [-3.533333333333333, 58.0, 0.0, 0.0, 26.0, 25.2193882931059, 0.3320555730854033, 0.0, 1.0, 48818.56520071268], 
processed observation next is [1.0, 0.9565217391304348, 0.36472760849492153, 0.58, 0.0, 0.0, 0.6666666666666666, 0.6016156910921584, 0.6106851910284677, 0.0, 1.0, 0.23246935809863178], 
reward next is 0.7675, 
noisyNet noise sample is [array([-1.1179452], dtype=float32), 1.6802976]. 
=============================================
[2019-04-04 12:19:14,868] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6784378e-07 3.0844469e-09 2.6579386e-14 1.7260522e-12 9.9999988e-01
 6.3553507e-10 1.9724500e-13], sum to 1.0000
[2019-04-04 12:19:14,870] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5766
[2019-04-04 12:19:14,884] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 91.83333333333333, 0.0, 0.0, 26.0, 24.99939582258835, 0.261135441963981, 0.0, 1.0, 55196.00784382456], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2865000.0000, 
sim time next is 2865600.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.92064870867575, 0.2598245080553075, 0.0, 1.0, 55605.71007299794], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.576720725722979, 0.5866081693517692, 0.0, 1.0, 0.2647890955857045], 
reward next is 0.7352, 
noisyNet noise sample is [array([-0.3480525], dtype=float32), 1.3890816]. 
=============================================
[2019-04-04 12:19:18,763] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.7119360e-10 1.4231286e-11 1.7135144e-17 3.1766827e-16 1.0000000e+00
 4.5829560e-12 2.2446098e-17], sum to 1.0000
[2019-04-04 12:19:18,764] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3656
[2019-04-04 12:19:18,812] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.833333333333334, 25.0, 110.3333333333333, 0.0, 26.0, 25.14635448920807, 0.3648080641547879, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2818200.0000, 
sim time next is 2818800.0000, 
raw observation next is [7.0, 24.0, 106.5, 0.0, 26.0, 25.533054179198, 0.3938554554338969, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6565096952908588, 0.24, 0.355, 0.0, 0.6666666666666666, 0.6277545149331667, 0.631285151811299, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14058785], dtype=float32), 0.16858432]. 
=============================================
[2019-04-04 12:19:24,906] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.9816054e-10 1.0481730e-11 8.9296670e-18 2.1550505e-16 1.0000000e+00
 1.9643642e-12 4.7429444e-18], sum to 1.0000
[2019-04-04 12:19:24,906] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1168
[2019-04-04 12:19:24,937] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 87.33333333333334, 0.0, 0.0, 26.0, 25.33463172081463, 0.4236022658614338, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2920800.0000, 
sim time next is 2921400.0000, 
raw observation next is [-1.0, 85.0, 0.0, 0.0, 26.0, 25.21798496041747, 0.4088426953810516, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.85, 0.0, 0.0, 0.6666666666666666, 0.601498746701456, 0.6362808984603505, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5172829], dtype=float32), -1.8438631]. 
=============================================
[2019-04-04 12:19:31,451] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7019777e-07 1.8270596e-09 2.4797310e-14 2.8634186e-13 9.9999988e-01
 1.8884863e-09 7.3580905e-14], sum to 1.0000
[2019-04-04 12:19:31,451] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9506
[2019-04-04 12:19:31,480] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.38998302717642, 0.348489788675757, 0.0, 1.0, 34268.19603914952], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3100800.0000, 
sim time next is 3101400.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.39412994586434, 0.3473448888064279, 0.0, 1.0, 34737.44462285269], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6161774954886949, 0.6157816296021427, 0.0, 1.0, 0.1654164029659652], 
reward next is 0.8346, 
noisyNet noise sample is [array([0.26649347], dtype=float32), -0.4827512]. 
=============================================
[2019-04-04 12:19:38,828] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2241863e-11 6.7622850e-13 1.3666735e-19 3.7406451e-18 1.0000000e+00
 3.7559040e-13 7.4907531e-20], sum to 1.0000
[2019-04-04 12:19:38,829] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6242
[2019-04-04 12:19:38,852] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 100.0, 33.0, 307.5, 26.0, 27.61533392789277, 0.9759951693467542, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3171600.0000, 
sim time next is 3172200.0000, 
raw observation next is [6.0, 100.0, 24.66666666666666, 243.6666666666666, 26.0, 27.23719554588139, 0.9313248264827757, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.0822222222222222, 0.269244935543278, 0.6666666666666666, 0.769766295490116, 0.8104416088275919, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9239534], dtype=float32), -1.7406129]. 
=============================================
[2019-04-04 12:19:43,766] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.7328031e-08 8.0709501e-09 4.7071413e-13 4.2370353e-13 9.9999988e-01
 8.0468593e-10 2.1325560e-13], sum to 1.0000
[2019-04-04 12:19:43,766] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8908
[2019-04-04 12:19:43,784] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.75, 77.0, 0.0, 0.0, 26.0, 24.72026312649141, 0.2874271666562144, 0.0, 1.0, 43879.2258136356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3297000.0000, 
sim time next is 3297600.0000, 
raw observation next is [-8.9, 77.0, 0.0, 0.0, 26.0, 24.76195741146029, 0.2665398379540462, 0.0, 1.0, 44206.42367176198], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5634964509550242, 0.5888466126513487, 0.0, 1.0, 0.21050677938934276], 
reward next is 0.7895, 
noisyNet noise sample is [array([-1.0520232], dtype=float32), 0.018195054]. 
=============================================
[2019-04-04 12:19:49,897] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.7202420e-09 5.9700009e-11 2.4765134e-17 2.3324873e-15 1.0000000e+00
 7.4143382e-12 7.4452403e-17], sum to 1.0000
[2019-04-04 12:19:49,900] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9061
[2019-04-04 12:19:49,912] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.666666666666667, 48.66666666666666, 110.0, 770.6666666666667, 26.0, 26.5621790803426, 0.6347500309696509, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3408000.0000, 
sim time next is 3408600.0000, 
raw observation next is [2.833333333333333, 48.83333333333334, 111.0, 777.3333333333333, 26.0, 26.61396238142093, 0.6261012094617419, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.541089566020314, 0.48833333333333345, 0.37, 0.8589318600368323, 0.6666666666666666, 0.7178301984517441, 0.708700403153914, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26688972], dtype=float32), -0.226842]. 
=============================================
[2019-04-04 12:19:53,183] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.8413814e-09 1.0727090e-10 3.0491529e-16 8.4903584e-15 1.0000000e+00
 9.3675970e-12 3.5476601e-16], sum to 1.0000
[2019-04-04 12:19:53,184] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5675
[2019-04-04 12:19:53,203] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.666666666666667, 53.33333333333333, 9.166666666666668, 110.8333333333333, 26.0, 25.51214730623169, 0.4918032127870256, 1.0, 1.0, 123413.6372143199], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3346800.0000, 
sim time next is 3347400.0000, 
raw observation next is [-2.833333333333333, 54.16666666666667, 0.0, 0.0, 26.0, 25.42407433301002, 0.5131780732031914, 1.0, 1.0, 74215.02156549937], 
processed observation next is [1.0, 0.7391304347826086, 0.3841181902123731, 0.5416666666666667, 0.0, 0.0, 0.6666666666666666, 0.6186728610841682, 0.6710593577343972, 1.0, 1.0, 0.35340486459761605], 
reward next is 0.6466, 
noisyNet noise sample is [array([0.6014195], dtype=float32), 0.62636435]. 
=============================================
[2019-04-04 12:19:57,363] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2912640e-10 1.7944726e-11 6.3952220e-18 6.5819913e-17 1.0000000e+00
 4.0723887e-12 1.3691118e-17], sum to 1.0000
[2019-04-04 12:19:57,364] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7873
[2019-04-04 12:19:57,376] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 67.0, 68.66666666666666, 576.6666666666667, 26.0, 25.41121114559335, 0.5407846625148426, 1.0, 1.0, 95886.12503815943], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3427800.0000, 
sim time next is 3428400.0000, 
raw observation next is [2.0, 67.0, 64.83333333333333, 544.8333333333333, 26.0, 25.81149239625668, 0.5924269525347053, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.2161111111111111, 0.602025782688766, 0.6666666666666666, 0.6509576996880565, 0.6974756508449018, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7546008], dtype=float32), -0.2785548]. 
=============================================
[2019-04-04 12:19:57,470] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3741102e-07 3.1681497e-09 1.2187380e-13 2.7809916e-12 9.9999988e-01
 5.2734488e-09 1.4847176e-13], sum to 1.0000
[2019-04-04 12:19:57,470] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1486
[2019-04-04 12:19:57,487] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.666666666666667, 67.33333333333333, 0.0, 0.0, 26.0, 25.28299903423859, 0.4282691485714247, 0.0, 1.0, 41632.60921148796], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3548400.0000, 
sim time next is 3549000.0000, 
raw observation next is [-2.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 25.26345399429319, 0.4226458293059926, 0.0, 1.0, 41341.536690245], 
processed observation next is [0.0, 0.043478260869565216, 0.3841181902123731, 0.6916666666666668, 0.0, 0.0, 0.6666666666666666, 0.6052878328577659, 0.6408819431019975, 0.0, 1.0, 0.19686446042973807], 
reward next is 0.8031, 
noisyNet noise sample is [array([1.0203826], dtype=float32), 0.9844067]. 
=============================================
[2019-04-04 12:19:57,502] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[70.92589 ]
 [71.87511 ]
 [73.02626 ]
 [73.7374  ]
 [74.663475]], R is [[69.66439056]
 [69.76950073]
 [69.87062073]
 [69.96296692]
 [70.03386688]].
[2019-04-04 12:19:58,379] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.16787658e-06 7.76043478e-08 4.03716427e-12 3.07189968e-11
 9.99998808e-01 1.41562735e-08 3.83851458e-12], sum to 1.0000
[2019-04-04 12:19:58,381] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1225
[2019-04-04 12:19:58,453] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.833333333333334, 70.0, 59.66666666666667, 323.6666666666667, 26.0, 24.19537692907846, 0.2037232420440963, 0.0, 1.0, 41301.35315720049], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3571800.0000, 
sim time next is 3572400.0000, 
raw observation next is [-6.666666666666667, 70.0, 73.83333333333334, 374.3333333333334, 26.0, 24.19498460923119, 0.2920640799993901, 0.0, 1.0, 202438.3946891641], 
processed observation next is [0.0, 0.34782608695652173, 0.27793167128347185, 0.7, 0.24611111111111114, 0.4136279926335176, 0.6666666666666666, 0.5162487174359326, 0.5973546933331301, 0.0, 1.0, 0.9639923556626862], 
reward next is 0.0360, 
noisyNet noise sample is [array([0.74577177], dtype=float32), 1.4403367]. 
=============================================
[2019-04-04 12:20:02,946] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7158246e-06 1.9646649e-07 4.0179120e-11 7.2258505e-11 9.9999797e-01
 9.0741928e-08 5.0940866e-11], sum to 1.0000
[2019-04-04 12:20:02,947] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0210
[2019-04-04 12:20:02,961] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.166666666666666, 27.33333333333333, 0.0, 0.0, 26.0, 25.47450123135517, 0.3782335464260771, 0.0, 1.0, 36726.58655489114], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3640200.0000, 
sim time next is 3640800.0000, 
raw observation next is [8.133333333333333, 27.66666666666667, 0.0, 0.0, 26.0, 25.58811054716416, 0.3783989453896491, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.687903970452447, 0.2766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6323425455970133, 0.6261329817965496, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0255072], dtype=float32), 0.41652492]. 
=============================================
[2019-04-04 12:20:02,966] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.8322318e-10 1.1704752e-11 2.9141844e-18 2.9921151e-17 1.0000000e+00
 3.0953620e-13 1.0763389e-17], sum to 1.0000
[2019-04-04 12:20:02,969] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8347
[2019-04-04 12:20:02,975] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.0, 96.5, 743.5, 26.0, 26.723155910212, 0.6799113797700557, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3769200.0000, 
sim time next is 3769800.0000, 
raw observation next is [0.0, 60.0, 93.33333333333334, 732.6666666666667, 26.0, 26.76072892622016, 0.6902577789872658, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.3111111111111111, 0.8095764272559853, 0.6666666666666666, 0.7300607438516801, 0.7300859263290885, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24078439], dtype=float32), 1.0598415]. 
=============================================
[2019-04-04 12:20:03,267] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2103327e-07 3.8363144e-09 1.1465351e-13 4.3277540e-13 9.9999988e-01
 4.6833715e-10 3.5160961e-14], sum to 1.0000
[2019-04-04 12:20:03,268] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1525
[2019-04-04 12:20:03,281] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.19173159092363, 0.2939875835777311, 0.0, 1.0, 41995.02114815161], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3733800.0000, 
sim time next is 3734400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.14969567417987, 0.2871418908726094, 0.0, 1.0, 41624.48821728862], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5958079728483225, 0.5957139636242031, 0.0, 1.0, 0.19821184865375535], 
reward next is 0.8018, 
noisyNet noise sample is [array([-0.8823029], dtype=float32), -1.577834]. 
=============================================
[2019-04-04 12:20:05,735] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7450877e-08 1.0303818e-09 4.4201774e-14 6.1685910e-13 1.0000000e+00
 1.1694788e-09 3.3061444e-13], sum to 1.0000
[2019-04-04 12:20:05,736] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0376
[2019-04-04 12:20:05,759] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.49748725526911, 0.4009771264818798, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3718200.0000, 
sim time next is 3718800.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.5853996419536, 0.3877028132688023, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6321166368294667, 0.6292342710896007, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0197735], dtype=float32), -0.34200338]. 
=============================================
[2019-04-04 12:20:14,010] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.4268312e-10 1.4440846e-11 2.8337114e-16 1.5316036e-15 1.0000000e+00
 1.1144260e-11 2.6523556e-16], sum to 1.0000
[2019-04-04 12:20:14,013] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9369
[2019-04-04 12:20:14,026] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.166666666666667, 60.83333333333333, 0.0, 0.0, 26.0, 25.37180990143093, 0.4702770381114514, 0.0, 1.0, 42287.41097927494], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3885000.0000, 
sim time next is 3885600.0000, 
raw observation next is [-1.333333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 25.408165414528, 0.4682057413804744, 0.0, 1.0, 21542.4740006693], 
processed observation next is [1.0, 1.0, 0.42566943674976926, 0.6166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6173471178773333, 0.6560685804601581, 0.0, 1.0, 0.10258320952699666], 
reward next is 0.8974, 
noisyNet noise sample is [array([-0.11067376], dtype=float32), -0.011330091]. 
=============================================
[2019-04-04 12:20:18,106] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.0183527e-08 3.5509629e-09 1.4202853e-13 3.7364781e-12 1.0000000e+00
 2.2267901e-09 2.3061951e-13], sum to 1.0000
[2019-04-04 12:20:18,107] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5494
[2019-04-04 12:20:18,124] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.0, 66.0, 0.0, 0.0, 26.0, 23.68767120361527, 0.005694453988419695, 0.0, 1.0, 43794.61125856296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3994200.0000, 
sim time next is 3994800.0000, 
raw observation next is [-13.0, 65.0, 0.0, 0.0, 26.0, 23.61078942266065, -0.009553579516479607, 0.0, 1.0, 43764.7202377022], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.65, 0.0, 0.0, 0.6666666666666666, 0.4675657852217207, 0.4968154734945068, 0.0, 1.0, 0.2084034297033438], 
reward next is 0.7916, 
noisyNet noise sample is [array([0.5047447], dtype=float32), 1.2238995]. 
=============================================
[2019-04-04 12:20:18,555] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6255686e-08 1.9444018e-09 3.7326275e-14 1.0668615e-13 1.0000000e+00
 3.5545389e-10 1.7078071e-14], sum to 1.0000
[2019-04-04 12:20:18,556] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9205
[2019-04-04 12:20:18,622] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.5, 61.0, 6.0, 163.0, 26.0, 25.15312569784513, 0.3759534430752751, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3915000.0000, 
sim time next is 3915600.0000, 
raw observation next is [-7.666666666666666, 60.0, 20.16666666666666, 213.5, 26.0, 25.44921049896939, 0.4333973043121939, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.25023084025854114, 0.6, 0.0672222222222222, 0.23591160220994475, 0.6666666666666666, 0.6207675415807824, 0.6444657681040646, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.82356626], dtype=float32), -0.54733104]. 
=============================================
[2019-04-04 12:20:25,421] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5461125e-07 5.0918782e-08 8.0319909e-12 5.4485340e-11 9.9999964e-01
 3.7330434e-08 9.3548632e-12], sum to 1.0000
[2019-04-04 12:20:25,422] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2157
[2019-04-04 12:20:25,436] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.333333333333333, 47.33333333333334, 0.0, 0.0, 26.0, 25.28472548590631, 0.3872633376575772, 0.0, 1.0, 39213.41290219859], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4155600.0000, 
sim time next is 4156200.0000, 
raw observation next is [-2.5, 48.0, 0.0, 0.0, 26.0, 25.25888696709608, 0.3806916425911496, 0.0, 1.0, 39269.83535536782], 
processed observation next is [0.0, 0.08695652173913043, 0.39335180055401664, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6049072472580065, 0.6268972141970499, 0.0, 1.0, 0.18699921597794197], 
reward next is 0.8130, 
noisyNet noise sample is [array([0.8375814], dtype=float32), -0.4590802]. 
=============================================
[2019-04-04 12:20:27,904] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4910749e-08 6.0864083e-09 8.9103915e-14 1.4272461e-12 1.0000000e+00
 1.6220139e-09 9.7468875e-13], sum to 1.0000
[2019-04-04 12:20:27,905] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0378
[2019-04-04 12:20:27,918] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 36.0, 0.0, 0.0, 26.0, 24.82899811774097, 0.2207144359255359, 0.0, 1.0, 40190.81714542563], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4080600.0000, 
sim time next is 4081200.0000, 
raw observation next is [-4.0, 36.66666666666666, 0.0, 0.0, 26.0, 24.81842458596958, 0.2128651160840288, 0.0, 1.0, 40176.57983001737], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.3666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5682020488307984, 0.5709550386946763, 0.0, 1.0, 0.1913170468096065], 
reward next is 0.8087, 
noisyNet noise sample is [array([-0.7564625], dtype=float32), 1.3635784]. 
=============================================
[2019-04-04 12:20:28,450] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5434061e-06 5.0399939e-07 2.6879858e-11 3.0036809e-10 9.9999583e-01
 1.4300915e-07 6.5152855e-11], sum to 1.0000
[2019-04-04 12:20:28,451] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1985
[2019-04-04 12:20:28,476] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.13710393042809, 0.3568067823417923, 0.0, 1.0, 39465.60914299987], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4159200.0000, 
sim time next is 4159800.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.16239829807768, 0.3551703134458843, 0.0, 1.0, 39467.04892085253], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5968665248398066, 0.6183901044819614, 0.0, 1.0, 0.18793832819453588], 
reward next is 0.8121, 
noisyNet noise sample is [array([0.31600386], dtype=float32), -1.5585467]. 
=============================================
[2019-04-04 12:20:32,491] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3186074e-07 1.1726058e-09 3.7339160e-14 2.5953244e-13 9.9999988e-01
 6.0825517e-10 1.7325913e-14], sum to 1.0000
[2019-04-04 12:20:32,494] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1291
[2019-04-04 12:20:32,507] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.05, 73.5, 0.0, 0.0, 26.0, 25.74510696099891, 0.4370977298307769, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4309800.0000, 
sim time next is 4310400.0000, 
raw observation next is [5.0, 74.0, 0.0, 0.0, 26.0, 25.69309284448873, 0.425237165063389, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.6011080332409973, 0.74, 0.0, 0.0, 0.6666666666666666, 0.641091070374061, 0.6417457216877963, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09431872], dtype=float32), -0.9434651]. 
=============================================
[2019-04-04 12:20:41,862] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5674954e-10 4.6495811e-12 2.2748478e-18 6.1191004e-18 1.0000000e+00
 3.0776185e-13 1.9478079e-18], sum to 1.0000
[2019-04-04 12:20:41,863] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0737
[2019-04-04 12:20:41,871] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 232.8333333333333, 121.6666666666667, 26.0, 26.44462320655974, 0.6557376195204013, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4444800.0000, 
sim time next is 4445400.0000, 
raw observation next is [1.0, 86.0, 214.6666666666667, 97.33333333333334, 26.0, 26.44872659543891, 0.6535030144227189, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.7155555555555557, 0.10755064456721916, 0.6666666666666666, 0.7040605496199092, 0.7178343381409062, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4734117], dtype=float32), 1.0604241]. 
=============================================
[2019-04-04 12:20:46,122] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.93794804e-07 1.22021715e-09 9.14171828e-15 1.06119296e-13
 9.99999762e-01 1.18857502e-09 1.10505994e-14], sum to 1.0000
[2019-04-04 12:20:46,122] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8341
[2019-04-04 12:20:46,173] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 71.0, 61.5, 85.5, 26.0, 25.46517348092254, 0.4038579169924736, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4608000.0000, 
sim time next is 4608600.0000, 
raw observation next is [-2.0, 71.0, 82.00000000000001, 114.0, 26.0, 25.47327050722632, 0.4019759074149611, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.2733333333333334, 0.12596685082872927, 0.6666666666666666, 0.62277254226886, 0.6339919691383203, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6816353], dtype=float32), 0.40122798]. 
=============================================
[2019-04-04 12:20:46,204] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0348117e-09 2.2312914e-10 2.2541023e-15 5.2744518e-14 1.0000000e+00
 3.8095740e-10 5.2544168e-15], sum to 1.0000
[2019-04-04 12:20:46,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1382
[2019-04-04 12:20:46,220] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 60.33333333333334, 0.0, 0.0, 26.0, 25.56089509608302, 0.4720827124324349, 0.0, 1.0, 57603.72680630014], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4672200.0000, 
sim time next is 4672800.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 26.0, 25.48361252547879, 0.4690012440416597, 0.0, 1.0, 83774.5021144752], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6236343771232326, 0.6563337480138866, 0.0, 1.0, 0.39892620054511996], 
reward next is 0.6011, 
noisyNet noise sample is [array([-0.12518908], dtype=float32), 0.56101835]. 
=============================================
[2019-04-04 12:20:47,042] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2858070e-10 1.7307074e-12 1.4546292e-17 2.3083468e-17 1.0000000e+00
 3.2629745e-13 2.5156746e-18], sum to 1.0000
[2019-04-04 12:20:47,043] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2457
[2019-04-04 12:20:47,065] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.5, 50.5, 122.0, 833.0, 26.0, 26.54399543585851, 0.4739409851136653, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4620600.0000, 
sim time next is 4621200.0000, 
raw observation next is [2.666666666666667, 50.0, 121.6666666666667, 837.3333333333334, 26.0, 26.68503846367531, 0.7140385586662207, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5364727608494922, 0.5, 0.40555555555555567, 0.9252302025782689, 0.6666666666666666, 0.7237532053062757, 0.7380128528887403, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3840095], dtype=float32), 0.75929207]. 
=============================================
[2019-04-04 12:21:02,912] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5152155e-08 2.9865397e-09 3.1597659e-14 9.3229244e-13 1.0000000e+00
 9.3186359e-10 3.6303673e-14], sum to 1.0000
[2019-04-04 12:21:02,912] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2607
[2019-04-04 12:21:02,925] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.03240729168493, 0.2370509708529146, 0.0, 1.0, 38643.65900430184], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4947000.0000, 
sim time next is 4947600.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.05941296763297, 0.2305281250309849, 0.0, 1.0, 38636.29423595886], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5882844139694141, 0.5768427083436617, 0.0, 1.0, 0.18398235350456602], 
reward next is 0.8160, 
noisyNet noise sample is [array([1.3222046], dtype=float32), -0.4861526]. 
=============================================
[2019-04-04 12:21:03,427] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1260769e-08 2.0028806e-10 5.1438320e-15 1.8203950e-13 1.0000000e+00
 1.1541043e-10 1.2747045e-14], sum to 1.0000
[2019-04-04 12:21:03,429] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4584
[2019-04-04 12:21:03,472] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.5, 55.5, 153.0, 730.0, 26.0, 25.42339568801107, 0.4520144503642296, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4789800.0000, 
sim time next is 4790400.0000, 
raw observation next is [-2.333333333333333, 52.33333333333333, 147.8333333333333, 748.1666666666667, 26.0, 25.37091760715116, 0.4477759928136941, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3979686057248385, 0.5233333333333333, 0.4927777777777776, 0.8267034990791897, 0.6666666666666666, 0.6142431339292633, 0.6492586642712314, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31206423], dtype=float32), -0.3549554]. 
=============================================
[2019-04-04 12:21:05,662] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8428775e-11 4.6234310e-12 2.0629297e-18 1.6961773e-16 1.0000000e+00
 3.0276471e-12 7.8047436e-18], sum to 1.0000
[2019-04-04 12:21:05,664] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3806
[2019-04-04 12:21:05,678] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.0, 27.66666666666667, 121.1666666666667, 838.0, 26.0, 26.73678123276969, 0.6450257282654178, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4965600.0000, 
sim time next is 4966200.0000, 
raw observation next is [4.5, 27.0, 122.0, 845.0, 26.0, 26.82759288295181, 0.6637605520334315, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5872576177285319, 0.27, 0.4066666666666667, 0.9337016574585635, 0.6666666666666666, 0.735632740245984, 0.7212535173444771, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.82956326], dtype=float32), 0.09691806]. 
=============================================
[2019-04-04 12:21:09,287] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:09,287] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:09,294] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run3
[2019-04-04 12:21:09,539] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.8852238e-10 3.9211488e-11 1.8475009e-16 4.0895828e-16 1.0000000e+00
 2.3323886e-12 9.4211848e-17], sum to 1.0000
[2019-04-04 12:21:09,542] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2849
[2019-04-04 12:21:09,561] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.0, 19.0, 0.0, 0.0, 26.0, 27.10706227812094, 0.8506095310726582, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5086800.0000, 
sim time next is 5087400.0000, 
raw observation next is [8.95, 19.0, 0.0, 0.0, 26.0, 27.04491496472167, 0.83746867023806, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7105263157894738, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7537429137268058, 0.7791562234126866, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6604747], dtype=float32), -0.31199914]. 
=============================================
[2019-04-04 12:21:10,112] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.8679145e-09 5.3159216e-10 1.6379827e-15 3.3689143e-14 1.0000000e+00
 2.9496884e-11 2.5353318e-15], sum to 1.0000
[2019-04-04 12:21:10,113] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7319
[2019-04-04 12:21:10,156] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 59.0, 90.66666666666667, 461.0, 26.0, 25.61724114374078, 0.4790134777738546, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5041200.0000, 
sim time next is 5041800.0000, 
raw observation next is [-0.5, 56.0, 97.0, 533.0, 26.0, 25.8407614700685, 0.521864192871217, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44875346260387816, 0.56, 0.3233333333333333, 0.5889502762430939, 0.6666666666666666, 0.653396789172375, 0.6739547309570724, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42331493], dtype=float32), 0.4052715]. 
=============================================
[2019-04-04 12:21:11,000] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:11,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:11,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:11,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:11,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run3
[2019-04-04 12:21:11,048] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run3
[2019-04-04 12:21:12,013] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:12,013] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:12,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run3
[2019-04-04 12:21:12,355] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:12,355] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:12,358] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run3
[2019-04-04 12:21:12,444] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:12,444] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:12,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run3
[2019-04-04 12:21:13,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:13,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:13,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run3
[2019-04-04 12:21:13,267] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:13,268] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:13,275] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run3
[2019-04-04 12:21:13,607] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6354865e-09 1.2802821e-10 2.0598965e-16 2.1644106e-15 1.0000000e+00
 2.3276258e-11 2.6871856e-16], sum to 1.0000
[2019-04-04 12:21:13,607] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8311
[2019-04-04 12:21:13,624] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.166666666666667, 39.0, 0.0, 0.0, 26.0, 25.5387772159312, 0.4672700071339391, 0.0, 1.0, 40745.86606644467], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5010600.0000, 
sim time next is 5011200.0000, 
raw observation next is [2.0, 40.0, 0.0, 0.0, 26.0, 25.51107242366506, 0.4630142680492297, 0.0, 1.0, 51014.14915343074], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6259227019720882, 0.6543380893497432, 0.0, 1.0, 0.24292451977824164], 
reward next is 0.7571, 
noisyNet noise sample is [array([-1.3967025], dtype=float32), 1.3879155]. 
=============================================
[2019-04-04 12:21:15,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:15,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:15,355] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run3
[2019-04-04 12:21:15,926] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:15,926] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:15,930] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run3
[2019-04-04 12:21:16,279] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:16,279] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:16,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run3
[2019-04-04 12:21:16,946] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:16,946] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:16,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run3
[2019-04-04 12:21:17,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:17,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:17,775] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run3
[2019-04-04 12:21:18,934] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:18,934] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:18,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run3
[2019-04-04 12:21:19,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:19,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:19,179] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run3
[2019-04-04 12:21:20,619] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7928782e-07 2.6403324e-08 2.6086357e-12 2.0237768e-11 9.9999976e-01
 3.8306926e-08 7.9245399e-13], sum to 1.0000
[2019-04-04 12:21:20,619] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6915
[2019-04-04 12:21:20,651] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 6.999999999999998, 0.0, 26.0, 21.40989788061336, -0.5219087090604678, 0.0, 1.0, 40276.95351903303], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 28200.0000, 
sim time next is 28800.0000, 
raw observation next is [7.7, 93.0, 10.5, 0.0, 26.0, 21.43065937668549, -0.5063447317097076, 0.0, 1.0, 40257.02696294181], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.035, 0.0, 0.6666666666666666, 0.2858882813904575, 0.3312184227634308, 0.0, 1.0, 0.191700128394961], 
reward next is 0.8083, 
noisyNet noise sample is [array([0.778089], dtype=float32), -2.5333753]. 
=============================================
[2019-04-04 12:21:22,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:21:22,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:21:22,551] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run3
[2019-04-04 12:21:25,250] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0534659e-08 2.8866480e-09 5.6608059e-15 2.7853792e-14 1.0000000e+00
 2.9463554e-10 5.1390069e-15], sum to 1.0000
[2019-04-04 12:21:25,250] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9185
[2019-04-04 12:21:25,283] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.15, 87.0, 0.0, 0.0, 26.0, 24.72770091105027, 0.232331165326519, 0.0, 1.0, 41220.11046029191], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 73800.0000, 
sim time next is 74400.0000, 
raw observation next is [1.966666666666667, 86.33333333333334, 0.0, 0.0, 26.0, 24.70813546853611, 0.2290079262144255, 0.0, 1.0, 40836.23754079591], 
processed observation next is [0.0, 0.8695652173913043, 0.5170821791320407, 0.8633333333333334, 0.0, 0.0, 0.6666666666666666, 0.5590112890446758, 0.5763359754048085, 0.0, 1.0, 0.19445827400379007], 
reward next is 0.8055, 
noisyNet noise sample is [array([1.0360332], dtype=float32), 0.11887512]. 
=============================================
[2019-04-04 12:21:38,605] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7693491e-08 5.4123102e-09 5.2586902e-14 1.1489898e-12 9.9999988e-01
 2.2849291e-09 8.2531952e-13], sum to 1.0000
[2019-04-04 12:21:38,607] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2272
[2019-04-04 12:21:38,630] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.41666666666667, 67.5, 0.0, 0.0, 26.0, 23.09025473004428, -0.1563241389121954, 0.0, 1.0, 47407.72484152374], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 276600.0000, 
sim time next is 277200.0000, 
raw observation next is [-10.6, 67.0, 0.0, 0.0, 26.0, 23.02376087078305, -0.1676891825244997, 0.0, 1.0, 47546.68552249872], 
processed observation next is [1.0, 0.21739130434782608, 0.1689750692520776, 0.67, 0.0, 0.0, 0.6666666666666666, 0.41864673923192086, 0.4441036058251668, 0.0, 1.0, 0.22641278820237487], 
reward next is 0.7736, 
noisyNet noise sample is [array([-0.1970404], dtype=float32), -1.0798193]. 
=============================================
[2019-04-04 12:21:48,123] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8819953e-06 4.6985697e-08 3.7963760e-12 2.2992710e-11 9.9999809e-01
 3.4719829e-08 2.3686396e-12], sum to 1.0000
[2019-04-04 12:21:48,125] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0306
[2019-04-04 12:21:48,151] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.5, 72.33333333333333, 0.0, 0.0, 26.0, 22.50257042021493, -0.3099578801066201, 0.0, 1.0, 49353.31232957191], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 359400.0000, 
sim time next is 360000.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.40486542498936, -0.3273401645611297, 0.0, 1.0, 49395.53214615085], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.6666666666666666, 0.36707211874911333, 0.3908866118129568, 0.0, 1.0, 0.23521681974357547], 
reward next is 0.7648, 
noisyNet noise sample is [array([0.90657693], dtype=float32), 0.11896684]. 
=============================================
[2019-04-04 12:21:48,155] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[68.419266]
 [68.451775]
 [68.48826 ]
 [68.53594 ]
 [68.59983 ]], R is [[68.50834656]
 [68.58824921]
 [68.6674881 ]
 [68.74593353]
 [68.82362366]].
[2019-04-04 12:21:56,407] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.8227625e-07 6.2733243e-08 1.6347876e-12 8.9938187e-12 9.9999905e-01
 1.6690450e-08 1.5177281e-11], sum to 1.0000
[2019-04-04 12:21:56,407] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5912
[2019-04-04 12:21:56,428] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.7, 50.0, 0.0, 0.0, 26.0, 23.18970246370147, -0.1620800120738617, 0.0, 1.0, 45943.12880916321], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 442200.0000, 
sim time next is 442800.0000, 
raw observation next is [-10.6, 49.0, 0.0, 0.0, 26.0, 23.18771093072552, -0.1705387235287718, 0.0, 1.0, 45968.53062398828], 
processed observation next is [1.0, 0.13043478260869565, 0.1689750692520776, 0.49, 0.0, 0.0, 0.6666666666666666, 0.4323092442271266, 0.4431537588237427, 0.0, 1.0, 0.21889776487613466], 
reward next is 0.7811, 
noisyNet noise sample is [array([-0.7386676], dtype=float32), -1.7683141]. 
=============================================
[2019-04-04 12:22:09,065] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.0189556e-08 1.1719016e-09 1.3471841e-13 7.5075146e-13 1.0000000e+00
 1.5330053e-09 1.0584986e-13], sum to 1.0000
[2019-04-04 12:22:09,065] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9664
[2019-04-04 12:22:09,087] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.42833615181409, 0.1458374866377136, 0.0, 1.0, 42138.92506098843], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 609000.0000, 
sim time next is 609600.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.39486756365323, 0.145506313443295, 0.0, 1.0, 42119.82224122762], 
processed observation next is [0.0, 0.043478260869565216, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5329056303044357, 0.5485021044810984, 0.0, 1.0, 0.20057058210108392], 
reward next is 0.7994, 
noisyNet noise sample is [array([0.6573633], dtype=float32), 1.3078898]. 
=============================================
[2019-04-04 12:22:13,400] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.1948396e-09 3.9307654e-10 8.7235567e-15 3.8691672e-14 1.0000000e+00
 3.1661854e-10 4.0321453e-15], sum to 1.0000
[2019-04-04 12:22:13,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7007
[2019-04-04 12:22:13,437] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 26.0, 24.91453178173848, 0.2543572336951418, 0.0, 1.0, 42971.49657767465], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 597600.0000, 
sim time next is 598200.0000, 
raw observation next is [-2.9, 83.0, 0.0, 0.0, 26.0, 24.88431210942075, 0.248108071775179, 0.0, 1.0, 42943.01626062771], 
processed observation next is [0.0, 0.9565217391304348, 0.38227146814404434, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5736926757850626, 0.5827026905917263, 0.0, 1.0, 0.2044905536220367], 
reward next is 0.7955, 
noisyNet noise sample is [array([-0.493571], dtype=float32), 0.23284706]. 
=============================================
[2019-04-04 12:22:17,634] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4874033e-08 7.7526696e-10 7.7072095e-15 1.5835545e-13 1.0000000e+00
 1.4548758e-10 3.5942986e-14], sum to 1.0000
[2019-04-04 12:22:17,637] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8814
[2019-04-04 12:22:17,654] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.9, 83.0, 0.0, 0.0, 26.0, 24.88417777105195, 0.248072400648462, 0.0, 1.0, 42943.05997185168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 598200.0000, 
sim time next is 598800.0000, 
raw observation next is [-3.0, 83.0, 0.0, 0.0, 26.0, 24.85442114531823, 0.24617251693362, 0.0, 1.0, 42900.91664030064], 
processed observation next is [0.0, 0.9565217391304348, 0.3795013850415513, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5712017621098525, 0.58205750564454, 0.0, 1.0, 0.20429007923952686], 
reward next is 0.7957, 
noisyNet noise sample is [array([-0.7881069], dtype=float32), -1.226457]. 
=============================================
[2019-04-04 12:22:19,568] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8778206e-08 4.9046857e-11 1.6368879e-17 4.5166499e-15 1.0000000e+00
 2.8945955e-11 1.3180948e-16], sum to 1.0000
[2019-04-04 12:22:19,568] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4681
[2019-04-04 12:22:19,581] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 63.00000000000001, 93.16666666666666, 660.5000000000001, 26.0, 25.82408996119889, 0.3751246261454294, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 732000.0000, 
sim time next is 732600.0000, 
raw observation next is [-0.6, 61.5, 84.0, 779.0, 26.0, 25.83382614857809, 0.3893236521159016, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.615, 0.28, 0.8607734806629834, 0.6666666666666666, 0.6528188457148408, 0.6297745507053005, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9899495], dtype=float32), 0.025672914]. 
=============================================
[2019-04-04 12:22:20,868] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2234214e-08 7.7531126e-10 6.4690371e-15 6.6835108e-14 1.0000000e+00
 5.0119169e-11 6.5104545e-15], sum to 1.0000
[2019-04-04 12:22:20,870] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5079
[2019-04-04 12:22:20,934] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 76.0, 19.33333333333334, 0.0, 26.0, 25.63106582927606, 0.2780827102866599, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 720600.0000, 
sim time next is 721200.0000, 
raw observation next is [-2.3, 76.0, 24.16666666666667, 0.0, 26.0, 25.59569227002499, 0.2795663873194876, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.08055555555555557, 0.0, 0.6666666666666666, 0.6329743558354158, 0.5931887957731625, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.998667], dtype=float32), -0.059875924]. 
=============================================
[2019-04-04 12:22:21,208] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2364392e-09 1.1554126e-10 9.0349712e-16 5.5863462e-15 1.0000000e+00
 3.6358402e-11 5.5724667e-16], sum to 1.0000
[2019-04-04 12:22:21,224] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2524
[2019-04-04 12:22:21,267] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.366666666666667, 65.0, 0.0, 0.0, 26.0, 24.68861529336029, 0.2354351788158744, 0.0, 1.0, 43195.55906783097], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 771600.0000, 
sim time next is 772200.0000, 
raw observation next is [-6.45, 65.5, 0.0, 0.0, 26.0, 24.65519564082457, 0.2280735763478866, 0.0, 1.0, 43069.02016286544], 
processed observation next is [1.0, 0.9565217391304348, 0.28393351800554023, 0.655, 0.0, 0.0, 0.6666666666666666, 0.5545996367353808, 0.5760245254492955, 0.0, 1.0, 0.20509057220412114], 
reward next is 0.7949, 
noisyNet noise sample is [array([-0.42715728], dtype=float32), 1.0935419]. 
=============================================
[2019-04-04 12:22:22,509] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.6555221e-11 2.5424495e-12 1.1462677e-18 7.7590322e-17 1.0000000e+00
 2.0593968e-13 2.0435367e-18], sum to 1.0000
[2019-04-04 12:22:22,513] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5842
[2019-04-04 12:22:22,559] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 79.0, 80.33333333333333, 0.0, 26.0, 26.18692062720876, 0.4605782812861062, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 827400.0000, 
sim time next is 828000.0000, 
raw observation next is [-3.9, 79.0, 75.0, 0.0, 26.0, 26.30347224080308, 0.4649673222378932, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.79, 0.25, 0.0, 0.6666666666666666, 0.6919560200669235, 0.654989107412631, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38732937], dtype=float32), 0.73563313]. 
=============================================
[2019-04-04 12:22:22,566] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[95.241936]
 [95.33145 ]
 [95.06999 ]
 [94.781555]
 [94.3206  ]], R is [[95.35542297]
 [95.40187073]
 [95.44785309]
 [95.49337769]
 [94.58427429]].
[2019-04-04 12:22:22,868] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5434276e-08 3.6231615e-09 2.6735280e-14 2.9778366e-13 1.0000000e+00
 5.7749396e-09 8.0877274e-14], sum to 1.0000
[2019-04-04 12:22:22,869] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9328
[2019-04-04 12:22:22,896] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 73.0, 0.0, 0.0, 26.0, 23.8029737288622, 0.0006060254461946237, 0.0, 1.0, 41495.61557503506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 793800.0000, 
sim time next is 794400.0000, 
raw observation next is [-7.3, 72.33333333333333, 0.0, 0.0, 26.0, 23.78091651716512, 0.005259224337685775, 0.0, 1.0, 41570.41596919889], 
processed observation next is [1.0, 0.17391304347826086, 0.26038781163434904, 0.7233333333333333, 0.0, 0.0, 0.6666666666666666, 0.4817430430970934, 0.5017530747792286, 0.0, 1.0, 0.19795436175808995], 
reward next is 0.8020, 
noisyNet noise sample is [array([0.8423357], dtype=float32), -0.25443938]. 
=============================================
[2019-04-04 12:22:32,481] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5446636e-10 8.9669184e-12 5.5496139e-18 4.5062354e-16 1.0000000e+00
 1.0117275e-12 1.3560749e-18], sum to 1.0000
[2019-04-04 12:22:32,482] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1337
[2019-04-04 12:22:32,494] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 76.0, 0.0, 0.0, 26.0, 25.81657304444961, 0.6228833923323679, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1027800.0000, 
sim time next is 1028400.0000, 
raw observation next is [14.4, 75.66666666666667, 0.0, 0.0, 26.0, 25.86377547751898, 0.6230046862364659, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6553146231265817, 0.7076682287454886, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.002753], dtype=float32), 0.08538619]. 
=============================================
[2019-04-04 12:22:38,296] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7654818e-12 2.5083218e-13 1.1983027e-19 1.3736415e-18 1.0000000e+00
 5.3679761e-14 4.1702976e-20], sum to 1.0000
[2019-04-04 12:22:38,297] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7523
[2019-04-04 12:22:38,305] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.26666666666667, 84.33333333333334, 120.5, 0.0, 26.0, 26.18727144899641, 0.6260738989593809, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 998400.0000, 
sim time next is 999000.0000, 
raw observation next is [13.55, 83.5, 119.0, 0.0, 26.0, 26.28732859421774, 0.6472027910597499, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8379501385041552, 0.835, 0.39666666666666667, 0.0, 0.6666666666666666, 0.6906107161848117, 0.7157342636865832, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5548809], dtype=float32), 0.23372576]. 
=============================================
[2019-04-04 12:22:38,324] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[105.03741 ]
 [104.93436 ]
 [104.82671 ]
 [104.68056 ]
 [104.598236]], R is [[105.08837891]
 [105.03749847]
 [104.98712158]
 [104.93724823]
 [104.88787842]].
[2019-04-04 12:22:38,380] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 12:22:38,381] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 12:22:38,382] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:22:38,383] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 12:22:38,383] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 12:22:38,384] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:22:38,384] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:22:38,391] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run4
[2019-04-04 12:22:38,392] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run4
[2019-04-04 12:22:38,392] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run4
[2019-04-04 12:23:00,074] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1476525], dtype=float32), 0.15946135]
[2019-04-04 12:23:00,075] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-12.86666666666667, 67.66666666666666, 0.0, 0.0, 26.0, 22.64201383086426, -0.3226365102927308, 0.0, 1.0, 48020.4647709077]
[2019-04-04 12:23:00,075] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 12:23:00,075] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.5995470e-06 3.1441215e-07 5.2052990e-11 4.0635931e-10 9.9999785e-01
 1.8703727e-07 7.3343220e-11], sampled 0.8650955275487033
[2019-04-04 12:23:10,542] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1476525], dtype=float32), 0.15946135]
[2019-04-04 12:23:10,542] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.990516937999999, 91.61756661, 70.801574125, 0.0, 26.0, 25.14866632247176, 0.2219177477611645, 1.0, 1.0, 0.0]
[2019-04-04 12:23:10,542] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 12:23:10,543] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.5481321e-09 8.4214760e-11 2.8822255e-16 4.6072265e-15 1.0000000e+00
 2.3557684e-11 5.0774093e-16], sampled 0.39606975293214186
[2019-04-04 12:23:50,964] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1476525], dtype=float32), 0.15946135]
[2019-04-04 12:23:50,964] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.139196941, 82.30700731499999, 0.0, 0.0, 26.0, 24.39558627698328, 0.1306747372112608, 0.0, 1.0, 41548.68549173737]
[2019-04-04 12:23:50,964] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 12:23:50,965] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.1677427e-07 2.8481956e-08 1.6138640e-12 1.4068435e-11 9.9999976e-01
 1.6908219e-08 2.3022270e-12], sampled 0.9486224137970418
[2019-04-04 12:24:03,935] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1476525], dtype=float32), 0.15946135]
[2019-04-04 12:24:03,936] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-0.6333333333333333, 65.66666666666667, 0.0, 0.0, 26.0, 25.2090287760475, 0.3850920179057988, 0.0, 1.0, 45133.97504957167]
[2019-04-04 12:24:03,936] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 12:24:03,937] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.9252091e-08 1.3859860e-08 4.0954585e-13 3.9205032e-12 9.9999988e-01
 5.1092912e-09 6.5048504e-13], sampled 0.1436958434579232
[2019-04-04 12:24:04,491] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1476525], dtype=float32), 0.15946135]
[2019-04-04 12:24:04,492] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.0, 59.0, 114.5, 256.0, 26.0, 25.20054460100573, 0.3705681278401413, 1.0, 1.0, 0.0]
[2019-04-04 12:24:04,492] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 12:24:04,493] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.2241553e-08 1.2888951e-09 1.7266336e-14 1.9320025e-13 1.0000000e+00
 3.0379352e-10 2.7997925e-14], sampled 0.9723707275651585
[2019-04-04 12:24:21,313] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4172 239942393.0563 1605.0250
[2019-04-04 12:24:41,081] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6351 263416630.7723 1557.0695
[2019-04-04 12:24:42,141] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6755 275798142.8329 1233.3125
[2019-04-04 12:24:43,164] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 300000, evaluation results [300000.0, 7241.6350915605335, 263416630.7722868, 1557.0695496036576, 7353.417175922201, 239942393.05634084, 1605.024971488376, 7182.675510319502, 275798142.8329035, 1233.3125213312358]
[2019-04-04 12:24:44,802] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1526957e-07 1.1370965e-09 3.3439787e-14 2.5891884e-13 9.9999988e-01
 2.7152223e-09 3.5907970e-14], sum to 1.0000
[2019-04-04 12:24:44,805] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9913
[2019-04-04 12:24:44,815] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.7, 83.33333333333333, 11.0, 0.6666666666666667, 26.0, 25.70981130086617, 0.6106843186907084, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1151400.0000, 
sim time next is 1152000.0000, 
raw observation next is [12.7, 84.0, 16.0, 0.5, 26.0, 25.6942277897203, 0.6066768563254835, 0.0, 1.0, 22217.42293495743], 
processed observation next is [0.0, 0.34782608695652173, 0.8144044321329641, 0.84, 0.05333333333333334, 0.0005524861878453039, 0.6666666666666666, 0.6411856491433584, 0.7022256187751612, 0.0, 1.0, 0.10579725207122587], 
reward next is 0.8942, 
noisyNet noise sample is [array([1.6455445], dtype=float32), -0.25679958]. 
=============================================
[2019-04-04 12:24:44,826] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[77.219894]
 [77.10458 ]
 [77.02698 ]
 [76.903046]
 [76.804306]], R is [[77.54509735]
 [77.76964569]
 [77.99195099]
 [78.12286377]
 [78.25247192]].
[2019-04-04 12:24:47,460] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.1151376e-08 4.2252232e-08 2.3236044e-13 1.0825373e-11 9.9999988e-01
 1.4283048e-09 9.7828516e-13], sum to 1.0000
[2019-04-04 12:24:47,462] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5089
[2019-04-04 12:24:47,467] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 65.0, 128.0, 0.0, 26.0, 25.07203206265401, 0.500618798212829, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1175400.0000, 
sim time next is 1176000.0000, 
raw observation next is [18.3, 65.0, 120.0, 0.0, 26.0, 25.0667658134353, 0.4975575113098434, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.4, 0.0, 0.6666666666666666, 0.5888971511196083, 0.6658525037699478, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6413752], dtype=float32), 0.28072867]. 
=============================================
[2019-04-04 12:24:47,478] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[67.613945]
 [67.64284 ]
 [67.67034 ]
 [67.63419 ]
 [67.55726 ]], R is [[67.89213562]
 [68.21321869]
 [68.53108978]
 [68.84577942]
 [69.15732574]].
[2019-04-04 12:24:48,319] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5958900e-10 1.1271717e-10 1.6124274e-16 5.2797506e-16 1.0000000e+00
 1.4222875e-11 3.6195086e-16], sum to 1.0000
[2019-04-04 12:24:48,324] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4603
[2019-04-04 12:24:48,361] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 5.999999999999998, 0.0, 26.0, 25.85705816516035, 0.5601647804348066, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1324200.0000, 
sim time next is 1324800.0000, 
raw observation next is [1.1, 92.0, 9.0, 0.0, 26.0, 25.72757053502118, 0.5471186932849185, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.49307479224376743, 0.92, 0.03, 0.0, 0.6666666666666666, 0.643964211251765, 0.6823728977616396, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7572697], dtype=float32), -1.0383451]. 
=============================================
[2019-04-04 12:24:53,056] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9870013e-09 1.2174156e-10 6.0457837e-17 3.0009300e-16 1.0000000e+00
 2.4478555e-11 2.0925028e-16], sum to 1.0000
[2019-04-04 12:24:53,059] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7989
[2019-04-04 12:24:53,097] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8, 92.0, 18.0, 0.0, 26.0, 25.72109661969663, 0.577002103847958, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1326600.0000, 
sim time next is 1327200.0000, 
raw observation next is [0.7000000000000001, 92.0, 22.5, 0.0, 26.0, 25.95731968090255, 0.591335356373157, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4819944598337951, 0.92, 0.075, 0.0, 0.6666666666666666, 0.6631099734085458, 0.697111785457719, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35848612], dtype=float32), 0.8779706]. 
=============================================
[2019-04-04 12:24:53,260] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.7521637e-07 1.3015210e-08 1.1736498e-13 2.8730193e-12 9.9999928e-01
 1.9253137e-09 1.8884490e-13], sum to 1.0000
[2019-04-04 12:24:53,263] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6409
[2019-04-04 12:24:53,306] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 98.0, 95.0, 0.0, 26.0, 25.00574518388384, 0.4794721511719577, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1251000.0000, 
sim time next is 1251600.0000, 
raw observation next is [14.4, 97.33333333333334, 96.0, 0.0, 26.0, 25.04367020509613, 0.4824698167974957, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 0.9733333333333334, 0.32, 0.0, 0.6666666666666666, 0.5869725170913442, 0.6608232722658319, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.74918646], dtype=float32), -1.1950666]. 
=============================================
[2019-04-04 12:25:08,477] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6746331e-10 1.4208992e-11 9.5936341e-17 1.0869861e-14 1.0000000e+00
 1.8475338e-11 1.2066853e-16], sum to 1.0000
[2019-04-04 12:25:08,477] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5259
[2019-04-04 12:25:08,489] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 81.50000000000001, 0.0, 0.0, 26.0, 25.65078009155314, 0.5595899469564575, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1559400.0000, 
sim time next is 1560000.0000, 
raw observation next is [5.0, 81.0, 0.0, 0.0, 26.0, 25.68035038968078, 0.555718309851589, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6011080332409973, 0.81, 0.0, 0.0, 0.6666666666666666, 0.640029199140065, 0.6852394366171963, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31203666], dtype=float32), 0.63620746]. 
=============================================
[2019-04-04 12:25:08,494] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[89.421814]
 [89.9604  ]
 [90.62107 ]
 [91.20298 ]
 [91.74433 ]], R is [[89.40571594]
 [89.51165771]
 [89.616539  ]
 [89.72037506]
 [89.82317352]].
[2019-04-04 12:25:09,208] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2027392e-09 1.3664400e-10 5.0543356e-16 3.9086556e-15 1.0000000e+00
 4.5560705e-11 3.1745749e-16], sum to 1.0000
[2019-04-04 12:25:09,209] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3900
[2019-04-04 12:25:09,224] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.5982757703896, 0.5185696089699424, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1575600.0000, 
sim time next is 1576200.0000, 
raw observation next is [4.95, 82.33333333333334, 0.0, 0.0, 26.0, 25.63560531070378, 0.517279697912499, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5997229916897507, 0.8233333333333335, 0.0, 0.0, 0.6666666666666666, 0.6363004425586484, 0.672426565970833, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36979562], dtype=float32), -0.82509315]. 
=============================================
[2019-04-04 12:25:09,305] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7663235e-09 2.0683111e-11 1.6152717e-16 5.6142470e-15 1.0000000e+00
 1.7803950e-12 4.2132216e-17], sum to 1.0000
[2019-04-04 12:25:09,308] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2719
[2019-04-04 12:25:09,334] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.70210623240487, 0.558303957039372, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1657800.0000, 
sim time next is 1658400.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.66918563652113, 0.5408009554034203, 0.0, 1.0, 23016.34797448177], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6390988030434276, 0.6802669851344735, 0.0, 1.0, 0.10960165702134177], 
reward next is 0.8904, 
noisyNet noise sample is [array([1.4612792], dtype=float32), 0.0359713]. 
=============================================
[2019-04-04 12:25:09,663] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4752523e-10 1.5453405e-10 1.3089653e-16 1.7644730e-15 1.0000000e+00
 1.3644651e-11 9.2976173e-17], sum to 1.0000
[2019-04-04 12:25:09,666] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2496
[2019-04-04 12:25:09,679] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.3531115e-10 3.6068169e-11 1.0921847e-16 6.8305279e-15 1.0000000e+00
 1.3936823e-11 8.0084153e-17], sum to 1.0000
[2019-04-04 12:25:09,682] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7713
[2019-04-04 12:25:09,704] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 96.33333333333333, 0.0, 0.0, 26.0, 25.69118678636204, 0.5731997344299052, 0.0, 1.0, 34470.65159875025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1650000.0000, 
sim time next is 1650600.0000, 
raw observation next is [6.9, 96.5, 0.0, 0.0, 26.0, 25.71790907919377, 0.5715412312796762, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6537396121883658, 0.965, 0.0, 0.0, 0.6666666666666666, 0.6431590899328142, 0.690513743759892, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40872324], dtype=float32), 0.7297623]. 
=============================================
[2019-04-04 12:25:09,713] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.266666666666667, 81.00000000000001, 25.0, 25.0, 26.0, 25.33033803657525, 0.480345428474672, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1584600.0000, 
sim time next is 1585200.0000, 
raw observation next is [5.533333333333333, 80.0, 31.0, 30.0, 26.0, 25.46337953137524, 0.5367278288197465, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6158818097876271, 0.8, 0.10333333333333333, 0.03314917127071823, 0.6666666666666666, 0.6219482942812699, 0.6789092762732488, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7470034], dtype=float32), -0.122727774]. 
=============================================
[2019-04-04 12:25:25,982] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5188178e-09 9.2551516e-10 3.3872001e-14 4.9388014e-14 1.0000000e+00
 1.8516022e-10 3.5744790e-14], sum to 1.0000
[2019-04-04 12:25:25,982] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5978
[2019-04-04 12:25:26,028] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 76.33333333333333, 43.33333333333333, 0.0, 26.0, 25.00627838350367, 0.2595391630017359, 0.0, 1.0, 52534.28728579702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1872600.0000, 
sim time next is 1873200.0000, 
raw observation next is [-4.5, 77.66666666666667, 36.16666666666666, 0.0, 26.0, 25.00871988984074, 0.2638889445349635, 0.0, 1.0, 46279.53455586803], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.7766666666666667, 0.12055555555555553, 0.0, 0.6666666666666666, 0.5840599908200618, 0.5879629815116545, 0.0, 1.0, 0.22037873598032395], 
reward next is 0.7796, 
noisyNet noise sample is [array([-0.23961729], dtype=float32), 0.32221186]. 
=============================================
[2019-04-04 12:25:31,617] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4082453e-08 5.3154758e-10 6.4048019e-15 1.1185098e-12 1.0000000e+00
 1.8535104e-10 1.2207289e-14], sum to 1.0000
[2019-04-04 12:25:31,617] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6238
[2019-04-04 12:25:31,699] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 26.0, 24.4452978489698, 0.1105834951628297, 1.0, 1.0, 155932.3830473522], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1928400.0000, 
sim time next is 1929000.0000, 
raw observation next is [-9.5, 91.0, 12.33333333333333, 7.666666666666665, 26.0, 24.79274058743896, 0.1526231350193654, 1.0, 1.0, 70895.38934126002], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0411111111111111, 0.008471454880294658, 0.6666666666666666, 0.5660617156199134, 0.5508743783397885, 1.0, 1.0, 0.33759709210123817], 
reward next is 0.6624, 
noisyNet noise sample is [array([-0.59703034], dtype=float32), -1.7925308]. 
=============================================
[2019-04-04 12:25:31,716] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.166916]
 [78.35874 ]
 [77.22179 ]
 [76.34606 ]
 [76.26586 ]], R is [[79.85887909]
 [79.31774902]
 [78.55657196]
 [77.80738831]
 [77.81739807]].
[2019-04-04 12:25:33,742] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4145270e-09 9.8524848e-11 1.5527398e-16 1.2152318e-15 1.0000000e+00
 3.0955599e-11 7.5811163e-17], sum to 1.0000
[2019-04-04 12:25:33,742] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0733
[2019-04-04 12:25:33,779] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.983333333333333, 64.16666666666667, 44.66666666666666, 0.0, 26.0, 25.53946233359245, 0.3109186153945699, 1.0, 1.0, 30873.87840436743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1959000.0000, 
sim time next is 1959600.0000, 
raw observation next is [-3.166666666666667, 66.33333333333334, 37.33333333333333, 0.0, 26.0, 25.50084909272839, 0.3160406298765137, 1.0, 1.0, 33520.62041777417], 
processed observation next is [1.0, 0.6956521739130435, 0.3748845798707295, 0.6633333333333334, 0.12444444444444443, 0.0, 0.6666666666666666, 0.6250707577273659, 0.6053468766255046, 1.0, 1.0, 0.1596220019894008], 
reward next is 0.8404, 
noisyNet noise sample is [array([0.9398483], dtype=float32), 0.56643176]. 
=============================================
[2019-04-04 12:25:38,169] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3616811e-10 4.1688854e-11 6.3747865e-18 7.0467842e-16 1.0000000e+00
 3.2685899e-11 1.3926689e-16], sum to 1.0000
[2019-04-04 12:25:38,169] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9948
[2019-04-04 12:25:38,219] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.55, 78.5, 208.0, 60.0, 26.0, 25.84745078425244, 0.3909340536378201, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2111400.0000, 
sim time next is 2112000.0000, 
raw observation next is [-7.466666666666667, 77.33333333333334, 222.1666666666667, 66.83333333333333, 26.0, 25.79898733542045, 0.3892468438479336, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.25577100646352724, 0.7733333333333334, 0.7405555555555557, 0.07384898710865562, 0.6666666666666666, 0.6499156112850374, 0.6297489479493112, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1094409], dtype=float32), 0.5573237]. 
=============================================
[2019-04-04 12:25:38,227] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[95.222275]
 [94.2769  ]
 [93.329025]
 [92.439674]
 [91.314835]], R is [[95.48355865]
 [95.52872467]
 [95.57344055]
 [95.6177063 ]
 [95.66152954]].
[2019-04-04 12:25:41,369] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.65807277e-08 8.32916669e-09 1.56870299e-14 5.29692555e-13
 9.99999881e-01 3.15491966e-09 1.02612167e-13], sum to 1.0000
[2019-04-04 12:25:41,380] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6295
[2019-04-04 12:25:41,411] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.37075245426759, 0.1707444184268242, 0.0, 1.0, 42569.46236449895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2167800.0000, 
sim time next is 2168400.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.49001785936949, 0.1699812239992621, 0.0, 1.0, 42515.79690205408], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5408348216141242, 0.556660407999754, 0.0, 1.0, 0.20245617572406704], 
reward next is 0.7975, 
noisyNet noise sample is [array([-0.51352316], dtype=float32), 0.30408996]. 
=============================================
[2019-04-04 12:25:42,090] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.0100439e-11 1.7473449e-12 1.8857497e-18 1.9541422e-17 1.0000000e+00
 8.7692088e-13 3.8883521e-17], sum to 1.0000
[2019-04-04 12:25:42,090] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0985
[2019-04-04 12:25:42,143] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.4, 85.33333333333334, 82.0, 0.0, 26.0, 26.19577913814474, 0.444625000974909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2041800.0000, 
sim time next is 2042400.0000, 
raw observation next is [-4.300000000000001, 84.66666666666667, 76.5, 0.0, 26.0, 26.14695091537763, 0.3264740837205793, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.34349030470914127, 0.8466666666666667, 0.255, 0.0, 0.6666666666666666, 0.6789125762814692, 0.6088246945735264, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0874785], dtype=float32), -0.8579189]. 
=============================================
[2019-04-04 12:25:44,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8515396e-09 1.6097273e-11 1.9954999e-16 5.6614755e-15 1.0000000e+00
 1.7910395e-11 3.9481707e-16], sum to 1.0000
[2019-04-04 12:25:44,411] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0059
[2019-04-04 12:25:44,431] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.516666666666667, 83.0, 0.0, 0.0, 26.0, 25.22879921330794, 0.4028103624089551, 0.0, 1.0, 42264.4063851926], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2152200.0000, 
sim time next is 2152800.0000, 
raw observation next is [-6.7, 83.0, 0.0, 0.0, 26.0, 25.21737428533619, 0.3976256699475048, 0.0, 1.0, 42206.82657616903], 
processed observation next is [1.0, 0.9565217391304348, 0.2770083102493075, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6014478571113493, 0.6325418899825016, 0.0, 1.0, 0.20098488845794774], 
reward next is 0.7990, 
noisyNet noise sample is [array([-0.541054], dtype=float32), -0.27412766]. 
=============================================
[2019-04-04 12:26:00,190] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3349730e-09 3.7858425e-11 1.7435494e-16 1.9659878e-15 1.0000000e+00
 1.0996625e-11 2.5840932e-16], sum to 1.0000
[2019-04-04 12:26:00,192] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3503
[2019-04-04 12:26:00,242] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.983333333333333, 79.5, 126.6666666666667, 42.66666666666667, 26.0, 25.6486401670879, 0.3179417178435578, 1.0, 1.0, 18732.41801364534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2281800.0000, 
sim time next is 2282400.0000, 
raw observation next is [-6.7, 78.0, 139.5, 44.5, 26.0, 25.62989757317027, 0.325842559357997, 1.0, 1.0, 18730.00607466905], 
processed observation next is [1.0, 0.43478260869565216, 0.2770083102493075, 0.78, 0.465, 0.049171270718232046, 0.6666666666666666, 0.635824797764189, 0.6086141864526656, 1.0, 1.0, 0.08919050511747167], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.7915137], dtype=float32), -0.1070522]. 
=============================================
[2019-04-04 12:26:05,361] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1273956e-07 5.6189123e-08 1.8614582e-11 2.4283101e-10 9.9999928e-01
 1.5999305e-08 3.6912837e-11], sum to 1.0000
[2019-04-04 12:26:05,362] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6123
[2019-04-04 12:26:05,377] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 61.0, 0.0, 0.0, 26.0, 22.98791672939569, -0.2090736148233522, 0.0, 1.0, 44087.85257692694], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2444400.0000, 
sim time next is 2445000.0000, 
raw observation next is [-9.5, 60.5, 0.0, 0.0, 26.0, 22.94446834786691, -0.2168442877654522, 0.0, 1.0, 44127.97671058767], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.605, 0.0, 0.0, 0.6666666666666666, 0.4120390289889091, 0.42771857074484926, 0.0, 1.0, 0.21013322243136984], 
reward next is 0.7899, 
noisyNet noise sample is [array([-1.129048], dtype=float32), 0.60960025]. 
=============================================
[2019-04-04 12:26:05,391] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[60.373917]
 [60.37328 ]
 [60.409267]
 [60.44062 ]
 [60.470524]], R is [[60.53247452]
 [60.71720886]
 [60.90024567]
 [61.08153534]
 [61.26102829]].
[2019-04-04 12:26:07,952] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6496511e-07 7.7761321e-08 6.0914594e-12 2.2947672e-10 9.9999952e-01
 1.0440006e-07 1.6757035e-11], sum to 1.0000
[2019-04-04 12:26:07,953] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6271
[2019-04-04 12:26:07,967] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.39240033423563, -0.1097425288081803, 0.0, 1.0, 44407.42444249868], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2436600.0000, 
sim time next is 2437200.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.35788628755606, -0.1141751998161646, 0.0, 1.0, 44379.89677315301], 
processed observation next is [0.0, 0.21739130434782608, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.446490523963005, 0.46194160006127843, 0.0, 1.0, 0.21133284177691908], 
reward next is 0.7887, 
noisyNet noise sample is [array([1.4125842], dtype=float32), -0.71354836]. 
=============================================
[2019-04-04 12:26:10,562] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.8625947e-08 6.4530230e-09 2.5142131e-13 5.0649583e-13 9.9999988e-01
 2.0084934e-09 7.8237472e-14], sum to 1.0000
[2019-04-04 12:26:10,563] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6999
[2019-04-04 12:26:10,580] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.95, 33.0, 0.0, 0.0, 26.0, 25.31341684961276, 0.2849895722664485, 0.0, 1.0, 40192.38418777019], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2493000.0000, 
sim time next is 2493600.0000, 
raw observation next is [-1.033333333333333, 34.33333333333333, 0.0, 0.0, 26.0, 25.31390910399884, 0.2828381323461586, 0.0, 1.0, 40064.83732233754], 
processed observation next is [0.0, 0.8695652173913043, 0.43397968605724846, 0.34333333333333327, 0.0, 0.0, 0.6666666666666666, 0.6094924253332366, 0.5942793774487195, 0.0, 1.0, 0.19078493963017876], 
reward next is 0.8092, 
noisyNet noise sample is [array([0.10800708], dtype=float32), -0.16347241]. 
=============================================
[2019-04-04 12:26:15,329] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5499288e-08 3.0361642e-09 8.7850076e-14 3.1030777e-12 1.0000000e+00
 9.0872265e-10 1.4907021e-13], sum to 1.0000
[2019-04-04 12:26:15,329] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2939
[2019-04-04 12:26:15,344] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 43.0, 0.0, 0.0, 26.0, 24.94857546907565, 0.178931114052672, 0.0, 1.0, 38639.71611160116], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2515800.0000, 
sim time next is 2516400.0000, 
raw observation next is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.92560882941622, 0.1766751921566819, 0.0, 1.0, 38626.28616648732], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.44, 0.0, 0.0, 0.6666666666666666, 0.5771340691180183, 0.558891730718894, 0.0, 1.0, 0.183934696030892], 
reward next is 0.8161, 
noisyNet noise sample is [array([2.3570302], dtype=float32), -0.96297556]. 
=============================================
[2019-04-04 12:26:19,888] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0328127e-11 4.3834901e-13 1.0540756e-18 1.7726209e-17 1.0000000e+00
 5.5729217e-13 5.7502241e-19], sum to 1.0000
[2019-04-04 12:26:19,889] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9937
[2019-04-04 12:26:19,906] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.666666666666667, 52.66666666666667, 88.66666666666667, 633.8333333333334, 26.0, 25.94745586327647, 0.5977084595102728, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2733600.0000, 
sim time next is 2734200.0000, 
raw observation next is [-3.5, 52.0, 86.0, 614.0, 26.0, 26.32949356463511, 0.631139328547249, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.36565096952908593, 0.52, 0.2866666666666667, 0.6784530386740332, 0.6666666666666666, 0.6941244637195926, 0.7103797761824163, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8091371], dtype=float32), 0.2581223]. 
=============================================
[2019-04-04 12:26:28,144] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1606260e-10 4.9234221e-11 1.6741858e-16 1.9629080e-15 1.0000000e+00
 2.3138109e-11 1.3834028e-16], sum to 1.0000
[2019-04-04 12:26:28,144] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4625
[2019-04-04 12:26:28,185] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.833333333333333, 63.16666666666666, 0.0, 0.0, 26.0, 25.35405072061497, 0.4592012076524208, 0.0, 1.0, 59921.58311726069], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2753400.0000, 
sim time next is 2754000.0000, 
raw observation next is [-6.0, 64.0, 0.0, 0.0, 26.0, 25.4107812517594, 0.4575849014605396, 0.0, 1.0, 33246.11428483505], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.64, 0.0, 0.0, 0.6666666666666666, 0.6175651043132833, 0.6525283004868465, 0.0, 1.0, 0.15831482992778595], 
reward next is 0.8417, 
noisyNet noise sample is [array([-0.73982275], dtype=float32), -0.73723716]. 
=============================================
[2019-04-04 12:26:28,202] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[86.74261 ]
 [87.25585 ]
 [87.19387 ]
 [87.568275]
 [87.814224]], R is [[86.14188385]
 [85.99512482]
 [85.78061676]
 [85.3878479 ]
 [84.67436218]].
[2019-04-04 12:26:44,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2007118e-08 1.7356055e-09 9.3712317e-14 1.0362079e-12 9.9999988e-01
 5.6143251e-10 8.5476222e-14], sum to 1.0000
[2019-04-04 12:26:44,577] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3976
[2019-04-04 12:26:44,593] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.75, 65.0, 0.0, 0.0, 26.0, 25.22241613077153, 0.3371485436748449, 0.0, 1.0, 39332.88959732042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3015000.0000, 
sim time next is 3015600.0000, 
raw observation next is [-3.833333333333333, 65.0, 0.0, 0.0, 26.0, 25.19161710240627, 0.3310810330604742, 0.0, 1.0, 39179.88505517791], 
processed observation next is [0.0, 0.9130434782608695, 0.3564173591874424, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5993014252005224, 0.6103603443534914, 0.0, 1.0, 0.1865708812151329], 
reward next is 0.8134, 
noisyNet noise sample is [array([-1.2567883], dtype=float32), 0.3924534]. 
=============================================
[2019-04-04 12:26:46,373] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1937925e-08 1.6051782e-10 4.8707884e-15 1.1208621e-14 1.0000000e+00
 1.0720647e-10 7.8902822e-15], sum to 1.0000
[2019-04-04 12:26:46,374] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0534
[2019-04-04 12:26:46,385] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.3714767508765, 0.3436797161328516, 0.0, 1.0, 53092.14301497051], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3102600.0000, 
sim time next is 3103200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.39565815745253, 0.3440185020273165, 0.0, 1.0, 34755.36802931406], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6163048464543776, 0.6146728340091056, 0.0, 1.0, 0.16550175252054314], 
reward next is 0.8345, 
noisyNet noise sample is [array([-1.0723108], dtype=float32), 1.0616994]. 
=============================================
[2019-04-04 12:26:47,548] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8696415e-13 9.5036067e-14 1.8999361e-20 2.2124577e-18 1.0000000e+00
 5.2498090e-15 9.5517344e-20], sum to 1.0000
[2019-04-04 12:26:47,548] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6744
[2019-04-04 12:26:47,557] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.166666666666667, 98.83333333333334, 112.6666666666667, 817.3333333333334, 26.0, 27.28389287518887, 0.8389320783393118, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3156600.0000, 
sim time next is 3157200.0000, 
raw observation next is [7.0, 100.0, 112.5, 814.5, 26.0, 27.27804787296546, 0.849433048651561, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.375, 0.9, 0.6666666666666666, 0.7731706560804549, 0.7831443495505203, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7858771], dtype=float32), 0.04980189]. 
=============================================
[2019-04-04 12:26:50,798] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8811544e-11 1.8974657e-13 8.5525176e-21 3.5067112e-19 1.0000000e+00
 3.2142054e-14 7.7325458e-20], sum to 1.0000
[2019-04-04 12:26:50,799] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2486
[2019-04-04 12:26:50,823] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 80.33333333333333, 114.3333333333333, 821.1666666666667, 26.0, 26.5905556085762, 0.7463334357058001, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3242400.0000, 
sim time next is 3243000.0000, 
raw observation next is [-2.0, 82.66666666666667, 113.6666666666667, 819.3333333333334, 26.0, 26.534514252136, 0.6213856472712118, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.8266666666666667, 0.378888888888889, 0.905340699815838, 0.6666666666666666, 0.7112095210113333, 0.7071285490904039, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.99787885], dtype=float32), -0.032861765]. 
=============================================
[2019-04-04 12:26:50,838] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[105.34465 ]
 [105.21571 ]
 [105.06639 ]
 [104.917015]
 [104.722824]], R is [[105.37822723]
 [105.32444763]
 [105.27120209]
 [105.2184906 ]
 [105.16630554]].
[2019-04-04 12:26:52,956] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.8856643e-11 1.0655849e-12 4.2141998e-18 2.8349101e-17 1.0000000e+00
 2.1444667e-13 9.8029019e-19], sum to 1.0000
[2019-04-04 12:26:52,957] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8976
[2019-04-04 12:26:52,979] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 100.0, 93.66666666666667, 562.0, 26.0, 26.29283189447042, 0.5392973739928885, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3143400.0000, 
sim time next is 3144000.0000, 
raw observation next is [7.0, 100.0, 96.33333333333333, 604.5, 26.0, 26.37749328642696, 0.5571459455816169, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.32111111111111107, 0.6679558011049723, 0.6666666666666666, 0.6981244405355801, 0.6857153151938723, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7420678], dtype=float32), 0.747035]. 
=============================================
[2019-04-04 12:26:52,999] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[97.93638 ]
 [97.166695]
 [96.25034 ]
 [95.416664]
 [94.358765]], R is [[98.62785339]
 [98.64157867]
 [98.65516663]
 [98.66861725]
 [98.68193054]].
[2019-04-04 12:26:57,689] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5037131e-10 1.9339291e-11 6.3150483e-19 5.2222797e-17 1.0000000e+00
 1.2000636e-12 2.3849660e-18], sum to 1.0000
[2019-04-04 12:26:57,689] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5604
[2019-04-04 12:26:57,712] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 54.0, 116.0, 805.5, 26.0, 25.94003067386038, 0.5578688918527934, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3330000.0000, 
sim time next is 3330600.0000, 
raw observation next is [-4.833333333333334, 53.33333333333333, 115.3333333333333, 803.6666666666666, 26.0, 25.95098779536426, 0.5577315783333939, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.32871652816251157, 0.5333333333333333, 0.3844444444444443, 0.8880294659300184, 0.6666666666666666, 0.6625823162803549, 0.6859105261111313, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05798423], dtype=float32), 0.6532107]. 
=============================================
[2019-04-04 12:26:58,065] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.4874223e-09 3.8128070e-11 1.4310903e-15 2.8960696e-15 1.0000000e+00
 3.8781280e-11 2.8338317e-15], sum to 1.0000
[2019-04-04 12:26:58,067] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2867
[2019-04-04 12:26:58,081] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.51951990338133, 0.5110927147788166, 0.0, 1.0, 45266.67097314513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3210600.0000, 
sim time next is 3211200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.3990028488166, 0.5059741711087019, 0.0, 1.0, 105845.9555555344], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6165835707347167, 0.668658057036234, 0.0, 1.0, 0.504028359788259], 
reward next is 0.4960, 
noisyNet noise sample is [array([0.31624657], dtype=float32), 0.05268687]. 
=============================================
[2019-04-04 12:26:58,257] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9105761e-09 3.6438261e-10 1.0983186e-15 1.2854480e-13 1.0000000e+00
 9.6021940e-11 1.5863826e-15], sum to 1.0000
[2019-04-04 12:26:58,257] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8576
[2019-04-04 12:26:58,274] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83074054190701, 0.2871864921064027, 0.0, 1.0, 41104.55487610382], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3379800.0000, 
sim time next is 3380400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83946724713513, 0.2829026761199563, 0.0, 1.0, 41086.01846527406], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5699556039279274, 0.5943008920399854, 0.0, 1.0, 0.1956477069774955], 
reward next is 0.8044, 
noisyNet noise sample is [array([-1.0745693], dtype=float32), 0.6597777]. 
=============================================
[2019-04-04 12:26:58,644] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7932564e-08 4.7741316e-10 1.9753745e-14 2.4288731e-13 1.0000000e+00
 1.3245716e-09 2.1465199e-14], sum to 1.0000
[2019-04-04 12:26:58,646] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9354
[2019-04-04 12:26:58,778] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.0, 77.33333333333334, 0.0, 0.0, 26.0, 23.86391696277907, 0.07456777232398827, 0.0, 1.0, 44038.68543911678], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3309000.0000, 
sim time next is 3309600.0000, 
raw observation next is [-11.0, 78.66666666666667, 0.0, 0.0, 26.0, 23.79699698727741, 0.1495091822105632, 1.0, 1.0, 202374.3300765972], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.4830830822731175, 0.5498363940701877, 1.0, 1.0, 0.9636872860790343], 
reward next is 0.0363, 
noisyNet noise sample is [array([-1.9856821], dtype=float32), 0.84747916]. 
=============================================
[2019-04-04 12:27:01,699] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7662019e-11 7.7591700e-12 2.1452466e-18 4.5169012e-18 1.0000000e+00
 2.0375804e-12 6.8812773e-19], sum to 1.0000
[2019-04-04 12:27:01,700] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4583
[2019-04-04 12:27:01,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1159316e-10 1.2661769e-11 9.2604813e-18 7.6538493e-17 1.0000000e+00
 8.4425080e-12 5.8816320e-17], sum to 1.0000
[2019-04-04 12:27:01,705] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1988
[2019-04-04 12:27:01,715] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 67.0, 0.0, 0.0, 26.0, 25.7325334613086, 0.4904622474046008, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3434400.0000, 
sim time next is 3435000.0000, 
raw observation next is [1.833333333333333, 69.0, 0.0, 0.0, 26.0, 25.36169856701655, 0.4460758192393652, 1.0, 1.0, 97230.68266347302], 
processed observation next is [1.0, 0.782608695652174, 0.5133887349953832, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6134748805847124, 0.6486919397464551, 1.0, 1.0, 0.46300325077844295], 
reward next is 0.5370, 
noisyNet noise sample is [array([-0.06783907], dtype=float32), 0.7399539]. 
=============================================
[2019-04-04 12:27:01,723] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[95.08272]
 [95.48231]
 [95.92154]
 [96.46599]
 [96.90889]], R is [[94.37902832]
 [94.4352417 ]
 [94.4908905 ]
 [94.54598236]
 [94.6005249 ]].
[2019-04-04 12:27:01,724] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.29347508935856, 0.5364012383232054, 0.0, 1.0, 62807.09586727117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3444600.0000, 
sim time next is 3445200.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.43105655502962, 0.5524756431992777, 0.0, 1.0, 18762.25063290586], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.619254712919135, 0.6841585477330926, 0.0, 1.0, 0.08934405063288504], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.2242213], dtype=float32), 0.84306055]. 
=============================================
[2019-04-04 12:27:04,804] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8200879e-11 4.6113368e-13 1.6964149e-18 1.4746083e-17 1.0000000e+00
 2.3698177e-13 1.3721384e-18], sum to 1.0000
[2019-04-04 12:27:04,807] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6838
[2019-04-04 12:27:04,844] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.833333333333333, 57.66666666666666, 115.3333333333333, 814.3333333333334, 26.0, 26.31061279744833, 0.5218988906756468, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3498600.0000, 
sim time next is 3499200.0000, 
raw observation next is [2.0, 57.0, 115.5, 816.5, 26.0, 25.85578202939934, 0.5631467461112635, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.57, 0.385, 0.9022099447513812, 0.6666666666666666, 0.654648502449945, 0.6877155820370878, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0314996], dtype=float32), -1.993636]. 
=============================================
[2019-04-04 12:27:08,314] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.3527931e-09 7.3747697e-10 1.1513229e-14 4.7443295e-14 1.0000000e+00
 1.3218912e-09 3.5622147e-14], sum to 1.0000
[2019-04-04 12:27:08,316] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6221
[2019-04-04 12:27:08,323] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 42.0, 94.0, 743.5, 26.0, 25.20328106518236, 0.4507142378542778, 0.0, 1.0, 18693.50687736228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3596400.0000, 
sim time next is 3597000.0000, 
raw observation next is [-0.8333333333333334, 42.16666666666666, 91.33333333333334, 728.6666666666667, 26.0, 25.19355906062094, 0.4565759758439992, 0.0, 1.0, 18693.64280335203], 
processed observation next is [0.0, 0.6521739130434783, 0.43951985226223456, 0.4216666666666666, 0.30444444444444446, 0.8051565377532229, 0.6666666666666666, 0.5994632550517451, 0.6521919919479998, 0.0, 1.0, 0.08901734668262871], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.6869006], dtype=float32), -0.49384502]. 
=============================================
[2019-04-04 12:27:08,334] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[77.39445 ]
 [77.522156]
 [77.65956 ]
 [77.77372 ]
 [77.812294]], R is [[77.41271973]
 [77.54957581]
 [77.77407837]
 [77.99633789]
 [78.12734222]].
[2019-04-04 12:27:10,541] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.0051248e-09 9.0505559e-10 9.9024462e-15 4.0223304e-14 1.0000000e+00
 1.9583730e-10 2.9624077e-15], sum to 1.0000
[2019-04-04 12:27:10,541] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4015
[2019-04-04 12:27:10,552] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 44.33333333333334, 105.5, 783.0, 26.0, 25.38536998095936, 0.4743965485065265, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3680400.0000, 
sim time next is 3681000.0000, 
raw observation next is [6.0, 45.0, 104.0, 776.0, 26.0, 25.43184594813372, 0.4840301588072793, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.45, 0.3466666666666667, 0.8574585635359117, 0.6666666666666666, 0.61932049567781, 0.6613433862690931, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54671556], dtype=float32), -0.39509133]. 
=============================================
[2019-04-04 12:27:10,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[79.94501 ]
 [79.91855 ]
 [79.85813 ]
 [79.780945]
 [79.741974]], R is [[80.17884827]
 [80.37705994]
 [80.57328796]
 [80.76755524]
 [80.95987701]].
[2019-04-04 12:27:11,409] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1388797e-07 2.6437077e-09 1.4674900e-13 3.2075311e-13 9.9999988e-01
 1.2761722e-09 1.1151476e-13], sum to 1.0000
[2019-04-04 12:27:11,410] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1394
[2019-04-04 12:27:11,421] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 43.0, 74.5, 607.0, 26.0, 25.33882690286542, 0.4605474376399811, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3600000.0000, 
sim time next is 3600600.0000, 
raw observation next is [0.0, 42.33333333333334, 70.66666666666667, 576.3333333333333, 26.0, 25.32340037463254, 0.453173526146937, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.42333333333333345, 0.23555555555555557, 0.6368324125230201, 0.6666666666666666, 0.6102833645527118, 0.651057842048979, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18034571], dtype=float32), -0.17494556]. 
=============================================
[2019-04-04 12:27:14,618] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1038398e-07 5.8678733e-08 1.3691666e-12 3.4481300e-12 9.9999940e-01
 1.6093789e-08 2.4868798e-12], sum to 1.0000
[2019-04-04 12:27:14,625] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5104
[2019-04-04 12:27:14,636] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.666666666666668, 27.66666666666667, 0.0, 0.0, 26.0, 25.46150998892444, 0.3598687347482199, 0.0, 1.0, 28093.68081547945], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3645600.0000, 
sim time next is 3646200.0000, 
raw observation next is [8.833333333333332, 27.33333333333333, 0.0, 0.0, 26.0, 25.474991009566, 0.364500036260835, 0.0, 1.0, 22430.77675451072], 
processed observation next is [0.0, 0.17391304347826086, 0.7072945521698984, 0.27333333333333326, 0.0, 0.0, 0.6666666666666666, 0.6229159174638333, 0.621500012086945, 0.0, 1.0, 0.10681322264052724], 
reward next is 0.8932, 
noisyNet noise sample is [array([-0.6733708], dtype=float32), -2.1796205]. 
=============================================
[2019-04-04 12:27:20,382] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0010742e-08 3.4383302e-10 4.8982530e-15 4.3105676e-14 1.0000000e+00
 1.3405833e-10 1.9254165e-14], sum to 1.0000
[2019-04-04 12:27:20,388] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8321
[2019-04-04 12:27:20,414] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.1232277604992, 0.2812800354424793, 0.0, 1.0, 41589.77581368232], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3735600.0000, 
sim time next is 3736200.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.08443231704625, 0.2819327868407913, 0.0, 1.0, 41667.75376687815], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5903693597538542, 0.5939775956135971, 0.0, 1.0, 0.19841787508037215], 
reward next is 0.8016, 
noisyNet noise sample is [array([-0.52570003], dtype=float32), 0.12170685]. 
=============================================
[2019-04-04 12:27:21,372] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.7094828e-09 2.6579356e-09 4.5892821e-15 5.7486818e-14 1.0000000e+00
 2.4056077e-10 5.0126330e-15], sum to 1.0000
[2019-04-04 12:27:21,377] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7087
[2019-04-04 12:27:21,390] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.06436468155176, 0.3399916919053947, 0.0, 1.0, 43786.81776357325], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3813600.0000, 
sim time next is 3814200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.09936295388381, 0.3339184951052963, 0.0, 1.0, 43623.3982939426], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5916135794903173, 0.6113061650350987, 0.0, 1.0, 0.20773046806639334], 
reward next is 0.7923, 
noisyNet noise sample is [array([0.35011652], dtype=float32), 0.20065254]. 
=============================================
[2019-04-04 12:27:28,415] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.10571285e-08 4.10663531e-10 9.86507722e-16 2.83579204e-13
 1.00000000e+00 2.84870405e-10 2.89007684e-15], sum to 1.0000
[2019-04-04 12:27:28,416] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9332
[2019-04-04 12:27:28,461] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.0, 53.0, 97.0, 571.0, 26.0, 26.28359166495021, 0.5030510463026306, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4006800.0000, 
sim time next is 4007400.0000, 
raw observation next is [-10.66666666666667, 51.50000000000001, 98.33333333333334, 613.3333333333334, 26.0, 26.36439213961834, 0.5175667633386015, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.16712834718374878, 0.5150000000000001, 0.32777777777777783, 0.6777163904235728, 0.6666666666666666, 0.6970326783015283, 0.6725222544462005, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5062082], dtype=float32), -0.54588693]. 
=============================================
[2019-04-04 12:27:34,815] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1758868e-10 3.3548173e-12 9.2906991e-18 1.2842796e-16 1.0000000e+00
 8.9415732e-12 3.2403120e-17], sum to 1.0000
[2019-04-04 12:27:34,816] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1114
[2019-04-04 12:27:34,825] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 33.0, 118.0, 841.0, 26.0, 26.46963137848415, 0.5798192819271496, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4019400.0000, 
sim time next is 4020000.0000, 
raw observation next is [-4.666666666666666, 31.66666666666666, 117.3333333333333, 839.1666666666667, 26.0, 26.3240205562903, 0.5801796099292243, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.33333333333333337, 0.3166666666666666, 0.391111111111111, 0.9272559852670351, 0.6666666666666666, 0.6936683796908584, 0.6933932033097414, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34570336], dtype=float32), 0.16730389]. 
=============================================
[2019-04-04 12:27:34,842] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[91.87382]
 [91.73668]
 [91.58393]
 [91.3791 ]
 [91.12789]], R is [[92.05690765]
 [92.13633728]
 [92.21497345]
 [92.29282379]
 [92.36989594]].
[2019-04-04 12:27:35,068] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.8862489e-09 2.4840394e-10 2.5818479e-15 9.1286746e-14 1.0000000e+00
 2.7052546e-10 4.1847303e-15], sum to 1.0000
[2019-04-04 12:27:35,071] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2600
[2019-04-04 12:27:35,082] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.666666666666667, 35.0, 0.0, 0.0, 26.0, 25.29380036371093, 0.4072617613450121, 0.0, 1.0, 61667.82756049067], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4056000.0000, 
sim time next is 4056600.0000, 
raw observation next is [-5.833333333333333, 36.0, 0.0, 0.0, 26.0, 25.25156059045041, 0.4008627296100863, 0.0, 1.0, 48155.69595108793], 
processed observation next is [1.0, 0.9565217391304348, 0.30101569713758086, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6042967158708675, 0.6336209098700287, 0.0, 1.0, 0.22931283786232348], 
reward next is 0.7707, 
noisyNet noise sample is [array([0.21653141], dtype=float32), -1.3046193]. 
=============================================
[2019-04-04 12:27:36,056] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0396592e-11 1.5589272e-12 1.3650798e-18 8.1610069e-18 1.0000000e+00
 6.4310785e-13 5.3411026e-19], sum to 1.0000
[2019-04-04 12:27:36,057] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0401
[2019-04-04 12:27:36,090] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.666666666666667, 35.66666666666667, 93.16666666666666, 480.0, 26.0, 27.35460685273068, 0.571860824764417, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4119600.0000, 
sim time next is 4120200.0000, 
raw observation next is [3.5, 36.0, 93.0, 437.0, 26.0, 27.30280359010295, 0.7847012632251958, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5595567867036012, 0.36, 0.31, 0.48287292817679556, 0.6666666666666666, 0.7752336325085792, 0.761567087741732, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16101834], dtype=float32), 1.0491052]. 
=============================================
[2019-04-04 12:27:41,013] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9944031e-08 2.2060428e-08 5.3123451e-13 5.2723845e-12 9.9999988e-01
 2.7869167e-09 2.1794682e-12], sum to 1.0000
[2019-04-04 12:27:41,014] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1304
[2019-04-04 12:27:41,032] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 46.33333333333334, 0.0, 0.0, 26.0, 25.39470834436528, 0.3304115210724056, 0.0, 1.0, 32302.61770636818], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4252800.0000, 
sim time next is 4253400.0000, 
raw observation next is [3.0, 47.0, 0.0, 0.0, 26.0, 25.38566436943594, 0.327652766659992, 0.0, 1.0, 40794.57620846788], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6154720307863283, 0.609217588886664, 0.0, 1.0, 0.1942598867069899], 
reward next is 0.8057, 
noisyNet noise sample is [array([0.8745492], dtype=float32), 0.6745137]. 
=============================================
[2019-04-04 12:27:42,095] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5294988e-08 1.9679935e-09 8.4300127e-14 3.9794158e-13 1.0000000e+00
 1.8829610e-10 9.5582518e-14], sum to 1.0000
[2019-04-04 12:27:42,099] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6769
[2019-04-04 12:27:42,141] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.833333333333333, 35.0, 112.0, 736.0, 26.0, 25.31988324465707, 0.395644441521649, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4183800.0000, 
sim time next is 4184400.0000, 
raw observation next is [-1.666666666666667, 35.0, 113.0, 755.0, 26.0, 25.25568473955314, 0.3957673651773551, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.4164358264081256, 0.35, 0.37666666666666665, 0.8342541436464088, 0.6666666666666666, 0.6046403949627616, 0.6319224550591184, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8483119], dtype=float32), -0.23003459]. 
=============================================
[2019-04-04 12:27:48,755] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0351497e-11 9.1231796e-13 9.6674294e-19 5.1134800e-18 1.0000000e+00
 1.6737349e-13 8.4774934e-19], sum to 1.0000
[2019-04-04 12:27:48,759] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9177
[2019-04-04 12:27:48,769] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.0, 58.0, 0.0, 0.0, 26.0, 27.06482059390569, 0.8791423917319307, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4392000.0000, 
sim time next is 4392600.0000, 
raw observation next is [10.86666666666667, 58.16666666666667, 0.0, 0.0, 26.0, 27.01449172607926, 0.8674452751129643, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7636195752539245, 0.5816666666666667, 0.0, 0.0, 0.6666666666666666, 0.7512076438399383, 0.7891484250376548, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.51744753], dtype=float32), -2.539577]. 
=============================================
[2019-04-04 12:27:49,321] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.3973766e-12 2.0567510e-13 3.3264513e-19 2.7302391e-18 1.0000000e+00
 1.2292013e-13 1.8543107e-20], sum to 1.0000
[2019-04-04 12:27:49,324] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0300
[2019-04-04 12:27:49,336] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.53333333333333, 30.33333333333333, 128.3333333333333, 806.5, 26.0, 28.19715874487028, 1.078779542597791, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4369200.0000, 
sim time next is 4369800.0000, 
raw observation next is [14.51666666666667, 30.66666666666667, 141.6666666666667, 771.0, 26.0, 28.37464325963791, 1.107415760905866, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8647276084949217, 0.3066666666666667, 0.4722222222222224, 0.8519337016574585, 0.6666666666666666, 0.8645536049698258, 0.869138586968622, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15453751], dtype=float32), -1.1410735]. 
=============================================
[2019-04-04 12:27:55,771] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.87038568e-10 9.47771202e-12 1.33450742e-16 3.03876204e-15
 1.00000000e+00 6.04914426e-12 1.08075415e-16], sum to 1.0000
[2019-04-04 12:27:55,774] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8884
[2019-04-04 12:27:55,789] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.57208939749144, 0.5705090155798861, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4483200.0000, 
sim time next is 4483800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.60483471126236, 0.5650252662310886, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.63373622593853, 0.6883417554103629, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.83544356], dtype=float32), 0.11396731]. 
=============================================
[2019-04-04 12:27:55,895] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.1644747e-10 7.1176384e-11 7.6377236e-16 2.3638755e-14 1.0000000e+00
 1.1931106e-10 2.1097834e-16], sum to 1.0000
[2019-04-04 12:27:55,897] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0647
[2019-04-04 12:27:55,913] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.9333333333333333, 73.0, 0.0, 0.0, 26.0, 25.38752116881196, 0.4350205114954058, 0.0, 1.0, 46905.38052353438], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4506000.0000, 
sim time next is 4506600.0000, 
raw observation next is [-0.9166666666666666, 73.0, 0.0, 0.0, 26.0, 25.37347519150806, 0.4342016972742398, 0.0, 1.0, 50373.89954506155], 
processed observation next is [1.0, 0.13043478260869565, 0.4372114496768237, 0.73, 0.0, 0.0, 0.6666666666666666, 0.614456265959005, 0.6447338990914132, 0.0, 1.0, 0.2398757121193407], 
reward next is 0.7601, 
noisyNet noise sample is [array([0.28641135], dtype=float32), 1.1432874]. 
=============================================
[2019-04-04 12:27:58,144] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9270828e-10 5.0174426e-12 6.0222566e-18 3.9194765e-17 1.0000000e+00
 6.6307602e-12 3.8454125e-17], sum to 1.0000
[2019-04-04 12:27:58,146] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6259
[2019-04-04 12:27:58,162] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.0, 23.33333333333334, 23.33333333333334, 26.0, 25.74745752754892, 0.4956400833567667, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4556400.0000, 
sim time next is 4557000.0000, 
raw observation next is [2.0, 52.0, 18.66666666666667, 18.66666666666667, 26.0, 25.56825878088381, 0.3454249106553425, 1.0, 1.0, 27884.03370244092], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.06222222222222224, 0.02062615101289135, 0.6666666666666666, 0.6306882317403174, 0.6151416368851141, 1.0, 1.0, 0.1327811128687663], 
reward next is 0.8672, 
noisyNet noise sample is [array([0.89294857], dtype=float32), -0.09518093]. 
=============================================
[2019-04-04 12:27:58,184] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[94.0519  ]
 [94.451836]
 [94.99181 ]
 [95.44008 ]
 [95.88744 ]], R is [[93.52993774]
 [93.59464264]
 [93.65869904]
 [93.72211456]
 [93.78489685]].
[2019-04-04 12:28:03,423] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7535081e-09 1.8191378e-10 1.8905990e-16 1.0505853e-14 1.0000000e+00
 2.8905680e-11 7.4923971e-16], sum to 1.0000
[2019-04-04 12:28:03,424] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9581
[2019-04-04 12:28:03,445] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.833333333333333, 57.66666666666666, 0.0, 0.0, 26.0, 25.55670037615043, 0.562428572669513, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4569000.0000, 
sim time next is 4569600.0000, 
raw observation next is [1.666666666666667, 58.33333333333334, 0.0, 0.0, 26.0, 25.68615312376028, 0.5658845560805047, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5087719298245615, 0.5833333333333335, 0.0, 0.0, 0.6666666666666666, 0.6405127603133568, 0.6886281853601682, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0233867], dtype=float32), -1.9360257]. 
=============================================
[2019-04-04 12:28:05,041] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.6006090e-08 2.4748661e-09 2.9834821e-13 1.4317504e-12 1.0000000e+00
 1.9736397e-09 1.6991653e-13], sum to 1.0000
[2019-04-04 12:28:05,045] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8964
[2019-04-04 12:28:05,074] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.22780227804212, 0.167442517512279, 0.0, 1.0, 41455.90488554393], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4773000.0000, 
sim time next is 4773600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.17989806433922, 0.1583427353332268, 0.0, 1.0, 41524.06762741193], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5149915053616017, 0.5527809117777422, 0.0, 1.0, 0.19773365536862825], 
reward next is 0.8023, 
noisyNet noise sample is [array([-0.82533085], dtype=float32), -1.045291]. 
=============================================
[2019-04-04 12:28:06,750] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5679398e-11 1.1783566e-13 1.5626132e-19 8.4846754e-19 1.0000000e+00
 1.0515917e-14 9.1838555e-19], sum to 1.0000
[2019-04-04 12:28:06,754] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2053
[2019-04-04 12:28:06,767] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.95, 49.83333333333334, 200.3333333333333, 442.3333333333334, 26.0, 27.20459239108473, 0.8552530069985483, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4632600.0000, 
sim time next is 4633200.0000, 
raw observation next is [5.0, 50.0, 199.0, 364.0, 26.0, 27.33526657649114, 0.8744631594101889, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6011080332409973, 0.5, 0.6633333333333333, 0.4022099447513812, 0.6666666666666666, 0.7779388813742617, 0.7914877198033964, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.45235923], dtype=float32), 0.2382418]. 
=============================================
[2019-04-04 12:28:07,615] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7886854e-09 1.5677312e-10 1.2213624e-14 1.5916266e-14 1.0000000e+00
 1.9057819e-10 4.0171914e-15], sum to 1.0000
[2019-04-04 12:28:07,616] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5409
[2019-04-04 12:28:07,639] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 26.0, 25.61653559418387, 0.485542799813776, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4675800.0000, 
sim time next is 4676400.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 26.0, 25.64533620494221, 0.48624233769371, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.518005540166205, 0.62, 0.0, 0.0, 0.6666666666666666, 0.637111350411851, 0.6620807792312366, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8518819], dtype=float32), -1.0178035]. 
=============================================
[2019-04-04 12:28:12,141] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 12:28:12,142] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 12:28:12,143] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:28:12,143] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 12:28:12,144] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 12:28:12,145] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:28:12,145] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:28:12,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run5
[2019-04-04 12:28:12,168] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run5
[2019-04-04 12:28:12,170] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run5
[2019-04-04 12:28:23,526] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1476357], dtype=float32), 0.16238075]
[2019-04-04 12:28:23,527] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.8, 65.0, 160.0, 313.0, 26.0, 25.07731473453638, 0.4333001825557384, 1.0, 1.0, 0.0]
[2019-04-04 12:28:23,527] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 12:28:23,528] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.1719530e-09 6.0629855e-11 2.0051144e-16 3.3392737e-15 1.0000000e+00
 1.5516217e-11 4.1496290e-16], sampled 0.04638383935439938
[2019-04-04 12:28:54,807] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.1476357], dtype=float32), 0.16238075]
[2019-04-04 12:28:54,807] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [9.8, 60.33333333333334, 0.0, 0.0, 26.0, 26.42479560322105, 0.6813316450584143, 1.0, 1.0, 0.0]
[2019-04-04 12:28:54,807] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 12:28:54,808] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.0717336e-10 2.3909002e-11 9.9811682e-17 1.1538020e-15 1.0000000e+00
 9.6268176e-12 1.3283973e-16], sampled 0.3193692294630057
[2019-04-04 12:29:26,070] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1476357], dtype=float32), 0.16238075]
[2019-04-04 12:29:26,071] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [6.550000000000001, 46.5, 0.0, 0.0, 26.0, 24.77692476881712, 0.2055921967881344, 0.0, 1.0, 22110.3593842661]
[2019-04-04 12:29:26,071] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 12:29:26,071] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.8446492e-08 1.5681416e-09 4.2473258e-14 2.4980804e-13 1.0000000e+00
 7.1243522e-10 4.7949651e-14], sampled 0.8187076628247021
[2019-04-04 12:29:54,683] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4172 239942393.0563 1605.0250
[2019-04-04 12:30:12,948] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.8201 263377775.6296 1552.0399
[2019-04-04 12:30:18,185] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 12:30:19,208] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 400000, evaluation results [400000.0, 7241.820116049566, 263377775.62958866, 1552.0399168798824, 7353.417175922201, 239942393.05634084, 1605.024971488376, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 12:30:20,061] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7452725e-09 1.4354206e-11 7.1690883e-18 7.3959860e-17 1.0000000e+00
 6.4316703e-12 1.5698920e-17], sum to 1.0000
[2019-04-04 12:30:20,061] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9814
[2019-04-04 12:30:20,102] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 92.0, 80.33333333333333, 0.0, 26.0, 26.17307037818779, 0.5222531394020781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4697400.0000, 
sim time next is 4698000.0000, 
raw observation next is [0.0, 92.0, 89.0, 0.0, 26.0, 26.14583952271615, 0.5242907502137305, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.2966666666666667, 0.0, 0.6666666666666666, 0.6788199602263459, 0.6747635834045768, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47233352], dtype=float32), 2.1022303]. 
=============================================
[2019-04-04 12:30:20,106] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[91.9611  ]
 [91.75769 ]
 [91.427246]
 [90.53066 ]
 [90.189   ]], R is [[92.60251617]
 [92.67649078]
 [92.74972534]
 [92.82222748]
 [92.89400482]].
[2019-04-04 12:30:23,824] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.32621677e-10 4.37042104e-11 6.04348108e-16 1.27023636e-14
 1.00000000e+00 1.71106688e-11 7.23276881e-16], sum to 1.0000
[2019-04-04 12:30:23,826] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1038
[2019-04-04 12:30:23,852] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 43.0, 146.0, 813.0, 26.0, 25.02686819274999, 0.419692794635402, 0.0, 1.0, 9353.685524341305], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4795200.0000, 
sim time next is 4795800.0000, 
raw observation next is [1.166666666666667, 42.5, 154.0, 804.3333333333334, 26.0, 25.05688904556877, 0.4251975829034005, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.49492151431209613, 0.425, 0.5133333333333333, 0.8887661141804789, 0.6666666666666666, 0.5880740871307308, 0.6417325276344669, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5479394], dtype=float32), -0.70221]. 
=============================================
[2019-04-04 12:30:28,384] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3178538e-08 7.9657730e-10 3.1408144e-14 5.7920151e-13 1.0000000e+00
 1.6599532e-09 1.2802998e-13], sum to 1.0000
[2019-04-04 12:30:28,385] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6643
[2019-04-04 12:30:28,401] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 39.0, 0.0, 0.0, 26.0, 25.40511363103611, 0.3507681245216811, 0.0, 1.0, 47814.03414213785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4921200.0000, 
sim time next is 4921800.0000, 
raw observation next is [0.1666666666666667, 39.16666666666666, 0.0, 0.0, 26.0, 25.38287685521041, 0.3487681049673995, 0.0, 1.0, 52741.75284248315], 
processed observation next is [0.0, 1.0, 0.4672206832871654, 0.39166666666666655, 0.0, 0.0, 0.6666666666666666, 0.6152397379342007, 0.6162560349891332, 0.0, 1.0, 0.2511512040118245], 
reward next is 0.7488, 
noisyNet noise sample is [array([0.13098198], dtype=float32), 0.23252897]. 
=============================================
[2019-04-04 12:30:29,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:29,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:29,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run4
[2019-04-04 12:30:29,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:29,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:30,002] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run4
[2019-04-04 12:30:30,434] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3031252e-10 2.0399641e-12 6.8358056e-19 4.6584347e-18 1.0000000e+00
 2.2940252e-13 9.3184512e-19], sum to 1.0000
[2019-04-04 12:30:30,436] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2512
[2019-04-04 12:30:30,455] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.83333333333333, 17.33333333333334, 0.0, 0.0, 26.0, 27.95629908040073, 1.013296757121352, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5080200.0000, 
sim time next is 5080800.0000, 
raw observation next is [10.66666666666667, 17.66666666666667, 0.0, 0.0, 26.0, 27.86694085280913, 0.9950934045529004, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7580794090489382, 0.17666666666666672, 0.0, 0.0, 0.6666666666666666, 0.8222450710674275, 0.8316978015176334, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6353728], dtype=float32), -1.0555795]. 
=============================================
[2019-04-04 12:30:31,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:31,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:31,079] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run4
[2019-04-04 12:30:31,838] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:31,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:31,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run4
[2019-04-04 12:30:31,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:31,982] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:31,990] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run4
[2019-04-04 12:30:32,190] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.2842264e-08 6.6601091e-10 4.0173922e-14 4.6327677e-13 9.9999988e-01
 8.3532176e-10 1.5867411e-13], sum to 1.0000
[2019-04-04 12:30:32,192] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6458
[2019-04-04 12:30:32,212] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 26.0, 25.39907418776829, 0.3469238801334301, 0.0, 1.0, 30917.06029845405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4924800.0000, 
sim time next is 4925400.0000, 
raw observation next is [0.8333333333333334, 40.5, 0.0, 0.0, 26.0, 25.41092139177171, 0.34262027524564, 0.0, 1.0, 27865.68671128015], 
processed observation next is [1.0, 0.0, 0.4856879039704525, 0.405, 0.0, 0.0, 0.6666666666666666, 0.6175767826476424, 0.6142067584152133, 0.0, 1.0, 0.13269374624419117], 
reward next is 0.8673, 
noisyNet noise sample is [array([-2.259073], dtype=float32), 0.03605432]. 
=============================================
[2019-04-04 12:30:33,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:33,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:33,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run4
[2019-04-04 12:30:33,094] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:33,095] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:33,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run4
[2019-04-04 12:30:33,692] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:33,692] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:33,695] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run4
[2019-04-04 12:30:34,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:34,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:34,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run4
[2019-04-04 12:30:34,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:34,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:34,983] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run4
[2019-04-04 12:30:35,608] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.2630567e-10 1.9649272e-11 1.0558192e-16 1.1371626e-15 1.0000000e+00
 1.0165095e-11 4.4715237e-17], sum to 1.0000
[2019-04-04 12:30:35,608] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6869
[2019-04-04 12:30:35,626] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.666666666666667, 24.0, 0.0, 0.0, 26.0, 25.85689062906118, 0.5545265154636542, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4997400.0000, 
sim time next is 4998000.0000, 
raw observation next is [5.333333333333333, 25.0, 0.0, 0.0, 26.0, 25.8081700799536, 0.5410761249735404, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6103416435826409, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6506808399961335, 0.6803587083245134, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3283832], dtype=float32), -0.30391568]. 
=============================================
[2019-04-04 12:30:35,630] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.08459 ]
 [91.092026]
 [92.590996]
 [93.36616 ]
 [93.72988 ]], R is [[89.51397705]
 [89.61883545]
 [89.72264862]
 [89.82542419]
 [89.9271698 ]].
[2019-04-04 12:30:35,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:35,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:35,823] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run4
[2019-04-04 12:30:36,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:36,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:36,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run4
[2019-04-04 12:30:36,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:36,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:36,971] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run4
[2019-04-04 12:30:37,878] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:37,878] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:37,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run4
[2019-04-04 12:30:39,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:39,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:39,215] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run4
[2019-04-04 12:30:41,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:30:41,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:30:41,479] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run4
[2019-04-04 12:30:45,764] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8795973e-10 7.7928421e-11 9.0380385e-16 2.2533983e-14 1.0000000e+00
 6.6538177e-12 4.1604228e-16], sum to 1.0000
[2019-04-04 12:30:45,766] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3575
[2019-04-04 12:30:45,779] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.25, 95.0, 0.0, 0.0, 26.0, 24.39213911621281, 0.1688693156443171, 0.0, 1.0, 40076.75124200032], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 83400.0000, 
sim time next is 84000.0000, 
raw observation next is [0.2, 95.0, 0.0, 0.0, 26.0, 24.36964306940152, 0.165586871590494, 0.0, 1.0, 40071.38571505113], 
processed observation next is [0.0, 1.0, 0.46814404432132967, 0.95, 0.0, 0.0, 0.6666666666666666, 0.5308035891167933, 0.555195623863498, 0.0, 1.0, 0.19081612245262444], 
reward next is 0.8092, 
noisyNet noise sample is [array([-1.54611], dtype=float32), -0.44794536]. 
=============================================
[2019-04-04 12:30:45,782] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[80.183105]
 [80.1655  ]
 [80.14243 ]
 [80.20481 ]
 [80.37415 ]], R is [[80.16617584]
 [80.17367554]
 [80.18104553]
 [80.18826294]
 [80.19528961]].
[2019-04-04 12:30:53,175] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9301015e-08 8.9798330e-10 4.3846563e-15 2.0707089e-13 1.0000000e+00
 9.5077779e-10 4.8240055e-15], sum to 1.0000
[2019-04-04 12:30:53,176] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5074
[2019-04-04 12:30:53,203] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.816666666666666, 73.5, 0.0, 0.0, 26.0, 23.73680847979713, 0.002556047883900357, 0.0, 1.0, 44403.65249226236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 175800.0000, 
sim time next is 176400.0000, 
raw observation next is [-8.9, 74.0, 0.0, 0.0, 26.0, 23.72374621840878, -0.007473928235012649, 0.0, 1.0, 44413.18915655867], 
processed observation next is [1.0, 0.043478260869565216, 0.21606648199445982, 0.74, 0.0, 0.0, 0.6666666666666666, 0.4769788515340651, 0.4975086905883291, 0.0, 1.0, 0.21149137693599368], 
reward next is 0.7885, 
noisyNet noise sample is [array([0.0394739], dtype=float32), -1.4246212]. 
=============================================
[2019-04-04 12:30:53,594] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1749101e-08 9.4277619e-10 1.3232727e-14 1.0384293e-13 1.0000000e+00
 3.6337902e-10 1.1239792e-14], sum to 1.0000
[2019-04-04 12:30:53,596] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0664
[2019-04-04 12:30:53,629] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.566666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 23.5600392461222, -0.03155394954341485, 0.0, 1.0, 44363.38845143696], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 105600.0000, 
sim time next is 106200.0000, 
raw observation next is [-5.85, 74.5, 0.0, 0.0, 26.0, 23.47934792248926, -0.04008694746815211, 0.0, 1.0, 44524.63341139942], 
processed observation next is [1.0, 0.21739130434782608, 0.30055401662049863, 0.745, 0.0, 0.0, 0.6666666666666666, 0.4566123268741051, 0.48663768417728265, 0.0, 1.0, 0.2120220638638068], 
reward next is 0.7880, 
noisyNet noise sample is [array([-0.7744157], dtype=float32), 0.2072935]. 
=============================================
[2019-04-04 12:30:58,741] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1530613e-08 5.0327089e-09 6.3539811e-14 5.4966079e-13 1.0000000e+00
 2.6106910e-09 2.6792248e-14], sum to 1.0000
[2019-04-04 12:30:58,747] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2209
[2019-04-04 12:30:58,783] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 77.33333333333333, 0.0, 0.0, 26.0, 23.44703702574226, -0.08226595229892962, 0.0, 1.0, 44239.52451735205], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 183000.0000, 
sim time next is 183600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.40460480661837, -0.0921791583340894, 0.0, 1.0, 44036.39763170204], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.45038373388486413, 0.46927361388863686, 0.0, 1.0, 0.20969713157953354], 
reward next is 0.7903, 
noisyNet noise sample is [array([-0.9127029], dtype=float32), 0.32557926]. 
=============================================
[2019-04-04 12:30:59,530] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.6416939e-09 2.8649022e-11 7.3928019e-16 2.1289301e-14 1.0000000e+00
 3.3409497e-11 1.6643167e-15], sum to 1.0000
[2019-04-04 12:30:59,532] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9219
[2019-04-04 12:30:59,557] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.733333333333334, 73.0, 0.0, 0.0, 26.0, 23.77859350802425, 0.009257441673686561, 0.0, 1.0, 44402.75067551641], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 175200.0000, 
sim time next is 175800.0000, 
raw observation next is [-8.816666666666666, 73.5, 0.0, 0.0, 26.0, 23.73774409671352, 0.002785663986738555, 0.0, 1.0, 44403.1251142316], 
processed observation next is [1.0, 0.0, 0.21837488457987075, 0.735, 0.0, 0.0, 0.6666666666666666, 0.47814534139279335, 0.5009285546622462, 0.0, 1.0, 0.2114434529249124], 
reward next is 0.7886, 
noisyNet noise sample is [array([-0.7134564], dtype=float32), 0.4086574]. 
=============================================
[2019-04-04 12:31:01,892] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.0341202e-09 2.5767283e-10 3.4265159e-16 2.4960741e-14 1.0000000e+00
 9.4657934e-11 3.5400779e-15], sum to 1.0000
[2019-04-04 12:31:01,892] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6929
[2019-04-04 12:31:01,953] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.86242308897737, 0.2660351956345365, 0.0, 1.0, 23788.0956620114], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 245400.0000, 
sim time next is 246000.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.93505601127948, 0.2622584867109932, 0.0, 1.0, 56497.0693726192], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5779213342732902, 0.587419495570331, 0.0, 1.0, 0.26903366367913906], 
reward next is 0.7310, 
noisyNet noise sample is [array([-2.009628], dtype=float32), -1.7539146]. 
=============================================
[2019-04-04 12:31:01,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[89.907486]
 [90.28385 ]
 [91.06944 ]
 [91.3753  ]
 [90.68634 ]], R is [[89.35445404]
 [89.34763336]
 [88.99384308]
 [88.30422974]
 [87.98884583]].
[2019-04-04 12:31:03,206] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.9732172e-11 6.9410155e-12 1.9690856e-17 9.4960176e-17 1.0000000e+00
 2.7233288e-13 1.7689934e-17], sum to 1.0000
[2019-04-04 12:31:03,207] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8228
[2019-04-04 12:31:03,291] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 42.0, 76.0, 550.0, 26.0, 25.24815336145667, 0.4525637075971103, 1.0, 1.0, 112482.3136055023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 313200.0000, 
sim time next is 313800.0000, 
raw observation next is [-9.5, 42.0, 74.0, 525.6666666666667, 26.0, 25.7259598887908, 0.494695200248963, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.24666666666666667, 0.5808471454880295, 0.6666666666666666, 0.6438299907325667, 0.6648984000829877, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86625904], dtype=float32), -0.117788926]. 
=============================================
[2019-04-04 12:31:13,315] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.2968926e-10 1.6571581e-11 5.3108761e-17 4.6820297e-15 1.0000000e+00
 9.7576070e-12 6.6076769e-17], sum to 1.0000
[2019-04-04 12:31:13,315] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5390
[2019-04-04 12:31:13,375] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 36.66666666666667, 17.5, 338.6666666666667, 26.0, 26.15024625641731, 0.3372747407889681, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 405600.0000, 
sim time next is 406200.0000, 
raw observation next is [-8.9, 36.33333333333333, 14.0, 274.3333333333334, 26.0, 26.20380913538944, 0.4573660580199519, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.3633333333333333, 0.04666666666666667, 0.3031307550644568, 0.6666666666666666, 0.6836507612824535, 0.6524553526733173, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8118984], dtype=float32), 0.2841583]. 
=============================================
[2019-04-04 12:31:19,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6951084e-09 1.8285337e-10 4.0474458e-16 3.6485250e-15 1.0000000e+00
 1.1701186e-10 6.4993097e-16], sum to 1.0000
[2019-04-04 12:31:19,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8433
[2019-04-04 12:31:19,736] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.55, 35.5, 56.0, 0.0, 26.0, 25.73308164954186, 0.2851009661321788, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 487800.0000, 
sim time next is 488400.0000, 
raw observation next is [0.7333333333333334, 35.0, 50.00000000000001, 0.0, 26.0, 25.70738586120813, 0.2756723099314218, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4829178208679595, 0.35, 0.16666666666666669, 0.0, 0.6666666666666666, 0.6422821551006775, 0.5918907699771406, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5907066], dtype=float32), -0.49572265]. 
=============================================
[2019-04-04 12:31:22,869] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.2045233e-09 3.3391023e-11 6.2129083e-16 5.9551249e-15 1.0000000e+00
 1.8098236e-11 2.6860992e-16], sum to 1.0000
[2019-04-04 12:31:22,869] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6705
[2019-04-04 12:31:22,927] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 35.33333333333334, 102.3333333333333, 0.0, 26.0, 25.53849043055923, 0.1570178570412996, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 483000.0000, 
sim time next is 483600.0000, 
raw observation next is [-0.4, 35.66666666666667, 98.16666666666667, 0.0, 26.0, 25.1377332196643, 0.1642458289281266, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.45152354570637127, 0.3566666666666667, 0.32722222222222225, 0.0, 0.6666666666666666, 0.5948111016386918, 0.5547486096427089, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04390602], dtype=float32), 0.6440497]. 
=============================================
[2019-04-04 12:31:26,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0294152e-09 6.9840427e-12 1.2708530e-16 2.6441523e-16 1.0000000e+00
 6.9628678e-12 5.8826868e-17], sum to 1.0000
[2019-04-04 12:31:26,170] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6161
[2019-04-04 12:31:26,234] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6000000000000001, 86.0, 0.0, 0.0, 26.0, 24.41277156751332, 0.1839013145893214, 1.0, 1.0, 199570.1954078077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 497400.0000, 
sim time next is 498000.0000, 
raw observation next is [0.7000000000000001, 88.0, 0.0, 0.0, 26.0, 24.63908089487336, 0.2110726625934585, 1.0, 1.0, 104325.7418297569], 
processed observation next is [1.0, 0.782608695652174, 0.4819944598337951, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5532567412394466, 0.5703575541978195, 1.0, 1.0, 0.49678924680836617], 
reward next is 0.5032, 
noisyNet noise sample is [array([1.1209317], dtype=float32), -0.2798606]. 
=============================================
[2019-04-04 12:31:26,239] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[88.04555 ]
 [87.25046 ]
 [86.76617 ]
 [86.945175]
 [86.610634]], R is [[88.47861481]
 [87.64349365]
 [86.90277863]
 [86.82324982]
 [86.95501709]].
[2019-04-04 12:31:27,628] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.8008213e-09 4.9396404e-10 3.8571987e-15 9.0439802e-14 1.0000000e+00
 6.7489292e-11 8.6162690e-15], sum to 1.0000
[2019-04-04 12:31:27,629] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0715
[2019-04-04 12:31:27,678] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.4, 88.33333333333334, 132.8333333333333, 109.3333333333333, 26.0, 24.90623190669235, 0.2546400626322027, 0.0, 1.0, 18735.04502911879], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 553200.0000, 
sim time next is 553800.0000, 
raw observation next is [-0.5, 87.66666666666666, 121.6666666666667, 115.6666666666667, 26.0, 24.83083024662009, 0.25213403526141, 0.0, 1.0, 63943.19842392824], 
processed observation next is [0.0, 0.391304347826087, 0.44875346260387816, 0.8766666666666666, 0.40555555555555567, 0.12780847145488033, 0.6666666666666666, 0.5692358538850074, 0.58404467842047, 0.0, 1.0, 0.30449142106632493], 
reward next is 0.6955, 
noisyNet noise sample is [array([-0.43326855], dtype=float32), -0.20912069]. 
=============================================
[2019-04-04 12:31:37,492] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.3003221e-08 8.5606761e-10 5.9631973e-14 3.9773536e-12 9.9999988e-01
 6.8417583e-10 5.8727512e-14], sum to 1.0000
[2019-04-04 12:31:37,492] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0811
[2019-04-04 12:31:37,519] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.97359410773492, 0.2275352834886891, 0.0, 1.0, 43632.90302046479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 677400.0000, 
sim time next is 678000.0000, 
raw observation next is [-3.0, 66.33333333333334, 0.0, 0.0, 26.0, 25.02444945140696, 0.2221717282644682, 0.0, 1.0, 43265.70573759906], 
processed observation next is [0.0, 0.8695652173913043, 0.3795013850415513, 0.6633333333333334, 0.0, 0.0, 0.6666666666666666, 0.5853707876172466, 0.5740572427548227, 0.0, 1.0, 0.20602717017904315], 
reward next is 0.7940, 
noisyNet noise sample is [array([0.7891541], dtype=float32), 0.32910615]. 
=============================================
[2019-04-04 12:31:37,524] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[72.17988 ]
 [72.08798 ]
 [72.05716 ]
 [71.983406]
 [71.83695 ]], R is [[72.32191467]
 [72.39091492]
 [72.45509338]
 [72.50614929]
 [72.51750183]].
[2019-04-04 12:31:46,784] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.3432924e-08 1.1837497e-09 2.3216939e-14 1.4447781e-12 1.0000000e+00
 8.6992868e-10 5.9503367e-14], sum to 1.0000
[2019-04-04 12:31:46,784] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2552
[2019-04-04 12:31:46,798] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.716666666666667, 74.16666666666667, 0.0, 0.0, 26.0, 23.92687385109691, 0.04642216353485534, 0.0, 1.0, 41256.53177869465], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 789000.0000, 
sim time next is 789600.0000, 
raw observation next is [-7.633333333333333, 74.33333333333334, 0.0, 0.0, 26.0, 23.9326907503522, 0.04264700600410081, 0.0, 1.0, 41209.49434597361], 
processed observation next is [1.0, 0.13043478260869565, 0.2511542012927055, 0.7433333333333334, 0.0, 0.0, 0.6666666666666666, 0.4943908958626834, 0.5142156686680336, 0.0, 1.0, 0.1962356873617791], 
reward next is 0.8038, 
noisyNet noise sample is [array([-1.4038103], dtype=float32), 1.3772081]. 
=============================================
[2019-04-04 12:31:52,728] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7326750e-08 1.2479420e-09 3.3115435e-15 3.8370240e-13 1.0000000e+00
 3.4437581e-10 2.2137877e-14], sum to 1.0000
[2019-04-04 12:31:52,729] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9128
[2019-04-04 12:31:52,740] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.616666666666667, 78.50000000000001, 0.0, 0.0, 26.0, 24.58480514745298, 0.1803158189320113, 0.0, 1.0, 39304.19919942499], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 875400.0000, 
sim time next is 876000.0000, 
raw observation next is [-1.533333333333333, 78.0, 0.0, 0.0, 26.0, 24.57566320659661, 0.1843557792699911, 0.0, 1.0, 39268.00197170453], 
processed observation next is [1.0, 0.13043478260869565, 0.42012927054478305, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5479719338830508, 0.5614519264233303, 0.0, 1.0, 0.18699048557954537], 
reward next is 0.8130, 
noisyNet noise sample is [array([0.00374938], dtype=float32), -0.047239054]. 
=============================================
[2019-04-04 12:31:52,743] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.807976]
 [82.856766]
 [82.94165 ]
 [83.21706 ]
 [83.20651 ]], R is [[82.80259705]
 [82.78740692]
 [82.77220917]
 [82.75701141]
 [82.74194336]].
[2019-04-04 12:31:53,278] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7971530e-09 3.0321406e-10 6.2693329e-16 2.0248746e-14 1.0000000e+00
 4.6814320e-11 6.0767503e-15], sum to 1.0000
[2019-04-04 12:31:53,278] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0588
[2019-04-04 12:31:53,297] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 26.0, 24.73788440297026, 0.1989674288886459, 0.0, 1.0, 39356.53097039001], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 873000.0000, 
sim time next is 873600.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 26.0, 24.68002640057985, 0.1900367505913944, 0.0, 1.0, 39375.40620904758], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5566688667149876, 0.5633455835304648, 0.0, 1.0, 0.187501934328798], 
reward next is 0.8125, 
noisyNet noise sample is [array([0.11323155], dtype=float32), 0.98083]. 
=============================================
[2019-04-04 12:31:54,380] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1587554e-10 1.0058824e-11 1.6882106e-17 7.8909701e-16 1.0000000e+00
 9.1824872e-12 6.2770799e-18], sum to 1.0000
[2019-04-04 12:31:54,382] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8642
[2019-04-04 12:31:54,392] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.56666666666667, 82.0, 0.0, 0.0, 26.0, 25.92351474947021, 0.6188596358201499, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1064400.0000, 
sim time next is 1065000.0000, 
raw observation next is [12.38333333333333, 82.5, 7.999999999999999, 27.66666666666666, 26.0, 25.89792859082768, 0.6249464517852553, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.8056325023084026, 0.825, 0.026666666666666665, 0.030570902394106807, 0.6666666666666666, 0.6581607159023065, 0.7083154839284185, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.525825], dtype=float32), -0.43708238]. 
=============================================
[2019-04-04 12:31:54,402] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[90.55577 ]
 [90.213554]
 [89.50244 ]
 [89.02266 ]
 [88.899185]], R is [[91.06669617]
 [91.15602875]
 [91.24446869]
 [91.33202362]
 [91.30083466]].
[2019-04-04 12:31:55,026] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1360303e-11 1.4179636e-12 3.3669596e-19 9.6837852e-18 1.0000000e+00
 5.5577179e-14 2.5678755e-19], sum to 1.0000
[2019-04-04 12:31:55,028] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3240
[2019-04-04 12:31:55,045] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [17.16666666666667, 62.0, 193.1666666666667, 300.0, 26.0, 26.52854067969509, 0.8631603599170906, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1081200.0000, 
sim time next is 1081800.0000, 
raw observation next is [17.45, 60.5, 181.0, 317.0, 26.0, 27.00579470250378, 0.7243764907867782, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.9459833795013851, 0.605, 0.6033333333333334, 0.35027624309392263, 0.6666666666666666, 0.7504828918753151, 0.7414588302622594, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.536794], dtype=float32), -1.2149056]. 
=============================================
[2019-04-04 12:31:57,982] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8193901e-11 4.5277641e-13 2.2056140e-19 8.3942433e-19 1.0000000e+00
 2.6328836e-13 1.7906438e-18], sum to 1.0000
[2019-04-04 12:31:57,987] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9247
[2019-04-04 12:31:57,996] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.76666666666667, 80.0, 0.0, 0.0, 26.0, 25.86680495988135, 0.5874932761362167, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1014000.0000, 
sim time next is 1014600.0000, 
raw observation next is [14.58333333333333, 80.5, 0.0, 0.0, 26.0, 26.00187161714966, 0.5781720564797942, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8665743305632503, 0.805, 0.0, 0.0, 0.6666666666666666, 0.6668226347624717, 0.6927240188265981, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5888909], dtype=float32), -0.24677841]. 
=============================================
[2019-04-04 12:31:58,323] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1381122e-09 7.0285561e-12 1.0713822e-17 1.5749855e-16 1.0000000e+00
 5.2792579e-12 7.0085483e-18], sum to 1.0000
[2019-04-04 12:31:58,329] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4715
[2019-04-04 12:31:58,341] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.89474382655858, 0.629996479723374, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1029600.0000, 
sim time next is 1030200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.97010565090778, 0.6272485508319503, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6641754709089817, 0.7090828502773169, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6060064], dtype=float32), -0.982785]. 
=============================================
[2019-04-04 12:32:00,596] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.31121100e-09 3.73908404e-10 4.47210823e-15 1.06570026e-13
 1.00000000e+00 2.77949261e-11 1.95178363e-14], sum to 1.0000
[2019-04-04 12:32:00,600] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3644
[2019-04-04 12:32:00,609] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 65.0, 145.0, 0.0, 26.0, 25.23935485642028, 0.5148263668696179, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1162800.0000, 
sim time next is 1163400.0000, 
raw observation next is [18.38333333333334, 64.66666666666667, 150.0, 0.0, 26.0, 25.2065055827811, 0.5110046660692807, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.9718374884579875, 0.6466666666666667, 0.5, 0.0, 0.6666666666666666, 0.6005421318984251, 0.6703348886897603, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.6454315], dtype=float32), -0.6815594]. 
=============================================
[2019-04-04 12:32:00,770] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9797097e-11 1.8273150e-12 5.3516259e-19 2.0075769e-18 1.0000000e+00
 3.8537907e-13 8.2296820e-20], sum to 1.0000
[2019-04-04 12:32:00,772] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6427
[2019-04-04 12:32:00,782] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.5, 76.0, 30.33333333333334, 0.0, 26.0, 24.55683488661801, 0.5318196855689381, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1009200.0000, 
sim time next is 1009800.0000, 
raw observation next is [15.5, 76.5, 25.0, 0.0, 26.0, 25.94376450576283, 0.6322353637497083, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.765, 0.08333333333333333, 0.0, 0.6666666666666666, 0.6619803754802357, 0.7107451212499027, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31511664], dtype=float32), 0.5763312]. 
=============================================
[2019-04-04 12:32:03,728] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5117965e-10 7.6806009e-12 1.2026820e-17 5.0498643e-16 1.0000000e+00
 4.6231664e-12 6.2846066e-17], sum to 1.0000
[2019-04-04 12:32:03,733] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0352
[2019-04-04 12:32:03,739] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 100.0, 89.0, 0.0, 26.0, 24.79157756733784, 0.4508225457192837, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1259400.0000, 
sim time next is 1260000.0000, 
raw observation next is [13.8, 100.0, 86.0, 0.0, 26.0, 24.77566674679137, 0.4484129145912534, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.2866666666666667, 0.0, 0.6666666666666666, 0.5646388955659475, 0.6494709715304178, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7645462], dtype=float32), -2.2176232]. 
=============================================
[2019-04-04 12:32:03,758] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[86.007965]
 [86.11365 ]
 [86.34367 ]
 [86.18276 ]
 [85.00484 ]], R is [[86.11788177]
 [86.25670624]
 [86.39414215]
 [86.53020477]
 [86.66490173]].
[2019-04-04 12:32:04,452] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5326720e-09 5.1156933e-11 4.6517474e-15 4.6185786e-14 1.0000000e+00
 4.6945673e-11 5.8975673e-15], sum to 1.0000
[2019-04-04 12:32:04,454] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5343
[2019-04-04 12:32:04,469] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [17.38333333333333, 66.66666666666667, 114.3333333333333, 0.0, 26.0, 25.45626763298506, 0.5388869664190313, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1159800.0000, 
sim time next is 1160400.0000, 
raw observation next is [17.56666666666667, 66.33333333333334, 122.1666666666667, 0.0, 26.0, 25.3873294247875, 0.5298418899788214, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.9492151431209604, 0.6633333333333334, 0.4072222222222223, 0.0, 0.6666666666666666, 0.6156107853989582, 0.6766139633262739, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4833366], dtype=float32), 0.4466932]. 
=============================================
[2019-04-04 12:32:22,810] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1033517e-11 9.9503565e-13 9.7848198e-19 2.3164345e-18 1.0000000e+00
 6.3088733e-15 6.1067617e-19], sum to 1.0000
[2019-04-04 12:32:22,812] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5192
[2019-04-04 12:32:22,831] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.63333333333333, 49.66666666666667, 89.16666666666666, 0.0, 26.0, 26.803627861358, 0.793213487992872, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1610400.0000, 
sim time next is 1611000.0000, 
raw observation next is [13.55, 50.0, 78.0, 0.0, 26.0, 27.04341204495685, 0.8135617703856455, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8379501385041552, 0.5, 0.26, 0.0, 0.6666666666666666, 0.7536176704130707, 0.7711872567952152, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5049043], dtype=float32), -0.45580325]. 
=============================================
[2019-04-04 12:32:22,852] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[100.050446]
 [100.35402 ]
 [100.651085]
 [100.96579 ]
 [101.25317 ]], R is [[99.73249054]
 [99.73516846]
 [99.73781586]
 [99.74044037]
 [99.74303436]].
[2019-04-04 12:32:23,256] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.3981922e-10 2.3387736e-11 6.6600354e-17 2.1617289e-15 1.0000000e+00
 3.8889035e-12 8.7253784e-17], sum to 1.0000
[2019-04-04 12:32:23,256] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1057
[2019-04-04 12:32:23,273] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.683333333333334, 97.0, 0.0, 0.0, 26.0, 25.46822750462174, 0.5386371629531276, 0.0, 1.0, 79180.16277720833], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1662600.0000, 
sim time next is 1663200.0000, 
raw observation next is [5.5, 97.0, 0.0, 0.0, 26.0, 25.47897680408785, 0.5520913445030906, 0.0, 1.0, 48029.67315138429], 
processed observation next is [1.0, 0.2608695652173913, 0.6149584487534627, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6232480670073208, 0.684030448167697, 0.0, 1.0, 0.22871272929230616], 
reward next is 0.7713, 
noisyNet noise sample is [array([0.1924299], dtype=float32), -2.1561866]. 
=============================================
[2019-04-04 12:32:24,713] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8379375e-11 2.5439629e-12 1.0445688e-18 5.5981832e-16 1.0000000e+00
 2.9470606e-12 1.3487799e-18], sum to 1.0000
[2019-04-04 12:32:24,714] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2803
[2019-04-04 12:32:24,750] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.57758321644909, 0.565647857631177, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1711800.0000, 
sim time next is 1712400.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.7084951012651, 0.572078462928711, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6423745917720917, 0.690692820976237, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0690271], dtype=float32), -0.5628018]. 
=============================================
[2019-04-04 12:32:33,374] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8119819e-09 5.0762627e-10 6.6824369e-15 2.2038882e-13 1.0000000e+00
 7.6673438e-11 3.1175021e-14], sum to 1.0000
[2019-04-04 12:32:33,375] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5242
[2019-04-04 12:32:33,425] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.00035234564549, 0.3202684522701204, 0.0, 1.0, 45553.70203233595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1792200.0000, 
sim time next is 1792800.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.00094299351564, 0.3190636971211951, 0.0, 1.0, 44765.37995813521], 
processed observation next is [0.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5834119161263033, 0.606354565707065, 0.0, 1.0, 0.21316847599112004], 
reward next is 0.7868, 
noisyNet noise sample is [array([-0.30073145], dtype=float32), 1.7131383]. 
=============================================
[2019-04-04 12:32:41,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.2288862e-10 1.1250303e-10 6.3456962e-18 4.0535339e-16 1.0000000e+00
 5.2573722e-12 7.5150927e-17], sum to 1.0000
[2019-04-04 12:32:41,223] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4384
[2019-04-04 12:32:41,236] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.9, 62.0, 99.33333333333333, 0.0, 26.0, 25.73519274182837, 0.3298256318336322, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1954200.0000, 
sim time next is 1954800.0000, 
raw observation next is [-2.8, 62.0, 93.0, 0.0, 26.0, 25.7090927140359, 0.3162621520966807, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.31, 0.0, 0.6666666666666666, 0.642424392836325, 0.6054207173655602, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5329206], dtype=float32), -2.3463352]. 
=============================================
[2019-04-04 12:32:42,090] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.8368278e-10 1.1968007e-10 1.5160130e-16 6.1551394e-15 1.0000000e+00
 1.4822999e-11 2.3840589e-16], sum to 1.0000
[2019-04-04 12:32:42,090] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9843
[2019-04-04 12:32:42,136] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.566666666666666, 80.16666666666667, 113.6666666666667, 444.6666666666667, 26.0, 25.6500110954325, 0.305192580715422, 1.0, 1.0, 21618.76181260417], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1936200.0000, 
sim time next is 1936800.0000, 
raw observation next is [-7.3, 79.0, 128.0, 392.5, 26.0, 25.64574529936694, 0.3195494836706362, 1.0, 1.0, 22669.92120063964], 
processed observation next is [1.0, 0.43478260869565216, 0.26038781163434904, 0.79, 0.4266666666666667, 0.43370165745856354, 0.6666666666666666, 0.6371454416139116, 0.6065164945568787, 1.0, 1.0, 0.10795200571733163], 
reward next is 0.8920, 
noisyNet noise sample is [array([-0.5039873], dtype=float32), 0.5872143]. 
=============================================
[2019-04-04 12:32:49,118] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8996656e-09 1.0754973e-10 8.7346031e-17 2.0319266e-15 1.0000000e+00
 8.3761045e-12 1.6841977e-16], sum to 1.0000
[2019-04-04 12:32:49,119] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4223
[2019-04-04 12:32:49,191] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 82.00000000000001, 0.0, 0.0, 26.0, 26.14878463318504, 0.5500761057371049, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2052600.0000, 
sim time next is 2053200.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 26.31075565217404, 0.5353325302774522, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6925629710145035, 0.6784441767591507, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4241021], dtype=float32), -2.8012116]. 
=============================================
[2019-04-04 12:32:59,102] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8961357e-08 3.3882472e-10 1.9905516e-15 1.2760060e-14 1.0000000e+00
 1.9516116e-11 1.1939568e-15], sum to 1.0000
[2019-04-04 12:32:59,102] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1927
[2019-04-04 12:32:59,156] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 75.0, 61.33333333333333, 325.0, 26.0, 25.90946058328553, 0.403255762432493, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2191800.0000, 
sim time next is 2192400.0000, 
raw observation next is [-5.6, 75.0, 71.5, 356.5, 26.0, 25.96182261559764, 0.4075526258764988, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.30747922437673136, 0.75, 0.23833333333333334, 0.3939226519337017, 0.6666666666666666, 0.66348521796647, 0.6358508752921662, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.78374594], dtype=float32), -0.73338795]. 
=============================================
[2019-04-04 12:33:03,930] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0587435e-08 8.7911273e-10 1.2226303e-14 3.5450614e-13 1.0000000e+00
 5.1084315e-10 1.4790950e-14], sum to 1.0000
[2019-04-04 12:33:03,930] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7984
[2019-04-04 12:33:03,953] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.22837779622602, 0.09755840283517149, 0.0, 1.0, 42168.90435320092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2173800.0000, 
sim time next is 2174400.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.15727742536755, 0.09611193491841703, 0.0, 1.0, 42155.03221966646], 
processed observation next is [1.0, 0.17391304347826086, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5131064521139624, 0.5320373116394723, 0.0, 1.0, 0.20073824866507836], 
reward next is 0.7993, 
noisyNet noise sample is [array([-0.5532001], dtype=float32), -0.14729744]. 
=============================================
[2019-04-04 12:33:08,639] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2264217e-09 5.1395076e-11 1.0644091e-15 6.2426794e-15 1.0000000e+00
 1.7157369e-11 1.3429132e-16], sum to 1.0000
[2019-04-04 12:33:08,639] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2859
[2019-04-04 12:33:08,729] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 70.0, 8.333333333333332, 0.0, 26.0, 25.34253423926619, 0.3222832534424031, 1.0, 1.0, 33453.02323566009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2222400.0000, 
sim time next is 2223000.0000, 
raw observation next is [-4.5, 69.5, 0.0, 0.0, 26.0, 24.62713476462772, 0.2786353870238841, 1.0, 1.0, 198788.7899444755], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.695, 0.0, 0.0, 0.6666666666666666, 0.5522612303856432, 0.5928784623412947, 1.0, 1.0, 0.9466132854498833], 
reward next is 0.0534, 
noisyNet noise sample is [array([-0.5612399], dtype=float32), 1.4103137]. 
=============================================
[2019-04-04 12:33:08,734] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[91.57379 ]
 [91.85677 ]
 [92.37323 ]
 [92.74281 ]
 [93.170586]], R is [[90.57861328]
 [90.51352692]
 [90.43565369]
 [90.31976318]
 [90.12641144]].
[2019-04-04 12:33:09,279] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6518639e-09 3.5697581e-10 5.4463744e-15 3.4069539e-15 1.0000000e+00
 1.9820889e-10 1.3562011e-15], sum to 1.0000
[2019-04-04 12:33:09,279] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0288
[2019-04-04 12:33:09,347] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 55.66666666666667, 0.0, 0.0, 26.0, 25.03416955601566, 0.3545413170480947, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2319000.0000, 
sim time next is 2319600.0000, 
raw observation next is [-1.7, 55.33333333333334, 0.0, 0.0, 26.0, 25.03607541608584, 0.3672129626477135, 0.0, 1.0, 180810.2749836278], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.5533333333333335, 0.0, 0.0, 0.6666666666666666, 0.5863396180071533, 0.6224043208825711, 0.0, 1.0, 0.8610013094458467], 
reward next is 0.1390, 
noisyNet noise sample is [array([0.08207542], dtype=float32), 0.8478626]. 
=============================================
[2019-04-04 12:33:19,489] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0876780e-06 3.7339383e-08 4.9469378e-12 2.2476817e-11 9.9999893e-01
 2.4611522e-08 1.5026889e-11], sum to 1.0000
[2019-04-04 12:33:19,489] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0531
[2019-04-04 12:33:19,513] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.2, 59.0, 0.0, 0.0, 26.0, 23.68019010004498, -0.04900167107377559, 0.0, 1.0, 44202.94978360659], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2432400.0000, 
sim time next is 2433000.0000, 
raw observation next is [-8.3, 60.0, 0.0, 0.0, 26.0, 23.63411990483594, -0.05851752328004563, 0.0, 1.0, 44267.83091852258], 
processed observation next is [0.0, 0.13043478260869565, 0.23268698060941828, 0.6, 0.0, 0.0, 0.6666666666666666, 0.46950999206966176, 0.4804941589066514, 0.0, 1.0, 0.21079919485010754], 
reward next is 0.7892, 
noisyNet noise sample is [array([0.06812292], dtype=float32), 0.2733191]. 
=============================================
[2019-04-04 12:33:19,516] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[60.796005]
 [60.788452]
 [60.782505]
 [60.785294]
 [60.784035]], R is [[60.98189163]
 [61.16158295]
 [61.33983231]
 [61.51669693]
 [61.69220734]].
[2019-04-04 12:33:27,682] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2644702e-08 1.1085103e-09 1.7422912e-14 5.5870097e-14 1.0000000e+00
 3.6438608e-10 1.6851602e-14], sum to 1.0000
[2019-04-04 12:33:27,682] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4611
[2019-04-04 12:33:27,709] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.3, 26.0, 55.0, 479.5, 26.0, 24.97287961711741, 0.2696196333724517, 0.0, 1.0, 18698.05932903607], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2476800.0000, 
sim time next is 2477400.0000, 
raw observation next is [3.3, 25.83333333333334, 52.66666666666666, 412.6666666666666, 26.0, 24.9690438521918, 0.2636487403263114, 0.0, 1.0, 18698.16057386722], 
processed observation next is [0.0, 0.6956521739130435, 0.554016620498615, 0.2583333333333334, 0.17555555555555552, 0.4559852670349907, 0.6666666666666666, 0.5807536543493166, 0.5878829134421039, 0.0, 1.0, 0.08903885987555818], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.2420877], dtype=float32), -0.060460348]. 
=============================================
[2019-04-04 12:33:30,136] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4512735e-09 2.3829921e-11 3.2885909e-17 1.6015594e-15 1.0000000e+00
 2.0845251e-11 6.2120046e-17], sum to 1.0000
[2019-04-04 12:33:30,136] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3209
[2019-04-04 12:33:30,142] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.55, 27.5, 159.0, 275.0, 26.0, 25.85438813155256, 0.3840481987056927, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2557800.0000, 
sim time next is 2558400.0000, 
raw observation next is [3.466666666666667, 28.0, 151.5, 287.6666666666667, 26.0, 25.84304385694051, 0.3810726036422676, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5586334256694367, 0.28, 0.505, 0.31786372007366487, 0.6666666666666666, 0.6535869880783759, 0.6270242012140892, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9367068], dtype=float32), -0.19927053]. 
=============================================
[2019-04-04 12:33:34,844] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.7225059e-10 8.5093729e-12 1.1937558e-16 1.1834503e-15 1.0000000e+00
 6.8077510e-12 2.3772284e-17], sum to 1.0000
[2019-04-04 12:33:34,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9485
[2019-04-04 12:33:34,901] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 60.0, 0.0, 0.0, 26.0, 25.06487058647085, 0.393459150681809, 1.0, 1.0, 20888.83668794326], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2660400.0000, 
sim time next is 2661000.0000, 
raw observation next is [-1.2, 60.5, 0.0, 0.0, 26.0, 25.19194566223795, 0.3902333788554735, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.605, 0.0, 0.0, 0.6666666666666666, 0.5993288051864957, 0.6300777929518245, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2429382], dtype=float32), -1.3048735]. 
=============================================
[2019-04-04 12:33:34,921] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[88.450455]
 [88.39863 ]
 [88.47554 ]
 [88.63678 ]
 [88.78464 ]], R is [[88.51026917]
 [88.5256958 ]
 [88.20055389]
 [87.72606659]
 [87.62160492]].
[2019-04-04 12:33:38,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.27633781e-09 4.59808823e-11 1.71345646e-16 1.06066855e-14
 1.00000000e+00 2.42628261e-11 2.52970921e-15], sum to 1.0000
[2019-04-04 12:33:38,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6001
[2019-04-04 12:33:38,293] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 63.33333333333333, 0.0, 0.0, 26.0, 25.0022096479429, 0.3740183522097125, 0.0, 1.0, 23329.42445964769], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2664600.0000, 
sim time next is 2665200.0000, 
raw observation next is [-1.2, 63.66666666666667, 0.0, 0.0, 26.0, 25.02916427204441, 0.3829862441695001, 0.0, 1.0, 189214.8487339524], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5857636893370343, 0.6276620813898334, 0.0, 1.0, 0.9010230892092972], 
reward next is 0.0990, 
noisyNet noise sample is [array([-0.3374961], dtype=float32), -0.4521835]. 
=============================================
[2019-04-04 12:33:42,557] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.5181158e-09 2.4748553e-10 2.3757711e-15 6.5891987e-14 1.0000000e+00
 1.3578533e-10 5.4907242e-15], sum to 1.0000
[2019-04-04 12:33:42,557] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5767
[2019-04-04 12:33:42,584] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.66399247706907, 0.2487134598649357, 0.0, 1.0, 43363.92443673552], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2766000.0000, 
sim time next is 2766600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.64154608933863, 0.2462152281881261, 0.0, 1.0, 43140.10434356279], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5534621741115524, 0.5820717427293753, 0.0, 1.0, 0.20542906830267993], 
reward next is 0.7946, 
noisyNet noise sample is [array([-0.18406789], dtype=float32), 0.7160618]. 
=============================================
[2019-04-04 12:33:44,999] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.8572211e-10 3.8298203e-10 1.9125282e-15 1.7509564e-14 1.0000000e+00
 5.0202117e-11 1.7713384e-15], sum to 1.0000
[2019-04-04 12:33:45,006] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3101
[2019-04-04 12:33:45,030] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 30.0, 0.0, 0.0, 26.0, 25.46137338955483, 0.3652269107535455, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2829600.0000, 
sim time next is 2830200.0000, 
raw observation next is [4.666666666666667, 31.16666666666666, 0.0, 0.0, 26.0, 25.46569636688149, 0.22787889244465, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5918744228993538, 0.3116666666666666, 0.0, 0.0, 0.6666666666666666, 0.622141363906791, 0.5759596308148833, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49491158], dtype=float32), 1.0472317]. 
=============================================
[2019-04-04 12:33:54,250] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8253390e-07 1.8005160e-08 5.6688280e-13 5.0353502e-12 9.9999976e-01
 5.5619194e-09 2.2126836e-12], sum to 1.0000
[2019-04-04 12:33:54,250] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0092
[2019-04-04 12:33:54,266] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.99024994950282, 0.04833235837562297, 0.0, 1.0, 40194.176882968], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3040200.0000, 
sim time next is 3040800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.96252460423194, 0.04195332736180832, 0.0, 1.0, 40210.04275653641], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.4968770503526618, 0.513984442453936, 0.0, 1.0, 0.1914763940787448], 
reward next is 0.8085, 
noisyNet noise sample is [array([-0.75498563], dtype=float32), -0.9318205]. 
=============================================
[2019-04-04 12:34:03,102] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0185003e-08 5.0951030e-09 3.0570757e-14 3.3399141e-13 1.0000000e+00
 1.1262653e-09 9.5111317e-14], sum to 1.0000
[2019-04-04 12:34:03,103] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6760
[2019-04-04 12:34:03,167] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 26.0, 24.99743046613969, 0.2753322757347422, 0.0, 1.0, 37542.43420937777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3092400.0000, 
sim time next is 3093000.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 26.0, 24.99308322459135, 0.2783914822198348, 0.0, 1.0, 37360.95761935737], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5827569353826124, 0.5927971607399449, 0.0, 1.0, 0.17790932199693985], 
reward next is 0.8221, 
noisyNet noise sample is [array([0.93552864], dtype=float32), 0.18888491]. 
=============================================
[2019-04-04 12:34:03,171] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5278258e-10 2.8106087e-12 8.6630229e-18 8.8032484e-16 1.0000000e+00
 1.5828246e-12 4.3333027e-18], sum to 1.0000
[2019-04-04 12:34:03,171] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3303
[2019-04-04 12:34:03,173] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[73.996956]
 [73.76025 ]
 [73.67501 ]
 [73.616806]
 [73.58639 ]], R is [[74.31704712]
 [74.39510345]
 [74.48593903]
 [74.59072876]
 [74.69800568]].
[2019-04-04 12:34:03,180] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.333333333333333, 100.0, 0.0, 0.0, 26.0, 26.0596852517516, 0.7039338382050722, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3181200.0000, 
sim time next is 3181800.0000, 
raw observation next is [3.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.93507069693752, 0.6834011844010738, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5503231763619576, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6612558914114599, 0.727800394800358, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6800305], dtype=float32), -0.9250838]. 
=============================================
[2019-04-04 12:34:05,920] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3109763e-09 8.5120390e-11 1.4920197e-16 2.7929535e-15 1.0000000e+00
 6.8163786e-12 4.3587053e-16], sum to 1.0000
[2019-04-04 12:34:05,921] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2855
[2019-04-04 12:34:05,936] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.29395940206466, 0.4757365611915021, 0.0, 1.0, 50591.88842381299], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3279000.0000, 
sim time next is 3279600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.31546051717849, 0.4710384877858147, 0.0, 1.0, 45616.82540371534], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6096217097648742, 0.6570128292619383, 0.0, 1.0, 0.21722297811293018], 
reward next is 0.7828, 
noisyNet noise sample is [array([-1.963085], dtype=float32), -1.4315467]. 
=============================================
[2019-04-04 12:34:19,238] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-04 12:34:19,239] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 12:34:19,240] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 12:34:19,240] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:34:19,240] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 12:34:19,240] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:34:19,240] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:34:19,243] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run6
[2019-04-04 12:34:19,254] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run6
[2019-04-04 12:34:19,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run6
[2019-04-04 12:35:07,026] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.14674678], dtype=float32), 0.16367303]
[2019-04-04 12:35:07,026] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.633333333333333, 83.0, 124.8333333333333, 0.0, 26.0, 24.9674101224044, 0.3565215298460506, 0.0, 1.0, 33893.49559521452]
[2019-04-04 12:35:07,026] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 12:35:07,027] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.8305904e-09 5.6877691e-10 7.7431034e-15 7.5723702e-14 1.0000000e+00
 1.8333389e-10 1.3940577e-14], sampled 0.21587105920079064
[2019-04-04 12:35:27,839] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.14674678], dtype=float32), 0.16367303]
[2019-04-04 12:35:27,839] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.383333333333334, 48.83333333333333, 0.0, 0.0, 26.0, 24.34109444539674, 0.09033266470758383, 0.0, 1.0, 43385.70521560071]
[2019-04-04 12:35:27,839] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 12:35:27,840] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.1075363e-07 3.2031249e-08 6.7915248e-12 3.2351524e-11 9.9999976e-01
 2.5272582e-08 8.7486962e-12], sampled 0.2004519485416354
[2019-04-04 12:35:54,591] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14674678], dtype=float32), 0.16367303]
[2019-04-04 12:35:54,591] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [14.9, 52.5, 216.0, 349.0, 26.0, 27.29260130949618, 0.9324546139547124, 1.0, 1.0, 0.0]
[2019-04-04 12:35:54,591] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 12:35:54,592] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.4887410e-10 2.3538461e-11 3.6579700e-17 4.6170591e-16 1.0000000e+00
 3.1105442e-12 8.3074139e-17], sampled 0.4178068595906418
[2019-04-04 12:35:59,874] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14674678], dtype=float32), 0.16367303]
[2019-04-04 12:35:59,874] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [5.300000000000001, 83.66666666666667, 0.0, 0.0, 26.0, 25.54120230671675, 0.5203710192350578, 0.0, 1.0, 31429.31811090249]
[2019-04-04 12:35:59,874] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 12:35:59,875] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.8445331e-09 2.5224692e-10 4.4708631e-15 5.8703666e-14 1.0000000e+00
 1.3598775e-10 7.1690167e-15], sampled 0.780449669595602
[2019-04-04 12:36:00,732] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 12:36:21,517] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 12:36:23,748] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7444 275783685.2516 1233.3387
[2019-04-04 12:36:24,771] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 500000, evaluation results [500000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.744355944586, 275783685.2516378, 1233.338732864913]
[2019-04-04 12:36:28,072] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1188826e-09 3.9717180e-10 3.2324755e-15 2.4582761e-14 1.0000000e+00
 9.7252636e-11 1.4423262e-14], sum to 1.0000
[2019-04-04 12:36:28,072] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1312
[2019-04-04 12:36:28,083] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 47.33333333333334, 114.6666666666667, 813.8333333333334, 26.0, 25.24170720408707, 0.454693296810986, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3590400.0000, 
sim time next is 3591000.0000, 
raw observation next is [-1.5, 46.0, 114.0, 812.0, 26.0, 25.2371751727518, 0.4512479542332379, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.4210526315789474, 0.46, 0.38, 0.8972375690607735, 0.6666666666666666, 0.6030979310626501, 0.6504159847444126, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.554174], dtype=float32), 0.5427206]. 
=============================================
[2019-04-04 12:36:28,090] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.98596 ]
 [78.03116 ]
 [78.09125 ]
 [78.104546]
 [78.141594]], R is [[78.13482666]
 [78.35347748]
 [78.56994629]
 [78.6951828 ]
 [78.90823364]].
[2019-04-04 12:36:28,343] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0712766e-09 1.3103514e-09 9.2792419e-14 1.4133403e-13 1.0000000e+00
 6.8048239e-10 3.6606416e-14], sum to 1.0000
[2019-04-04 12:36:28,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0074
[2019-04-04 12:36:28,367] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 41.66666666666667, 66.83333333333333, 545.6666666666666, 26.0, 25.30576459706464, 0.4471428729929647, 0.0, 1.0, 18684.31268463445], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3601200.0000, 
sim time next is 3601800.0000, 
raw observation next is [0.0, 41.0, 63.0, 515.0, 26.0, 25.28088803187378, 0.4396396001614134, 0.0, 1.0, 9342.108645421342], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.41, 0.21, 0.569060773480663, 0.6666666666666666, 0.6067406693228149, 0.6465465333871377, 0.0, 1.0, 0.04448623164486353], 
reward next is 0.9555, 
noisyNet noise sample is [array([0.61317605], dtype=float32), -2.481736]. 
=============================================
[2019-04-04 12:36:30,063] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3203597e-08 6.2624922e-10 4.6712008e-15 2.6323297e-14 1.0000000e+00
 1.8188567e-10 3.7691120e-15], sum to 1.0000
[2019-04-04 12:36:30,064] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0634
[2019-04-04 12:36:30,122] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3333333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.15200245762094, 0.385075699847838, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3482400.0000, 
sim time next is 3483000.0000, 
raw observation next is [-0.5, 71.5, 3.0, 107.0, 26.0, 25.37672967621328, 0.4031471120940153, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44875346260387816, 0.715, 0.01, 0.11823204419889503, 0.6666666666666666, 0.6147274730177733, 0.6343823706980051, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20214303], dtype=float32), 0.6774514]. 
=============================================
[2019-04-04 12:36:30,127] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[78.77167 ]
 [78.885315]
 [78.80709 ]
 [78.66987 ]
 [78.59479 ]], R is [[79.42021942]
 [79.62601471]
 [79.56677246]
 [79.52474976]
 [79.51123047]].
[2019-04-04 12:36:31,176] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6319543e-08 7.7601703e-10 6.1521350e-15 4.9587650e-14 1.0000000e+00
 3.9312603e-10 1.7366676e-14], sum to 1.0000
[2019-04-04 12:36:31,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0176
[2019-04-04 12:36:31,206] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.43371138396233, 0.4669863745541012, 0.0, 1.0, 18763.23138167567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3544200.0000, 
sim time next is 3544800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.42331664942629, 0.4558118579599546, 0.0, 1.0, 28011.98381753528], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6186097207855242, 0.6519372859866516, 0.0, 1.0, 0.13339039913112039], 
reward next is 0.8666, 
noisyNet noise sample is [array([-1.0988196], dtype=float32), 0.33161232]. 
=============================================
[2019-04-04 12:36:37,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1307991e-08 2.4204644e-10 4.2757583e-14 1.5202398e-13 1.0000000e+00
 6.6502087e-10 2.8333901e-14], sum to 1.0000
[2019-04-04 12:36:37,533] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8805
[2019-04-04 12:36:37,555] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 66.0, 0.0, 0.0, 26.0, 25.39382439154377, 0.3923631808874828, 0.0, 1.0, 42517.48631290064], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3712200.0000, 
sim time next is 3712800.0000, 
raw observation next is [-3.0, 67.0, 0.0, 0.0, 26.0, 25.38726562540651, 0.3898805739093169, 0.0, 1.0, 43632.86456612797], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6156054687838758, 0.6299601913031057, 0.0, 1.0, 0.20777554555299035], 
reward next is 0.7922, 
noisyNet noise sample is [array([0.7596389], dtype=float32), 0.34164277]. 
=============================================
[2019-04-04 12:36:37,977] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9909306e-08 3.5588961e-09 9.3977065e-14 1.0778614e-13 1.0000000e+00
 7.1281037e-10 8.5293670e-14], sum to 1.0000
[2019-04-04 12:36:37,993] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5590
[2019-04-04 12:36:38,016] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.26822345624289, 0.3519761259181549, 0.0, 1.0, 47734.86308708674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3722400.0000, 
sim time next is 3723000.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.31659815397821, 0.3528622583460335, 0.0, 1.0, 42945.77734942087], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6097165128315174, 0.6176207527820111, 0.0, 1.0, 0.20450370166390888], 
reward next is 0.7955, 
noisyNet noise sample is [array([0.7668825], dtype=float32), -0.411188]. 
=============================================
[2019-04-04 12:36:38,051] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.82673 ]
 [74.95482 ]
 [74.357796]
 [74.58422 ]
 [74.45078 ]], R is [[75.17333221]
 [75.19429016]
 [75.15156555]
 [74.99074554]
 [74.93294525]].
[2019-04-04 12:36:40,958] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.5586336e-09 2.2789653e-10 1.0819716e-15 2.2905089e-14 1.0000000e+00
 2.8268251e-11 2.5289664e-15], sum to 1.0000
[2019-04-04 12:36:40,959] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2894
[2019-04-04 12:36:40,970] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.0, 59.0, 23.16666666666666, 224.5, 26.0, 25.35893634458277, 0.4077383330533548, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3691200.0000, 
sim time next is 3691800.0000, 
raw observation next is [4.0, 59.0, 15.0, 165.0, 26.0, 25.30659093338998, 0.3902015901878302, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.05, 0.18232044198895028, 0.6666666666666666, 0.6088825777824983, 0.6300671967292767, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5536384], dtype=float32), 1.2960001]. 
=============================================
[2019-04-04 12:36:42,427] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9847522e-10 1.2189602e-11 2.0321182e-17 3.5501782e-16 1.0000000e+00
 1.2074942e-11 9.0801551e-18], sum to 1.0000
[2019-04-04 12:36:42,427] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0572
[2019-04-04 12:36:42,441] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.5, 60.0, 110.0, 775.0, 26.0, 26.54753596437558, 0.6209960694293939, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3839400.0000, 
sim time next is 3840000.0000, 
raw observation next is [-1.333333333333333, 60.0, 111.1666666666667, 782.8333333333334, 26.0, 26.55612517354617, 0.6275865479282564, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.6, 0.3705555555555557, 0.8650092081031308, 0.6666666666666666, 0.7130104311288475, 0.7091955159760855, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8431232], dtype=float32), -0.24922945]. 
=============================================
[2019-04-04 12:36:42,454] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[94.06134]
 [93.32768]
 [92.76131]
 [92.20981]
 [91.38007]], R is [[94.2300415 ]
 [94.28774261]
 [94.34486389]
 [94.40141296]
 [94.45739746]].
[2019-04-04 12:36:51,200] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8508737e-07 6.0759562e-09 4.6773295e-13 2.1515921e-12 9.9999976e-01
 5.2178191e-09 7.7744679e-13], sum to 1.0000
[2019-04-04 12:36:51,200] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8736
[2019-04-04 12:36:51,215] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 37.5, 0.0, 0.0, 26.0, 24.88781897539701, 0.2394611985206917, 0.0, 1.0, 40407.10498520556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4077000.0000, 
sim time next is 4077600.0000, 
raw observation next is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 26.0, 24.84516892751611, 0.2286926485071905, 0.0, 1.0, 40387.37840132401], 
processed observation next is [1.0, 0.17391304347826086, 0.3425669436749769, 0.3633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5704307439596757, 0.5762308828357302, 0.0, 1.0, 0.19232084953011433], 
reward next is 0.8077, 
noisyNet noise sample is [array([-1.1746693], dtype=float32), -1.1149485]. 
=============================================
[2019-04-04 12:36:51,732] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1427699e-08 1.1352671e-10 4.1260039e-14 3.1915775e-14 1.0000000e+00
 3.4261866e-10 1.0546891e-14], sum to 1.0000
[2019-04-04 12:36:51,735] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7827
[2019-04-04 12:36:51,752] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.833333333333332, 52.33333333333334, 0.0, 0.0, 26.0, 25.19479744531808, 0.3967151428760687, 0.0, 1.0, 46821.70640026026], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3970200.0000, 
sim time next is 3970800.0000, 
raw observation next is [-9.0, 53.0, 0.0, 0.0, 26.0, 25.14484576782098, 0.3854684263960345, 0.0, 1.0, 44825.96285388575], 
processed observation next is [1.0, 1.0, 0.21329639889196678, 0.53, 0.0, 0.0, 0.6666666666666666, 0.5954038139850816, 0.6284894754653448, 0.0, 1.0, 0.21345696597088454], 
reward next is 0.7865, 
noisyNet noise sample is [array([1.6674637], dtype=float32), 0.75538254]. 
=============================================
[2019-04-04 12:36:55,869] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1392215e-09 1.4780263e-10 2.8637710e-15 6.6323093e-14 1.0000000e+00
 5.8472879e-11 3.7481356e-15], sum to 1.0000
[2019-04-04 12:36:55,870] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4125
[2019-04-04 12:36:55,883] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 26.0, 25.71966180432339, 0.5897693970202159, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4136400.0000, 
sim time next is 4137000.0000, 
raw observation next is [1.0, 36.66666666666666, 0.0, 0.0, 26.0, 25.86550842223248, 0.5939934594095875, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.3666666666666666, 0.0, 0.0, 0.6666666666666666, 0.65545903518604, 0.6979978198031959, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.87235725], dtype=float32), -0.59966487]. 
=============================================
[2019-04-04 12:36:55,897] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[81.62793 ]
 [81.82542 ]
 [81.972824]
 [81.74527 ]
 [82.33027 ]], R is [[81.41106415]
 [81.59695435]
 [81.56919861]
 [80.88822174]
 [80.13584137]].
[2019-04-04 12:37:00,491] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0524532e-08 5.5947873e-09 1.0769079e-13 4.3347520e-13 1.0000000e+00
 8.6525596e-09 6.9523673e-13], sum to 1.0000
[2019-04-04 12:37:00,491] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5550
[2019-04-04 12:37:00,504] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 26.0, 25.41379166171766, 0.3548219476637663, 0.0, 1.0, 41818.14038437163], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4234200.0000, 
sim time next is 4234800.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.39813240381957, 0.3530482738635543, 0.0, 1.0, 48542.88099196129], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6165110336516308, 0.6176827579545181, 0.0, 1.0, 0.23115657615219662], 
reward next is 0.7688, 
noisyNet noise sample is [array([0.4378871], dtype=float32), -0.5283846]. 
=============================================
[2019-04-04 12:37:00,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5235026e-08 1.3140838e-08 1.3542542e-12 3.9876847e-12 1.0000000e+00
 7.5030062e-09 4.7363215e-13], sum to 1.0000
[2019-04-04 12:37:00,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9589
[2019-04-04 12:37:00,661] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.55858398414591, 0.4182041201773676, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4220400.0000, 
sim time next is 4221000.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.57649627234017, 0.4094664909550632, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6313746893616807, 0.6364888303183543, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46132872], dtype=float32), 1.2897787]. 
=============================================
[2019-04-04 12:37:00,673] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.32069]
 [70.43531]
 [70.48968]
 [70.53822]
 [70.47043]], R is [[70.6427536 ]
 [70.93632507]
 [71.22695923]
 [71.29288483]
 [71.29430389]].
[2019-04-04 12:37:01,052] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3194426e-09 7.7241200e-11 1.3510787e-15 6.9269067e-15 1.0000000e+00
 2.0728053e-11 9.3949235e-15], sum to 1.0000
[2019-04-04 12:37:01,054] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1749
[2019-04-04 12:37:01,073] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 55.0, 162.5, 713.0, 26.0, 25.27084845983894, 0.398165575875906, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4273200.0000, 
sim time next is 4273800.0000, 
raw observation next is [5.333333333333334, 54.5, 148.6666666666667, 749.3333333333334, 26.0, 25.25425645486551, 0.3972033807040876, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6103416435826409, 0.545, 0.4955555555555557, 0.8279926335174954, 0.6666666666666666, 0.6045213712387923, 0.6324011269013625, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38618645], dtype=float32), 0.44283867]. 
=============================================
[2019-04-04 12:37:03,021] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4200843e-10 6.8330662e-12 5.1659857e-18 1.2654729e-17 1.0000000e+00
 6.7739418e-13 3.9226758e-18], sum to 1.0000
[2019-04-04 12:37:03,022] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1904
[2019-04-04 12:37:03,033] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.833333333333333, 35.33333333333334, 93.33333333333333, 523.0, 26.0, 27.03611270135232, 0.7827224458211451, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4119000.0000, 
sim time next is 4119600.0000, 
raw observation next is [3.666666666666667, 35.66666666666667, 93.16666666666666, 480.0, 26.0, 27.26256419869257, 0.8002028938597259, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.564173591874423, 0.3566666666666667, 0.31055555555555553, 0.5303867403314917, 0.6666666666666666, 0.7718803498910475, 0.766734297953242, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5618778], dtype=float32), -1.6605296]. 
=============================================
[2019-04-04 12:37:04,956] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.8262321e-08 8.2470364e-11 8.1713883e-16 7.5231352e-14 1.0000000e+00
 6.7558840e-11 8.2778402e-16], sum to 1.0000
[2019-04-04 12:37:04,959] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1593
[2019-04-04 12:37:04,974] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.4, 70.33333333333333, 0.0, 0.0, 26.0, 25.4883938520454, 0.3732185047725408, 0.0, 1.0, 18753.64281563447], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4340400.0000, 
sim time next is 4341000.0000, 
raw observation next is [3.35, 70.66666666666667, 0.0, 0.0, 26.0, 25.44952956078424, 0.3794010724543144, 0.0, 1.0, 42499.11672938539], 
processed observation next is [1.0, 0.21739130434782608, 0.5554016620498616, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6207941300653532, 0.6264670241514382, 0.0, 1.0, 0.20237674633040661], 
reward next is 0.7976, 
noisyNet noise sample is [array([0.8770924], dtype=float32), 1.4840844]. 
=============================================
[2019-04-04 12:37:04,998] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.34721]
 [81.44567]
 [81.55776]
 [81.54889]
 [81.35155]], R is [[81.1932373 ]
 [81.29200745]
 [81.38977051]
 [81.38497925]
 [81.25600433]].
[2019-04-04 12:37:05,437] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4737265e-08 6.2721983e-10 6.0923739e-14 2.2119233e-13 1.0000000e+00
 1.0062557e-09 1.4654007e-13], sum to 1.0000
[2019-04-04 12:37:05,440] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2555
[2019-04-04 12:37:05,456] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 26.0, 25.4225177456202, 0.3701821494404705, 0.0, 1.0, 41736.0396490909], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4229400.0000, 
sim time next is 4230000.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 25.41073420493903, 0.3689162100991141, 0.0, 1.0, 45433.30446168321], 
processed observation next is [0.0, 1.0, 0.4903047091412743, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6175611837449191, 0.622972070033038, 0.0, 1.0, 0.21634906886515815], 
reward next is 0.7837, 
noisyNet noise sample is [array([0.47020015], dtype=float32), 0.30657032]. 
=============================================
[2019-04-04 12:37:05,472] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[70.821526]
 [71.055145]
 [71.36381 ]
 [71.31443 ]
 [71.28927 ]], R is [[70.87895966]
 [70.97142792]
 [71.07365417]
 [71.1155777 ]
 [71.19363403]].
[2019-04-04 12:37:06,555] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.4229335e-11 2.5703593e-12 4.6122660e-18 2.1788144e-17 1.0000000e+00
 5.6155835e-12 3.6359175e-18], sum to 1.0000
[2019-04-04 12:37:06,555] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3527
[2019-04-04 12:37:06,566] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.8, 40.0, 14.0, 0.0, 26.0, 27.32283810493345, 0.9146853447999564, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4382400.0000, 
sim time next is 4383000.0000, 
raw observation next is [12.7, 41.0, 9.0, 0.0, 26.0, 27.42327086521936, 0.924039725014833, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8144044321329641, 0.41, 0.03, 0.0, 0.6666666666666666, 0.7852725721016135, 0.808013241671611, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8596968], dtype=float32), 2.8168032]. 
=============================================
[2019-04-04 12:37:06,585] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[95.39494 ]
 [96.02443 ]
 [96.53306 ]
 [97.04578 ]
 [97.547554]], R is [[94.87957764]
 [94.93078613]
 [94.98147583]
 [95.03166199]
 [95.0813446 ]].
[2019-04-04 12:37:09,855] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.1733045e-08 7.6901369e-10 7.0896082e-14 9.2738154e-13 1.0000000e+00
 4.9285731e-10 2.4897228e-13], sum to 1.0000
[2019-04-04 12:37:09,855] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7971
[2019-04-04 12:37:09,897] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 51.0, 110.0, 53.0, 26.0, 25.54304932916789, 0.3937819920793159, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4264200.0000, 
sim time next is 4264800.0000, 
raw observation next is [3.0, 51.66666666666666, 122.0, 66.0, 26.0, 25.70477950000328, 0.4011623175645465, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.5166666666666666, 0.4066666666666667, 0.07292817679558011, 0.6666666666666666, 0.6420649583336067, 0.6337207725215155, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.07246345], dtype=float32), 0.8384033]. 
=============================================
[2019-04-04 12:37:12,144] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5256975e-10 1.6897300e-11 1.5062759e-17 2.5109699e-16 1.0000000e+00
 6.1309651e-12 1.0237562e-17], sum to 1.0000
[2019-04-04 12:37:12,146] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5416
[2019-04-04 12:37:12,155] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.2, 84.66666666666667, 157.5, 64.5, 26.0, 26.15352160556519, 0.6020697216109603, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4440000.0000, 
sim time next is 4440600.0000, 
raw observation next is [1.15, 85.0, 165.0, 31.0, 26.0, 26.23060479034528, 0.6152975997693312, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49445983379501385, 0.85, 0.55, 0.03425414364640884, 0.6666666666666666, 0.6858837325287732, 0.7050991999231103, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5243668], dtype=float32), -1.2776787]. 
=============================================
[2019-04-04 12:37:12,190] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4452516e-11 1.3286940e-12 5.4932687e-19 1.7543836e-17 1.0000000e+00
 3.0466114e-13 1.3687351e-18], sum to 1.0000
[2019-04-04 12:37:12,192] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8902
[2019-04-04 12:37:12,206] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 89.0, 102.0, 0.0, 26.0, 26.30800179614326, 0.5829271599707387, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4451400.0000, 
sim time next is 4452000.0000, 
raw observation next is [0.3333333333333334, 90.0, 117.6666666666667, 0.9999999999999998, 26.0, 26.19032847316939, 0.5645618141921525, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4718374884579871, 0.9, 0.3922222222222223, 0.0011049723756906074, 0.6666666666666666, 0.6825273727641159, 0.6881872713973841, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01285461], dtype=float32), 1.4016776]. 
=============================================
[2019-04-04 12:37:12,214] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[96.27359]
 [96.50168]
 [96.72789]
 [96.88724]
 [96.98058]], R is [[96.15512848]
 [96.19358063]
 [96.23164368]
 [96.26932526]
 [96.306633  ]].
[2019-04-04 12:37:13,124] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9471135e-10 1.1291249e-11 4.6003200e-17 1.3986211e-16 1.0000000e+00
 2.3631613e-12 4.3447420e-17], sum to 1.0000
[2019-04-04 12:37:13,127] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4932
[2019-04-04 12:37:13,152] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 48.0, 122.5, 0.0, 26.0, 26.19201331313189, 0.5356320453702064, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4536000.0000, 
sim time next is 4536600.0000, 
raw observation next is [2.0, 48.66666666666666, 124.0, 0.0, 26.0, 26.20560012144517, 0.4272645924947922, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.4866666666666666, 0.41333333333333333, 0.0, 0.6666666666666666, 0.6838000101204308, 0.6424215308315974, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4713496], dtype=float32), -1.5024718]. 
=============================================
[2019-04-04 12:37:17,304] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6604242e-10 5.1775455e-11 6.0074601e-17 2.3863883e-16 1.0000000e+00
 7.0742171e-12 1.2017195e-16], sum to 1.0000
[2019-04-04 12:37:17,304] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4520
[2019-04-04 12:37:17,311] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 49.5, 121.0, 0.0, 26.0, 26.18895769015136, 0.5321013839453259, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4535400.0000, 
sim time next is 4536000.0000, 
raw observation next is [2.0, 48.0, 122.5, 0.0, 26.0, 26.19196891519966, 0.5356197990383501, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.48, 0.4083333333333333, 0.0, 0.6666666666666666, 0.6826640762666383, 0.6785399330127834, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.1636846], dtype=float32), 1.3036225]. 
=============================================
[2019-04-04 12:37:17,331] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[91.045395]
 [91.22885 ]
 [91.39748 ]
 [91.36384 ]
 [91.437744]], R is [[90.97956848]
 [91.06977081]
 [91.15907288]
 [91.2474823 ]
 [91.33500671]].
[2019-04-04 12:37:19,886] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8592135e-09 6.3617550e-10 3.6056627e-14 4.6409640e-14 1.0000000e+00
 1.0330561e-09 3.3374398e-15], sum to 1.0000
[2019-04-04 12:37:19,888] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2084
[2019-04-04 12:37:19,901] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 72.5, 0.0, 0.0, 26.0, 25.18779700784056, 0.3696101576264129, 0.0, 1.0, 36223.0333398637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4599000.0000, 
sim time next is 4599600.0000, 
raw observation next is [-2.4, 73.0, 0.0, 0.0, 26.0, 25.15226721975237, 0.3719087795312796, 0.0, 1.0, 36217.98491990193], 
processed observation next is [1.0, 0.21739130434782608, 0.39612188365650974, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5960222683126976, 0.6239695931770932, 0.0, 1.0, 0.17246659485667587], 
reward next is 0.8275, 
noisyNet noise sample is [array([1.638174], dtype=float32), -1.3417575]. 
=============================================
[2019-04-04 12:37:20,200] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.0703417e-12 1.8055336e-12 4.5298060e-19 1.0217586e-17 1.0000000e+00
 1.8285379e-13 1.0789117e-18], sum to 1.0000
[2019-04-04 12:37:20,203] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2849
[2019-04-04 12:37:20,214] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.733333333333333, 44.0, 130.0, 144.0, 26.0, 26.27178919478806, 0.7454859036432477, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4638000.0000, 
sim time next is 4638600.0000, 
raw observation next is [5.6, 44.5, 117.0, 147.0, 26.0, 26.74762272902826, 0.79592093770184, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6177285318559557, 0.445, 0.39, 0.16243093922651933, 0.6666666666666666, 0.7289685607523552, 0.7653069792339466, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.6702833], dtype=float32), -0.79220635]. 
=============================================
[2019-04-04 12:37:20,540] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.04495176e-10 1.34702111e-11 1.94297017e-17 6.94026027e-16
 1.00000000e+00 1.90928107e-11 8.99414788e-17], sum to 1.0000
[2019-04-04 12:37:20,544] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2230
[2019-04-04 12:37:20,560] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.4666666666666667, 72.83333333333333, 0.0, 0.0, 26.0, 25.34533093320249, 0.4824733444909229, 0.0, 1.0, 43459.73334926413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4492200.0000, 
sim time next is 4492800.0000, 
raw observation next is [-0.5, 73.0, 0.0, 0.0, 26.0, 25.39268478233083, 0.479614982982846, 0.0, 1.0, 32272.8485780317], 
processed observation next is [1.0, 0.0, 0.44875346260387816, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6160570651942358, 0.659871660994282, 0.0, 1.0, 0.15368023132396047], 
reward next is 0.8463, 
noisyNet noise sample is [array([0.04891634], dtype=float32), 1.4062653]. 
=============================================
[2019-04-04 12:37:23,482] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0171689e-07 6.1300267e-09 2.3632170e-14 6.6687427e-13 9.9999988e-01
 1.3546282e-09 5.1082453e-14], sum to 1.0000
[2019-04-04 12:37:23,483] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9952
[2019-04-04 12:37:23,506] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 65.66666666666667, 0.0, 0.0, 26.0, 25.45664283533968, 0.4423281480725518, 0.0, 1.0, 30414.83255106307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4587600.0000, 
sim time next is 4588200.0000, 
raw observation next is [-0.65, 66.0, 0.0, 0.0, 26.0, 25.48931925161568, 0.4431457225905237, 0.0, 1.0, 18753.25900288507], 
processed observation next is [1.0, 0.08695652173913043, 0.4445983379501386, 0.66, 0.0, 0.0, 0.6666666666666666, 0.62410993763464, 0.647715240863508, 0.0, 1.0, 0.08930123334707175], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.478553], dtype=float32), -1.4998744]. 
=============================================
[2019-04-04 12:37:24,200] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0857948e-10 1.7510984e-12 4.1694090e-19 3.2702139e-17 1.0000000e+00
 4.3309009e-13 1.5829413e-17], sum to 1.0000
[2019-04-04 12:37:24,201] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6448
[2019-04-04 12:37:24,208] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.833333333333333, 49.5, 121.3333333333333, 841.6666666666666, 26.0, 26.39372376759061, 0.707307755435154, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4621800.0000, 
sim time next is 4622400.0000, 
raw observation next is [3.0, 49.0, 121.0, 846.0, 26.0, 26.72624347380614, 0.7451530827572466, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.49, 0.4033333333333333, 0.9348066298342541, 0.6666666666666666, 0.7271869561505117, 0.7483843609190822, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.70180386], dtype=float32), -0.15845133]. 
=============================================
[2019-04-04 12:37:27,112] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1443460e-08 6.7982762e-09 2.1692153e-13 7.0247631e-12 1.0000000e+00
 2.4691185e-09 5.7665190e-13], sum to 1.0000
[2019-04-04 12:37:27,112] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4975
[2019-04-04 12:37:27,222] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.666666666666666, 87.0, 103.3333333333333, 349.1666666666667, 26.0, 23.88128617881758, 0.2284955669925253, 0.0, 1.0, 202434.7082113439], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4782000.0000, 
sim time next is 4782600.0000, 
raw observation next is [-5.5, 84.5, 124.0, 419.0, 26.0, 24.31782150922402, 0.3489431683607606, 0.0, 1.0, 202145.2211270249], 
processed observation next is [0.0, 0.34782608695652173, 0.3102493074792244, 0.845, 0.41333333333333333, 0.46298342541436466, 0.6666666666666666, 0.5264851257686685, 0.6163143894535869, 0.0, 1.0, 0.962596291081071], 
reward next is 0.0374, 
noisyNet noise sample is [array([-0.0454801], dtype=float32), -0.8734429]. 
=============================================
[2019-04-04 12:37:27,963] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2560337e-11 1.2529821e-12 9.8527683e-18 4.3178272e-17 1.0000000e+00
 3.3961113e-12 9.6192027e-19], sum to 1.0000
[2019-04-04 12:37:27,967] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5250
[2019-04-04 12:37:27,981] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.9, 49.66666666666666, 201.6666666666667, 520.6666666666666, 26.0, 26.97731488654789, 0.8264786568179509, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4632000.0000, 
sim time next is 4632600.0000, 
raw observation next is [4.95, 49.83333333333334, 200.3333333333333, 442.3333333333334, 26.0, 27.20460453730458, 0.8552564301664486, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5997229916897507, 0.4983333333333334, 0.6677777777777776, 0.48876611418047894, 0.6666666666666666, 0.7670503781087149, 0.7850854767221495, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5083785], dtype=float32), -0.39328054]. 
=============================================
[2019-04-04 12:37:35,713] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.2528631e-10 8.3872680e-11 1.2857501e-16 2.4627602e-16 1.0000000e+00
 3.9199748e-11 1.4915018e-16], sum to 1.0000
[2019-04-04 12:37:35,715] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7915
[2019-04-04 12:37:35,744] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.333333333333333, 29.33333333333334, 117.8333333333333, 810.0, 26.0, 26.62167868149775, 0.59806150829449, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4963200.0000, 
sim time next is 4963800.0000, 
raw observation next is [2.666666666666667, 29.16666666666667, 118.6666666666667, 817.0, 26.0, 26.68705747517865, 0.6029313547549043, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5364727608494922, 0.29166666666666674, 0.39555555555555566, 0.9027624309392265, 0.6666666666666666, 0.7239214562648876, 0.7009771182516348, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5932565], dtype=float32), -0.44148663]. 
=============================================
[2019-04-04 12:37:41,342] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.8342076e-10 3.0661668e-11 1.1777621e-15 2.0761328e-15 1.0000000e+00
 2.6232119e-11 8.2148315e-16], sum to 1.0000
[2019-04-04 12:37:41,342] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2136
[2019-04-04 12:37:41,349] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.3, 25.0, 0.0, 0.0, 26.0, 26.01536863312196, 0.5727434365154485, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5095200.0000, 
sim time next is 5095800.0000, 
raw observation next is [8.25, 27.5, 0.0, 0.0, 26.0, 25.93534602878092, 0.5558817985628172, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6911357340720222, 0.275, 0.0, 0.0, 0.6666666666666666, 0.6612788357317433, 0.6852939328542723, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23974636], dtype=float32), 1.0176852]. 
=============================================
[2019-04-04 12:37:42,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:42,138] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:42,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run5
[2019-04-04 12:37:42,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:42,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:42,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run5
[2019-04-04 12:37:43,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:43,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:43,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run5
[2019-04-04 12:37:43,683] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:43,683] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:43,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run5
[2019-04-04 12:37:43,886] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:43,887] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:43,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run5
[2019-04-04 12:37:44,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:44,016] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:44,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run5
[2019-04-04 12:37:45,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:45,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:45,727] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run5
[2019-04-04 12:37:45,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:45,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:45,972] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run5
[2019-04-04 12:37:46,208] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.1106063e-11 1.3866497e-12 4.8881633e-19 2.6552799e-17 1.0000000e+00
 2.1046583e-13 7.9419032e-19], sum to 1.0000
[2019-04-04 12:37:46,208] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0086
[2019-04-04 12:37:46,258] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.5, 17.0, 36.0, 292.0, 26.0, 28.96717235457718, 1.13156432266129, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5074200.0000, 
sim time next is 5074800.0000, 
raw observation next is [11.33333333333333, 17.0, 30.0, 243.3333333333333, 26.0, 28.51584323214242, 1.136336823787742, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7765466297322253, 0.17, 0.1, 0.2688766114180478, 0.6666666666666666, 0.8763202693452016, 0.8787789412625807, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.38478035], dtype=float32), 0.7541131]. 
=============================================
[2019-04-04 12:37:47,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:47,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:47,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run5
[2019-04-04 12:37:47,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:47,348] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:47,350] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run5
[2019-04-04 12:37:47,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:47,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:47,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run5
[2019-04-04 12:37:48,258] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:48,259] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:48,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run5
[2019-04-04 12:37:48,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:48,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:48,855] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run5
[2019-04-04 12:37:51,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:51,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:51,043] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run5
[2019-04-04 12:37:51,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:51,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:51,491] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run5
[2019-04-04 12:37:51,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:37:51,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:37:51,543] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run5
[2019-04-04 12:38:02,246] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5810792e-08 1.2767079e-09 3.7028439e-15 3.2148568e-13 1.0000000e+00
 4.2228829e-10 3.0020894e-14], sum to 1.0000
[2019-04-04 12:38:02,246] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8894
[2019-04-04 12:38:02,281] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.283333333333333, 74.16666666666667, 0.0, 0.0, 26.0, 23.63824521568807, -0.01872610740736499, 0.0, 1.0, 44221.43978230678], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 105000.0000, 
sim time next is 105600.0000, 
raw observation next is [-5.566666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 23.55295237433619, -0.03330859249740428, 0.0, 1.0, 44369.33998463253], 
processed observation next is [1.0, 0.21739130434782608, 0.3084025854108957, 0.7433333333333334, 0.0, 0.0, 0.6666666666666666, 0.4627460311946825, 0.4888971358341985, 0.0, 1.0, 0.211282571355393], 
reward next is 0.7887, 
noisyNet noise sample is [array([1.1595588], dtype=float32), 1.7804673]. 
=============================================
[2019-04-04 12:38:04,119] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4819437e-07 5.7643192e-09 2.0718781e-13 5.8274214e-12 9.9999988e-01
 1.3970456e-08 2.0847533e-13], sum to 1.0000
[2019-04-04 12:38:04,122] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9522
[2019-04-04 12:38:04,136] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 74.66666666666667, 0.0, 0.0, 26.0, 23.5047852861759, -0.06334173371652944, 0.0, 1.0, 44234.28628601265], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 180600.0000, 
sim time next is 181200.0000, 
raw observation next is [-8.900000000000002, 75.33333333333334, 0.0, 0.0, 26.0, 23.45459818424639, -0.06099688281454933, 0.0, 1.0, 44194.13633280256], 
processed observation next is [1.0, 0.08695652173913043, 0.2160664819944598, 0.7533333333333334, 0.0, 0.0, 0.6666666666666666, 0.4545498486871991, 0.47966770572848355, 0.0, 1.0, 0.21044826825144078], 
reward next is 0.7896, 
noisyNet noise sample is [array([-0.12033279], dtype=float32), 1.3356192]. 
=============================================
[2019-04-04 12:38:17,927] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.871358e-09 8.705258e-11 3.459271e-16 7.016156e-15 1.000000e+00
 2.252421e-12 7.155243e-16], sum to 1.0000
[2019-04-04 12:38:17,927] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1324
[2019-04-04 12:38:17,991] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.8, 63.66666666666666, 92.33333333333334, 426.0, 26.0, 25.7553381132805, 0.3512207666400731, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 294600.0000, 
sim time next is 295200.0000, 
raw observation next is [-11.7, 63.0, 91.0, 447.5, 26.0, 25.79450323370921, 0.3557837976212483, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.13850415512465375, 0.63, 0.30333333333333334, 0.494475138121547, 0.6666666666666666, 0.6495419361424343, 0.6185945992070828, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39196995], dtype=float32), -0.348454]. 
=============================================
[2019-04-04 12:38:23,344] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.9361532e-09 1.1474055e-10 9.5823006e-16 2.9791135e-14 1.0000000e+00
 1.4427694e-10 1.2888795e-14], sum to 1.0000
[2019-04-04 12:38:23,345] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7776
[2019-04-04 12:38:23,361] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.9, 67.33333333333334, 0.0, 0.0, 26.0, 23.6588280705121, -0.01461191188425765, 0.0, 1.0, 47224.4205613323], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 344400.0000, 
sim time next is 345000.0000, 
raw observation next is [-13.9, 66.66666666666666, 0.0, 0.0, 26.0, 23.65375149415149, -0.02664107534318358, 0.0, 1.0, 47252.0134595437], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.6666666666666665, 0.0, 0.0, 0.6666666666666666, 0.4711459578459574, 0.4911196415522721, 0.0, 1.0, 0.22500958790258904], 
reward next is 0.7750, 
noisyNet noise sample is [array([0.4769049], dtype=float32), 1.0110871]. 
=============================================
[2019-04-04 12:38:23,369] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[77.82298 ]
 [77.98527 ]
 [78.215256]
 [78.428444]
 [78.79663 ]], R is [[77.63516998]
 [77.63394165]
 [77.63291168]
 [77.63208008]
 [77.63139343]].
[2019-04-04 12:38:24,662] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0852139e-08 1.3144190e-09 2.0602668e-15 1.9009180e-13 1.0000000e+00
 3.2696643e-10 5.1864700e-15], sum to 1.0000
[2019-04-04 12:38:24,662] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7991
[2019-04-04 12:38:24,724] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.6, 90.0, 32.0, 607.5, 26.0, 25.27369705323454, 0.2104426870125358, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 378000.0000, 
sim time next is 378600.0000, 
raw observation next is [-15.41666666666667, 86.0, 34.33333333333334, 651.0, 26.0, 25.38356487944959, 0.2122684626246538, 1.0, 1.0, 18801.31329496738], 
processed observation next is [1.0, 0.391304347826087, 0.0355493998153277, 0.86, 0.11444444444444447, 0.7193370165745856, 0.6666666666666666, 0.6152970732874659, 0.5707561542082179, 1.0, 1.0, 0.08953006330936848], 
reward next is 0.9105, 
noisyNet noise sample is [array([-1.5613813], dtype=float32), 1.6440734]. 
=============================================
[2019-04-04 12:38:27,278] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.2219242e-08 1.3411062e-08 3.8879812e-13 2.4630187e-12 9.9999988e-01
 7.6009963e-09 5.9148536e-13], sum to 1.0000
[2019-04-04 12:38:27,278] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8085
[2019-04-04 12:38:27,299] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.15096849347694, -0.3633066959883511, 0.0, 1.0, 49342.57060897521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 361800.0000, 
sim time next is 362400.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.15087442606505, -0.3704086048476918, 0.0, 1.0, 49233.23269098564], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.6666666666666666, 0.3459062021720876, 0.3765304650507694, 0.0, 1.0, 0.23444396519516972], 
reward next is 0.7656, 
noisyNet noise sample is [array([-1.9216688], dtype=float32), 1.506106]. 
=============================================
[2019-04-04 12:38:28,872] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5394098e-09 4.8768428e-10 3.3011633e-15 7.9624146e-14 1.0000000e+00
 3.0253469e-10 4.0898969e-15], sum to 1.0000
[2019-04-04 12:38:28,873] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3000
[2019-04-04 12:38:28,947] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.1, 29.16666666666667, 122.3333333333333, 0.0, 26.0, 24.97024412655644, 0.04144893452784474, 1.0, 1.0, 56026.22363327091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 479400.0000, 
sim time next is 480000.0000, 
raw observation next is [-1.0, 30.33333333333334, 120.6666666666667, 0.0, 26.0, 24.97884001938641, 0.2004844300289224, 1.0, 1.0, 35589.55972803864], 
processed observation next is [1.0, 0.5652173913043478, 0.4349030470914128, 0.3033333333333334, 0.4022222222222223, 0.0, 0.6666666666666666, 0.5815700016155342, 0.5668281433429742, 1.0, 1.0, 0.16947409394304117], 
reward next is 0.8305, 
noisyNet noise sample is [array([1.5591023], dtype=float32), 0.5241761]. 
=============================================
[2019-04-04 12:38:28,966] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.77737]
 [80.54996]
 [80.26711]
 [79.96136]
 [79.72958]], R is [[80.98406982]
 [80.90744019]
 [80.7505188 ]
 [80.51200104]
 [80.46395111]].
[2019-04-04 12:38:39,028] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.21928689e-09 1.03094686e-11 1.01439190e-16 2.88917306e-15
 1.00000000e+00 1.08392574e-11 6.62481779e-16], sum to 1.0000
[2019-04-04 12:38:39,029] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1703
[2019-04-04 12:38:39,078] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 80.0, 132.5, 531.0, 26.0, 25.00247673548377, 0.3492078730480048, 0.0, 1.0, 18723.87988911812], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 568800.0000, 
sim time next is 569400.0000, 
raw observation next is [-1.2, 80.5, 130.6666666666667, 509.6666666666666, 26.0, 24.99984870270947, 0.3477121579587782, 0.0, 1.0, 25909.07190836613], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.805, 0.4355555555555557, 0.5631675874769796, 0.6666666666666666, 0.5833207252257893, 0.615904052652926, 0.0, 1.0, 0.12337653289698158], 
reward next is 0.8766, 
noisyNet noise sample is [array([0.4489229], dtype=float32), 0.57056767]. 
=============================================
[2019-04-04 12:38:40,997] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.3508498e-09 3.0661451e-10 5.1663877e-15 3.5767572e-14 1.0000000e+00
 2.5353722e-10 6.3868453e-15], sum to 1.0000
[2019-04-04 12:38:41,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0022
[2019-04-04 12:38:41,049] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.65, 88.5, 0.0, 0.0, 26.0, 25.02946468993036, 0.258586749665571, 0.0, 1.0, 39520.4821898879], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 523800.0000, 
sim time next is 524400.0000, 
raw observation next is [4.533333333333333, 88.33333333333334, 0.0, 0.0, 26.0, 24.99999602296261, 0.2528800920850799, 0.0, 1.0, 39560.24747977252], 
processed observation next is [0.0, 0.043478260869565216, 0.5881809787626964, 0.8833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5833330019135508, 0.58429336402836, 0.0, 1.0, 0.18838213085605962], 
reward next is 0.8116, 
noisyNet noise sample is [array([-1.4276724], dtype=float32), 0.4576746]. 
=============================================
[2019-04-04 12:38:41,899] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7896093e-08 2.9970852e-09 3.1567902e-14 5.1581715e-13 1.0000000e+00
 2.6473108e-09 3.0841808e-14], sum to 1.0000
[2019-04-04 12:38:41,901] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5258
[2019-04-04 12:38:41,949] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 65.0, 117.5, 25.5, 26.0, 24.99687860026904, 0.1926365632033205, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 640800.0000, 
sim time next is 641400.0000, 
raw observation next is [-3.816666666666666, 65.0, 111.6666666666667, 17.0, 26.0, 24.89732415906966, 0.1823473106649681, 0.0, 1.0, 73379.72359123467], 
processed observation next is [0.0, 0.43478260869565216, 0.3568790397045245, 0.65, 0.37222222222222234, 0.01878453038674033, 0.6666666666666666, 0.5747770132558051, 0.5607824368883226, 0.0, 1.0, 0.34942725519635554], 
reward next is 0.6506, 
noisyNet noise sample is [array([0.06245367], dtype=float32), 0.7120088]. 
=============================================
[2019-04-04 12:38:44,136] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5938613e-08 8.5414520e-09 2.0316650e-13 2.3415486e-12 1.0000000e+00
 2.8882507e-09 6.4140018e-13], sum to 1.0000
[2019-04-04 12:38:44,153] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5716
[2019-04-04 12:38:44,173] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 67.0, 0.0, 0.0, 26.0, 23.66682610379284, -0.03470029794266954, 0.0, 1.0, 43735.80492633331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 628800.0000, 
sim time next is 629400.0000, 
raw observation next is [-4.5, 67.5, 0.0, 0.0, 26.0, 23.63963961542373, -0.041940243205721, 0.0, 1.0, 43757.44572633934], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.675, 0.0, 0.0, 0.6666666666666666, 0.4699699679519774, 0.4860199189314263, 0.0, 1.0, 0.2083687891730445], 
reward next is 0.7916, 
noisyNet noise sample is [array([-1.6018242], dtype=float32), 0.24579675]. 
=============================================
[2019-04-04 12:38:47,589] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1497502e-09 1.0401059e-10 2.3676444e-17 5.2366854e-16 1.0000000e+00
 1.3233100e-11 2.2850606e-16], sum to 1.0000
[2019-04-04 12:38:47,589] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1542
[2019-04-04 12:38:47,609] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 66.0, 111.5, 423.5, 26.0, 25.87277224626731, 0.3543258215071786, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 730800.0000, 
sim time next is 731400.0000, 
raw observation next is [-0.6, 64.5, 102.3333333333333, 542.0, 26.0, 25.82353787117998, 0.3681358883002245, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.645, 0.341111111111111, 0.5988950276243094, 0.6666666666666666, 0.6519614892649983, 0.6227119627667416, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41501486], dtype=float32), 0.7732351]. 
=============================================
[2019-04-04 12:38:52,502] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.8962718e-08 3.5883005e-09 2.2059153e-13 4.2461293e-13 1.0000000e+00
 2.7955507e-10 1.3933088e-13], sum to 1.0000
[2019-04-04 12:38:52,502] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3200
[2019-04-04 12:38:52,539] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 65.0, 0.0, 0.0, 26.0, 24.9745345819202, 0.2208415033905298, 0.0, 1.0, 44502.79105527214], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 676800.0000, 
sim time next is 677400.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.97332815546941, 0.2275049760413395, 0.0, 1.0, 43633.05511476785], 
processed observation next is [0.0, 0.8695652173913043, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5811106796224509, 0.5758349920137799, 0.0, 1.0, 0.20777645292746594], 
reward next is 0.7922, 
noisyNet noise sample is [array([0.41847363], dtype=float32), 1.1883913]. 
=============================================
[2019-04-04 12:39:00,662] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6485682e-10 2.0180687e-11 6.4747827e-17 3.3024353e-15 1.0000000e+00
 2.5969166e-11 4.9610402e-16], sum to 1.0000
[2019-04-04 12:39:00,663] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0268
[2019-04-04 12:39:00,684] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.83393871725163, 0.2599977935987063, 0.0, 1.0, 41629.30195160395], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 856200.0000, 
sim time next is 856800.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.8119019171198, 0.2606117585791277, 0.0, 1.0, 41620.62847627708], 
processed observation next is [1.0, 0.9565217391304348, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5676584930933167, 0.5868705861930426, 0.0, 1.0, 0.19819346893465276], 
reward next is 0.8018, 
noisyNet noise sample is [array([0.62203765], dtype=float32), 0.16859683]. 
=============================================
[2019-04-04 12:39:05,660] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8716237e-11 8.4452416e-13 2.2773037e-19 3.0214092e-17 1.0000000e+00
 3.9497057e-13 1.3727038e-18], sum to 1.0000
[2019-04-04 12:39:05,660] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9952
[2019-04-04 12:39:05,670] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.76666666666667, 80.0, 0.0, 0.0, 26.0, 25.86486321200488, 0.5870051415170545, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1014000.0000, 
sim time next is 1014600.0000, 
raw observation next is [14.58333333333333, 80.5, 0.0, 0.0, 26.0, 25.99987632360288, 0.5776844736780745, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8665743305632503, 0.805, 0.0, 0.0, 0.6666666666666666, 0.66665636030024, 0.6925614912260248, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3929379], dtype=float32), 0.40356162]. 
=============================================
[2019-04-04 12:39:09,746] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.7433331e-09 1.2624976e-10 7.5245664e-15 2.4520760e-14 1.0000000e+00
 1.0584840e-10 3.5097293e-15], sum to 1.0000
[2019-04-04 12:39:09,748] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0173
[2019-04-04 12:39:09,755] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.56666666666667, 78.0, 39.66666666666666, 0.0, 26.0, 25.73413718764276, 0.6198122946935162, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1154400.0000, 
sim time next is 1155000.0000, 
raw observation next is [15.03333333333333, 76.5, 48.33333333333333, 0.0, 26.0, 25.73913514040438, 0.615384641408251, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.879039704524469, 0.765, 0.1611111111111111, 0.0, 0.6666666666666666, 0.6449279283670316, 0.7051282138027504, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17957045], dtype=float32), -0.6947199]. 
=============================================
[2019-04-04 12:39:09,778] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.607994]
 [80.40866 ]
 [80.239784]
 [80.05252 ]
 [79.85275 ]], R is [[80.94644165]
 [81.13697815]
 [81.3256073 ]
 [81.51235199]
 [81.53205872]].
[2019-04-04 12:39:10,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8173170e-08 6.1884728e-09 8.9505101e-13 5.5585718e-12 1.0000000e+00
 8.1451623e-10 2.3761771e-13], sum to 1.0000
[2019-04-04 12:39:10,672] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0804
[2019-04-04 12:39:10,680] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.55, 64.0, 171.0, 0.0, 26.0, 25.08461786944801, 0.5048903524745961, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1168200.0000, 
sim time next is 1168800.0000, 
raw observation next is [18.46666666666667, 64.33333333333333, 169.0, 0.0, 26.0, 25.09732634811847, 0.5046287555522806, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.9741458910433982, 0.6433333333333333, 0.5633333333333334, 0.0, 0.6666666666666666, 0.591443862343206, 0.6682095851840936, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01677799], dtype=float32), -1.2630084]. 
=============================================
[2019-04-04 12:39:11,487] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8121605e-09 1.4034562e-11 3.0115131e-17 4.8311668e-16 1.0000000e+00
 9.7282824e-12 6.7463607e-17], sum to 1.0000
[2019-04-04 12:39:11,489] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6642
[2019-04-04 12:39:11,540] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.7, 92.5, 27.0, 0.0, 26.0, 25.71156757701782, 0.5083115925931501, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 981000.0000, 
sim time next is 981600.0000, 
raw observation next is [9.8, 92.33333333333333, 32.5, 0.0, 26.0, 25.92291039152164, 0.5361059219994951, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7340720221606649, 0.9233333333333333, 0.10833333333333334, 0.0, 0.6666666666666666, 0.6602425326268033, 0.6787019739998317, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1260921], dtype=float32), -0.7383429]. 
=============================================
[2019-04-04 12:39:17,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4962731e-10 2.5486301e-12 5.4005845e-17 8.4450032e-16 1.0000000e+00
 3.2098467e-12 3.2794207e-17], sum to 1.0000
[2019-04-04 12:39:17,408] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3898
[2019-04-04 12:39:17,419] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.0, 67.66666666666667, 0.0, 0.0, 26.0, 25.71078456456397, 0.6693005270058027, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1120800.0000, 
sim time next is 1121400.0000, 
raw observation next is [11.9, 68.5, 0.0, 0.0, 26.0, 25.73737954084573, 0.6660459058190621, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.7922437673130196, 0.685, 0.0, 0.0, 0.6666666666666666, 0.6447816284038108, 0.7220153019396873, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.7399576], dtype=float32), 0.15096311]. 
=============================================
[2019-04-04 12:39:20,494] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.3561630e-07 1.7338546e-07 3.1878070e-12 9.9001564e-11 9.9999940e-01
 3.1232890e-08 2.6113891e-11], sum to 1.0000
[2019-04-04 12:39:20,502] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8756
[2019-04-04 12:39:20,511] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.54170756066547, 0.1569828890310901, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1233000.0000, 
sim time next is 1233600.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.52424913057878, 0.1524589959520403, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.4603540942148984, 0.5508196653173468, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3174082], dtype=float32), 0.7101973]. 
=============================================
[2019-04-04 12:39:20,728] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6025168e-07 5.7713727e-08 1.6650363e-12 4.5179582e-11 9.9999964e-01
 8.6745020e-09 2.0575451e-11], sum to 1.0000
[2019-04-04 12:39:20,733] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6823
[2019-04-04 12:39:20,739] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 26.0, 24.57968318887937, 0.378775112881111, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1196400.0000, 
sim time next is 1197000.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 26.0, 24.57666596894815, 0.3754416758110799, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.9529085872576178, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5480554974123457, 0.6251472252703599, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2581359], dtype=float32), -0.54822564]. 
=============================================
[2019-04-04 12:39:20,744] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[62.030323]
 [62.06203 ]
 [62.06314 ]
 [62.0557  ]
 [62.09062 ]], R is [[62.39545441]
 [62.77149963]
 [63.14378357]
 [63.51234818]
 [63.87722397]].
[2019-04-04 12:39:24,734] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2408734e-10 5.7594866e-12 4.5362605e-17 1.1224147e-15 1.0000000e+00
 2.8876899e-12 1.5599465e-17], sum to 1.0000
[2019-04-04 12:39:24,734] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1830
[2019-04-04 12:39:24,769] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 18.0, 0.0, 26.0, 25.8060692894018, 0.4981458379745059, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1441800.0000, 
sim time next is 1442400.0000, 
raw observation next is [1.1, 92.0, 15.0, 0.0, 26.0, 25.83594540929954, 0.499686654517546, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.05, 0.0, 0.6666666666666666, 0.6529954507749617, 0.6665622181725154, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47827867], dtype=float32), -0.2707193]. 
=============================================
[2019-04-04 12:39:30,815] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0487386e-11 2.3177497e-12 3.0725138e-18 1.9238681e-17 1.0000000e+00
 8.9548133e-13 2.8742074e-18], sum to 1.0000
[2019-04-04 12:39:30,816] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7211
[2019-04-04 12:39:30,829] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.7, 51.66666666666667, 76.33333333333333, 539.6666666666666, 26.0, 25.99701522800559, 0.6898715776024531, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1523400.0000, 
sim time next is 1524000.0000, 
raw observation next is [11.8, 51.33333333333334, 76.66666666666667, 508.8333333333334, 26.0, 26.4355065757437, 0.7178439252860063, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7894736842105264, 0.5133333333333334, 0.2555555555555556, 0.5622467771639044, 0.6666666666666666, 0.7029588813119751, 0.7392813084286688, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5540444], dtype=float32), -0.7647036]. 
=============================================
[2019-04-04 12:39:30,839] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[102.74183 ]
 [103.0903  ]
 [103.315834]
 [103.61094 ]
 [103.81529 ]], R is [[102.33348083]
 [102.31015015]
 [102.28704834]
 [102.26417542]
 [102.24153137]].
[2019-04-04 12:39:31,668] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7054052e-11 2.6027271e-12 1.0683377e-17 2.2584389e-16 1.0000000e+00
 2.3526420e-12 5.2516264e-17], sum to 1.0000
[2019-04-04 12:39:31,669] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1628
[2019-04-04 12:39:31,684] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.416666666666667, 82.00000000000001, 0.0, 0.0, 26.0, 25.52724604171061, 0.5793613053033965, 0.0, 1.0, 51370.32025630781], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1552200.0000, 
sim time next is 1552800.0000, 
raw observation next is [5.333333333333334, 82.0, 0.0, 0.0, 26.0, 25.58434253957916, 0.5739347260831248, 0.0, 1.0, 18739.46116390277], 
processed observation next is [1.0, 1.0, 0.6103416435826409, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6320285449649301, 0.6913115753610416, 0.0, 1.0, 0.08923552935191796], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.14294527], dtype=float32), -0.7912235]. 
=============================================
[2019-04-04 12:39:32,913] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.2181554e-10 4.7404719e-12 7.3097603e-18 4.3169728e-16 1.0000000e+00
 1.5201102e-12 2.8111859e-17], sum to 1.0000
[2019-04-04 12:39:32,915] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7827
[2019-04-04 12:39:32,934] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.616666666666667, 73.83333333333334, 0.0, 0.0, 26.0, 25.30884928628886, 0.6306613415343977, 0.0, 1.0, 173759.826239571], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1543800.0000, 
sim time next is 1544400.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 26.0, 25.45993267826132, 0.6886914616701819, 0.0, 1.0, 42377.96746876843], 
processed observation next is [1.0, 0.9130434782608695, 0.6759002770083103, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6216610565217767, 0.7295638205567273, 0.0, 1.0, 0.2017998450893735], 
reward next is 0.7982, 
noisyNet noise sample is [array([-0.26956537], dtype=float32), 0.032664273]. 
=============================================
[2019-04-04 12:39:34,795] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2579376e-10 2.1051182e-11 2.3693621e-16 1.8539546e-15 1.0000000e+00
 8.7073126e-12 7.2467394e-17], sum to 1.0000
[2019-04-04 12:39:34,797] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8407
[2019-04-04 12:39:34,833] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.8, 79.0, 37.0, 35.0, 26.0, 25.82594876051554, 0.5630202142607871, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1585800.0000, 
sim time next is 1586400.0000, 
raw observation next is [6.066666666666666, 78.0, 50.0, 51.83333333333333, 26.0, 26.05913490619497, 0.58364442266758, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6306555863342568, 0.78, 0.16666666666666666, 0.057274401473296495, 0.6666666666666666, 0.6715945755162475, 0.6945481408891934, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1998364], dtype=float32), -0.005690758]. 
=============================================
[2019-04-04 12:39:37,680] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6112772e-11 1.2473857e-12 3.9757642e-18 2.1146583e-16 1.0000000e+00
 7.2360640e-13 1.4574175e-17], sum to 1.0000
[2019-04-04 12:39:37,682] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7490
[2019-04-04 12:39:37,702] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.433333333333333, 92.0, 61.66666666666667, 0.0, 26.0, 26.02457716645505, 0.554317938209158, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1678200.0000, 
sim time next is 1678800.0000, 
raw observation next is [1.366666666666667, 92.0, 63.83333333333333, 0.0, 26.0, 25.99143469938765, 0.5521466954001494, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5004616805170823, 0.92, 0.21277777777777776, 0.0, 0.6666666666666666, 0.6659528916156375, 0.6840488984667165, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30323374], dtype=float32), 0.028924664]. 
=============================================
[2019-04-04 12:39:39,481] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2078708e-09 9.8317743e-12 7.7276331e-17 1.9819290e-15 1.0000000e+00
 3.8975693e-12 4.5089847e-17], sum to 1.0000
[2019-04-04 12:39:39,485] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6462
[2019-04-04 12:39:39,501] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.2, 82.0, 0.0, 0.0, 26.0, 25.5898401825248, 0.583613518286071, 0.0, 1.0, 93418.84051648348], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1638000.0000, 
sim time next is 1638600.0000, 
raw observation next is [7.2, 82.66666666666667, 0.0, 0.0, 26.0, 25.53142472217041, 0.6088248626438326, 0.0, 1.0, 91388.5079616094], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6276187268475342, 0.7029416208812775, 0.0, 1.0, 0.43518337124575907], 
reward next is 0.5648, 
noisyNet noise sample is [array([-0.9180976], dtype=float32), -0.30975792]. 
=============================================
[2019-04-04 12:39:47,818] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.9227444e-09 1.0478103e-09 3.7889718e-14 5.3917926e-14 1.0000000e+00
 1.1409022e-10 5.7135157e-14], sum to 1.0000
[2019-04-04 12:39:47,818] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5871
[2019-04-04 12:39:47,889] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.2, 82.5, 0.0, 0.0, 26.0, 25.00170275430253, 0.3164938867280267, 0.0, 1.0, 49356.57451267962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1794600.0000, 
sim time next is 1795200.0000, 
raw observation next is [-4.3, 82.66666666666666, 0.0, 0.0, 26.0, 25.00334314178462, 0.316125208470447, 0.0, 1.0, 49064.18551204998], 
processed observation next is [0.0, 0.782608695652174, 0.34349030470914127, 0.8266666666666665, 0.0, 0.0, 0.6666666666666666, 0.5836119284820516, 0.6053750694901491, 0.0, 1.0, 0.23363897862880945], 
reward next is 0.7664, 
noisyNet noise sample is [array([1.6097519], dtype=float32), 0.32957232]. 
=============================================
[2019-04-04 12:39:56,710] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.3566464e-10 2.1014033e-11 1.6999139e-17 2.6095332e-15 1.0000000e+00
 4.3097522e-11 5.7626177e-16], sum to 1.0000
[2019-04-04 12:39:56,711] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9329
[2019-04-04 12:39:56,795] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.233333333333333, 79.0, 0.0, 0.0, 26.0, 24.97702191836166, 0.3044058963301706, 1.0, 1.0, 156878.4678978423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1971600.0000, 
sim time next is 1972200.0000, 
raw observation next is [-5.416666666666667, 81.0, 0.0, 0.0, 26.0, 24.86444507978598, 0.3400064603824755, 0.0, 1.0, 176696.9340538022], 
processed observation next is [1.0, 0.8260869565217391, 0.3125577100646353, 0.81, 0.0, 0.0, 0.6666666666666666, 0.5720370899821651, 0.6133354867941585, 0.0, 1.0, 0.8414139716847724], 
reward next is 0.1586, 
noisyNet noise sample is [array([-0.43267363], dtype=float32), -1.0377964]. 
=============================================
[2019-04-04 12:39:59,086] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.0853384e-08 7.0666757e-09 5.9327482e-14 1.8617889e-12 1.0000000e+00
 9.9476827e-09 3.7791797e-13], sum to 1.0000
[2019-04-04 12:39:59,086] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6196
[2019-04-04 12:39:59,107] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 26.0, 24.11044945718922, -0.009860974433957462, 0.0, 1.0, 44984.00633612327], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1911600.0000, 
sim time next is 1912200.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.99076460792798, -0.01569326071242118, 0.0, 1.0, 44824.11679677207], 
processed observation next is [1.0, 0.13043478260869565, 0.2299168975069252, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4992303839939982, 0.49476891309585963, 0.0, 1.0, 0.21344817522272413], 
reward next is 0.7866, 
noisyNet noise sample is [array([0.2621203], dtype=float32), 0.5763164]. 
=============================================
[2019-04-04 12:39:59,155] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1294470e-09 9.3586319e-12 3.3320444e-16 2.7256749e-15 1.0000000e+00
 3.9241114e-11 4.6357838e-16], sum to 1.0000
[2019-04-04 12:39:59,158] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8058
[2019-04-04 12:39:59,174] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.405637359944, 0.1705307826935973, 0.0, 1.0, 42430.46455786597], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1988400.0000, 
sim time next is 1989000.0000, 
raw observation next is [-5.9, 85.0, 0.0, 0.0, 26.0, 24.37618060937896, 0.1664459118544107, 0.0, 1.0, 42325.2008447865], 
processed observation next is [1.0, 0.0, 0.2991689750692521, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5313483841149133, 0.5554819706181369, 0.0, 1.0, 0.2015485754513643], 
reward next is 0.7985, 
noisyNet noise sample is [array([-1.0610083], dtype=float32), -1.3072993]. 
=============================================
[2019-04-04 12:39:59,187] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.77119]
 [81.17747]
 [81.7494 ]
 [82.37647]
 [82.48696]], R is [[80.32944489]
 [80.32410431]
 [80.31829834]
 [80.31201172]
 [80.30530548]].
[2019-04-04 12:40:02,741] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4435104e-09 1.8450917e-09 1.2467011e-15 1.1121273e-13 1.0000000e+00
 7.5043013e-11 8.4345440e-15], sum to 1.0000
[2019-04-04 12:40:02,741] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8735
[2019-04-04 12:40:02,769] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.34272012780169, 0.1253028739941154, 0.0, 1.0, 41355.13466663202], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1999200.0000, 
sim time next is 1999800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.381207653364, 0.1178964925241744, 0.0, 1.0, 41283.30922350926], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.531767304447, 0.5392988308413914, 0.0, 1.0, 0.19658718677861553], 
reward next is 0.8034, 
noisyNet noise sample is [array([1.2229693], dtype=float32), 0.07772304]. 
=============================================
[2019-04-04 12:40:05,314] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2753603e-08 1.9392461e-09 8.2200084e-15 2.2531890e-13 1.0000000e+00
 4.2125697e-10 4.9835312e-14], sum to 1.0000
[2019-04-04 12:40:05,314] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7005
[2019-04-04 12:40:05,425] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.9, 78.33333333333334, 0.0, 0.0, 26.0, 23.73381054211688, 0.09466527143197044, 1.0, 1.0, 202393.2886562137], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2100000.0000, 
sim time next is 2100600.0000, 
raw observation next is [-7.0, 78.5, 0.0, 0.0, 26.0, 24.14464258604696, 0.1866143690009765, 1.0, 1.0, 202918.6153903155], 
processed observation next is [1.0, 0.30434782608695654, 0.2686980609418283, 0.785, 0.0, 0.0, 0.6666666666666666, 0.5120535488372466, 0.5622047896669922, 1.0, 1.0, 0.9662791209062643], 
reward next is 0.0337, 
noisyNet noise sample is [array([1.5529644], dtype=float32), -1.4236944]. 
=============================================
[2019-04-04 12:40:05,520] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0521877e-10 1.8576751e-11 1.8495370e-18 3.6080806e-16 1.0000000e+00
 1.2122063e-11 9.6155759e-18], sum to 1.0000
[2019-04-04 12:40:05,521] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7870
[2019-04-04 12:40:05,562] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 82.00000000000001, 45.0, 0.0, 26.0, 25.86806821967293, 0.4128241152655052, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2045400.0000, 
sim time next is 2046000.0000, 
raw observation next is [-3.9, 82.0, 38.5, 0.0, 26.0, 25.84881735611472, 0.4039286059210323, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.12833333333333333, 0.0, 0.6666666666666666, 0.6540681130095599, 0.6346428686403441, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3113405], dtype=float32), -0.9001672]. 
=============================================
[2019-04-04 12:40:05,577] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[90.24545 ]
 [90.65133 ]
 [90.731804]
 [90.91525 ]
 [90.664024]], R is [[89.93227386]
 [90.03295135]
 [90.13262177]
 [90.23129272]
 [89.38459778]].
[2019-04-04 12:40:06,807] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.7286389e-10 1.5264688e-10 2.3349244e-17 5.1622878e-16 1.0000000e+00
 3.2455655e-12 6.3709001e-17], sum to 1.0000
[2019-04-04 12:40:06,807] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1029
[2019-04-04 12:40:06,854] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.466666666666667, 77.33333333333334, 222.1666666666667, 66.83333333333333, 26.0, 25.79898728470677, 0.3892468128944826, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2112000.0000, 
sim time next is 2112600.0000, 
raw observation next is [-7.383333333333333, 76.16666666666666, 236.3333333333333, 73.66666666666666, 26.0, 25.77627183665338, 0.388040841973438, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.25807940904893817, 0.7616666666666666, 0.7877777777777776, 0.08139963167587476, 0.6666666666666666, 0.6480226530544483, 0.6293469473244794, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.61239785], dtype=float32), 0.3463362]. 
=============================================
[2019-04-04 12:40:08,319] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6358178e-09 3.5236952e-11 5.1900297e-16 2.8306647e-16 1.0000000e+00
 1.1930099e-11 3.5842090e-17], sum to 1.0000
[2019-04-04 12:40:08,321] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5677
[2019-04-04 12:40:08,391] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 78.33333333333334, 153.3333333333333, 0.0, 26.0, 25.32275724778901, 0.3100823980901157, 1.0, 1.0, 35237.51937499521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2033400.0000, 
sim time next is 2034000.0000, 
raw observation next is [-4.5, 79.0, 152.0, 0.0, 26.0, 25.36736161275193, 0.2169407446237221, 1.0, 1.0, 32062.39150060414], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.79, 0.5066666666666667, 0.0, 0.6666666666666666, 0.613946801062661, 0.5723135815412407, 1.0, 1.0, 0.1526780547647816], 
reward next is 0.8473, 
noisyNet noise sample is [array([-1.8686326], dtype=float32), -0.5839471]. 
=============================================
[2019-04-04 12:40:08,395] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.10467 ]
 [89.937874]
 [89.75242 ]
 [89.56356 ]
 [89.42898 ]], R is [[90.18901062]
 [90.11932373]
 [90.01173401]
 [89.86127472]
 [89.96266174]].
[2019-04-04 12:40:10,454] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 12:40:10,455] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 12:40:10,455] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:40:10,456] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 12:40:10,456] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 12:40:10,456] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:40:10,456] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:40:10,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run7
[2019-04-04 12:40:10,473] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run7
[2019-04-04 12:40:10,486] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run7
[2019-04-04 12:40:45,786] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.14972538], dtype=float32), 0.16864273]
[2019-04-04 12:40:45,787] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [14.71442868, 72.73527936, 0.0, 0.0, 26.0, 25.87510965648086, 0.5906729664245113, 0.0, 1.0, 0.0]
[2019-04-04 12:40:45,787] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 12:40:45,788] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.00322536e-09 1.11677778e-10 7.35076103e-16 1.03663585e-14
 1.00000000e+00 4.61018931e-11 1.00438118e-15], sampled 0.27972029813048704
[2019-04-04 12:40:46,886] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.14972538], dtype=float32), 0.16864273]
[2019-04-04 12:40:46,886] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [11.04321078, 60.4855455, 0.0, 0.0, 26.0, 25.60088747824604, 0.634543256474265, 0.0, 1.0, 36395.21893683517]
[2019-04-04 12:40:46,887] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 12:40:46,888] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.4692289e-09 1.0234207e-09 1.1563891e-14 1.2187799e-13 1.0000000e+00
 2.9590383e-10 1.7577289e-14], sampled 0.6084641337492589
[2019-04-04 12:40:54,693] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.14972538], dtype=float32), 0.16864273]
[2019-04-04 12:40:54,693] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [5.489615024500001, 78.895094135, 0.0, 0.0, 26.0, 25.71716275618532, 0.5218036604594294, 0.0, 1.0, 0.0]
[2019-04-04 12:40:54,693] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 12:40:54,694] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.1779002e-09 1.6240687e-10 2.1576591e-15 1.8543764e-14 1.0000000e+00
 6.4128709e-11 3.7644994e-15], sampled 0.9819107578493832
[2019-04-04 12:40:56,846] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14972538], dtype=float32), 0.16864273]
[2019-04-04 12:40:56,846] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [12.66666666666667, 71.5, 146.3333333333333, 280.3333333333334, 26.0, 25.63418793549112, 0.623150668053546, 0.0, 1.0, 0.0]
[2019-04-04 12:40:56,846] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 12:40:56,847] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.7578340e-09 1.1464845e-10 4.3837510e-16 6.4201887e-15 1.0000000e+00
 2.3757307e-11 8.7420746e-16], sampled 0.7798583274298843
[2019-04-04 12:41:51,745] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 12:42:13,246] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.8201 263377775.6296 1552.0399
[2019-04-04 12:42:15,491] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5924 275815597.0423 1233.3720
[2019-04-04 12:42:16,514] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 600000, evaluation results [600000.0, 7241.820116049566, 263377775.62958866, 1552.0399168798824, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.592395036608, 275815597.04230845, 1233.372046191091]
[2019-04-04 12:42:22,802] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1914950e-08 6.5960903e-10 6.5447178e-15 9.0176605e-14 1.0000000e+00
 5.7038479e-10 7.2240043e-15], sum to 1.0000
[2019-04-04 12:42:22,802] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7838
[2019-04-04 12:42:22,810] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 26.0, 24.0013681410062, 0.05442001833149528, 0.0, 1.0, 41985.49149604121], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2178000.0000, 
sim time next is 2178600.0000, 
raw observation next is [-6.2, 75.66666666666667, 0.0, 0.0, 26.0, 23.97494249009321, 0.04286466264166387, 0.0, 1.0, 41960.29159538803], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.4979118741744341, 0.5142882208805546, 0.0, 1.0, 0.19981091235899062], 
reward next is 0.8002, 
noisyNet noise sample is [array([0.47879228], dtype=float32), -0.36669204]. 
=============================================
[2019-04-04 12:42:34,172] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6254984e-08 4.1459649e-09 5.2384713e-13 5.2013775e-12 1.0000000e+00
 1.2945049e-09 2.3825532e-13], sum to 1.0000
[2019-04-04 12:42:34,172] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3453
[2019-04-04 12:42:34,184] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.6964143628705, 0.2380238101315368, 0.0, 1.0, 38831.09888728323], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2339400.0000, 
sim time next is 2340000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.70190822272131, 0.2351848978657849, 0.0, 1.0, 38909.75849395445], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5584923518934426, 0.5783949659552616, 0.0, 1.0, 0.18528456425692594], 
reward next is 0.8147, 
noisyNet noise sample is [array([1.4927144], dtype=float32), -0.026806727]. 
=============================================
[2019-04-04 12:42:34,209] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[70.73258 ]
 [71.324005]
 [71.69869 ]
 [72.35703 ]
 [72.668236]], R is [[70.19962311]
 [70.31271362]
 [70.42520142]
 [70.53708649]
 [70.64804077]].
[2019-04-04 12:42:34,264] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3310963e-08 7.2638864e-09 2.0865037e-13 5.6682688e-12 9.9999988e-01
 5.2564877e-09 1.1120726e-12], sum to 1.0000
[2019-04-04 12:42:34,264] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4819
[2019-04-04 12:42:34,276] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.69102365494408, 0.2397118679276278, 0.0, 1.0, 38992.9378765277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2340600.0000, 
sim time next is 2341200.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.74856987618876, 0.2363332613616806, 0.0, 1.0, 39071.34735222619], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.56238082301573, 0.5787777537872268, 0.0, 1.0, 0.18605403501060092], 
reward next is 0.8139, 
noisyNet noise sample is [array([-0.26299325], dtype=float32), -1.3246434]. 
=============================================
[2019-04-04 12:42:51,776] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3870748e-08 9.8308084e-10 1.0006492e-14 3.5801375e-13 1.0000000e+00
 4.6208878e-10 2.2502416e-14], sum to 1.0000
[2019-04-04 12:42:51,777] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3281
[2019-04-04 12:42:51,791] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.58333333333333, 81.83333333333334, 0.0, 0.0, 26.0, 23.96966267230423, 0.08782126841914435, 0.0, 1.0, 44478.09683173716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2688600.0000, 
sim time next is 2689200.0000, 
raw observation next is [-12.9, 83.0, 0.0, 0.0, 26.0, 23.91404964319495, 0.07046155879200171, 0.0, 1.0, 44496.80085001457], 
processed observation next is [1.0, 0.13043478260869565, 0.10526315789473682, 0.83, 0.0, 0.0, 0.6666666666666666, 0.49283747026624586, 0.5234871862640006, 0.0, 1.0, 0.21188952785721227], 
reward next is 0.7881, 
noisyNet noise sample is [array([0.3968549], dtype=float32), 1.1443542]. 
=============================================
[2019-04-04 12:42:52,517] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0423031e-09 2.6850419e-10 3.0852735e-15 1.1721653e-13 1.0000000e+00
 4.2332596e-11 1.5643220e-15], sum to 1.0000
[2019-04-04 12:42:52,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5809
[2019-04-04 12:42:52,549] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.04999999999999999, 35.5, 0.0, 0.0, 26.0, 25.19652034829296, 0.3268468105454008, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2572200.0000, 
sim time next is 2572800.0000, 
raw observation next is [-0.2333333333333333, 35.66666666666667, 0.0, 0.0, 26.0, 25.07734371042526, 0.3170262828436811, 1.0, 1.0, 54573.86105268296], 
processed observation next is [1.0, 0.782608695652174, 0.456140350877193, 0.3566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5897786425354384, 0.6056754276145604, 1.0, 1.0, 0.2598755288222998], 
reward next is 0.7401, 
noisyNet noise sample is [array([0.21103528], dtype=float32), 0.722617]. 
=============================================
[2019-04-04 12:42:58,922] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.5080078e-08 2.3819311e-09 2.9395238e-14 2.0057141e-13 1.0000000e+00
 3.3629910e-09 7.6697898e-14], sum to 1.0000
[2019-04-04 12:42:58,924] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6671
[2019-04-04 12:42:58,943] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 26.0, 24.48126200918618, 0.140989198123086, 0.0, 1.0, 40847.32819530793], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2781600.0000, 
sim time next is 2782200.0000, 
raw observation next is [-6.833333333333334, 63.16666666666666, 0.0, 0.0, 26.0, 24.44077206906452, 0.1289856885764653, 0.0, 1.0, 40925.00477668494], 
processed observation next is [1.0, 0.17391304347826086, 0.27331486611265005, 0.6316666666666666, 0.0, 0.0, 0.6666666666666666, 0.5367310057553766, 0.5429952295254884, 0.0, 1.0, 0.19488097512707114], 
reward next is 0.8051, 
noisyNet noise sample is [array([-1.3843703], dtype=float32), 0.5523839]. 
=============================================
[2019-04-04 12:42:59,885] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9762265e-10 9.1444031e-12 4.5908685e-18 7.1351786e-17 1.0000000e+00
 4.7063542e-12 4.0559293e-17], sum to 1.0000
[2019-04-04 12:42:59,887] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7082
[2019-04-04 12:42:59,916] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.333333333333334, 31.66666666666667, 227.1666666666667, 144.1666666666667, 26.0, 25.99380895158565, 0.3212833970150971, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2814000.0000, 
sim time next is 2814600.0000, 
raw observation next is [5.666666666666666, 30.83333333333333, 205.3333333333333, 115.3333333333333, 26.0, 25.91547168170512, 0.4434825810809295, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6195752539242845, 0.3083333333333333, 0.6844444444444443, 0.12744014732965006, 0.6666666666666666, 0.6596226401420934, 0.6478275270269765, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9360427], dtype=float32), -0.61220425]. 
=============================================
[2019-04-04 12:43:01,697] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7782016e-08 3.1662413e-09 1.2240170e-13 9.5700043e-13 1.0000000e+00
 8.2075585e-10 1.2213723e-13], sum to 1.0000
[2019-04-04 12:43:01,700] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1643
[2019-04-04 12:43:01,733] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.70036789498669, 0.2112592691355217, 0.0, 1.0, 41495.81440800427], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2773200.0000, 
sim time next is 2773800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.63610436253457, 0.2054145795092158, 0.0, 1.0, 41387.06519730682], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5530086968778809, 0.5684715265030719, 0.0, 1.0, 0.1970812628443182], 
reward next is 0.8029, 
noisyNet noise sample is [array([0.63180053], dtype=float32), 0.32249996]. 
=============================================
[2019-04-04 12:43:08,839] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2845823e-10 6.3485515e-11 9.5869029e-17 1.6806238e-15 1.0000000e+00
 1.1165834e-11 7.9472754e-17], sum to 1.0000
[2019-04-04 12:43:08,850] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5587
[2019-04-04 12:43:08,874] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6666666666666666, 95.33333333333334, 79.5, 0.0, 26.0, 25.42633989319921, 0.3133047198424763, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2889600.0000, 
sim time next is 2890200.0000, 
raw observation next is [0.8333333333333334, 94.16666666666666, 81.0, 0.0, 26.0, 25.44109148297376, 0.3165273749199371, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.4856879039704525, 0.9416666666666665, 0.27, 0.0, 0.6666666666666666, 0.62009095691448, 0.6055091249733123, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.8628812], dtype=float32), -1.3577033]. 
=============================================
[2019-04-04 12:43:09,875] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2967881e-09 1.0776883e-10 1.0343245e-15 6.9264311e-15 1.0000000e+00
 1.6226752e-10 3.4124429e-15], sum to 1.0000
[2019-04-04 12:43:09,877] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4577
[2019-04-04 12:43:09,948] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 193.5, 623.1666666666667, 26.0, 25.0476130719808, 0.386681984929447, 0.0, 1.0, 20944.1395616923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2982000.0000, 
sim time next is 2982600.0000, 
raw observation next is [-3.0, 65.0, 181.0, 691.0, 26.0, 25.04818707354564, 0.3923796391365241, 0.0, 1.0, 28298.04132218865], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.6033333333333334, 0.7635359116022099, 0.6666666666666666, 0.5873489227954701, 0.6307932130455081, 0.0, 1.0, 0.13475257772470786], 
reward next is 0.8652, 
noisyNet noise sample is [array([1.2658777], dtype=float32), -0.9099286]. 
=============================================
[2019-04-04 12:43:19,337] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2653854e-08 3.3456526e-09 3.7506117e-14 2.6006267e-13 1.0000000e+00
 3.1941740e-09 4.1733262e-14], sum to 1.0000
[2019-04-04 12:43:19,338] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2149
[2019-04-04 12:43:19,407] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8666666666666667, 88.66666666666666, 0.0, 0.0, 26.0, 25.00409101048785, 0.2799796775161535, 0.0, 1.0, 31522.7272346504], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3091200.0000, 
sim time next is 3091800.0000, 
raw observation next is [-0.9333333333333333, 90.33333333333334, 0.0, 0.0, 26.0, 25.00179711698852, 0.2774127679037526, 0.0, 1.0, 34697.93401853715], 
processed observation next is [0.0, 0.782608695652174, 0.4367497691597415, 0.9033333333333334, 0.0, 0.0, 0.6666666666666666, 0.5834830930823767, 0.5924709226345842, 0.0, 1.0, 0.16522825723112927], 
reward next is 0.8348, 
noisyNet noise sample is [array([0.14151376], dtype=float32), -0.20500617]. 
=============================================
[2019-04-04 12:43:21,603] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2486319e-10 1.4805840e-12 1.0543490e-18 1.3282699e-17 1.0000000e+00
 7.0874356e-13 5.4458645e-19], sum to 1.0000
[2019-04-04 12:43:21,604] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9795
[2019-04-04 12:43:21,611] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 26.76494238257215, 0.8204252162971954, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3177000.0000, 
sim time next is 3177600.0000, 
raw observation next is [4.666666666666666, 100.0, 0.0, 0.0, 26.0, 26.59149729799517, 0.8119925146855115, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5918744228993538, 1.0, 0.0, 0.0, 0.6666666666666666, 0.715958108166264, 0.7706641715618372, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28849286], dtype=float32), 1.566726]. 
=============================================
[2019-04-04 12:43:28,786] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.0168848e-10 3.7527786e-11 1.1939334e-16 8.0191494e-16 1.0000000e+00
 8.2459942e-12 9.2703124e-17], sum to 1.0000
[2019-04-04 12:43:28,795] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2259
[2019-04-04 12:43:28,810] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.333333333333333, 51.66666666666667, 19.16666666666666, 194.3333333333333, 26.0, 26.44443845851575, 0.539624588109458, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3345600.0000, 
sim time next is 3346200.0000, 
raw observation next is [-2.5, 52.5, 11.0, 133.0, 26.0, 25.71167221545306, 0.471482319031125, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.39335180055401664, 0.525, 0.03666666666666667, 0.14696132596685083, 0.6666666666666666, 0.642639351287755, 0.657160773010375, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.102234], dtype=float32), -0.7579281]. 
=============================================
[2019-04-04 12:43:39,690] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.0506739e-10 2.7966480e-11 2.0754971e-17 7.4681137e-16 1.0000000e+00
 3.4723270e-12 1.3012536e-16], sum to 1.0000
[2019-04-04 12:43:39,690] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9033
[2019-04-04 12:43:39,703] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.5, 48.5, 109.0, 764.0, 26.0, 26.54282233251485, 0.621042862178886, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3407400.0000, 
sim time next is 3408000.0000, 
raw observation next is [2.666666666666667, 48.66666666666666, 110.0, 770.6666666666667, 26.0, 26.56217862179639, 0.6347499615298526, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5364727608494922, 0.4866666666666666, 0.36666666666666664, 0.8515653775322285, 0.6666666666666666, 0.7135148851496993, 0.7115833205099508, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0620944], dtype=float32), -1.0975631]. 
=============================================
[2019-04-04 12:43:39,711] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[89.80671 ]
 [89.19647 ]
 [88.79288 ]
 [88.36571 ]
 [87.669136]], R is [[89.98934174]
 [90.08944702]
 [90.18855286]
 [90.28666687]
 [90.38380432]].
[2019-04-04 12:43:47,307] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.46254350e-09 2.54501864e-10 2.47052427e-15 1.32783375e-14
 1.00000000e+00 5.66941841e-11 6.98640356e-15], sum to 1.0000
[2019-04-04 12:43:47,309] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9351
[2019-04-04 12:43:47,350] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 59.5, 111.0, 758.0, 26.0, 25.57124844329383, 0.4816605553540442, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3580200.0000, 
sim time next is 3580800.0000, 
raw observation next is [-4.333333333333334, 57.66666666666667, 111.5, 767.6666666666667, 26.0, 25.5001787406907, 0.4720153699928732, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3425669436749769, 0.5766666666666667, 0.37166666666666665, 0.8482504604051566, 0.6666666666666666, 0.6250148950575584, 0.657338456664291, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04069211], dtype=float32), 0.76438916]. 
=============================================
[2019-04-04 12:43:50,915] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0468307e-09 3.9654030e-11 1.8675264e-15 7.0351880e-14 1.0000000e+00
 3.0892369e-11 2.0140385e-15], sum to 1.0000
[2019-04-04 12:43:50,917] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7141
[2019-04-04 12:43:50,938] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.2667299339836, 0.4023148131331706, 0.0, 1.0, 44178.81176154229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3806400.0000, 
sim time next is 3807000.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.30862239525782, 0.4026604670861376, 0.0, 1.0, 44162.28474514392], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6090518662714851, 0.6342201556953792, 0.0, 1.0, 0.21029659402449488], 
reward next is 0.7897, 
noisyNet noise sample is [array([-0.44148874], dtype=float32), -0.04954192]. 
=============================================
[2019-04-04 12:43:50,954] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.689705]
 [80.64458 ]
 [80.99408 ]
 [81.401184]
 [81.70011 ]], R is [[80.39797974]
 [80.38362885]
 [80.36952972]
 [80.35595703]
 [80.34306335]].
[2019-04-04 12:43:51,457] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6317768e-08 1.4286783e-09 4.4449233e-14 2.0196106e-13 1.0000000e+00
 2.0446914e-10 3.3606933e-14], sum to 1.0000
[2019-04-04 12:43:51,460] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3082
[2019-04-04 12:43:51,468] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.0, 24.0, 113.5, 789.5, 26.0, 25.69011849031549, 0.4994559072107124, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3668400.0000, 
sim time next is 3669000.0000, 
raw observation next is [10.66666666666667, 27.5, 114.3333333333333, 798.3333333333334, 26.0, 25.69985030858938, 0.499987691503147, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.7580794090489382, 0.275, 0.381111111111111, 0.8821362799263353, 0.6666666666666666, 0.6416541923824483, 0.6666625638343823, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6393276], dtype=float32), -0.18097903]. 
=============================================
[2019-04-04 12:43:51,481] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.23183 ]
 [76.95873 ]
 [76.70795 ]
 [76.412224]
 [76.04467 ]], R is [[77.71822357]
 [77.94104004]
 [78.16162872]
 [78.38001251]
 [78.59621429]].
[2019-04-04 12:44:01,835] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.5240986e-10 8.0120147e-11 2.5023733e-16 1.3093505e-15 1.0000000e+00
 5.6541568e-12 1.8069906e-16], sum to 1.0000
[2019-04-04 12:44:01,835] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8348
[2019-04-04 12:44:01,878] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.166666666666667, 49.66666666666667, 104.3333333333333, 719.6666666666666, 26.0, 26.20877216443724, 0.5581738901622545, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3923400.0000, 
sim time next is 3924000.0000, 
raw observation next is [-7.0, 49.0, 106.5, 733.5, 26.0, 26.263336699541, 0.5654166535368005, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2686980609418283, 0.49, 0.355, 0.8104972375690608, 0.6666666666666666, 0.6886113916284167, 0.6884722178456002, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10480491], dtype=float32), -1.0387326]. 
=============================================
[2019-04-04 12:44:01,904] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[85.66512 ]
 [85.490166]
 [84.74726 ]
 [84.63646 ]
 [84.28935 ]], R is [[86.26570892]
 [86.40305328]
 [86.53902435]
 [86.67363739]
 [86.80690002]].
[2019-04-04 12:44:04,392] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2752303e-10 3.1500344e-11 2.0105359e-17 7.9672528e-16 1.0000000e+00
 7.2152982e-12 1.0687703e-16], sum to 1.0000
[2019-04-04 12:44:04,392] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5935
[2019-04-04 12:44:04,418] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 49.0, 116.5, 792.5, 26.0, 26.3081321380316, 0.5952355637693056, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3927600.0000, 
sim time next is 3928200.0000, 
raw observation next is [-6.0, 49.0, 117.6666666666667, 798.3333333333334, 26.0, 26.30544652188167, 0.6011150001613556, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.296398891966759, 0.49, 0.3922222222222223, 0.8821362799263353, 0.6666666666666666, 0.6921205434901392, 0.7003716667204518, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47456697], dtype=float32), -1.950405]. 
=============================================
[2019-04-04 12:44:08,199] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9968948e-10 1.1014615e-11 2.0401339e-17 2.0125475e-16 1.0000000e+00
 2.6556513e-12 3.2988562e-17], sum to 1.0000
[2019-04-04 12:44:08,200] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2814
[2019-04-04 12:44:08,209] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.166666666666667, 21.0, 99.0, 766.6666666666666, 26.0, 26.42599495858227, 0.6852799963803969, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4027800.0000, 
sim time next is 4028400.0000, 
raw observation next is [-2.0, 20.0, 96.5, 753.0, 26.0, 26.72075497273509, 0.7203018289818702, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.40720221606648205, 0.2, 0.32166666666666666, 0.8320441988950277, 0.6666666666666666, 0.7267295810612575, 0.7401006096606234, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0837535], dtype=float32), 0.8402984]. 
=============================================
[2019-04-04 12:44:08,646] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1472753e-08 9.6019059e-10 2.9400117e-14 2.2211735e-13 1.0000000e+00
 2.4363236e-10 5.9987104e-14], sum to 1.0000
[2019-04-04 12:44:08,649] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3415
[2019-04-04 12:44:08,682] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.833333333333334, 48.33333333333334, 0.0, 0.0, 26.0, 25.43441111698981, 0.4977141999311867, 0.0, 1.0, 59401.8877368758], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3966600.0000, 
sim time next is 3967200.0000, 
raw observation next is [-8.0, 49.0, 0.0, 0.0, 26.0, 25.41328282298794, 0.4957901517001053, 0.0, 1.0, 60124.22346659374], 
processed observation next is [1.0, 0.9565217391304348, 0.24099722991689754, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6177735685823285, 0.6652633839000351, 0.0, 1.0, 0.28630582603139876], 
reward next is 0.7137, 
noisyNet noise sample is [array([-1.6460946], dtype=float32), -0.83260566]. 
=============================================
[2019-04-04 12:44:12,126] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7298544e-10 5.1439797e-11 8.0437167e-17 1.4775652e-16 1.0000000e+00
 1.2230660e-11 1.1934820e-17], sum to 1.0000
[2019-04-04 12:44:12,126] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6950
[2019-04-04 12:44:12,148] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 30.0, 121.0, 816.0, 26.0, 26.77385466836493, 0.4096484775126956, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4102200.0000, 
sim time next is 4102800.0000, 
raw observation next is [0.3333333333333333, 29.33333333333333, 120.8333333333333, 820.1666666666666, 26.0, 26.73685844683334, 0.6360009412425386, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4718374884579871, 0.2933333333333333, 0.4027777777777777, 0.9062615101289134, 0.6666666666666666, 0.7280715372361118, 0.7120003137475129, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0931094], dtype=float32), -0.64814115]. 
=============================================
[2019-04-04 12:44:15,496] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.549837e-10 7.453888e-11 6.488902e-17 7.079845e-16 1.000000e+00
 8.618308e-11 9.108021e-16], sum to 1.0000
[2019-04-04 12:44:15,498] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5793
[2019-04-04 12:44:15,507] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 35.0, 0.0, 0.0, 26.0, 26.48127553929983, 0.6545546033638159, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4126800.0000, 
sim time next is 4127400.0000, 
raw observation next is [3.0, 35.5, 0.0, 0.0, 26.0, 26.45879661416611, 0.6189576307675075, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.355, 0.0, 0.0, 0.6666666666666666, 0.704899717847176, 0.7063192102558359, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1709934], dtype=float32), -0.79110575]. 
=============================================
[2019-04-04 12:44:16,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.45262575e-08 4.82423879e-10 8.43814815e-15 6.16515302e-14
 1.00000000e+00 1.62907909e-10 2.78164163e-14], sum to 1.0000
[2019-04-04 12:44:16,247] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2837
[2019-04-04 12:44:16,274] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.25, 73.0, 0.0, 0.0, 26.0, 25.78537593830789, 0.4748255048280678, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4307400.0000, 
sim time next is 4308000.0000, 
raw observation next is [5.199999999999999, 73.0, 0.0, 0.0, 26.0, 25.80580122204622, 0.4693013436407416, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.6066481994459835, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6504834351705183, 0.6564337812135805, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08999877], dtype=float32), 0.8372893]. 
=============================================
[2019-04-04 12:44:16,291] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.21541]
 [76.06269]
 [76.08917]
 [75.96729]
 [75.75262]], R is [[76.64363861]
 [76.8772049 ]
 [77.10843658]
 [77.33735657]
 [77.28010559]].
[2019-04-04 12:44:26,106] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.2518250e-10 2.6463590e-11 2.1603501e-17 2.7526739e-16 1.0000000e+00
 1.5034331e-11 2.9114968e-17], sum to 1.0000
[2019-04-04 12:44:26,106] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1981
[2019-04-04 12:44:26,138] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.2, 59.0, 0.0, 0.0, 26.0, 26.8629855582455, 0.8471213551343898, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4395600.0000, 
sim time next is 4396200.0000, 
raw observation next is [10.06666666666667, 59.33333333333334, 0.0, 0.0, 26.0, 26.81907421430817, 0.8357098312856458, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7414589104339798, 0.5933333333333334, 0.0, 0.0, 0.6666666666666666, 0.7349228511923475, 0.7785699437618819, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47513595], dtype=float32), 0.7335144]. 
=============================================
[2019-04-04 12:44:29,261] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.67677136e-10 1.61775766e-11 1.04255164e-16 1.45245716e-15
 1.00000000e+00 2.21188215e-12 6.21297608e-17], sum to 1.0000
[2019-04-04 12:44:29,262] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8242
[2019-04-04 12:44:29,291] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 78.0, 37.0, 27.5, 26.0, 25.50407909879952, 0.5233501422400922, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4467600.0000, 
sim time next is 4468200.0000, 
raw observation next is [0.0, 77.0, 33.0, 36.66666666666667, 26.0, 25.81165902435387, 0.5492567269228625, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.77, 0.11, 0.04051565377532229, 0.6666666666666666, 0.6509715853628224, 0.6830855756409542, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3918581], dtype=float32), -0.6241962]. 
=============================================
[2019-04-04 12:44:32,607] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0035059e-09 8.8102588e-11 4.9668720e-16 6.8485045e-15 1.0000000e+00
 8.2149426e-12 1.4697351e-15], sum to 1.0000
[2019-04-04 12:44:32,607] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5692
[2019-04-04 12:44:32,625] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.9166666666666666, 73.0, 0.0, 0.0, 26.0, 25.3735456814596, 0.4342143046213112, 0.0, 1.0, 50415.1089191789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4506600.0000, 
sim time next is 4507200.0000, 
raw observation next is [-0.9, 73.0, 0.0, 0.0, 26.0, 25.37421666558233, 0.4260970819283205, 0.0, 1.0, 45334.94409769873], 
processed observation next is [1.0, 0.17391304347826086, 0.43767313019390586, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6145180554651942, 0.6420323606427735, 0.0, 1.0, 0.21588068617951778], 
reward next is 0.7841, 
noisyNet noise sample is [array([-0.6707767], dtype=float32), -0.17699577]. 
=============================================
[2019-04-04 12:44:39,899] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4003389e-08 1.7555354e-09 1.6680780e-13 2.2700522e-12 1.0000000e+00
 6.6426438e-09 5.6096363e-14], sum to 1.0000
[2019-04-04 12:44:39,900] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8514
[2019-04-04 12:44:39,914] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.666666666666666, 88.5, 0.0, 0.0, 26.0, 24.79311843675584, 0.3015209410487842, 0.0, 1.0, 40526.19425128529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4762200.0000, 
sim time next is 4762800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.73913244433583, 0.2883634992373491, 0.0, 1.0, 40548.27411111189], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5615943703613192, 0.5961211664124497, 0.0, 1.0, 0.19308701957672328], 
reward next is 0.8069, 
noisyNet noise sample is [array([0.00272022], dtype=float32), -1.3038092]. 
=============================================
[2019-04-04 12:44:44,263] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4443521e-08 3.3553600e-09 3.4873502e-14 6.7816466e-13 1.0000000e+00
 1.7049115e-09 1.6874994e-13], sum to 1.0000
[2019-04-04 12:44:44,271] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3339
[2019-04-04 12:44:44,285] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.40878652011274, 0.2210887770015194, 0.0, 1.0, 41156.66989949167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4768800.0000, 
sim time next is 4769400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.39948879760225, 0.2106869786212927, 0.0, 1.0, 41192.67382498774], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5332907331335207, 0.5702289928737643, 0.0, 1.0, 0.19615558964279875], 
reward next is 0.8038, 
noisyNet noise sample is [array([-0.08009076], dtype=float32), -0.41753155]. 
=============================================
[2019-04-04 12:44:47,047] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0626842e-09 1.8883956e-10 5.7058977e-16 1.6806431e-15 1.0000000e+00
 4.1761195e-11 2.8041201e-15], sum to 1.0000
[2019-04-04 12:44:47,051] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8114
[2019-04-04 12:44:47,067] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 44.66666666666667, 273.5, 388.3333333333334, 26.0, 25.05578932155248, 0.3640042108716871, 0.0, 1.0, 18698.47810066418], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4886400.0000, 
sim time next is 4887000.0000, 
raw observation next is [1.7, 44.5, 272.0, 388.0, 26.0, 25.07584850788792, 0.3667072785138409, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5096952908587258, 0.445, 0.9066666666666666, 0.4287292817679558, 0.6666666666666666, 0.5896540423239932, 0.6222357595046136, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0800065], dtype=float32), -0.3588831]. 
=============================================
[2019-04-04 12:44:47,073] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.03047 ]
 [82.032104]
 [81.97816 ]
 [81.87703 ]
 [81.840096]], R is [[82.15441132]
 [82.24382782]
 [82.33191681]
 [82.35824585]
 [82.44564056]].
[2019-04-04 12:44:47,567] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2265369e-08 6.0818013e-09 5.6321504e-14 1.0079158e-12 1.0000000e+00
 3.6509935e-09 7.0720545e-13], sum to 1.0000
[2019-04-04 12:44:47,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1540
[2019-04-04 12:44:47,587] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333334, 42.0, 0.0, 0.0, 26.0, 25.49375613106555, 0.3696698827717057, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4927200.0000, 
sim time next is 4927800.0000, 
raw observation next is [0.1666666666666666, 42.5, 0.0, 0.0, 26.0, 25.6144735008429, 0.371555248432113, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.4672206832871654, 0.425, 0.0, 0.0, 0.6666666666666666, 0.6345394584035752, 0.623851749477371, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.572422], dtype=float32), 0.44246414]. 
=============================================
[2019-04-04 12:44:49,317] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9767095e-08 1.7213307e-09 7.5242831e-14 2.7052609e-13 1.0000000e+00
 5.1127108e-10 2.2164167e-13], sum to 1.0000
[2019-04-04 12:44:49,318] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2880
[2019-04-04 12:44:49,331] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.29345449288046, 0.2949968820420242, 0.0, 1.0, 54685.57491511018], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4935600.0000, 
sim time next is 4936200.0000, 
raw observation next is [-1.166666666666667, 50.0, 0.0, 0.0, 26.0, 25.24035676822307, 0.2936526625105207, 0.0, 1.0, 43873.47483789016], 
processed observation next is [1.0, 0.13043478260869565, 0.43028624192059095, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6033630640185891, 0.5978842208368402, 0.0, 1.0, 0.20892130875185788], 
reward next is 0.7911, 
noisyNet noise sample is [array([0.8438651], dtype=float32), -1.854404]. 
=============================================
[2019-04-04 12:44:55,816] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:44:55,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:44:55,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run6
[2019-04-04 12:44:56,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:44:56,389] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:44:56,418] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run6
[2019-04-04 12:44:56,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:44:56,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:44:56,758] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run6
[2019-04-04 12:44:57,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:44:57,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:44:57,378] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run6
[2019-04-04 12:44:57,742] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:44:57,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:44:57,756] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run6
[2019-04-04 12:44:57,854] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:44:57,855] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:44:57,859] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run6
[2019-04-04 12:45:00,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:45:00,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:45:00,594] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run6
[2019-04-04 12:45:00,671] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:45:00,671] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:45:00,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run6
[2019-04-04 12:45:00,695] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:45:00,696] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:45:00,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run6
[2019-04-04 12:45:01,155] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4858526e-11 1.6349825e-12 2.3231068e-18 1.0848139e-17 1.0000000e+00
 2.1040320e-13 4.7925596e-18], sum to 1.0000
[2019-04-04 12:45:01,155] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5618
[2019-04-04 12:45:01,168] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.0, 17.66666666666666, 69.33333333333333, 536.1666666666666, 26.0, 29.05445722872283, 1.207172776237873, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5071200.0000, 
sim time next is 5071800.0000, 
raw observation next is [12.0, 17.33333333333334, 62.66666666666667, 487.3333333333334, 26.0, 29.14111346716919, 1.034450969212861, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.1733333333333334, 0.2088888888888889, 0.5384898710865563, 0.6666666666666666, 0.9284261222640993, 0.8448169897376202, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49614692], dtype=float32), -0.058273643]. 
=============================================
[2019-04-04 12:45:01,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:45:01,242] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:45:01,246] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run6
[2019-04-04 12:45:01,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:45:01,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:45:01,268] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run6
[2019-04-04 12:45:01,287] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:45:01,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:45:01,296] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run6
[2019-04-04 12:45:01,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:45:01,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:45:01,826] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run6
[2019-04-04 12:45:02,644] A3C_AGENT_WORKER-Thread-17 INFO:Local step 42500, global step 679670: loss 116.2464
[2019-04-04 12:45:02,645] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 42500, global step 679670: learning rate 0.0000
[2019-04-04 12:45:02,956] A3C_AGENT_WORKER-Thread-14 INFO:Local step 42500, global step 679732: loss 115.6126
[2019-04-04 12:45:02,962] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 42500, global step 679732: learning rate 0.0000
[2019-04-04 12:45:03,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:45:03,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:45:03,128] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run6
[2019-04-04 12:45:03,160] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:45:03,161] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:45:03,175] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run6
[2019-04-04 12:45:03,288] A3C_AGENT_WORKER-Thread-15 INFO:Local step 42500, global step 679778: loss 116.1327
[2019-04-04 12:45:03,289] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 42500, global step 679778: learning rate 0.0000
[2019-04-04 12:45:04,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:45:04,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:45:04,130] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run6
[2019-04-04 12:45:04,683] A3C_AGENT_WORKER-Thread-13 INFO:Local step 42500, global step 679892: loss 116.0694
[2019-04-04 12:45:04,683] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 42500, global step 679892: learning rate 0.0000
[2019-04-04 12:45:04,763] A3C_AGENT_WORKER-Thread-18 INFO:Local step 42500, global step 679902: loss 120.6742
[2019-04-04 12:45:04,764] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 42500, global step 679902: learning rate 0.0000
[2019-04-04 12:45:05,396] A3C_AGENT_WORKER-Thread-2 INFO:Local step 42500, global step 679994: loss 123.2360
[2019-04-04 12:45:05,397] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 42500, global step 679994: learning rate 0.0000
[2019-04-04 12:45:08,488] A3C_AGENT_WORKER-Thread-20 INFO:Local step 42500, global step 680257: loss 117.2858
[2019-04-04 12:45:08,488] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 42500, global step 680257: learning rate 0.0000
[2019-04-04 12:45:08,892] A3C_AGENT_WORKER-Thread-11 INFO:Local step 42500, global step 680334: loss 114.6400
[2019-04-04 12:45:08,895] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 42500, global step 680334: learning rate 0.0000
[2019-04-04 12:45:09,289] A3C_AGENT_WORKER-Thread-19 INFO:Local step 42500, global step 680423: loss 115.6671
[2019-04-04 12:45:09,290] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 42500, global step 680423: learning rate 0.0000
[2019-04-04 12:45:09,306] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.0907725e-09 6.3163064e-10 2.0281397e-15 1.9818781e-13 1.0000000e+00
 1.4535360e-10 2.9458645e-15], sum to 1.0000
[2019-04-04 12:45:09,309] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0794
[2019-04-04 12:45:09,355] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 86.0, 83.0, 0.0, 26.0, 24.41335654346785, 0.1497430667686561, 0.0, 1.0, 29755.42081039073], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 50400.0000, 
sim time next is 51000.0000, 
raw observation next is [7.616666666666667, 86.0, 81.66666666666666, 0.0, 26.0, 24.42513406868891, 0.1555697645237777, 0.0, 1.0, 30057.56249631051], 
processed observation next is [0.0, 0.6086956521739131, 0.6735918744228995, 0.86, 0.2722222222222222, 0.0, 0.6666666666666666, 0.5354278390574091, 0.5518565881745926, 0.0, 1.0, 0.143131249982431], 
reward next is 0.8569, 
noisyNet noise sample is [array([0.49499238], dtype=float32), -1.3167152]. 
=============================================
[2019-04-04 12:45:09,364] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.5332  ]
 [82.564926]
 [82.60972 ]
 [82.62904 ]
 [82.61477 ]], R is [[82.48554993]
 [82.51900482]
 [82.53273773]
 [82.51077271]
 [82.47288513]].
[2019-04-04 12:45:09,552] A3C_AGENT_WORKER-Thread-5 INFO:Local step 42500, global step 680512: loss 118.4351
[2019-04-04 12:45:09,553] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 42500, global step 680513: learning rate 0.0000
[2019-04-04 12:45:09,657] A3C_AGENT_WORKER-Thread-16 INFO:Local step 42500, global step 680544: loss 124.2305
[2019-04-04 12:45:09,666] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 42500, global step 680548: learning rate 0.0000
[2019-04-04 12:45:09,865] A3C_AGENT_WORKER-Thread-4 INFO:Local step 42500, global step 680609: loss 116.6857
[2019-04-04 12:45:09,866] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 42500, global step 680609: learning rate 0.0000
[2019-04-04 12:45:10,506] A3C_AGENT_WORKER-Thread-3 INFO:Local step 42500, global step 680828: loss 117.5262
[2019-04-04 12:45:10,507] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 42500, global step 680828: learning rate 0.0000
[2019-04-04 12:45:11,081] A3C_AGENT_WORKER-Thread-12 INFO:Local step 42500, global step 680998: loss 118.0268
[2019-04-04 12:45:11,082] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 42500, global step 680998: learning rate 0.0000
[2019-04-04 12:45:11,623] A3C_AGENT_WORKER-Thread-6 INFO:Local step 42500, global step 681200: loss 116.7618
[2019-04-04 12:45:11,625] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 42500, global step 681200: learning rate 0.0000
[2019-04-04 12:45:11,975] A3C_AGENT_WORKER-Thread-10 INFO:Local step 42500, global step 681323: loss 117.2292
[2019-04-04 12:45:11,976] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 42500, global step 681323: learning rate 0.0000
[2019-04-04 12:45:17,372] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.02557407e-09 6.28851068e-11 7.25434909e-16 1.15627939e-13
 1.00000000e+00 1.01666515e-10 8.81611956e-16], sum to 1.0000
[2019-04-04 12:45:17,372] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4235
[2019-04-04 12:45:17,394] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.09999999999999999, 94.33333333333334, 0.0, 0.0, 26.0, 24.28852198911307, 0.1463865069188613, 0.0, 1.0, 40127.1308809961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 87000.0000, 
sim time next is 87600.0000, 
raw observation next is [-0.2, 93.66666666666667, 0.0, 0.0, 26.0, 24.26669516698351, 0.1467445284037311, 0.0, 1.0, 40190.20857886903], 
processed observation next is [1.0, 0.0, 0.4570637119113574, 0.9366666666666668, 0.0, 0.0, 0.6666666666666666, 0.5222245972486258, 0.5489148428012437, 0.0, 1.0, 0.19138194561366206], 
reward next is 0.8086, 
noisyNet noise sample is [array([0.4516334], dtype=float32), -0.21222259]. 
=============================================
[2019-04-04 12:45:28,121] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43000, global step 686477: loss 0.0954
[2019-04-04 12:45:28,125] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43000, global step 686479: learning rate 0.0000
[2019-04-04 12:45:29,099] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43000, global step 686699: loss 0.1045
[2019-04-04 12:45:29,100] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43000, global step 686699: learning rate 0.0000
[2019-04-04 12:45:29,154] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43000, global step 686711: loss 0.1618
[2019-04-04 12:45:29,154] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43000, global step 686711: learning rate 0.0000
[2019-04-04 12:45:30,219] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43000, global step 687086: loss 0.1235
[2019-04-04 12:45:30,221] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43000, global step 687086: learning rate 0.0000
[2019-04-04 12:45:30,889] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43000, global step 687361: loss 0.1146
[2019-04-04 12:45:30,912] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43000, global step 687361: learning rate 0.0000
[2019-04-04 12:45:30,930] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43000, global step 687374: loss 0.1353
[2019-04-04 12:45:30,940] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43000, global step 687375: learning rate 0.0000
[2019-04-04 12:45:33,816] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43000, global step 688307: loss 0.0992
[2019-04-04 12:45:33,818] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43000, global step 688307: learning rate 0.0000
[2019-04-04 12:45:33,885] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43000, global step 688328: loss 0.0692
[2019-04-04 12:45:33,889] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43000, global step 688329: learning rate 0.0000
[2019-04-04 12:45:34,067] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.8329227e-11 9.3522794e-12 1.3192159e-17 7.6411917e-16 1.0000000e+00
 4.5896655e-12 1.6595539e-17], sum to 1.0000
[2019-04-04 12:45:34,068] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4380
[2019-04-04 12:45:34,106] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 43.66666666666667, 86.33333333333333, 625.6666666666666, 26.0, 26.44084789944734, 0.5277327456960473, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 310200.0000, 
sim time next is 310800.0000, 
raw observation next is [-9.5, 43.33333333333334, 84.16666666666667, 624.3333333333334, 26.0, 26.38742367943327, 0.520364427461215, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4333333333333334, 0.28055555555555556, 0.6898710865561695, 0.6666666666666666, 0.6989519732861057, 0.6734548091537383, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18079837], dtype=float32), 0.50717145]. 
=============================================
[2019-04-04 12:45:34,131] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43000, global step 688409: loss 0.0795
[2019-04-04 12:45:34,131] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43000, global step 688409: learning rate 0.0000
[2019-04-04 12:45:34,237] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43000, global step 688439: loss 0.0943
[2019-04-04 12:45:34,238] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43000, global step 688439: learning rate 0.0000
[2019-04-04 12:45:34,408] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43000, global step 688489: loss 0.0567
[2019-04-04 12:45:34,410] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43000, global step 688489: learning rate 0.0000
[2019-04-04 12:45:34,556] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43000, global step 688529: loss 0.0717
[2019-04-04 12:45:34,559] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43000, global step 688530: learning rate 0.0000
[2019-04-04 12:45:34,784] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43000, global step 688595: loss 0.1081
[2019-04-04 12:45:34,785] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43000, global step 688595: learning rate 0.0000
[2019-04-04 12:45:36,247] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43000, global step 688933: loss 0.1012
[2019-04-04 12:45:36,247] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43000, global step 688933: learning rate 0.0000
[2019-04-04 12:45:37,185] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43000, global step 689202: loss 0.0668
[2019-04-04 12:45:37,187] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43000, global step 689202: learning rate 0.0000
[2019-04-04 12:45:37,505] A3C_AGENT_WORKER-Thread-6 INFO:Local step 43000, global step 689324: loss 0.0570
[2019-04-04 12:45:37,513] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 43000, global step 689324: learning rate 0.0000
[2019-04-04 12:45:44,635] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6603296e-07 2.8667918e-08 2.5803055e-12 1.2428334e-11 9.9999976e-01
 1.4768383e-08 1.2681772e-12], sum to 1.0000
[2019-04-04 12:45:44,635] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2425
[2019-04-04 12:45:44,650] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.8, 50.0, 0.0, 0.0, 26.0, 23.18483534901611, -0.1800317027058938, 0.0, 1.0, 46010.67392690943], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 444000.0000, 
sim time next is 444600.0000, 
raw observation next is [-10.9, 50.5, 0.0, 0.0, 26.0, 23.11715966782695, -0.1991127916065451, 0.0, 1.0, 46083.0397167549], 
processed observation next is [1.0, 0.13043478260869565, 0.16066481994459833, 0.505, 0.0, 0.0, 0.6666666666666666, 0.4264299723189125, 0.43362906946448493, 0.0, 1.0, 0.21944304627026143], 
reward next is 0.7806, 
noisyNet noise sample is [array([1.5037003], dtype=float32), -0.4890403]. 
=============================================
[2019-04-04 12:45:45,402] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5533389e-07 3.7167887e-08 4.0706772e-13 5.5989410e-12 9.9999976e-01
 5.9067928e-09 3.2378729e-12], sum to 1.0000
[2019-04-04 12:45:45,404] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5900
[2019-04-04 12:45:45,419] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.8, 50.0, 0.0, 0.0, 26.0, 23.18475054186389, -0.1800566266228922, 0.0, 1.0, 46010.74710015835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 444000.0000, 
sim time next is 444600.0000, 
raw observation next is [-10.9, 50.5, 0.0, 0.0, 26.0, 23.11707375993032, -0.1991378495155859, 0.0, 1.0, 46083.11351607571], 
processed observation next is [1.0, 0.13043478260869565, 0.16066481994459833, 0.505, 0.0, 0.0, 0.6666666666666666, 0.42642281332752674, 0.433620716828138, 0.0, 1.0, 0.21944339769559862], 
reward next is 0.7806, 
noisyNet noise sample is [array([-0.42063707], dtype=float32), 1.9068007]. 
=============================================
[2019-04-04 12:45:51,756] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43500, global step 694011: loss 0.0752
[2019-04-04 12:45:51,756] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43500, global step 694011: learning rate 0.0000
[2019-04-04 12:45:52,971] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43500, global step 694551: loss 0.0578
[2019-04-04 12:45:52,984] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43500, global step 694555: learning rate 0.0000
[2019-04-04 12:45:53,132] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43500, global step 694612: loss 0.0656
[2019-04-04 12:45:53,145] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43500, global step 694614: learning rate 0.0000
[2019-04-04 12:45:54,277] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43500, global step 695049: loss 0.0672
[2019-04-04 12:45:54,278] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43500, global step 695049: learning rate 0.0000
[2019-04-04 12:45:54,692] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43500, global step 695203: loss 0.0516
[2019-04-04 12:45:54,693] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43500, global step 695203: learning rate 0.0000
[2019-04-04 12:45:55,453] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43500, global step 695483: loss 0.0636
[2019-04-04 12:45:55,454] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43500, global step 695483: learning rate 0.0000
[2019-04-04 12:45:57,360] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43500, global step 696066: loss 0.0509
[2019-04-04 12:45:57,361] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43500, global step 696066: learning rate 0.0000
[2019-04-04 12:45:57,571] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43500, global step 696156: loss 0.0506
[2019-04-04 12:45:57,598] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43500, global step 696165: learning rate 0.0000
[2019-04-04 12:45:57,841] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43500, global step 696278: loss 0.0695
[2019-04-04 12:45:57,843] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43500, global step 696278: learning rate 0.0000
[2019-04-04 12:45:58,014] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43500, global step 696357: loss 0.0566
[2019-04-04 12:45:58,017] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43500, global step 696360: learning rate 0.0000
[2019-04-04 12:45:58,296] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43500, global step 696489: loss 0.0633
[2019-04-04 12:45:58,297] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43500, global step 696489: learning rate 0.0000
[2019-04-04 12:45:58,412] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43500, global step 696544: loss 0.0380
[2019-04-04 12:45:58,414] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43500, global step 696545: learning rate 0.0000
[2019-04-04 12:45:58,556] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43500, global step 696605: loss 0.0763
[2019-04-04 12:45:58,558] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43500, global step 696606: learning rate 0.0000
[2019-04-04 12:45:59,679] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43500, global step 697031: loss 0.0546
[2019-04-04 12:45:59,680] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43500, global step 697031: learning rate 0.0000
[2019-04-04 12:46:00,653] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43500, global step 697402: loss 0.0724
[2019-04-04 12:46:00,660] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43500, global step 697402: learning rate 0.0000
[2019-04-04 12:46:01,187] A3C_AGENT_WORKER-Thread-6 INFO:Local step 43500, global step 697611: loss 0.0526
[2019-04-04 12:46:01,198] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 43500, global step 697612: learning rate 0.0000
[2019-04-04 12:46:03,344] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4597926e-09 1.6094151e-10 3.1948283e-15 9.5434914e-15 1.0000000e+00
 5.4250278e-11 1.6430517e-15], sum to 1.0000
[2019-04-04 12:46:03,346] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3751
[2019-04-04 12:46:03,406] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 53.5, 0.0, 0.0, 26.0, 25.05869150421371, 0.2963396947743284, 1.0, 1.0, 76427.49796071232], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 759000.0000, 
sim time next is 759600.0000, 
raw observation next is [-3.9, 53.0, 0.0, 0.0, 26.0, 24.96187487683472, 0.2984213943098291, 1.0, 1.0, 111903.0272971566], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.53, 0.0, 0.0, 0.6666666666666666, 0.5801562397362267, 0.5994737981032764, 1.0, 1.0, 0.5328715585578886], 
reward next is 0.4671, 
noisyNet noise sample is [array([-0.9292513], dtype=float32), -0.6010102]. 
=============================================
[2019-04-04 12:46:06,292] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2639232e-10 2.2699755e-11 1.5673138e-16 5.2963149e-15 1.0000000e+00
 1.0235426e-11 1.9789439e-16], sum to 1.0000
[2019-04-04 12:46:06,295] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0834
[2019-04-04 12:46:06,316] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 66.0, 111.5, 423.5, 26.0, 25.87277009738929, 0.3543256163136259, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 730800.0000, 
sim time next is 731400.0000, 
raw observation next is [-0.6, 64.5, 102.3333333333333, 542.0, 26.0, 25.82353591546933, 0.3681357159940455, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.645, 0.341111111111111, 0.5988950276243094, 0.6666666666666666, 0.6519613262891107, 0.6227119053313485, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0124885], dtype=float32), 0.725085]. 
=============================================
[2019-04-04 12:46:07,116] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-04 12:46:07,119] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 12:46:07,120] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:46:07,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run8
[2019-04-04 12:46:07,140] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 12:46:07,142] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:46:07,143] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 12:46:07,144] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:46:07,148] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run8
[2019-04-04 12:46:07,161] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run8
[2019-04-04 12:47:50,112] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.8472 239852080.7440 1605.0602
[2019-04-04 12:48:10,090] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 12:48:11,109] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7444 275783685.2516 1233.3387
[2019-04-04 12:48:12,132] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 700000, evaluation results [700000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.8472345525515, 239852080.74396583, 1605.0602217004598, 7182.744355944586, 275783685.2516378, 1233.338732864913]
[2019-04-04 12:48:17,138] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44000, global step 701845: loss 0.4977
[2019-04-04 12:48:17,140] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44000, global step 701845: learning rate 0.0000
[2019-04-04 12:48:18,937] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44000, global step 702500: loss 0.5405
[2019-04-04 12:48:18,938] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44000, global step 702500: learning rate 0.0000
[2019-04-04 12:48:19,026] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44000, global step 702531: loss 0.5114
[2019-04-04 12:48:19,028] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44000, global step 702531: learning rate 0.0000
[2019-04-04 12:48:19,881] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44000, global step 702806: loss 0.5165
[2019-04-04 12:48:19,887] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44000, global step 702810: learning rate 0.0000
[2019-04-04 12:48:20,075] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44000, global step 702872: loss 0.5779
[2019-04-04 12:48:20,076] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44000, global step 702872: learning rate 0.0000
[2019-04-04 12:48:21,141] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44000, global step 703300: loss 0.5240
[2019-04-04 12:48:21,141] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44000, global step 703300: learning rate 0.0000
[2019-04-04 12:48:22,914] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44000, global step 704166: loss 0.5573
[2019-04-04 12:48:22,914] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44000, global step 704166: learning rate 0.0000
[2019-04-04 12:48:23,100] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44000, global step 704246: loss 0.5569
[2019-04-04 12:48:23,103] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44000, global step 704246: learning rate 0.0000
[2019-04-04 12:48:23,134] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44000, global step 704265: loss 0.5419
[2019-04-04 12:48:23,136] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44000, global step 704265: learning rate 0.0000
[2019-04-04 12:48:23,347] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44000, global step 704370: loss 0.4794
[2019-04-04 12:48:23,348] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44000, global step 704370: learning rate 0.0000
[2019-04-04 12:48:23,946] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44000, global step 704701: loss 0.5139
[2019-04-04 12:48:23,948] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44000, global step 704701: learning rate 0.0000
[2019-04-04 12:48:24,155] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44000, global step 704819: loss 0.5535
[2019-04-04 12:48:24,156] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44000, global step 704819: loss 0.5284
[2019-04-04 12:48:24,158] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44000, global step 704819: learning rate 0.0000
[2019-04-04 12:48:24,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44000, global step 704819: learning rate 0.0000
[2019-04-04 12:48:24,617] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6301269e-10 3.8388140e-11 8.7953221e-17 2.8079199e-15 1.0000000e+00
 2.2691054e-11 1.5698749e-16], sum to 1.0000
[2019-04-04 12:48:24,619] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7011
[2019-04-04 12:48:24,714] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.9, 100.0, 0.0, 0.0, 26.0, 24.69290502504299, 0.2739192944524287, 1.0, 1.0, 73321.25356385979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 935400.0000, 
sim time next is 936000.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 24.63065121222059, 0.2873229277777908, 1.0, 1.0, 87177.61780784442], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5525542676850493, 0.5957743092592636, 1.0, 1.0, 0.4151315133706877], 
reward next is 0.5849, 
noisyNet noise sample is [array([0.63701195], dtype=float32), 0.1358579]. 
=============================================
[2019-04-04 12:48:24,718] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[91.80282 ]
 [91.47523 ]
 [92.16706 ]
 [92.76543 ]
 [92.840164]], R is [[92.17137146]
 [91.90050507]
 [91.89239502]
 [91.9734726 ]
 [92.05374146]].
[2019-04-04 12:48:25,458] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44000, global step 705551: loss 0.5242
[2019-04-04 12:48:25,461] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44000, global step 705553: learning rate 0.0000
[2019-04-04 12:48:26,211] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44000, global step 705997: loss 0.5595
[2019-04-04 12:48:26,212] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44000, global step 705997: learning rate 0.0000
[2019-04-04 12:48:26,584] A3C_AGENT_WORKER-Thread-6 INFO:Local step 44000, global step 706256: loss 0.5620
[2019-04-04 12:48:26,586] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 44000, global step 706257: learning rate 0.0000
[2019-04-04 12:48:28,117] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0036639e-10 1.3507053e-11 2.2188051e-18 2.4411806e-17 1.0000000e+00
 4.0373998e-12 3.5206544e-17], sum to 1.0000
[2019-04-04 12:48:28,121] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8266
[2019-04-04 12:48:28,129] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.9, 86.0, 120.0, 0.0, 26.0, 26.67963904603448, 0.6810714720344974, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 991800.0000, 
sim time next is 992400.0000, 
raw observation next is [12.0, 86.0, 121.3333333333333, 0.0, 26.0, 26.67331514589162, 0.6842748699433541, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7950138504155125, 0.86, 0.40444444444444433, 0.0, 0.6666666666666666, 0.7227762621576351, 0.7280916233144513, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.81235015], dtype=float32), 0.08355279]. 
=============================================
[2019-04-04 12:48:28,918] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44500, global step 707704: loss 0.1282
[2019-04-04 12:48:28,922] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44500, global step 707704: learning rate 0.0000
[2019-04-04 12:48:30,853] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.3447801e-11 2.4460713e-12 2.7531540e-18 6.2227487e-17 1.0000000e+00
 8.1977990e-13 2.2946321e-18], sum to 1.0000
[2019-04-04 12:48:30,861] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1537
[2019-04-04 12:48:30,869] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.2, 83.0, 48.0, 124.0, 26.0, 26.49777336674731, 0.7235465062990304, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1068600.0000, 
sim time next is 1069200.0000, 
raw observation next is [12.2, 83.0, 61.0, 151.5, 26.0, 26.50132617817673, 0.7314756039278043, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.8005540166204987, 0.83, 0.20333333333333334, 0.16740331491712707, 0.6666666666666666, 0.708443848181394, 0.7438252013092681, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2463142], dtype=float32), 1.0563416]. 
=============================================
[2019-04-04 12:48:30,978] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44500, global step 709067: loss 0.1143
[2019-04-04 12:48:30,980] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44500, global step 709067: learning rate 0.0000
[2019-04-04 12:48:31,294] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44500, global step 709281: loss 0.1282
[2019-04-04 12:48:31,298] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44500, global step 709284: learning rate 0.0000
[2019-04-04 12:48:32,125] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44500, global step 709833: loss 0.0975
[2019-04-04 12:48:32,126] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44500, global step 709833: learning rate 0.0000
[2019-04-04 12:48:32,261] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44500, global step 709916: loss 0.1134
[2019-04-04 12:48:32,263] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44500, global step 709918: learning rate 0.0000
[2019-04-04 12:48:33,418] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44500, global step 710704: loss 0.1184
[2019-04-04 12:48:33,419] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44500, global step 710705: learning rate 0.0000
[2019-04-04 12:48:35,647] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44500, global step 712168: loss 0.0832
[2019-04-04 12:48:35,649] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44500, global step 712168: learning rate 0.0000
[2019-04-04 12:48:35,822] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44500, global step 712296: loss 0.1171
[2019-04-04 12:48:35,823] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44500, global step 712296: learning rate 0.0000
[2019-04-04 12:48:36,023] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44500, global step 712430: loss 0.0786
[2019-04-04 12:48:36,026] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44500, global step 712433: learning rate 0.0000
[2019-04-04 12:48:36,088] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44500, global step 712477: loss 0.0869
[2019-04-04 12:48:36,089] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44500, global step 712477: learning rate 0.0000
[2019-04-04 12:48:36,604] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44500, global step 712826: loss 0.0625
[2019-04-04 12:48:36,606] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44500, global step 712827: learning rate 0.0000
[2019-04-04 12:48:37,022] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44500, global step 713115: loss 0.0821
[2019-04-04 12:48:37,023] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44500, global step 713117: learning rate 0.0000
[2019-04-04 12:48:37,043] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44500, global step 713132: loss 0.0849
[2019-04-04 12:48:37,044] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44500, global step 713132: learning rate 0.0000
[2019-04-04 12:48:37,448] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7138079e-11 2.8348564e-13 4.0706679e-18 8.9944225e-17 1.0000000e+00
 5.5418745e-13 4.3999448e-19], sum to 1.0000
[2019-04-04 12:48:37,450] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8836
[2019-04-04 12:48:37,460] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 115.3333333333333, 0.0, 26.0, 25.99586449620833, 0.5567607925607427, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1340400.0000, 
sim time next is 1341000.0000, 
raw observation next is [1.1, 92.0, 113.0, 0.0, 26.0, 25.91565361831919, 0.5409767161145093, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.37666666666666665, 0.0, 0.6666666666666666, 0.6596378015265992, 0.6803255720381697, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.97961164], dtype=float32), -0.002508787]. 
=============================================
[2019-04-04 12:48:37,474] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[94.51709 ]
 [94.466286]
 [94.36198 ]
 [94.194916]
 [94.00913 ]], R is [[94.60121918]
 [94.65520477]
 [94.70865631]
 [94.76157379]
 [94.81395721]].
[2019-04-04 12:48:38,122] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.94390537e-10 3.20007978e-11 8.38046192e-17 2.98378621e-15
 1.00000000e+00 1.25679805e-11 1.36034550e-16], sum to 1.0000
[2019-04-04 12:48:38,122] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0062
[2019-04-04 12:48:38,133] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4631270e-08 4.6507056e-09 1.1589025e-13 4.8450922e-12 1.0000000e+00
 5.4868016e-10 4.7530540e-13], sum to 1.0000
[2019-04-04 12:48:38,135] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7446
[2019-04-04 12:48:38,141] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.55, 64.0, 29.0, 0.0, 26.0, 24.997815402392, 0.4708885744074556, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1182600.0000, 
sim time next is 1183200.0000, 
raw observation next is [18.46666666666667, 64.33333333333333, 24.16666666666667, 0.0, 26.0, 24.97709365632422, 0.4660788371085194, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.9741458910433982, 0.6433333333333333, 0.08055555555555557, 0.0, 0.6666666666666666, 0.5814244713603518, 0.6553596123695065, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8807502], dtype=float32), 0.8140144]. 
=============================================
[2019-04-04 12:48:38,144] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.24234727492127, 0.4404258593984114, 0.0, 1.0, 38488.89322210879], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1402200.0000, 
sim time next is 1402800.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.20619445740169, 0.4356784889876606, 0.0, 1.0, 38473.10655435454], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6005162047834741, 0.6452261629958869, 0.0, 1.0, 0.1832052693064502], 
reward next is 0.8168, 
noisyNet noise sample is [array([-0.3431862], dtype=float32), 1.0101904]. 
=============================================
[2019-04-04 12:48:38,386] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44500, global step 713988: loss 0.0711
[2019-04-04 12:48:38,387] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44500, global step 713988: learning rate 0.0000
[2019-04-04 12:48:38,934] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44500, global step 714295: loss 0.0957
[2019-04-04 12:48:38,938] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44500, global step 714297: learning rate 0.0000
[2019-04-04 12:48:39,346] A3C_AGENT_WORKER-Thread-6 INFO:Local step 44500, global step 714561: loss 0.0577
[2019-04-04 12:48:39,348] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 44500, global step 714562: learning rate 0.0000
[2019-04-04 12:48:43,274] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3573950e-10 2.5745738e-12 3.8110504e-17 2.5847930e-16 1.0000000e+00
 2.8615476e-12 1.2792084e-17], sum to 1.0000
[2019-04-04 12:48:43,276] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4319
[2019-04-04 12:48:43,285] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8, 94.5, 18.0, 0.0, 26.0, 25.69498625456007, 0.4854200389060108, 1.0, 1.0, 52296.45701809537], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1355400.0000, 
sim time next is 1356000.0000, 
raw observation next is [0.7000000000000001, 95.0, 15.0, 0.0, 26.0, 25.55343991520429, 0.4887718288871967, 1.0, 1.0, 36300.1051718007], 
processed observation next is [1.0, 0.6956521739130435, 0.4819944598337951, 0.95, 0.05, 0.0, 0.6666666666666666, 0.6294533262670242, 0.6629239429623989, 1.0, 1.0, 0.17285764367524142], 
reward next is 0.8271, 
noisyNet noise sample is [array([1.8943013], dtype=float32), -0.96107274]. 
=============================================
[2019-04-04 12:48:43,306] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[93.59838]
 [93.81991]
 [94.06977]
 [94.24735]
 [94.30867]], R is [[93.30162811]
 [93.11958313]
 [93.18838501]
 [93.25650024]
 [93.32393646]].
[2019-04-04 12:48:44,906] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45000, global step 717361: loss 7.1959
[2019-04-04 12:48:44,908] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45000, global step 717361: learning rate 0.0000
[2019-04-04 12:48:46,279] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9875215e-10 1.7646044e-11 2.2070461e-18 2.6297977e-16 1.0000000e+00
 8.7891185e-13 9.3770071e-18], sum to 1.0000
[2019-04-04 12:48:46,281] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7089
[2019-04-04 12:48:46,319] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.516666666666667, 100.0, 27.66666666666666, 0.0, 26.0, 25.76764673156732, 0.4890138787830078, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1500600.0000, 
sim time next is 1501200.0000, 
raw observation next is [1.6, 100.0, 32.5, 0.0, 26.0, 25.82592006272535, 0.4933968007560033, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5069252077562327, 1.0, 0.10833333333333334, 0.0, 0.6666666666666666, 0.6521600052271124, 0.6644656002520011, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6190583], dtype=float32), -0.8428568]. 
=============================================
[2019-04-04 12:48:46,778] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3921003e-10 4.3832941e-11 3.2040657e-16 4.3118051e-16 1.0000000e+00
 5.3395555e-11 2.1743900e-16], sum to 1.0000
[2019-04-04 12:48:46,779] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2009
[2019-04-04 12:48:46,795] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 80.0, 0.0, 0.0, 26.0, 25.69678683910647, 0.5254503304103717, 0.0, 1.0, 9361.297653111433], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1561200.0000, 
sim time next is 1561800.0000, 
raw observation next is [5.0, 79.5, 0.0, 0.0, 26.0, 25.57627593759518, 0.5124312345065295, 0.0, 1.0, 72044.57215117462], 
processed observation next is [1.0, 0.043478260869565216, 0.6011080332409973, 0.795, 0.0, 0.0, 0.6666666666666666, 0.6313563281329317, 0.6708104115021766, 0.0, 1.0, 0.34306939119606966], 
reward next is 0.6569, 
noisyNet noise sample is [array([-1.4545151], dtype=float32), 0.094690524]. 
=============================================
[2019-04-04 12:48:46,807] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45000, global step 718338: loss 7.2137
[2019-04-04 12:48:46,809] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45000, global step 718338: learning rate 0.0000
[2019-04-04 12:48:46,957] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45000, global step 718407: loss 7.2168
[2019-04-04 12:48:46,961] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45000, global step 718410: learning rate 0.0000
[2019-04-04 12:48:47,520] A3C_AGENT_WORKER-Thread-13 INFO:Local step 45000, global step 718709: loss 7.2328
[2019-04-04 12:48:47,523] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 45000, global step 718711: learning rate 0.0000
[2019-04-04 12:48:47,975] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45000, global step 718919: loss 7.2396
[2019-04-04 12:48:47,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45000, global step 718919: learning rate 0.0000
[2019-04-04 12:48:48,088] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2569233e-10 4.0871169e-13 5.1527599e-18 5.0841707e-17 1.0000000e+00
 2.4774787e-12 8.6849274e-18], sum to 1.0000
[2019-04-04 12:48:48,091] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3927
[2019-04-04 12:48:48,136] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 24.87646601041996, 0.4274823756061049, 1.0, 1.0, 111018.7568935212], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1454400.0000, 
sim time next is 1455000.0000, 
raw observation next is [1.183333333333333, 91.5, 0.0, 0.0, 26.0, 24.89628927806874, 0.4403542084932903, 0.0, 1.0, 52427.86530992354], 
processed observation next is [1.0, 0.8695652173913043, 0.49538319482917825, 0.915, 0.0, 0.0, 0.6666666666666666, 0.5746907731723949, 0.6467847361644301, 0.0, 1.0, 0.24965650147582638], 
reward next is 0.7503, 
noisyNet noise sample is [array([1.2713311], dtype=float32), 0.9655848]. 
=============================================
[2019-04-04 12:48:48,144] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[91.40776 ]
 [90.72552 ]
 [90.35021 ]
 [90.998825]
 [91.38438 ]], R is [[90.64336395]
 [90.20826721]
 [89.83979797]
 [89.84503937]
 [89.94658661]].
[2019-04-04 12:48:48,516] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.1553327e-11 1.5590552e-12 1.3880109e-18 8.8882099e-17 1.0000000e+00
 9.3274412e-13 4.2035069e-18], sum to 1.0000
[2019-04-04 12:48:48,516] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0598
[2019-04-04 12:48:48,549] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8, 92.0, 90.0, 0.0, 26.0, 25.63649694848839, 0.5490290729878319, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1431000.0000, 
sim time next is 1431600.0000, 
raw observation next is [0.9000000000000001, 92.0, 87.0, 0.0, 26.0, 25.99676122247569, 0.5721922337877413, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.48753462603878117, 0.92, 0.29, 0.0, 0.6666666666666666, 0.6663967685396409, 0.6907307445959138, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4475112], dtype=float32), 1.3955247]. 
=============================================
[2019-04-04 12:48:48,555] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3774619e-10 1.4150681e-11 4.2855797e-18 9.5287173e-16 1.0000000e+00
 1.5710522e-12 3.2480220e-17], sum to 1.0000
[2019-04-04 12:48:48,559] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2387
[2019-04-04 12:48:48,567] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.91666666666667, 51.33333333333334, 83.66666666666667, 177.9999999999999, 26.0, 27.0810716147555, 0.807501392308263, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1527000.0000, 
sim time next is 1527600.0000, 
raw observation next is [11.63333333333333, 52.66666666666667, 85.33333333333333, 103.0, 26.0, 27.17236652465806, 0.8139761666819932, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7848568790397045, 0.5266666666666667, 0.28444444444444444, 0.1138121546961326, 0.6666666666666666, 0.7643638770548383, 0.7713253888939978, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6585641], dtype=float32), -0.25216466]. 
=============================================
[2019-04-04 12:48:48,761] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45000, global step 719323: loss 7.2428
[2019-04-04 12:48:48,762] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45000, global step 719323: learning rate 0.0000
[2019-04-04 12:48:49,110] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0541956e-10 1.6189770e-11 1.3995658e-16 4.4852613e-16 1.0000000e+00
 1.3695338e-11 2.6307912e-16], sum to 1.0000
[2019-04-04 12:48:49,112] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0117
[2019-04-04 12:48:49,124] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.37202989916993, 0.4846600251041966, 0.0, 1.0, 46644.29145911751], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1468200.0000, 
sim time next is 1468800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.3558234533264, 0.4832139204532866, 0.0, 1.0, 42359.32267590512], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6129852877772001, 0.6610713068177622, 0.0, 1.0, 0.20171106036145295], 
reward next is 0.7983, 
noisyNet noise sample is [array([0.4213606], dtype=float32), -1.9728308]. 
=============================================
[2019-04-04 12:48:50,778] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45000, global step 720490: loss 7.2789
[2019-04-04 12:48:50,782] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45000, global step 720491: learning rate 0.0000
[2019-04-04 12:48:50,896] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45000, global step 720558: loss 7.2133
[2019-04-04 12:48:50,897] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45000, global step 720558: learning rate 0.0000
[2019-04-04 12:48:51,072] A3C_AGENT_WORKER-Thread-11 INFO:Local step 45000, global step 720657: loss 7.1482
[2019-04-04 12:48:51,074] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 45000, global step 720657: learning rate 0.0000
[2019-04-04 12:48:51,261] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45000, global step 720774: loss 7.1957
[2019-04-04 12:48:51,264] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45000, global step 720776: learning rate 0.0000
[2019-04-04 12:48:51,558] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45000, global step 720959: loss 7.2659
[2019-04-04 12:48:51,560] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45000, global step 720959: learning rate 0.0000
[2019-04-04 12:48:52,087] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45000, global step 721294: loss 7.1782
[2019-04-04 12:48:52,089] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45000, global step 721294: learning rate 0.0000
[2019-04-04 12:48:52,098] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45000, global step 721302: loss 7.3018
[2019-04-04 12:48:52,099] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45000, global step 721302: learning rate 0.0000
[2019-04-04 12:48:52,957] A3C_AGENT_WORKER-Thread-12 INFO:Local step 45000, global step 721894: loss 7.2071
[2019-04-04 12:48:52,961] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 45000, global step 721896: learning rate 0.0000
[2019-04-04 12:48:54,202] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45000, global step 722572: loss 7.2515
[2019-04-04 12:48:54,203] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45000, global step 722572: learning rate 0.0000
[2019-04-04 12:48:54,557] A3C_AGENT_WORKER-Thread-6 INFO:Local step 45000, global step 722777: loss 7.1751
[2019-04-04 12:48:54,561] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 45000, global step 722777: learning rate 0.0000
[2019-04-04 12:48:56,476] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0275296e-10 9.0473367e-12 5.1537819e-18 9.6570451e-17 1.0000000e+00
 1.8913098e-12 8.4230528e-18], sum to 1.0000
[2019-04-04 12:48:56,476] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2592
[2019-04-04 12:48:56,500] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.2, 82.0, 0.0, 0.0, 26.0, 25.58984018453801, 0.5836135174773877, 0.0, 1.0, 93418.83920282668], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1638000.0000, 
sim time next is 1638600.0000, 
raw observation next is [7.2, 82.66666666666667, 0.0, 0.0, 26.0, 25.5314247227762, 0.6088248615086206, 0.0, 1.0, 91388.50824011308], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6276187268980168, 0.7029416205028736, 0.0, 1.0, 0.43518337257196704], 
reward next is 0.5648, 
noisyNet noise sample is [array([-0.2712187], dtype=float32), -0.7311615]. 
=============================================
[2019-04-04 12:48:57,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0186047e-10 2.9447174e-12 1.9737558e-17 7.5435854e-17 1.0000000e+00
 3.7848080e-12 1.0810876e-17], sum to 1.0000
[2019-04-04 12:48:57,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5613
[2019-04-04 12:48:57,010] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 49.0, 165.6666666666667, 0.0, 26.0, 27.23319573690603, 0.8450324422547081, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1605000.0000, 
sim time next is 1605600.0000, 
raw observation next is [13.8, 49.0, 160.5, 0.0, 26.0, 27.30238732481267, 0.8474252605630297, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.844875346260388, 0.49, 0.535, 0.0, 0.6666666666666666, 0.7751989437343892, 0.7824750868543432, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.85192394], dtype=float32), 0.309556]. 
=============================================
[2019-04-04 12:49:00,647] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45500, global step 725811: loss 0.0638
[2019-04-04 12:49:00,657] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45500, global step 725811: learning rate 0.0000
[2019-04-04 12:49:02,772] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45500, global step 726705: loss 0.0537
[2019-04-04 12:49:02,773] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45500, global step 726705: learning rate 0.0000
[2019-04-04 12:49:02,783] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45500, global step 726714: loss 0.0625
[2019-04-04 12:49:02,791] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45500, global step 726715: learning rate 0.0000
[2019-04-04 12:49:03,557] A3C_AGENT_WORKER-Thread-13 INFO:Local step 45500, global step 727019: loss 0.0716
[2019-04-04 12:49:03,558] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 45500, global step 727019: learning rate 0.0000
[2019-04-04 12:49:04,119] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45500, global step 727240: loss 0.0635
[2019-04-04 12:49:04,120] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45500, global step 727241: learning rate 0.0000
[2019-04-04 12:49:04,674] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45500, global step 727459: loss 0.0550
[2019-04-04 12:49:04,675] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45500, global step 727459: learning rate 0.0000
[2019-04-04 12:49:06,990] A3C_AGENT_WORKER-Thread-11 INFO:Local step 45500, global step 728177: loss 0.0326
[2019-04-04 12:49:06,991] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 45500, global step 728177: learning rate 0.0000
[2019-04-04 12:49:07,212] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45500, global step 728253: loss 0.0787
[2019-04-04 12:49:07,212] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45500, global step 728253: learning rate 0.0000
[2019-04-04 12:49:07,288] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45500, global step 728282: loss 0.0500
[2019-04-04 12:49:07,292] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45500, global step 728283: learning rate 0.0000
[2019-04-04 12:49:07,475] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45500, global step 728355: loss 0.0817
[2019-04-04 12:49:07,476] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45500, global step 728355: learning rate 0.0000
[2019-04-04 12:49:07,953] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45500, global step 728548: loss 0.0502
[2019-04-04 12:49:07,957] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45500, global step 728549: learning rate 0.0000
[2019-04-04 12:49:08,106] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9180203e-08 1.1707376e-09 7.2695376e-14 4.6836065e-13 1.0000000e+00
 1.5476331e-09 1.8449402e-13], sum to 1.0000
[2019-04-04 12:49:08,109] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5197
[2019-04-04 12:49:08,147] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.01340685008416, 0.3192168444346346, 0.0, 1.0, 121177.5730344474], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1797600.0000, 
sim time next is 1798200.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.01102731033565, 0.332258889671403, 0.0, 1.0, 79134.89193526145], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5842522758613041, 0.610752963223801, 0.0, 1.0, 0.3768328187393402], 
reward next is 0.6232, 
noisyNet noise sample is [array([-1.2100155], dtype=float32), 1.5400785]. 
=============================================
[2019-04-04 12:49:08,580] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45500, global step 728801: loss 0.0502
[2019-04-04 12:49:08,582] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45500, global step 728801: learning rate 0.0000
[2019-04-04 12:49:08,843] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45500, global step 728900: loss 0.0596
[2019-04-04 12:49:08,845] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45500, global step 728900: learning rate 0.0000
[2019-04-04 12:49:09,559] A3C_AGENT_WORKER-Thread-12 INFO:Local step 45500, global step 729168: loss 0.0579
[2019-04-04 12:49:09,561] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 45500, global step 729168: learning rate 0.0000
[2019-04-04 12:49:10,834] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45500, global step 729652: loss 0.0566
[2019-04-04 12:49:10,836] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45500, global step 729652: learning rate 0.0000
[2019-04-04 12:49:11,003] A3C_AGENT_WORKER-Thread-6 INFO:Local step 45500, global step 729719: loss 0.0411
[2019-04-04 12:49:11,003] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 45500, global step 729719: learning rate 0.0000
[2019-04-04 12:49:11,057] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.8701383e-08 7.9801321e-09 7.0518759e-13 5.9008835e-12 9.9999988e-01
 4.8378874e-09 5.1551710e-13], sum to 1.0000
[2019-04-04 12:49:11,057] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4509
[2019-04-04 12:49:11,074] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 23.26335249452545, -0.09961464434379906, 0.0, 1.0, 47181.9242841981], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1842000.0000, 
sim time next is 1842600.0000, 
raw observation next is [-6.7, 78.0, 9.666666666666664, 0.0, 26.0, 23.23717174774418, -0.1040500362080387, 0.0, 1.0, 47149.18531646925], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.032222222222222215, 0.0, 0.6666666666666666, 0.43643097897868177, 0.4653166545973204, 0.0, 1.0, 0.224519930078425], 
reward next is 0.7755, 
noisyNet noise sample is [array([0.84349203], dtype=float32), 0.8026899]. 
=============================================
[2019-04-04 12:49:22,551] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46000, global step 733730: loss 1.9484
[2019-04-04 12:49:22,552] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46000, global step 733730: learning rate 0.0000
[2019-04-04 12:49:24,446] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.42445344e-10 2.10848658e-10 7.20459861e-16 1.31192105e-14
 1.00000000e+00 2.24286631e-11 5.07694449e-16], sum to 1.0000
[2019-04-04 12:49:24,452] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6448
[2019-04-04 12:49:24,503] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.633333333333333, 81.0, 89.0, 50.5, 26.0, 25.54012966420338, 0.3148201107098973, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2104800.0000, 
sim time next is 2105400.0000, 
raw observation next is [-7.716666666666667, 81.5, 106.0, 63.99999999999999, 26.0, 25.45550190869843, 0.3225827015322141, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.24884579870729456, 0.815, 0.35333333333333333, 0.07071823204419889, 0.6666666666666666, 0.6212918257248692, 0.6075275671774046, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.98137516], dtype=float32), -0.069354825]. 
=============================================
[2019-04-04 12:49:25,117] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46000, global step 734626: loss 2.0197
[2019-04-04 12:49:25,118] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46000, global step 734626: learning rate 0.0000
[2019-04-04 12:49:25,755] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46000, global step 734805: loss 1.9866
[2019-04-04 12:49:25,756] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46000, global step 734805: learning rate 0.0000
[2019-04-04 12:49:25,847] A3C_AGENT_WORKER-Thread-13 INFO:Local step 46000, global step 734827: loss 2.0344
[2019-04-04 12:49:25,868] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 46000, global step 734827: learning rate 0.0000
[2019-04-04 12:49:26,808] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46000, global step 735118: loss 2.1334
[2019-04-04 12:49:26,809] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46000, global step 735118: learning rate 0.0000
[2019-04-04 12:49:27,991] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46000, global step 735572: loss 2.0765
[2019-04-04 12:49:27,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46000, global step 735572: learning rate 0.0000
[2019-04-04 12:49:29,670] A3C_AGENT_WORKER-Thread-11 INFO:Local step 46000, global step 736142: loss 1.9606
[2019-04-04 12:49:29,671] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 46000, global step 736143: learning rate 0.0000
[2019-04-04 12:49:29,681] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46000, global step 736146: loss 1.9181
[2019-04-04 12:49:29,681] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46000, global step 736146: learning rate 0.0000
[2019-04-04 12:49:29,757] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46000, global step 736178: loss 1.9113
[2019-04-04 12:49:29,765] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46000, global step 736178: learning rate 0.0000
[2019-04-04 12:49:30,047] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46000, global step 736296: loss 2.0344
[2019-04-04 12:49:30,048] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46000, global step 736296: learning rate 0.0000
[2019-04-04 12:49:30,125] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46000, global step 736326: loss 2.0342
[2019-04-04 12:49:30,125] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46000, global step 736326: learning rate 0.0000
[2019-04-04 12:49:30,645] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46000, global step 736511: loss 1.9529
[2019-04-04 12:49:30,647] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46000, global step 736513: learning rate 0.0000
[2019-04-04 12:49:31,407] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46000, global step 736763: loss 2.0749
[2019-04-04 12:49:31,408] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46000, global step 736763: learning rate 0.0000
[2019-04-04 12:49:31,860] A3C_AGENT_WORKER-Thread-12 INFO:Local step 46000, global step 736922: loss 2.0182
[2019-04-04 12:49:31,863] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 46000, global step 736922: learning rate 0.0000
[2019-04-04 12:49:33,636] A3C_AGENT_WORKER-Thread-6 INFO:Local step 46000, global step 737469: loss 2.0025
[2019-04-04 12:49:33,637] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 46000, global step 737469: learning rate 0.0000
[2019-04-04 12:49:33,733] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46000, global step 737508: loss 1.9723
[2019-04-04 12:49:33,733] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46000, global step 737508: learning rate 0.0000
[2019-04-04 12:49:43,900] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.6154015e-08 6.3233720e-09 1.1300954e-13 6.1835747e-13 9.9999988e-01
 3.2498923e-10 2.4379419e-13], sum to 1.0000
[2019-04-04 12:49:43,905] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7091
[2019-04-04 12:49:43,969] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 69.0, 79.0, 180.0, 26.0, 25.33227007685912, 0.3230099067661984, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2365200.0000, 
sim time next is 2365800.0000, 
raw observation next is [-3.3, 68.33333333333333, 93.0, 240.0, 26.0, 25.33221430793607, 0.3224266955954547, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.37119113573407203, 0.6833333333333332, 0.31, 0.26519337016574585, 0.6666666666666666, 0.6110178589946725, 0.6074755651984849, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.93205315], dtype=float32), 0.59259635]. 
=============================================
[2019-04-04 12:49:45,292] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46500, global step 741782: loss 0.0832
[2019-04-04 12:49:45,294] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46500, global step 741782: learning rate 0.0000
[2019-04-04 12:49:46,452] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.3575191e-08 1.1693451e-09 2.0737048e-13 1.0109868e-12 1.0000000e+00
 4.5539639e-09 1.9608519e-13], sum to 1.0000
[2019-04-04 12:49:46,454] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3264
[2019-04-04 12:49:46,482] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 63.0, 0.0, 0.0, 26.0, 24.84117439205109, 0.2710209773422809, 0.0, 1.0, 38471.52817542883], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2335200.0000, 
sim time next is 2335800.0000, 
raw observation next is [-2.3, 62.5, 0.0, 0.0, 26.0, 24.81828029488618, 0.2649517954861125, 0.0, 1.0, 38511.03404110864], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.625, 0.0, 0.0, 0.6666666666666666, 0.5681900245738483, 0.5883172651620375, 0.0, 1.0, 0.18338587638623163], 
reward next is 0.8166, 
noisyNet noise sample is [array([-0.11236726], dtype=float32), -1.0510463]. 
=============================================
[2019-04-04 12:49:47,401] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46500, global step 742618: loss 0.0945
[2019-04-04 12:49:47,402] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46500, global step 742618: learning rate 0.0000
[2019-04-04 12:49:47,892] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46500, global step 742835: loss 0.1016
[2019-04-04 12:49:47,896] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46500, global step 742835: learning rate 0.0000
[2019-04-04 12:49:47,932] A3C_AGENT_WORKER-Thread-13 INFO:Local step 46500, global step 742846: loss 0.1124
[2019-04-04 12:49:47,932] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 46500, global step 742846: learning rate 0.0000
[2019-04-04 12:49:49,290] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46500, global step 743388: loss 0.1072
[2019-04-04 12:49:49,318] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46500, global step 743399: learning rate 0.0000
[2019-04-04 12:49:49,680] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.3992037e-09 8.1243401e-10 3.8944868e-15 6.4478032e-14 1.0000000e+00
 2.8598918e-10 1.9500421e-14], sum to 1.0000
[2019-04-04 12:49:49,681] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7140
[2019-04-04 12:49:49,729] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.383333333333333, 49.5, 195.3333333333333, 345.3333333333334, 26.0, 24.93942881961836, 0.3082774938111066, 0.0, 1.0, 50746.33186879537], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2375400.0000, 
sim time next is 2376000.0000, 
raw observation next is [-1.2, 47.0, 209.5, 365.0, 26.0, 24.92567762621669, 0.319674375186687, 0.0, 1.0, 45549.46699993768], 
processed observation next is [0.0, 0.5217391304347826, 0.42936288088642666, 0.47, 0.6983333333333334, 0.40331491712707185, 0.6666666666666666, 0.5771398021847242, 0.6065581250622291, 0.0, 1.0, 0.21690222380922705], 
reward next is 0.7831, 
noisyNet noise sample is [array([-0.18888947], dtype=float32), -0.53720164]. 
=============================================
[2019-04-04 12:49:49,736] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[78.096756]
 [78.279366]
 [78.44372 ]
 [78.45747 ]
 [78.517685]], R is [[78.36321259]
 [78.3379364 ]
 [78.3759079 ]
 [78.50296783]
 [78.71794128]].
[2019-04-04 12:49:50,212] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46500, global step 743708: loss 0.1363
[2019-04-04 12:49:50,212] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46500, global step 743708: learning rate 0.0000
[2019-04-04 12:49:52,022] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46500, global step 744393: loss 0.0966
[2019-04-04 12:49:52,025] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46500, global step 744395: learning rate 0.0000
[2019-04-04 12:49:52,087] A3C_AGENT_WORKER-Thread-11 INFO:Local step 46500, global step 744422: loss 0.1170
[2019-04-04 12:49:52,091] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46500, global step 744422: loss 0.0875
[2019-04-04 12:49:52,091] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46500, global step 744422: learning rate 0.0000
[2019-04-04 12:49:52,093] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 46500, global step 744422: learning rate 0.0000
[2019-04-04 12:49:52,127] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46500, global step 744433: loss 0.0876
[2019-04-04 12:49:52,130] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46500, global step 744433: learning rate 0.0000
[2019-04-04 12:49:52,304] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46500, global step 744510: loss 0.0975
[2019-04-04 12:49:52,305] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46500, global step 744511: learning rate 0.0000
[2019-04-04 12:49:52,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3078849e-08 4.3218158e-09 6.2317285e-13 1.5553692e-12 1.0000000e+00
 1.8064181e-09 6.2181342e-13], sum to 1.0000
[2019-04-04 12:49:52,630] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8647
[2019-04-04 12:49:52,670] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.883333333333333, 44.16666666666667, 66.0, 702.3333333333333, 26.0, 25.22071213313537, 0.2465256673257226, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2454600.0000, 
sim time next is 2455200.0000, 
raw observation next is [-5.6, 43.0, 68.5, 721.0, 26.0, 25.1684668164735, 0.2416536182040317, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.43, 0.22833333333333333, 0.7966850828729282, 0.6666666666666666, 0.5973722347061251, 0.5805512060680106, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1938775], dtype=float32), -1.4612745]. 
=============================================
[2019-04-04 12:49:52,733] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46500, global step 744691: loss 0.1006
[2019-04-04 12:49:52,734] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46500, global step 744691: learning rate 0.0000
[2019-04-04 12:49:53,349] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46500, global step 745000: loss 0.1050
[2019-04-04 12:49:53,350] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46500, global step 745000: learning rate 0.0000
[2019-04-04 12:49:53,686] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.8628732e-09 2.6537159e-09 6.9251401e-14 4.6585001e-13 1.0000000e+00
 1.0214316e-09 2.0855888e-13], sum to 1.0000
[2019-04-04 12:49:53,687] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6458
[2019-04-04 12:49:53,720] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.92560882932779, 0.1766751920676289, 0.0, 1.0, 38626.28616661159], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2516400.0000, 
sim time next is 2517000.0000, 
raw observation next is [-1.7, 44.83333333333334, 0.0, 0.0, 26.0, 24.92735402910093, 0.1840188681340851, 0.0, 1.0, 38577.21825511711], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4483333333333334, 0.0, 0.0, 0.6666666666666666, 0.5772795024250774, 0.5613396227113617, 0.0, 1.0, 0.1837010393100815], 
reward next is 0.8163, 
noisyNet noise sample is [array([-1.7402006], dtype=float32), -0.3839939]. 
=============================================
[2019-04-04 12:49:53,731] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[72.79816]
 [72.26682]
 [72.03039]
 [71.27303]
 [70.75631]], R is [[73.21713257]
 [73.30102539]
 [73.38401794]
 [73.46607971]
 [73.54711151]].
[2019-04-04 12:49:54,026] A3C_AGENT_WORKER-Thread-12 INFO:Local step 46500, global step 745326: loss 0.0879
[2019-04-04 12:49:54,028] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 46500, global step 745326: learning rate 0.0000
[2019-04-04 12:49:55,584] A3C_AGENT_WORKER-Thread-6 INFO:Local step 46500, global step 745907: loss 0.0718
[2019-04-04 12:49:55,587] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 46500, global step 745907: learning rate 0.0000
[2019-04-04 12:49:55,706] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46500, global step 745971: loss 0.0930
[2019-04-04 12:49:55,711] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46500, global step 745971: learning rate 0.0000
[2019-04-04 12:50:01,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7425271e-09 9.7516023e-11 1.5652772e-15 2.4480479e-14 1.0000000e+00
 6.5191408e-11 6.0746614e-16], sum to 1.0000
[2019-04-04 12:50:01,995] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4130
[2019-04-04 12:50:02,051] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.299999999999999, 79.0, 30.33333333333333, 5.333333333333334, 26.0, 25.21423284124845, 0.2905819928982705, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2620200.0000, 
sim time next is 2620800.0000, 
raw observation next is [-7.3, 79.0, 42.0, 4.0, 26.0, 25.35074890475964, 0.2980758576018095, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.26038781163434904, 0.79, 0.14, 0.004419889502762431, 0.6666666666666666, 0.61256240872997, 0.5993586192006032, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.67824197], dtype=float32), 0.62384504]. 
=============================================
[2019-04-04 12:50:03,346] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47000, global step 749659: loss 19.7535
[2019-04-04 12:50:03,348] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47000, global step 749660: learning rate 0.0000
[2019-04-04 12:50:05,269] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47000, global step 750435: loss 19.7931
[2019-04-04 12:50:05,288] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47000, global step 750435: learning rate 0.0000
[2019-04-04 12:50:05,398] A3C_AGENT_WORKER-Thread-13 INFO:Local step 47000, global step 750482: loss 19.8830
[2019-04-04 12:50:05,399] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 47000, global step 750482: learning rate 0.0000
[2019-04-04 12:50:06,059] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47000, global step 750767: loss 19.8849
[2019-04-04 12:50:06,059] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47000, global step 750767: learning rate 0.0000
[2019-04-04 12:50:07,086] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47000, global step 751181: loss 19.7746
[2019-04-04 12:50:07,087] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47000, global step 751181: learning rate 0.0000
[2019-04-04 12:50:08,199] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47000, global step 751665: loss 19.7964
[2019-04-04 12:50:08,202] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47000, global step 751667: learning rate 0.0000
[2019-04-04 12:50:09,285] A3C_AGENT_WORKER-Thread-11 INFO:Local step 47000, global step 752128: loss 19.7197
[2019-04-04 12:50:09,286] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 47000, global step 752128: learning rate 0.0000
[2019-04-04 12:50:09,455] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47000, global step 752200: loss 19.5366
[2019-04-04 12:50:09,455] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47000, global step 752200: learning rate 0.0000
[2019-04-04 12:50:09,808] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47000, global step 752344: loss 19.7817
[2019-04-04 12:50:09,809] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47000, global step 752344: learning rate 0.0000
[2019-04-04 12:50:09,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2086126e-09 4.5632653e-10 2.8998946e-15 2.7123377e-14 1.0000000e+00
 7.4496137e-10 9.8233613e-15], sum to 1.0000
[2019-04-04 12:50:09,943] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4120
[2019-04-04 12:50:09,971] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47000, global step 752407: loss 19.7810
[2019-04-04 12:50:09,972] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47000, global step 752407: learning rate 0.0000
[2019-04-04 12:50:09,980] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 61.00000000000001, 0.0, 0.0, 26.0, 25.19958971884815, 0.3729138758932766, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2661600.0000, 
sim time next is 2662200.0000, 
raw observation next is [-1.2, 61.5, 0.0, 0.0, 26.0, 25.11127177079529, 0.3532388723792416, 0.0, 1.0, 25677.83915335724], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.615, 0.0, 0.0, 0.6666666666666666, 0.5926059808996076, 0.6177462907930805, 0.0, 1.0, 0.12227542453979638], 
reward next is 0.8777, 
noisyNet noise sample is [array([1.4604275], dtype=float32), -0.6535181]. 
=============================================
[2019-04-04 12:50:10,309] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47000, global step 752530: loss 19.6454
[2019-04-04 12:50:10,310] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47000, global step 752530: learning rate 0.0000
[2019-04-04 12:50:10,552] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47000, global step 752619: loss 19.6651
[2019-04-04 12:50:10,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47000, global step 752619: learning rate 0.0000
[2019-04-04 12:50:11,158] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47000, global step 752838: loss 19.5699
[2019-04-04 12:50:11,159] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47000, global step 752838: learning rate 0.0000
[2019-04-04 12:50:11,793] A3C_AGENT_WORKER-Thread-12 INFO:Local step 47000, global step 753118: loss 19.7013
[2019-04-04 12:50:11,794] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 47000, global step 753118: learning rate 0.0000
[2019-04-04 12:50:12,965] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47000, global step 753588: loss 19.7310
[2019-04-04 12:50:12,967] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47000, global step 753588: learning rate 0.0000
[2019-04-04 12:50:13,411] A3C_AGENT_WORKER-Thread-6 INFO:Local step 47000, global step 753762: loss 19.6491
[2019-04-04 12:50:13,412] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 47000, global step 753762: learning rate 0.0000
[2019-04-04 12:50:22,200] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47500, global step 757730: loss 0.7526
[2019-04-04 12:50:22,201] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47500, global step 757730: learning rate 0.0000
[2019-04-04 12:50:23,002] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.0323173e-08 1.2346392e-08 8.0467702e-13 2.4888419e-11 9.9999988e-01
 8.5844380e-09 2.3557939e-12], sum to 1.0000
[2019-04-04 12:50:23,003] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0868
[2019-04-04 12:50:23,033] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.42346484431128, 0.1540426015147497, 0.0, 1.0, 38521.25483205608], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3031200.0000, 
sim time next is 3031800.0000, 
raw observation next is [-5.166666666666667, 72.0, 0.0, 0.0, 26.0, 24.42007610417912, 0.1485755914165367, 0.0, 1.0, 38641.56955614068], 
processed observation next is [0.0, 0.08695652173913043, 0.31948291782086796, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5350063420149267, 0.5495251971388456, 0.0, 1.0, 0.18400747407686038], 
reward next is 0.8160, 
noisyNet noise sample is [array([0.9974051], dtype=float32), -0.28511745]. 
=============================================
[2019-04-04 12:50:24,084] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47500, global step 758535: loss 0.7374
[2019-04-04 12:50:24,085] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47500, global step 758535: learning rate 0.0000
[2019-04-04 12:50:24,233] A3C_AGENT_WORKER-Thread-13 INFO:Local step 47500, global step 758607: loss 0.7510
[2019-04-04 12:50:24,233] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 47500, global step 758607: learning rate 0.0000
[2019-04-04 12:50:25,388] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47500, global step 759121: loss 0.8452
[2019-04-04 12:50:25,392] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47500, global step 759125: learning rate 0.0000
[2019-04-04 12:50:25,817] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47500, global step 759293: loss 0.8175
[2019-04-04 12:50:25,818] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47500, global step 759293: learning rate 0.0000
[2019-04-04 12:50:27,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.12935279e-08 3.34533845e-10 1.21990956e-14 1.01601033e-13
 1.00000000e+00 3.24091698e-10 3.92754778e-15], sum to 1.0000
[2019-04-04 12:50:27,151] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6441
[2019-04-04 12:50:27,220] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.833333333333333, 54.16666666666666, 109.3333333333333, 789.6666666666667, 26.0, 25.10174212691854, 0.342180937820764, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3064200.0000, 
sim time next is 3064800.0000, 
raw observation next is [-3.666666666666667, 54.33333333333334, 110.1666666666667, 797.3333333333334, 26.0, 25.16554329286106, 0.3435581002839684, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.3610341643582641, 0.5433333333333334, 0.36722222222222234, 0.8810313075506446, 0.6666666666666666, 0.5971286077384216, 0.6145193667613228, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.61928725], dtype=float32), 0.30783984]. 
=============================================
[2019-04-04 12:50:27,548] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47500, global step 759955: loss 0.7317
[2019-04-04 12:50:27,550] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47500, global step 759955: learning rate 0.0000
[2019-04-04 12:50:28,361] A3C_AGENT_WORKER-Thread-11 INFO:Local step 47500, global step 760287: loss 0.7374
[2019-04-04 12:50:28,365] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 47500, global step 760287: learning rate 0.0000
[2019-04-04 12:50:28,579] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47500, global step 760372: loss 0.7895
[2019-04-04 12:50:28,589] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47500, global step 760374: learning rate 0.0000
[2019-04-04 12:50:28,870] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47500, global step 760478: loss 0.8209
[2019-04-04 12:50:28,872] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47500, global step 760478: learning rate 0.0000
[2019-04-04 12:50:28,991] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47500, global step 760520: loss 0.8152
[2019-04-04 12:50:28,992] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47500, global step 760521: learning rate 0.0000
[2019-04-04 12:50:29,248] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47500, global step 760646: loss 0.7815
[2019-04-04 12:50:29,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47500, global step 760646: learning rate 0.0000
[2019-04-04 12:50:29,348] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47500, global step 760690: loss 0.7271
[2019-04-04 12:50:29,349] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47500, global step 760691: learning rate 0.0000
[2019-04-04 12:50:30,210] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47500, global step 761152: loss 0.7293
[2019-04-04 12:50:30,211] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47500, global step 761152: learning rate 0.0000
[2019-04-04 12:50:30,788] A3C_AGENT_WORKER-Thread-12 INFO:Local step 47500, global step 761409: loss 0.7528
[2019-04-04 12:50:30,789] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 47500, global step 761410: learning rate 0.0000
[2019-04-04 12:50:31,717] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47500, global step 761828: loss 0.8008
[2019-04-04 12:50:31,719] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47500, global step 761830: learning rate 0.0000
[2019-04-04 12:50:32,206] A3C_AGENT_WORKER-Thread-6 INFO:Local step 47500, global step 762076: loss 0.7276
[2019-04-04 12:50:32,208] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 47500, global step 762077: learning rate 0.0000
[2019-04-04 12:50:37,190] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48000, global step 764834: loss 7.7736
[2019-04-04 12:50:37,191] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48000, global step 764834: learning rate 0.0000
[2019-04-04 12:50:39,311] A3C_AGENT_WORKER-Thread-13 INFO:Local step 48000, global step 766040: loss 7.8027
[2019-04-04 12:50:39,312] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 48000, global step 766040: learning rate 0.0000
[2019-04-04 12:50:39,511] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48000, global step 766144: loss 7.7672
[2019-04-04 12:50:39,513] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48000, global step 766145: learning rate 0.0000
[2019-04-04 12:50:39,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0207538e-09 4.5477792e-11 9.0619629e-16 1.6391273e-14 1.0000000e+00
 4.8933701e-11 5.8058211e-16], sum to 1.0000
[2019-04-04 12:50:39,672] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3914
[2019-04-04 12:50:39,726] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.0, 77.0, 95.0, 505.5, 26.0, 26.02541596587943, 0.5086785713378104, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3315600.0000, 
sim time next is 3316200.0000, 
raw observation next is [-8.833333333333334, 75.83333333333334, 98.0, 542.0, 26.0, 26.12435615181302, 0.5233322289776516, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.21791320406278855, 0.7583333333333334, 0.32666666666666666, 0.5988950276243094, 0.6666666666666666, 0.6770296793177518, 0.674444076325884, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.131054], dtype=float32), 1.1000577]. 
=============================================
[2019-04-04 12:50:40,575] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48000, global step 766673: loss 7.8574
[2019-04-04 12:50:40,576] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48000, global step 766673: learning rate 0.0000
[2019-04-04 12:50:40,999] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48000, global step 766887: loss 7.8788
[2019-04-04 12:50:40,999] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48000, global step 766887: learning rate 0.0000
[2019-04-04 12:50:42,847] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48000, global step 767762: loss 7.7797
[2019-04-04 12:50:42,847] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48000, global step 767762: learning rate 0.0000
[2019-04-04 12:50:43,506] A3C_AGENT_WORKER-Thread-11 INFO:Local step 48000, global step 768080: loss 7.8921
[2019-04-04 12:50:43,508] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 48000, global step 768080: learning rate 0.0000
[2019-04-04 12:50:43,931] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48000, global step 768287: loss 7.8598
[2019-04-04 12:50:43,932] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48000, global step 768287: learning rate 0.0000
[2019-04-04 12:50:44,199] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48000, global step 768405: loss 7.7852
[2019-04-04 12:50:44,199] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48000, global step 768405: learning rate 0.0000
[2019-04-04 12:50:44,426] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48000, global step 768496: loss 7.8555
[2019-04-04 12:50:44,426] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48000, global step 768496: learning rate 0.0000
[2019-04-04 12:50:44,632] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48000, global step 768576: loss 7.8579
[2019-04-04 12:50:44,642] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48000, global step 768578: learning rate 0.0000
[2019-04-04 12:50:44,890] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48000, global step 768679: loss 7.8107
[2019-04-04 12:50:44,892] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48000, global step 768680: learning rate 0.0000
[2019-04-04 12:50:45,823] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48000, global step 769079: loss 7.8560
[2019-04-04 12:50:45,825] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48000, global step 769080: learning rate 0.0000
[2019-04-04 12:50:46,217] A3C_AGENT_WORKER-Thread-12 INFO:Local step 48000, global step 769280: loss 7.7216
[2019-04-04 12:50:46,217] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 48000, global step 769280: learning rate 0.0000
[2019-04-04 12:50:47,214] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48000, global step 769814: loss 7.9136
[2019-04-04 12:50:47,215] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48000, global step 769816: learning rate 0.0000
[2019-04-04 12:50:47,765] A3C_AGENT_WORKER-Thread-6 INFO:Local step 48000, global step 770098: loss 7.9291
[2019-04-04 12:50:47,766] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 48000, global step 770098: learning rate 0.0000
[2019-04-04 12:50:53,269] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48500, global step 773067: loss 0.2761
[2019-04-04 12:50:53,273] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48500, global step 773068: learning rate 0.0000
[2019-04-04 12:50:55,075] A3C_AGENT_WORKER-Thread-13 INFO:Local step 48500, global step 773987: loss 0.2804
[2019-04-04 12:50:55,076] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 48500, global step 773987: learning rate 0.0000
[2019-04-04 12:50:55,545] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48500, global step 774235: loss 0.3056
[2019-04-04 12:50:55,547] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48500, global step 774237: learning rate 0.0000
[2019-04-04 12:50:56,264] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48500, global step 774653: loss 0.2624
[2019-04-04 12:50:56,265] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48500, global step 774653: learning rate 0.0000
[2019-04-04 12:50:56,365] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48500, global step 774705: loss 0.2548
[2019-04-04 12:50:56,366] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48500, global step 774705: learning rate 0.0000
[2019-04-04 12:50:58,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3267644e-09 1.6819245e-09 1.4550144e-14 6.2357800e-14 1.0000000e+00
 1.4281136e-10 3.1034061e-14], sum to 1.0000
[2019-04-04 12:50:58,211] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0249
[2019-04-04 12:50:58,258] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.833333333333334, 63.16666666666667, 107.3333333333333, 730.6666666666666, 26.0, 25.70166132974076, 0.4987244584603532, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3579000.0000, 
sim time next is 3579600.0000, 
raw observation next is [-4.666666666666667, 61.33333333333334, 109.1666666666667, 744.3333333333334, 26.0, 25.63804074252464, 0.490097004078879, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3333333333333333, 0.6133333333333334, 0.363888888888889, 0.8224677716390424, 0.6666666666666666, 0.6365033952103868, 0.663365668026293, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.96601075], dtype=float32), -1.296216]. 
=============================================
[2019-04-04 12:50:58,847] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48500, global step 775998: loss 0.2781
[2019-04-04 12:50:58,855] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48500, global step 776004: learning rate 0.0000
[2019-04-04 12:50:59,278] A3C_AGENT_WORKER-Thread-11 INFO:Local step 48500, global step 776224: loss 0.2770
[2019-04-04 12:50:59,281] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 48500, global step 776224: learning rate 0.0000
[2019-04-04 12:50:59,745] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48500, global step 776480: loss 0.2611
[2019-04-04 12:50:59,747] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48500, global step 776480: learning rate 0.0000
[2019-04-04 12:51:00,086] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.9994578e-09 4.9636201e-10 4.6719136e-15 4.6907380e-14 1.0000000e+00
 4.2638747e-11 1.8159273e-14], sum to 1.0000
[2019-04-04 12:51:00,089] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5369
[2019-04-04 12:51:00,101] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48500, global step 776684: loss 0.2404
[2019-04-04 12:51:00,101] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48500, global step 776684: learning rate 0.0000
[2019-04-04 12:51:00,145] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.5, 67.5, 100.0, 676.0, 26.0, 25.89934943938579, 0.5270313933836328, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3576600.0000, 
sim time next is 3577200.0000, 
raw observation next is [-5.333333333333333, 66.66666666666666, 101.8333333333333, 689.6666666666666, 26.0, 25.86297173476941, 0.5216927104727674, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3148661126500462, 0.6666666666666665, 0.3394444444444443, 0.7620626151012891, 0.6666666666666666, 0.6552476445641174, 0.6738975701575892, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2824879], dtype=float32), -0.64031816]. 
=============================================
[2019-04-04 12:51:00,499] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48500, global step 776904: loss 0.2655
[2019-04-04 12:51:00,500] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48500, global step 776905: learning rate 0.0000
[2019-04-04 12:51:00,693] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48500, global step 776997: loss 0.2486
[2019-04-04 12:51:00,695] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48500, global step 776997: learning rate 0.0000
[2019-04-04 12:51:00,906] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48500, global step 777103: loss 0.2485
[2019-04-04 12:51:00,910] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48500, global step 777104: learning rate 0.0000
[2019-04-04 12:51:01,956] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48500, global step 777668: loss 0.2909
[2019-04-04 12:51:01,957] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48500, global step 777668: learning rate 0.0000
[2019-04-04 12:51:02,186] A3C_AGENT_WORKER-Thread-12 INFO:Local step 48500, global step 777801: loss 0.2790
[2019-04-04 12:51:02,187] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 48500, global step 777801: learning rate 0.0000
[2019-04-04 12:51:03,248] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48500, global step 778450: loss 0.2779
[2019-04-04 12:51:03,249] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48500, global step 778450: learning rate 0.0000
[2019-04-04 12:51:03,788] A3C_AGENT_WORKER-Thread-6 INFO:Local step 48500, global step 778776: loss 0.3083
[2019-04-04 12:51:03,790] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 48500, global step 778777: learning rate 0.0000
[2019-04-04 12:51:05,172] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1421011e-08 1.0955482e-08 1.6713519e-13 2.2794096e-13 1.0000000e+00
 1.7118423e-09 5.8612620e-13], sum to 1.0000
[2019-04-04 12:51:05,181] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0168
[2019-04-04 12:51:05,195] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.0, 32.0, 46.5, 262.0, 26.0, 25.48153391373375, 0.396945809877811, 0.0, 1.0, 18753.19936548641], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3657600.0000, 
sim time next is 3658200.0000, 
raw observation next is [8.5, 31.0, 60.66666666666668, 309.0, 26.0, 25.53201179891075, 0.4071581534343641, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.698060941828255, 0.31, 0.20222222222222228, 0.3414364640883978, 0.6666666666666666, 0.6276676499092293, 0.6357193844781214, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31315172], dtype=float32), -0.35366002]. 
=============================================
[2019-04-04 12:51:06,237] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.96245631e-10 1.36197425e-11 1.79201136e-17 2.20570159e-16
 1.00000000e+00 5.56254587e-13 1.58070261e-17], sum to 1.0000
[2019-04-04 12:51:06,240] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6347
[2019-04-04 12:51:06,263] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.0, 71.66666666666666, 596.3333333333333, 26.0, 26.9096799805421, 0.7158404930101775, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3773400.0000, 
sim time next is 3774000.0000, 
raw observation next is [0.0, 60.00000000000001, 67.83333333333333, 567.6666666666666, 26.0, 26.94264666283271, 0.4443733955049413, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6000000000000001, 0.2261111111111111, 0.627255985267035, 0.6666666666666666, 0.7452205552360592, 0.6481244651683138, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3119048], dtype=float32), -0.82507616]. 
=============================================
[2019-04-04 12:51:06,279] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[91.87942 ]
 [92.05098 ]
 [92.19924 ]
 [92.372894]
 [92.52328 ]], R is [[91.68735504]
 [91.77048492]
 [91.8527832 ]
 [91.93425751]
 [92.01491547]].
[2019-04-04 12:51:07,057] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49000, global step 780578: loss 0.7369
[2019-04-04 12:51:07,059] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49000, global step 780583: learning rate 0.0000
[2019-04-04 12:51:09,198] A3C_AGENT_WORKER-Thread-13 INFO:Local step 49000, global step 781822: loss 0.7194
[2019-04-04 12:51:09,199] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 49000, global step 781822: learning rate 0.0000
[2019-04-04 12:51:09,304] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49000, global step 781871: loss 0.7047
[2019-04-04 12:51:09,329] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49000, global step 781877: learning rate 0.0000
[2019-04-04 12:51:10,375] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49000, global step 782363: loss 0.6591
[2019-04-04 12:51:10,376] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49000, global step 782363: learning rate 0.0000
[2019-04-04 12:51:10,509] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49000, global step 782430: loss 0.7164
[2019-04-04 12:51:10,509] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49000, global step 782430: learning rate 0.0000
[2019-04-04 12:51:11,754] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6041938e-10 3.5728896e-11 4.1692404e-16 6.8375146e-15 1.0000000e+00
 1.1667489e-10 8.3779415e-16], sum to 1.0000
[2019-04-04 12:51:11,757] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3026
[2019-04-04 12:51:11,777] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.333333333333333, 50.0, 0.0, 0.0, 26.0, 25.3800269595874, 0.5267062136157914, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3868800.0000, 
sim time next is 3869400.0000, 
raw observation next is [1.166666666666667, 50.5, 0.0, 0.0, 26.0, 25.48801160519466, 0.5274738084517159, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.49492151431209613, 0.505, 0.0, 0.0, 0.6666666666666666, 0.6240009670995551, 0.6758246028172387, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33625272], dtype=float32), -0.09245509]. 
=============================================
[2019-04-04 12:51:12,966] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49000, global step 783620: loss 0.7692
[2019-04-04 12:51:12,969] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49000, global step 783621: learning rate 0.0000
[2019-04-04 12:51:13,016] A3C_AGENT_WORKER-Thread-11 INFO:Local step 49000, global step 783646: loss 0.6503
[2019-04-04 12:51:13,019] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 49000, global step 783646: learning rate 0.0000
[2019-04-04 12:51:13,057] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.9202813e-09 2.2833599e-10 9.5294129e-15 6.4785606e-14 1.0000000e+00
 3.1892292e-10 3.3304450e-15], sum to 1.0000
[2019-04-04 12:51:13,057] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7016
[2019-04-04 12:51:13,067] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6666666666666667, 52.5, 0.0, 0.0, 26.0, 25.12721468329223, 0.4413179917864507, 0.0, 1.0, 65787.574258152], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3874200.0000, 
sim time next is 3874800.0000, 
raw observation next is [0.3333333333333334, 54.00000000000001, 0.0, 0.0, 26.0, 25.09848752210593, 0.4479592133001357, 0.0, 1.0, 197173.5309006435], 
processed observation next is [1.0, 0.8695652173913043, 0.4718374884579871, 0.54, 0.0, 0.0, 0.6666666666666666, 0.5915406268421609, 0.6493197377667119, 0.0, 1.0, 0.93892157571735], 
reward next is 0.0611, 
noisyNet noise sample is [array([-0.47416845], dtype=float32), 0.234607]. 
=============================================
[2019-04-04 12:51:13,648] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49000, global step 783953: loss 0.7176
[2019-04-04 12:51:13,650] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49000, global step 783953: learning rate 0.0000
[2019-04-04 12:51:14,293] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49000, global step 784250: loss 0.6117
[2019-04-04 12:51:14,299] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49000, global step 784251: learning rate 0.0000
[2019-04-04 12:51:14,566] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49000, global step 784398: loss 0.8621
[2019-04-04 12:51:14,567] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49000, global step 784398: learning rate 0.0000
[2019-04-04 12:51:14,833] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49000, global step 784518: loss 0.7277
[2019-04-04 12:51:14,836] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49000, global step 784518: learning rate 0.0000
[2019-04-04 12:51:14,841] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49000, global step 784520: loss 0.6825
[2019-04-04 12:51:14,842] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49000, global step 784520: learning rate 0.0000
[2019-04-04 12:51:16,280] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49000, global step 785142: loss 0.7202
[2019-04-04 12:51:16,283] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49000, global step 785142: learning rate 0.0000
[2019-04-04 12:51:16,522] A3C_AGENT_WORKER-Thread-12 INFO:Local step 49000, global step 785238: loss 0.6686
[2019-04-04 12:51:16,523] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 49000, global step 785238: learning rate 0.0000
[2019-04-04 12:51:17,684] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49000, global step 785782: loss 0.6788
[2019-04-04 12:51:17,685] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49000, global step 785782: learning rate 0.0000
[2019-04-04 12:51:17,898] A3C_AGENT_WORKER-Thread-6 INFO:Local step 49000, global step 785881: loss 0.7240
[2019-04-04 12:51:17,898] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 49000, global step 785881: learning rate 0.0000
[2019-04-04 12:51:23,761] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49500, global step 788775: loss 4.9380
[2019-04-04 12:51:23,762] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49500, global step 788775: learning rate 0.0000
[2019-04-04 12:51:25,649] A3C_AGENT_WORKER-Thread-13 INFO:Local step 49500, global step 789745: loss 4.5379
[2019-04-04 12:51:25,649] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 49500, global step 789745: learning rate 0.0000
[2019-04-04 12:51:25,979] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49500, global step 789928: loss 4.5257
[2019-04-04 12:51:25,981] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49500, global step 789928: learning rate 0.0000
[2019-04-04 12:51:27,175] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49500, global step 790629: loss 4.8804
[2019-04-04 12:51:27,180] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49500, global step 790631: learning rate 0.0000
[2019-04-04 12:51:27,350] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49500, global step 790750: loss 4.7818
[2019-04-04 12:51:27,351] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49500, global step 790750: learning rate 0.0000
[2019-04-04 12:51:28,819] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.3446570e-10 1.1819373e-10 4.3185849e-17 9.5290084e-16 1.0000000e+00
 1.1387798e-11 8.6205719e-16], sum to 1.0000
[2019-04-04 12:51:28,821] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1643
[2019-04-04 12:51:28,835] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.333333333333333, 28.16666666666667, 120.3333333333333, 832.6666666666667, 26.0, 26.59574527397086, 0.6227456400377805, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4104600.0000, 
sim time next is 4105200.0000, 
raw observation next is [1.666666666666667, 28.33333333333334, 120.1666666666667, 836.8333333333334, 26.0, 26.16089186325267, 0.6041792460771348, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5087719298245615, 0.2833333333333334, 0.40055555555555566, 0.9246777163904236, 0.6666666666666666, 0.6800743219377224, 0.7013930820257116, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19532412], dtype=float32), -2.0858555]. 
=============================================
[2019-04-04 12:51:29,255] A3C_AGENT_WORKER-Thread-11 INFO:Local step 49500, global step 791830: loss 5.0360
[2019-04-04 12:51:29,255] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 49500, global step 791830: learning rate 0.0000
[2019-04-04 12:51:29,378] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49500, global step 791905: loss 4.8191
[2019-04-04 12:51:29,380] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49500, global step 791906: learning rate 0.0000
[2019-04-04 12:51:29,826] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2823014e-09 6.8451789e-10 1.5339433e-14 1.0165279e-13 1.0000000e+00
 1.7258305e-10 1.5167838e-14], sum to 1.0000
[2019-04-04 12:51:29,826] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6747
[2019-04-04 12:51:29,838] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 38.0, 209.5, 572.3333333333334, 26.0, 25.10690134160112, 0.4104830310240861, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4196400.0000, 
sim time next is 4197000.0000, 
raw observation next is [2.0, 39.0, 205.0, 475.6666666666667, 26.0, 25.1379803204648, 0.4029189793440887, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.39, 0.6833333333333333, 0.525598526703499, 0.6666666666666666, 0.5948316933720665, 0.6343063264480295, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.881964], dtype=float32), 1.810057]. 
=============================================
[2019-04-04 12:51:29,860] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[77.96735 ]
 [77.78766 ]
 [77.44291 ]
 [77.19078 ]
 [76.950066]], R is [[78.16854095]
 [78.38685608]
 [78.6029892 ]
 [78.8169632 ]
 [78.93978119]].
[2019-04-04 12:51:30,192] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49500, global step 792373: loss 4.7769
[2019-04-04 12:51:30,193] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49500, global step 792373: learning rate 0.0000
[2019-04-04 12:51:30,403] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49500, global step 792516: loss 5.2502
[2019-04-04 12:51:30,406] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49500, global step 792519: learning rate 0.0000
[2019-04-04 12:51:30,558] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5201465e-10 7.4558606e-12 4.5440914e-16 1.9136749e-14 1.0000000e+00
 2.3675657e-11 3.2994258e-15], sum to 1.0000
[2019-04-04 12:51:30,558] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4641
[2019-04-04 12:51:30,586] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.2, 75.0, 0.0, 0.0, 26.0, 25.50353399775633, 0.4048944003877477, 0.0, 1.0, 37298.59378024314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4323600.0000, 
sim time next is 4324200.0000, 
raw observation next is [4.25, 74.5, 0.0, 0.0, 26.0, 25.53305170849246, 0.4099882391517264, 0.0, 1.0, 19304.21681493607], 
processed observation next is [1.0, 0.043478260869565216, 0.5803324099722993, 0.745, 0.0, 0.0, 0.6666666666666666, 0.6277543090410385, 0.6366627463839089, 0.0, 1.0, 0.09192484197588605], 
reward next is 0.9081, 
noisyNet noise sample is [array([0.20298067], dtype=float32), -0.48924118]. 
=============================================
[2019-04-04 12:51:30,719] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49500, global step 792716: loss 5.0839
[2019-04-04 12:51:30,721] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49500, global step 792718: learning rate 0.0000
[2019-04-04 12:51:30,955] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49500, global step 792851: loss 5.3394
[2019-04-04 12:51:30,967] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49500, global step 792852: learning rate 0.0000
[2019-04-04 12:51:31,119] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49500, global step 792939: loss 4.7167
[2019-04-04 12:51:31,121] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49500, global step 792940: learning rate 0.0000
[2019-04-04 12:51:31,900] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.2653465e-08 3.3179830e-09 2.3713502e-13 9.4708930e-13 9.9999988e-01
 7.1313564e-09 4.8565156e-13], sum to 1.0000
[2019-04-04 12:51:31,904] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0925
[2019-04-04 12:51:31,917] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 26.0, 25.41862282502426, 0.358670171060806, 0.0, 1.0, 37140.84443143949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4234800.0000, 
sim time next is 4235400.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.42071871298756, 0.3583651074329805, 0.0, 1.0, 36016.01695514028], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6183932260822967, 0.6194550358109935, 0.0, 1.0, 0.17150484264352514], 
reward next is 0.8285, 
noisyNet noise sample is [array([-0.9474195], dtype=float32), -1.4621824]. 
=============================================
[2019-04-04 12:51:32,282] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49500, global step 793574: loss 5.2255
[2019-04-04 12:51:32,284] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49500, global step 793575: learning rate 0.0000
[2019-04-04 12:51:32,431] A3C_AGENT_WORKER-Thread-12 INFO:Local step 49500, global step 793661: loss 5.3544
[2019-04-04 12:51:32,434] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 49500, global step 793661: learning rate 0.0000
[2019-04-04 12:51:33,617] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49500, global step 794336: loss 5.0022
[2019-04-04 12:51:33,621] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49500, global step 794337: learning rate 0.0000
[2019-04-04 12:51:33,794] A3C_AGENT_WORKER-Thread-6 INFO:Local step 49500, global step 794437: loss 5.0717
[2019-04-04 12:51:33,798] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 49500, global step 794440: learning rate 0.0000
[2019-04-04 12:51:36,528] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0053528e-08 1.1603933e-09 1.0127922e-14 1.4336625e-13 1.0000000e+00
 1.0379126e-09 2.8422831e-14], sum to 1.0000
[2019-04-04 12:51:36,530] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5633
[2019-04-04 12:51:36,537] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.133333333333334, 65.66666666666667, 0.0, 0.0, 26.0, 25.20137411506479, 0.3238580596932596, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4299600.0000, 
sim time next is 4300200.0000, 
raw observation next is [6.1, 66.5, 0.0, 0.0, 26.0, 25.13663145339499, 0.3109951346145273, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6315789473684211, 0.665, 0.0, 0.0, 0.6666666666666666, 0.5947192877829158, 0.6036650448715091, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5644305], dtype=float32), 0.3716873]. 
=============================================
[2019-04-04 12:51:37,211] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.1808881e-10 6.1395780e-12 6.2907437e-18 8.8206226e-17 1.0000000e+00
 3.5681733e-13 1.2544663e-17], sum to 1.0000
[2019-04-04 12:51:37,212] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7119
[2019-04-04 12:51:37,231] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.766666666666666, 47.0, 108.3333333333333, 694.1666666666667, 26.0, 26.92269906520482, 0.7054102009369864, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4354800.0000, 
sim time next is 4355400.0000, 
raw observation next is [9.383333333333333, 44.5, 109.6666666666667, 711.3333333333333, 26.0, 27.10220592019371, 0.7360767167940043, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.7225300092336104, 0.445, 0.3655555555555557, 0.7860036832412522, 0.6666666666666666, 0.7585171600161426, 0.7453589055980014, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5705158], dtype=float32), -0.96564853]. 
=============================================
[2019-04-04 12:51:37,241] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50000, global step 796474: loss 0.2526
[2019-04-04 12:51:37,243] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50000, global step 796474: learning rate 0.0000
[2019-04-04 12:51:38,877] A3C_AGENT_WORKER-Thread-13 INFO:Local step 50000, global step 797417: loss 0.2134
[2019-04-04 12:51:38,878] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 50000, global step 797417: learning rate 0.0000
[2019-04-04 12:51:39,188] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50000, global step 797605: loss 0.2248
[2019-04-04 12:51:39,189] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50000, global step 797606: learning rate 0.0000
[2019-04-04 12:51:40,793] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3961657e-10 4.7890153e-10 3.0385518e-16 1.4796247e-15 1.0000000e+00
 9.8421679e-12 1.1115869e-16], sum to 1.0000
[2019-04-04 12:51:40,796] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6233
[2019-04-04 12:51:40,818] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 26.0, 25.58013865667624, 0.4988278962875783, 0.0, 1.0, 30140.36229757636], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4432200.0000, 
sim time next is 4432800.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 26.0, 25.53390395755829, 0.5015270342549518, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6278253297965243, 0.667175678084984, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3081589], dtype=float32), 0.15833803]. 
=============================================
[2019-04-04 12:51:40,856] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50000, global step 798577: loss 0.2129
[2019-04-04 12:51:40,858] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50000, global step 798577: learning rate 0.0000
[2019-04-04 12:51:41,011] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50000, global step 798677: loss 0.2341
[2019-04-04 12:51:41,014] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50000, global step 798677: learning rate 0.0000
[2019-04-04 12:51:42,881] A3C_AGENT_WORKER-Thread-11 INFO:Local step 50000, global step 799695: loss 0.2491
[2019-04-04 12:51:42,885] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 50000, global step 799699: learning rate 0.0000
[2019-04-04 12:51:43,290] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8933231e-09 1.0016939e-10 1.4334724e-15 4.7216501e-15 1.0000000e+00
 3.6762086e-11 7.5291031e-15], sum to 1.0000
[2019-04-04 12:51:43,294] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2812
[2019-04-04 12:51:43,320] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.233333333333333, 67.66666666666667, 0.0, 0.0, 26.0, 25.39706659147348, 0.4365546064350753, 0.0, 1.0, 27131.67607687751], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4591200.0000, 
sim time next is 4591800.0000, 
raw observation next is [-1.3, 68.0, 0.0, 0.0, 26.0, 25.47148321126959, 0.4330175016483557, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.42659279778393355, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6226236009391325, 0.6443391672161186, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8104907], dtype=float32), -0.25445944]. 
=============================================
[2019-04-04 12:51:43,402] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50000, global step 799966: loss 0.2288
[2019-04-04 12:51:43,403] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50000, global step 799966: learning rate 0.0000
[2019-04-04 12:51:43,477] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 12:51:43,478] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 12:51:43,478] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:51:43,479] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 12:51:43,479] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 12:51:43,479] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:51:43,481] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:51:43,482] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run9
[2019-04-04 12:51:43,503] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run9
[2019-04-04 12:51:43,517] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run9
[2019-04-04 12:52:42,662] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15643705], dtype=float32), 0.17747499]
[2019-04-04 12:52:42,662] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-0.8333333333333334, 52.33333333333334, 17.5, 71.16666666666666, 26.0, 25.01669614606812, 0.2872553163541796, 0.0, 1.0, 28111.66014668912]
[2019-04-04 12:52:42,662] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 12:52:42,663] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.5306162e-08 1.6677760e-09 7.0643131e-14 3.3396083e-13 1.0000000e+00
 8.6631574e-10 9.4657606e-14], sampled 0.9387863660041957
[2019-04-04 12:53:26,222] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 12:53:45,506] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.9546 263349534.0755 1552.0578
[2019-04-04 12:53:47,251] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 12:53:48,274] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 800000, evaluation results [800000.0, 7241.954599640733, 263349534.0754509, 1552.057776716069, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 12:53:48,549] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50000, global step 800135: loss 0.2947
[2019-04-04 12:53:48,550] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50000, global step 800135: learning rate 0.0000
[2019-04-04 12:53:49,506] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50000, global step 800620: loss 0.2146
[2019-04-04 12:53:49,508] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50000, global step 800620: learning rate 0.0000
[2019-04-04 12:53:49,716] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50000, global step 800736: loss 0.2293
[2019-04-04 12:53:49,717] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50000, global step 800736: learning rate 0.0000
[2019-04-04 12:53:49,796] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50000, global step 800784: loss 0.1873
[2019-04-04 12:53:49,802] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50000, global step 800785: learning rate 0.0000
[2019-04-04 12:53:49,874] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50000, global step 800830: loss 0.2070
[2019-04-04 12:53:49,880] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50000, global step 800831: learning rate 0.0000
[2019-04-04 12:53:50,846] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50000, global step 801295: loss 0.2226
[2019-04-04 12:53:50,847] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50000, global step 801295: learning rate 0.0000
[2019-04-04 12:53:50,980] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.26720300e-09 2.12824022e-10 9.76440100e-16 1.78414954e-14
 1.00000000e+00 1.78190865e-10 1.23441195e-14], sum to 1.0000
[2019-04-04 12:53:50,981] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9838
[2019-04-04 12:53:50,992] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 54.5, 0.0, 0.0, 26.0, 25.98758376127605, 0.653546690260176, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4656600.0000, 
sim time next is 4657200.0000, 
raw observation next is [2.0, 55.33333333333334, 0.0, 0.0, 26.0, 25.98348104781402, 0.6458273265388702, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.5533333333333335, 0.0, 0.0, 0.6666666666666666, 0.665290087317835, 0.7152757755129567, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.6802123], dtype=float32), -0.0746418]. 
=============================================
[2019-04-04 12:53:51,335] A3C_AGENT_WORKER-Thread-12 INFO:Local step 50000, global step 801555: loss 0.2399
[2019-04-04 12:53:51,336] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 50000, global step 801555: learning rate 0.0000
[2019-04-04 12:53:52,082] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50000, global step 801971: loss 0.1832
[2019-04-04 12:53:52,082] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50000, global step 801971: learning rate 0.0000
[2019-04-04 12:53:52,951] A3C_AGENT_WORKER-Thread-6 INFO:Local step 50000, global step 802432: loss 0.2300
[2019-04-04 12:53:52,954] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 50000, global step 802432: learning rate 0.0000
[2019-04-04 12:53:54,621] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6615267e-11 2.3472769e-12 5.4297469e-19 1.2541559e-16 1.0000000e+00
 4.2422272e-13 6.3484077e-18], sum to 1.0000
[2019-04-04 12:53:54,623] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3507
[2019-04-04 12:53:54,653] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.833333333333333, 44.16666666666667, 169.0, 135.0, 26.0, 27.39730131597974, 0.878228213257251, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4636200.0000, 
sim time next is 4636800.0000, 
raw observation next is [6.0, 43.0, 156.0, 138.0, 26.0, 27.60081104332747, 0.7752662351131798, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6288088642659281, 0.43, 0.52, 0.15248618784530388, 0.6666666666666666, 0.800067586943956, 0.7584220783710599, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51661366], dtype=float32), -0.21910249]. 
=============================================
[2019-04-04 12:53:57,618] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50500, global step 804894: loss 0.1632
[2019-04-04 12:53:57,621] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50500, global step 804895: learning rate 0.0000
[2019-04-04 12:53:57,685] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2258418e-09 2.4374114e-10 8.2658181e-16 1.9960501e-14 1.0000000e+00
 5.4360927e-10 2.4938363e-15], sum to 1.0000
[2019-04-04 12:53:57,688] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1146
[2019-04-04 12:53:57,739] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.04108247485341, 0.4173759372445494, 0.0, 1.0, 82735.78854105824], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4737600.0000, 
sim time next is 4738200.0000, 
raw observation next is [-1.166666666666667, 79.16666666666667, 0.0, 0.0, 26.0, 24.97269058958238, 0.428706828773702, 0.0, 1.0, 102046.201939598], 
processed observation next is [1.0, 0.8695652173913043, 0.43028624192059095, 0.7916666666666667, 0.0, 0.0, 0.6666666666666666, 0.5810575491318651, 0.6429022762579006, 0.0, 1.0, 0.4859342949504667], 
reward next is 0.5141, 
noisyNet noise sample is [array([0.31934312], dtype=float32), -1.599496]. 
=============================================
[2019-04-04 12:53:58,430] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5442772e-09 1.3182547e-10 3.1372909e-16 1.7399632e-14 1.0000000e+00
 2.4492297e-10 1.7641700e-15], sum to 1.0000
[2019-04-04 12:53:58,432] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0587
[2019-04-04 12:53:58,457] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.28085824685093, 0.4122384424836749, 0.0, 1.0, 101266.3489529473], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4684800.0000, 
sim time next is 4685400.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.2480303171883, 0.4498364618432537, 0.0, 1.0, 64343.3691955408], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6040025264323582, 0.6499454872810846, 0.0, 1.0, 0.3063969961692419], 
reward next is 0.6936, 
noisyNet noise sample is [array([-0.14464377], dtype=float32), 0.55222034]. 
=============================================
[2019-04-04 12:53:58,756] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2011329e-09 4.6405730e-11 4.7226949e-15 6.1592501e-15 1.0000000e+00
 1.2645003e-10 3.6029245e-15], sum to 1.0000
[2019-04-04 12:53:58,756] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1339
[2019-04-04 12:53:58,773] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 26.0, 25.65630565313035, 0.5168397414115833, 0.0, 1.0, 80259.7397160754], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4660200.0000, 
sim time next is 4660800.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 26.0, 25.51047072230646, 0.5165993309868049, 0.0, 1.0, 132286.0475562558], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6258725601922052, 0.6721997769956016, 0.0, 1.0, 0.6299335597916943], 
reward next is 0.3701, 
noisyNet noise sample is [array([0.86072713], dtype=float32), 0.29090276]. 
=============================================
[2019-04-04 12:53:59,444] A3C_AGENT_WORKER-Thread-13 INFO:Local step 50500, global step 805823: loss 0.2211
[2019-04-04 12:53:59,446] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 50500, global step 805823: learning rate 0.0000
[2019-04-04 12:53:59,742] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50500, global step 805982: loss 0.2627
[2019-04-04 12:53:59,744] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50500, global step 805982: learning rate 0.0000
[2019-04-04 12:54:01,250] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.9713969e-10 9.8621623e-12 3.4851240e-17 8.0088434e-17 1.0000000e+00
 7.8405919e-12 7.9661549e-17], sum to 1.0000
[2019-04-04 12:54:01,252] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8145
[2019-04-04 12:54:01,290] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 92.0, 52.5, 0.0, 26.0, 25.68803818745685, 0.5056494992100061, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4695600.0000, 
sim time next is 4696200.0000, 
raw observation next is [0.0, 92.0, 63.0, 0.0, 26.0, 25.96364056396854, 0.5208932836216473, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.21, 0.0, 0.6666666666666666, 0.663636713664045, 0.673631094540549, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42314607], dtype=float32), -1.0800872]. 
=============================================
[2019-04-04 12:54:01,625] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50500, global step 806854: loss 0.2368
[2019-04-04 12:54:01,625] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50500, global step 806854: learning rate 0.0000
[2019-04-04 12:54:01,864] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50500, global step 806992: loss 0.2479
[2019-04-04 12:54:01,866] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50500, global step 806992: learning rate 0.0000
[2019-04-04 12:54:03,775] A3C_AGENT_WORKER-Thread-11 INFO:Local step 50500, global step 807910: loss 0.2547
[2019-04-04 12:54:03,776] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 50500, global step 807910: learning rate 0.0000
[2019-04-04 12:54:04,028] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50500, global step 808037: loss 0.1878
[2019-04-04 12:54:04,030] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50500, global step 808037: learning rate 0.0000
[2019-04-04 12:54:04,034] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1328434e-08 7.9203227e-10 2.8105041e-14 3.5105632e-13 1.0000000e+00
 1.6802819e-10 1.7226665e-14], sum to 1.0000
[2019-04-04 12:54:04,035] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8757
[2019-04-04 12:54:04,047] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 35.0, 73.83333333333334, 488.3333333333333, 26.0, 25.21558016347118, 0.4215216326504747, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4812000.0000, 
sim time next is 4812600.0000, 
raw observation next is [3.0, 34.5, 65.66666666666667, 427.6666666666667, 26.0, 25.20814943814356, 0.4087683285435268, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.345, 0.2188888888888889, 0.47255985267034994, 0.6666666666666666, 0.6006791198452968, 0.636256109514509, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.69326526], dtype=float32), -0.99553776]. 
=============================================
[2019-04-04 12:54:04,510] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50500, global step 808259: loss 0.1651
[2019-04-04 12:54:04,511] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50500, global step 808259: learning rate 0.0000
[2019-04-04 12:54:05,564] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50500, global step 808754: loss 0.2183
[2019-04-04 12:54:05,566] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50500, global step 808754: learning rate 0.0000
[2019-04-04 12:54:05,626] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50500, global step 808791: loss 0.2154
[2019-04-04 12:54:05,627] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50500, global step 808791: learning rate 0.0000
[2019-04-04 12:54:05,672] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50500, global step 808824: loss 0.2340
[2019-04-04 12:54:05,674] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50500, global step 808824: learning rate 0.0000
[2019-04-04 12:54:05,717] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50500, global step 808850: loss 0.2384
[2019-04-04 12:54:05,718] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50500, global step 808850: learning rate 0.0000
[2019-04-04 12:54:06,361] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50500, global step 809240: loss 0.2583
[2019-04-04 12:54:06,362] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50500, global step 809241: learning rate 0.0000
[2019-04-04 12:54:06,850] A3C_AGENT_WORKER-Thread-12 INFO:Local step 50500, global step 809513: loss 0.2724
[2019-04-04 12:54:06,853] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 50500, global step 809513: learning rate 0.0000
[2019-04-04 12:54:07,521] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9067437e-08 1.7728208e-09 5.9003380e-14 4.8527290e-13 1.0000000e+00
 1.2395821e-09 7.0212201e-14], sum to 1.0000
[2019-04-04 12:54:07,521] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7695
[2019-04-04 12:54:07,534] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 49.0, 0.0, 0.0, 26.0, 25.53001250956783, 0.4306117096672839, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4825800.0000, 
sim time next is 4826400.0000, 
raw observation next is [0.3333333333333334, 49.66666666666667, 0.0, 0.0, 26.0, 25.5285987393007, 0.4202556122560115, 0.0, 1.0, 18749.90090601354], 
processed observation next is [0.0, 0.8695652173913043, 0.4718374884579871, 0.4966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6273832282750584, 0.6400852040853372, 0.0, 1.0, 0.08928524240958828], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.22620875], dtype=float32), 0.40331078]. 
=============================================
[2019-04-04 12:54:07,814] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50500, global step 810038: loss 0.2617
[2019-04-04 12:54:07,816] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50500, global step 810039: learning rate 0.0000
[2019-04-04 12:54:08,338] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5313288e-10 1.6125984e-10 5.7899657e-15 7.2248753e-14 1.0000000e+00
 1.4330852e-10 3.4584811e-15], sum to 1.0000
[2019-04-04 12:54:08,338] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4358
[2019-04-04 12:54:08,347] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 45.0, 90.5, 295.3333333333334, 26.0, 25.20801601448845, 0.3610763250576415, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4898400.0000, 
sim time next is 4899000.0000, 
raw observation next is [3.0, 45.0, 79.00000000000001, 273.6666666666667, 26.0, 25.17938394949487, 0.3495173855825682, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.26333333333333336, 0.3023941068139963, 0.6666666666666666, 0.5982819957912392, 0.6165057951941894, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.54534036], dtype=float32), -0.78402495]. 
=============================================
[2019-04-04 12:54:08,362] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.539986]
 [79.023506]
 [79.47344 ]
 [79.95704 ]
 [80.380196]], R is [[78.27984619]
 [78.49704742]
 [78.71207428]
 [78.92495728]
 [79.13571167]].
[2019-04-04 12:54:08,653] A3C_AGENT_WORKER-Thread-6 INFO:Local step 50500, global step 810499: loss 0.2886
[2019-04-04 12:54:08,655] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 50500, global step 810500: learning rate 0.0000
[2019-04-04 12:54:08,881] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7403494e-09 1.8951503e-10 6.4075391e-15 1.8759025e-14 1.0000000e+00
 4.6273704e-10 1.6391836e-14], sum to 1.0000
[2019-04-04 12:54:08,881] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4866
[2019-04-04 12:54:08,891] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 26.0, 26.32319564011541, 0.662297741516884, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4993200.0000, 
sim time next is 4993800.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 26.0, 26.34906873211974, 0.6538091035785021, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.6288088642659281, 0.23, 0.0, 0.0, 0.6666666666666666, 0.695755727676645, 0.7179363678595007, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.094809], dtype=float32), 2.4162142]. 
=============================================
[2019-04-04 12:54:09,693] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1373901e-08 5.6172605e-10 6.7946679e-14 4.5018955e-13 1.0000000e+00
 8.3028812e-10 3.3893797e-13], sum to 1.0000
[2019-04-04 12:54:09,698] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5000
[2019-04-04 12:54:09,716] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.51440274154695, 0.3123481207604588, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4933800.0000, 
sim time next is 4934400.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.41465357185336, 0.2967461376526185, 0.0, 1.0, 65475.10942325972], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6178877976544467, 0.5989153792175395, 0.0, 1.0, 0.3117862353488558], 
reward next is 0.6882, 
noisyNet noise sample is [array([0.04047551], dtype=float32), 0.6004626]. 
=============================================
[2019-04-04 12:54:11,658] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0220323e-08 1.3931621e-10 9.0231222e-16 1.7992166e-14 1.0000000e+00
 2.4590115e-11 6.6109582e-16], sum to 1.0000
[2019-04-04 12:54:11,660] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6770
[2019-04-04 12:54:11,695] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 41.33333333333334, 95.5, 586.1666666666666, 26.0, 25.43766120046036, 0.3456016635134538, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4956000.0000, 
sim time next is 4956600.0000, 
raw observation next is [-1.166666666666667, 40.16666666666666, 98.0, 612.3333333333333, 26.0, 25.52268391453682, 0.3810636058209161, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.43028624192059095, 0.40166666666666656, 0.32666666666666666, 0.676611418047882, 0.6666666666666666, 0.6268903262114017, 0.6270212019403053, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3388844], dtype=float32), -0.03622196]. 
=============================================
[2019-04-04 12:54:12,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:12,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:12,252] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run7
[2019-04-04 12:54:13,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:13,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:13,908] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run7
[2019-04-04 12:54:14,178] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:14,178] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:14,198] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run7
[2019-04-04 12:54:15,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:15,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:15,603] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run7
[2019-04-04 12:54:16,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:16,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:16,030] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run7
[2019-04-04 12:54:17,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:17,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:17,655] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run7
[2019-04-04 12:54:18,051] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:18,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:18,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run7
[2019-04-04 12:54:18,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:18,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:18,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run7
[2019-04-04 12:54:18,909] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.5813564e-11 5.4334337e-12 2.1488996e-18 9.1865587e-18 1.0000000e+00
 4.3930844e-12 5.2799869e-18], sum to 1.0000
[2019-04-04 12:54:18,910] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2621
[2019-04-04 12:54:18,935] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.0, 22.5, 118.0, 860.0, 26.0, 27.93756642412049, 0.8539347068771689, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5059800.0000, 
sim time next is 5060400.0000, 
raw observation next is [10.33333333333333, 21.66666666666666, 116.8333333333333, 853.1666666666667, 26.0, 27.55568717892058, 0.9272939985507748, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.7488457987072946, 0.21666666666666662, 0.3894444444444443, 0.9427255985267036, 0.6666666666666666, 0.7963072649100482, 0.8090979995169248, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65828365], dtype=float32), 0.010152223]. 
=============================================
[2019-04-04 12:54:19,000] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:19,000] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:19,006] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run7
[2019-04-04 12:54:19,161] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:19,161] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:19,164] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run7
[2019-04-04 12:54:19,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:19,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:19,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run7
[2019-04-04 12:54:19,443] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:19,443] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:19,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run7
[2019-04-04 12:54:19,708] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.3566976e-09 1.0801310e-10 1.3054952e-15 2.1221679e-14 1.0000000e+00
 3.5817344e-10 2.2626742e-15], sum to 1.0000
[2019-04-04 12:54:19,709] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0642
[2019-04-04 12:54:19,729] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.5, 19.66666666666666, 0.0, 0.0, 26.0, 26.37820400555838, 0.646384330167788, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5092800.0000, 
sim time next is 5093400.0000, 
raw observation next is [8.450000000000001, 19.83333333333334, 0.0, 0.0, 26.0, 26.26708086266966, 0.6279278492325208, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6966759002770084, 0.1983333333333334, 0.0, 0.0, 0.6666666666666666, 0.6889234052224715, 0.709309283077507, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7990542], dtype=float32), -1.1203822]. 
=============================================
[2019-04-04 12:54:19,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:19,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:19,844] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run7
[2019-04-04 12:54:20,074] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:20,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:20,079] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run7
[2019-04-04 12:54:20,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:20,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:20,920] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run7
[2019-04-04 12:54:21,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 12:54:21,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:54:21,649] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run7
[2019-04-04 12:54:30,515] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0548628e-09 3.9639531e-10 1.2354005e-15 4.3112500e-14 1.0000000e+00
 2.0816496e-10 1.2281831e-15], sum to 1.0000
[2019-04-04 12:54:30,516] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7874
[2019-04-04 12:54:30,567] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.583333333333334, 88.5, 0.0, 0.0, 26.0, 24.56812841502387, 0.2006389324496204, 0.0, 1.0, 46479.81782262964], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 64200.0000, 
sim time next is 64800.0000, 
raw observation next is [4.4, 89.0, 0.0, 0.0, 26.0, 24.5805628082141, 0.2031754109643814, 0.0, 1.0, 40458.85387966829], 
processed observation next is [0.0, 0.782608695652174, 0.5844875346260389, 0.89, 0.0, 0.0, 0.6666666666666666, 0.5483802340178418, 0.5677251369881272, 0.0, 1.0, 0.19266120895080138], 
reward next is 0.8073, 
noisyNet noise sample is [array([1.920533], dtype=float32), -1.0209157]. 
=============================================
[2019-04-04 12:54:32,515] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.0631319e-09 1.1924788e-09 1.7121580e-14 7.2680401e-14 1.0000000e+00
 5.8445260e-10 1.0701503e-14], sum to 1.0000
[2019-04-04 12:54:32,515] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5855
[2019-04-04 12:54:32,575] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.583333333333334, 88.5, 0.0, 0.0, 26.0, 24.56588089503239, 0.1992726616017365, 0.0, 1.0, 46527.16001203679], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 64200.0000, 
sim time next is 64800.0000, 
raw observation next is [4.4, 89.0, 0.0, 0.0, 26.0, 24.57847170783189, 0.2015079981745796, 0.0, 1.0, 40461.40861635192], 
processed observation next is [0.0, 0.782608695652174, 0.5844875346260389, 0.89, 0.0, 0.0, 0.6666666666666666, 0.5482059756526576, 0.5671693327248598, 0.0, 1.0, 0.19267337436358056], 
reward next is 0.8073, 
noisyNet noise sample is [array([0.15379609], dtype=float32), -0.06231693]. 
=============================================
[2019-04-04 12:54:33,602] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.0514233e-09 3.5600550e-10 8.2690347e-16 4.0194620e-14 1.0000000e+00
 9.6807909e-11 2.1565565e-15], sum to 1.0000
[2019-04-04 12:54:33,602] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8934
[2019-04-04 12:54:33,661] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.9, 84.0, 50.0, 0.0, 26.0, 24.46353370570381, 0.1786923169507566, 0.0, 1.0, 39430.65547232907], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 55800.0000, 
sim time next is 56400.0000, 
raw observation next is [6.8, 83.33333333333334, 44.66666666666667, 0.0, 26.0, 24.50006505917283, 0.1811829141649417, 0.0, 1.0, 20778.92341415155], 
processed observation next is [0.0, 0.6521739130434783, 0.6509695290858727, 0.8333333333333335, 0.1488888888888889, 0.0, 0.6666666666666666, 0.5416720882644025, 0.5603943047216472, 0.0, 1.0, 0.09894725435310263], 
reward next is 0.9011, 
noisyNet noise sample is [array([1.4725512], dtype=float32), 2.2040417]. 
=============================================
[2019-04-04 12:54:47,022] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2858753e-09 6.0452754e-10 2.6277135e-15 3.7003817e-14 1.0000000e+00
 1.0448546e-09 6.2279527e-16], sum to 1.0000
[2019-04-04 12:54:47,022] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9612
[2019-04-04 12:54:47,080] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.9348625745741, 0.2623225962873026, 0.0, 1.0, 56529.85561011331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 246000.0000, 
sim time next is 246600.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.93462412523466, 0.2546397250824329, 0.0, 1.0, 50372.2664557042], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5778853437695549, 0.5848799083608109, 0.0, 1.0, 0.23986793550335334], 
reward next is 0.7601, 
noisyNet noise sample is [array([-0.11907781], dtype=float32), -0.7243664]. 
=============================================
[2019-04-04 12:54:52,325] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3729913e-10 6.7750176e-12 4.1631852e-16 1.2378158e-15 1.0000000e+00
 1.0999687e-11 6.8070843e-17], sum to 1.0000
[2019-04-04 12:54:52,325] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2069
[2019-04-04 12:54:52,367] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 44.0, 88.5, 627.0, 26.0, 26.43156274554909, 0.5367044127121721, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 309600.0000, 
sim time next is 310200.0000, 
raw observation next is [-9.5, 43.66666666666667, 86.33333333333333, 625.6666666666666, 26.0, 26.44122867456986, 0.5277641828775451, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4366666666666667, 0.28777777777777774, 0.6913443830570902, 0.6666666666666666, 0.7034357228808217, 0.675921394292515, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6689028], dtype=float32), 1.0420401]. 
=============================================
[2019-04-04 12:55:02,937] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0782383e-07 2.3961759e-08 1.9064446e-12 1.9885361e-11 9.9999964e-01
 7.8276052e-08 2.0944509e-12], sum to 1.0000
[2019-04-04 12:55:02,938] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9514
[2019-04-04 12:55:02,950] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.9, 52.0, 0.0, 0.0, 26.0, 23.26850422859276, -0.1430612929143944, 0.0, 1.0, 45871.15095070061], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 441000.0000, 
sim time next is 441600.0000, 
raw observation next is [-10.8, 51.0, 0.0, 0.0, 26.0, 23.2597300257755, -0.1566255768216517, 0.0, 1.0, 45897.81845551586], 
processed observation next is [1.0, 0.08695652173913043, 0.1634349030470914, 0.51, 0.0, 0.0, 0.6666666666666666, 0.4383108354812917, 0.4477914743927827, 0.0, 1.0, 0.21856104026436124], 
reward next is 0.7814, 
noisyNet noise sample is [array([-2.3401427], dtype=float32), -1.73976]. 
=============================================
[2019-04-04 12:55:08,718] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2784885e-09 2.8279321e-10 1.0878982e-15 1.7123833e-14 1.0000000e+00
 1.5972559e-10 5.1109333e-15], sum to 1.0000
[2019-04-04 12:55:08,720] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9865
[2019-04-04 12:55:08,763] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 26.0, 24.97024553665863, 0.316983105377466, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 562800.0000, 
sim time next is 563400.0000, 
raw observation next is [-1.0, 80.5, 130.0, 396.0, 26.0, 24.96632711814342, 0.3143792523226868, 0.0, 1.0, 18730.72966097655], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.805, 0.43333333333333335, 0.4375690607734807, 0.6666666666666666, 0.5805272598452849, 0.6047930841075623, 0.0, 1.0, 0.08919395076655501], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.30607817], dtype=float32), 0.66268724]. 
=============================================
[2019-04-04 12:55:19,287] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3152768e-09 3.8652786e-11 1.5743007e-16 1.9621143e-15 1.0000000e+00
 5.0400595e-12 1.7328022e-15], sum to 1.0000
[2019-04-04 12:55:19,287] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5512
[2019-04-04 12:55:19,295] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.2333333333333334, 54.66666666666667, 123.1666666666667, 504.0, 26.0, 25.79166498146929, 0.3739693889285339, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 735600.0000, 
sim time next is 736200.0000, 
raw observation next is [-0.04999999999999999, 53.5, 131.0, 449.0, 26.0, 25.74934289186584, 0.361085134434981, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.461218836565097, 0.535, 0.43666666666666665, 0.49613259668508286, 0.6666666666666666, 0.6457785743221534, 0.620361711478327, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.76885885], dtype=float32), 1.1825756]. 
=============================================
[2019-04-04 12:55:20,792] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3636295e-08 5.1142618e-10 6.0897136e-14 3.3288289e-13 1.0000000e+00
 1.0464403e-09 3.6201612e-14], sum to 1.0000
[2019-04-04 12:55:20,792] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6350
[2019-04-04 12:55:20,810] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 24.71867761717925, 0.1558110137431093, 0.0, 1.0, 41699.50157116403], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 686400.0000, 
sim time next is 687000.0000, 
raw observation next is [-3.816666666666666, 70.66666666666667, 0.0, 0.0, 26.0, 24.68520167067255, 0.1480111871376663, 0.0, 1.0, 41630.89268653015], 
processed observation next is [0.0, 0.9565217391304348, 0.3568790397045245, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.5571001392227126, 0.5493370623792221, 0.0, 1.0, 0.19824234612633404], 
reward next is 0.8018, 
noisyNet noise sample is [array([-1.2517965], dtype=float32), 0.9179848]. 
=============================================
[2019-04-04 12:55:20,812] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.30794 ]
 [73.489914]
 [73.50484 ]
 [73.55989 ]
 [73.55586 ]], R is [[73.29574585]
 [73.36421967]
 [73.43161774]
 [73.49803162]
 [73.56351471]].
[2019-04-04 12:55:26,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2312829e-10 2.4050834e-11 4.1180902e-16 1.5537360e-15 1.0000000e+00
 2.8233804e-12 7.4968859e-16], sum to 1.0000
[2019-04-04 12:55:26,612] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3387
[2019-04-04 12:55:26,633] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.9, 79.66666666666667, 0.0, 0.0, 26.0, 24.78992092339084, 0.2479712404688331, 0.0, 1.0, 41164.83268914613], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 859800.0000, 
sim time next is 860400.0000, 
raw observation next is [-2.8, 79.0, 0.0, 0.0, 26.0, 24.79127911542975, 0.2419548332751285, 0.0, 1.0, 41034.67993281442], 
processed observation next is [1.0, 1.0, 0.38504155124653744, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5659399262858124, 0.5806516110917095, 0.0, 1.0, 0.19540323777530674], 
reward next is 0.8046, 
noisyNet noise sample is [array([0.41417384], dtype=float32), 0.48990113]. 
=============================================
[2019-04-04 12:55:30,702] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7600400e-08 7.7700490e-10 1.4359880e-14 3.6069921e-13 1.0000000e+00
 2.1521898e-10 1.8463826e-14], sum to 1.0000
[2019-04-04 12:55:30,703] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6633
[2019-04-04 12:55:30,718] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.9, 68.33333333333333, 0.0, 0.0, 26.0, 23.52353289257687, -0.06770849887698009, 0.0, 1.0, 42069.28455643335], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 801600.0000, 
sim time next is 802200.0000, 
raw observation next is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 26.0, 23.51506603800055, -0.07783891625938807, 0.0, 1.0, 42040.81952507074], 
processed observation next is [1.0, 0.2608695652173913, 0.2742382271468144, 0.6766666666666667, 0.0, 0.0, 0.6666666666666666, 0.45958883650004595, 0.474053694580204, 0.0, 1.0, 0.20019437869081305], 
reward next is 0.7998, 
noisyNet noise sample is [array([-0.59656584], dtype=float32), -1.7904962]. 
=============================================
[2019-04-04 12:55:36,807] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2773746e-10 1.7901515e-11 4.1876644e-17 5.7003719e-16 1.0000000e+00
 5.9604292e-12 1.9401198e-16], sum to 1.0000
[2019-04-04 12:55:36,809] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6288
[2019-04-04 12:55:36,829] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.4, 96.66666666666666, 0.0, 0.0, 26.0, 25.07084299046677, 0.2688755455604734, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 929400.0000, 
sim time next is 930000.0000, 
raw observation next is [4.4, 97.33333333333333, 0.0, 0.0, 26.0, 24.86895971307145, 0.2533145789868047, 1.0, 1.0, 97064.194441834], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.9733333333333333, 0.0, 0.0, 0.6666666666666666, 0.5724133094226209, 0.5844381929956016, 1.0, 1.0, 0.46221044972301906], 
reward next is 0.5378, 
noisyNet noise sample is [array([0.3053362], dtype=float32), -0.46044397]. 
=============================================
[2019-04-04 12:55:36,836] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[91.24809 ]
 [91.345375]
 [91.28683 ]
 [91.508705]
 [91.58859 ]], R is [[90.93156433]
 [91.02224731]
 [91.1120224 ]
 [91.20090485]
 [91.28889465]].
[2019-04-04 12:55:40,288] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7771534e-10 9.8119582e-11 5.6617502e-17 1.9499138e-16 1.0000000e+00
 8.5364554e-12 9.8209955e-16], sum to 1.0000
[2019-04-04 12:55:40,293] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6452
[2019-04-04 12:55:40,301] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.63333333333333, 54.66666666666667, 165.8333333333333, 0.0, 26.0, 27.48268086249493, 0.8480808414234048, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1086000.0000, 
sim time next is 1086600.0000, 
raw observation next is [18.71666666666667, 54.33333333333333, 160.6666666666667, 0.0, 26.0, 27.13214917071312, 0.9009350338474059, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.981071098799631, 0.5433333333333333, 0.5355555555555557, 0.0, 0.6666666666666666, 0.76101243089276, 0.8003116779491353, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9874824], dtype=float32), 0.31840542]. 
=============================================
[2019-04-04 12:55:41,307] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1063859e-08 5.3564297e-10 7.1610359e-15 6.8854563e-14 1.0000000e+00
 9.8636349e-11 4.8818189e-15], sum to 1.0000
[2019-04-04 12:55:41,313] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6685
[2019-04-04 12:55:41,319] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.63333333333333, 69.66666666666667, 90.83333333333333, 0.0, 26.0, 25.67846641770106, 0.5779012902914583, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1158000.0000, 
sim time next is 1158600.0000, 
raw observation next is [16.91666666666667, 68.33333333333333, 98.66666666666666, 0.0, 26.0, 25.60674510817127, 0.5624901721398338, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.9312096029547556, 0.6833333333333332, 0.32888888888888884, 0.0, 0.6666666666666666, 0.6338954256809393, 0.6874967240466113, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.64530957], dtype=float32), -0.030417956]. 
=============================================
[2019-04-04 12:55:42,669] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6089331e-10 6.9734340e-12 1.0416652e-16 2.1580285e-16 1.0000000e+00
 2.3041738e-12 1.0912935e-16], sum to 1.0000
[2019-04-04 12:55:42,672] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7400
[2019-04-04 12:55:42,689] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.7439422719972, 0.6334939485667626, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1048200.0000, 
sim time next is 1048800.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 25.93694839440095, 0.6452161508529174, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6614123662000791, 0.7150720502843058, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-3.5743492], dtype=float32), 0.318029]. 
=============================================
[2019-04-04 12:55:50,357] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2209113e-07 4.6753339e-08 3.6493087e-12 5.7251731e-11 9.9999964e-01
 1.1961168e-08 5.6920696e-12], sum to 1.0000
[2019-04-04 12:55:50,358] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7227
[2019-04-04 12:55:50,369] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.1, 79.33333333333333, 0.0, 0.0, 26.0, 24.12406340681297, 0.2835890004956961, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1212000.0000, 
sim time next is 1212600.0000, 
raw observation next is [16.1, 79.66666666666667, 0.0, 0.0, 26.0, 24.14439609257419, 0.2794456390185909, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5120330077145159, 0.5931485463395303, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.034236], dtype=float32), 0.32515788]. 
=============================================
[2019-04-04 12:55:51,865] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.53860089e-09 1.27766700e-10 4.78461860e-16 3.05467457e-14
 1.00000000e+00 1.15128566e-10 6.29992348e-16], sum to 1.0000
[2019-04-04 12:55:51,866] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1956
[2019-04-04 12:55:51,880] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.600000000000001, 99.33333333333334, 0.0, 0.0, 26.0, 25.52830650977625, 0.6029617293815989, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1284600.0000, 
sim time next is 1285200.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.5301846127222, 0.5951781715005421, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6275153843935165, 0.698392723833514, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9160787], dtype=float32), 0.012880545]. 
=============================================
[2019-04-04 12:55:54,838] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.8290067e-10 4.3481378e-11 1.0834207e-16 5.0603379e-15 1.0000000e+00
 3.6749746e-11 6.2896940e-16], sum to 1.0000
[2019-04-04 12:55:54,841] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2969
[2019-04-04 12:55:54,870] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.200000000000001, 95.0, 0.0, 0.0, 26.0, 25.41072692872288, 0.590695422192915, 0.0, 1.0, 40479.48734479403], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1297200.0000, 
sim time next is 1297800.0000, 
raw observation next is [4.1, 94.5, 0.0, 0.0, 26.0, 25.48786076419876, 0.5924165064278707, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.5761772853185596, 0.945, 0.0, 0.0, 0.6666666666666666, 0.6239883970165634, 0.6974721688092903, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05798084], dtype=float32), -1.7866813]. 
=============================================
[2019-04-04 12:55:55,217] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.8506915e-10 1.1465124e-11 2.0945552e-16 4.0221135e-15 1.0000000e+00
 4.7785361e-12 2.5168771e-16], sum to 1.0000
[2019-04-04 12:55:55,218] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5596
[2019-04-04 12:55:55,231] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.44001753808602, 0.5947424856601512, 0.0, 1.0, 34532.10112494566], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1288800.0000, 
sim time next is 1289400.0000, 
raw observation next is [5.500000000000001, 100.0, 0.0, 0.0, 26.0, 25.45073173900299, 0.5948524333673694, 0.0, 1.0, 28918.16352707963], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6208943115835824, 0.6982841444557898, 0.0, 1.0, 0.1377055406051411], 
reward next is 0.8623, 
noisyNet noise sample is [array([-0.14019835], dtype=float32), 0.1428039]. 
=============================================
[2019-04-04 12:55:55,483] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6804678e-11 7.4407026e-12 1.6236106e-17 1.6302708e-16 1.0000000e+00
 2.2942321e-12 8.5434053e-17], sum to 1.0000
[2019-04-04 12:55:55,486] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9690
[2019-04-04 12:55:55,523] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 36.0, 0.0, 26.0, 26.05969478306725, 0.5780948305819665, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1329000.0000, 
sim time next is 1329600.0000, 
raw observation next is [0.5, 92.0, 40.5, 0.0, 26.0, 26.00237402491443, 0.5814912792267374, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.135, 0.0, 0.6666666666666666, 0.6668645020762026, 0.6938304264089125, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0614353], dtype=float32), 0.09207353]. 
=============================================
[2019-04-04 12:56:04,737] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5557098e-09 1.5419574e-10 3.8245822e-16 2.2862652e-15 1.0000000e+00
 1.1761085e-10 1.1284466e-15], sum to 1.0000
[2019-04-04 12:56:04,737] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8881
[2019-04-04 12:56:04,749] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 82.00000000000001, 0.0, 0.0, 26.0, 25.45988316201126, 0.5550283084019634, 0.0, 1.0, 53152.93446353033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1555800.0000, 
sim time next is 1556400.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.45909044762929, 0.5589669481036421, 0.0, 1.0, 42457.21795152664], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6215908706357741, 0.6863223160345474, 0.0, 1.0, 0.20217722834060306], 
reward next is 0.7978, 
noisyNet noise sample is [array([1.4645392], dtype=float32), 0.25149614]. 
=============================================
[2019-04-04 12:56:06,927] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3377277e-10 1.1295723e-12 4.6719399e-19 6.1506034e-17 1.0000000e+00
 2.6652825e-12 2.1989458e-17], sum to 1.0000
[2019-04-04 12:56:06,928] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0789
[2019-04-04 12:56:06,949] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.266666666666667, 79.66666666666667, 97.5, 700.1666666666667, 26.0, 25.04896042811094, 0.4998282251462601, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1514400.0000, 
sim time next is 1515000.0000, 
raw observation next is [6.733333333333333, 76.33333333333333, 95.0, 700.3333333333334, 26.0, 25.31015733721325, 0.5485806838723387, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.649122807017544, 0.7633333333333333, 0.31666666666666665, 0.7738489871086557, 0.6666666666666666, 0.6091797781011042, 0.6828602279574462, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45795113], dtype=float32), 0.19714954]. 
=============================================
[2019-04-04 12:56:06,973] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[99.29805]
 [99.15938]
 [98.85928]
 [98.43969]
 [97.92667]], R is [[99.30019379]
 [99.30718994]
 [99.31411743]
 [99.32097626]
 [99.32776642]].
[2019-04-04 12:56:13,213] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7642693e-09 6.3474982e-11 1.0664164e-16 2.0089972e-15 1.0000000e+00
 1.0388152e-10 2.3110264e-16], sum to 1.0000
[2019-04-04 12:56:13,214] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4314
[2019-04-04 12:56:13,245] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.25, 93.5, 0.0, 0.0, 26.0, 25.43541144089194, 0.4840968144155151, 0.0, 1.0, 59715.88232666881], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1722600.0000, 
sim time next is 1723200.0000, 
raw observation next is [0.1666666666666667, 94.0, 0.0, 0.0, 26.0, 25.36615097947244, 0.4880868403450229, 0.0, 1.0, 83856.70365706121], 
processed observation next is [1.0, 0.9565217391304348, 0.4672206832871654, 0.94, 0.0, 0.0, 0.6666666666666666, 0.6138459149560367, 0.6626956134483409, 0.0, 1.0, 0.39931763646219626], 
reward next is 0.6007, 
noisyNet noise sample is [array([-0.16449887], dtype=float32), 0.29072815]. 
=============================================
[2019-04-04 12:56:16,532] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.7746662e-09 6.8276540e-10 8.4104154e-15 9.8760064e-14 1.0000000e+00
 6.7208489e-11 3.2370042e-15], sum to 1.0000
[2019-04-04 12:56:16,535] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0239
[2019-04-04 12:56:16,590] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 87.0, 108.0, 0.0, 26.0, 24.89826037136509, 0.335672938168311, 0.0, 1.0, 63799.49107580001], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1767600.0000, 
sim time next is 1768200.0000, 
raw observation next is [-2.3, 86.33333333333333, 111.6666666666667, 0.0, 26.0, 24.89496022366657, 0.3414907312035306, 0.0, 1.0, 55560.83401214974], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.8633333333333333, 0.37222222222222234, 0.0, 0.6666666666666666, 0.5745800186388808, 0.6138302437345101, 0.0, 1.0, 0.2645754000578559], 
reward next is 0.7354, 
noisyNet noise sample is [array([-1.245361], dtype=float32), -1.6764278]. 
=============================================
[2019-04-04 12:56:31,728] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.9318146e-10 8.6649209e-11 1.5943778e-16 3.8542574e-15 1.0000000e+00
 2.3741376e-10 2.0872411e-16], sum to 1.0000
[2019-04-04 12:56:31,729] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5777
[2019-04-04 12:56:31,783] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.05, 79.0, 147.0, 0.0, 26.0, 25.61352569069711, 0.3350823063451669, 1.0, 1.0, 24356.03192696632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2028600.0000, 
sim time next is 2029200.0000, 
raw observation next is [-4.866666666666667, 77.66666666666667, 148.5, 0.0, 26.0, 25.64371396602892, 0.3401586953553579, 1.0, 1.0, 23531.22701046582], 
processed observation next is [1.0, 0.4782608695652174, 0.3277931671283472, 0.7766666666666667, 0.495, 0.0, 0.6666666666666666, 0.6369761638357433, 0.6133862317851193, 1.0, 1.0, 0.11205346195459914], 
reward next is 0.8879, 
noisyNet noise sample is [array([1.1630671], dtype=float32), 0.11016531]. 
=============================================
[2019-04-04 12:56:42,681] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7026043e-09 6.6378403e-10 4.0966892e-15 1.4544594e-14 1.0000000e+00
 2.8591718e-10 3.3128195e-15], sum to 1.0000
[2019-04-04 12:56:42,681] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0206
[2019-04-04 12:56:42,724] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 70.5, 128.0, 0.0, 26.0, 26.07871888346779, 0.4904730482255305, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2211000.0000, 
sim time next is 2211600.0000, 
raw observation next is [-3.9, 70.0, 124.0, 0.0, 26.0, 26.26037101348868, 0.5044942653357004, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.7, 0.41333333333333333, 0.0, 0.6666666666666666, 0.6883642511240566, 0.6681647551119001, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1156396], dtype=float32), 1.1593436]. 
=============================================
[2019-04-04 12:56:51,089] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1157966e-08 9.5953157e-10 1.6034339e-14 1.7544774e-13 1.0000000e+00
 1.4233962e-09 8.4938897e-15], sum to 1.0000
[2019-04-04 12:56:51,089] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8207
[2019-04-04 12:56:51,130] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666666, 70.16666666666667, 0.0, 0.0, 26.0, 25.41523387832932, 0.3803971607900744, 1.0, 1.0, 27581.76156883023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2229000.0000, 
sim time next is 2229600.0000, 
raw observation next is [-4.733333333333333, 70.33333333333334, 0.0, 0.0, 26.0, 25.2443245278082, 0.3543646828154648, 1.0, 1.0, 32414.26670428433], 
processed observation next is [1.0, 0.8260869565217391, 0.3314866112650046, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.6036937106506833, 0.6181215609384882, 1.0, 1.0, 0.15435365097278253], 
reward next is 0.8456, 
noisyNet noise sample is [array([0.00756308], dtype=float32), -1.1250906]. 
=============================================
[2019-04-04 12:57:01,165] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0425151e-07 2.9773494e-08 5.4841873e-13 6.5600858e-12 9.9999976e-01
 4.7351554e-09 1.3724250e-12], sum to 1.0000
[2019-04-04 12:57:01,166] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1914
[2019-04-04 12:57:01,195] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 26.0, 24.21362900763091, 0.1088185192360192, 0.0, 1.0, 41173.27373945966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2355600.0000, 
sim time next is 2356200.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 26.0, 24.19917596976952, 0.09932683023787825, 0.0, 1.0, 41201.83455078412], 
processed observation next is [0.0, 0.2608695652173913, 0.37673130193905824, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5165979974807934, 0.533108943412626, 0.0, 1.0, 0.19619921214659106], 
reward next is 0.8038, 
noisyNet noise sample is [array([0.40964428], dtype=float32), -1.4722077]. 
=============================================
[2019-04-04 12:57:06,446] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.0956103e-08 2.0554697e-08 1.2287191e-12 1.2808187e-11 9.9999988e-01
 8.3575733e-09 7.2281764e-12], sum to 1.0000
[2019-04-04 12:57:06,448] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1305
[2019-04-04 12:57:06,466] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.9, 56.0, 0.0, 0.0, 26.0, 23.81871273398088, -0.01910342111326721, 0.0, 1.0, 43957.95838510197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2430600.0000, 
sim time next is 2431200.0000, 
raw observation next is [-8.0, 57.0, 0.0, 0.0, 26.0, 23.77428931563996, -0.02954008641373759, 0.0, 1.0, 44044.33087890616], 
processed observation next is [0.0, 0.13043478260869565, 0.24099722991689754, 0.57, 0.0, 0.0, 0.6666666666666666, 0.4811907763033301, 0.4901533045287541, 0.0, 1.0, 0.2097349089471722], 
reward next is 0.7903, 
noisyNet noise sample is [array([0.31358525], dtype=float32), 1.0969619]. 
=============================================
[2019-04-04 12:57:13,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.64401786e-09 6.18040064e-10 1.05744115e-13 4.66586297e-13
 1.00000000e+00 7.33721028e-10 6.85637595e-14], sum to 1.0000
[2019-04-04 12:57:13,376] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4536
[2019-04-04 12:57:13,428] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.95026502414984, 0.3355338535063581, 0.0, 1.0, 76772.5658528699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2577600.0000, 
sim time next is 2578200.0000, 
raw observation next is [-1.883333333333333, 46.0, 0.0, 0.0, 26.0, 24.95971637045575, 0.343801589145188, 0.0, 1.0, 54248.03499164766], 
processed observation next is [1.0, 0.8695652173913043, 0.4104339796860573, 0.46, 0.0, 0.0, 0.6666666666666666, 0.5799763642046459, 0.6146005297150626, 0.0, 1.0, 0.2583239761507031], 
reward next is 0.7417, 
noisyNet noise sample is [array([-0.36359453], dtype=float32), -0.109544836]. 
=============================================
[2019-04-04 12:57:19,464] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.4233163e-09 2.5641284e-10 8.9264449e-15 2.1131927e-14 1.0000000e+00
 2.0159575e-10 1.8300162e-14], sum to 1.0000
[2019-04-04 12:57:19,465] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1071
[2019-04-04 12:57:19,511] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 63.33333333333333, 0.0, 0.0, 26.0, 25.00220964840918, 0.3740183525861651, 0.0, 1.0, 23329.42470285992], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2664600.0000, 
sim time next is 2665200.0000, 
raw observation next is [-1.2, 63.66666666666667, 0.0, 0.0, 26.0, 25.02916427239285, 0.3829862445738735, 0.0, 1.0, 189214.8492330335], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5857636893660709, 0.6276620815246244, 0.0, 1.0, 0.9010230915858739], 
reward next is 0.0990, 
noisyNet noise sample is [array([0.5040701], dtype=float32), 0.012229527]. 
=============================================
[2019-04-04 12:57:22,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.0134480e-09 6.0720617e-10 3.2012698e-15 7.5644596e-14 1.0000000e+00
 3.1251765e-10 2.1073887e-14], sum to 1.0000
[2019-04-04 12:57:22,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8704
[2019-04-04 12:57:22,947] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.833333333333334, 63.16666666666666, 0.0, 0.0, 26.0, 24.44104423795924, 0.1290425923172348, 0.0, 1.0, 40924.89058643717], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2782200.0000, 
sim time next is 2782800.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.38290779869365, 0.1178839018853583, 0.0, 1.0, 41020.63150544681], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5319089832244709, 0.5392946339617861, 0.0, 1.0, 0.19533634050212764], 
reward next is 0.8047, 
noisyNet noise sample is [array([0.59631336], dtype=float32), -0.6703448]. 
=============================================
[2019-04-04 12:57:25,402] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.21565344e-08 2.89084506e-10 1.23206914e-14 9.27503110e-14
 1.00000000e+00 1.59071936e-10 9.25459813e-15], sum to 1.0000
[2019-04-04 12:57:25,403] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4628
[2019-04-04 12:57:25,421] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.76888677290717, 0.2720552368837709, 0.0, 1.0, 44134.24324881612], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2764200.0000, 
sim time next is 2764800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.73270997945984, 0.2636259061317492, 0.0, 1.0, 43861.36954513732], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5610591649549868, 0.5878753020439164, 0.0, 1.0, 0.2088636645006539], 
reward next is 0.7911, 
noisyNet noise sample is [array([-0.23449644], dtype=float32), 0.19882895]. 
=============================================
[2019-04-04 12:57:26,986] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.3353272e-09 6.6931177e-10 1.4254519e-14 1.2365239e-13 1.0000000e+00
 1.0139951e-09 4.0639889e-14], sum to 1.0000
[2019-04-04 12:57:26,987] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3879
[2019-04-04 12:57:27,012] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.833333333333334, 63.16666666666666, 0.0, 0.0, 26.0, 24.44104575743191, 0.1290431027787829, 0.0, 1.0, 40924.88927073566], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2782200.0000, 
sim time next is 2782800.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.38290930853793, 0.1178844079576715, 0.0, 1.0, 41020.63019022803], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5319091090448275, 0.5392948026525571, 0.0, 1.0, 0.1953363342391811], 
reward next is 0.8047, 
noisyNet noise sample is [array([-0.14425875], dtype=float32), -0.39524966]. 
=============================================
[2019-04-04 12:57:27,567] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2381337e-09 2.8335619e-11 1.8596698e-16 1.4911963e-14 1.0000000e+00
 5.0448686e-12 8.6500540e-16], sum to 1.0000
[2019-04-04 12:57:27,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2966
[2019-04-04 12:57:27,597] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.5, 56.5, 159.3333333333333, 324.6666666666666, 26.0, 25.92652492156721, 0.4110572763108416, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2800200.0000, 
sim time next is 2800800.0000, 
raw observation next is [-3.0, 55.0, 163.0, 370.5, 26.0, 25.93313409137191, 0.420393932301562, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.55, 0.5433333333333333, 0.4093922651933702, 0.6666666666666666, 0.661094507614326, 0.6401313107671873, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37613755], dtype=float32), -0.4084615]. 
=============================================
[2019-04-04 12:57:27,680] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.0391436e-10 1.0140908e-11 1.1473227e-17 1.3956266e-15 1.0000000e+00
 1.6625751e-11 4.4631246e-16], sum to 1.0000
[2019-04-04 12:57:27,681] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7545
[2019-04-04 12:57:27,711] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 93.0, 38.5, 47.5, 26.0, 25.87507327160234, 0.4709921661693466, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2912400.0000, 
sim time next is 2913000.0000, 
raw observation next is [1.833333333333333, 93.0, 27.66666666666666, 45.33333333333333, 26.0, 25.92634356562808, 0.4723752770606142, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5133887349953832, 0.93, 0.0922222222222222, 0.050092081031307543, 0.6666666666666666, 0.6605286304690067, 0.6574584256868714, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4240407], dtype=float32), 1.7767229]. 
=============================================
[2019-04-04 12:57:27,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[92.13217]
 [92.45335]
 [92.59358]
 [92.55978]
 [92.55131]], R is [[91.962883  ]
 [92.04325867]
 [92.12282562]
 [92.20159912]
 [92.27958679]].
[2019-04-04 12:57:30,139] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0640917e-09 1.5795630e-10 1.7257372e-15 9.1841546e-15 1.0000000e+00
 1.6728143e-11 3.8030069e-16], sum to 1.0000
[2019-04-04 12:57:30,140] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1575
[2019-04-04 12:57:30,163] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 81.5, 0.0, 0.0, 26.0, 25.39428440242559, 0.4813016243895221, 0.0, 1.0, 48323.2523035367], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2928600.0000, 
sim time next is 2929200.0000, 
raw observation next is [-1.0, 80.33333333333334, 0.0, 0.0, 26.0, 25.38145814156428, 0.481657374213853, 0.0, 1.0, 52583.10307273634], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.8033333333333335, 0.0, 0.0, 0.6666666666666666, 0.6151215117970233, 0.6605524580712844, 0.0, 1.0, 0.2503957289177921], 
reward next is 0.7496, 
noisyNet noise sample is [array([-0.39792114], dtype=float32), -0.7171181]. 
=============================================
[2019-04-04 12:57:31,931] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6763654e-10 5.5124988e-11 2.3014446e-17 9.9563408e-16 1.0000000e+00
 1.4739857e-11 3.6055350e-16], sum to 1.0000
[2019-04-04 12:57:31,933] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5975
[2019-04-04 12:57:31,952] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 97.66666666666666, 0.0, 0.0, 26.0, 24.99192311093055, 0.2441483376127192, 0.0, 1.0, 55925.1354633003], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2871600.0000, 
sim time next is 2872200.0000, 
raw observation next is [1.0, 98.83333333333334, 0.0, 0.0, 26.0, 24.89310544857761, 0.2448947600178673, 0.0, 1.0, 55708.69950202957], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9883333333333334, 0.0, 0.0, 0.6666666666666666, 0.5744254540481343, 0.5816315866726224, 0.0, 1.0, 0.2652795214382361], 
reward next is 0.7347, 
noisyNet noise sample is [array([1.7836592], dtype=float32), 0.65252644]. 
=============================================
[2019-04-04 12:57:41,720] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7009163e-08 1.7428242e-09 5.8000053e-14 1.2548285e-12 1.0000000e+00
 9.9505426e-10 1.4682910e-13], sum to 1.0000
[2019-04-04 12:57:41,720] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4667
[2019-04-04 12:57:41,779] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.08182573462041, 0.3274516489682588, 0.0, 1.0, 27736.13699984196], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3002400.0000, 
sim time next is 3003000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.04668687153268, 0.3206188442964366, 0.0, 1.0, 47589.38864612818], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5872239059610566, 0.6068729480988122, 0.0, 1.0, 0.2266161364101342], 
reward next is 0.7734, 
noisyNet noise sample is [array([-0.1597547], dtype=float32), -2.6726139]. 
=============================================
[2019-04-04 12:57:41,788] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[71.96404 ]
 [72.48094 ]
 [73.104256]
 [73.77584 ]
 [74.3971  ]], R is [[71.5928421 ]
 [71.7448349 ]
 [71.93827057]
 [71.97571564]
 [72.02547455]].
[2019-04-04 12:57:48,429] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-04 12:57:48,430] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 12:57:48,430] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 12:57:48,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:57:48,430] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 12:57:48,431] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:57:48,431] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 12:57:48,438] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run10
[2019-04-04 12:57:48,451] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run10
[2019-04-04 12:57:48,471] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run10
[2019-04-04 12:58:04,778] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15772577], dtype=float32), 0.17982055]
[2019-04-04 12:58:04,779] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-16.51666666666667, 82.5, 16.66666666666667, 334.6666666666667, 26.0, 23.65175439941385, -0.07160918142903352, 1.0, 1.0, 88994.86954837579]
[2019-04-04 12:58:04,780] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 12:58:04,781] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.3986841e-08 1.3630780e-08 5.2071910e-13 5.8197796e-12 9.9999988e-01
 4.3843182e-09 1.1843056e-12], sampled 0.9783800648124303
[2019-04-04 12:58:15,183] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15772577], dtype=float32), 0.17982055]
[2019-04-04 12:58:15,183] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.153134546833334, 68.21662669333334, 0.0, 0.0, 26.0, 24.72985079024107, 0.1577394563735525, 0.0, 1.0, 41316.56036203846]
[2019-04-04 12:58:15,183] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 12:58:15,184] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2438672e-08 8.2522644e-10 3.9272127e-14 2.4909198e-13 1.0000000e+00
 5.7919686e-10 5.5990111e-14], sampled 0.18903142411517615
[2019-04-04 12:58:22,884] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15772577], dtype=float32), 0.17982055]
[2019-04-04 12:58:22,885] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [12.98333333333333, 85.16666666666667, 122.0, 0.0, 26.0, 26.09101992070197, 0.6068249093248652, 1.0, 1.0, 0.0]
[2019-04-04 12:58:22,885] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 12:58:22,886] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.2684575e-10 1.2768966e-11 3.7135906e-17 6.2219684e-16 1.0000000e+00
 5.2420126e-12 6.3554139e-17], sampled 0.5070582813724911
[2019-04-04 12:58:30,290] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15772577], dtype=float32), 0.17982055]
[2019-04-04 12:58:30,290] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [5.0, 76.0, 139.5, 194.0, 26.0, 25.85182015254647, 0.5721530007940981, 1.0, 1.0, 0.0]
[2019-04-04 12:58:30,291] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 12:58:30,292] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.9506550e-10 4.0659955e-11 1.3566351e-16 2.3200107e-15 1.0000000e+00
 9.2745447e-12 2.6598026e-16], sampled 0.8917945782478591
[2019-04-04 12:59:30,453] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4172 239942393.0563 1605.0250
[2019-04-04 12:59:48,452] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.5627 263431841.6182 1551.9825
[2019-04-04 12:59:53,820] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6682 275799682.2396 1231.4887
[2019-04-04 12:59:54,842] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 900000, evaluation results [900000.0, 7241.562658960993, 263431841.61818957, 1551.9825344114975, 7353.417175922201, 239942393.05634084, 1605.024971488376, 7182.668179811327, 275799682.2396222, 1231.4886876164037]
[2019-04-04 12:59:56,231] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2742970e-09 6.1532272e-11 3.8096010e-15 3.2379181e-15 1.0000000e+00
 1.5380421e-10 2.5551388e-15], sum to 1.0000
[2019-04-04 12:59:56,234] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3337
[2019-04-04 12:59:56,285] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.83275282386899, 0.5446230670975601, 1.0, 1.0, 198613.680901878], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3264000.0000, 
sim time next is 3264600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.25046208781159, 0.6073636979042182, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6042051739842993, 0.7024545659680728, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36651078], dtype=float32), 0.11762895]. 
=============================================
[2019-04-04 13:00:00,450] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2596010e-10 4.5151084e-11 4.8607601e-17 2.2794887e-16 1.0000000e+00
 9.8787862e-12 1.0518915e-16], sum to 1.0000
[2019-04-04 13:00:00,452] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9910
[2019-04-04 13:00:00,470] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 26.0, 25.5657062564126, 0.6911221514701782, 0.0, 1.0, 161095.7275284027], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3184800.0000, 
sim time next is 3185400.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 26.0, 25.59060577691871, 0.7100543341263194, 0.0, 1.0, 69110.77130944654], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6325504814098926, 0.7366847780421065, 0.0, 1.0, 0.3290989109973645], 
reward next is 0.6709, 
noisyNet noise sample is [array([-0.93082446], dtype=float32), -0.24779494]. 
=============================================
[2019-04-04 13:00:01,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0304075e-09 1.6855148e-09 7.0940144e-15 3.4519864e-13 1.0000000e+00
 4.8785920e-10 1.2016199e-14], sum to 1.0000
[2019-04-04 13:00:01,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0174
[2019-04-04 13:00:01,634] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 77.0, 0.0, 0.0, 26.0, 24.76195458024647, 0.2665391113849586, 0.0, 1.0, 44206.4252363448], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3297600.0000, 
sim time next is 3298200.0000, 
raw observation next is [-9.083333333333334, 76.83333333333334, 0.0, 0.0, 26.0, 24.66028106469301, 0.2671806405980895, 0.0, 1.0, 43998.25219366558], 
processed observation next is [1.0, 0.17391304347826086, 0.21098799630655585, 0.7683333333333334, 0.0, 0.0, 0.6666666666666666, 0.5550234220577508, 0.5890602135326964, 0.0, 1.0, 0.20951548663650277], 
reward next is 0.7905, 
noisyNet noise sample is [array([-0.1456351], dtype=float32), -0.2849716]. 
=============================================
[2019-04-04 13:00:01,900] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7415578e-11 1.8563848e-12 1.5839976e-18 8.6481687e-17 1.0000000e+00
 1.2141493e-12 6.0927336e-18], sum to 1.0000
[2019-04-04 13:00:01,902] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5870
[2019-04-04 13:00:01,919] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.2, 75.5, 113.0, 811.0, 26.0, 26.59204230768594, 0.7464503555221759, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3238200.0000, 
sim time next is 3238800.0000, 
raw observation next is [-2.133333333333333, 74.0, 113.3333333333333, 813.0, 26.0, 26.61058765338333, 0.7484904949955982, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4035087719298246, 0.74, 0.37777777777777766, 0.8983425414364641, 0.6666666666666666, 0.7175489711152775, 0.7494968316651994, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2799352], dtype=float32), -0.48195386]. 
=============================================
[2019-04-04 13:00:03,613] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.1858446e-09 1.2257710e-10 7.7856907e-16 7.4921724e-15 1.0000000e+00
 9.4901260e-11 1.2370416e-15], sum to 1.0000
[2019-04-04 13:00:03,613] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6088
[2019-04-04 13:00:03,639] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 67.0, 0.0, 0.0, 26.0, 25.9513531394758, 0.6289477858830212, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3266400.0000, 
sim time next is 3267000.0000, 
raw observation next is [-4.0, 68.0, 0.0, 0.0, 26.0, 25.9147717909134, 0.6149697855852061, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6595643159094499, 0.704989928528402, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40176558], dtype=float32), 0.44032556]. 
=============================================
[2019-04-04 13:00:03,652] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[81.155464]
 [81.9617  ]
 [82.213936]
 [82.44002 ]
 [82.770065]], R is [[81.14021301]
 [81.32881165]
 [81.51552582]
 [81.70037079]
 [81.88336945]].
[2019-04-04 13:00:16,315] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5045247e-09 7.5550732e-10 3.5042979e-15 1.5527473e-14 1.0000000e+00
 5.0479154e-10 4.2590053e-15], sum to 1.0000
[2019-04-04 13:00:16,317] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4722
[2019-04-04 13:00:16,335] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.34332934545293, 0.3432775419844434, 0.0, 1.0, 41025.39590857465], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3724800.0000, 
sim time next is 3725400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.30902407907861, 0.338026041231724, 0.0, 1.0, 41044.68734234237], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6090853399232176, 0.6126753470772414, 0.0, 1.0, 0.19545089210639222], 
reward next is 0.8045, 
noisyNet noise sample is [array([0.01424119], dtype=float32), 0.7256428]. 
=============================================
[2019-04-04 13:00:19,558] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.2054475e-08 7.4654449e-10 1.0978674e-13 5.3770990e-13 1.0000000e+00
 2.6354110e-09 2.2353006e-13], sum to 1.0000
[2019-04-04 13:00:19,560] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9855
[2019-04-04 13:00:19,572] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 54.16666666666666, 0.0, 0.0, 26.0, 25.31789215335619, 0.367592918719642, 0.0, 1.0, 38574.90154910761], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3625800.0000, 
sim time next is 3626400.0000, 
raw observation next is [1.0, 48.33333333333334, 0.0, 0.0, 26.0, 25.31639464522011, 0.3707691383639962, 0.0, 1.0, 38478.60943869619], 
processed observation next is [0.0, 1.0, 0.4903047091412743, 0.48333333333333345, 0.0, 0.0, 0.6666666666666666, 0.6096995537683426, 0.6235897127879987, 0.0, 1.0, 0.1832314735176009], 
reward next is 0.8168, 
noisyNet noise sample is [array([2.0771797], dtype=float32), 1.3437371]. 
=============================================
[2019-04-04 13:00:21,345] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1615632e-08 1.2071960e-09 7.6861620e-14 1.0624428e-13 1.0000000e+00
 8.0667258e-11 3.2084537e-14], sum to 1.0000
[2019-04-04 13:00:21,347] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0474
[2019-04-04 13:00:21,363] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.0, 26.66666666666667, 99.0, 619.6666666666666, 26.0, 25.73551376671594, 0.4609251124665588, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3662400.0000, 
sim time next is 3663000.0000, 
raw observation next is [11.0, 27.0, 101.0, 663.0, 26.0, 25.71879279632576, 0.464139468168304, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.27, 0.33666666666666667, 0.7325966850828729, 0.6666666666666666, 0.6432327330271468, 0.6547131560561014, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37875208], dtype=float32), -0.20205048]. 
=============================================
[2019-04-04 13:00:21,373] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.26598 ]
 [74.692726]
 [74.07435 ]
 [73.44191 ]
 [72.85058 ]], R is [[76.05588531]
 [76.29532623]
 [76.53237152]
 [76.7670517 ]
 [76.99938202]].
[2019-04-04 13:00:28,061] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5610114e-09 1.1502650e-10 3.4639984e-16 4.8223496e-15 1.0000000e+00
 6.5392657e-11 6.6464351e-16], sum to 1.0000
[2019-04-04 13:00:28,062] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6567
[2019-04-04 13:00:28,078] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 73.0, 0.0, 0.0, 26.0, 25.32150738403237, 0.4440873862110463, 0.0, 1.0, 71101.48148278231], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3796800.0000, 
sim time next is 3797400.0000, 
raw observation next is [-3.0, 72.0, 0.0, 0.0, 26.0, 25.29921410089747, 0.4427335726657338, 0.0, 1.0, 55546.93784840955], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6082678417414558, 0.6475778575552446, 0.0, 1.0, 0.26450922784956926], 
reward next is 0.7355, 
noisyNet noise sample is [array([-2.8736494], dtype=float32), 0.065665565]. 
=============================================
[2019-04-04 13:00:29,250] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.9418198e-09 3.0733585e-10 3.8860576e-15 3.0167668e-14 1.0000000e+00
 8.9125131e-11 3.7122489e-15], sum to 1.0000
[2019-04-04 13:00:29,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0617
[2019-04-04 13:00:29,276] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.09887631754037, 0.3342664980662781, 0.0, 1.0, 43621.29332616163], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3814200.0000, 
sim time next is 3814800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.06095617447244, 0.324118042354987, 0.0, 1.0, 43615.37637605055], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5884130145393701, 0.6080393474516623, 0.0, 1.0, 0.2076922684573836], 
reward next is 0.7923, 
noisyNet noise sample is [array([-1.460288], dtype=float32), -0.5003052]. 
=============================================
[2019-04-04 13:00:29,698] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.6042393e-10 1.5644864e-11 2.5804656e-18 1.3751475e-16 1.0000000e+00
 1.9288553e-12 3.4380753e-17], sum to 1.0000
[2019-04-04 13:00:29,700] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9010
[2019-04-04 13:00:29,731] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 67.33333333333334, 99.33333333333333, 641.1666666666667, 26.0, 26.16052108004094, 0.5356941577802207, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3835200.0000, 
sim time next is 3835800.0000, 
raw observation next is [-3.0, 65.5, 101.0, 680.0, 26.0, 26.23515037954987, 0.5550730775399934, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.655, 0.33666666666666667, 0.7513812154696132, 0.6666666666666666, 0.686262531629156, 0.6850243591799977, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0899365], dtype=float32), -1.5074246]. 
=============================================
[2019-04-04 13:00:29,741] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.1913170e-10 6.6417122e-11 1.3886266e-16 2.7667188e-15 1.0000000e+00
 3.7153589e-11 2.3516144e-16], sum to 1.0000
[2019-04-04 13:00:29,741] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4419
[2019-04-04 13:00:29,750] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.0, 75.5, 625.0, 26.0, 26.87360174252905, 0.7122180522555975, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3772800.0000, 
sim time next is 3773400.0000, 
raw observation next is [0.0, 60.0, 71.66666666666666, 596.3333333333333, 26.0, 26.9096799805421, 0.7158404930101775, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6, 0.23888888888888885, 0.6589318600368324, 0.6666666666666666, 0.7424733317118418, 0.7386134976700592, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.65167457], dtype=float32), 0.7539894]. 
=============================================
[2019-04-04 13:00:34,889] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.72159403e-09 1.35810974e-10 5.77831562e-15 1.10503966e-13
 1.00000000e+00 2.80817342e-10 6.01551534e-15], sum to 1.0000
[2019-04-04 13:00:34,891] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1334
[2019-04-04 13:00:34,910] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.35004121415036, 0.4375273077150797, 0.0, 1.0, 40048.33842740517], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3890400.0000, 
sim time next is 3891000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.36258287851281, 0.435812542225472, 0.0, 1.0, 39843.54756274127], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6135485732094009, 0.6452708474084906, 0.0, 1.0, 0.1897311788701965], 
reward next is 0.8103, 
noisyNet noise sample is [array([0.46279046], dtype=float32), 0.93867636]. 
=============================================
[2019-04-04 13:00:34,924] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[79.70585 ]
 [79.71817 ]
 [79.780556]
 [79.78148 ]
 [80.08968 ]], R is [[79.68894958]
 [79.70135498]
 [79.71101379]
 [79.7128067 ]
 [79.69313812]].
[2019-04-04 13:00:41,536] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3951886e-08 1.2184338e-09 3.7482661e-14 1.9194115e-13 1.0000000e+00
 3.2760707e-09 8.8579975e-14], sum to 1.0000
[2019-04-04 13:00:41,537] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5914
[2019-04-04 13:00:41,562] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.12340783229716, 0.3374208911349341, 0.0, 1.0, 40818.53046298795], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4066200.0000, 
sim time next is 4066800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.18714140761337, 0.3354875598597832, 0.0, 1.0, 40816.78917764825], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5989284506344476, 0.6118291866199277, 0.0, 1.0, 0.19436566275070596], 
reward next is 0.8056, 
noisyNet noise sample is [array([-2.3931131], dtype=float32), -0.21326753]. 
=============================================
[2019-04-04 13:00:43,247] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5344319e-10 1.6986526e-10 1.2286557e-16 3.2193452e-15 1.0000000e+00
 2.5731809e-11 4.1869797e-16], sum to 1.0000
[2019-04-04 13:00:43,248] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8587
[2019-04-04 13:00:43,265] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6666666666666667, 28.66666666666667, 120.6666666666667, 824.3333333333334, 26.0, 26.28429347454077, 0.6183333379755839, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4103400.0000, 
sim time next is 4104000.0000, 
raw observation next is [1.0, 28.0, 120.5, 828.5, 26.0, 26.52100943701521, 0.4038100185454721, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.28, 0.40166666666666667, 0.9154696132596685, 0.6666666666666666, 0.7100841197512674, 0.6346033395151573, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3095649], dtype=float32), 0.990372]. 
=============================================
[2019-04-04 13:00:43,272] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[85.3984  ]
 [85.221375]
 [85.030846]
 [84.87566 ]
 [84.67467 ]], R is [[85.60260773]
 [85.74658203]
 [85.88911438]
 [86.03022766]
 [86.1699295 ]].
[2019-04-04 13:00:43,704] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8499336e-09 1.8021500e-10 1.5502785e-15 1.8746828e-14 1.0000000e+00
 2.6782748e-10 2.6419659e-15], sum to 1.0000
[2019-04-04 13:00:43,704] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7111
[2019-04-04 13:00:43,732] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 32.0, 117.5, 792.5, 26.0, 26.68425530128665, 0.6305498161113834, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4100400.0000, 
sim time next is 4101000.0000, 
raw observation next is [-0.6666666666666667, 31.33333333333334, 118.6666666666667, 800.3333333333334, 26.0, 26.7121346590459, 0.6381722488613221, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44413665743305636, 0.3133333333333334, 0.39555555555555566, 0.8843462246777164, 0.6666666666666666, 0.7260112215871585, 0.7127240829537741, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3939936], dtype=float32), -1.3665669]. 
=============================================
[2019-04-04 13:00:43,739] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.78936 ]
 [84.545555]
 [84.27953 ]
 [84.55433 ]
 [84.35561 ]], R is [[85.15003204]
 [85.29853058]
 [85.44554901]
 [85.59109497]
 [85.73518372]].
[2019-04-04 13:00:46,443] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0424301e-09 3.8960994e-11 1.6305763e-15 1.9167837e-14 1.0000000e+00
 7.8788684e-11 3.3024985e-15], sum to 1.0000
[2019-04-04 13:00:46,444] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3175
[2019-04-04 13:00:46,456] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 41.5, 0.0, 0.0, 26.0, 25.56946708597057, 0.4413435094836546, 0.0, 1.0, 104670.8267464719], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4141800.0000, 
sim time next is 4142400.0000, 
raw observation next is [0.3333333333333334, 42.0, 0.0, 0.0, 26.0, 25.41303357879806, 0.4422891897381485, 0.0, 1.0, 153824.7130770072], 
processed observation next is [1.0, 0.9565217391304348, 0.4718374884579871, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6177527982331718, 0.6474297299127162, 0.0, 1.0, 0.7324986337000343], 
reward next is 0.2675, 
noisyNet noise sample is [array([-0.602427], dtype=float32), -0.5696147]. 
=============================================
[2019-04-04 13:00:47,235] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.2004427e-08 2.4562015e-09 2.3892154e-13 2.1718410e-12 1.0000000e+00
 2.9580183e-09 2.6015743e-13], sum to 1.0000
[2019-04-04 13:00:47,236] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1314
[2019-04-04 13:00:47,273] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.23242824930461, 0.3665320902179959, 0.0, 1.0, 39452.37110626601], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4158000.0000, 
sim time next is 4158600.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.22268703544869, 0.3608733537546152, 0.0, 1.0, 39477.41059305484], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6018905862873908, 0.620291117918205, 0.0, 1.0, 0.18798766949073734], 
reward next is 0.8120, 
noisyNet noise sample is [array([1.5919883], dtype=float32), -0.8762144]. 
=============================================
[2019-04-04 13:00:49,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4558072e-08 1.6324641e-09 3.2587993e-14 1.4756494e-13 1.0000000e+00
 1.0279863e-10 3.2711857e-14], sum to 1.0000
[2019-04-04 13:00:49,667] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7462
[2019-04-04 13:00:49,708] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.666666666666667, 53.66666666666667, 185.6666666666667, 209.8333333333333, 26.0, 25.69826532888011, 0.4032646611311155, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4268400.0000, 
sim time next is 4269000.0000, 
raw observation next is [3.833333333333333, 53.83333333333333, 189.3333333333333, 288.6666666666666, 26.0, 25.63697601719255, 0.4088178396683237, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.5687903970452447, 0.5383333333333333, 0.631111111111111, 0.31896869244935533, 0.6666666666666666, 0.6364146680993791, 0.6362726132227746, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9715844], dtype=float32), -0.6361218]. 
=============================================
[2019-04-04 13:00:49,726] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.24185 ]
 [77.695526]
 [77.187874]
 [76.66774 ]
 [76.138916]], R is [[79.06354523]
 [79.27291107]
 [79.48018646]
 [79.68538666]
 [79.88853455]].
[2019-04-04 13:00:53,076] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1603234e-10 5.5291128e-12 2.6636190e-17 3.0793930e-16 1.0000000e+00
 1.3353148e-12 9.7127581e-17], sum to 1.0000
[2019-04-04 13:00:53,076] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4472
[2019-04-04 13:00:53,091] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 208.0, 88.5, 26.0, 26.32481398868743, 0.625153619792907, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4442400.0000, 
sim time next is 4443000.0000, 
raw observation next is [1.0, 86.0, 222.3333333333333, 107.6666666666667, 26.0, 26.34165610450786, 0.6380983582650701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.7411111111111109, 0.11896869244935547, 0.6666666666666666, 0.6951380087089882, 0.7126994527550234, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4698402], dtype=float32), 0.40190527]. 
=============================================
[2019-04-04 13:00:53,117] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[95.08784 ]
 [94.679405]
 [94.67191 ]
 [94.319466]
 [94.48031 ]], R is [[95.38790894]
 [95.43402863]
 [95.47969055]
 [95.52489471]
 [95.56964874]].
[2019-04-04 13:00:55,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.19889926e-10 8.59953601e-12 5.05265327e-18 1.03221136e-16
 1.00000000e+00 1.00155149e-12 1.97243120e-17], sum to 1.0000
[2019-04-04 13:00:55,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7546
[2019-04-04 13:00:55,291] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 251.0, 146.0, 26.0, 26.4552231734658, 0.6678855396756761, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4444200.0000, 
sim time next is 4444800.0000, 
raw observation next is [1.0, 86.0, 232.8333333333333, 121.6666666666667, 26.0, 26.48177001793555, 0.6653935465123301, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.776111111111111, 0.134438305709024, 0.6666666666666666, 0.7068141681612957, 0.7217978488374434, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1102729], dtype=float32), 0.44796062]. 
=============================================
[2019-04-04 13:01:00,982] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5004917e-10 8.5763089e-12 2.1791635e-17 6.3243641e-16 1.0000000e+00
 4.0961327e-12 1.3452967e-17], sum to 1.0000
[2019-04-04 13:01:00,984] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3235
[2019-04-04 13:01:01,005] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 78.0, 60.0, 0.0, 26.0, 26.27342764758215, 0.6133411032652768, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4464000.0000, 
sim time next is 4464600.0000, 
raw observation next is [0.0, 78.0, 56.33333333333333, 0.0, 26.0, 26.30516934017589, 0.6084701117733079, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.18777777777777777, 0.0, 0.6666666666666666, 0.6920974450146575, 0.7028233705911027, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0010108], dtype=float32), -0.5555887]. 
=============================================
[2019-04-04 13:01:02,108] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6478801e-10 6.2388796e-12 5.0794600e-17 3.5189777e-16 1.0000000e+00
 2.0357556e-11 8.4056274e-17], sum to 1.0000
[2019-04-04 13:01:02,112] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6649
[2019-04-04 13:01:02,157] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.81750787481053, 0.5948746585555122, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4471800.0000, 
sim time next is 4472400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.96598739596197, 0.5999384138666183, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.663832282996831, 0.6999794712888727, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13442639], dtype=float32), -0.6864485]. 
=============================================
[2019-04-04 13:01:05,691] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9657099e-08 5.5399912e-10 4.0228657e-15 4.5435413e-14 1.0000000e+00
 4.2263559e-10 2.0165612e-14], sum to 1.0000
[2019-04-04 13:01:05,703] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0503
[2019-04-04 13:01:05,727] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.57569374120136, 0.5448584014233823, 0.0, 1.0, 25841.57552309011], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4651800.0000, 
sim time next is 4652400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.49772253320247, 0.5485201360440115, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6248102111002058, 0.6828400453480038, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([-0.27472684], dtype=float32), 1.6021144]. 
=============================================
[2019-04-04 13:01:07,592] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7205411e-09 9.3144020e-11 4.1598538e-15 4.8081321e-15 1.0000000e+00
 3.4508857e-10 1.5568924e-15], sum to 1.0000
[2019-04-04 13:01:07,592] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3261
[2019-04-04 13:01:07,610] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.28207225936853, 0.4681002732716795, 0.0, 1.0, 48188.73746931923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4575600.0000, 
sim time next is 4576200.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.34754589496808, 0.4734842651853977, 0.0, 1.0, 41178.93226659994], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6122954912473398, 0.6578280883951325, 0.0, 1.0, 0.1960901536504759], 
reward next is 0.8039, 
noisyNet noise sample is [array([-1.4905567], dtype=float32), 0.6534449]. 
=============================================
[2019-04-04 13:01:08,516] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2205595e-09 3.6878123e-10 1.2796344e-15 1.5270547e-13 1.0000000e+00
 7.4026132e-11 8.4499702e-15], sum to 1.0000
[2019-04-04 13:01:08,517] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3852
[2019-04-04 13:01:08,594] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.833333333333333, 76.0, 0.0, 0.0, 26.0, 24.98957644119882, 0.3217974053656201, 0.0, 1.0, 36279.50673121424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4605000.0000, 
sim time next is 4605600.0000, 
raw observation next is [-2.666666666666667, 75.0, 0.0, 0.0, 26.0, 24.96422695534218, 0.3422100520189668, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.38873499538319484, 0.75, 0.0, 0.0, 0.6666666666666666, 0.580352246278515, 0.6140700173396556, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42617095], dtype=float32), 0.21577713]. 
=============================================
[2019-04-04 13:01:12,239] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.12718244e-10 5.01665098e-11 3.93190643e-17 1.17972716e-15
 1.00000000e+00 5.48152409e-12 7.81499662e-17], sum to 1.0000
[2019-04-04 13:01:12,239] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6464
[2019-04-04 13:01:12,253] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.4456268250396, 0.462571548246641, 0.0, 1.0, 18762.56086085694], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4686000.0000, 
sim time next is 4686600.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.593550252281, 0.4596464693864519, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6327958543567499, 0.6532154897954839, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4934537], dtype=float32), -0.7447098]. 
=============================================
[2019-04-04 13:01:15,663] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3118976e-10 5.3311904e-11 1.4255374e-16 8.2133275e-16 1.0000000e+00
 1.4585286e-11 6.2026515e-17], sum to 1.0000
[2019-04-04 13:01:15,663] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2329
[2019-04-04 13:01:15,687] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 72.0, 100.0, 11.0, 26.0, 26.1422845196872, 0.4325712899426994, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4725000.0000, 
sim time next is 4725600.0000, 
raw observation next is [1.0, 72.0, 88.16666666666667, 13.83333333333333, 26.0, 25.6757983611958, 0.4669931020837002, 1.0, 1.0, 63173.61031373393], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.72, 0.2938888888888889, 0.015285451197053403, 0.6666666666666666, 0.6396498634329832, 0.6556643673612333, 1.0, 1.0, 0.3008267157796854], 
reward next is 0.6992, 
noisyNet noise sample is [array([0.03074294], dtype=float32), -0.28703532]. 
=============================================
[2019-04-04 13:01:22,766] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7975136e-10 5.8705769e-11 1.2624745e-16 2.6974469e-15 1.0000000e+00
 3.9457264e-11 3.0556385e-16], sum to 1.0000
[2019-04-04 13:01:22,766] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8196
[2019-04-04 13:01:22,783] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.333333333333333, 24.66666666666667, 122.8333333333333, 861.6666666666667, 26.0, 27.12936802674267, 0.7268317546542243, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4969200.0000, 
sim time next is 4969800.0000, 
raw observation next is [6.5, 24.5, 123.0, 865.0, 26.0, 27.08799577820001, 0.7271159330444146, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6426592797783934, 0.245, 0.41, 0.9558011049723757, 0.6666666666666666, 0.7573329815166675, 0.7423719776814716, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32493034], dtype=float32), -1.065094]. 
=============================================
[2019-04-04 13:01:25,414] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8566862e-08 9.2645830e-10 1.6462680e-14 6.4949788e-14 1.0000000e+00
 5.9096983e-10 5.6157705e-14], sum to 1.0000
[2019-04-04 13:01:25,417] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9308
[2019-04-04 13:01:25,430] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.833333333333333, 49.33333333333334, 0.0, 0.0, 26.0, 25.03745137651805, 0.2320528959865091, 0.0, 1.0, 38646.89735431648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4945800.0000, 
sim time next is 4946400.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.01348269307109, 0.2339237049121905, 0.0, 1.0, 38658.70368384671], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5844568910892575, 0.5779745683040635, 0.0, 1.0, 0.18408906516117482], 
reward next is 0.8159, 
noisyNet noise sample is [array([0.22843672], dtype=float32), 0.45324072]. 
=============================================
[2019-04-04 13:01:27,086] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:27,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:27,150] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run8
[2019-04-04 13:01:27,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:27,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:27,383] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run8
[2019-04-04 13:01:29,224] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:29,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:29,236] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run8
[2019-04-04 13:01:29,439] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:29,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:29,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run8
[2019-04-04 13:01:30,399] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:30,400] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:30,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run8
[2019-04-04 13:01:30,587] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.0373044e-09 7.9227552e-10 1.8662809e-14 4.4626693e-14 1.0000000e+00
 2.9705119e-10 2.2970877e-15], sum to 1.0000
[2019-04-04 13:01:30,590] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7299
[2019-04-04 13:01:30,607] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 49.16666666666667, 0.0, 0.0, 26.0, 25.35314096340349, 0.3648936897754471, 0.0, 1.0, 61546.98169400336], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5033400.0000, 
sim time next is 5034000.0000, 
raw observation next is [-1.666666666666667, 52.33333333333334, 0.0, 0.0, 26.0, 25.33589397176345, 0.3581632372060812, 0.0, 1.0, 46066.03101429222], 
processed observation next is [1.0, 0.2608695652173913, 0.4164358264081256, 0.5233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6113244976469542, 0.6193877457353604, 0.0, 1.0, 0.21936205244901055], 
reward next is 0.7806, 
noisyNet noise sample is [array([0.16294219], dtype=float32), -0.40620315]. 
=============================================
[2019-04-04 13:01:30,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.28474 ]
 [82.210335]
 [82.098236]
 [82.06814 ]
 [82.105064]], R is [[82.0952301 ]
 [81.98119354]
 [81.77065277]
 [81.75215912]
 [81.79197693]].
[2019-04-04 13:01:32,651] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:32,651] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:32,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run8
[2019-04-04 13:01:32,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:32,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:32,754] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run8
[2019-04-04 13:01:33,071] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4739112e-04 5.4934504e-04 1.2315087e-06 7.3306833e-06 9.9898261e-01
 2.1105523e-04 1.0955061e-06], sum to 1.0000
[2019-04-04 13:01:33,072] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2598
[2019-04-04 13:01:33,085] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 22.5, 19.98525131905813, -0.8403984454905183, 0.0, 1.0, 46729.94630320209], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 5400.0000, 
sim time next is 6000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 23.0, 20.08493066718028, -0.8215530270588088, 0.0, 1.0, 44911.03880771768], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.4166666666666667, 0.1737442222650234, 0.22614899098039706, 0.0, 1.0, 0.2138620895605604], 
reward next is 0.7861, 
noisyNet noise sample is [array([0.23555574], dtype=float32), -0.72160023]. 
=============================================
[2019-04-04 13:01:33,087] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[32.319893]
 [28.903267]
 [25.246708]
 [21.59525 ]
 [19.07415 ]], R is [[36.39242172]
 [36.80597687]
 [37.19158173]
 [37.51277542]
 [37.73672867]].
[2019-04-04 13:01:33,130] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:33,130] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:33,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:33,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:33,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run8
[2019-04-04 13:01:33,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run8
[2019-04-04 13:01:33,198] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:33,198] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:33,203] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run8
[2019-04-04 13:01:33,494] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:33,494] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:33,502] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run8
[2019-04-04 13:01:33,940] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:33,940] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:33,944] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run8
[2019-04-04 13:01:33,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:33,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:33,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run8
[2019-04-04 13:01:35,954] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:35,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:35,959] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run8
[2019-04-04 13:01:36,002] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:36,002] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:36,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run8
[2019-04-04 13:01:36,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:01:36,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:01:36,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run8
[2019-04-04 13:01:46,207] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1203157e-09 1.3053442e-10 8.6975349e-16 3.3873227e-14 1.0000000e+00
 6.2688944e-11 4.3088477e-15], sum to 1.0000
[2019-04-04 13:01:46,207] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0024
[2019-04-04 13:01:46,260] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.533333333333333, 86.0, 80.33333333333334, 0.0, 26.0, 24.4548885295524, 0.1581889647977375, 0.0, 1.0, 20634.32395677726], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 51600.0000, 
sim time next is 52200.0000, 
raw observation next is [7.45, 86.0, 79.0, 0.0, 26.0, 24.45762780685796, 0.1655506387710453, 0.0, 1.0, 29061.80755755866], 
processed observation next is [0.0, 0.6086956521739131, 0.6689750692520776, 0.86, 0.2633333333333333, 0.0, 0.6666666666666666, 0.5381356505714967, 0.555183546257015, 0.0, 1.0, 0.13838955979789838], 
reward next is 0.8616, 
noisyNet noise sample is [array([0.16397211], dtype=float32), -0.008792629]. 
=============================================
[2019-04-04 13:02:08,732] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7400682e-08 4.7146431e-10 1.2683408e-14 2.6460030e-13 1.0000000e+00
 3.8489190e-10 4.5380847e-14], sum to 1.0000
[2019-04-04 13:02:08,732] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6968
[2019-04-04 13:02:08,747] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-14.2, 67.5, 0.0, 0.0, 26.0, 23.39066258829356, -0.08733972952596958, 0.0, 1.0, 47518.78781576727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 347400.0000, 
sim time next is 348000.0000, 
raw observation next is [-14.3, 68.0, 0.0, 0.0, 26.0, 23.32013632025695, -0.09517050763522372, 0.0, 1.0, 47583.22396281879], 
processed observation next is [1.0, 0.0, 0.06648199445983377, 0.68, 0.0, 0.0, 0.6666666666666666, 0.4433446933547458, 0.46827649745492544, 0.0, 1.0, 0.22658678077532757], 
reward next is 0.7734, 
noisyNet noise sample is [array([0.19642834], dtype=float32), -0.27396816]. 
=============================================
[2019-04-04 13:02:08,776] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.534706]
 [75.04241 ]
 [75.52719 ]
 [76.20037 ]
 [76.89971 ]], R is [[74.12880707]
 [74.16123962]
 [74.19367218]
 [74.22612762]
 [74.25860596]].
[2019-04-04 13:02:12,075] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2117660e-09 2.0747525e-10 1.0013093e-15 1.1384870e-14 1.0000000e+00
 1.0700851e-10 3.0877343e-15], sum to 1.0000
[2019-04-04 13:02:12,075] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5851
[2019-04-04 13:02:12,134] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.61666666666667, 51.0, 58.0, 858.0, 26.0, 25.99690693365066, 0.4181739019512168, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 389400.0000, 
sim time next is 390000.0000, 
raw observation next is [-12.43333333333333, 51.00000000000001, 58.0, 881.5, 26.0, 25.71895862409225, 0.3561031829095083, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.11819021237303795, 0.5100000000000001, 0.19333333333333333, 0.9740331491712707, 0.6666666666666666, 0.6432465520076874, 0.6187010609698361, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3035085], dtype=float32), -0.4278972]. 
=============================================
[2019-04-04 13:02:12,137] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[86.32698]
 [86.57243]
 [86.74543]
 [86.5742 ]
 [86.47946]], R is [[86.04424286]
 [86.18379974]
 [86.32196045]
 [85.76477051]
 [84.95464325]].
[2019-04-04 13:02:14,806] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.8843799e-09 2.0258098e-10 1.0891694e-14 5.6108885e-14 1.0000000e+00
 3.6210182e-10 4.5087310e-15], sum to 1.0000
[2019-04-04 13:02:14,813] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4027
[2019-04-04 13:02:14,934] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-16.15, 85.5, 25.0, 477.0, 26.0, 24.29436616590292, 0.07876528722391458, 1.0, 1.0, 84923.83306879135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 376200.0000, 
sim time next is 376800.0000, 
raw observation next is [-15.96666666666667, 87.0, 27.33333333333334, 520.5, 26.0, 24.70534990009992, 0.1477037269484628, 1.0, 1.0, 83905.30324622849], 
processed observation next is [1.0, 0.34782608695652173, 0.020313942751615764, 0.87, 0.09111111111111113, 0.5751381215469613, 0.6666666666666666, 0.5587791583416601, 0.5492345756494875, 1.0, 1.0, 0.3995490630772785], 
reward next is 0.6005, 
noisyNet noise sample is [array([0.9657222], dtype=float32), 1.3031883]. 
=============================================
[2019-04-04 13:02:20,339] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0451225e-08 2.0955937e-09 1.1760881e-14 1.4958117e-13 1.0000000e+00
 1.3365704e-09 2.8091214e-14], sum to 1.0000
[2019-04-04 13:02:20,340] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6781
[2019-04-04 13:02:20,385] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 35.0, 106.5, 0.0, 26.0, 25.5754128531393, 0.2513228844463852, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 482400.0000, 
sim time next is 483000.0000, 
raw observation next is [-0.5, 35.33333333333334, 102.3333333333333, 0.0, 26.0, 25.53833892097365, 0.1569862696600645, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.44875346260387816, 0.35333333333333344, 0.341111111111111, 0.0, 0.6666666666666666, 0.6281949100811376, 0.5523287565533549, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8938891], dtype=float32), -0.45123821]. 
=============================================
[2019-04-04 13:02:20,393] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[76.67055 ]
 [76.59708 ]
 [76.518936]
 [76.46562 ]
 [76.32293 ]], R is [[76.8911972 ]
 [77.12228394]
 [77.35105896]
 [77.5775528 ]
 [77.58086395]].
[2019-04-04 13:02:21,212] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1384188e-08 3.3978012e-09 6.1259442e-14 5.7025120e-13 1.0000000e+00
 2.8905047e-09 3.4044677e-14], sum to 1.0000
[2019-04-04 13:02:21,212] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9604
[2019-04-04 13:02:21,260] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.45, 26.5, 129.0, 0.0, 26.0, 25.16586331004726, 0.1540862020176708, 1.0, 1.0, 20039.58524093452], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 477000.0000, 
sim time next is 477600.0000, 
raw observation next is [-1.366666666666667, 27.0, 127.3333333333333, 0.0, 26.0, 25.11012217253636, 0.1441598501923167, 1.0, 1.0, 51189.96171353793], 
processed observation next is [1.0, 0.5217391304347826, 0.42474607571560485, 0.27, 0.42444444444444435, 0.0, 0.6666666666666666, 0.5925101810446968, 0.5480532833974389, 1.0, 1.0, 0.24376172244541872], 
reward next is 0.7562, 
noisyNet noise sample is [array([-0.8817892], dtype=float32), 0.84874725]. 
=============================================
[2019-04-04 13:02:26,590] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9328668e-09 7.9296000e-11 1.1915856e-16 1.6931397e-15 1.0000000e+00
 8.2007318e-11 1.7185756e-16], sum to 1.0000
[2019-04-04 13:02:26,590] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3467
[2019-04-04 13:02:26,633] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.516666666666667, 96.0, 0.0, 0.0, 26.0, 24.86175784382915, 0.2353612167913812, 0.0, 1.0, 47634.43953954786], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 507000.0000, 
sim time next is 507600.0000, 
raw observation next is [1.6, 96.0, 0.0, 0.0, 26.0, 24.87110058023067, 0.2358363506855707, 0.0, 1.0, 43771.67088988822], 
processed observation next is [1.0, 0.9130434782608695, 0.5069252077562327, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5725917150192226, 0.5786121168951902, 0.0, 1.0, 0.20843652804708676], 
reward next is 0.7916, 
noisyNet noise sample is [array([-0.54209775], dtype=float32), 1.7110558]. 
=============================================
[2019-04-04 13:02:26,954] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.4525063e-09 3.8625263e-09 7.7043881e-15 2.2142829e-13 1.0000000e+00
 3.3534134e-10 3.7674033e-14], sum to 1.0000
[2019-04-04 13:02:26,958] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7847
[2019-04-04 13:02:26,973] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 80.5, 0.0, 0.0, 26.0, 24.22130168971999, 0.103328626010452, 0.0, 1.0, 42430.78509452075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 613800.0000, 
sim time next is 614400.0000, 
raw observation next is [-3.899999999999999, 78.66666666666667, 0.0, 0.0, 26.0, 24.21732096989514, 0.0972993389224915, 0.0, 1.0, 42541.64545377057], 
processed observation next is [0.0, 0.08695652173913043, 0.35457063711911363, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5181100808245951, 0.5324331129741638, 0.0, 1.0, 0.20257926406557417], 
reward next is 0.7974, 
noisyNet noise sample is [array([1.110977], dtype=float32), -1.2919277]. 
=============================================
[2019-04-04 13:02:30,002] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0294781e-09 5.9727570e-11 2.2574324e-15 3.1328674e-15 1.0000000e+00
 1.5032094e-11 2.2401895e-15], sum to 1.0000
[2019-04-04 13:02:30,002] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9033
[2019-04-04 13:02:30,078] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.7666666666666667, 81.33333333333333, 102.6666666666667, 222.0, 26.0, 24.82394677690134, 0.2864824295398752, 0.0, 1.0, 58419.26072958916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 561000.0000, 
sim time next is 561600.0000, 
raw observation next is [-0.8, 81.0, 109.5, 265.5, 26.0, 24.83442827740254, 0.3077324379436036, 0.0, 1.0, 44118.84389192008], 
processed observation next is [0.0, 0.5217391304347826, 0.4404432132963989, 0.81, 0.365, 0.29337016574585634, 0.6666666666666666, 0.5695356897835451, 0.6025774793145345, 0.0, 1.0, 0.21008973281866705], 
reward next is 0.7899, 
noisyNet noise sample is [array([-1.1218436], dtype=float32), -0.19887228]. 
=============================================
[2019-04-04 13:02:32,175] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0547827e-08 1.0634160e-09 4.0678120e-14 1.1910625e-13 1.0000000e+00
 3.6285056e-10 4.5587235e-14], sum to 1.0000
[2019-04-04 13:02:32,176] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3758
[2019-04-04 13:02:32,189] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.56945608716747, 0.1262469785081362, 0.0, 1.0, 41448.47381779071], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 688800.0000, 
sim time next is 689400.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.53298911007905, 0.1199521056775246, 0.0, 1.0, 41377.97505052869], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5444157591732542, 0.5399840352258415, 0.0, 1.0, 0.19703797643108897], 
reward next is 0.8030, 
noisyNet noise sample is [array([-1.0123827], dtype=float32), 2.3965302]. 
=============================================
[2019-04-04 13:02:40,163] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.6292403e-09 1.2936947e-10 3.2666764e-15 1.4082572e-13 1.0000000e+00
 1.4955516e-10 1.8357067e-15], sum to 1.0000
[2019-04-04 13:02:40,163] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9589
[2019-04-04 13:02:40,179] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.31293230033046, 0.0578212978252603, 0.0, 1.0, 41267.06942617976], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 700800.0000, 
sim time next is 701400.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.27338387178298, 0.0541826952985431, 0.0, 1.0, 41347.44285766885], 
processed observation next is [1.0, 0.08695652173913043, 0.368421052631579, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5227819893152482, 0.5180608984328477, 0.0, 1.0, 0.19689258503651835], 
reward next is 0.8031, 
noisyNet noise sample is [array([-0.5087688], dtype=float32), 0.42455858]. 
=============================================
[2019-04-04 13:02:40,902] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8961528e-09 2.5962432e-11 5.4329900e-15 7.5922951e-15 1.0000000e+00
 9.8009226e-11 1.4049907e-15], sum to 1.0000
[2019-04-04 13:02:40,904] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0374
[2019-04-04 13:02:40,917] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.65923987095005, 0.2138826453185798, 0.0, 1.0, 40039.51215557791], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 864600.0000, 
sim time next is 865200.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.63525540625769, 0.2140634029270569, 0.0, 1.0, 39912.65325929441], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.6666666666666666, 0.5529379505214743, 0.5713544676423523, 0.0, 1.0, 0.19006025361568768], 
reward next is 0.8099, 
noisyNet noise sample is [array([0.9732665], dtype=float32), -1.240294]. 
=============================================
[2019-04-04 13:02:51,500] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1489057e-10 3.5671972e-11 4.9088540e-17 6.0390531e-17 1.0000000e+00
 4.8353248e-12 9.7850536e-17], sum to 1.0000
[2019-04-04 13:02:51,503] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2558
[2019-04-04 13:02:51,516] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 56.0, 176.0, 158.5, 26.0, 26.92865930950432, 0.8690070168089935, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1083600.0000, 
sim time next is 1084200.0000, 
raw observation next is [18.38333333333334, 55.66666666666667, 174.3333333333333, 105.6666666666666, 26.0, 27.05238169801985, 0.9049213574355006, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.9718374884579875, 0.5566666666666668, 0.5811111111111109, 0.11675874769797415, 0.6666666666666666, 0.7543651415016542, 0.8016404524785002, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6101159], dtype=float32), 0.089119926]. 
=============================================
[2019-04-04 13:02:53,552] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4309247e-10 4.9575528e-11 1.0751078e-17 4.3921060e-15 1.0000000e+00
 7.4829046e-11 1.5239565e-16], sum to 1.0000
[2019-04-04 13:02:53,554] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6783
[2019-04-04 13:02:53,567] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.48069166941104, 0.4610635938767084, 0.0, 1.0, 18757.31018437667], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 969000.0000, 
sim time next is 969600.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 26.0, 25.53902361684786, 0.4560312125260388, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.7063711911357342, 0.83, 0.0, 0.0, 0.6666666666666666, 0.628251968070655, 0.6520104041753463, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4506121], dtype=float32), 0.41001123]. 
=============================================
[2019-04-04 13:02:54,681] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5121905e-12 8.1056498e-12 4.0417641e-18 3.9092781e-17 1.0000000e+00
 1.5385099e-12 4.3595181e-17], sum to 1.0000
[2019-04-04 13:02:54,686] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4428
[2019-04-04 13:02:54,700] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.13333333333333, 71.66666666666667, 160.6666666666667, 71.66666666666666, 26.0, 27.26012040722195, 0.891902595749964, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1075800.0000, 
sim time next is 1076400.0000, 
raw observation next is [15.5, 70.0, 184.0, 107.5, 26.0, 27.26126781115158, 0.9081179583746491, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8919667590027703, 0.7, 0.6133333333333333, 0.11878453038674033, 0.6666666666666666, 0.771772317595965, 0.8027059861248831, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09090539], dtype=float32), -1.3669994]. 
=============================================
[2019-04-04 13:02:56,017] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.07246351e-10 1.75655844e-11 1.32887814e-17 3.27622153e-16
 1.00000000e+00 1.29716307e-11 2.33886460e-17], sum to 1.0000
[2019-04-04 13:02:56,020] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5708
[2019-04-04 13:02:56,031] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 76.0, 0.0, 0.0, 26.0, 25.8101745081609, 0.6215077411404419, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1027800.0000, 
sim time next is 1028400.0000, 
raw observation next is [14.4, 75.66666666666667, 0.0, 0.0, 26.0, 25.85742950461852, 0.6216346989881454, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6547857920515433, 0.7072115663293818, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.58223426], dtype=float32), 0.99882543]. 
=============================================
[2019-04-04 13:02:59,459] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5705736e-10 2.0397900e-11 7.8624510e-17 4.6191578e-15 1.0000000e+00
 8.6895977e-11 1.8555588e-17], sum to 1.0000
[2019-04-04 13:02:59,464] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6478
[2019-04-04 13:02:59,477] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.8, 92.0, 0.0, 0.0, 26.0, 25.52982976706482, 0.5687627339742763, 0.0, 1.0, 18749.45429273306], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1306200.0000, 
sim time next is 1306800.0000, 
raw observation next is [2.7, 92.0, 0.0, 0.0, 26.0, 25.52823814040941, 0.5647880853489209, 0.0, 1.0, 18746.34826034325], 
processed observation next is [1.0, 0.13043478260869565, 0.5373961218836566, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6273531783674509, 0.688262695116307, 0.0, 1.0, 0.08926832504925357], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.09383], dtype=float32), -0.4482805]. 
=============================================
[2019-04-04 13:03:00,194] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.31078455e-08 1.60690061e-09 3.95719156e-14 2.75753351e-13
 1.00000000e+00 1.21508714e-09 7.51111633e-15], sum to 1.0000
[2019-04-04 13:03:00,197] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8446
[2019-04-04 13:03:00,212] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.91666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.63859876446761, 0.6292653202786657, 0.0, 1.0, 21432.20060596871], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1133400.0000, 
sim time next is 1134000.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.64377201221104, 0.6282781389755918, 0.0, 1.0, 20036.27001187746], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6369810010175868, 0.7094260463251972, 0.0, 1.0, 0.09541080958036885], 
reward next is 0.9046, 
noisyNet noise sample is [array([0.8061628], dtype=float32), -1.5618172]. 
=============================================
[2019-04-04 13:03:00,222] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[84.073   ]
 [84.03213 ]
 [83.89941 ]
 [83.402435]
 [83.32562 ]], R is [[84.18460083]
 [84.24069977]
 [84.28321838]
 [84.30773163]
 [84.31658936]].
[2019-04-04 13:03:04,307] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3250516e-09 5.6239041e-11 1.8686869e-16 4.7748480e-15 1.0000000e+00
 6.1196092e-12 2.4488953e-16], sum to 1.0000
[2019-04-04 13:03:04,310] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1616
[2019-04-04 13:03:04,328] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.2, 97.33333333333333, 100.0, 0.0, 26.0, 25.04255237635704, 0.4811889263330959, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1254000.0000, 
sim time next is 1254600.0000, 
raw observation next is [14.1, 98.0, 101.0, 0.0, 26.0, 25.0130561161769, 0.477789598885596, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8531855955678671, 0.98, 0.33666666666666667, 0.0, 0.6666666666666666, 0.5844213430147418, 0.659263199628532, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2701192], dtype=float32), -0.2986973]. 
=============================================
[2019-04-04 13:03:07,783] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.5010281e-10 2.9475006e-11 8.3367471e-17 2.2706984e-15 1.0000000e+00
 9.6933876e-12 1.0443628e-16], sum to 1.0000
[2019-04-04 13:03:07,784] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7629
[2019-04-04 13:03:07,798] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.26040644172564, 0.5151966344809412, 0.0, 1.0, 40730.79149045727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1383600.0000, 
sim time next is 1384200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.39882217121571, 0.5177170986699184, 0.0, 1.0, 26081.12543739089], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6165685142679758, 0.6725723662233062, 0.0, 1.0, 0.1241958354161471], 
reward next is 0.8758, 
noisyNet noise sample is [array([-0.4774412], dtype=float32), -0.0796617]. 
=============================================
[2019-04-04 13:03:08,217] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1970940e-10 8.5395978e-12 9.9459727e-17 3.0399314e-16 1.0000000e+00
 6.9836428e-12 1.7856039e-17], sum to 1.0000
[2019-04-04 13:03:08,219] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3492
[2019-04-04 13:03:08,259] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 92.0, 12.0, 0.0, 26.0, 25.62581089292097, 0.5300073233241079, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1325400.0000, 
sim time next is 1326000.0000, 
raw observation next is [0.9000000000000001, 92.0, 15.0, 0.0, 26.0, 25.54458381353959, 0.5550287669888755, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.48753462603878117, 0.92, 0.05, 0.0, 0.6666666666666666, 0.6287153177949657, 0.6850095889962918, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3765643], dtype=float32), -0.8668308]. 
=============================================
[2019-04-04 13:03:08,275] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[94.01836]
 [93.65453]
 [93.14181]
 [92.90168]
 [92.76748]], R is [[94.20626068]
 [94.2641983 ]
 [94.32155609]
 [94.37834167]
 [94.43456268]].
[2019-04-04 13:03:09,052] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5025796e-10 2.5662309e-11 5.6451662e-17 9.3038671e-16 1.0000000e+00
 4.3072459e-11 3.8975741e-17], sum to 1.0000
[2019-04-04 13:03:09,061] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7754
[2019-04-04 13:03:09,096] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.33991598384427, 0.4793360488559338, 0.0, 1.0, 37798.11413233882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1470000.0000, 
sim time next is 1470600.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.35312418344309, 0.511142484936583, 0.0, 1.0, 37513.06248006945], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6127603486202574, 0.6703808283121943, 0.0, 1.0, 0.17863363085747358], 
reward next is 0.8214, 
noisyNet noise sample is [array([-2.491757], dtype=float32), -0.23638672]. 
=============================================
[2019-04-04 13:03:09,422] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0565456e-10 5.5971831e-11 1.9191486e-16 2.8910238e-16 1.0000000e+00
 9.0436617e-12 1.4065666e-16], sum to 1.0000
[2019-04-04 13:03:09,425] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2251
[2019-04-04 13:03:09,436] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.2, 93.0, 0.0, 0.0, 26.0, 25.50475132355183, 0.4808965871675209, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1477800.0000, 
sim time next is 1478400.0000, 
raw observation next is [2.2, 93.33333333333334, 0.0, 0.0, 26.0, 25.48039149124892, 0.4676734915313812, 0.0, 1.0, 18756.84385699767], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6233659576040766, 0.6558911638437938, 0.0, 1.0, 0.0893183040809413], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.7336277], dtype=float32), 0.75278765]. 
=============================================
[2019-04-04 13:03:13,137] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7559306e-09 1.5810520e-10 4.1528120e-16 1.1353298e-14 1.0000000e+00
 5.0439090e-11 2.6007878e-15], sum to 1.0000
[2019-04-04 13:03:13,138] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0434
[2019-04-04 13:03:13,151] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.2, 92.66666666666667, 0.0, 0.0, 26.0, 25.4355476339344, 0.4889172817397491, 0.0, 1.0, 18764.61214560379], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1477200.0000, 
sim time next is 1477800.0000, 
raw observation next is [2.2, 93.0, 0.0, 0.0, 26.0, 25.50531159113831, 0.4808904607263477, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6254426325948591, 0.6602968202421159, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2583439], dtype=float32), 0.923948]. 
=============================================
[2019-04-04 13:03:20,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3290016e-09 1.6130353e-10 1.3812574e-15 1.5565845e-14 1.0000000e+00
 8.9010292e-11 1.7263025e-16], sum to 1.0000
[2019-04-04 13:03:20,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4465
[2019-04-04 13:03:20,846] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.083333333333334, 81.50000000000001, 0.0, 0.0, 26.0, 25.69082565846347, 0.5051467154616155, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1577400.0000, 
sim time next is 1578000.0000, 
raw observation next is [5.166666666666667, 81.0, 0.0, 0.0, 26.0, 25.63322381843054, 0.4874029831350006, 0.0, 1.0, 22163.84467355729], 
processed observation next is [1.0, 0.2608695652173913, 0.6057248384118191, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6361019848692117, 0.6624676610450002, 0.0, 1.0, 0.10554211749312996], 
reward next is 0.8945, 
noisyNet noise sample is [array([-1.2017066], dtype=float32), 1.364322]. 
=============================================
[2019-04-04 13:03:20,858] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[89.706406]
 [89.763954]
 [89.790596]
 [89.823814]
 [89.85835 ]], R is [[89.58963776]
 [89.69374084]
 [89.79680634]
 [89.89884186]
 [89.99985504]].
[2019-04-04 13:03:23,191] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.40278694e-10 1.21690895e-11 5.01186086e-18 1.06583130e-15
 1.00000000e+00 2.52172588e-12 6.85043124e-17], sum to 1.0000
[2019-04-04 13:03:23,194] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7970
[2019-04-04 13:03:23,205] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.800000000000001, 84.66666666666667, 0.0, 0.0, 26.0, 25.99131065986735, 0.6266943578465319, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1635600.0000, 
sim time next is 1636200.0000, 
raw observation next is [6.9, 84.0, 0.0, 0.0, 26.0, 25.82975373054571, 0.6095682961377173, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6537396121883658, 0.84, 0.0, 0.0, 0.6666666666666666, 0.652479477545476, 0.7031894320459058, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6365104], dtype=float32), -0.3442135]. 
=============================================
[2019-04-04 13:03:23,948] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1510711e-09 1.1433028e-10 3.4042111e-16 1.6994432e-14 1.0000000e+00
 3.2147129e-11 2.4657494e-16], sum to 1.0000
[2019-04-04 13:03:23,949] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0517
[2019-04-04 13:03:23,961] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.800000000000001, 94.0, 0.0, 0.0, 26.0, 25.61915653062324, 0.5772380182594563, 0.0, 1.0, 36897.13463763845], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1646400.0000, 
sim time next is 1647000.0000, 
raw observation next is [6.9, 94.5, 0.0, 0.0, 26.0, 25.61634399145638, 0.5887102341972509, 0.0, 1.0, 33647.6733705372], 
processed observation next is [1.0, 0.043478260869565216, 0.6537396121883658, 0.945, 0.0, 0.0, 0.6666666666666666, 0.6346953326213649, 0.6962367447324169, 0.0, 1.0, 0.16022701605017717], 
reward next is 0.8398, 
noisyNet noise sample is [array([0.51986265], dtype=float32), 0.53496754]. 
=============================================
[2019-04-04 13:03:24,000] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[88.968994]
 [88.51857 ]
 [88.435776]
 [88.43994 ]
 [88.40604 ]], R is [[89.06467438]
 [88.99832916]
 [89.01691437]
 [88.96794891]
 [88.98234558]].
[2019-04-04 13:03:24,141] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.2109996e-10 6.7184133e-11 9.9820054e-17 2.9859253e-16 1.0000000e+00
 7.7807188e-12 2.9064146e-17], sum to 1.0000
[2019-04-04 13:03:24,145] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5332
[2019-04-04 13:03:24,152] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.63333333333333, 49.66666666666667, 89.16666666666666, 0.0, 26.0, 26.8036329013938, 0.7932147707214048, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1610400.0000, 
sim time next is 1611000.0000, 
raw observation next is [13.55, 50.0, 78.0, 0.0, 26.0, 27.04341717221623, 0.8135630670402887, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8379501385041552, 0.5, 0.26, 0.0, 0.6666666666666666, 0.7536180976846859, 0.7711876890134296, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0696916], dtype=float32), -0.668797]. 
=============================================
[2019-04-04 13:03:24,168] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[90.4884  ]
 [90.79545 ]
 [91.086136]
 [91.37229 ]
 [91.58443 ]], R is [[90.23740387]
 [90.3350296 ]
 [90.43167877]
 [90.52736664]
 [90.6220932 ]].
[2019-04-04 13:03:27,414] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4563611e-10 1.0383670e-11 3.0366730e-17 9.6963631e-16 1.0000000e+00
 1.3870862e-12 2.7546157e-17], sum to 1.0000
[2019-04-04 13:03:27,415] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1271
[2019-04-04 13:03:27,453] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 75.5, 0.0, 26.0, 25.75329659793566, 0.525200143946966, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1694400.0000, 
sim time next is 1695000.0000, 
raw observation next is [1.1, 88.0, 71.0, 0.0, 26.0, 24.9933174844882, 0.4905564234057754, 1.0, 1.0, 102303.6397119963], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.88, 0.23666666666666666, 0.0, 0.6666666666666666, 0.5827764570406831, 0.6635188078019251, 1.0, 1.0, 0.48716018910474423], 
reward next is 0.5128, 
noisyNet noise sample is [array([-0.63507843], dtype=float32), 0.13949482]. 
=============================================
[2019-04-04 13:03:27,458] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[91.38935 ]
 [91.40894 ]
 [91.44699 ]
 [91.5072  ]
 [91.507126]], R is [[91.02079773]
 [91.11058807]
 [91.19948578]
 [91.28749084]
 [91.37461853]].
[2019-04-04 13:03:27,582] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4309450e-09 4.9053126e-11 8.4776357e-16 2.0461970e-14 1.0000000e+00
 7.1761042e-11 2.2932879e-15], sum to 1.0000
[2019-04-04 13:03:27,582] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8923
[2019-04-04 13:03:27,612] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.9000000000000001, 89.33333333333334, 0.0, 0.0, 26.0, 25.62408920592033, 0.5397870150145042, 0.0, 1.0, 57340.81635273551], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1714800.0000, 
sim time next is 1715400.0000, 
raw observation next is [0.8, 90.0, 0.0, 0.0, 26.0, 25.59001105207806, 0.5352749009877834, 0.0, 1.0, 63300.28903280217], 
processed observation next is [1.0, 0.8695652173913043, 0.4847645429362882, 0.9, 0.0, 0.0, 0.6666666666666666, 0.6325009210065051, 0.6784249669959278, 0.0, 1.0, 0.3014299477752484], 
reward next is 0.6986, 
noisyNet noise sample is [array([-0.02092577], dtype=float32), -0.21330793]. 
=============================================
[2019-04-04 13:03:34,121] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-04 13:03:34,123] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 13:03:34,123] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 13:03:34,124] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:03:34,124] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:03:34,125] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 13:03:34,126] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:03:34,818] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run11
[2019-04-04 13:03:34,838] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run11
[2019-04-04 13:03:34,851] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run11
[2019-04-04 13:05:16,157] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.9620 239827972.8165 1604.9664
[2019-04-04 13:05:36,701] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 13:05:40,051] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7937 275773314.1886 1233.4787
[2019-04-04 13:05:41,084] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1000000, evaluation results [1000000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.962034207256, 239827972.81648064, 1604.9664307934922, 7182.793741958887, 275773314.1886366, 1233.4787487051678]
[2019-04-04 13:05:42,310] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3080689e-09 6.0520096e-11 3.8504068e-15 3.1122796e-14 1.0000000e+00
 1.7337289e-10 7.9666149e-16], sum to 1.0000
[2019-04-04 13:05:42,310] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0591
[2019-04-04 13:05:42,333] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1553212e-08 2.4513200e-09 1.2104808e-14 3.0035398e-13 1.0000000e+00
 7.4138928e-10 7.2415693e-14], sum to 1.0000
[2019-04-04 13:05:42,337] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0833
[2019-04-04 13:05:42,372] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 91.0, 12.33333333333333, 7.666666666666665, 26.0, 24.7927301061569, 0.1526211868235954, 1.0, 1.0, 70901.46299013161], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1929000.0000, 
sim time next is 1929600.0000, 
raw observation next is [-9.5, 91.0, 17.5, 11.0, 26.0, 24.98534223488354, 0.1813040331395846, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.1994459833795014, 0.91, 0.058333333333333334, 0.012154696132596685, 0.6666666666666666, 0.5821118529069617, 0.5604346777131949, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.72096103], dtype=float32), 0.16522183]. 
=============================================
[2019-04-04 13:05:42,384] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 79.0, 72.0, 0.0, 26.0, 25.04392411495846, 0.2652101956151175, 0.0, 1.0, 51357.32567829097], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1870200.0000, 
sim time next is 1870800.0000, 
raw observation next is [-4.5, 77.66666666666667, 64.83333333333333, 0.0, 26.0, 25.04672624853971, 0.2626318113096386, 0.0, 1.0, 45191.44755567658], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.7766666666666667, 0.2161111111111111, 0.0, 0.6666666666666666, 0.5872271873783091, 0.5875439371032128, 0.0, 1.0, 0.21519736931274563], 
reward next is 0.7848, 
noisyNet noise sample is [array([-0.5725769], dtype=float32), 1.1579033]. 
=============================================
[2019-04-04 13:05:45,749] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.8102739e-09 3.2185307e-10 3.7421208e-15 3.0549947e-14 1.0000000e+00
 6.7985804e-11 2.2092322e-14], sum to 1.0000
[2019-04-04 13:05:45,749] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8477
[2019-04-04 13:05:45,795] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 75.0, 183.3333333333333, 76.66666666666667, 26.0, 25.02868317817422, 0.2816131777979878, 0.0, 1.0, 41838.01951581884], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1866000.0000, 
sim time next is 1866600.0000, 
raw observation next is [-4.5, 77.0, 186.0, 84.0, 26.0, 25.02389678445434, 0.2848437624112722, 0.0, 1.0, 45306.65096778243], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.77, 0.62, 0.09281767955801105, 0.6666666666666666, 0.5853247320378617, 0.5949479208037575, 0.0, 1.0, 0.21574595698944013], 
reward next is 0.7843, 
noisyNet noise sample is [array([2.1600876], dtype=float32), 0.56377053]. 
=============================================
[2019-04-04 13:05:53,685] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.7299857e-09 2.6153796e-10 7.0889560e-15 3.5518074e-14 1.0000000e+00
 1.8720389e-10 2.3895867e-15], sum to 1.0000
[2019-04-04 13:05:53,686] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4110
[2019-04-04 13:05:53,734] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 83.0, 117.0, 0.0, 26.0, 25.4946731477112, 0.3055680650752667, 1.0, 1.0, 32874.88740606121], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2026200.0000, 
sim time next is 2026800.0000, 
raw observation next is [-5.6, 83.0, 124.5, 0.0, 26.0, 25.53241427633755, 0.31527923527171, 1.0, 1.0, 32967.33048249839], 
processed observation next is [1.0, 0.4782608695652174, 0.30747922437673136, 0.83, 0.415, 0.0, 0.6666666666666666, 0.6277011896947959, 0.6050930784239034, 1.0, 1.0, 0.15698728801189712], 
reward next is 0.8430, 
noisyNet noise sample is [array([0.7134872], dtype=float32), 0.75969255]. 
=============================================
[2019-04-04 13:05:55,176] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.8832302e-09 5.0852167e-10 3.8042566e-15 6.0723277e-14 1.0000000e+00
 1.6446988e-10 1.7156886e-14], sum to 1.0000
[2019-04-04 13:05:55,176] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8421
[2019-04-04 13:05:55,190] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.24099299350637, 0.07411458105076403, 0.0, 1.0, 41065.97825402208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2007000.0000, 
sim time next is 2007600.0000, 
raw observation next is [-6.199999999999999, 87.0, 0.0, 0.0, 26.0, 24.21321929103442, 0.06389955281973443, 0.0, 1.0, 41079.61298178577], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5177682742528683, 0.5212998509399115, 0.0, 1.0, 0.19561720467517035], 
reward next is 0.8044, 
noisyNet noise sample is [array([-0.36329725], dtype=float32), 3.8803918]. 
=============================================
[2019-04-04 13:05:59,394] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2102999e-09 7.9902873e-10 1.5403809e-15 1.9604620e-14 1.0000000e+00
 9.2012516e-11 1.6528774e-15], sum to 1.0000
[2019-04-04 13:05:59,395] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3641
[2019-04-04 13:05:59,453] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 79.0, 36.5, 18.5, 26.0, 25.21510818239818, 0.3086296667528, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2102400.0000, 
sim time next is 2103000.0000, 
raw observation next is [-7.383333333333333, 79.50000000000001, 48.33333333333334, 24.66666666666667, 26.0, 25.3625209027467, 0.2937382143712873, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.25807940904893817, 0.7950000000000002, 0.16111111111111115, 0.027255985267034995, 0.6666666666666666, 0.6135434085622249, 0.5979127381237624, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0346396], dtype=float32), 0.42895046]. 
=============================================
[2019-04-04 13:05:59,462] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[82.508995]
 [82.07044 ]
 [81.72515 ]
 [81.37485 ]
 [80.41238 ]], R is [[83.21273041]
 [83.3806076 ]
 [83.54679871]
 [83.4644165 ]
 [82.66349792]].
[2019-04-04 13:06:15,753] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2459109e-09 4.0453571e-11 1.9062133e-15 3.6136973e-14 1.0000000e+00
 4.1228319e-11 8.6926259e-16], sum to 1.0000
[2019-04-04 13:06:15,753] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2028
[2019-04-04 13:06:15,768] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.366666666666667, 76.0, 0.0, 0.0, 26.0, 24.6712087355221, 0.2435578952747839, 0.0, 1.0, 44124.53194712897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2244000.0000, 
sim time next is 2244600.0000, 
raw observation next is [-6.45, 76.5, 0.0, 0.0, 26.0, 24.61607529505847, 0.2322844173895247, 0.0, 1.0, 44119.31403441701], 
processed observation next is [1.0, 1.0, 0.28393351800554023, 0.765, 0.0, 0.0, 0.6666666666666666, 0.551339607921539, 0.5774281391298416, 0.0, 1.0, 0.21009197159246196], 
reward next is 0.7899, 
noisyNet noise sample is [array([-1.5905277], dtype=float32), -0.17714445]. 
=============================================
[2019-04-04 13:06:20,430] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0654522e-08 6.8749495e-09 1.8385433e-13 1.8924248e-12 1.0000000e+00
 3.2429406e-09 6.3897271e-13], sum to 1.0000
[2019-04-04 13:06:20,433] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2345
[2019-04-04 13:06:20,451] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.68653064201931, 0.2182912956267939, 0.0, 1.0, 39302.48632610941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2342400.0000, 
sim time next is 2343000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.64512216287724, 0.2108473980857156, 0.0, 1.0, 39421.29276923845], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5537601802397699, 0.5702824660285718, 0.0, 1.0, 0.18772044175827832], 
reward next is 0.8123, 
noisyNet noise sample is [array([-0.19859284], dtype=float32), -0.91497284]. 
=============================================
[2019-04-04 13:06:20,463] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.68399]
 [72.73444]
 [72.35814]
 [72.39074]
 [72.65822]], R is [[72.680336  ]
 [72.76638031]
 [72.85216522]
 [72.93759155]
 [73.0225296 ]].
[2019-04-04 13:06:21,193] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.8016376e-08 8.2537470e-09 1.1950788e-13 1.8954699e-12 9.9999988e-01
 1.5730154e-09 2.0135255e-13], sum to 1.0000
[2019-04-04 13:06:21,193] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9977
[2019-04-04 13:06:21,217] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.60574440894906, 0.2032584690193503, 0.0, 1.0, 39536.29772544464], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2343600.0000, 
sim time next is 2344200.0000, 
raw observation next is [-2.383333333333333, 62.5, 0.0, 0.0, 26.0, 24.56869867069001, 0.1962132924139383, 0.0, 1.0, 39648.08060075869], 
processed observation next is [0.0, 0.13043478260869565, 0.3965835641735919, 0.625, 0.0, 0.0, 0.6666666666666666, 0.5473915558908343, 0.5654044308046461, 0.0, 1.0, 0.1888003838131366], 
reward next is 0.8112, 
noisyNet noise sample is [array([1.4822854], dtype=float32), 0.069547586]. 
=============================================
[2019-04-04 13:06:23,071] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2237625e-07 2.8827701e-08 4.8667186e-12 2.2384726e-11 9.9999988e-01
 3.2663511e-08 1.1075981e-11], sum to 1.0000
[2019-04-04 13:06:23,071] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4612
[2019-04-04 13:06:23,085] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.116666666666666, 52.16666666666667, 0.0, 0.0, 26.0, 24.18704178235494, 0.0560394105200401, 0.0, 1.0, 43527.38275079281], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2425800.0000, 
sim time next is 2426400.0000, 
raw observation next is [-7.3, 53.0, 0.0, 0.0, 26.0, 24.13099443282784, 0.04506271701049594, 0.0, 1.0, 43551.70830198372], 
processed observation next is [0.0, 0.08695652173913043, 0.26038781163434904, 0.53, 0.0, 0.0, 0.6666666666666666, 0.5109162027356534, 0.5150209056701653, 0.0, 1.0, 0.20738908715230345], 
reward next is 0.7926, 
noisyNet noise sample is [array([0.0915374], dtype=float32), -1.2400706]. 
=============================================
[2019-04-04 13:06:33,362] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2789357e-07 1.0536707e-08 1.2737139e-12 5.1522436e-12 9.9999976e-01
 5.8544347e-09 1.6057549e-12], sum to 1.0000
[2019-04-04 13:06:33,365] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3338
[2019-04-04 13:06:33,407] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.9166666666666667, 28.33333333333334, 0.0, 0.0, 26.0, 24.92738785659735, 0.2196867389437004, 0.0, 1.0, 18719.20266029471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2484600.0000, 
sim time next is 2485200.0000, 
raw observation next is [0.7333333333333335, 28.66666666666667, 0.0, 0.0, 26.0, 24.92754947521984, 0.2144897317372718, 0.0, 1.0, 28515.99261811295], 
processed observation next is [0.0, 0.782608695652174, 0.4829178208679595, 0.28666666666666674, 0.0, 0.0, 0.6666666666666666, 0.5772957896016534, 0.5714965772457573, 0.0, 1.0, 0.13579044103863308], 
reward next is 0.8642, 
noisyNet noise sample is [array([-0.85737896], dtype=float32), -2.0616753]. 
=============================================
[2019-04-04 13:06:41,010] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.6547907e-09 3.8137110e-10 3.5717922e-15 3.4899255e-14 1.0000000e+00
 6.8691094e-11 6.5754482e-15], sum to 1.0000
[2019-04-04 13:06:41,011] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7187
[2019-04-04 13:06:41,025] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 83.0, 0.0, 0.0, 26.0, 24.30502291655296, 0.09987116480685747, 0.0, 1.0, 42997.1252886474], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2610000.0000, 
sim time next is 2610600.0000, 
raw observation next is [-6.283333333333333, 82.16666666666667, 0.0, 0.0, 26.0, 24.22274178582882, 0.08480649099272458, 0.0, 1.0, 43124.78926044491], 
processed observation next is [1.0, 0.21739130434782608, 0.288550323176362, 0.8216666666666668, 0.0, 0.0, 0.6666666666666666, 0.518561815485735, 0.5282688303309082, 0.0, 1.0, 0.20535613933545194], 
reward next is 0.7946, 
noisyNet noise sample is [array([-0.00171536], dtype=float32), -0.2916361]. 
=============================================
[2019-04-04 13:06:41,420] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4050305e-08 1.0664852e-09 4.4128994e-14 9.2548018e-13 1.0000000e+00
 3.2825864e-10 3.0985797e-14], sum to 1.0000
[2019-04-04 13:06:41,421] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6166
[2019-04-04 13:06:41,433] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.78557632706628, 0.2505033392092006, 0.0, 1.0, 42218.74972214334], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2769600.0000, 
sim time next is 2770200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.78303843132515, 0.2471348902029179, 0.0, 1.0, 42073.26772873785], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5652532026104291, 0.582378296734306, 0.0, 1.0, 0.2003488939463707], 
reward next is 0.7997, 
noisyNet noise sample is [array([-0.59625], dtype=float32), -0.7178877]. 
=============================================
[2019-04-04 13:06:42,783] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2674728e-10 1.0999099e-11 4.8826708e-17 4.6514608e-16 1.0000000e+00
 1.8806621e-12 1.3306542e-16], sum to 1.0000
[2019-04-04 13:06:42,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7324
[2019-04-04 13:06:42,830] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 59.5, 152.0, 233.0, 26.0, 25.87469052844439, 0.3960673963137112, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2799000.0000, 
sim time next is 2799600.0000, 
raw observation next is [-4.0, 58.0, 155.6666666666667, 278.8333333333333, 26.0, 25.89875059116742, 0.4056042808718878, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3518005540166205, 0.58, 0.5188888888888891, 0.30810313075506446, 0.6666666666666666, 0.6582292159306183, 0.635201426957296, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18202403], dtype=float32), -0.27172637]. 
=============================================
[2019-04-04 13:06:54,309] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.4716844e-10 8.8768187e-11 7.6288716e-16 3.5810012e-15 1.0000000e+00
 1.1410018e-11 5.5764853e-16], sum to 1.0000
[2019-04-04 13:06:54,310] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4560
[2019-04-04 13:06:54,333] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 83.66666666666666, 0.0, 0.0, 26.0, 25.10387272366912, 0.2915500622387551, 0.0, 1.0, 55698.55687051822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2860800.0000, 
sim time next is 2861400.0000, 
raw observation next is [1.0, 84.83333333333334, 0.0, 0.0, 26.0, 25.05711726193895, 0.2934852665781901, 0.0, 1.0, 54752.01068102413], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.8483333333333334, 0.0, 0.0, 0.6666666666666666, 0.5880931051615791, 0.59782842219273, 0.0, 1.0, 0.2607238603858292], 
reward next is 0.7393, 
noisyNet noise sample is [array([-0.42643422], dtype=float32), -0.74556905]. 
=============================================
[2019-04-04 13:06:57,144] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2325593e-10 1.9751780e-11 1.2477996e-16 7.8768990e-15 1.0000000e+00
 4.4681356e-11 1.2935532e-15], sum to 1.0000
[2019-04-04 13:06:57,145] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0672
[2019-04-04 13:06:57,187] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.5, 62.5, 110.0, 800.0, 26.0, 25.12358306527952, 0.409327719338324, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2986200.0000, 
sim time next is 2986800.0000, 
raw observation next is [-2.333333333333333, 61.66666666666667, 108.5, 791.8333333333334, 26.0, 25.12884688685212, 0.4063783560909659, 0.0, 1.0, 18716.68195802214], 
processed observation next is [0.0, 0.5652173913043478, 0.3979686057248385, 0.6166666666666667, 0.3616666666666667, 0.8749539594843463, 0.6666666666666666, 0.5940705739043434, 0.635459452030322, 0.0, 1.0, 0.08912705694296258], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.30368453], dtype=float32), -1.6928823]. 
=============================================
[2019-04-04 13:07:03,059] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5869835e-09 3.4040462e-10 1.1076129e-15 1.6373369e-14 1.0000000e+00
 6.0112380e-11 2.6373137e-15], sum to 1.0000
[2019-04-04 13:07:03,060] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2064
[2019-04-04 13:07:03,105] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 92.0, 709.0, 26.0, 25.11853187436407, 0.4057655301249328, 0.0, 1.0, 38858.40703283678], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2991600.0000, 
sim time next is 2992200.0000, 
raw observation next is [-1.833333333333333, 59.16666666666666, 89.0, 695.0, 26.0, 25.10434913316946, 0.4088373891053307, 0.0, 1.0, 34765.98315620289], 
processed observation next is [0.0, 0.6521739130434783, 0.41181902123730385, 0.5916666666666666, 0.2966666666666667, 0.7679558011049724, 0.6666666666666666, 0.5920290944307883, 0.6362791297017769, 0.0, 1.0, 0.1655523007438233], 
reward next is 0.8344, 
noisyNet noise sample is [array([-1.7407542], dtype=float32), -1.9649426]. 
=============================================
[2019-04-04 13:07:12,094] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7253869e-09 5.0020994e-11 1.7909931e-17 5.1613055e-15 1.0000000e+00
 1.0379096e-11 6.8897680e-17], sum to 1.0000
[2019-04-04 13:07:12,095] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4331
[2019-04-04 13:07:12,107] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.52481409542491, 0.5122316526793266, 0.0, 1.0, 39943.75410062572], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3210600.0000, 
sim time next is 3211200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.40535684135331, 0.5063129898674933, 0.0, 1.0, 102353.3608220742], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6171130701127758, 0.6687709966224977, 0.0, 1.0, 0.48739695629559143], 
reward next is 0.5126, 
noisyNet noise sample is [array([-1.6119841], dtype=float32), 1.6640775]. 
=============================================
[2019-04-04 13:07:14,128] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5894902e-09 1.1035917e-11 3.6593517e-17 4.7469860e-16 1.0000000e+00
 4.9573800e-12 6.4789550e-18], sum to 1.0000
[2019-04-04 13:07:14,129] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1859
[2019-04-04 13:07:14,153] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 80.33333333333333, 114.3333333333333, 821.1666666666667, 26.0, 26.59063479999635, 0.746350289677899, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3242400.0000, 
sim time next is 3243000.0000, 
raw observation next is [-2.0, 82.66666666666667, 113.6666666666667, 819.3333333333334, 26.0, 26.53459369562999, 0.6214020365154822, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.8266666666666667, 0.378888888888889, 0.905340699815838, 0.6666666666666666, 0.7112161413024992, 0.7071340121718274, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0701916], dtype=float32), 0.3830675]. 
=============================================
[2019-04-04 13:07:14,171] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[94.41665 ]
 [94.50326 ]
 [94.594734]
 [94.71354 ]
 [94.83283 ]], R is [[94.3699646 ]
 [94.42626953]
 [94.48200989]
 [94.5371933 ]
 [94.59181976]].
[2019-04-04 13:07:16,047] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2266844e-08 5.8287841e-10 4.0669177e-15 4.5426401e-14 1.0000000e+00
 1.6746828e-10 7.3994926e-15], sum to 1.0000
[2019-04-04 13:07:16,051] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1119
[2019-04-04 13:07:16,077] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.600000000000001, 77.0, 0.0, 0.0, 26.0, 24.73223865029715, 0.2866553171180883, 0.0, 1.0, 43903.77635474853], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3296400.0000, 
sim time next is 3297000.0000, 
raw observation next is [-8.75, 77.0, 0.0, 0.0, 26.0, 24.72087682670342, 0.287558063095421, 0.0, 1.0, 43878.8660592505], 
processed observation next is [1.0, 0.13043478260869565, 0.22022160664819945, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5600730688919517, 0.5958526876984737, 0.0, 1.0, 0.20894698123452618], 
reward next is 0.7911, 
noisyNet noise sample is [array([-0.2350382], dtype=float32), -0.8985121]. 
=============================================
[2019-04-04 13:07:16,091] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.02355 ]
 [81.143196]
 [81.29629 ]
 [81.49154 ]
 [81.45747 ]], R is [[80.90244293]
 [80.88435364]
 [80.86630249]
 [80.84837341]
 [80.83070374]].
[2019-04-04 13:07:20,447] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6092424e-09 1.5608212e-10 4.7414321e-15 4.2307348e-14 1.0000000e+00
 4.3887591e-10 7.6206733e-15], sum to 1.0000
[2019-04-04 13:07:20,452] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8976
[2019-04-04 13:07:20,480] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.02201047158345, 0.3635290525234596, 0.0, 1.0, 41524.7475050437], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3369600.0000, 
sim time next is 3370200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.01137132981059, 0.3556875188702702, 0.0, 1.0, 41485.75726812951], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5842809441508825, 0.61856250629009, 0.0, 1.0, 0.19755122508633102], 
reward next is 0.8024, 
noisyNet noise sample is [array([-1.3519969], dtype=float32), 1.0460885]. 
=============================================
[2019-04-04 13:07:21,634] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7388765e-10 1.2076738e-11 3.1410525e-15 1.0876705e-14 1.0000000e+00
 2.0124692e-10 1.1704974e-15], sum to 1.0000
[2019-04-04 13:07:21,635] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2699
[2019-04-04 13:07:21,651] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.29351384831092, 0.5363946168467743, 0.0, 1.0, 62801.49823606496], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3444600.0000, 
sim time next is 3445200.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.43106292572619, 0.5524650713664403, 0.0, 1.0, 18762.25115575776], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.619255243810516, 0.6841550237888135, 0.0, 1.0, 0.089344053122656], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.38068467], dtype=float32), 0.6186441]. 
=============================================
[2019-04-04 13:07:22,801] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.4169150e-09 6.6187215e-11 1.7058725e-15 2.6022131e-14 1.0000000e+00
 2.0685094e-10 9.4963490e-16], sum to 1.0000
[2019-04-04 13:07:22,802] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4671
[2019-04-04 13:07:22,813] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 67.83333333333333, 0.0, 0.0, 26.0, 25.51427998352053, 0.4184622321546665, 0.0, 1.0, 18750.89141215602], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3469800.0000, 
sim time next is 3470400.0000, 
raw observation next is [1.0, 67.0, 0.0, 0.0, 26.0, 25.43968686845389, 0.4030627841645814, 0.0, 1.0, 59185.58736178323], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6199739057044908, 0.6343542613881938, 0.0, 1.0, 0.28183613029420584], 
reward next is 0.7182, 
noisyNet noise sample is [array([0.26327115], dtype=float32), 0.00046139682]. 
=============================================
[2019-04-04 13:07:37,466] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8358317e-08 9.6905186e-09 1.2577033e-13 2.1078772e-12 1.0000000e+00
 3.0789693e-09 7.3814853e-13], sum to 1.0000
[2019-04-04 13:07:37,471] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8041
[2019-04-04 13:07:37,490] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.5, 28.0, 0.0, 0.0, 26.0, 25.44930478023275, 0.3582947701481939, 0.0, 1.0, 40133.32003912804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3645000.0000, 
sim time next is 3645600.0000, 
raw observation next is [8.666666666666668, 27.66666666666667, 0.0, 0.0, 26.0, 25.46169548670196, 0.3599033165473761, 0.0, 1.0, 28114.31039335595], 
processed observation next is [0.0, 0.17391304347826086, 0.7026777469990768, 0.2766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6218079572251632, 0.6199677721824587, 0.0, 1.0, 0.13387766853979025], 
reward next is 0.8661, 
noisyNet noise sample is [array([0.14027357], dtype=float32), 1.862585]. 
=============================================
[2019-04-04 13:07:40,460] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.4916750e-09 7.1857914e-10 1.0972429e-14 4.6973723e-14 1.0000000e+00
 3.7801515e-10 1.4092453e-14], sum to 1.0000
[2019-04-04 13:07:40,465] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6689
[2019-04-04 13:07:40,480] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 76.0, 0.0, 0.0, 26.0, 25.4353526398596, 0.4684411931388278, 0.0, 1.0, 66139.03018218043], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3795000.0000, 
sim time next is 3795600.0000, 
raw observation next is [-3.0, 75.0, 0.0, 0.0, 26.0, 25.45920939681897, 0.458580010266276, 0.0, 1.0, 37279.03155450062], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6216007830682475, 0.652860003422092, 0.0, 1.0, 0.1775191978785744], 
reward next is 0.8225, 
noisyNet noise sample is [array([0.05349216], dtype=float32), -0.3656969]. 
=============================================
[2019-04-04 13:07:56,345] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8743739e-08 5.5624594e-10 8.4325493e-15 3.7298520e-14 1.0000000e+00
 9.5239448e-11 3.8730510e-14], sum to 1.0000
[2019-04-04 13:07:56,346] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9550
[2019-04-04 13:07:56,375] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6666666666666667, 34.16666666666667, 117.3333333333333, 806.0, 26.0, 25.11597177409936, 0.3893473381808738, 0.0, 1.0, 18705.59221519202], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4187400.0000, 
sim time next is 4188000.0000, 
raw observation next is [-0.3333333333333334, 33.33333333333334, 118.1666666666667, 814.0, 26.0, 25.1326359166537, 0.3866202888562766, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4533702677747, 0.3333333333333334, 0.393888888888889, 0.8994475138121547, 0.6666666666666666, 0.5943863263878084, 0.6288734296187589, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45005235], dtype=float32), -0.52557313]. 
=============================================
[2019-04-04 13:07:56,392] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.97807 ]
 [76.256035]
 [76.56259 ]
 [76.906364]
 [77.0929  ]], R is [[75.89302063]
 [76.04502106]
 [76.19549561]
 [76.43354034]
 [76.66920471]].
[2019-04-04 13:07:57,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1172140e-07 1.1715398e-08 8.4894261e-13 2.6297324e-12 9.9999976e-01
 1.5332514e-08 7.3771767e-13], sum to 1.0000
[2019-04-04 13:07:57,429] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7986
[2019-04-04 13:07:57,442] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.5, 39.5, 0.0, 0.0, 26.0, 25.09356604283821, 0.2996474664613993, 0.0, 1.0, 40815.0882756855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4069800.0000, 
sim time next is 4070400.0000, 
raw observation next is [-5.333333333333333, 39.0, 0.0, 0.0, 26.0, 25.09525150021745, 0.2941778254175791, 0.0, 1.0, 40791.90110576456], 
processed observation next is [1.0, 0.08695652173913043, 0.3148661126500462, 0.39, 0.0, 0.0, 0.6666666666666666, 0.5912709583514543, 0.598059275139193, 0.0, 1.0, 0.1942471481226884], 
reward next is 0.8058, 
noisyNet noise sample is [array([0.22386304], dtype=float32), 0.8410716]. 
=============================================
[2019-04-04 13:07:58,514] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.41613405e-08 8.68151773e-10 1.37698162e-13 1.92112927e-13
 1.00000000e+00 2.70265654e-09 9.66387750e-14], sum to 1.0000
[2019-04-04 13:07:58,515] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8633
[2019-04-04 13:07:58,527] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.833333333333333, 36.0, 0.0, 0.0, 26.0, 25.25142782779456, 0.4008508247047926, 0.0, 1.0, 48135.01224689217], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4056600.0000, 
sim time next is 4057200.0000, 
raw observation next is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.24435168521387, 0.3956008505116836, 0.0, 1.0, 43088.89372162735], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.37, 0.0, 0.0, 0.6666666666666666, 0.6036959737678226, 0.6318669501705613, 0.0, 1.0, 0.20518520819822547], 
reward next is 0.7948, 
noisyNet noise sample is [array([0.05087692], dtype=float32), 0.065790154]. 
=============================================
[2019-04-04 13:08:01,476] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0096394e-09 1.4326287e-10 5.4866384e-16 8.1046789e-15 1.0000000e+00
 9.3141356e-11 1.2192763e-15], sum to 1.0000
[2019-04-04 13:08:01,477] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0796
[2019-04-04 13:08:01,517] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.666666666666667, 37.0, 102.0, 644.0, 26.0, 26.09289587117308, 0.4899640557844873, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4094400.0000, 
sim time next is 4095000.0000, 
raw observation next is [-2.5, 36.5, 104.0, 679.0, 26.0, 26.19203914290233, 0.5136374058069054, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.39335180055401664, 0.365, 0.3466666666666667, 0.7502762430939226, 0.6666666666666666, 0.6826699285751943, 0.6712124686023019, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8195479], dtype=float32), 0.49981582]. 
=============================================
[2019-04-04 13:08:01,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[83.38157]
 [83.01748]
 [82.39692]
 [81.72214]
 [80.9092 ]], R is [[83.71134949]
 [83.87423706]
 [84.03549194]
 [84.19513702]
 [84.35318756]].
[2019-04-04 13:08:01,550] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0438567e-09 5.2082401e-11 3.2173917e-16 1.1234690e-14 1.0000000e+00
 1.6361135e-10 1.9067442e-15], sum to 1.0000
[2019-04-04 13:08:01,551] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8090
[2019-04-04 13:08:01,569] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333333, 29.33333333333333, 120.8333333333333, 820.1666666666666, 26.0, 26.7224333727732, 0.6317175200418077, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4102800.0000, 
sim time next is 4103400.0000, 
raw observation next is [0.6666666666666667, 28.66666666666667, 120.6666666666667, 824.3333333333334, 26.0, 26.28429347454077, 0.6183333379755839, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4810710987996307, 0.28666666666666674, 0.4022222222222223, 0.910865561694291, 0.6666666666666666, 0.6903577895450642, 0.7061111126585279, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0281562], dtype=float32), -0.57351846]. 
=============================================
[2019-04-04 13:08:01,822] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3573498e-08 1.6131467e-09 3.9465410e-14 3.2384525e-13 1.0000000e+00
 6.9368139e-10 1.4985561e-13], sum to 1.0000
[2019-04-04 13:08:01,825] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5460
[2019-04-04 13:08:01,844] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.32520453202488, 0.3206945996101371, 0.0, 1.0, 53384.17354430076], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4250400.0000, 
sim time next is 4251000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.33180230330196, 0.3229883365963099, 0.0, 1.0, 44617.61935257292], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6109835252751633, 0.6076627788654366, 0.0, 1.0, 0.21246485405987103], 
reward next is 0.7875, 
noisyNet noise sample is [array([1.4086076], dtype=float32), 1.1450694]. 
=============================================
[2019-04-04 13:08:01,875] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.935616]
 [73.896286]
 [73.88947 ]
 [73.93127 ]
 [74.028015]], R is [[73.99414062]
 [73.99998474]
 [73.92913818]
 [73.93154907]
 [74.10292053]].
[2019-04-04 13:08:07,997] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9689765e-10 4.7682230e-11 6.7223700e-16 6.3262489e-15 1.0000000e+00
 1.5121354e-11 6.4343323e-16], sum to 1.0000
[2019-04-04 13:08:07,998] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9840
[2019-04-04 13:08:08,017] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.416666666666667, 75.16666666666667, 0.0, 0.0, 26.0, 25.59556201924798, 0.4194225349218421, 0.0, 1.0, 18738.67428119312], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4317000.0000, 
sim time next is 4317600.0000, 
raw observation next is [4.433333333333334, 75.33333333333334, 0.0, 0.0, 26.0, 25.60241902017649, 0.4113851985643514, 0.0, 1.0, 18736.64396550028], 
processed observation next is [0.0, 1.0, 0.5854108956602032, 0.7533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6335349183480409, 0.6371283995214504, 0.0, 1.0, 0.0892221141214299], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.11200729], dtype=float32), -0.48419714]. 
=============================================
[2019-04-04 13:08:09,259] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.24558891e-09 2.11106188e-10 2.00043962e-15 1.06948514e-14
 1.00000000e+00 9.68348735e-11 7.88278666e-16], sum to 1.0000
[2019-04-04 13:08:09,260] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1628
[2019-04-04 13:08:09,276] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.533333333333334, 68.0, 0.0, 0.0, 26.0, 25.68775053538691, 0.5597206605822734, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4425600.0000, 
sim time next is 4426200.0000, 
raw observation next is [3.4, 68.0, 0.0, 0.0, 26.0, 25.74400613597285, 0.5452441405418044, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.556786703601108, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6453338446644041, 0.6817480468472681, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0429397], dtype=float32), -0.65276814]. 
=============================================
[2019-04-04 13:08:09,476] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9193113e-10 3.6094662e-11 6.9591893e-16 3.0908573e-15 1.0000000e+00
 3.7323131e-11 1.6964684e-16], sum to 1.0000
[2019-04-04 13:08:09,479] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2191
[2019-04-04 13:08:09,490] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.0, 36.0, 49.66666666666666, 0.0, 26.0, 27.90911254496772, 1.052178991538269, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4378800.0000, 
sim time next is 4379400.0000, 
raw observation next is [13.0, 36.5, 39.0, 0.0, 26.0, 28.25510311011208, 1.078324108848098, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.365, 0.13, 0.0, 0.6666666666666666, 0.8545919258426734, 0.8594413696160327, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0684147], dtype=float32), 0.15993463]. 
=============================================
[2019-04-04 13:08:09,798] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.3678650e-10 6.2278377e-11 1.3242435e-16 8.1835893e-15 1.0000000e+00
 4.5204510e-11 5.7817527e-16], sum to 1.0000
[2019-04-04 13:08:09,799] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1379
[2019-04-04 13:08:09,806] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.8, 60.0, 172.5, 520.0, 26.0, 25.47880203039144, 0.4575591919521306, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4287600.0000, 
sim time next is 4288200.0000, 
raw observation next is [6.833333333333334, 59.33333333333333, 153.6666666666667, 565.0, 26.0, 25.47205441212383, 0.4591330070773056, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.651892890120037, 0.5933333333333333, 0.5122222222222224, 0.6243093922651933, 0.6666666666666666, 0.622671201010319, 0.6530443356924353, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1483294], dtype=float32), -1.7374974]. 
=============================================
[2019-04-04 13:08:11,050] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7945806e-09 4.2237748e-11 6.9590829e-16 2.2025245e-15 1.0000000e+00
 2.5136691e-11 2.2157467e-15], sum to 1.0000
[2019-04-04 13:08:11,054] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9473
[2019-04-04 13:08:11,063] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.6, 42.0, 7.5, 0.0, 26.0, 27.44628682231164, 0.9950756068299594, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4383600.0000, 
sim time next is 4384200.0000, 
raw observation next is [12.5, 43.0, 6.000000000000001, 0.0, 26.0, 27.54863703144623, 1.006411836126936, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.808864265927978, 0.43, 0.020000000000000004, 0.0, 0.6666666666666666, 0.7957197526205192, 0.835470612042312, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5329056], dtype=float32), 0.020713769]. 
=============================================
[2019-04-04 13:08:11,318] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6305289e-11 1.7086112e-11 3.3365819e-18 1.0793616e-16 1.0000000e+00
 1.6706484e-12 2.8560861e-18], sum to 1.0000
[2019-04-04 13:08:11,319] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0204
[2019-04-04 13:08:11,341] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 31.16666666666667, 168.3333333333333, 700.0, 26.0, 28.55430211718917, 1.14431531424369, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4371000.0000, 
sim time next is 4371600.0000, 
raw observation next is [14.3, 31.33333333333334, 181.6666666666667, 664.5, 26.0, 28.62483064946358, 1.04013309935033, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8587257617728533, 0.3133333333333334, 0.6055555555555557, 0.7342541436464088, 0.6666666666666666, 0.8854025541219649, 0.8467110331167765, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.50480556], dtype=float32), -1.1121225]. 
=============================================
[2019-04-04 13:08:12,910] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8527185e-10 8.0499375e-12 1.0731171e-16 1.8298897e-15 1.0000000e+00
 3.0036005e-11 3.1415184e-16], sum to 1.0000
[2019-04-04 13:08:12,915] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8246
[2019-04-04 13:08:12,927] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 26.0, 25.57521569673165, 0.5048154243686821, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4433400.0000, 
sim time next is 4434000.0000, 
raw observation next is [2.0, 80.0, 20.0, 38.66666666666666, 26.0, 25.5879908537487, 0.5065500774878173, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.06666666666666667, 0.04272559852670349, 0.6666666666666666, 0.6323325711457249, 0.6688500258292724, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40577123], dtype=float32), -0.3983695]. 
=============================================
[2019-04-04 13:08:12,938] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[89.830894]
 [89.198074]
 [88.64198 ]
 [88.56709 ]
 [88.509605]], R is [[90.24674988]
 [90.34428406]
 [90.44084167]
 [90.39270782]
 [90.39956665]].
[2019-04-04 13:08:18,097] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.6629760e-10 3.7177410e-11 5.0821153e-17 7.7684838e-16 1.0000000e+00
 3.9126469e-11 3.4528090e-17], sum to 1.0000
[2019-04-04 13:08:18,102] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7935
[2019-04-04 13:08:18,127] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 86.16666666666667, 80.33333333333333, 0.0, 26.0, 26.17023924651723, 0.5923412585389042, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4459800.0000, 
sim time next is 4460400.0000, 
raw observation next is [0.0, 85.0, 78.0, 0.0, 26.0, 25.90458904152934, 0.5678806179088413, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.85, 0.26, 0.0, 0.6666666666666666, 0.6587157534607785, 0.689293539302947, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.54257673], dtype=float32), -1.3282659]. 
=============================================
[2019-04-04 13:08:18,967] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1329486e-09 2.0875728e-11 8.9018848e-17 2.5827739e-15 1.0000000e+00
 8.3848007e-12 1.4229954e-15], sum to 1.0000
[2019-04-04 13:08:18,970] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6526
[2019-04-04 13:08:19,020] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.2666666666666667, 72.33333333333334, 113.0, 55.0, 26.0, 25.84230258240235, 0.4858192415172644, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4524000.0000, 
sim time next is 4524600.0000, 
raw observation next is [-0.1333333333333333, 72.16666666666666, 115.0, 44.00000000000001, 26.0, 25.92610267112814, 0.5088498022818203, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4589104339796861, 0.7216666666666666, 0.38333333333333336, 0.04861878453038675, 0.6666666666666666, 0.6605085559273451, 0.6696166007606067, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4688693], dtype=float32), -0.6894547]. 
=============================================
[2019-04-04 13:08:22,367] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.6303541e-09 3.4564981e-10 2.2369969e-14 1.3585973e-13 1.0000000e+00
 1.3117317e-09 6.3183868e-15], sum to 1.0000
[2019-04-04 13:08:22,370] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4424
[2019-04-04 13:08:22,384] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.0, 23.33333333333334, 23.33333333333334, 26.0, 25.67250337587865, 0.4920394628969758, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4556400.0000, 
sim time next is 4557000.0000, 
raw observation next is [2.0, 52.0, 18.66666666666667, 18.66666666666667, 26.0, 25.53450603217421, 0.4983980421522708, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.06222222222222224, 0.02062615101289135, 0.6666666666666666, 0.6278755026811842, 0.6661326807174236, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.8950356], dtype=float32), 2.0971856]. 
=============================================
[2019-04-04 13:08:22,393] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[80.91828 ]
 [81.259476]
 [81.66963 ]
 [82.06832 ]
 [82.41902 ]], R is [[80.86850739]
 [81.05982208]
 [81.2492218 ]
 [81.43672943]
 [81.62236023]].
[2019-04-04 13:08:22,815] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9126582e-09 1.4708747e-10 2.9149783e-15 8.0708958e-15 1.0000000e+00
 2.5398639e-10 2.0343542e-15], sum to 1.0000
[2019-04-04 13:08:22,820] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9781
[2019-04-04 13:08:22,851] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 26.0, 25.9357654939688, 0.6263971551693124, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4658400.0000, 
sim time next is 4659000.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 26.0, 25.89682463680975, 0.5634799355102015, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6580687197341458, 0.6878266451700671, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5713758], dtype=float32), -1.4602102]. 
=============================================
[2019-04-04 13:08:22,870] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.58691 ]
 [82.07022 ]
 [81.577736]
 [81.36313 ]
 [81.402885]], R is [[82.98673248]
 [83.15686798]
 [83.32530212]
 [83.49205017]
 [83.65712738]].
[2019-04-04 13:08:23,068] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2731808e-08 1.7182604e-09 8.6477199e-14 4.0391849e-13 1.0000000e+00
 9.5638153e-10 4.4924408e-14], sum to 1.0000
[2019-04-04 13:08:23,070] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7352
[2019-04-04 13:08:23,087] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.92860702657732, 0.3432091322233304, 0.0, 1.0, 40790.02179453801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4758000.0000, 
sim time next is 4758600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.88928740721372, 0.3359280142186645, 0.0, 1.0, 40741.10168489369], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5741072839344765, 0.6119760047395548, 0.0, 1.0, 0.1940052461185414], 
reward next is 0.8060, 
noisyNet noise sample is [array([0.590027], dtype=float32), -2.2580037]. 
=============================================
[2019-04-04 13:08:23,302] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3847086e-10 5.6487779e-12 7.2409885e-18 4.8708227e-16 1.0000000e+00
 2.6193858e-12 2.4077198e-17], sum to 1.0000
[2019-04-04 13:08:23,303] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8874
[2019-04-04 13:08:23,327] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.7, 49.0, 171.0, 706.0, 26.0, 27.20465311621098, 0.7253542525648836, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4629600.0000, 
sim time next is 4630200.0000, 
raw observation next is [4.75, 49.16666666666667, 181.6666666666667, 670.3333333333333, 26.0, 26.6879752118042, 0.783351820215714, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5941828254847646, 0.4916666666666667, 0.6055555555555557, 0.7406998158379373, 0.6666666666666666, 0.7239979343170168, 0.761117273405238, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01360365], dtype=float32), 1.4871681]. 
=============================================
[2019-04-04 13:08:31,294] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.20851205e-08 3.97045286e-10 3.21773858e-14 2.53046472e-13
 1.00000000e+00 2.26032054e-10 3.69315445e-14], sum to 1.0000
[2019-04-04 13:08:31,294] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1047
[2019-04-04 13:08:31,310] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.67584151157942, 0.2766529013656872, 0.0, 1.0, 40588.71672977787], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4763400.0000, 
sim time next is 4764000.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.61517310200296, 0.2676771490389499, 0.0, 1.0, 40634.84551864236], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5512644251669133, 0.5892257163463167, 0.0, 1.0, 0.19349926437448742], 
reward next is 0.8065, 
noisyNet noise sample is [array([1.7654933], dtype=float32), 0.13546519]. 
=============================================
[2019-04-04 13:08:31,342] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[77.54317]
 [77.33263]
 [77.34785]
 [77.39548]
 [77.45016]], R is [[77.52347565]
 [77.55496216]
 [77.5863266 ]
 [77.61747742]
 [77.64828491]].
[2019-04-04 13:08:39,729] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0229094e-09 2.7817148e-11 1.4272352e-16 5.9469950e-16 1.0000000e+00
 7.7328872e-12 5.6441352e-16], sum to 1.0000
[2019-04-04 13:08:39,731] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7820
[2019-04-04 13:08:39,747] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.5, 25.5, 92.0, 774.0, 26.0, 26.79419722717531, 0.8118878827106172, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4980600.0000, 
sim time next is 4981200.0000, 
raw observation next is [8.666666666666668, 25.33333333333333, 88.66666666666667, 751.8333333333333, 26.0, 27.24034279447856, 0.8551172832578272, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7026777469990768, 0.2533333333333333, 0.29555555555555557, 0.8307550644567219, 0.6666666666666666, 0.7700285662065468, 0.7850390944192758, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16792747], dtype=float32), 0.3065469]. 
=============================================
[2019-04-04 13:08:40,522] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4289235e-09 7.0971723e-10 3.1063299e-15 2.5482873e-14 1.0000000e+00
 7.6585308e-11 7.2942564e-15], sum to 1.0000
[2019-04-04 13:08:40,528] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3903
[2019-04-04 13:08:40,588] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.166666666666667, 46.66666666666667, 30.99999999999999, 186.6666666666666, 26.0, 25.28676403992835, 0.2734875537340551, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4953000.0000, 
sim time next is 4953600.0000, 
raw observation next is [-2.0, 46.0, 46.5, 280.0, 26.0, 25.22901643763205, 0.2753289364259394, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.46, 0.155, 0.30939226519337015, 0.6666666666666666, 0.6024180364693376, 0.5917763121419798, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.52086735], dtype=float32), -2.5719233]. 
=============================================
[2019-04-04 13:08:40,713] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.9653374e-10 3.6389364e-10 1.0067480e-15 1.0610975e-14 1.0000000e+00
 3.6994699e-11 2.4313782e-15], sum to 1.0000
[2019-04-04 13:08:40,714] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4412
[2019-04-04 13:08:40,770] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 43.66666666666667, 77.5, 466.6666666666667, 26.0, 25.1204587628499, 0.2970263624160107, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4954800.0000, 
sim time next is 4955400.0000, 
raw observation next is [-1.5, 42.5, 93.0, 560.0, 26.0, 25.23931613316641, 0.3292409383691895, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4210526315789474, 0.425, 0.31, 0.6187845303867403, 0.6666666666666666, 0.6032763444305344, 0.6097469794563964, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.52086735], dtype=float32), -2.5719233]. 
=============================================
[2019-04-04 13:08:41,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:41,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:41,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4935355e-10 1.9382496e-11 8.6743698e-17 2.9705668e-15 1.0000000e+00
 3.9305909e-11 2.2031619e-16], sum to 1.0000
[2019-04-04 13:08:41,355] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8087
[2019-04-04 13:08:41,377] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.0, 26.0, 103.3333333333333, 804.0, 26.0, 27.69939001127503, 0.8856510444986587, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4978200.0000, 
sim time next is 4978800.0000, 
raw observation next is [8.0, 26.0, 100.5, 796.5, 26.0, 27.76036828963077, 0.7525818316127547, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6842105263157896, 0.26, 0.335, 0.8801104972375691, 0.6666666666666666, 0.8133640241358974, 0.7508606105375849, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1555418], dtype=float32), 1.1923505]. 
=============================================
[2019-04-04 13:08:41,391] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run9
[2019-04-04 13:08:41,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:41,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:41,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run9
[2019-04-04 13:08:43,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:43,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:43,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run9
[2019-04-04 13:08:43,733] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:43,733] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:43,740] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run9
[2019-04-04 13:08:44,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:44,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:44,147] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run9
[2019-04-04 13:08:44,457] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8489952e-09 5.1543436e-10 3.0769285e-15 4.7222985e-15 1.0000000e+00
 8.2818093e-11 9.8657548e-16], sum to 1.0000
[2019-04-04 13:08:44,458] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5388
[2019-04-04 13:08:44,469] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.333333333333334, 25.66666666666667, 40.33333333333333, 360.1666666666666, 26.0, 27.55134290756683, 0.8366949035664127, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4987200.0000, 
sim time next is 4987800.0000, 
raw observation next is [7.0, 25.5, 34.0, 304.0, 26.0, 27.50523803938095, 0.8256053838767308, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6565096952908588, 0.255, 0.11333333333333333, 0.33591160220994476, 0.6666666666666666, 0.7921031699484123, 0.7752017946255769, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02460674], dtype=float32), -0.8788697]. 
=============================================
[2019-04-04 13:08:45,922] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:45,923] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:45,934] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run9
[2019-04-04 13:08:47,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:47,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:47,071] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run9
[2019-04-04 13:08:47,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:47,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:47,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run9
[2019-04-04 13:08:47,382] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:47,383] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:47,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run9
[2019-04-04 13:08:47,526] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:47,538] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:47,543] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run9
[2019-04-04 13:08:47,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:47,578] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:47,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run9
[2019-04-04 13:08:47,876] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:47,876] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:47,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run9
[2019-04-04 13:08:49,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:49,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:49,278] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run9
[2019-04-04 13:08:49,788] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:49,788] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:49,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run9
[2019-04-04 13:08:50,471] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:50,471] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:50,475] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run9
[2019-04-04 13:08:50,496] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:08:50,496] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:08:50,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run9
[2019-04-04 13:09:12,065] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9582262e-10 1.0636001e-10 5.3472470e-16 1.4230314e-14 1.0000000e+00
 4.4639022e-11 1.5750816e-16], sum to 1.0000
[2019-04-04 13:09:12,065] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5433
[2019-04-04 13:09:12,168] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 59.0, 86.5, 0.0, 26.0, 25.62766749903614, 0.2945455807427153, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 226800.0000, 
sim time next is 227400.0000, 
raw observation next is [-2.9, 59.5, 76.33333333333333, 0.0, 26.0, 24.78778368643279, 0.2960445101051869, 1.0, 1.0, 198943.0916883596], 
processed observation next is [1.0, 0.6521739130434783, 0.38227146814404434, 0.595, 0.2544444444444444, 0.0, 0.6666666666666666, 0.5656486405360658, 0.5986815033683957, 1.0, 1.0, 0.9473480556588552], 
reward next is 0.0527, 
noisyNet noise sample is [array([-1.2083977], dtype=float32), -0.06389113]. 
=============================================
[2019-04-04 13:09:17,772] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3706666e-08 6.5901423e-10 9.9282799e-15 4.3706000e-13 1.0000000e+00
 7.5763296e-10 1.9292949e-14], sum to 1.0000
[2019-04-04 13:09:17,779] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0724
[2019-04-04 13:09:17,789] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.866666666666667, 69.0, 0.0, 0.0, 26.0, 23.29154338992554, -0.1183271753282035, 0.0, 1.0, 46906.50827140933], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 274800.0000, 
sim time next is 275400.0000, 
raw observation next is [-10.05, 68.5, 0.0, 0.0, 26.0, 23.20625697579497, -0.1288250029810742, 0.0, 1.0, 47095.11940338247], 
processed observation next is [1.0, 0.17391304347826086, 0.18421052631578946, 0.685, 0.0, 0.0, 0.6666666666666666, 0.4338547479829143, 0.45705833233964194, 0.0, 1.0, 0.22426247334944036], 
reward next is 0.7757, 
noisyNet noise sample is [array([1.3014884], dtype=float32), 0.96730787]. 
=============================================
[2019-04-04 13:09:33,290] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-04 13:09:33,290] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 13:09:33,290] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:09:33,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run12
[2019-04-04 13:09:33,312] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 13:09:33,313] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:09:33,315] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run12
[2019-04-04 13:09:33,335] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 13:09:33,335] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:09:33,337] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run12
[2019-04-04 13:10:12,854] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.16312885], dtype=float32), 0.18677941]
[2019-04-04 13:10:12,854] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.6, 100.0, 0.0, 0.0, 26.0, 25.14799865632356, 0.4308776501652345, 0.0, 1.0, 38488.92318215944]
[2019-04-04 13:10:12,855] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 13:10:12,855] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.0863090e-09 8.9391057e-11 3.8084364e-16 6.8216226e-15 1.0000000e+00
 2.0454938e-11 6.5586354e-16], sampled 0.15097160300231938
[2019-04-04 13:10:54,778] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16312885], dtype=float32), 0.18677941]
[2019-04-04 13:10:54,778] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.533333333333333, 50.66666666666667, 134.5, 261.0, 26.0, 25.77385163763825, 0.4549747175542804, 0.0, 1.0, 0.0]
[2019-04-04 13:10:54,779] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 13:10:54,779] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.2151333e-09 3.8133546e-10 2.8762848e-15 2.6854615e-14 1.0000000e+00
 7.4587378e-11 4.8837747e-15], sampled 0.13374488873469081
[2019-04-04 13:11:13,748] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.3752 239951216.5112 1605.3734
[2019-04-04 13:11:33,388] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.4099 263463930.4480 1556.9858
[2019-04-04 13:11:40,152] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 13:11:41,176] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1100000, evaluation results [1100000.0, 7241.409855009758, 263463930.4479512, 1556.9858411143289, 7353.375159470349, 239951216.51123026, 1605.37337911546, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 13:12:01,959] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.3486914e-09 3.9414710e-10 9.2573480e-16 3.0518733e-14 1.0000000e+00
 6.4978634e-11 1.4348566e-15], sum to 1.0000
[2019-04-04 13:12:01,959] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5332
[2019-04-04 13:12:01,972] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.383333333333333, 75.83333333333333, 0.0, 0.0, 26.0, 24.26170252574903, 0.03987820947830716, 0.0, 1.0, 41568.54131079445], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 708600.0000, 
sim time next is 709200.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.26798333027131, 0.03811531215771926, 0.0, 1.0, 41565.93001944633], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5223319441892759, 0.5127051040525731, 0.0, 1.0, 0.19793300009260156], 
reward next is 0.8021, 
noisyNet noise sample is [array([0.10053814], dtype=float32), -1.8640634]. 
=============================================
[2019-04-04 13:12:02,163] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6715601e-08 2.5275033e-09 7.1467802e-14 6.5656383e-14 1.0000000e+00
 5.6721039e-10 8.8257526e-14], sum to 1.0000
[2019-04-04 13:12:02,165] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8254
[2019-04-04 13:12:02,186] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 64.0, 0.0, 0.0, 26.0, 24.75838254936153, 0.2501480557757204, 0.0, 1.0, 43387.70191247764], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 770400.0000, 
sim time next is 771000.0000, 
raw observation next is [-6.283333333333333, 64.5, 0.0, 0.0, 26.0, 24.72319481027977, 0.2426105228722016, 0.0, 1.0, 43306.89152580922], 
processed observation next is [1.0, 0.9565217391304348, 0.288550323176362, 0.645, 0.0, 0.0, 0.6666666666666666, 0.560266234189981, 0.5808701742907338, 0.0, 1.0, 0.20622329298004388], 
reward next is 0.7938, 
noisyNet noise sample is [array([0.56094915], dtype=float32), 0.13089605]. 
=============================================
[2019-04-04 13:12:02,191] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.19038]
 [77.99233]
 [77.82282]
 [77.84075]
 [77.91357]], R is [[78.39396667]
 [78.40341949]
 [78.41266632]
 [78.42076874]
 [78.4267807 ]].
[2019-04-04 13:12:07,143] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.59859500e-10 1.18068853e-11 1.08235855e-17 8.03482748e-16
 1.00000000e+00 1.35205076e-11 2.14470066e-16], sum to 1.0000
[2019-04-04 13:12:07,143] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6425
[2019-04-04 13:12:07,174] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.8, 93.0, 97.33333333333333, 0.0, 26.0, 25.0751531164931, 0.2471599682775044, 1.0, 1.0, 43331.07017422162], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 912000.0000, 
sim time next is 912600.0000, 
raw observation next is [3.8, 93.0, 96.0, 0.0, 26.0, 25.06159383731583, 0.1595961007898746, 1.0, 1.0, 36146.44493560378], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.32, 0.0, 0.6666666666666666, 0.5884661531096524, 0.5531987002632915, 1.0, 1.0, 0.1721259282647799], 
reward next is 0.8279, 
noisyNet noise sample is [array([0.10605418], dtype=float32), 0.26864052]. 
=============================================
[2019-04-04 13:12:13,982] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6461193e-09 9.4507659e-11 2.8565591e-15 9.5029126e-15 1.0000000e+00
 5.5276075e-11 4.7005517e-15], sum to 1.0000
[2019-04-04 13:12:13,984] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3028
[2019-04-04 13:12:13,997] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.49600999256258, 0.154440472002027, 0.0, 1.0, 38818.49193183413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 885600.0000, 
sim time next is 886200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.49313581599843, 0.1499807329985945, 0.0, 1.0, 38742.0894603573], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5410946513332023, 0.5499935776661982, 0.0, 1.0, 0.18448614028741572], 
reward next is 0.8155, 
noisyNet noise sample is [array([-0.9253821], dtype=float32), -0.56641424]. 
=============================================
[2019-04-04 13:12:20,180] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8801684e-08 6.7761006e-09 4.3067484e-13 4.5170530e-12 1.0000000e+00
 8.7080354e-10 8.3907474e-13], sum to 1.0000
[2019-04-04 13:12:20,183] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4762
[2019-04-04 13:12:20,187] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 65.0, 112.0, 0.0, 26.0, 25.05446614438063, 0.4942176320765163, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1176600.0000, 
sim time next is 1177200.0000, 
raw observation next is [18.3, 65.0, 104.0, 0.0, 26.0, 25.04165588253048, 0.49705579366161, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.9695290858725764, 0.65, 0.3466666666666667, 0.0, 0.6666666666666666, 0.5868046568775401, 0.66568526455387, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1309671], dtype=float32), 1.8280255]. 
=============================================
[2019-04-04 13:12:26,929] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0732628e-10 8.3420534e-11 9.1856819e-18 1.2838932e-15 1.0000000e+00
 7.8333425e-12 6.0733606e-17], sum to 1.0000
[2019-04-04 13:12:26,930] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7803
[2019-04-04 13:12:26,944] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.17823082295295, 0.4252616769830402, 0.0, 1.0, 38485.32168321328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1404600.0000, 
sim time next is 1405200.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.13878022734492, 0.4145810959221876, 0.0, 1.0, 38508.24166538025], 
processed observation next is [1.0, 0.2608695652173913, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5948983522787433, 0.6381936986407292, 0.0, 1.0, 0.18337257935895357], 
reward next is 0.8166, 
noisyNet noise sample is [array([-1.0089194], dtype=float32), 0.6803967]. 
=============================================
[2019-04-04 13:12:30,253] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.2635958e-11 1.2440907e-11 3.1194483e-17 4.1221136e-16 1.0000000e+00
 1.4173768e-12 7.3299485e-18], sum to 1.0000
[2019-04-04 13:12:30,255] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4387
[2019-04-04 13:12:30,272] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.300000000000001, 95.5, 0.0, 0.0, 26.0, 25.4108805338883, 0.5828741717905211, 0.0, 1.0, 44326.36894658029], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1296600.0000, 
sim time next is 1297200.0000, 
raw observation next is [4.200000000000001, 95.0, 0.0, 0.0, 26.0, 25.41073626110586, 0.5907076529355575, 0.0, 1.0, 40479.36088607195], 
processed observation next is [1.0, 0.0, 0.5789473684210527, 0.95, 0.0, 0.0, 0.6666666666666666, 0.617561355092155, 0.696902550978519, 0.0, 1.0, 0.19275886136224737], 
reward next is 0.8072, 
noisyNet noise sample is [array([-1.0525098], dtype=float32), -0.81270987]. 
=============================================
[2019-04-04 13:12:30,320] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6793837e-10 1.1080975e-10 1.6673470e-16 1.5881870e-15 1.0000000e+00
 1.3652670e-11 7.9538914e-16], sum to 1.0000
[2019-04-04 13:12:30,321] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0915
[2019-04-04 13:12:30,342] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 100.0, 98.0, 0.0, 26.0, 24.9183556332452, 0.4656226461845929, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1256400.0000, 
sim time next is 1257000.0000, 
raw observation next is [13.8, 100.0, 97.0, 0.0, 26.0, 24.88348501394701, 0.4610978702558146, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.3233333333333333, 0.0, 0.6666666666666666, 0.5736237511622507, 0.6536992900852715, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01480731], dtype=float32), -0.78295445]. 
=============================================
[2019-04-04 13:12:30,352] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[91.302   ]
 [90.2636  ]
 [89.476326]
 [88.46699 ]
 [87.46691 ]], R is [[92.25880432]
 [92.33621979]
 [92.41285706]
 [92.48873138]
 [92.56384277]].
[2019-04-04 13:12:36,568] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.48023896e-10 5.89110358e-11 6.33252313e-16 2.38495944e-16
 1.00000000e+00 1.24280525e-11 1.54819160e-16], sum to 1.0000
[2019-04-04 13:12:36,568] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1241
[2019-04-04 13:12:36,586] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.27586224738944, 0.4931913274981918, 0.0, 1.0, 40592.40540672405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1383000.0000, 
sim time next is 1383600.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.26040642749282, 0.5151966296487698, 0.0, 1.0, 40730.79148180826], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6050338689577351, 0.6717322098829234, 0.0, 1.0, 0.19395614991337265], 
reward next is 0.8060, 
noisyNet noise sample is [array([-1.2748847], dtype=float32), -0.6719566]. 
=============================================
[2019-04-04 13:12:36,873] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5017487e-10 1.2429143e-11 1.5156710e-17 1.3950298e-16 1.0000000e+00
 2.8811101e-12 2.9233718e-17], sum to 1.0000
[2019-04-04 13:12:36,874] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5034
[2019-04-04 13:12:36,930] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 100.0, 18.0, 0.0, 26.0, 25.55988724041561, 0.4802937192224375, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1413000.0000, 
sim time next is 1413600.0000, 
raw observation next is [-0.6, 100.0, 22.66666666666666, 0.0, 26.0, 25.73793938760602, 0.4959952417330107, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.07555555555555554, 0.0, 0.6666666666666666, 0.6448282823005016, 0.6653317472443369, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1543013], dtype=float32), 0.09679837]. 
=============================================
[2019-04-04 13:12:37,941] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.7058143e-11 2.7096775e-12 1.4249007e-17 2.6016113e-15 1.0000000e+00
 1.7832371e-11 2.3115630e-17], sum to 1.0000
[2019-04-04 13:12:37,944] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3669
[2019-04-04 13:12:37,982] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.9000000000000001, 92.0, 87.0, 0.0, 26.0, 25.99676120895116, 0.5721922327953323, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1431600.0000, 
sim time next is 1432200.0000, 
raw observation next is [1.0, 92.0, 84.0, 0.0, 26.0, 26.16851356761293, 0.5894432349057384, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4903047091412743, 0.92, 0.28, 0.0, 0.6666666666666666, 0.6807094639677441, 0.6964810783019129, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20442095], dtype=float32), -0.61779106]. 
=============================================
[2019-04-04 13:12:41,431] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1811380e-09 7.5540572e-12 7.5955324e-17 1.4068885e-16 1.0000000e+00
 2.6423681e-12 2.1837237e-17], sum to 1.0000
[2019-04-04 13:12:41,432] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7840
[2019-04-04 13:12:41,442] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.433333333333333, 92.0, 0.0, 0.0, 26.0, 25.31060814992483, 0.4834933215247154, 0.0, 1.0, 60988.42064947961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1464000.0000, 
sim time next is 1464600.0000, 
raw observation next is [1.516666666666667, 92.0, 0.0, 0.0, 26.0, 25.31672357837434, 0.4875627909074887, 0.0, 1.0, 46622.74716873468], 
processed observation next is [1.0, 0.9565217391304348, 0.5046168051708219, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6097269648645284, 0.6625209303024963, 0.0, 1.0, 0.22201308175587942], 
reward next is 0.7780, 
noisyNet noise sample is [array([1.6445329], dtype=float32), 1.0222195]. 
=============================================
[2019-04-04 13:12:42,959] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.2542586e-10 3.0055430e-11 4.5399133e-17 6.8200059e-16 1.0000000e+00
 9.2104475e-12 4.0135899e-16], sum to 1.0000
[2019-04-04 13:12:42,962] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7306
[2019-04-04 13:12:42,973] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.95, 82.33333333333334, 0.0, 0.0, 26.0, 25.6355151496122, 0.5172761002774516, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1576200.0000, 
sim time next is 1576800.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.65216364756676, 0.5179597146769954, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.6011080332409973, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6376803039638966, 0.672653238225665, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8203708], dtype=float32), -2.6352928]. 
=============================================
[2019-04-04 13:12:45,418] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5000282e-10 4.9306366e-12 1.9919623e-17 4.1212156e-17 1.0000000e+00
 1.0544810e-12 2.6535267e-18], sum to 1.0000
[2019-04-04 13:12:45,418] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4761
[2019-04-04 13:12:45,431] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 92.0, 0.0, 0.0, 26.0, 25.58664469761855, 0.5376822537661226, 0.0, 1.0, 28059.92352470983], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1666800.0000, 
sim time next is 1667400.0000, 
raw observation next is [4.716666666666667, 92.0, 0.0, 0.0, 26.0, 25.58344062681918, 0.5349659593458885, 0.0, 1.0, 30748.68109643362], 
processed observation next is [1.0, 0.30434782608695654, 0.5932594644506002, 0.92, 0.0, 0.0, 0.6666666666666666, 0.631953385568265, 0.6783219864486295, 0.0, 1.0, 0.14642229093539819], 
reward next is 0.8536, 
noisyNet noise sample is [array([-0.7563452], dtype=float32), 0.40493304]. 
=============================================
[2019-04-04 13:12:53,881] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5318486e-08 4.6109503e-09 4.8030383e-14 3.4022560e-13 1.0000000e+00
 1.1764703e-09 5.0948957e-14], sum to 1.0000
[2019-04-04 13:12:53,882] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4025
[2019-04-04 13:12:53,914] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.199999999999999, 84.33333333333333, 0.0, 0.0, 26.0, 23.93644749637197, 0.05159073715527748, 0.0, 1.0, 46850.7561213314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1827600.0000, 
sim time next is 1828200.0000, 
raw observation next is [-6.2, 83.66666666666667, 0.0, 0.0, 26.0, 23.8991749454517, 0.04316473099460083, 0.0, 1.0, 46886.4375567712], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.4915979121209751, 0.5143882436648669, 0.0, 1.0, 0.22326875027033904], 
reward next is 0.7767, 
noisyNet noise sample is [array([-2.327057], dtype=float32), -2.08983]. 
=============================================
[2019-04-04 13:12:56,865] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1247868e-08 1.3853592e-09 1.1430781e-14 1.9676705e-13 1.0000000e+00
 1.1709983e-10 2.7310155e-14], sum to 1.0000
[2019-04-04 13:12:56,867] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1359
[2019-04-04 13:12:56,883] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.66666666666667, 0.0, 0.0, 26.0, 24.72419584751629, 0.1725511034163314, 0.0, 1.0, 44765.79111600825], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1892400.0000, 
sim time next is 1893000.0000, 
raw observation next is [-6.1, 76.33333333333333, 0.0, 0.0, 26.0, 24.68672787153469, 0.164789958601198, 0.0, 1.0, 44787.61117912028], 
processed observation next is [0.0, 0.9130434782608695, 0.29362880886426596, 0.7633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5572273226278908, 0.5549299862003993, 0.0, 1.0, 0.2132743389481918], 
reward next is 0.7867, 
noisyNet noise sample is [array([1.2234461], dtype=float32), 1.2783389]. 
=============================================
[2019-04-04 13:12:56,915] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[76.53836]
 [76.69726]
 [76.86556]
 [77.01302]
 [77.07111]], R is [[76.35484314]
 [76.37812042]
 [76.40125275]
 [76.42420197]
 [76.44696808]].
[2019-04-04 13:12:58,463] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4265524e-08 1.5280892e-09 9.5014674e-14 2.3953766e-12 1.0000000e+00
 4.7298376e-10 3.8481916e-14], sum to 1.0000
[2019-04-04 13:12:58,463] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4326
[2019-04-04 13:12:58,487] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 23.35188835003242, -0.08095906303550617, 0.0, 1.0, 47173.95559764375], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1840200.0000, 
sim time next is 1840800.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 23.32053857987047, -0.08742471119737598, 0.0, 1.0, 47192.67248196817], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.44337821498920577, 0.47085842960087465, 0.0, 1.0, 0.22472701181889604], 
reward next is 0.7753, 
noisyNet noise sample is [array([1.7340453], dtype=float32), -0.7149601]. 
=============================================
[2019-04-04 13:13:00,735] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.48385112e-09 6.95585256e-10 4.11967942e-15 3.49792894e-14
 1.00000000e+00 1.10899345e-11 6.25290394e-15], sum to 1.0000
[2019-04-04 13:13:00,735] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3490
[2019-04-04 13:13:00,804] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.583333333333333, 83.5, 0.0, 0.0, 26.0, 25.02853813152514, 0.2401586079503625, 0.0, 1.0, 37513.57429820497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1882200.0000, 
sim time next is 1882800.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.01443393387777, 0.2373745356532135, 0.0, 1.0, 46071.14328483002], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5845361611564807, 0.5791248452177379, 0.0, 1.0, 0.21938639659442866], 
reward next is 0.7806, 
noisyNet noise sample is [array([1.4877663], dtype=float32), -1.3044121]. 
=============================================
[2019-04-04 13:13:04,768] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6006634e-09 9.8970283e-11 3.1727364e-15 5.4919627e-14 1.0000000e+00
 3.3045230e-10 2.9796800e-15], sum to 1.0000
[2019-04-04 13:13:04,780] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8919
[2019-04-04 13:13:04,799] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.2, 62.0, 116.1666666666667, 0.0, 26.0, 25.76057878575094, 0.3445324960951944, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1952400.0000, 
sim time next is 1953000.0000, 
raw observation next is [-3.1, 62.0, 112.0, 0.0, 26.0, 25.76909205071495, 0.3420711769765912, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.37673130193905824, 0.62, 0.37333333333333335, 0.0, 0.6666666666666666, 0.6474243375595791, 0.6140237256588638, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6638577], dtype=float32), 0.09290674]. 
=============================================
[2019-04-04 13:13:04,805] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.73179 ]
 [81.12259 ]
 [81.481804]
 [81.824684]
 [82.176346]], R is [[80.57185364]
 [80.76613617]
 [80.95847321]
 [81.14888763]
 [81.2278595 ]].
[2019-04-04 13:13:14,203] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.3684206e-09 1.1136027e-09 4.8204366e-15 4.9208420e-14 1.0000000e+00
 3.9968415e-10 2.7632275e-15], sum to 1.0000
[2019-04-04 13:13:14,203] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0177
[2019-04-04 13:13:14,255] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 75.0, 151.5, 0.0, 26.0, 25.69511802084746, 0.3523479874246633, 1.0, 1.0, 22250.8187601266], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2030400.0000, 
sim time next is 2031000.0000, 
raw observation next is [-4.5, 75.66666666666667, 153.0, 0.0, 26.0, 25.717937180038, 0.3553206059613597, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7566666666666667, 0.51, 0.0, 0.6666666666666666, 0.6431614316698333, 0.6184402019871199, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0409653], dtype=float32), -1.6499014]. 
=============================================
[2019-04-04 13:13:14,260] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[86.31654]
 [86.1909 ]
 [86.03822]
 [85.82393]
 [85.56129]], R is [[86.48371887]
 [86.51292419]
 [86.53877258]
 [86.5613327 ]
 [86.5797348 ]].
[2019-04-04 13:13:22,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4123698e-09 7.2746148e-10 1.5649249e-15 1.9908639e-14 1.0000000e+00
 5.6661321e-11 2.6588301e-15], sum to 1.0000
[2019-04-04 13:13:22,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3718
[2019-04-04 13:13:22,290] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.383333333333333, 76.16666666666666, 236.3333333333333, 73.66666666666666, 26.0, 25.77627183656023, 0.3880408419230777, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2112600.0000, 
sim time next is 2113200.0000, 
raw observation next is [-7.3, 75.0, 250.5, 80.5, 26.0, 25.75074458518536, 0.383038209742916, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.26038781163434904, 0.75, 0.835, 0.08895027624309393, 0.6666666666666666, 0.64589538209878, 0.6276794032476386, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6701406], dtype=float32), 1.01253]. 
=============================================
[2019-04-04 13:13:24,698] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0051681e-09 1.5108620e-10 6.8532573e-16 2.2041308e-14 1.0000000e+00
 1.0906762e-11 2.4724841e-16], sum to 1.0000
[2019-04-04 13:13:24,699] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3933
[2019-04-04 13:13:24,746] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 75.0, 71.5, 356.5, 26.0, 25.96182469659184, 0.4075528619308668, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2192400.0000, 
sim time next is 2193000.0000, 
raw observation next is [-5.5, 74.33333333333333, 81.66666666666669, 388.0, 26.0, 25.95226779352625, 0.4103408484738801, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3102493074792244, 0.7433333333333333, 0.2722222222222223, 0.4287292817679558, 0.6666666666666666, 0.6626889827938541, 0.6367802828246267, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24676804], dtype=float32), 0.40226287]. 
=============================================
[2019-04-04 13:13:24,750] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[86.24353 ]
 [85.79076 ]
 [85.38223 ]
 [84.769424]
 [84.12358 ]], R is [[86.79024506]
 [86.92234039]
 [87.05311584]
 [87.18258667]
 [87.3107605 ]].
[2019-04-04 13:13:26,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.1651717e-08 1.0935060e-09 5.3883251e-15 2.0671968e-13 1.0000000e+00
 1.2092193e-09 7.1008915e-15], sum to 1.0000
[2019-04-04 13:13:26,158] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8449
[2019-04-04 13:13:26,170] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 75.66666666666667, 0.0, 0.0, 26.0, 23.97494249009321, 0.04286466264166387, 0.0, 1.0, 41960.29159538803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2178600.0000, 
sim time next is 2179200.0000, 
raw observation next is [-6.2, 76.33333333333334, 0.0, 0.0, 26.0, 23.91386946123134, 0.03290725307741663, 0.0, 1.0, 41953.82717137316], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7633333333333334, 0.0, 0.0, 0.6666666666666666, 0.49282245510261163, 0.5109690843591389, 0.0, 1.0, 0.19978012938749123], 
reward next is 0.8002, 
noisyNet noise sample is [array([-1.5326223], dtype=float32), 0.025292289]. 
=============================================
[2019-04-04 13:13:30,870] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3362748e-09 1.0747652e-10 2.8402819e-15 3.5231031e-15 1.0000000e+00
 4.0784383e-11 2.0444288e-15], sum to 1.0000
[2019-04-04 13:13:30,870] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0507
[2019-04-04 13:13:30,927] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 70.5, 13.66666666666666, 0.0, 26.0, 25.68560474347157, 0.3129328094752439, 1.0, 1.0, 36275.17176850855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2221800.0000, 
sim time next is 2222400.0000, 
raw observation next is [-4.5, 70.0, 8.333333333333332, 0.0, 26.0, 25.34253479380829, 0.3222834657607149, 1.0, 1.0, 33453.02674383736], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.7, 0.027777777777777773, 0.0, 0.6666666666666666, 0.6118778994840243, 0.6074278219202384, 1.0, 1.0, 0.15930012735160648], 
reward next is 0.8407, 
noisyNet noise sample is [array([-0.53055763], dtype=float32), 0.8137212]. 
=============================================
[2019-04-04 13:13:33,364] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6686327e-08 9.0895147e-10 5.9735230e-14 2.7920721e-13 1.0000000e+00
 4.0147143e-09 1.9984815e-13], sum to 1.0000
[2019-04-04 13:13:33,368] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6556
[2019-04-04 13:13:33,428] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8, 50.0, 11.0, 0.0, 26.0, 24.50605708785345, 0.26297702924008, 1.0, 1.0, 196978.98116789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2308800.0000, 
sim time next is 2309400.0000, 
raw observation next is [-0.8999999999999999, 50.5, 0.0, 0.0, 26.0, 24.89694893779941, 0.3163109564841892, 1.0, 1.0, 133675.7422501435], 
processed observation next is [1.0, 0.7391304347826086, 0.43767313019390586, 0.505, 0.0, 0.0, 0.6666666666666666, 0.5747457448166177, 0.6054369854947298, 1.0, 1.0, 0.6365511535721119], 
reward next is 0.3634, 
noisyNet noise sample is [array([1.3384259], dtype=float32), 0.69423085]. 
=============================================
[2019-04-04 13:13:38,210] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.6324886e-09 7.3469153e-10 7.0358821e-15 5.9175030e-14 1.0000000e+00
 5.3363383e-11 1.7803015e-14], sum to 1.0000
[2019-04-04 13:13:38,213] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1229
[2019-04-04 13:13:38,265] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.633333333333333, 64.0, 136.0, 435.0, 26.0, 24.97732925653763, 0.2825121573736933, 0.0, 1.0, 45070.65678910382], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2370000.0000, 
sim time next is 2370600.0000, 
raw observation next is [-2.55, 63.5, 139.0, 450.0, 26.0, 24.91671526074346, 0.28865769130713, 0.0, 1.0, 73411.02284631158], 
processed observation next is [0.0, 0.43478260869565216, 0.3919667590027701, 0.635, 0.4633333333333333, 0.4972375690607735, 0.6666666666666666, 0.5763929383952883, 0.59621923043571, 0.0, 1.0, 0.34957629926815037], 
reward next is 0.6504, 
noisyNet noise sample is [array([-0.8232712], dtype=float32), -0.18463254]. 
=============================================
[2019-04-04 13:13:39,008] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.1463462e-08 2.4859991e-08 1.5292173e-13 1.0963382e-12 9.9999988e-01
 1.5474411e-09 8.5609850e-14], sum to 1.0000
[2019-04-04 13:13:39,009] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0969
[2019-04-04 13:13:39,020] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.70190822240816, 0.2351848978208539, 0.0, 1.0, 38909.75849403705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2340000.0000, 
sim time next is 2340600.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.69102359870517, 0.2397118534981123, 0.0, 1.0, 38992.9378946548], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5575852998920974, 0.5799039511660374, 0.0, 1.0, 0.18568065664121333], 
reward next is 0.8143, 
noisyNet noise sample is [array([0.80745757], dtype=float32), -0.19079643]. 
=============================================
[2019-04-04 13:13:41,162] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7527231e-07 7.4573636e-09 1.6566955e-12 6.1422356e-12 9.9999976e-01
 7.0505450e-09 4.8299958e-12], sum to 1.0000
[2019-04-04 13:13:41,162] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9336
[2019-04-04 13:13:41,181] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.9, 45.5, 0.0, 0.0, 26.0, 24.55176793764406, 0.1346142892015435, 0.0, 1.0, 43152.39630623006], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2421000.0000, 
sim time next is 2421600.0000, 
raw observation next is [-6.0, 46.33333333333334, 0.0, 0.0, 26.0, 24.50659603209051, 0.1235163609994964, 0.0, 1.0, 43191.20697287077], 
processed observation next is [0.0, 0.0, 0.296398891966759, 0.46333333333333343, 0.0, 0.0, 0.6666666666666666, 0.5422163360075425, 0.5411721203331655, 0.0, 1.0, 0.20567241415652748], 
reward next is 0.7943, 
noisyNet noise sample is [array([-2.3682687], dtype=float32), 0.93181384]. 
=============================================
[2019-04-04 13:13:42,283] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3547806e-07 8.2239531e-09 1.4228345e-12 3.7016063e-12 9.9999988e-01
 1.5495388e-08 1.5379874e-12], sum to 1.0000
[2019-04-04 13:13:42,284] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8125
[2019-04-04 13:13:42,307] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 59.0, 9.166666666666664, 102.6666666666667, 26.0, 22.84322241088918, -0.229594019141632, 0.0, 1.0, 44229.21921132791], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2446800.0000, 
sim time next is 2447400.0000, 
raw observation next is [-9.5, 58.5, 15.33333333333333, 165.3333333333333, 26.0, 22.82091504046378, -0.2235073724585136, 0.0, 1.0, 44179.56036799567], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.585, 0.0511111111111111, 0.18268876611418042, 0.6666666666666666, 0.4017429200386484, 0.4254975425138288, 0.0, 1.0, 0.21037885889521749], 
reward next is 0.7896, 
noisyNet noise sample is [array([-0.3763419], dtype=float32), 0.48799482]. 
=============================================
[2019-04-04 13:13:46,108] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.43884655e-08 1.80347043e-10 2.31458317e-15 2.59544210e-14
 1.00000000e+00 1.45343904e-10 1.66737335e-15], sum to 1.0000
[2019-04-04 13:13:46,108] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4535
[2019-04-04 13:13:46,142] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8, 47.66666666666667, 149.5, 49.33333333333333, 26.0, 25.77782554553446, 0.3018036705982403, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2544000.0000, 
sim time next is 2544600.0000, 
raw observation next is [-0.7, 47.33333333333334, 166.0, 53.66666666666666, 26.0, 25.76500204087464, 0.2991750007864951, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.443213296398892, 0.47333333333333344, 0.5533333333333333, 0.059300184162062605, 0.6666666666666666, 0.6470835034062201, 0.599725000262165, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8427085], dtype=float32), 1.5271534]. 
=============================================
[2019-04-04 13:13:48,885] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1131747e-07 3.8962220e-09 1.3532729e-12 4.1189777e-12 9.9999988e-01
 6.1897936e-09 9.9943318e-13], sum to 1.0000
[2019-04-04 13:13:48,885] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5810
[2019-04-04 13:13:48,948] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1833333333333333, 29.66666666666666, 0.0, 0.0, 26.0, 24.90222959559537, 0.2121668773808011, 0.0, 1.0, 45497.64675856138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2487000.0000, 
sim time next is 2487600.0000, 
raw observation next is [0.0, 30.0, 0.0, 0.0, 26.0, 24.90638450831539, 0.2132178166322215, 0.0, 1.0, 41117.30099028038], 
processed observation next is [0.0, 0.8260869565217391, 0.46260387811634357, 0.3, 0.0, 0.0, 0.6666666666666666, 0.5755320423596159, 0.5710726055440739, 0.0, 1.0, 0.19579667138228754], 
reward next is 0.8042, 
noisyNet noise sample is [array([-0.55483043], dtype=float32), -0.056845892]. 
=============================================
[2019-04-04 13:13:51,743] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7647753e-09 5.6614220e-11 1.4899495e-16 4.1140255e-15 1.0000000e+00
 2.5141391e-11 8.6656759e-16], sum to 1.0000
[2019-04-04 13:13:51,744] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4340
[2019-04-04 13:13:51,783] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.566666666666666, 68.33333333333333, 102.8333333333333, 121.6666666666667, 26.0, 25.87453131087127, 0.3648260989888376, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2626800.0000, 
sim time next is 2627400.0000, 
raw observation next is [-5.283333333333333, 66.66666666666667, 112.6666666666667, 152.3333333333333, 26.0, 25.86916515127101, 0.3641132428485607, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.31625115420129274, 0.6666666666666667, 0.37555555555555564, 0.16832412523020251, 0.6666666666666666, 0.6557637626059174, 0.6213710809495202, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8377554], dtype=float32), 0.600678]. 
=============================================
[2019-04-04 13:13:53,834] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.0377991e-09 3.8221071e-10 2.0950208e-14 3.2204724e-13 1.0000000e+00
 5.3894489e-10 3.4389202e-14], sum to 1.0000
[2019-04-04 13:13:53,837] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7920
[2019-04-04 13:13:53,861] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 68.0, 0.0, 0.0, 26.0, 24.76136850277294, 0.240152773989124, 0.0, 1.0, 41810.51589318649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2595600.0000, 
sim time next is 2596200.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 26.0, 24.77660116535049, 0.2452729850617457, 0.0, 1.0, 41773.92781756782], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5647167637792073, 0.5817576616872485, 0.0, 1.0, 0.19892346579794198], 
reward next is 0.8011, 
noisyNet noise sample is [array([0.30905887], dtype=float32), -0.436143]. 
=============================================
[2019-04-04 13:13:54,471] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1405874e-09 1.8144146e-10 5.1495648e-15 1.3495760e-13 1.0000000e+00
 3.5293402e-10 7.4218535e-15], sum to 1.0000
[2019-04-04 13:13:54,473] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9805
[2019-04-04 13:13:54,501] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 64.33333333333333, 0.0, 0.0, 26.0, 25.11178003142101, 0.4280626656374258, 0.0, 1.0, 69899.39044286135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2666400.0000, 
sim time next is 2667000.0000, 
raw observation next is [-1.2, 64.66666666666667, 0.0, 0.0, 26.0, 25.24265086935931, 0.4437879492600342, 0.0, 1.0, 52633.80784548139], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.6466666666666667, 0.0, 0.0, 0.6666666666666666, 0.6035542391132758, 0.6479293164200114, 0.0, 1.0, 0.25063718021657805], 
reward next is 0.7494, 
noisyNet noise sample is [array([-0.79447085], dtype=float32), -0.5504984]. 
=============================================
[2019-04-04 13:13:54,517] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.95967]
 [80.82306]
 [80.32789]
 [79.63118]
 [79.59992]], R is [[81.37073517]
 [81.2241745 ]
 [80.84555817]
 [80.13607788]
 [80.22362518]].
[2019-04-04 13:13:57,755] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7207502e-08 6.9774719e-10 6.2792213e-15 2.3796329e-13 1.0000000e+00
 7.9988571e-10 1.2783250e-14], sum to 1.0000
[2019-04-04 13:13:57,755] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8681
[2019-04-04 13:13:57,787] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.26666666666667, 80.66666666666666, 0.0, 0.0, 26.0, 24.06676582549721, 0.1011715966451802, 0.0, 1.0, 44448.1726287596], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2688000.0000, 
sim time next is 2688600.0000, 
raw observation next is [-12.58333333333333, 81.83333333333334, 0.0, 0.0, 26.0, 23.96966267632404, 0.0878212694810725, 0.0, 1.0, 44478.09682935595], 
processed observation next is [1.0, 0.08695652173913043, 0.11403508771929832, 0.8183333333333335, 0.0, 0.0, 0.6666666666666666, 0.4974718896936701, 0.5292737564936908, 0.0, 1.0, 0.21180046109217118], 
reward next is 0.7882, 
noisyNet noise sample is [array([1.7321974], dtype=float32), 0.14224912]. 
=============================================
[2019-04-04 13:13:58,193] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5243224e-09 6.8982570e-11 1.5816203e-16 5.3345568e-15 1.0000000e+00
 6.3817892e-12 2.5505994e-16], sum to 1.0000
[2019-04-04 13:13:58,193] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2352
[2019-04-04 13:13:58,229] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.666666666666668, 64.0, 112.8333333333333, 763.1666666666667, 26.0, 26.04560151193266, 0.4791479548901019, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2719200.0000, 
sim time next is 2719800.0000, 
raw observation next is [-8.5, 64.0, 112.0, 781.0, 26.0, 26.03370086955621, 0.4770820838724543, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.22714681440443216, 0.64, 0.37333333333333335, 0.8629834254143647, 0.6666666666666666, 0.6694750724630175, 0.6590273612908181, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11840502], dtype=float32), 0.17473309]. 
=============================================
[2019-04-04 13:13:59,505] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1120679e-09 4.4892293e-10 4.6737139e-15 3.5872648e-14 1.0000000e+00
 1.9621205e-09 5.4119946e-15], sum to 1.0000
[2019-04-04 13:13:59,507] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1561
[2019-04-04 13:13:59,551] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 63.0, 0.0, 0.0, 26.0, 24.95541146597098, 0.3727447394239703, 0.0, 1.0, 64368.91402298488], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2664000.0000, 
sim time next is 2664600.0000, 
raw observation next is [-1.2, 63.33333333333333, 0.0, 0.0, 26.0, 25.0022096479429, 0.3740183522097125, 0.0, 1.0, 23329.42445964769], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.6333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5835174706619085, 0.6246727840699041, 0.0, 1.0, 0.11109249742689375], 
reward next is 0.8889, 
noisyNet noise sample is [array([-0.667266], dtype=float32), 0.30024093]. 
=============================================
[2019-04-04 13:14:02,395] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3509230e-09 8.7321650e-11 2.0650193e-15 1.3166790e-14 1.0000000e+00
 4.1824108e-10 2.7791901e-15], sum to 1.0000
[2019-04-04 13:14:02,397] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3322
[2019-04-04 13:14:02,411] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.666666666666666, 30.83333333333333, 205.3333333333333, 115.3333333333333, 26.0, 25.91481586539268, 0.4433567312374745, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2814600.0000, 
sim time next is 2815200.0000, 
raw observation next is [6.0, 30.0, 183.5, 86.5, 26.0, 25.62846943455152, 0.417903513866839, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6288088642659281, 0.3, 0.6116666666666667, 0.09558011049723757, 0.6666666666666666, 0.6357057862126266, 0.6393011712889464, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.44146], dtype=float32), 1.393796]. 
=============================================
[2019-04-04 13:14:04,916] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.15708543e-10 1.30301525e-11 1.26452781e-16 5.68152855e-16
 1.00000000e+00 5.21558361e-12 1.18195626e-16], sum to 1.0000
[2019-04-04 13:14:04,920] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4033
[2019-04-04 13:14:04,949] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 100.0, 80.33333333333333, 18.0, 26.0, 25.92256783944558, 0.4590155204506943, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2908200.0000, 
sim time next is 2908800.0000, 
raw observation next is [2.0, 100.0, 78.0, 27.0, 26.0, 25.92699103207257, 0.3547892446909445, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 1.0, 0.26, 0.02983425414364641, 0.6666666666666666, 0.6605825860060476, 0.6182630815636482, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.439088], dtype=float32), -0.34789774]. 
=============================================
[2019-04-04 13:14:06,624] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0896543e-09 5.6639526e-10 5.9329237e-16 1.0414954e-14 1.0000000e+00
 1.1772239e-10 3.7980796e-15], sum to 1.0000
[2019-04-04 13:14:06,624] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6214
[2019-04-04 13:14:06,645] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 75.5, 0.0, 0.0, 26.0, 25.17578929029746, 0.3090962178628656, 0.0, 1.0, 51550.41016502316], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2856600.0000, 
sim time next is 2857200.0000, 
raw observation next is [1.0, 76.66666666666667, 0.0, 0.0, 26.0, 25.13627783880226, 0.30708545188398, 0.0, 1.0, 54032.04712208622], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7666666666666667, 0.0, 0.0, 0.6666666666666666, 0.5946898199001884, 0.60236181729466, 0.0, 1.0, 0.25729546248612484], 
reward next is 0.7427, 
noisyNet noise sample is [array([0.9859458], dtype=float32), -0.55157685]. 
=============================================
[2019-04-04 13:14:08,101] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0869134e-09 1.5068502e-10 2.7476499e-15 1.7416966e-14 1.0000000e+00
 1.8076339e-10 2.6965417e-15], sum to 1.0000
[2019-04-04 13:14:08,101] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7749
[2019-04-04 13:14:08,165] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 39.5, 178.0, 685.0, 26.0, 24.47360516052577, 0.2947791056245659, 1.0, 1.0, 196217.9094192963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2809800.0000, 
sim time next is 2810400.0000, 
raw observation next is [3.333333333333333, 38.0, 189.8333333333333, 599.6666666666667, 26.0, 24.76466586979123, 0.3882731598148244, 1.0, 1.0, 181226.9191349661], 
processed observation next is [1.0, 0.5217391304347826, 0.5549399815327793, 0.38, 0.6327777777777777, 0.6626151012891345, 0.6666666666666666, 0.5637221558159359, 0.6294243866049415, 1.0, 1.0, 0.8629853292141243], 
reward next is 0.1370, 
noisyNet noise sample is [array([-0.22250602], dtype=float32), 0.20835553]. 
=============================================
[2019-04-04 13:14:11,976] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3495739e-09 2.2370528e-10 1.1440023e-15 4.4829395e-14 1.0000000e+00
 1.8558487e-10 3.2602525e-15], sum to 1.0000
[2019-04-04 13:14:11,978] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2628
[2019-04-04 13:14:11,994] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.77293840776252, 0.2987411381095923, 0.0, 1.0, 43143.86357743706], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2944800.0000, 
sim time next is 2945400.0000, 
raw observation next is [-2.166666666666667, 84.83333333333334, 0.0, 0.0, 26.0, 24.7361925469068, 0.2955050117979505, 0.0, 1.0, 43105.1076595132], 
processed observation next is [0.0, 0.08695652173913043, 0.4025854108956602, 0.8483333333333334, 0.0, 0.0, 0.6666666666666666, 0.5613493789089, 0.5985016705993168, 0.0, 1.0, 0.20526241742625334], 
reward next is 0.7947, 
noisyNet noise sample is [array([-0.9346027], dtype=float32), -0.06889388]. 
=============================================
[2019-04-04 13:14:17,273] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9360194e-08 3.7047201e-09 3.5137098e-14 5.5175320e-13 1.0000000e+00
 2.3811635e-09 1.4337208e-12], sum to 1.0000
[2019-04-04 13:14:17,273] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4626
[2019-04-04 13:14:17,301] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 72.33333333333333, 0.0, 0.0, 26.0, 23.84154308235659, 0.009861247614751123, 0.0, 1.0, 40147.63858515339], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3044400.0000, 
sim time next is 3045000.0000, 
raw observation next is [-6.0, 71.16666666666667, 0.0, 0.0, 26.0, 23.81503007289386, 0.003373307854165591, 0.0, 1.0, 40163.68043550339], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.7116666666666667, 0.0, 0.0, 0.6666666666666666, 0.4845858394078218, 0.5011244359513886, 0.0, 1.0, 0.1912556211214447], 
reward next is 0.8087, 
noisyNet noise sample is [array([1.0083456], dtype=float32), 0.8217928]. 
=============================================
[2019-04-04 13:14:17,311] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[74.34524 ]
 [74.44834 ]
 [74.53475 ]
 [74.607544]
 [74.64777 ]], R is [[74.28672791]
 [74.35268402]
 [74.41798401]
 [74.48255157]
 [74.54638672]].
[2019-04-04 13:14:19,198] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1183970e-08 3.3613218e-10 8.4594888e-14 1.1818477e-13 1.0000000e+00
 5.4568067e-10 1.2669433e-14], sum to 1.0000
[2019-04-04 13:14:19,199] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4332
[2019-04-04 13:14:19,215] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 40.5, 99.0, 775.0, 26.0, 25.11467646398395, 0.3634129527295855, 0.0, 1.0, 18704.79828359369], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3076200.0000, 
sim time next is 3076800.0000, 
raw observation next is [-0.3333333333333334, 40.0, 96.5, 758.0, 26.0, 25.11654394227409, 0.3630462373463961, 0.0, 1.0, 18703.96488840054], 
processed observation next is [0.0, 0.6086956521739131, 0.4533702677747, 0.4, 0.32166666666666666, 0.8375690607734807, 0.6666666666666666, 0.593045328522841, 0.6210154124487987, 0.0, 1.0, 0.089066499468574], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.0077231], dtype=float32), -0.5072152]. 
=============================================
[2019-04-04 13:14:33,121] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.0118070e-09 3.9678488e-10 6.3347493e-15 9.1105326e-14 1.0000000e+00
 4.0372344e-10 1.0330790e-14], sum to 1.0000
[2019-04-04 13:14:33,122] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1092
[2019-04-04 13:14:33,159] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.68721173322299, 0.5576780118462225, 0.0, 1.0, 43766.80195727103], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3268800.0000, 
sim time next is 3269400.0000, 
raw observation next is [-4.166666666666667, 72.66666666666667, 0.0, 0.0, 26.0, 25.58821807001023, 0.545826058219329, 0.0, 1.0, 42685.13265477082], 
processed observation next is [1.0, 0.8695652173913043, 0.3471837488457987, 0.7266666666666667, 0.0, 0.0, 0.6666666666666666, 0.632351505834186, 0.681942019406443, 0.0, 1.0, 0.20326253645128964], 
reward next is 0.7967, 
noisyNet noise sample is [array([-0.83448386], dtype=float32), 1.0758682]. 
=============================================
[2019-04-04 13:14:50,720] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4691830e-08 3.0534064e-09 3.1517622e-13 9.9930372e-13 1.0000000e+00
 1.0584321e-09 3.3344638e-14], sum to 1.0000
[2019-04-04 13:14:50,725] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6750
[2019-04-04 13:14:50,755] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.2, 27.0, 0.0, 0.0, 26.0, 25.47927758190128, 0.3600547033323033, 0.0, 1.0, 37422.98156014001], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3639600.0000, 
sim time next is 3640200.0000, 
raw observation next is [8.166666666666666, 27.33333333333333, 0.0, 0.0, 26.0, 25.47450125002256, 0.3782335576481248, 0.0, 1.0, 36726.58361929486], 
processed observation next is [0.0, 0.13043478260869565, 0.6888273314866113, 0.27333333333333326, 0.0, 0.0, 0.6666666666666666, 0.6228751041685466, 0.626077852549375, 0.0, 1.0, 0.17488849342521362], 
reward next is 0.8251, 
noisyNet noise sample is [array([0.76410717], dtype=float32), -1.024419]. 
=============================================
[2019-04-04 13:14:52,056] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4040497e-08 3.9903005e-09 1.2265683e-14 8.3578076e-14 1.0000000e+00
 1.1818433e-09 9.3602650e-14], sum to 1.0000
[2019-04-04 13:14:52,058] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0789
[2019-04-04 13:14:52,066] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.666666666666666, 38.0, 116.1666666666667, 818.1666666666666, 26.0, 25.64460408562882, 0.4868448396768173, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3670800.0000, 
sim time next is 3671400.0000, 
raw observation next is [5.333333333333333, 41.5, 116.3333333333333, 820.3333333333334, 26.0, 25.59441292760549, 0.4762664399567642, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6103416435826409, 0.415, 0.38777777777777767, 0.9064456721915286, 0.6666666666666666, 0.6328677439671241, 0.658755479985588, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3347989], dtype=float32), 0.93525386]. 
=============================================
[2019-04-04 13:14:54,170] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0291305e-09 6.5518535e-10 1.5385435e-14 2.9538795e-13 1.0000000e+00
 1.1382657e-10 1.7107674e-14], sum to 1.0000
[2019-04-04 13:14:54,172] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5301
[2019-04-04 13:14:54,194] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 67.0, 0.0, 0.0, 26.0, 25.51597923177305, 0.4030115921958948, 0.0, 1.0, 82663.0685763576], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3706200.0000, 
sim time next is 3706800.0000, 
raw observation next is [0.6666666666666667, 68.66666666666667, 0.0, 0.0, 26.0, 25.45375880727784, 0.4048932638732558, 0.0, 1.0, 89476.66854211177], 
processed observation next is [0.0, 0.9130434782608695, 0.4810710987996307, 0.6866666666666668, 0.0, 0.0, 0.6666666666666666, 0.6211465672731533, 0.6349644212910853, 0.0, 1.0, 0.4260793740100561], 
reward next is 0.5739, 
noisyNet noise sample is [array([-0.46515206], dtype=float32), 2.0863996]. 
=============================================
[2019-04-04 13:14:58,662] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3419600e-09 1.7745545e-11 2.0236950e-16 2.4208000e-15 1.0000000e+00
 2.5378819e-11 5.3669469e-16], sum to 1.0000
[2019-04-04 13:14:58,662] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4398
[2019-04-04 13:14:58,674] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 60.0, 115.0, 818.0, 26.0, 26.22333361838097, 0.6072738174020901, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3763800.0000, 
sim time next is 3764400.0000, 
raw observation next is [-0.3333333333333334, 60.0, 113.5, 811.0, 26.0, 26.37469784386521, 0.6264907489607064, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4533702677747, 0.6, 0.37833333333333335, 0.8961325966850828, 0.6666666666666666, 0.6978914869887675, 0.7088302496535688, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30205512], dtype=float32), -0.2088974]. 
=============================================
[2019-04-04 13:15:01,959] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.8611990e-09 6.8743261e-11 4.6038007e-15 1.6534720e-13 1.0000000e+00
 1.2849302e-10 1.9980069e-15], sum to 1.0000
[2019-04-04 13:15:01,959] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2278
[2019-04-04 13:15:01,972] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 33.0, 118.0, 841.0, 26.0, 26.46573561013935, 0.579173886349824, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4019400.0000, 
sim time next is 4020000.0000, 
raw observation next is [-4.666666666666666, 31.66666666666666, 117.3333333333333, 839.1666666666667, 26.0, 26.32047466380178, 0.5795275273239565, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.33333333333333337, 0.3166666666666666, 0.391111111111111, 0.9272559852670351, 0.6666666666666666, 0.6933728886501482, 0.6931758424413189, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01615994], dtype=float32), -2.034916]. 
=============================================
[2019-04-04 13:15:01,979] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[79.71895 ]
 [79.83386 ]
 [79.95467 ]
 [80.060486]
 [80.13859 ]], R is [[79.79067993]
 [79.99277496]
 [80.19284821]
 [80.39092255]
 [80.58701324]].
[2019-04-04 13:15:03,762] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.7416702e-08 4.8789661e-09 7.2099661e-14 4.8878794e-13 9.9999988e-01
 5.7054059e-09 1.5589077e-14], sum to 1.0000
[2019-04-04 13:15:03,762] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5081
[2019-04-04 13:15:03,772] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 26.0, 25.46277025224435, 0.4880114801750954, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3871800.0000, 
sim time next is 3872400.0000, 
raw observation next is [1.0, 51.0, 0.0, 0.0, 26.0, 25.3754596436936, 0.4713803023547343, 0.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.51, 0.0, 0.0, 0.6666666666666666, 0.6146216369744666, 0.657126767451578, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.0332593], dtype=float32), 1.3960027]. 
=============================================
[2019-04-04 13:15:05,235] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6353004e-09 3.7906907e-11 4.7590430e-16 1.8636202e-14 1.0000000e+00
 2.0061151e-10 3.9157744e-15], sum to 1.0000
[2019-04-04 13:15:05,238] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8815
[2019-04-04 13:15:05,255] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.666666666666666, 62.0, 0.0, 0.0, 26.0, 25.00477269481232, 0.2996106019262002, 0.0, 1.0, 41580.87458622423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3909000.0000, 
sim time next is 3909600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.92137362713659, 0.2939757552946549, 0.0, 1.0, 41752.84906455474], 
processed observation next is [1.0, 0.2608695652173913, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5767811355947158, 0.5979919184315516, 0.0, 1.0, 0.198823090783594], 
reward next is 0.8012, 
noisyNet noise sample is [array([0.20873407], dtype=float32), 0.2977926]. 
=============================================
[2019-04-04 13:15:09,731] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.5171116e-09 3.3972680e-10 6.9411906e-15 1.6119371e-14 1.0000000e+00
 7.8191953e-11 5.8706737e-16], sum to 1.0000
[2019-04-04 13:15:09,733] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9120
[2019-04-04 13:15:09,784] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.333333333333333, 36.0, 105.6666666666667, 694.0, 26.0, 26.29244351353863, 0.5244547053942723, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4095600.0000, 
sim time next is 4096200.0000, 
raw observation next is [-2.166666666666667, 35.5, 107.3333333333333, 709.0, 26.0, 26.33948581270214, 0.5404892476316953, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4025854108956602, 0.355, 0.3577777777777777, 0.7834254143646409, 0.6666666666666666, 0.6949571510585116, 0.6801630825438983, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14016739], dtype=float32), 0.42416105]. 
=============================================
[2019-04-04 13:15:10,906] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3985935e-08 4.5593613e-09 1.3150458e-13 9.7799048e-13 1.0000000e+00
 4.3019708e-09 5.8937806e-14], sum to 1.0000
[2019-04-04 13:15:10,907] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0127
[2019-04-04 13:15:10,960] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.53133591199392, 0.4894742782187571, 0.0, 1.0, 26827.23465259663], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4045800.0000, 
sim time next is 4046400.0000, 
raw observation next is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.49035979887336, 0.4876777494259095, 0.0, 1.0, 26822.97044646073], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.31, 0.0, 0.0, 0.6666666666666666, 0.6241966499061133, 0.6625592498086365, 0.0, 1.0, 0.12772843069743206], 
reward next is 0.8723, 
noisyNet noise sample is [array([-0.60828054], dtype=float32), 0.3543364]. 
=============================================
[2019-04-04 13:15:11,582] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6279904e-09 5.8780869e-10 1.8958730e-14 8.6793813e-14 1.0000000e+00
 4.9051652e-10 1.4863461e-14], sum to 1.0000
[2019-04-04 13:15:11,585] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6123
[2019-04-04 13:15:11,593] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.5, 21.0, 89.0, 712.0, 26.0, 27.08656336474501, 0.7770805591007397, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4030200.0000, 
sim time next is 4030800.0000, 
raw observation next is [-1.333333333333333, 21.33333333333334, 85.33333333333334, 684.6666666666667, 26.0, 27.15256256693188, 0.783401847490123, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.42566943674976926, 0.2133333333333334, 0.2844444444444445, 0.7565377532228362, 0.6666666666666666, 0.7627135472443234, 0.7611339491633743, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9861953], dtype=float32), -1.5487044]. 
=============================================
[2019-04-04 13:15:12,574] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.4067588e-09 7.8422902e-10 2.0456662e-14 4.3252378e-13 1.0000000e+00
 1.1443899e-09 5.1874720e-14], sum to 1.0000
[2019-04-04 13:15:12,575] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7712
[2019-04-04 13:15:12,621] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 29.66666666666666, 0.0, 0.0, 26.0, 25.47823706627354, 0.521852799170247, 0.0, 1.0, 51730.45138291249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4048800.0000, 
sim time next is 4049400.0000, 
raw observation next is [-4.0, 29.33333333333333, 0.0, 0.0, 26.0, 25.58058165517296, 0.5279482683358331, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.2933333333333333, 0.0, 0.0, 0.6666666666666666, 0.63171513793108, 0.6759827561119444, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6685089], dtype=float32), 0.20706385]. 
=============================================
[2019-04-04 13:15:13,790] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5975379e-08 1.5421259e-09 1.0534311e-13 1.9462234e-13 1.0000000e+00
 3.0402554e-09 1.0716910e-13], sum to 1.0000
[2019-04-04 13:15:13,796] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6475
[2019-04-04 13:15:13,805] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.166666666666667, 29.33333333333334, 0.0, 0.0, 26.0, 25.68633156435184, 0.5134799098165591, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4050600.0000, 
sim time next is 4051200.0000, 
raw observation next is [-4.333333333333334, 29.66666666666667, 0.0, 0.0, 26.0, 25.64828024587177, 0.4986974305147748, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3425669436749769, 0.2966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6373566871559809, 0.6662324768382583, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12774141], dtype=float32), -0.13999984]. 
=============================================
[2019-04-04 13:15:18,195] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 13:15:18,196] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 13:15:18,198] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:15:18,198] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 13:15:18,198] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 13:15:18,198] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:15:18,199] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:15:18,206] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run13
[2019-04-04 13:15:18,206] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run13
[2019-04-04 13:15:18,235] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run13
[2019-04-04 13:16:56,546] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.165344], dtype=float32), 0.18997048]
[2019-04-04 13:16:56,547] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.571269619666666, 73.91444200333333, 95.73338154166666, 699.0095465333334, 26.0, 25.57622386773361, 0.4172380839105747, 1.0, 1.0, 18708.73289222914]
[2019-04-04 13:16:56,547] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 13:16:56,548] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.0243913e-09 7.4996939e-11 5.6199011e-16 6.3326957e-15 1.0000000e+00
 1.9060645e-11 8.3186756e-16], sampled 0.5142621919556691
[2019-04-04 13:17:00,596] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 13:17:20,341] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 13:17:21,555] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 13:17:22,581] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1200000, evaluation results [1200000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 13:17:23,249] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.9392475e-09 1.6031535e-09 3.6566223e-14 5.4148885e-14 1.0000000e+00
 4.0278203e-10 1.2573867e-13], sum to 1.0000
[2019-04-04 13:17:23,252] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3933
[2019-04-04 13:17:23,262] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.9, 69.66666666666667, 0.0, 0.0, 26.0, 24.92461726335324, 0.2889743928266004, 0.0, 1.0, 55949.55268033745], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4302600.0000, 
sim time next is 4303200.0000, 
raw observation next is [5.8, 70.33333333333334, 0.0, 0.0, 26.0, 24.88597452308905, 0.2970640692333615, 0.0, 1.0, 196792.3740908471], 
processed observation next is [0.0, 0.8260869565217391, 0.6232686980609419, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.5738312102574209, 0.5990213564111205, 0.0, 1.0, 0.9371065432897481], 
reward next is 0.0629, 
noisyNet noise sample is [array([-0.3329213], dtype=float32), 0.029696736]. 
=============================================
[2019-04-04 13:17:25,765] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0060260e-08 1.8791160e-09 2.2804390e-14 2.2205466e-13 1.0000000e+00
 1.3274497e-09 6.0069422e-14], sum to 1.0000
[2019-04-04 13:17:25,769] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5748
[2019-04-04 13:17:25,792] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.50414780380787, 0.3775437252658279, 0.0, 1.0, 47504.43225497962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4222800.0000, 
sim time next is 4223400.0000, 
raw observation next is [1.0, 43.66666666666667, 0.0, 0.0, 26.0, 25.4624356196189, 0.3756094466912649, 0.0, 1.0, 63166.28451238402], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6218696349682418, 0.6252031488970883, 0.0, 1.0, 0.3007918310113525], 
reward next is 0.6992, 
noisyNet noise sample is [array([0.6739937], dtype=float32), -0.3615953]. 
=============================================
[2019-04-04 13:17:26,864] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8232897e-09 7.0534315e-11 4.2492662e-16 2.4826265e-15 1.0000000e+00
 8.0485445e-11 7.0267323e-16], sum to 1.0000
[2019-04-04 13:17:26,868] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6206
[2019-04-04 13:17:26,882] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.233333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.56750731101911, 0.3829129231444173, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4342200.0000, 
sim time next is 4342800.0000, 
raw observation next is [3.166666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.54345355660199, 0.3710624944588054, 0.0, 1.0, 18745.39234814179], 
processed observation next is [1.0, 0.2608695652173913, 0.5503231763619576, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6286211297168324, 0.6236874981529351, 0.0, 1.0, 0.08926377308638948], 
reward next is 0.9107, 
noisyNet noise sample is [array([2.6209788], dtype=float32), 1.0250891]. 
=============================================
[2019-04-04 13:17:29,219] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.1630887e-10 2.8280007e-11 1.6985599e-16 2.5104443e-15 1.0000000e+00
 1.6504173e-11 2.1334981e-15], sum to 1.0000
[2019-04-04 13:17:29,223] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9363
[2019-04-04 13:17:29,232] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 52.0, 120.5, 834.5, 26.0, 25.21157871866269, 0.4034664730265651, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4276800.0000, 
sim time next is 4277400.0000, 
raw observation next is [7.0, 52.0, 120.3333333333333, 838.6666666666667, 26.0, 25.23119587023443, 0.4071540685759897, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.401111111111111, 0.9267034990791898, 0.6666666666666666, 0.6025996558528691, 0.6357180228586632, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16038033], dtype=float32), -0.765408]. 
=============================================
[2019-04-04 13:17:29,471] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.6419942e-10 3.2046914e-11 4.4334970e-16 7.7647510e-16 1.0000000e+00
 2.3395945e-11 4.1493124e-16], sum to 1.0000
[2019-04-04 13:17:29,474] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9196
[2019-04-04 13:17:29,480] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.66666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 27.72181281866555, 0.9823405255548082, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4389600.0000, 
sim time next is 4390200.0000, 
raw observation next is [11.5, 54.0, 0.0, 0.0, 26.0, 27.58411239004346, 0.965841407071968, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7811634349030472, 0.54, 0.0, 0.0, 0.6666666666666666, 0.7986760325036218, 0.8219471356906559, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20470203], dtype=float32), -0.40898013]. 
=============================================
[2019-04-04 13:17:29,556] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6774840e-09 4.6297011e-10 2.8021739e-15 7.6474649e-15 1.0000000e+00
 3.8473464e-11 1.1916817e-15], sum to 1.0000
[2019-04-04 13:17:29,562] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8034
[2019-04-04 13:17:29,569] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.933333333333334, 58.00000000000001, 222.1666666666667, 440.3333333333334, 26.0, 25.42454425480155, 0.4459594345116258, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4285200.0000, 
sim time next is 4285800.0000, 
raw observation next is [6.9, 58.5, 229.0, 385.0, 26.0, 25.42774409428822, 0.4434226385205982, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6537396121883658, 0.585, 0.7633333333333333, 0.425414364640884, 0.6666666666666666, 0.6189786745240182, 0.6478075461735328, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9234809], dtype=float32), -0.052934133]. 
=============================================
[2019-04-04 13:17:29,714] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.06281437e-08 6.95512314e-10 7.59956774e-15 1.00651475e-13
 1.00000000e+00 2.36811348e-10 1.21835627e-14], sum to 1.0000
[2019-04-04 13:17:29,718] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2071
[2019-04-04 13:17:29,727] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.166666666666667, 64.83333333333334, 0.0, 0.0, 26.0, 25.26706143825781, 0.3383991956800572, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4299000.0000, 
sim time next is 4299600.0000, 
raw observation next is [6.133333333333334, 65.66666666666667, 0.0, 0.0, 26.0, 25.20137411506479, 0.3238580596932596, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6325023084025855, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6001145095887326, 0.6079526865644199, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.94185317], dtype=float32), 2.377737]. 
=============================================
[2019-04-04 13:17:41,814] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5069133e-10 6.2429777e-11 1.4837897e-16 1.0735189e-15 1.0000000e+00
 2.5324276e-11 2.2379762e-16], sum to 1.0000
[2019-04-04 13:17:41,815] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7731
[2019-04-04 13:17:41,828] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 45.0, 208.5, 62.5, 26.0, 26.09367519357403, 0.5745092714617956, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4546800.0000, 
sim time next is 4547400.0000, 
raw observation next is [2.833333333333333, 45.5, 190.0, 45.66666666666666, 26.0, 26.27166177368671, 0.5899307356988605, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.541089566020314, 0.455, 0.6333333333333333, 0.05046040515653774, 0.6666666666666666, 0.6893051478072257, 0.6966435785662868, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1569364], dtype=float32), 0.17247997]. 
=============================================
[2019-04-04 13:17:42,890] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.70579961e-09 2.38220249e-10 4.06958515e-16 1.13672076e-14
 1.00000000e+00 5.69758199e-11 1.02558665e-15], sum to 1.0000
[2019-04-04 13:17:42,897] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3466
[2019-04-04 13:17:42,911] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.1, 67.0, 0.0, 0.0, 26.0, 25.33678791036241, 0.4206317347205817, 0.0, 1.0, 68595.50873284691], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4590000.0000, 
sim time next is 4590600.0000, 
raw observation next is [-1.166666666666667, 67.33333333333334, 0.0, 0.0, 26.0, 25.35958942688657, 0.4279041174560718, 0.0, 1.0, 49270.46575742459], 
processed observation next is [1.0, 0.13043478260869565, 0.43028624192059095, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.613299118907214, 0.6426347058186906, 0.0, 1.0, 0.23462126551154566], 
reward next is 0.7654, 
noisyNet noise sample is [array([-1.920356], dtype=float32), -0.67248744]. 
=============================================
[2019-04-04 13:17:43,751] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0580817e-09 7.1223374e-11 6.3323753e-17 9.8525212e-15 1.0000000e+00
 3.3552543e-11 4.9167277e-16], sum to 1.0000
[2019-04-04 13:17:43,753] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4751
[2019-04-04 13:17:43,763] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.59388403288776, 0.4506639810444855, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4687200.0000, 
sim time next is 4687800.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.57942402131499, 0.4374569916150472, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6316186684429157, 0.6458189972050158, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18682152], dtype=float32), -1.4142493]. 
=============================================
[2019-04-04 13:17:48,015] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0919580e-08 1.6009168e-09 2.0116975e-14 2.0977008e-13 1.0000000e+00
 9.0509356e-10 6.9003783e-14], sum to 1.0000
[2019-04-04 13:17:48,019] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6323
[2019-04-04 13:17:48,029] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 35.0, 73.83333333333334, 488.3333333333333, 26.0, 25.21070579468701, 0.4205323611842324, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4812000.0000, 
sim time next is 4812600.0000, 
raw observation next is [3.0, 34.5, 65.66666666666667, 427.6666666666667, 26.0, 25.20335768509565, 0.4078852072919708, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.345, 0.2188888888888889, 0.47255985267034994, 0.6666666666666666, 0.6002798070913041, 0.6359617357639903, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.25393522], dtype=float32), -0.42814004]. 
=============================================
[2019-04-04 13:17:48,827] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.0906236e-10 6.5593149e-11 1.7100536e-17 6.9449745e-16 1.0000000e+00
 2.7352825e-12 1.6130786e-17], sum to 1.0000
[2019-04-04 13:17:48,828] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3029
[2019-04-04 13:17:48,862] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3333333333333334, 94.66666666666666, 10.5, 0.0, 26.0, 25.80559374035487, 0.4845831049174071, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4693200.0000, 
sim time next is 4693800.0000, 
raw observation next is [-0.1666666666666666, 93.33333333333334, 21.0, 0.0, 26.0, 25.81263130902079, 0.4789809694332459, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4579870729455217, 0.9333333333333335, 0.07, 0.0, 0.6666666666666666, 0.6510526090850659, 0.6596603231444153, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4290829], dtype=float32), 0.7246805]. 
=============================================
[2019-04-04 13:17:50,285] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.21864763e-09 7.13961112e-11 1.84029216e-15 1.25920037e-14
 1.00000000e+00 1.05305105e-10 2.22713592e-15], sum to 1.0000
[2019-04-04 13:17:50,289] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8152
[2019-04-04 13:17:50,304] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.666666666666667, 84.33333333333334, 0.0, 0.0, 26.0, 25.44797411807832, 0.5084998112620461, 0.0, 1.0, 59773.83512763988], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4743600.0000, 
sim time next is 4744200.0000, 
raw observation next is [-2.833333333333333, 84.16666666666666, 0.0, 0.0, 26.0, 25.40071387069114, 0.5084918386924403, 0.0, 1.0, 74879.18414594691], 
processed observation next is [1.0, 0.9130434782608695, 0.3841181902123731, 0.8416666666666666, 0.0, 0.0, 0.6666666666666666, 0.6167261558909282, 0.6694972795641467, 0.0, 1.0, 0.35656754355212816], 
reward next is 0.6434, 
noisyNet noise sample is [array([0.19027711], dtype=float32), -2.7113476]. 
=============================================
[2019-04-04 13:17:53,844] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.62938130e-09 6.00096223e-11 3.13695582e-16 2.09363759e-15
 1.00000000e+00 1.23682245e-11 4.93175563e-16], sum to 1.0000
[2019-04-04 13:17:53,848] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2287
[2019-04-04 13:17:53,890] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.666666666666667, 58.66666666666667, 156.5, 678.5, 26.0, 25.47515189107802, 0.4567333648159682, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4789200.0000, 
sim time next is 4789800.0000, 
raw observation next is [-2.5, 55.5, 153.0, 730.0, 26.0, 25.4234042153134, 0.4520164603736592, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.39335180055401664, 0.555, 0.51, 0.8066298342541437, 0.6666666666666666, 0.6186170179427833, 0.6506721534578864, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.5108936], dtype=float32), -1.4855747]. 
=============================================
[2019-04-04 13:17:54,612] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.48214505e-10 9.33950278e-11 6.38947339e-16 1.16008286e-14
 1.00000000e+00 9.21666823e-12 6.21945324e-16], sum to 1.0000
[2019-04-04 13:17:54,612] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5914
[2019-04-04 13:17:54,652] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.5, 55.5, 153.0, 730.0, 26.0, 25.42362596170256, 0.4520589667679203, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4789800.0000, 
sim time next is 4790400.0000, 
raw observation next is [-2.333333333333333, 52.33333333333333, 147.8333333333333, 748.1666666666667, 26.0, 25.37113297783105, 0.4478186357658943, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3979686057248385, 0.5233333333333333, 0.4927777777777776, 0.8267034990791897, 0.6666666666666666, 0.6142610814859207, 0.6492728785886315, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1596653], dtype=float32), 1.4037844]. 
=============================================
[2019-04-04 13:17:55,007] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2899532e-09 8.1728124e-11 1.7580700e-15 2.5788962e-14 1.0000000e+00
 8.1990109e-11 9.5335574e-15], sum to 1.0000
[2019-04-04 13:17:55,007] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3137
[2019-04-04 13:17:55,036] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.7666666666666667, 47.83333333333334, 282.3333333333333, 335.3333333333333, 26.0, 25.05651304333153, 0.3522071540091414, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4881000.0000, 
sim time next is 4881600.0000, 
raw observation next is [1.0, 47.0, 282.0, 349.0, 26.0, 25.05742469704098, 0.3514761495827483, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4903047091412743, 0.47, 0.94, 0.3856353591160221, 0.6666666666666666, 0.5881187247534149, 0.6171587165275828, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1590328], dtype=float32), -0.16570005]. 
=============================================
[2019-04-04 13:17:56,099] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4575965e-08 2.2130799e-09 1.0184326e-15 7.6960932e-14 1.0000000e+00
 1.1152516e-10 2.5098219e-15], sum to 1.0000
[2019-04-04 13:17:56,101] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5774
[2019-04-04 13:17:56,127] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 55.5, 0.0, 0.0, 26.0, 25.28980104778433, 0.3499908693783726, 0.0, 1.0, 39980.64969332508], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5034600.0000, 
sim time next is 5035200.0000, 
raw observation next is [-2.333333333333333, 58.66666666666666, 0.0, 0.0, 26.0, 25.2458863650219, 0.3471602157662015, 0.0, 1.0, 38023.83439897328], 
processed observation next is [1.0, 0.2608695652173913, 0.3979686057248385, 0.5866666666666666, 0.0, 0.0, 0.6666666666666666, 0.603823863751825, 0.6157200719220671, 0.0, 1.0, 0.18106587809034896], 
reward next is 0.8189, 
noisyNet noise sample is [array([-0.13656273], dtype=float32), 0.5992489]. 
=============================================
[2019-04-04 13:17:56,178] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.3399987e-09 5.8710867e-09 3.8319682e-14 1.8591806e-12 1.0000000e+00
 5.0682619e-10 1.0304799e-13], sum to 1.0000
[2019-04-04 13:17:56,179] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2443
[2019-04-04 13:17:56,194] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 24.60254703648043, 0.1814282145117248, 0.0, 1.0, 39491.55340868184], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4861200.0000, 
sim time next is 4861800.0000, 
raw observation next is [-3.5, 62.5, 0.0, 0.0, 26.0, 24.57684250125638, 0.1767932969518152, 0.0, 1.0, 39480.93899097785], 
processed observation next is [0.0, 0.2608695652173913, 0.36565096952908593, 0.625, 0.0, 0.0, 0.6666666666666666, 0.5480702084380317, 0.5589310989839383, 0.0, 1.0, 0.1880044713856088], 
reward next is 0.8120, 
noisyNet noise sample is [array([0.2960692], dtype=float32), 0.9319778]. 
=============================================
[2019-04-04 13:17:59,023] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.8713527e-10 2.8035121e-10 2.6832022e-15 8.2866910e-15 1.0000000e+00
 1.8078709e-11 1.3922501e-15], sum to 1.0000
[2019-04-04 13:17:59,027] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1873
[2019-04-04 13:17:59,059] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.466666666666667, 57.33333333333334, 284.5, 166.5, 26.0, 25.20180872532139, 0.3339613477849896, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4875600.0000, 
sim time next is 4876200.0000, 
raw observation next is [-1.2, 56.0, 300.0, 164.0, 26.0, 25.13760809353214, 0.3281663283309165, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.42936288088642666, 0.56, 1.0, 0.18121546961325966, 0.6666666666666666, 0.5948006744610117, 0.6093887761103055, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6007626], dtype=float32), -0.33203614]. 
=============================================
[2019-04-04 13:18:00,234] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:00,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:00,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run10
[2019-04-04 13:18:00,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:00,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:00,774] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run10
[2019-04-04 13:18:03,006] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:03,006] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:03,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run10
[2019-04-04 13:18:04,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:04,034] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:04,108] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run10
[2019-04-04 13:18:04,250] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7690511e-09 5.6332872e-11 1.8887464e-16 1.0784608e-15 1.0000000e+00
 2.6457231e-11 1.6138858e-16], sum to 1.0000
[2019-04-04 13:18:04,252] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1821
[2019-04-04 13:18:04,278] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.666666666666667, 36.83333333333333, 118.3333333333333, 821.0, 26.0, 27.40084909793282, 0.8198078272866147, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5050200.0000, 
sim time next is 5050800.0000, 
raw observation next is [5.0, 36.0, 119.5, 827.0, 26.0, 27.43367045023824, 0.606739017339665, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6011080332409973, 0.36, 0.3983333333333333, 0.9138121546961326, 0.6666666666666666, 0.78613920418652, 0.7022463391132217, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3099647], dtype=float32), -0.005944591]. 
=============================================
[2019-04-04 13:18:04,325] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.1304211e-10 3.9418076e-11 3.4363333e-16 5.0276661e-15 1.0000000e+00
 2.8844985e-11 9.7835281e-16], sum to 1.0000
[2019-04-04 13:18:04,325] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5036
[2019-04-04 13:18:04,392] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.5, 42.5, 93.0, 560.0, 26.0, 25.23824839185051, 0.3290227067683104, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4955400.0000, 
sim time next is 4956000.0000, 
raw observation next is [-1.333333333333333, 41.33333333333334, 95.5, 586.1666666666666, 26.0, 25.43764119168837, 0.3455645576076664, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.42566943674976926, 0.41333333333333344, 0.31833333333333336, 0.6476979742173112, 0.6666666666666666, 0.6198034326406976, 0.6151881858692222, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09993922], dtype=float32), -0.2531018]. 
=============================================
[2019-04-04 13:18:04,398] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.840225]
 [84.04245 ]
 [83.35436 ]
 [82.505005]
 [81.7346  ]], R is [[85.64017487]
 [85.78377533]
 [85.92594147]
 [86.06668091]
 [86.20601654]].
[2019-04-04 13:18:05,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:05,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:05,412] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run10
[2019-04-04 13:18:05,883] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:05,883] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:05,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run10
[2019-04-04 13:18:06,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:06,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:06,179] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run10
[2019-04-04 13:18:06,921] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0252853e-09 5.6785987e-10 6.4459309e-15 4.2232209e-14 1.0000000e+00
 1.0360311e-10 9.8758511e-15], sum to 1.0000
[2019-04-04 13:18:06,923] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3567
[2019-04-04 13:18:06,938] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 53.33333333333334, 0.0, 0.0, 26.0, 25.33494106080821, 0.3964696768672326, 0.0, 1.0, 48468.82493550383], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5026800.0000, 
sim time next is 5027400.0000, 
raw observation next is [-1.0, 52.5, 0.0, 0.0, 26.0, 25.40492327465294, 0.3955451636528012, 0.0, 1.0, 24037.3431913008], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.525, 0.0, 0.0, 0.6666666666666666, 0.6170769395544117, 0.631848387884267, 0.0, 1.0, 0.11446353900619428], 
reward next is 0.8855, 
noisyNet noise sample is [array([0.38370746], dtype=float32), -0.23534328]. 
=============================================
[2019-04-04 13:18:07,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:07,043] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:07,046] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run10
[2019-04-04 13:18:07,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:07,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:07,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run10
[2019-04-04 13:18:08,027] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:08,027] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:08,034] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run10
[2019-04-04 13:18:08,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:08,554] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:08,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run10
[2019-04-04 13:18:08,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:08,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:09,008] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run10
[2019-04-04 13:18:09,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:09,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:09,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run10
[2019-04-04 13:18:09,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:09,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:09,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run10
[2019-04-04 13:18:09,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:09,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:09,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run10
[2019-04-04 13:18:10,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:18:10,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:18:10,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run10
[2019-04-04 13:18:22,019] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1678010e-09 1.2262083e-10 1.4258461e-14 2.1431489e-14 1.0000000e+00
 3.0648120e-10 2.1835177e-15], sum to 1.0000
[2019-04-04 13:18:22,019] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3500
[2019-04-04 13:18:22,074] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 69.33333333333334, 0.0, 0.0, 26.0, 25.60182955283713, 0.2914165139901616, 1.0, 1.0, 56724.60811626386], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 148200.0000, 
sim time next is 148800.0000, 
raw observation next is [-7.300000000000001, 67.66666666666667, 0.0, 0.0, 26.0, 25.30505279602847, 0.3045183245671437, 1.0, 1.0, 46925.59582536288], 
processed observation next is [1.0, 0.7391304347826086, 0.26038781163434904, 0.6766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6087543996690391, 0.6015061081890479, 1.0, 1.0, 0.2234552182160137], 
reward next is 0.7765, 
noisyNet noise sample is [array([-1.531625], dtype=float32), -0.15154882]. 
=============================================
[2019-04-04 13:18:24,142] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6811363e-08 1.5092094e-09 2.8343035e-14 2.7163463e-13 1.0000000e+00
 8.4245971e-10 3.9352432e-14], sum to 1.0000
[2019-04-04 13:18:24,143] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0589
[2019-04-04 13:18:24,165] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 70.0, 0.0, 0.0, 26.0, 24.40198508068561, 0.1658208529164477, 0.0, 1.0, 45766.39566822183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 164400.0000, 
sim time next is 165000.0000, 
raw observation next is [-8.4, 70.5, 0.0, 0.0, 26.0, 24.34317931302077, 0.1537389125694532, 0.0, 1.0, 45709.14642020054], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.705, 0.0, 0.0, 0.6666666666666666, 0.5285982760850642, 0.5512463041898178, 0.0, 1.0, 0.21766260200095494], 
reward next is 0.7823, 
noisyNet noise sample is [array([-0.8789556], dtype=float32), 1.1392589]. 
=============================================
[2019-04-04 13:18:24,168] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[79.599396]
 [79.69864 ]
 [79.64419 ]
 [79.690445]
 [79.97681 ]], R is [[79.76422119]
 [79.7486496 ]
 [79.73297882]
 [79.71729279]
 [79.70185089]].
[2019-04-04 13:18:25,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.8704531e-09 6.7728539e-10 1.9409351e-15 4.9206543e-14 1.0000000e+00
 1.9389161e-10 6.0247243e-15], sum to 1.0000
[2019-04-04 13:18:25,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9960
[2019-04-04 13:18:25,950] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.8, 71.83333333333334, 93.33333333333333, 12.0, 26.0, 25.35739618364329, 0.2593612016832074, 1.0, 1.0, 38273.78716986112], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 121800.0000, 
sim time next is 122400.0000, 
raw observation next is [-7.8, 74.0, 117.5, 18.0, 26.0, 25.32323701989976, 0.26033048686473, 1.0, 1.0, 41012.64107075182], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.74, 0.39166666666666666, 0.019889502762430938, 0.6666666666666666, 0.6102697516583134, 0.58677682895491, 1.0, 1.0, 0.19529829081310393], 
reward next is 0.8047, 
noisyNet noise sample is [array([-0.9735311], dtype=float32), -0.19300835]. 
=============================================
[2019-04-04 13:18:35,475] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.8157304e-09 4.1608264e-09 7.8556842e-15 2.0382751e-13 1.0000000e+00
 2.1708949e-10 2.7756937e-15], sum to 1.0000
[2019-04-04 13:18:35,475] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3472
[2019-04-04 13:18:35,555] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 43.66666666666667, 86.33333333333333, 625.6666666666666, 26.0, 26.4408458343474, 0.5277378750219769, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 310200.0000, 
sim time next is 310800.0000, 
raw observation next is [-9.5, 43.33333333333334, 84.16666666666667, 624.3333333333334, 26.0, 26.38753277988879, 0.5203776213122777, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4333333333333334, 0.28055555555555556, 0.6898710865561695, 0.6666666666666666, 0.6989610649907325, 0.6734592071040927, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8464096], dtype=float32), -0.5426478]. 
=============================================
[2019-04-04 13:18:43,602] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.2387430e-09 7.9256723e-10 3.5968634e-14 4.8130892e-14 1.0000000e+00
 1.7942750e-10 1.3764401e-14], sum to 1.0000
[2019-04-04 13:18:43,602] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0969
[2019-04-04 13:18:43,623] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.9, 68.66666666666667, 0.0, 0.0, 26.0, 23.79210494936048, 0.005468713811463947, 0.0, 1.0, 47144.93918184697], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 343200.0000, 
sim time next is 343800.0000, 
raw observation next is [-13.9, 68.0, 0.0, 0.0, 26.0, 23.726566132634, -0.009096927247750639, 0.0, 1.0, 47184.86413492465], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.68, 0.0, 0.0, 0.6666666666666666, 0.4772138443861668, 0.4969676909174165, 0.0, 1.0, 0.2246898292139269], 
reward next is 0.7753, 
noisyNet noise sample is [array([1.3693917], dtype=float32), -0.76364523]. 
=============================================
[2019-04-04 13:18:51,414] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9101947e-08 5.9891336e-09 1.5510964e-13 2.0342486e-12 1.0000000e+00
 2.7599492e-09 1.3639701e-12], sum to 1.0000
[2019-04-04 13:18:51,415] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5602
[2019-04-04 13:18:51,429] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.57866737299659, -0.06686251814294941, 0.0, 1.0, 45445.7285762422], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 436200.0000, 
sim time next is 436800.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.58714316024038, -0.07485395677755896, 0.0, 1.0, 45486.30405062284], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 0.6666666666666666, 0.46559526335336504, 0.475048681074147, 0.0, 1.0, 0.21660144786010876], 
reward next is 0.7834, 
noisyNet noise sample is [array([0.4840708], dtype=float32), 0.26434416]. 
=============================================
[2019-04-04 13:18:59,686] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5733946e-09 6.8025884e-11 9.9495395e-17 1.3550744e-14 1.0000000e+00
 6.0026796e-11 8.4903872e-16], sum to 1.0000
[2019-04-04 13:18:59,687] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4742
[2019-04-04 13:18:59,707] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.7, 92.0, 0.0, 0.0, 26.0, 24.85926465284757, 0.234104353733214, 0.0, 1.0, 41107.65469211125], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 511200.0000, 
sim time next is 511800.0000, 
raw observation next is [2.8, 92.66666666666667, 0.0, 0.0, 26.0, 24.85396003806601, 0.2335230979849157, 0.0, 1.0, 41010.82785850688], 
processed observation next is [1.0, 0.9565217391304348, 0.5401662049861496, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5711633365055008, 0.5778410326616386, 0.0, 1.0, 0.19528965646908036], 
reward next is 0.8047, 
noisyNet noise sample is [array([0.03504287], dtype=float32), -1.0720216]. 
=============================================
[2019-04-04 13:18:59,898] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.46785706e-09 2.10806025e-10 1.11020236e-14 7.21802684e-15
 1.00000000e+00 1.42470188e-10 1.92016016e-15], sum to 1.0000
[2019-04-04 13:18:59,898] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4702
[2019-04-04 13:18:59,959] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 24.84160019215528, 0.1820115511921822, 0.0, 1.0, 40399.35101688548], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 501600.0000, 
sim time next is 502200.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 24.73208273418197, 0.1780885070801212, 1.0, 1.0, 100214.3307210234], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5610068945151641, 0.5593628356933737, 1.0, 1.0, 0.47721109867154005], 
reward next is 0.5228, 
noisyNet noise sample is [array([0.14348185], dtype=float32), -3.1462693]. 
=============================================
[2019-04-04 13:19:00,206] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.2768573e-10 7.2287877e-11 3.8209365e-16 4.0812004e-15 1.0000000e+00
 2.5928088e-11 9.0591296e-16], sum to 1.0000
[2019-04-04 13:19:00,206] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3609
[2019-04-04 13:19:00,278] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.25, 91.5, 34.0, 104.0, 26.0, 24.62265587450662, 0.2713420779651825, 0.0, 1.0, 10432.47587726739], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 549000.0000, 
sim time next is 549600.0000, 
raw observation next is [0.1666666666666667, 91.33333333333334, 52.33333333333333, 103.8333333333333, 26.0, 25.00117029848907, 0.2932176403210866, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.4672206832871654, 0.9133333333333334, 0.17444444444444443, 0.11473296500920807, 0.6666666666666666, 0.5834308582074224, 0.5977392134403622, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.124454], dtype=float32), 1.2049067]. 
=============================================
[2019-04-04 13:19:08,215] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.3113326e-10 2.6938340e-10 2.1541145e-15 1.9505778e-14 1.0000000e+00
 6.2067178e-11 6.6711282e-15], sum to 1.0000
[2019-04-04 13:19:08,219] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8982
[2019-04-04 13:19:08,287] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.283333333333334, 64.33333333333334, 92.66666666666667, 25.33333333333334, 26.0, 24.88751849114681, 0.2052409245765505, 0.0, 1.0, 35950.46793970876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 645000.0000, 
sim time next is 645600.0000, 
raw observation next is [-3.166666666666667, 63.66666666666667, 90.83333333333334, 31.66666666666667, 26.0, 24.86615190673492, 0.2046242402807458, 0.0, 1.0, 52295.36203770612], 
processed observation next is [0.0, 0.4782608695652174, 0.3748845798707295, 0.6366666666666667, 0.3027777777777778, 0.03499079189686925, 0.6666666666666666, 0.5721793255612434, 0.5682080800935819, 0.0, 1.0, 0.2490255335128863], 
reward next is 0.7510, 
noisyNet noise sample is [array([3.3657265], dtype=float32), -0.56521076]. 
=============================================
[2019-04-04 13:19:17,253] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5815898e-09 1.7025459e-09 8.5199156e-15 9.5927152e-14 1.0000000e+00
 2.6726871e-10 4.2096616e-14], sum to 1.0000
[2019-04-04 13:19:17,261] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9892
[2019-04-04 13:19:17,275] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.63440496241382, -0.03996504419086674, 0.0, 1.0, 41992.87390941266], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 797400.0000, 
sim time next is 798000.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.60391747283335, -0.04478558415745906, 0.0, 1.0, 42039.13517380296], 
processed observation next is [1.0, 0.21739130434782608, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.4669931227361124, 0.4850714719475136, 0.0, 1.0, 0.2001863579704903], 
reward next is 0.7998, 
noisyNet noise sample is [array([0.49469948], dtype=float32), -0.81870276]. 
=============================================
[2019-04-04 13:19:17,277] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[79.626045]
 [79.69489 ]
 [79.76802 ]
 [79.81364 ]
 [79.8482  ]], R is [[79.55555725]
 [79.56003571]
 [79.56481934]
 [79.57002258]
 [79.57569122]].
[2019-04-04 13:19:17,386] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.5030272e-08 8.2288765e-10 1.0118577e-14 1.3962350e-13 1.0000000e+00
 7.0518613e-10 4.6148805e-14], sum to 1.0000
[2019-04-04 13:19:17,387] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7104
[2019-04-04 13:19:17,409] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.25277695427169, 0.1386421519142427, 0.0, 1.0, 41705.45994597278], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 779400.0000, 
sim time next is 780000.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.25357467374609, 0.1409503233721204, 0.0, 1.0, 41651.74287610393], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5211312228121742, 0.5469834411240401, 0.0, 1.0, 0.19834163274335204], 
reward next is 0.8017, 
noisyNet noise sample is [array([-0.8128814], dtype=float32), 0.7139439]. 
=============================================
[2019-04-04 13:19:17,430] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[78.948494]
 [79.11135 ]
 [79.26636 ]
 [79.62687 ]
 [80.04987 ]], R is [[78.87651825]
 [78.88915253]
 [78.90143585]
 [78.91332245]
 [78.92475891]].
[2019-04-04 13:19:29,932] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4276172e-07 1.7559413e-08 6.1252343e-13 7.1673860e-12 9.9999988e-01
 1.7843308e-09 4.2866039e-13], sum to 1.0000
[2019-04-04 13:19:29,935] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7999
[2019-04-04 13:19:29,940] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.48179615275655, 0.1449880800691613, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1234800.0000, 
sim time next is 1235400.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.46142431935627, 0.1448719681940756, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.45511869327968907, 0.5482906560646919, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03915078], dtype=float32), 0.04479117]. 
=============================================
[2019-04-04 13:19:30,704] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6976727e-10 2.6575679e-11 2.8740443e-17 1.8056126e-16 1.0000000e+00
 4.0690424e-12 4.6585082e-17], sum to 1.0000
[2019-04-04 13:19:30,704] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6016
[2019-04-04 13:19:30,723] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.5, 75.5, 35.66666666666666, 0.0, 26.0, 25.88681025531867, 0.616502037055537, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1008600.0000, 
sim time next is 1009200.0000, 
raw observation next is [15.5, 76.0, 30.33333333333334, 0.0, 26.0, 24.55849054050939, 0.5322568216627603, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.76, 0.10111111111111114, 0.0, 0.6666666666666666, 0.5465408783757825, 0.6774189405542534, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1530366], dtype=float32), -1.6340857]. 
=============================================
[2019-04-04 13:19:31,832] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1052775e-10 2.6857305e-12 2.8756661e-18 1.1487822e-15 1.0000000e+00
 1.3906067e-12 1.5888336e-17], sum to 1.0000
[2019-04-04 13:19:31,840] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9425
[2019-04-04 13:19:31,850] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.58333333333333, 80.5, 0.0, 0.0, 26.0, 25.99987632360288, 0.5776844736780745, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1014600.0000, 
sim time next is 1015200.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 26.09443237362697, 0.5721203169892283, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6745360311355807, 0.6907067723297428, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.439826], dtype=float32), 0.15340364]. 
=============================================
[2019-04-04 13:19:32,059] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0590700e-10 8.5724813e-12 3.2821240e-17 8.9401395e-16 1.0000000e+00
 3.3868481e-12 3.7497192e-17], sum to 1.0000
[2019-04-04 13:19:32,059] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2246
[2019-04-04 13:19:32,102] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.62141232456466, 0.5839096967654812, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1323000.0000, 
sim time next is 1323600.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.84881418865855, 0.5807319121364982, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6540678490548792, 0.6935773040454993, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22431542], dtype=float32), 0.63980925]. 
=============================================
[2019-04-04 13:19:36,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5344995e-08 7.0218804e-09 4.7918382e-14 1.8224177e-12 1.0000000e+00
 2.4717373e-10 9.6594919e-14], sum to 1.0000
[2019-04-04 13:19:36,774] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8153
[2019-04-04 13:19:36,823] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.6, 100.0, 80.83333333333333, 0.0, 26.0, 24.05621985208497, 0.3842206034841805, 0.0, 1.0, 64126.66971990851], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1248000.0000, 
sim time next is 1248600.0000, 
raw observation next is [14.5, 100.0, 83.66666666666667, 0.0, 26.0, 24.45726050401588, 0.425984319782822, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.8642659279778394, 1.0, 0.2788888888888889, 0.0, 0.6666666666666666, 0.5381050420013235, 0.6419947732609407, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9891928], dtype=float32), -0.5187186]. 
=============================================
[2019-04-04 13:19:36,968] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8748446e-07 5.5656894e-08 2.3644997e-12 1.5231180e-10 9.9999976e-01
 7.4522308e-09 3.6941084e-12], sum to 1.0000
[2019-04-04 13:19:36,971] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2543
[2019-04-04 13:19:36,977] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.0, 84.66666666666667, 0.0, 0.0, 26.0, 23.96296553033594, 0.254279542018727, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1217400.0000, 
sim time next is 1218000.0000, 
raw observation next is [15.9, 86.33333333333334, 0.0, 0.0, 26.0, 24.01895756127327, 0.2528394580737626, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9030470914127425, 0.8633333333333334, 0.0, 0.0, 0.6666666666666666, 0.5015797967727726, 0.584279819357921, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.100625], dtype=float32), -1.6275661]. 
=============================================
[2019-04-04 13:19:36,995] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[65.41068 ]
 [65.220535]
 [65.24712 ]
 [64.909195]
 [64.90222 ]], R is [[66.06174469]
 [66.40113068]
 [66.73712158]
 [67.06974792]
 [67.39904785]].
[2019-04-04 13:19:52,852] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.1003659e-10 1.7456668e-11 3.6021257e-16 4.5423607e-15 1.0000000e+00
 5.5552042e-11 7.2168630e-17], sum to 1.0000
[2019-04-04 13:19:52,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2487
[2019-04-04 13:19:52,858] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.3, 51.0, 64.0, 18.5, 26.0, 27.35813853957588, 0.8535157724418685, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1612800.0000, 
sim time next is 1613400.0000, 
raw observation next is [13.11666666666667, 51.5, 59.33333333333333, 24.66666666666667, 26.0, 27.40778151191257, 0.8603176754114424, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8259464450600187, 0.515, 0.19777777777777777, 0.027255985267034995, 0.6666666666666666, 0.783981792659381, 0.7867725584704809, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3467848], dtype=float32), -0.33110377]. 
=============================================
[2019-04-04 13:19:54,908] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5892637e-10 1.4067526e-11 1.6594525e-17 7.5159563e-16 1.0000000e+00
 2.3741170e-12 5.4766375e-18], sum to 1.0000
[2019-04-04 13:19:54,911] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7338
[2019-04-04 13:19:54,928] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.533333333333333, 64.66666666666666, 81.66666666666666, 688.3333333333333, 26.0, 26.49890315135366, 0.7083654259055728, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1518600.0000, 
sim time next is 1519200.0000, 
raw observation next is [10.0, 63.0, 80.0, 682.0, 26.0, 26.63110348157565, 0.7308035579246727, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.739612188365651, 0.63, 0.26666666666666666, 0.7535911602209945, 0.6666666666666666, 0.7192586234646375, 0.7436011859748909, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8313208], dtype=float32), 1.901821]. 
=============================================
[2019-04-04 13:19:56,804] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.23374369e-10 1.17956304e-11 1.66581496e-16 4.19577347e-16
 1.00000000e+00 7.12615592e-11 1.67219431e-16], sum to 1.0000
[2019-04-04 13:19:56,808] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9720
[2019-04-04 13:19:56,817] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 49.0, 165.6666666666667, 0.0, 26.0, 27.23319573690603, 0.8450324422547081, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1605000.0000, 
sim time next is 1605600.0000, 
raw observation next is [13.8, 49.0, 160.5, 0.0, 26.0, 27.30238732481267, 0.8474252605630297, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.844875346260388, 0.49, 0.535, 0.0, 0.6666666666666666, 0.7751989437343892, 0.7824750868543432, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27939048], dtype=float32), 0.8629898]. 
=============================================
[2019-04-04 13:20:12,513] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.6395391e-09 7.3756279e-10 9.3763413e-15 5.9440639e-14 1.0000000e+00
 1.3533001e-10 9.0566804e-15], sum to 1.0000
[2019-04-04 13:20:12,513] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8861
[2019-04-04 13:20:12,565] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.833333333333333, 85.0, 0.0, 0.0, 26.0, 25.03765559486096, 0.2482291332071402, 0.0, 1.0, 38938.4312447023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1878000.0000, 
sim time next is 1878600.0000, 
raw observation next is [-4.916666666666667, 85.5, 0.0, 0.0, 26.0, 25.02548023463168, 0.2454515304681974, 0.0, 1.0, 50271.40692918793], 
processed observation next is [0.0, 0.7391304347826086, 0.32640812557710064, 0.855, 0.0, 0.0, 0.6666666666666666, 0.5854566862193066, 0.5818171768227325, 0.0, 1.0, 0.23938765204375204], 
reward next is 0.7606, 
noisyNet noise sample is [array([0.32506934], dtype=float32), 0.37202117]. 
=============================================
[2019-04-04 13:20:13,063] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.1477137e-09 1.2342685e-10 5.3884484e-15 3.0769359e-14 1.0000000e+00
 1.2148688e-10 1.6608009e-14], sum to 1.0000
[2019-04-04 13:20:13,065] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0267
[2019-04-04 13:20:13,123] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 76.5, 120.0, 51.0, 26.0, 24.87667942574867, 0.2338474410329988, 0.0, 1.0, 75501.81431228008], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1852200.0000, 
sim time next is 1852800.0000, 
raw observation next is [-5.6, 76.0, 130.6666666666667, 56.0, 26.0, 24.91511293131853, 0.2435775619203701, 0.0, 1.0, 42170.77199789142], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.76, 0.4355555555555557, 0.061878453038674036, 0.6666666666666666, 0.5762594109432108, 0.5811925206401234, 0.0, 1.0, 0.20081319998995914], 
reward next is 0.7992, 
noisyNet noise sample is [array([-0.06169102], dtype=float32), -1.5355753]. 
=============================================
[2019-04-04 13:20:21,763] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.69480643e-08 3.09032444e-09 3.18446306e-14 2.25866648e-13
 1.00000000e+00 3.19784976e-09 1.08646436e-13], sum to 1.0000
[2019-04-04 13:20:21,764] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2802
[2019-04-04 13:20:21,826] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.166666666666667, 66.33333333333334, 37.33333333333333, 0.0, 26.0, 25.50084345165071, 0.3160391910111111, 1.0, 1.0, 33520.62331789077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1959600.0000, 
sim time next is 1960200.0000, 
raw observation next is [-3.35, 68.5, 30.0, 0.0, 26.0, 25.53503031850128, 0.3250318906004653, 1.0, 1.0, 34846.72153559775], 
processed observation next is [1.0, 0.6956521739130435, 0.3698060941828255, 0.685, 0.1, 0.0, 0.6666666666666666, 0.6279191932084401, 0.6083439635334884, 1.0, 1.0, 0.16593676921713216], 
reward next is 0.8341, 
noisyNet noise sample is [array([1.311234], dtype=float32), -0.8677716]. 
=============================================
[2019-04-04 13:20:31,216] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5006781e-09 3.6784126e-10 1.3336208e-14 8.5106254e-15 1.0000000e+00
 1.4871489e-10 1.6164497e-15], sum to 1.0000
[2019-04-04 13:20:31,218] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1466
[2019-04-04 13:20:31,240] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.616666666666667, 83.66666666666667, 0.0, 0.0, 26.0, 24.13590745001056, 0.07610600707734703, 0.0, 1.0, 43500.84823615674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2094600.0000, 
sim time next is 2095200.0000, 
raw observation next is [-6.7, 83.0, 0.0, 0.0, 26.0, 24.0624122030224, 0.07261119820174386, 0.0, 1.0, 43563.55716944933], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5052010169185334, 0.5242037327339146, 0.0, 1.0, 0.20744551033071107], 
reward next is 0.7926, 
noisyNet noise sample is [array([-0.09158009], dtype=float32), 0.99516773]. 
=============================================
[2019-04-04 13:20:39,605] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6205288e-09 4.6298071e-10 1.4255382e-15 3.4993103e-14 1.0000000e+00
 4.0538996e-11 1.1024281e-15], sum to 1.0000
[2019-04-04 13:20:39,605] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5094
[2019-04-04 13:20:39,658] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 75.0, 51.16666666666667, 293.5, 26.0, 25.82565141103336, 0.3919443157969573, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2191200.0000, 
sim time next is 2191800.0000, 
raw observation next is [-5.6, 75.0, 61.33333333333333, 325.0, 26.0, 25.90946058328553, 0.403255762432493, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.30747922437673136, 0.75, 0.20444444444444443, 0.35911602209944754, 0.6666666666666666, 0.6591217152737942, 0.6344185874774977, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46124095], dtype=float32), -0.55973625]. 
=============================================
[2019-04-04 13:20:43,087] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2342992e-09 2.2014129e-10 7.6761666e-15 2.1627116e-14 1.0000000e+00
 1.3435911e-10 2.6258998e-15], sum to 1.0000
[2019-04-04 13:20:43,087] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2856
[2019-04-04 13:20:43,122] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.100000000000001, 68.5, 0.0, 0.0, 26.0, 25.28300087140016, 0.4138140058624618, 0.0, 1.0, 45955.38822616472], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2236200.0000, 
sim time next is 2236800.0000, 
raw observation next is [-5.2, 69.0, 0.0, 0.0, 26.0, 25.29225120292898, 0.4128961861914116, 0.0, 1.0, 45578.27459618282], 
processed observation next is [1.0, 0.9130434782608695, 0.31855955678670367, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6076876002440817, 0.6376320620638039, 0.0, 1.0, 0.2170394028389658], 
reward next is 0.7830, 
noisyNet noise sample is [array([2.774452], dtype=float32), 0.16666351]. 
=============================================
[2019-04-04 13:20:43,208] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6362377e-09 2.1666879e-10 3.4539587e-15 1.1982967e-14 1.0000000e+00
 4.3275550e-10 6.2409172e-15], sum to 1.0000
[2019-04-04 13:20:43,208] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0447
[2019-04-04 13:20:43,270] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.733333333333333, 70.33333333333334, 0.0, 0.0, 26.0, 25.24432452775536, 0.3543646827905398, 1.0, 1.0, 32414.26670119195], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2229600.0000, 
sim time next is 2230200.0000, 
raw observation next is [-4.8, 70.5, 0.0, 0.0, 26.0, 25.09527870502282, 0.3455202309998014, 1.0, 1.0, 110779.2662321799], 
processed observation next is [1.0, 0.8260869565217391, 0.3296398891966759, 0.705, 0.0, 0.0, 0.6666666666666666, 0.5912732254185684, 0.6151734103332671, 1.0, 1.0, 0.5275203153913328], 
reward next is 0.4725, 
noisyNet noise sample is [array([0.89858025], dtype=float32), -0.69478285]. 
=============================================
[2019-04-04 13:20:59,323] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2913115e-09 5.5049049e-10 1.2526266e-15 6.6263408e-14 1.0000000e+00
 1.5360367e-10 1.1335079e-14], sum to 1.0000
[2019-04-04 13:20:59,324] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5915
[2019-04-04 13:20:59,335] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 28.0, 171.0, 482.0, 26.0, 25.41834293443686, 0.3428142724160503, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2554200.0000, 
sim time next is 2554800.0000, 
raw observation next is [3.266666666666667, 27.33333333333334, 169.0, 447.5, 26.0, 25.63608393242848, 0.3692025396751665, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5530932594644506, 0.2733333333333334, 0.5633333333333334, 0.494475138121547, 0.6666666666666666, 0.6363403277023734, 0.6230675132250555, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0305685], dtype=float32), -0.025535593]. 
=============================================
[2019-04-04 13:21:04,524] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1022480e-09 1.9850599e-09 6.0137747e-14 9.2330971e-13 1.0000000e+00
 1.9259321e-09 2.0475283e-14], sum to 1.0000
[2019-04-04 13:21:04,524] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9908
[2019-04-04 13:21:04,586] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 26.0, 25.01223537869182, 0.3252990395828078, 0.0, 1.0, 72571.96969759038], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2576400.0000, 
sim time next is 2577000.0000, 
raw observation next is [-1.516666666666667, 42.66666666666667, 0.0, 0.0, 26.0, 24.98104561169529, 0.3284642584726459, 0.0, 1.0, 76036.09413439865], 
processed observation next is [1.0, 0.8260869565217391, 0.4205909510618652, 0.4266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5817538009746075, 0.6094880861575486, 0.0, 1.0, 0.36207663873523166], 
reward next is 0.6379, 
noisyNet noise sample is [array([0.83291036], dtype=float32), 1.1264116]. 
=============================================
[2019-04-04 13:21:04,591] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[74.19976 ]
 [74.23184 ]
 [74.60623 ]
 [75.12095 ]
 [74.927086]], R is [[74.34449768]
 [74.25547028]
 [74.4238205 ]
 [74.67958069]
 [74.83330536]].
[2019-04-04 13:21:05,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1505487e-09 1.6693250e-10 9.4489035e-16 3.7051326e-15 1.0000000e+00
 4.6116493e-11 2.1907669e-16], sum to 1.0000
[2019-04-04 13:21:05,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1290
[2019-04-04 13:21:05,340] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.2, 60.5, 0.0, 0.0, 26.0, 24.91212441618204, 0.2826067804665637, 0.0, 1.0, 41907.53943347617], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2590200.0000, 
sim time next is 2590800.0000, 
raw observation next is [-4.3, 61.0, 0.0, 0.0, 26.0, 24.89250553431097, 0.2745739458620014, 0.0, 1.0, 41888.35314208679], 
processed observation next is [1.0, 1.0, 0.34349030470914127, 0.61, 0.0, 0.0, 0.6666666666666666, 0.5743754611925809, 0.5915246486206671, 0.0, 1.0, 0.19946834829565138], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.11157189], dtype=float32), -1.1318659]. 
=============================================
[2019-04-04 13:21:19,176] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.29903826e-09 2.47874454e-09 5.19094316e-15 1.00169615e-13
 1.00000000e+00 6.62840227e-10 1.31420762e-14], sum to 1.0000
[2019-04-04 13:21:19,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7734
[2019-04-04 13:21:19,185] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.110223024625157e-16, 48.0, 133.1666666666667, 720.5, 26.0, 26.06124552352647, 0.4605177145916466, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2805600.0000, 
sim time next is 2806200.0000, 
raw observation next is [0.5, 47.0, 125.0, 763.0, 26.0, 26.02839973547854, 0.4537229095861594, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4764542936288089, 0.47, 0.4166666666666667, 0.8430939226519337, 0.6666666666666666, 0.6690333112898784, 0.6512409698620532, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8944754], dtype=float32), 1.3917159]. 
=============================================
[2019-04-04 13:21:20,392] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 13:21:20,392] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 13:21:20,393] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 13:21:20,394] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:21:20,394] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:21:20,394] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 13:21:20,397] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:21:20,404] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run14
[2019-04-04 13:21:20,421] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run14
[2019-04-04 13:21:20,436] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run14
[2019-04-04 13:21:46,436] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16484591], dtype=float32), 0.19074647]
[2019-04-04 13:21:46,436] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-5.9, 47.0, 162.0, 460.5, 26.0, 25.00153440083282, 0.2174696974298779, 1.0, 1.0, 32036.32866228218]
[2019-04-04 13:21:46,436] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 13:21:46,437] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.2114281e-09 1.1955008e-09 3.4809707e-14 1.9645917e-13 1.0000000e+00
 7.4997403e-10 5.7788918e-14], sampled 0.39755910536917927
[2019-04-04 13:22:10,134] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16484591], dtype=float32), 0.19074647]
[2019-04-04 13:22:10,134] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.899999999999999, 83.0, 0.0, 0.0, 26.0, 25.4208903995264, 0.4976294576345192, 0.0, 1.0, 42339.29604434007]
[2019-04-04 13:22:10,134] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 13:22:10,134] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.0725781e-10 8.7227427e-11 5.1378086e-16 7.7325062e-15 1.0000000e+00
 3.0465446e-11 8.9987859e-16], sampled 0.25295781103881765
[2019-04-04 13:22:10,150] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16484591], dtype=float32), 0.19074647]
[2019-04-04 13:22:10,150] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.9, 82.0, 0.0, 0.0, 26.0, 25.42004754866697, 0.4958972942879515, 0.0, 1.0, 39828.07567230937]
[2019-04-04 13:22:10,150] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 13:22:10,151] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.3178758e-10 1.1691571e-10 5.4512563e-16 9.7586717e-15 1.0000000e+00
 3.7702990e-11 1.0433440e-15], sampled 0.19528372039829545
[2019-04-04 13:22:56,965] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16484591], dtype=float32), 0.19074647]
[2019-04-04 13:22:56,966] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [19.7, 55.66666666666667, 102.5, 435.5, 26.0, 29.7960895571285, 1.587214958664379, 1.0, 0.0, 0.0]
[2019-04-04 13:22:56,966] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 13:22:56,966] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.2194470e-09 6.4010325e-10 1.1691191e-14 9.6894586e-14 1.0000000e+00
 1.5231474e-10 3.4151274e-14], sampled 0.2056673508317971
[2019-04-04 13:23:02,274] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5576 239912898.3460 1605.1887
[2019-04-04 13:23:10,177] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16484591], dtype=float32), 0.19074647]
[2019-04-04 13:23:10,178] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.365372652, 55.75331799, 121.26053615, 11.00236090333333, 26.0, 25.35045482264897, 0.3397100812341265, 0.0, 1.0, 0.0]
[2019-04-04 13:23:10,178] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 13:23:10,178] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.4883640e-09 6.7043870e-10 7.4713380e-15 7.2678043e-14 1.0000000e+00
 1.6151409e-10 1.1969169e-14], sampled 0.5043211024616655
[2019-04-04 13:23:21,623] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 13:23:24,608] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 13:23:25,632] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1300000, evaluation results [1300000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.557626923972, 239912898.345967, 1605.1886706289615, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 13:23:35,886] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9230473e-09 1.1546024e-09 1.8060931e-14 1.0018242e-13 1.0000000e+00
 1.7993791e-09 2.7227457e-14], sum to 1.0000
[2019-04-04 13:23:35,891] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6981
[2019-04-04 13:23:35,914] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.1666666666666666, 39.5, 94.0, 741.0, 26.0, 25.11724189136687, 0.3637911834564829, 0.0, 1.0, 18702.78260068323], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3077400.0000, 
sim time next is 3078000.0000, 
raw observation next is [0.0, 39.0, 91.5, 724.0, 26.0, 25.1194115537939, 0.3653356299190771, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.39, 0.305, 0.8, 0.6666666666666666, 0.5932842961494916, 0.6217785433063591, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44873095], dtype=float32), 1.1883125]. 
=============================================
[2019-04-04 13:23:35,927] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.86914 ]
 [75.076096]
 [75.27088 ]
 [75.44231 ]
 [75.666916]], R is [[74.85326385]
 [75.01567078]
 [75.17645264]
 [75.33561707]
 [75.49318695]].
[2019-04-04 13:23:44,231] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2390438e-08 4.4607978e-10 9.4442598e-15 8.5105812e-14 1.0000000e+00
 6.7950190e-10 2.3466510e-15], sum to 1.0000
[2019-04-04 13:23:44,232] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2996
[2019-04-04 13:23:44,250] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 50.0, 102.8333333333333, 739.0, 26.0, 26.36657422129582, 0.6727091011165099, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3336000.0000, 
sim time next is 3336600.0000, 
raw observation next is [-3.166666666666667, 50.0, 99.66666666666666, 726.0, 26.0, 26.5163221083867, 0.6920631873310527, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3748845798707295, 0.5, 0.3322222222222222, 0.8022099447513812, 0.6666666666666666, 0.709693509032225, 0.7306877291103508, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7701201], dtype=float32), 0.98695207]. 
=============================================
[2019-04-04 13:23:46,414] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9273605e-10 7.2706819e-12 1.4702111e-16 6.1117350e-16 1.0000000e+00
 2.4786853e-11 1.2896110e-16], sum to 1.0000
[2019-04-04 13:23:46,418] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2775
[2019-04-04 13:23:46,428] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.166666666666667, 98.83333333333334, 0.0, 0.0, 26.0, 25.57084413782835, 0.6148879234403946, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3199800.0000, 
sim time next is 3200400.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 26.0, 25.69015318137225, 0.6134277731418734, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6408460984476875, 0.7044759243806245, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7092936], dtype=float32), -0.23311742]. 
=============================================
[2019-04-04 13:23:49,004] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.5343117e-10 1.9556471e-11 4.2873666e-16 3.3801830e-15 1.0000000e+00
 6.0744916e-11 6.6531281e-17], sum to 1.0000
[2019-04-04 13:23:49,007] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8733
[2019-04-04 13:23:49,015] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 90.33333333333334, 102.6666666666667, 776.1666666666667, 26.0, 26.82653675238075, 0.8238859196633371, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3248400.0000, 
sim time next is 3249000.0000, 
raw observation next is [-3.0, 85.5, 101.0, 769.0, 26.0, 26.89292948876345, 0.8343330492233875, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.855, 0.33666666666666667, 0.8497237569060774, 0.6666666666666666, 0.7410774573969542, 0.7781110164077959, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.07490788], dtype=float32), 1.4624032]. 
=============================================
[2019-04-04 13:23:49,027] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.36493]
 [90.27509]
 [90.20569]
 [89.97316]
 [89.93117]], R is [[90.52685547]
 [90.62158966]
 [90.71537781]
 [90.80822754]
 [90.90014648]].
[2019-04-04 13:23:51,993] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3986162e-08 1.8790074e-10 9.7414879e-15 1.9301498e-13 1.0000000e+00
 6.2695432e-10 8.9052902e-15], sum to 1.0000
[2019-04-04 13:23:51,995] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7597
[2019-04-04 13:23:52,010] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 26.0, 23.91354272282273, 0.09472621997273922, 0.0, 1.0, 43971.53192011512], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3307200.0000, 
sim time next is 3307800.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 26.0, 23.87146102065963, 0.09081514596011715, 0.0, 1.0, 44008.41982521913], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 0.6666666666666666, 0.48928841838830256, 0.530271715320039, 0.0, 1.0, 0.20956390392961488], 
reward next is 0.7904, 
noisyNet noise sample is [array([-0.0881319], dtype=float32), -0.66996837]. 
=============================================
[2019-04-04 13:23:54,467] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9629061e-10 5.3753200e-11 6.9729277e-16 1.2056506e-15 1.0000000e+00
 7.7521184e-11 1.7617691e-15], sum to 1.0000
[2019-04-04 13:23:54,477] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2845
[2019-04-04 13:23:54,490] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 67.0, 44.66666666666667, 382.3333333333334, 26.0, 26.47202072155866, 0.6192043564021303, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3430200.0000, 
sim time next is 3430800.0000, 
raw observation next is [2.0, 67.0, 36.5, 317.0, 26.0, 26.45305383554324, 0.466404224868566, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.12166666666666667, 0.35027624309392263, 0.6666666666666666, 0.7044211529619367, 0.6554680749561886, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.894997], dtype=float32), -0.40855193]. 
=============================================
[2019-04-04 13:23:55,672] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8978848e-09 1.6686152e-10 1.6552436e-15 2.7556499e-14 1.0000000e+00
 1.1794152e-10 2.4732589e-15], sum to 1.0000
[2019-04-04 13:23:55,673] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7882
[2019-04-04 13:23:55,736] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.166666666666667, 60.83333333333333, 30.33333333333333, 212.0, 26.0, 25.57673380009956, 0.3973333422869234, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3397800.0000, 
sim time next is 3398400.0000, 
raw observation next is [-2.0, 60.0, 44.5, 264.5, 26.0, 25.68033075281753, 0.3822061854245928, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.6, 0.14833333333333334, 0.29226519337016577, 0.6666666666666666, 0.6400275627347941, 0.6274020618081976, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6498647], dtype=float32), -0.5863992]. 
=============================================
[2019-04-04 13:24:00,119] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.9005922e-10 2.6606415e-11 4.3347596e-17 6.2301179e-15 1.0000000e+00
 1.1745095e-11 2.3103387e-16], sum to 1.0000
[2019-04-04 13:24:00,119] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7983
[2019-04-04 13:24:00,279] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.37675012052411, 0.4571035082759352, 0.0, 1.0, 79748.78170971693], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3456000.0000, 
sim time next is 3456600.0000, 
raw observation next is [1.0, 84.83333333333334, 0.0, 0.0, 26.0, 25.37626627214895, 0.462833065308156, 0.0, 1.0, 58871.88085223293], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.8483333333333334, 0.0, 0.0, 0.6666666666666666, 0.6146888560124125, 0.654277688436052, 0.0, 1.0, 0.2803422897725377], 
reward next is 0.7197, 
noisyNet noise sample is [array([0.6164584], dtype=float32), -0.5687788]. 
=============================================
[2019-04-04 13:24:05,830] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7374246e-09 6.5292238e-10 7.2586717e-15 5.7625576e-14 1.0000000e+00
 8.5246143e-11 1.9554614e-14], sum to 1.0000
[2019-04-04 13:24:05,833] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3431
[2019-04-04 13:24:05,861] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.333333333333333, 62.33333333333333, 0.0, 0.0, 26.0, 25.75024121367722, 0.4579943019977479, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3703200.0000, 
sim time next is 3703800.0000, 
raw observation next is [2.166666666666667, 62.16666666666667, 0.0, 0.0, 26.0, 25.72351657031236, 0.4454624927086897, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.5226223453370269, 0.6216666666666667, 0.0, 0.0, 0.6666666666666666, 0.6436263808593633, 0.6484874975695633, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39260346], dtype=float32), 0.98184884]. 
=============================================
[2019-04-04 13:24:07,657] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3956795e-08 3.9951570e-10 1.0965385e-13 1.2361820e-13 1.0000000e+00
 3.0068920e-10 1.5591882e-13], sum to 1.0000
[2019-04-04 13:24:07,657] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0451
[2019-04-04 13:24:07,671] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 47.33333333333333, 0.0, 0.0, 26.0, 25.39979535508249, 0.3907677026425795, 0.0, 1.0, 55744.67117318227], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3620400.0000, 
sim time next is 3621000.0000, 
raw observation next is [-1.833333333333333, 48.66666666666667, 0.0, 0.0, 26.0, 25.3904142911624, 0.3934664265723167, 0.0, 1.0, 49351.44854668745], 
processed observation next is [0.0, 0.9130434782608695, 0.41181902123730385, 0.4866666666666667, 0.0, 0.0, 0.6666666666666666, 0.6158678575968667, 0.6311554755241056, 0.0, 1.0, 0.2350068978413688], 
reward next is 0.7650, 
noisyNet noise sample is [array([-0.355589], dtype=float32), -0.6978735]. 
=============================================
[2019-04-04 13:24:07,675] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[72.880974]
 [72.79756 ]
 [72.73714 ]
 [72.65288 ]
 [72.509705]], R is [[72.93223572]
 [72.93746185]
 [72.92327118]
 [72.92183685]
 [72.97085571]].
[2019-04-04 13:24:08,038] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8291135e-08 3.7309147e-09 2.7760883e-13 9.2293469e-13 1.0000000e+00
 4.9442335e-09 1.7700061e-13], sum to 1.0000
[2019-04-04 13:24:08,039] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9492
[2019-04-04 13:24:08,057] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.833333333333334, 25.33333333333334, 0.0, 0.0, 26.0, 25.53838613545853, 0.3650350082989409, 0.0, 1.0, 18746.77944411587], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3651000.0000, 
sim time next is 3651600.0000, 
raw observation next is [9.666666666666668, 25.66666666666667, 0.0, 0.0, 26.0, 25.55138187752233, 0.3612814004584536, 0.0, 1.0, 18745.13287912997], 
processed observation next is [0.0, 0.2608695652173913, 0.7303785780240075, 0.2566666666666667, 0.0, 0.0, 0.6666666666666666, 0.629281823126861, 0.6204271334861512, 0.0, 1.0, 0.08926253751966653], 
reward next is 0.9107, 
noisyNet noise sample is [array([1.8876108], dtype=float32), -1.1473541]. 
=============================================
[2019-04-04 13:24:10,276] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.0352917e-08 3.0862359e-09 7.1173861e-13 1.3536962e-12 9.9999988e-01
 6.8366185e-10 3.1017029e-13], sum to 1.0000
[2019-04-04 13:24:10,278] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6092
[2019-04-04 13:24:10,288] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.666666666666668, 27.66666666666667, 0.0, 0.0, 26.0, 25.46150998892444, 0.3598687347482199, 0.0, 1.0, 28093.68081547945], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3645600.0000, 
sim time next is 3646200.0000, 
raw observation next is [8.833333333333332, 27.33333333333333, 0.0, 0.0, 26.0, 25.474991009566, 0.364500036260835, 0.0, 1.0, 22430.77675451072], 
processed observation next is [0.0, 0.17391304347826086, 0.7072945521698984, 0.27333333333333326, 0.0, 0.0, 0.6666666666666666, 0.6229159174638333, 0.621500012086945, 0.0, 1.0, 0.10681322264052724], 
reward next is 0.8932, 
noisyNet noise sample is [array([-0.4377125], dtype=float32), -0.035229452]. 
=============================================
[2019-04-04 13:24:12,524] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.34487315e-09 4.18856017e-09 1.03798916e-14 1.03965014e-13
 1.00000000e+00 4.80889661e-10 1.81351046e-15], sum to 1.0000
[2019-04-04 13:24:12,525] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9848
[2019-04-04 13:24:12,547] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.35886388068787, 0.3341375041788204, 0.0, 1.0, 52830.14143953903], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3729600.0000, 
sim time next is 3730200.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.37348581207403, 0.3346825019808182, 0.0, 1.0, 46184.96270012982], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6144571510061692, 0.6115608339936061, 0.0, 1.0, 0.219928393810142], 
reward next is 0.7801, 
noisyNet noise sample is [array([-2.224107], dtype=float32), 0.9852085]. 
=============================================
[2019-04-04 13:24:13,464] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5194255e-09 2.8335256e-10 6.1720183e-16 2.6286257e-15 1.0000000e+00
 2.3864136e-10 1.1424236e-15], sum to 1.0000
[2019-04-04 13:24:13,464] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8942
[2019-04-04 13:24:13,482] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 60.0, 117.0, 824.1666666666666, 26.0, 26.52723077096338, 0.645694953224004, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3843600.0000, 
sim time next is 3844200.0000, 
raw observation next is [-1.0, 60.0, 117.0, 826.3333333333334, 26.0, 26.54665611065793, 0.6529605191325154, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.39, 0.9130755064456723, 0.6666666666666666, 0.7122213425548276, 0.7176535063775051, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.94704384], dtype=float32), -0.021395044]. 
=============================================
[2019-04-04 13:24:17,486] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.4263337e-09 1.6970432e-10 3.5533909e-15 3.9393137e-14 1.0000000e+00
 1.0607638e-10 3.7677321e-15], sum to 1.0000
[2019-04-04 13:24:17,486] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3217
[2019-04-04 13:24:17,512] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.86945264252664, 0.2519714741537828, 0.0, 1.0, 43147.03507028923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3824400.0000, 
sim time next is 3825000.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.78271792494645, 0.2434241498254154, 0.0, 1.0, 43077.44810549774], 
processed observation next is [1.0, 0.2608695652173913, 0.32409972299168976, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5652264937455375, 0.5811413832751385, 0.0, 1.0, 0.20513070526427493], 
reward next is 0.7949, 
noisyNet noise sample is [array([0.35655516], dtype=float32), -0.30941606]. 
=============================================
[2019-04-04 13:24:17,528] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[83.641365]
 [83.66769 ]
 [83.69094 ]
 [83.735405]
 [83.83557 ]], R is [[83.57512665]
 [83.53392029]
 [83.49262238]
 [83.45101166]
 [83.40911865]].
[2019-04-04 13:24:19,609] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5195276e-08 1.9881244e-10 7.8780112e-15 9.2142331e-14 1.0000000e+00
 2.7712785e-10 1.2418840e-14], sum to 1.0000
[2019-04-04 13:24:19,611] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8264
[2019-04-04 13:24:19,624] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 59.16666666666666, 0.0, 0.0, 26.0, 25.63900469934908, 0.5745698154277283, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3877800.0000, 
sim time next is 3878400.0000, 
raw observation next is [-1.0, 58.33333333333334, 0.0, 0.0, 26.0, 25.71680273061818, 0.5729681454121531, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.5833333333333335, 0.0, 0.0, 0.6666666666666666, 0.6430668942181818, 0.690989381804051, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6712514], dtype=float32), -1.2585213]. 
=============================================
[2019-04-04 13:24:22,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0246903e-09 5.9026437e-11 1.4600677e-15 3.2341548e-14 1.0000000e+00
 1.2972577e-10 2.9048658e-15], sum to 1.0000
[2019-04-04 13:24:22,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3431
[2019-04-04 13:24:22,588] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.72031822666871, 0.2394694455596455, 0.0, 1.0, 42633.24710775695], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3913200.0000, 
sim time next is 3913800.0000, 
raw observation next is [-7.166666666666667, 63.0, 0.0, 0.0, 26.0, 24.64317809231732, 0.2368435409084782, 0.0, 1.0, 42752.68852485064], 
processed observation next is [1.0, 0.30434782608695654, 0.26408125577100644, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5535981743597768, 0.5789478469694928, 0.0, 1.0, 0.20358423107071733], 
reward next is 0.7964, 
noisyNet noise sample is [array([-0.8083185], dtype=float32), -0.05900489]. 
=============================================
[2019-04-04 13:24:23,803] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6986057e-08 3.4242398e-10 5.7845493e-14 9.5200432e-14 1.0000000e+00
 3.5521261e-10 3.3310628e-14], sum to 1.0000
[2019-04-04 13:24:23,804] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2936
[2019-04-04 13:24:23,822] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.5, 51.0, 0.0, 0.0, 26.0, 25.32948002099668, 0.4202102737403355, 0.0, 1.0, 57004.69734365746], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3969000.0000, 
sim time next is 3969600.0000, 
raw observation next is [-8.666666666666668, 51.66666666666666, 0.0, 0.0, 26.0, 25.25135543997915, 0.4071221902580266, 0.0, 1.0, 50108.5613314442], 
processed observation next is [1.0, 0.9565217391304348, 0.22253000923361033, 0.5166666666666666, 0.0, 0.0, 0.6666666666666666, 0.6042796199982625, 0.6357073967526755, 0.0, 1.0, 0.23861219681640097], 
reward next is 0.7614, 
noisyNet noise sample is [array([-0.5342579], dtype=float32), 0.605292]. 
=============================================
[2019-04-04 13:24:24,420] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9704794e-09 5.6081606e-10 1.7853444e-14 3.1057273e-14 1.0000000e+00
 5.9810712e-10 7.9788454e-15], sum to 1.0000
[2019-04-04 13:24:24,422] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7721
[2019-04-04 13:24:24,452] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 36.0, 66.0, 536.0, 26.0, 26.79077369565869, 0.7025018670531941, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3947400.0000, 
sim time next is 3948000.0000, 
raw observation next is [-4.666666666666666, 36.66666666666666, 58.16666666666666, 474.8333333333334, 26.0, 26.1974488140591, 0.6573040472250782, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.33333333333333337, 0.3666666666666666, 0.19388888888888886, 0.5246777163904237, 0.6666666666666666, 0.683120734504925, 0.719101349075026, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.042714], dtype=float32), -0.29394946]. 
=============================================
[2019-04-04 13:24:24,469] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.83006]
 [77.17873]
 [77.73522]
 [78.14904]
 [78.59866]], R is [[76.49595642]
 [76.73099518]
 [76.96368408]
 [77.19404602]
 [77.42210388]].
[2019-04-04 13:24:38,252] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6635809e-11 1.1226375e-11 3.9396665e-18 8.1575248e-17 1.0000000e+00
 8.6371090e-13 1.5642012e-17], sum to 1.0000
[2019-04-04 13:24:38,253] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8233
[2019-04-04 13:24:38,260] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 251.0, 146.0, 26.0, 26.4555104611279, 0.667948196420097, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4444200.0000, 
sim time next is 4444800.0000, 
raw observation next is [1.0, 86.0, 232.8333333333333, 121.6666666666667, 26.0, 26.48205431656583, 0.665455742052666, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.776111111111111, 0.134438305709024, 0.6666666666666666, 0.7068378597138191, 0.721818580684222, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8120537], dtype=float32), -1.6250039]. 
=============================================
[2019-04-04 13:24:44,534] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5651359e-09 9.6895478e-11 1.9378058e-15 2.7039079e-14 1.0000000e+00
 2.0743953e-11 7.7973120e-16], sum to 1.0000
[2019-04-04 13:24:44,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4705
[2019-04-04 13:24:44,560] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.54253139625071, 0.5193287210558352, 0.0, 1.0, 58234.21786496577], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4428600.0000, 
sim time next is 4429200.0000, 
raw observation next is [2.666666666666667, 72.0, 0.0, 0.0, 26.0, 25.52828751982943, 0.5164742908636005, 0.0, 1.0, 50350.101921195], 
processed observation next is [1.0, 0.2608695652173913, 0.5364727608494922, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6273572933191192, 0.6721580969545334, 0.0, 1.0, 0.23976239010092856], 
reward next is 0.7602, 
noisyNet noise sample is [array([-1.7235959], dtype=float32), -0.4339016]. 
=============================================
[2019-04-04 13:24:54,302] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.4271981e-10 3.4656857e-11 9.0450064e-16 5.6011981e-15 1.0000000e+00
 1.5399265e-10 3.8673648e-15], sum to 1.0000
[2019-04-04 13:24:54,303] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5761
[2019-04-04 13:24:54,316] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 51.33333333333334, 82.0, 54.66666666666667, 26.0, 25.72537188059517, 0.5060612329315378, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4553400.0000, 
sim time next is 4554000.0000, 
raw observation next is [2.0, 52.0, 68.5, 48.0, 26.0, 25.91366781911533, 0.5252840828921194, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.22833333333333333, 0.05303867403314917, 0.6666666666666666, 0.6594723182596107, 0.6750946942973731, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.087661], dtype=float32), 2.7372775]. 
=============================================
[2019-04-04 13:24:54,345] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[80.47051]
 [80.79127]
 [81.11713]
 [81.33261]
 [81.77428]], R is [[80.38589478]
 [80.58203888]
 [80.77622223]
 [80.58018494]
 [80.77438354]].
[2019-04-04 13:24:58,088] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5404855e-09 7.1486865e-11 1.1111085e-15 3.8542278e-15 1.0000000e+00
 2.1796856e-11 1.1097614e-15], sum to 1.0000
[2019-04-04 13:24:58,088] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8421
[2019-04-04 13:24:58,101] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 26.0, 25.89615915357615, 0.5634506275311614, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4659000.0000, 
sim time next is 4659600.0000, 
raw observation next is [2.0, 57.00000000000001, 0.0, 0.0, 26.0, 25.83086068183827, 0.5353871483950335, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.5700000000000001, 0.0, 0.0, 0.6666666666666666, 0.6525717234865226, 0.6784623827983446, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36862963], dtype=float32), 0.13792676]. 
=============================================
[2019-04-04 13:25:00,549] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4609728e-09 4.3247328e-10 1.2216739e-15 1.8380830e-14 1.0000000e+00
 3.7445776e-11 2.5889291e-15], sum to 1.0000
[2019-04-04 13:25:00,551] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7129
[2019-04-04 13:25:00,588] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.733333333333333, 58.66666666666666, 269.0, 169.0, 26.0, 25.26258728893189, 0.3397608854916226, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4875000.0000, 
sim time next is 4875600.0000, 
raw observation next is [-1.466666666666667, 57.33333333333334, 284.5, 166.5, 26.0, 25.19575326297388, 0.3328295107082956, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.42197599261311175, 0.5733333333333335, 0.9483333333333334, 0.1839779005524862, 0.6666666666666666, 0.5996461052478234, 0.6109431702360985, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14645879], dtype=float32), 0.844024]. 
=============================================
[2019-04-04 13:25:10,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:10,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:10,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run11
[2019-04-04 13:25:12,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:12,002] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:12,062] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run11
[2019-04-04 13:25:16,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:16,095] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:16,109] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run11
[2019-04-04 13:25:16,426] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:16,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:16,456] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run11
[2019-04-04 13:25:16,755] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:16,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:16,795] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run11
[2019-04-04 13:25:17,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:17,286] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:17,306] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run11
[2019-04-04 13:25:17,498] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85000, global step 1358456: loss 0.0649
[2019-04-04 13:25:17,500] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85000, global step 1358457: learning rate 0.0000
[2019-04-04 13:25:18,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:18,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:18,418] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run11
[2019-04-04 13:25:18,660] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85000, global step 1358885: loss 0.0622
[2019-04-04 13:25:18,664] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85000, global step 1358886: learning rate 0.0000
[2019-04-04 13:25:19,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:19,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:19,103] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run11
[2019-04-04 13:25:19,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:19,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:19,862] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run11
[2019-04-04 13:25:19,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:19,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:19,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run11
[2019-04-04 13:25:20,221] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.0042125e-10 6.7146982e-11 1.0218806e-15 1.4034594e-14 1.0000000e+00
 1.6004642e-10 4.9263428e-15], sum to 1.0000
[2019-04-04 13:25:20,233] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5359
[2019-04-04 13:25:20,244] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.0, 19.0, 0.0, 0.0, 26.0, 27.48725532643844, 0.9290393457317315, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5083200.0000, 
sim time next is 5083800.0000, 
raw observation next is [9.833333333333334, 19.0, 0.0, 0.0, 26.0, 27.40947547807522, 0.9148735103329558, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7349953831948293, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7841229565062683, 0.804957836777652, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0197568], dtype=float32), -0.36993933]. 
=============================================
[2019-04-04 13:25:21,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:21,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:21,496] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run11
[2019-04-04 13:25:21,838] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:21,838] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:21,842] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run11
[2019-04-04 13:25:21,862] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:21,863] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:21,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:21,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:21,874] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run11
[2019-04-04 13:25:21,870] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run11
[2019-04-04 13:25:21,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:21,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:22,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run11
[2019-04-04 13:25:22,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:25:22,294] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:25:22,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run11
[2019-04-04 13:25:22,929] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85000, global step 1359696: loss 0.3156
[2019-04-04 13:25:22,929] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85000, global step 1359696: learning rate 0.0000
[2019-04-04 13:25:24,010] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85000, global step 1359854: loss 0.1691
[2019-04-04 13:25:24,011] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85000, global step 1359854: learning rate 0.0000
[2019-04-04 13:25:24,399] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85000, global step 1359907: loss 0.0575
[2019-04-04 13:25:24,400] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85000, global step 1359907: learning rate 0.0000
[2019-04-04 13:25:25,138] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85000, global step 1359980: loss 0.0843
[2019-04-04 13:25:25,138] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85000, global step 1359980: learning rate 0.0000
[2019-04-04 13:25:26,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.46431338e-11 3.57438731e-12 2.27888772e-17 1.18929686e-16
 1.00000000e+00 8.56659307e-13 1.08296976e-17], sum to 1.0000
[2019-04-04 13:25:26,607] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9335
[2019-04-04 13:25:26,664] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.9, 90.66666666666667, 92.5, 0.0, 26.0, 24.31302030504014, 0.1092863644306539, 0.0, 1.0, 35410.2030073198], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 44400.0000, 
sim time next is 45000.0000, 
raw observation next is [8.0, 89.5, 96.0, 0.0, 26.0, 24.33680959862573, 0.1155099110610531, 0.0, 1.0, 24220.52529228139], 
processed observation next is [0.0, 0.5217391304347826, 0.6842105263157896, 0.895, 0.32, 0.0, 0.6666666666666666, 0.5280674665521442, 0.5385033036870177, 0.0, 1.0, 0.11533583472514947], 
reward next is 0.8847, 
noisyNet noise sample is [array([-0.43019846], dtype=float32), -0.7155431]. 
=============================================
[2019-04-04 13:25:26,667] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[89.77584 ]
 [89.67451 ]
 [89.51972 ]
 [89.34736 ]
 [89.252205]], R is [[89.79894257]
 [89.73233795]
 [89.64855957]
 [89.58202362]
 [89.57541656]].
[2019-04-04 13:25:26,689] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85000, global step 1360163: loss 0.0470
[2019-04-04 13:25:26,691] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85000, global step 1360163: learning rate 0.0000
[2019-04-04 13:25:27,500] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85000, global step 1360288: loss 0.0554
[2019-04-04 13:25:27,500] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85000, global step 1360288: learning rate 0.0000
[2019-04-04 13:25:28,488] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85000, global step 1360448: loss 0.0480
[2019-04-04 13:25:28,492] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85000, global step 1360448: learning rate 0.0000
[2019-04-04 13:25:28,505] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85000, global step 1360449: loss 0.0590
[2019-04-04 13:25:28,509] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85000, global step 1360449: learning rate 0.0000
[2019-04-04 13:25:30,128] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85000, global step 1360841: loss 0.0598
[2019-04-04 13:25:30,130] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85000, global step 1360841: learning rate 0.0000
[2019-04-04 13:25:30,541] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85000, global step 1361039: loss 0.1745
[2019-04-04 13:25:30,553] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85000, global step 1361039: learning rate 0.0000
[2019-04-04 13:25:30,593] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85000, global step 1361067: loss 0.1627
[2019-04-04 13:25:30,595] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85000, global step 1361067: learning rate 0.0000
[2019-04-04 13:25:30,677] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85000, global step 1361108: loss 0.1000
[2019-04-04 13:25:30,678] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85000, global step 1361108: learning rate 0.0000
[2019-04-04 13:25:30,944] A3C_AGENT_WORKER-Thread-6 INFO:Local step 85000, global step 1361238: loss 0.0643
[2019-04-04 13:25:30,956] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 85000, global step 1361238: learning rate 0.0000
[2019-04-04 13:25:31,411] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85000, global step 1361406: loss 0.0964
[2019-04-04 13:25:31,415] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85000, global step 1361406: learning rate 0.0000
[2019-04-04 13:25:34,688] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3710961e-09 7.0069728e-10 3.5890561e-15 3.1246100e-14 1.0000000e+00
 1.3983667e-10 1.3486636e-15], sum to 1.0000
[2019-04-04 13:25:34,691] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4031
[2019-04-04 13:25:34,760] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.716666666666667, 62.16666666666666, 39.66666666666666, 6.000000000000001, 26.0, 25.38766773413486, 0.2733879571982175, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 118200.0000, 
sim time next is 118800.0000, 
raw observation next is [-7.8, 61.0, 41.0, 4.5, 26.0, 25.39316432982247, 0.2596886414372599, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.61, 0.13666666666666666, 0.004972375690607734, 0.6666666666666666, 0.6160970274852057, 0.5865628804790867, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1588333], dtype=float32), -1.2407138]. 
=============================================
[2019-04-04 13:25:43,226] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85500, global step 1365099: loss 0.2158
[2019-04-04 13:25:43,230] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85500, global step 1365099: learning rate 0.0000
[2019-04-04 13:25:43,504] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85500, global step 1365195: loss 0.2273
[2019-04-04 13:25:43,504] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85500, global step 1365195: learning rate 0.0000
[2019-04-04 13:25:46,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4181802e-08 2.3028328e-09 7.7470921e-15 3.5462312e-13 1.0000000e+00
 3.4923628e-10 1.5566734e-14], sum to 1.0000
[2019-04-04 13:25:46,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8218
[2019-04-04 13:25:46,266] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.8, 69.5, 0.0, 0.0, 26.0, 22.72960943379653, -0.2390867298101089, 0.0, 1.0, 47819.06372161816], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 281400.0000, 
sim time next is 282000.0000, 
raw observation next is [-11.9, 69.0, 0.0, 0.0, 26.0, 22.64955392276489, -0.2524145197734732, 0.0, 1.0, 47829.081406238], 
processed observation next is [1.0, 0.2608695652173913, 0.13296398891966757, 0.69, 0.0, 0.0, 0.6666666666666666, 0.3874628268970743, 0.41586182674217564, 0.0, 1.0, 0.22775753050589526], 
reward next is 0.7722, 
noisyNet noise sample is [array([-1.6842201], dtype=float32), -0.38440007]. 
=============================================
[2019-04-04 13:25:46,291] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.722305]
 [76.90481 ]
 [77.0739  ]
 [77.283455]
 [77.50504 ]], R is [[76.52059937]
 [76.52768707]
 [76.53465271]
 [76.54154205]
 [76.54845428]].
[2019-04-04 13:25:49,785] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85500, global step 1367168: loss 0.2249
[2019-04-04 13:25:49,787] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85500, global step 1367168: learning rate 0.0000
[2019-04-04 13:25:50,172] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85500, global step 1367293: loss 0.2205
[2019-04-04 13:25:50,174] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85500, global step 1367293: learning rate 0.0000
[2019-04-04 13:25:50,931] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85500, global step 1367548: loss 0.2311
[2019-04-04 13:25:50,934] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85500, global step 1367548: learning rate 0.0000
[2019-04-04 13:25:51,571] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85500, global step 1367789: loss 0.2326
[2019-04-04 13:25:51,585] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85500, global step 1367791: learning rate 0.0000
[2019-04-04 13:25:52,153] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3570889e-08 6.7221084e-10 2.4455044e-14 1.3566863e-13 1.0000000e+00
 1.7732436e-09 2.9963116e-14], sum to 1.0000
[2019-04-04 13:25:52,153] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0032
[2019-04-04 13:25:52,173] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-14.58333333333333, 69.0, 0.0, 0.0, 26.0, 23.26869452671967, -0.1139947866018163, 0.0, 1.0, 47726.69404553359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 349800.0000, 
sim time next is 350400.0000, 
raw observation next is [-14.66666666666667, 69.0, 0.0, 0.0, 26.0, 23.25581194638117, -0.1130215196485651, 0.0, 1.0, 47800.39444627356], 
processed observation next is [1.0, 0.043478260869565216, 0.05632502308402576, 0.69, 0.0, 0.0, 0.6666666666666666, 0.43798432886509736, 0.46232616011714495, 0.0, 1.0, 0.227620925934636], 
reward next is 0.7724, 
noisyNet noise sample is [array([-0.13136505], dtype=float32), 0.88090014]. 
=============================================
[2019-04-04 13:25:52,937] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85500, global step 1368246: loss 0.2356
[2019-04-04 13:25:52,939] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85500, global step 1368247: learning rate 0.0000
[2019-04-04 13:25:53,113] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85500, global step 1368309: loss 0.2327
[2019-04-04 13:25:53,114] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85500, global step 1368310: learning rate 0.0000
[2019-04-04 13:25:54,202] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85500, global step 1368627: loss 0.2336
[2019-04-04 13:25:54,203] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85500, global step 1368627: learning rate 0.0000
[2019-04-04 13:25:54,323] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85500, global step 1368657: loss 0.2338
[2019-04-04 13:25:54,323] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85500, global step 1368657: learning rate 0.0000
[2019-04-04 13:25:55,403] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85500, global step 1368959: loss 0.2203
[2019-04-04 13:25:55,404] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85500, global step 1368959: learning rate 0.0000
[2019-04-04 13:25:55,918] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85500, global step 1369121: loss 0.2313
[2019-04-04 13:25:55,918] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85500, global step 1369121: learning rate 0.0000
[2019-04-04 13:25:56,148] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85500, global step 1369186: loss 0.2360
[2019-04-04 13:25:56,148] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85500, global step 1369186: learning rate 0.0000
[2019-04-04 13:25:56,361] A3C_AGENT_WORKER-Thread-6 INFO:Local step 85500, global step 1369237: loss 0.2329
[2019-04-04 13:25:56,361] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 85500, global step 1369237: learning rate 0.0000
[2019-04-04 13:25:56,816] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85500, global step 1369357: loss 0.2374
[2019-04-04 13:25:56,818] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85500, global step 1369358: learning rate 0.0000
[2019-04-04 13:25:56,891] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85500, global step 1369383: loss 0.2362
[2019-04-04 13:25:56,892] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85500, global step 1369383: learning rate 0.0000
[2019-04-04 13:26:06,787] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86000, global step 1372730: loss 1.0429
[2019-04-04 13:26:06,788] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86000, global step 1372730: learning rate 0.0000
[2019-04-04 13:26:06,952] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86000, global step 1372788: loss 0.9915
[2019-04-04 13:26:06,965] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86000, global step 1372792: learning rate 0.0000
[2019-04-04 13:26:07,923] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.0052394e-09 2.8943443e-09 2.4804125e-14 8.6141374e-14 1.0000000e+00
 6.8895428e-10 5.3068135e-14], sum to 1.0000
[2019-04-04 13:26:07,923] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6602
[2019-04-04 13:26:07,967] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.2, 36.33333333333333, 87.66666666666667, 0.0, 26.0, 24.93913043450153, 0.228392008704506, 1.0, 1.0, 24105.975209857], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 484800.0000, 
sim time next is 485400.0000, 
raw observation next is [-0.09999999999999998, 36.66666666666667, 81.33333333333334, 0.0, 26.0, 25.38748392308331, 0.2642778682182311, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4598337950138504, 0.3666666666666667, 0.27111111111111114, 0.0, 0.6666666666666666, 0.6156236602569424, 0.5880926227394104, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.9569473], dtype=float32), 1.4441175]. 
=============================================
[2019-04-04 13:26:10,774] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8966737e-09 4.8264936e-10 8.5875250e-15 6.5588173e-14 1.0000000e+00
 9.8100550e-10 3.3354798e-15], sum to 1.0000
[2019-04-04 13:26:10,774] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1664
[2019-04-04 13:26:10,835] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 24.72669947763993, 0.1784726746039051, 1.0, 1.0, 102069.7086559256], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 502200.0000, 
sim time next is 502800.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 24.66574165840733, 0.2036678312130023, 1.0, 1.0, 108830.212971207], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5554784715339443, 0.5678892770710008, 1.0, 1.0, 0.5182391093867], 
reward next is 0.4818, 
noisyNet noise sample is [array([1.0341352], dtype=float32), 1.1798234]. 
=============================================
[2019-04-04 13:26:12,256] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86000, global step 1374593: loss 0.9938
[2019-04-04 13:26:12,259] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86000, global step 1374594: learning rate 0.0000
[2019-04-04 13:26:12,667] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8451923e-09 3.4726022e-10 2.5170238e-14 1.1671056e-13 1.0000000e+00
 6.9722804e-11 1.6980110e-14], sum to 1.0000
[2019-04-04 13:26:12,668] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1965
[2019-04-04 13:26:12,695] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.65, 71.5, 0.0, 0.0, 26.0, 24.39741353656148, 0.09159105718631388, 0.0, 1.0, 40964.45704765277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 693000.0000, 
sim time next is 693600.0000, 
raw observation next is [-3.566666666666666, 71.66666666666667, 0.0, 0.0, 26.0, 24.36988640497635, 0.09521044285577081, 0.0, 1.0, 40950.16498190151], 
processed observation next is [1.0, 0.0, 0.3638042474607572, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5308238670813624, 0.5317368142852569, 0.0, 1.0, 0.1950007856281024], 
reward next is 0.8050, 
noisyNet noise sample is [array([0.5432768], dtype=float32), 0.07066806]. 
=============================================
[2019-04-04 13:26:13,389] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86000, global step 1375114: loss 0.9586
[2019-04-04 13:26:13,389] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86000, global step 1375114: learning rate 0.0000
[2019-04-04 13:26:13,580] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0421344e-08 6.8931316e-10 3.5837361e-14 2.0564922e-13 1.0000000e+00
 1.8693343e-10 4.4488847e-14], sum to 1.0000
[2019-04-04 13:26:13,580] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5163
[2019-04-04 13:26:13,595] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.39483467358132, 0.1455013658785473, 0.0, 1.0, 42119.84179279876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 609600.0000, 
sim time next is 610200.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.40786177378774, 0.140616570547207, 0.0, 1.0, 42102.35191987358], 
processed observation next is [0.0, 0.043478260869565216, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5339884811489783, 0.5468721901824023, 0.0, 1.0, 0.2004873900946361], 
reward next is 0.7995, 
noisyNet noise sample is [array([0.20198584], dtype=float32), 0.21050026]. 
=============================================
[2019-04-04 13:26:14,101] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8508925e-10 5.8057139e-11 1.6115972e-16 3.3781720e-15 1.0000000e+00
 5.5647136e-12 7.8466951e-16], sum to 1.0000
[2019-04-04 13:26:14,102] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9090
[2019-04-04 13:26:14,120] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 12.0, 37.99999999999999, 26.0, 24.31845484061466, 0.1464202267673994, 0.0, 1.0, 41086.95598592655], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 546600.0000, 
sim time next is 547200.0000, 
raw observation next is [0.5, 92.0, 17.5, 54.5, 26.0, 24.34822484266061, 0.1476437402506418, 0.0, 1.0, 41025.98491334811], 
processed observation next is [0.0, 0.34782608695652173, 0.4764542936288089, 0.92, 0.058333333333333334, 0.06022099447513812, 0.6666666666666666, 0.5290187368883842, 0.5492145800835473, 0.0, 1.0, 0.1953618329207053], 
reward next is 0.8046, 
noisyNet noise sample is [array([-0.8682148], dtype=float32), 0.618299]. 
=============================================
[2019-04-04 13:26:14,268] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86000, global step 1375424: loss 1.0203
[2019-04-04 13:26:14,273] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86000, global step 1375426: learning rate 0.0000
[2019-04-04 13:26:14,746] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86000, global step 1375587: loss 1.0282
[2019-04-04 13:26:14,748] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86000, global step 1375589: learning rate 0.0000
[2019-04-04 13:26:15,503] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86000, global step 1375862: loss 0.9706
[2019-04-04 13:26:15,516] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86000, global step 1375862: learning rate 0.0000
[2019-04-04 13:26:15,989] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86000, global step 1376020: loss 0.9666
[2019-04-04 13:26:15,989] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86000, global step 1376020: learning rate 0.0000
[2019-04-04 13:26:16,950] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6477337e-08 4.4833786e-09 7.8382362e-14 5.4163580e-13 1.0000000e+00
 1.1813430e-09 6.1476689e-14], sum to 1.0000
[2019-04-04 13:26:16,950] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5004
[2019-04-04 13:26:17,014] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.1, 56.5, 18.0, 10.66666666666667, 26.0, 24.8738127618761, 0.2125217635850606, 0.0, 1.0, 51106.98140315043], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 665400.0000, 
sim time next is 666000.0000, 
raw observation next is [-1.2, 57.0, 13.5, 8.5, 26.0, 24.87708899239405, 0.2133208398337729, 0.0, 1.0, 46818.80166719897], 
processed observation next is [0.0, 0.7391304347826086, 0.42936288088642666, 0.57, 0.045, 0.009392265193370166, 0.6666666666666666, 0.5730907493661709, 0.5711069466112576, 0.0, 1.0, 0.22294667460570938], 
reward next is 0.7771, 
noisyNet noise sample is [array([1.4254706], dtype=float32), -0.8611511]. 
=============================================
[2019-04-04 13:26:17,022] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[75.034294]
 [75.003586]
 [75.04416 ]
 [75.184395]
 [75.383255]], R is [[75.09751129]
 [75.10316467]
 [75.10320282]
 [75.12612915]
 [75.19395447]].
[2019-04-04 13:26:17,498] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86000, global step 1376541: loss 1.0923
[2019-04-04 13:26:17,501] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86000, global step 1376543: learning rate 0.0000
[2019-04-04 13:26:17,785] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86000, global step 1376677: loss 0.9332
[2019-04-04 13:26:17,787] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86000, global step 1376678: learning rate 0.0000
[2019-04-04 13:26:18,015] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86000, global step 1376788: loss 1.0498
[2019-04-04 13:26:18,016] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86000, global step 1376788: learning rate 0.0000
[2019-04-04 13:26:18,417] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86000, global step 1376984: loss 0.9440
[2019-04-04 13:26:18,422] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86000, global step 1376987: learning rate 0.0000
[2019-04-04 13:26:18,606] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86000, global step 1377070: loss 1.0515
[2019-04-04 13:26:18,610] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86000, global step 1377070: learning rate 0.0000
[2019-04-04 13:26:18,847] A3C_AGENT_WORKER-Thread-6 INFO:Local step 86000, global step 1377180: loss 0.9448
[2019-04-04 13:26:18,848] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 86000, global step 1377180: learning rate 0.0000
[2019-04-04 13:26:19,388] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86000, global step 1377369: loss 0.9164
[2019-04-04 13:26:19,392] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86000, global step 1377370: learning rate 0.0000
[2019-04-04 13:26:20,048] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86000, global step 1377592: loss 0.9930
[2019-04-04 13:26:20,052] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86000, global step 1377594: learning rate 0.0000
[2019-04-04 13:26:24,728] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1951498e-09 2.3541885e-10 8.5713196e-16 1.2463071e-14 1.0000000e+00
 4.3801685e-11 2.7770176e-15], sum to 1.0000
[2019-04-04 13:26:24,728] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9488
[2019-04-04 13:26:24,744] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.25337850304933, 0.03336365619498886, 0.0, 1.0, 41575.94313130419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 709800.0000, 
sim time next is 710400.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.23041059133402, 0.02952793133498189, 0.0, 1.0, 41605.39250858816], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5192008826111684, 0.5098426437783273, 0.0, 1.0, 0.19812091670756266], 
reward next is 0.8019, 
noisyNet noise sample is [array([0.38085443], dtype=float32), -0.4453387]. 
=============================================
[2019-04-04 13:26:26,766] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.0988922e-08 7.5995588e-10 1.9859865e-14 2.3525241e-13 1.0000000e+00
 2.5897668e-09 2.2414516e-14], sum to 1.0000
[2019-04-04 13:26:26,766] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5174
[2019-04-04 13:26:26,792] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.433333333333334, 52.5, 45.33333333333334, 2.666666666666667, 26.0, 25.87506222912211, 0.4394330974056254, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 751800.0000, 
sim time next is 752400.0000, 
raw observation next is [-2.8, 54.0, 34.0, 2.5, 26.0, 25.97670251704167, 0.4398676313701766, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.38504155124653744, 0.54, 0.11333333333333333, 0.0027624309392265192, 0.6666666666666666, 0.6647252097534725, 0.6466225437900589, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1646521], dtype=float32), -0.9265993]. 
=============================================
[2019-04-04 13:26:27,832] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86500, global step 1380678: loss 0.7212
[2019-04-04 13:26:27,832] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86500, global step 1380678: learning rate 0.0000
[2019-04-04 13:26:27,854] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86500, global step 1380686: loss 0.7062
[2019-04-04 13:26:27,858] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86500, global step 1380686: learning rate 0.0000
[2019-04-04 13:26:32,846] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86500, global step 1382630: loss 0.6856
[2019-04-04 13:26:32,846] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86500, global step 1382630: learning rate 0.0000
[2019-04-04 13:26:34,294] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86500, global step 1383207: loss 0.6856
[2019-04-04 13:26:34,294] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86500, global step 1383207: learning rate 0.0000
[2019-04-04 13:26:35,325] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86500, global step 1383660: loss 0.6633
[2019-04-04 13:26:35,333] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86500, global step 1383661: learning rate 0.0000
[2019-04-04 13:26:35,339] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86500, global step 1383665: loss 0.7129
[2019-04-04 13:26:35,344] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86500, global step 1383666: learning rate 0.0000
[2019-04-04 13:26:36,368] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86500, global step 1384230: loss 0.6029
[2019-04-04 13:26:36,369] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86500, global step 1384230: learning rate 0.0000
[2019-04-04 13:26:36,775] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86500, global step 1384456: loss 0.7200
[2019-04-04 13:26:36,778] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86500, global step 1384456: learning rate 0.0000
[2019-04-04 13:26:36,933] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2744145e-10 9.3616850e-12 5.2273818e-17 6.0422825e-16 1.0000000e+00
 2.2395308e-12 2.0798166e-17], sum to 1.0000
[2019-04-04 13:26:36,934] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4896
[2019-04-04 13:26:36,945] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.333333333333334, 91.33333333333333, 0.0, 0.0, 26.0, 25.31554689842098, 0.4212821767478749, 0.0, 1.0, 38034.28989606615], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 952800.0000, 
sim time next is 953400.0000, 
raw observation next is [5.416666666666667, 90.16666666666667, 0.0, 0.0, 26.0, 25.31662730160779, 0.420612045208689, 0.0, 1.0, 38042.82175014164], 
processed observation next is [1.0, 0.0, 0.6126500461680519, 0.9016666666666667, 0.0, 0.0, 0.6666666666666666, 0.6097189418006493, 0.640204015069563, 0.0, 1.0, 0.1811562940482935], 
reward next is 0.8188, 
noisyNet noise sample is [array([-0.59973645], dtype=float32), -0.60253024]. 
=============================================
[2019-04-04 13:26:38,232] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86500, global step 1385309: loss 0.7035
[2019-04-04 13:26:38,233] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86500, global step 1385309: learning rate 0.0000
[2019-04-04 13:26:38,333] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86500, global step 1385368: loss 0.7299
[2019-04-04 13:26:38,334] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86500, global step 1385368: learning rate 0.0000
[2019-04-04 13:26:38,682] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87000, global step 1385577: loss 2.4244
[2019-04-04 13:26:38,686] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87000, global step 1385577: learning rate 0.0000
[2019-04-04 13:26:38,765] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86500, global step 1385633: loss 0.7062
[2019-04-04 13:26:38,766] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86500, global step 1385633: learning rate 0.0000
[2019-04-04 13:26:38,806] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86500, global step 1385659: loss 0.6879
[2019-04-04 13:26:38,809] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86500, global step 1385659: learning rate 0.0000
[2019-04-04 13:26:39,193] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87000, global step 1385901: loss 2.4172
[2019-04-04 13:26:39,196] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87000, global step 1385901: learning rate 0.0000
[2019-04-04 13:26:39,228] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86500, global step 1385918: loss 0.7968
[2019-04-04 13:26:39,229] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86500, global step 1385918: learning rate 0.0000
[2019-04-04 13:26:39,300] A3C_AGENT_WORKER-Thread-6 INFO:Local step 86500, global step 1385959: loss 0.6838
[2019-04-04 13:26:39,301] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 86500, global step 1385960: learning rate 0.0000
[2019-04-04 13:26:40,205] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86500, global step 1386501: loss 0.7896
[2019-04-04 13:26:40,206] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86500, global step 1386501: learning rate 0.0000
[2019-04-04 13:26:40,252] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86500, global step 1386531: loss 0.7379
[2019-04-04 13:26:40,256] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86500, global step 1386531: learning rate 0.0000
[2019-04-04 13:26:43,959] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9324882e-10 5.5900881e-11 6.2012321e-17 1.5198237e-15 1.0000000e+00
 5.2131173e-12 4.5523998e-17], sum to 1.0000
[2019-04-04 13:26:43,962] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9534
[2019-04-04 13:26:43,972] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.62265872117499, 0.5721060281373798, 0.0, 1.0, 71496.65992993982], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1035600.0000, 
sim time next is 1036200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.59859971486321, 0.5796369947569137, 0.0, 1.0, 57038.99535228965], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6332166429052674, 0.6932123315856379, 0.0, 1.0, 0.27161426358233165], 
reward next is 0.7284, 
noisyNet noise sample is [array([0.36321414], dtype=float32), -0.20642705]. 
=============================================
[2019-04-04 13:26:44,257] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87000, global step 1389068: loss 2.4367
[2019-04-04 13:26:44,259] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87000, global step 1389069: learning rate 0.0000
[2019-04-04 13:26:44,511] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.6676240e-10 1.0122608e-11 3.0878255e-17 4.3922544e-16 1.0000000e+00
 1.2149052e-11 1.3108235e-17], sum to 1.0000
[2019-04-04 13:26:44,518] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5247
[2019-04-04 13:26:44,534] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 76.66666666666667, 0.0, 0.0, 26.0, 25.51204552479179, 0.6040058690199904, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1026600.0000, 
sim time next is 1027200.0000, 
raw observation next is [14.4, 76.33333333333334, 0.0, 0.0, 26.0, 25.73326747945383, 0.6198346169026295, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7633333333333334, 0.0, 0.0, 0.6666666666666666, 0.6444389566211525, 0.7066115389675431, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39665762], dtype=float32), 0.489515]. 
=============================================
[2019-04-04 13:26:44,950] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3223760e-08 3.9108525e-09 4.8250202e-14 2.7809972e-12 1.0000000e+00
 6.2473288e-10 3.1723886e-13], sum to 1.0000
[2019-04-04 13:26:44,953] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4171
[2019-04-04 13:26:44,959] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.46666666666667, 64.33333333333333, 169.0, 0.0, 26.0, 25.09653299689212, 0.5044225644721861, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1168800.0000, 
sim time next is 1169400.0000, 
raw observation next is [18.38333333333333, 64.66666666666667, 167.0, 0.0, 26.0, 25.08941817156336, 0.5022917546321997, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.9718374884579871, 0.6466666666666667, 0.5566666666666666, 0.0, 0.6666666666666666, 0.5907848476302799, 0.6674305848773999, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5419865], dtype=float32), -0.9704193]. 
=============================================
[2019-04-04 13:26:45,449] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87000, global step 1389883: loss 2.3998
[2019-04-04 13:26:45,451] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87000, global step 1389884: learning rate 0.0000
[2019-04-04 13:26:46,577] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87000, global step 1390680: loss 2.3621
[2019-04-04 13:26:46,579] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87000, global step 1390680: learning rate 0.0000
[2019-04-04 13:26:46,643] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87000, global step 1390726: loss 2.3581
[2019-04-04 13:26:46,644] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87000, global step 1390726: learning rate 0.0000
[2019-04-04 13:26:48,047] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87000, global step 1391657: loss 2.3434
[2019-04-04 13:26:48,049] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87000, global step 1391658: learning rate 0.0000
[2019-04-04 13:26:48,380] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7849861e-07 3.3616388e-08 2.4649592e-12 3.4625663e-11 9.9999976e-01
 5.4667990e-09 4.0803185e-12], sum to 1.0000
[2019-04-04 13:26:48,381] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1594
[2019-04-04 13:26:48,385] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.1, 79.0, 0.0, 0.0, 26.0, 24.11484937877451, 0.2809740988652162, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1211400.0000, 
sim time next is 1212000.0000, 
raw observation next is [16.1, 79.33333333333333, 0.0, 0.0, 26.0, 24.12406314642709, 0.2835889398422699, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7933333333333333, 0.0, 0.0, 0.6666666666666666, 0.5103385955355909, 0.59452964661409, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.66836524], dtype=float32), 1.2383015]. 
=============================================
[2019-04-04 13:26:48,399] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[65.77111]
 [65.87758]
 [65.97601]
 [66.28021]
 [66.65132]], R is [[66.1003418 ]
 [66.43933868]
 [66.77494812]
 [67.10720062]
 [67.43612671]].
[2019-04-04 13:26:48,804] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87000, global step 1392172: loss 2.3110
[2019-04-04 13:26:48,807] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87000, global step 1392172: learning rate 0.0000
[2019-04-04 13:26:50,073] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87000, global step 1393023: loss 2.3191
[2019-04-04 13:26:50,075] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87000, global step 1393024: learning rate 0.0000
[2019-04-04 13:26:50,208] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87000, global step 1393113: loss 2.3618
[2019-04-04 13:26:50,209] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87000, global step 1393114: learning rate 0.0000
[2019-04-04 13:26:50,507] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87000, global step 1393313: loss 2.3533
[2019-04-04 13:26:50,508] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87000, global step 1393313: learning rate 0.0000
[2019-04-04 13:26:50,642] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87000, global step 1393398: loss 2.3398
[2019-04-04 13:26:50,644] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87000, global step 1393399: learning rate 0.0000
[2019-04-04 13:26:51,203] A3C_AGENT_WORKER-Thread-6 INFO:Local step 87000, global step 1393775: loss 2.3107
[2019-04-04 13:26:51,204] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 87000, global step 1393776: learning rate 0.0000
[2019-04-04 13:26:51,609] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87000, global step 1394033: loss 2.3214
[2019-04-04 13:26:51,611] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87000, global step 1394033: learning rate 0.0000
[2019-04-04 13:26:52,234] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0382511e-10 9.9103781e-12 9.0938523e-17 2.6509103e-15 1.0000000e+00
 3.1016043e-12 3.6255455e-17], sum to 1.0000
[2019-04-04 13:26:52,234] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2288
[2019-04-04 13:26:52,246] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 115.3333333333333, 0.0, 26.0, 25.99462516570605, 0.5564896745018996, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1340400.0000, 
sim time next is 1341000.0000, 
raw observation next is [1.1, 92.0, 113.0, 0.0, 26.0, 25.91333610437379, 0.5403738563526171, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.37666666666666665, 0.0, 0.6666666666666666, 0.6594446753644826, 0.6801246187842057, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5870681], dtype=float32), 2.4575856]. 
=============================================
[2019-04-04 13:26:52,256] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[93.82113 ]
 [93.951866]
 [94.0295  ]
 [94.0432  ]
 [94.02972 ]], R is [[93.7352066 ]
 [93.79785919]
 [93.85987854]
 [93.92127991]
 [93.98207092]].
[2019-04-04 13:26:52,280] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87000, global step 1394455: loss 2.2584
[2019-04-04 13:26:52,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87000, global step 1394455: learning rate 0.0000
[2019-04-04 13:26:52,329] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87000, global step 1394489: loss 2.3819
[2019-04-04 13:26:52,332] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87000, global step 1394489: learning rate 0.0000
[2019-04-04 13:26:54,312] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87500, global step 1395541: loss 1.3105
[2019-04-04 13:26:54,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87500, global step 1395541: learning rate 0.0000
[2019-04-04 13:26:54,461] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87500, global step 1395621: loss 1.2317
[2019-04-04 13:26:54,463] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87500, global step 1395621: learning rate 0.0000
[2019-04-04 13:26:57,393] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6965868e-10 1.2688944e-11 6.1438264e-17 4.8798055e-16 1.0000000e+00
 2.9157057e-12 3.8411313e-17], sum to 1.0000
[2019-04-04 13:26:57,396] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0336
[2019-04-04 13:26:57,416] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 99.16666666666667, 0.0, 0.0, 26.0, 25.29121213394912, 0.4543449019055543, 0.0, 1.0, 57177.13569841257], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1396200.0000, 
sim time next is 1396800.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.24128194542043, 0.4589012404121981, 0.0, 1.0, 45629.28417774481], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.603440162118369, 0.6529670801373993, 0.0, 1.0, 0.21728230560830863], 
reward next is 0.7827, 
noisyNet noise sample is [array([0.44949356], dtype=float32), 0.5166495]. 
=============================================
[2019-04-04 13:26:57,922] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7857506e-10 8.7298450e-12 9.6867462e-17 2.9262183e-16 1.0000000e+00
 3.3051374e-12 5.8732020e-17], sum to 1.0000
[2019-04-04 13:26:57,922] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4136
[2019-04-04 13:26:57,942] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.30107438444728, 0.5051447625697684, 0.0, 1.0, 41646.07188538922], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1380000.0000, 
sim time next is 1380600.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.28772917968728, 0.5022341149060388, 0.0, 1.0, 41390.27752360418], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6073107649739399, 0.6674113716353464, 0.0, 1.0, 0.19709655963621037], 
reward next is 0.8029, 
noisyNet noise sample is [array([-0.04345739], dtype=float32), 0.13929154]. 
=============================================
[2019-04-04 13:26:59,623] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.75681075e-10 4.85766080e-11 3.72345122e-16 2.85840120e-15
 1.00000000e+00 1.02657556e-10 2.27801094e-16], sum to 1.0000
[2019-04-04 13:26:59,626] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9470
[2019-04-04 13:26:59,650] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 89.0, 0.0, 0.0, 26.0, 25.47081419120911, 0.5593187197143695, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1458000.0000, 
sim time next is 1458600.0000, 
raw observation next is [1.516666666666667, 89.5, 0.0, 0.0, 26.0, 25.54640891784729, 0.5594027017102848, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5046168051708219, 0.895, 0.0, 0.0, 0.6666666666666666, 0.6288674098206076, 0.6864675672367616, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.81499374], dtype=float32), 1.368744]. 
=============================================
[2019-04-04 13:26:59,852] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87500, global step 1398248: loss 1.3018
[2019-04-04 13:26:59,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87500, global step 1398248: learning rate 0.0000
[2019-04-04 13:27:01,504] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87500, global step 1399064: loss 1.1284
[2019-04-04 13:27:01,505] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87500, global step 1399064: learning rate 0.0000
[2019-04-04 13:27:01,541] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8370503e-09 1.3057203e-10 4.6588840e-16 3.2076996e-15 1.0000000e+00
 9.4713690e-12 5.2381439e-16], sum to 1.0000
[2019-04-04 13:27:01,574] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6989
[2019-04-04 13:27:01,580] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.9, 51.0, 77.0, 478.0, 26.0, 26.6460720176742, 0.7563460146234018, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1524600.0000, 
sim time next is 1525200.0000, 
raw observation next is [12.0, 50.66666666666666, 78.66666666666667, 403.0000000000001, 26.0, 26.82906388821437, 0.7680219097496267, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.5066666666666666, 0.26222222222222225, 0.44530386740331507, 0.6666666666666666, 0.7357553240178643, 0.7560073032498756, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4845619], dtype=float32), 0.35159615]. 
=============================================
[2019-04-04 13:27:02,674] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87500, global step 1399610: loss 1.2634
[2019-04-04 13:27:02,675] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87500, global step 1399610: learning rate 0.0000
[2019-04-04 13:27:02,757] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87500, global step 1399646: loss 1.0946
[2019-04-04 13:27:02,761] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87500, global step 1399651: learning rate 0.0000
[2019-04-04 13:27:03,453] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-04 13:27:03,453] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 13:27:03,454] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 13:27:03,454] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:27:03,454] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 13:27:03,455] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:27:03,455] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:27:03,462] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run15
[2019-04-04 13:27:03,487] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run15
[2019-04-04 13:27:03,500] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run15
[2019-04-04 13:28:43,642] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4465 239936225.3969 1605.1790
[2019-04-04 13:29:04,018] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.5526 263433950.7303 1551.9663
[2019-04-04 13:29:08,270] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.9272 275745287.8084 1233.2662
[2019-04-04 13:29:09,294] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1400000, evaluation results [1400000.0, 7241.5526155697635, 263433950.73034763, 1551.9662627209138, 7353.446545728992, 239936225.3969133, 1605.1790028959388, 7182.9272009123515, 275745287.8084067, 1233.2661861620734]
[2019-04-04 13:29:09,696] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87500, global step 1400229: loss 1.1610
[2019-04-04 13:29:09,698] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87500, global step 1400229: learning rate 0.0000
[2019-04-04 13:29:10,412] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87500, global step 1400621: loss 1.2488
[2019-04-04 13:29:10,415] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87500, global step 1400621: learning rate 0.0000
[2019-04-04 13:29:11,650] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87500, global step 1401317: loss 1.1527
[2019-04-04 13:29:11,651] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87500, global step 1401317: learning rate 0.0000
[2019-04-04 13:29:11,776] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87500, global step 1401387: loss 1.1324
[2019-04-04 13:29:11,777] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87500, global step 1401387: learning rate 0.0000
[2019-04-04 13:29:11,977] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87500, global step 1401502: loss 1.1415
[2019-04-04 13:29:11,988] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87500, global step 1401502: learning rate 0.0000
[2019-04-04 13:29:12,315] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87500, global step 1401688: loss 1.0751
[2019-04-04 13:29:12,318] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87500, global step 1401688: learning rate 0.0000
[2019-04-04 13:29:13,089] A3C_AGENT_WORKER-Thread-6 INFO:Local step 87500, global step 1402102: loss 1.2128
[2019-04-04 13:29:13,091] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 87500, global step 1402102: learning rate 0.0000
[2019-04-04 13:29:13,241] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87500, global step 1402180: loss 1.1598
[2019-04-04 13:29:13,242] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87500, global step 1402181: learning rate 0.0000
[2019-04-04 13:29:13,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3069842e-09 9.8195223e-11 1.2882645e-15 3.9326600e-15 1.0000000e+00
 5.3420615e-11 2.1438990e-16], sum to 1.0000
[2019-04-04 13:29:13,611] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0675
[2019-04-04 13:29:13,628] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 84.0, 95.0, 0.0, 26.0, 25.86053518993326, 0.5222310383418596, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1684800.0000, 
sim time next is 1685400.0000, 
raw observation next is [1.1, 84.66666666666667, 99.0, 0.0, 26.0, 25.85255911825676, 0.5212899245428887, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.8466666666666667, 0.33, 0.0, 0.6666666666666666, 0.6543799265213966, 0.673763308180963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30571634], dtype=float32), 0.05110622]. 
=============================================
[2019-04-04 13:29:13,891] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87500, global step 1402562: loss 1.1718
[2019-04-04 13:29:13,892] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87500, global step 1402562: learning rate 0.0000
[2019-04-04 13:29:14,078] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87500, global step 1402674: loss 1.0967
[2019-04-04 13:29:14,079] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87500, global step 1402674: learning rate 0.0000
[2019-04-04 13:29:14,111] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.8451773e-10 7.0678511e-12 1.1877786e-15 3.2878291e-15 1.0000000e+00
 7.6687184e-11 2.8599702e-16], sum to 1.0000
[2019-04-04 13:29:14,114] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5223
[2019-04-04 13:29:14,123] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.71666666666667, 49.33333333333334, 100.3333333333333, 0.0, 26.0, 26.48395102618052, 0.7601817841600735, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1609800.0000, 
sim time next is 1610400.0000, 
raw observation next is [13.63333333333333, 49.66666666666667, 89.16666666666666, 0.0, 26.0, 26.8036329013938, 0.7932147707214048, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.840258541089566, 0.4966666666666667, 0.29722222222222217, 0.0, 0.6666666666666666, 0.73363607511615, 0.7644049235738016, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9222514], dtype=float32), 0.2796363]. 
=============================================
[2019-04-04 13:29:16,491] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88000, global step 1404020: loss 0.3386
[2019-04-04 13:29:16,494] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88000, global step 1404022: learning rate 0.0000
[2019-04-04 13:29:16,687] A3C_AGENT_WORKER-Thread-13 INFO:Local step 88000, global step 1404115: loss 0.3834
[2019-04-04 13:29:16,689] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 88000, global step 1404115: learning rate 0.0000
[2019-04-04 13:29:20,133] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5473609e-10 4.4181613e-11 5.6119718e-17 3.6524243e-15 1.0000000e+00
 6.1889972e-11 5.9051055e-16], sum to 1.0000
[2019-04-04 13:29:20,133] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7577
[2019-04-04 13:29:20,210] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.20176843624563, 0.3300356958084243, 1.0, 1.0, 22362.61208886165], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1708200.0000, 
sim time next is 1708800.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 24.37712295226944, 0.3324548630239639, 1.0, 1.0, 196524.3309263374], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5314269126891199, 0.6108182876746546, 1.0, 1.0, 0.9358301472682733], 
reward next is 0.0642, 
noisyNet noise sample is [array([-1.6660026], dtype=float32), -0.6563662]. 
=============================================
[2019-04-04 13:29:21,568] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88000, global step 1406313: loss 0.3571
[2019-04-04 13:29:21,569] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88000, global step 1406313: learning rate 0.0000
[2019-04-04 13:29:23,324] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88000, global step 1407074: loss 0.3755
[2019-04-04 13:29:23,324] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88000, global step 1407074: learning rate 0.0000
[2019-04-04 13:29:24,575] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88000, global step 1407549: loss 0.3770
[2019-04-04 13:29:24,575] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88000, global step 1407549: learning rate 0.0000
[2019-04-04 13:29:24,766] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88000, global step 1407615: loss 0.3769
[2019-04-04 13:29:24,767] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88000, global step 1407615: learning rate 0.0000
[2019-04-04 13:29:26,229] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88000, global step 1408070: loss 0.3796
[2019-04-04 13:29:26,231] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88000, global step 1408070: learning rate 0.0000
[2019-04-04 13:29:26,532] A3C_AGENT_WORKER-Thread-11 INFO:Local step 88000, global step 1408167: loss 0.3731
[2019-04-04 13:29:26,545] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 88000, global step 1408174: learning rate 0.0000
[2019-04-04 13:29:28,045] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88000, global step 1408676: loss 0.3784
[2019-04-04 13:29:28,046] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88000, global step 1408676: learning rate 0.0000
[2019-04-04 13:29:28,291] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88000, global step 1408767: loss 0.3990
[2019-04-04 13:29:28,291] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88000, global step 1408767: learning rate 0.0000
[2019-04-04 13:29:28,378] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88000, global step 1408797: loss 0.4121
[2019-04-04 13:29:28,379] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88000, global step 1408797: learning rate 0.0000
[2019-04-04 13:29:28,805] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88000, global step 1408976: loss 0.3877
[2019-04-04 13:29:28,807] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88000, global step 1408978: learning rate 0.0000
[2019-04-04 13:29:29,474] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88000, global step 1409282: loss 0.4183
[2019-04-04 13:29:29,476] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88000, global step 1409282: learning rate 0.0000
[2019-04-04 13:29:29,606] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.6840037e-09 2.4940994e-10 5.2233435e-14 5.9451183e-14 1.0000000e+00
 8.5379905e-11 5.0827021e-15], sum to 1.0000
[2019-04-04 13:29:29,606] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4821
[2019-04-04 13:29:29,636] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 80.5, 0.0, 0.0, 26.0, 24.62119991445099, 0.2120684884942892, 0.0, 1.0, 45483.42450399778], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1812600.0000, 
sim time next is 1813200.0000, 
raw observation next is [-5.0, 80.0, 0.0, 0.0, 26.0, 24.59067457788466, 0.2054774170117566, 0.0, 1.0, 45486.32118720991], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.8, 0.0, 0.0, 0.6666666666666666, 0.5492228814903882, 0.5684924723372522, 0.0, 1.0, 0.21660152946290434], 
reward next is 0.7834, 
noisyNet noise sample is [array([-1.0937401], dtype=float32), 0.6751579]. 
=============================================
[2019-04-04 13:29:29,692] A3C_AGENT_WORKER-Thread-6 INFO:Local step 88000, global step 1409375: loss 0.3938
[2019-04-04 13:29:29,697] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 88000, global step 1409376: learning rate 0.0000
[2019-04-04 13:29:29,917] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5544611e-09 1.7436479e-10 5.7095124e-16 2.3968185e-14 1.0000000e+00
 7.2982377e-11 3.4722406e-16], sum to 1.0000
[2019-04-04 13:29:29,918] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6285
[2019-04-04 13:29:29,962] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.016666666666667, 78.33333333333334, 142.3333333333333, 340.3333333333333, 26.0, 25.71693069978127, 0.3307760807183928, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1937400.0000, 
sim time next is 1938000.0000, 
raw observation next is [-6.733333333333333, 77.66666666666667, 156.6666666666667, 288.1666666666667, 26.0, 25.78419916269839, 0.338331079983027, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2760849492151431, 0.7766666666666667, 0.5222222222222224, 0.3184162062615101, 0.6666666666666666, 0.648683263558199, 0.612777026661009, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.58768034], dtype=float32), -0.013107675]. 
=============================================
[2019-04-04 13:29:29,970] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[85.89789]
 [86.15261]
 [86.17192]
 [86.27055]
 [85.90294]], R is [[85.75884247]
 [85.90125275]
 [85.93428802]
 [85.97198486]
 [86.0202179 ]].
[2019-04-04 13:29:30,049] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88000, global step 1409525: loss 0.4123
[2019-04-04 13:29:30,049] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88000, global step 1409525: learning rate 0.0000
[2019-04-04 13:29:30,154] A3C_AGENT_WORKER-Thread-12 INFO:Local step 88000, global step 1409573: loss 0.4234
[2019-04-04 13:29:30,160] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 88000, global step 1409573: learning rate 0.0000
[2019-04-04 13:29:34,991] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.2367907e-10 1.3266441e-10 4.6245530e-15 3.5315819e-14 1.0000000e+00
 1.1045904e-10 2.1350613e-15], sum to 1.0000
[2019-04-04 13:29:34,992] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7588
[2019-04-04 13:29:35,051] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 25.008584373802, 0.3376473495414554, 1.0, 1.0, 141966.6324934565], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2058600.0000, 
sim time next is 2059200.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.91251592992491, 0.3660793151746995, 0.0, 1.0, 152091.1017079971], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5760429941604093, 0.6220264383915665, 0.0, 1.0, 0.7242433414666528], 
reward next is 0.2758, 
noisyNet noise sample is [array([1.9311097], dtype=float32), -1.0200373]. 
=============================================
[2019-04-04 13:29:39,248] A3C_AGENT_WORKER-Thread-13 INFO:Local step 88500, global step 1412719: loss 0.6855
[2019-04-04 13:29:39,251] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 88500, global step 1412720: learning rate 0.0000
[2019-04-04 13:29:39,645] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88500, global step 1412848: loss 0.6493
[2019-04-04 13:29:39,646] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88500, global step 1412848: learning rate 0.0000
[2019-04-04 13:29:42,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.1106948e-10 2.8291405e-10 5.8901221e-16 8.2505117e-15 1.0000000e+00
 2.6725226e-11 3.2363729e-16], sum to 1.0000
[2019-04-04 13:29:42,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6413
[2019-04-04 13:29:42,532] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.066666666666666, 86.33333333333333, 35.66666666666666, 0.0, 26.0, 25.63110116318249, 0.3137345031113873, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2018400.0000, 
sim time next is 2019000.0000, 
raw observation next is [-6.033333333333333, 86.16666666666667, 42.33333333333333, 0.0, 26.0, 25.71106098851503, 0.3121540834923967, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.29547553093259465, 0.8616666666666667, 0.1411111111111111, 0.0, 0.6666666666666666, 0.6425884157095858, 0.6040513611641322, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.73057306], dtype=float32), 0.451668]. 
=============================================
[2019-04-04 13:29:42,542] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[85.69434 ]
 [85.568474]
 [85.47166 ]
 [85.35726 ]
 [84.9977  ]], R is [[85.69029236]
 [85.83338928]
 [85.97505951]
 [86.11531067]
 [86.25415802]].
[2019-04-04 13:29:42,600] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3952609e-09 5.4579830e-10 1.4132527e-15 4.7592840e-14 1.0000000e+00
 9.1672897e-10 7.5686409e-15], sum to 1.0000
[2019-04-04 13:29:42,602] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3338
[2019-04-04 13:29:42,627] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.9, 85.0, 0.0, 0.0, 26.0, 24.36717775571808, 0.1432405141373864, 0.0, 1.0, 41756.16535666296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1992600.0000, 
sim time next is 1993200.0000, 
raw observation next is [-5.8, 84.33333333333333, 0.0, 0.0, 26.0, 24.32223148305135, 0.1378532051328752, 0.0, 1.0, 41740.81186510764], 
processed observation next is [1.0, 0.043478260869565216, 0.30193905817174516, 0.8433333333333333, 0.0, 0.0, 0.6666666666666666, 0.5268526235876125, 0.5459510683776251, 0.0, 1.0, 0.19876577078622684], 
reward next is 0.8012, 
noisyNet noise sample is [array([-1.2612417], dtype=float32), 1.241547]. 
=============================================
[2019-04-04 13:29:44,249] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88500, global step 1414518: loss 0.6145
[2019-04-04 13:29:44,252] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88500, global step 1414518: learning rate 0.0000
[2019-04-04 13:29:46,723] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88500, global step 1415237: loss 0.5503
[2019-04-04 13:29:46,724] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88500, global step 1415237: learning rate 0.0000
[2019-04-04 13:29:47,681] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88500, global step 1415569: loss 0.5722
[2019-04-04 13:29:47,681] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88500, global step 1415569: learning rate 0.0000
[2019-04-04 13:29:47,911] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88500, global step 1415653: loss 0.6114
[2019-04-04 13:29:47,913] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88500, global step 1415657: learning rate 0.0000
[2019-04-04 13:29:49,224] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88500, global step 1416097: loss 0.5834
[2019-04-04 13:29:49,224] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88500, global step 1416097: learning rate 0.0000
[2019-04-04 13:29:49,538] A3C_AGENT_WORKER-Thread-11 INFO:Local step 88500, global step 1416219: loss 0.6419
[2019-04-04 13:29:49,542] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 88500, global step 1416219: learning rate 0.0000
[2019-04-04 13:29:50,896] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88500, global step 1416762: loss 0.6305
[2019-04-04 13:29:50,897] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88500, global step 1416762: learning rate 0.0000
[2019-04-04 13:29:50,961] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88500, global step 1416783: loss 0.7162
[2019-04-04 13:29:50,962] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88500, global step 1416783: learning rate 0.0000
[2019-04-04 13:29:50,974] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88500, global step 1416783: loss 0.6114
[2019-04-04 13:29:50,975] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88500, global step 1416783: learning rate 0.0000
[2019-04-04 13:29:51,007] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88500, global step 1416795: loss 0.5847
[2019-04-04 13:29:51,008] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88500, global step 1416795: learning rate 0.0000
[2019-04-04 13:29:52,642] A3C_AGENT_WORKER-Thread-6 INFO:Local step 88500, global step 1417305: loss 0.6065
[2019-04-04 13:29:52,643] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 88500, global step 1417305: learning rate 0.0000
[2019-04-04 13:29:52,692] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88500, global step 1417316: loss 0.6122
[2019-04-04 13:29:52,693] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88500, global step 1417316: learning rate 0.0000
[2019-04-04 13:29:52,970] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88500, global step 1417405: loss 0.6514
[2019-04-04 13:29:52,970] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88500, global step 1417405: learning rate 0.0000
[2019-04-04 13:29:53,929] A3C_AGENT_WORKER-Thread-12 INFO:Local step 88500, global step 1417745: loss 0.6177
[2019-04-04 13:29:53,930] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 88500, global step 1417745: learning rate 0.0000
[2019-04-04 13:30:01,302] A3C_AGENT_WORKER-Thread-13 INFO:Local step 89000, global step 1420395: loss 0.2979
[2019-04-04 13:30:01,303] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 89000, global step 1420395: learning rate 0.0000
[2019-04-04 13:30:01,650] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89000, global step 1420570: loss 0.3098
[2019-04-04 13:30:01,661] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89000, global step 1420573: learning rate 0.0000
[2019-04-04 13:30:05,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5704323e-08 2.0819604e-09 3.5164722e-14 2.1006638e-13 1.0000000e+00
 6.8721273e-10 5.9860347e-14], sum to 1.0000
[2019-04-04 13:30:05,577] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7560
[2019-04-04 13:30:05,592] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.1059921589539, 0.2782895726696759, 0.0, 1.0, 43114.71098338318], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2408400.0000, 
sim time next is 2409000.0000, 
raw observation next is [-3.583333333333333, 42.33333333333334, 0.0, 0.0, 26.0, 25.15300857587875, 0.2736350270698616, 0.0, 1.0, 43056.96234746063], 
processed observation next is [0.0, 0.9130434782608695, 0.36334256694367506, 0.42333333333333345, 0.0, 0.0, 0.6666666666666666, 0.5960840479898959, 0.5912116756899538, 0.0, 1.0, 0.2050331540355268], 
reward next is 0.7950, 
noisyNet noise sample is [array([0.31014976], dtype=float32), -1.838502]. 
=============================================
[2019-04-04 13:30:05,598] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[71.928276]
 [71.96237 ]
 [72.051216]
 [72.046455]
 [72.011856]], R is [[71.94754791]
 [72.02276611]
 [72.09703827]
 [72.17056274]
 [72.24355316]].
[2019-04-04 13:30:06,173] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0855220e-08 8.8836192e-09 9.9488202e-13 2.0025349e-12 1.0000000e+00
 2.3434645e-09 4.1256202e-13], sum to 1.0000
[2019-04-04 13:30:06,174] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9900
[2019-04-04 13:30:06,191] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.7833333333333333, 30.33333333333334, 0.0, 0.0, 26.0, 25.28946654023552, 0.2866569717867968, 0.0, 1.0, 40744.81278848728], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2491800.0000, 
sim time next is 2492400.0000, 
raw observation next is [-0.8666666666666667, 31.66666666666667, 0.0, 0.0, 26.0, 25.30695946473351, 0.2863798082047632, 0.0, 1.0, 40354.34948402391], 
processed observation next is [0.0, 0.8695652173913043, 0.4385964912280702, 0.3166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6089132887277925, 0.5954599360682544, 0.0, 1.0, 0.1921635689715424], 
reward next is 0.8078, 
noisyNet noise sample is [array([0.26971507], dtype=float32), -1.0728441]. 
=============================================
[2019-04-04 13:30:06,250] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89000, global step 1422303: loss 0.2928
[2019-04-04 13:30:06,254] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89000, global step 1422306: learning rate 0.0000
[2019-04-04 13:30:06,816] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0015631e-08 3.4137340e-10 9.1542137e-15 7.3668536e-14 1.0000000e+00
 2.9047512e-10 1.2481197e-14], sum to 1.0000
[2019-04-04 13:30:06,817] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8715
[2019-04-04 13:30:06,860] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.09999999999999998, 52.33333333333334, 180.6666666666667, 0.0, 26.0, 24.93247105999506, 0.3010788921108439, 0.0, 1.0, 32071.85956560613], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2382600.0000, 
sim time next is 2383200.0000, 
raw observation next is [0.0, 52.0, 175.5, 0.0, 26.0, 24.94660948534303, 0.3057619293905147, 0.0, 1.0, 19947.55494375768], 
processed observation next is [0.0, 0.6086956521739131, 0.46260387811634357, 0.52, 0.585, 0.0, 0.6666666666666666, 0.5788841237785857, 0.6019206431301716, 0.0, 1.0, 0.09498835687503658], 
reward next is 0.9050, 
noisyNet noise sample is [array([-0.08303664], dtype=float32), -0.5137939]. 
=============================================
[2019-04-04 13:30:09,042] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89000, global step 1423500: loss 0.2964
[2019-04-04 13:30:09,043] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89000, global step 1423500: learning rate 0.0000
[2019-04-04 13:30:09,277] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89000, global step 1423613: loss 0.2744
[2019-04-04 13:30:09,290] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89000, global step 1423617: learning rate 0.0000
[2019-04-04 13:30:09,655] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89000, global step 1423782: loss 0.2767
[2019-04-04 13:30:09,656] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89000, global step 1423782: learning rate 0.0000
[2019-04-04 13:30:10,100] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.5686765e-08 1.8996831e-08 4.4974515e-12 1.1168729e-11 9.9999988e-01
 1.4115057e-09 9.9655885e-13], sum to 1.0000
[2019-04-04 13:30:10,103] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5591
[2019-04-04 13:30:10,122] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.100000000000001, 60.33333333333334, 0.0, 0.0, 26.0, 23.15246282988277, -0.1725401289231918, 0.0, 1.0, 44042.25735335058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2442000.0000, 
sim time next is 2442600.0000, 
raw observation next is [-9.2, 60.5, 0.0, 0.0, 26.0, 23.11100956727515, -0.1809825719091684, 0.0, 1.0, 44035.53386399274], 
processed observation next is [0.0, 0.2608695652173913, 0.20775623268698065, 0.605, 0.0, 0.0, 0.6666666666666666, 0.4259174639395959, 0.4396724760302772, 0.0, 1.0, 0.20969301839996543], 
reward next is 0.7903, 
noisyNet noise sample is [array([0.25716352], dtype=float32), 1.3670382]. 
=============================================
[2019-04-04 13:30:10,856] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89000, global step 1424206: loss 0.2487
[2019-04-04 13:30:10,858] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89000, global step 1424206: learning rate 0.0000
[2019-04-04 13:30:11,631] A3C_AGENT_WORKER-Thread-11 INFO:Local step 89000, global step 1424493: loss 0.2771
[2019-04-04 13:30:11,637] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 89000, global step 1424493: learning rate 0.0000
[2019-04-04 13:30:12,624] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89000, global step 1424931: loss 0.2763
[2019-04-04 13:30:12,624] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89000, global step 1424931: learning rate 0.0000
[2019-04-04 13:30:12,733] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89000, global step 1424970: loss 0.2590
[2019-04-04 13:30:12,734] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89000, global step 1424970: learning rate 0.0000
[2019-04-04 13:30:12,909] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89000, global step 1425034: loss 0.2654
[2019-04-04 13:30:12,909] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89000, global step 1425034: learning rate 0.0000
[2019-04-04 13:30:12,921] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89000, global step 1425040: loss 0.2537
[2019-04-04 13:30:12,928] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89000, global step 1425040: learning rate 0.0000
[2019-04-04 13:30:13,863] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89000, global step 1425451: loss 0.2679
[2019-04-04 13:30:13,873] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89000, global step 1425451: learning rate 0.0000
[2019-04-04 13:30:14,081] A3C_AGENT_WORKER-Thread-6 INFO:Local step 89000, global step 1425545: loss 0.2548
[2019-04-04 13:30:14,082] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 89000, global step 1425546: learning rate 0.0000
[2019-04-04 13:30:14,665] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89000, global step 1425819: loss 0.2519
[2019-04-04 13:30:14,666] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89000, global step 1425820: learning rate 0.0000
[2019-04-04 13:30:15,483] A3C_AGENT_WORKER-Thread-12 INFO:Local step 89000, global step 1426163: loss 0.2450
[2019-04-04 13:30:15,484] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 89000, global step 1426163: learning rate 0.0000
[2019-04-04 13:30:19,203] A3C_AGENT_WORKER-Thread-13 INFO:Local step 89500, global step 1427912: loss 0.3706
[2019-04-04 13:30:19,203] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 89500, global step 1427912: learning rate 0.0000
[2019-04-04 13:30:19,648] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89500, global step 1428109: loss 0.4114
[2019-04-04 13:30:19,648] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89500, global step 1428109: learning rate 0.0000
[2019-04-04 13:30:21,694] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3452571e-09 3.7379930e-10 4.0965173e-15 9.1385179e-14 1.0000000e+00
 2.5805844e-10 1.0728849e-14], sum to 1.0000
[2019-04-04 13:30:21,694] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3629
[2019-04-04 13:30:21,721] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.366666666666667, 81.33333333333334, 0.0, 0.0, 26.0, 24.13916472190463, 0.07090284666995336, 0.0, 1.0, 43267.94138879701], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2611200.0000, 
sim time next is 2611800.0000, 
raw observation next is [-6.45, 80.5, 0.0, 0.0, 26.0, 24.06125077458235, 0.05793485591419001, 0.0, 1.0, 43426.67511912868], 
processed observation next is [1.0, 0.21739130434782608, 0.28393351800554023, 0.805, 0.0, 0.0, 0.6666666666666666, 0.505104231215196, 0.5193116186380634, 0.0, 1.0, 0.20679369104346992], 
reward next is 0.7932, 
noisyNet noise sample is [array([-0.01514078], dtype=float32), -0.6495599]. 
=============================================
[2019-04-04 13:30:23,001] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1052701e-08 8.9707769e-10 4.6194331e-14 3.2551978e-13 1.0000000e+00
 4.3664428e-09 1.7331708e-13], sum to 1.0000
[2019-04-04 13:30:23,001] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2941
[2019-04-04 13:30:23,048] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.4166666666666667, 35.83333333333333, 0.0, 0.0, 26.0, 25.01531880748777, 0.3197946894123827, 1.0, 1.0, 80486.34720553472], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2573400.0000, 
sim time next is 2574000.0000, 
raw observation next is [-0.6, 36.0, 0.0, 0.0, 26.0, 24.97790189553787, 0.3394776146461649, 1.0, 1.0, 81194.6547098023], 
processed observation next is [1.0, 0.8260869565217391, 0.44598337950138506, 0.36, 0.0, 0.0, 0.6666666666666666, 0.5814918246281557, 0.613159204882055, 1.0, 1.0, 0.3866412129038205], 
reward next is 0.6134, 
noisyNet noise sample is [array([-0.37744135], dtype=float32), 1.1981152]. 
=============================================
[2019-04-04 13:30:23,064] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.73171]
 [74.44448]
 [74.1986 ]
 [74.30258]
 [74.43904]], R is [[74.98503876]
 [74.85192108]
 [74.85287476]
 [75.10434723]
 [75.353302  ]].
[2019-04-04 13:30:24,445] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89500, global step 1430333: loss 0.3754
[2019-04-04 13:30:24,445] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89500, global step 1430333: learning rate 0.0000
[2019-04-04 13:30:26,955] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89500, global step 1431328: loss 0.4436
[2019-04-04 13:30:26,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89500, global step 1431328: learning rate 0.0000
[2019-04-04 13:30:27,143] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89500, global step 1431402: loss 0.3801
[2019-04-04 13:30:27,143] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89500, global step 1431402: learning rate 0.0000
[2019-04-04 13:30:27,923] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89500, global step 1431754: loss 0.3972
[2019-04-04 13:30:27,923] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89500, global step 1431754: learning rate 0.0000
[2019-04-04 13:30:29,252] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89500, global step 1432324: loss 0.4147
[2019-04-04 13:30:29,253] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89500, global step 1432324: learning rate 0.0000
[2019-04-04 13:30:29,368] A3C_AGENT_WORKER-Thread-11 INFO:Local step 89500, global step 1432365: loss 0.3727
[2019-04-04 13:30:29,368] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 89500, global step 1432365: learning rate 0.0000
[2019-04-04 13:30:30,219] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89500, global step 1432740: loss 0.3874
[2019-04-04 13:30:30,220] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89500, global step 1432740: learning rate 0.0000
[2019-04-04 13:30:30,817] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89500, global step 1432991: loss 0.3984
[2019-04-04 13:30:30,818] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89500, global step 1432991: learning rate 0.0000
[2019-04-04 13:30:31,106] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89500, global step 1433108: loss 0.3675
[2019-04-04 13:30:31,106] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89500, global step 1433108: learning rate 0.0000
[2019-04-04 13:30:31,178] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89500, global step 1433131: loss 0.3579
[2019-04-04 13:30:31,179] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89500, global step 1433131: learning rate 0.0000
[2019-04-04 13:30:31,623] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89500, global step 1433258: loss 0.4023
[2019-04-04 13:30:31,623] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89500, global step 1433258: learning rate 0.0000
[2019-04-04 13:30:32,506] A3C_AGENT_WORKER-Thread-6 INFO:Local step 89500, global step 1433611: loss 0.4001
[2019-04-04 13:30:32,509] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 89500, global step 1433612: learning rate 0.0000
[2019-04-04 13:30:32,978] A3C_AGENT_WORKER-Thread-12 INFO:Local step 89500, global step 1433828: loss 0.3866
[2019-04-04 13:30:32,979] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 89500, global step 1433828: learning rate 0.0000
[2019-04-04 13:30:32,986] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89500, global step 1433834: loss 0.3962
[2019-04-04 13:30:32,986] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89500, global step 1433834: learning rate 0.0000
[2019-04-04 13:30:37,552] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5652690e-09 2.2859690e-11 1.3595628e-15 2.0126224e-14 1.0000000e+00
 9.0863206e-11 4.6689361e-16], sum to 1.0000
[2019-04-04 13:30:37,554] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4933
[2019-04-04 13:30:37,567] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 25.01827242666038, 0.3750600170137967, 0.0, 1.0, 43507.74900262372], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2935800.0000, 
sim time next is 2936400.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.98297080435946, 0.3791525176797033, 0.0, 1.0, 43461.01548729616], 
processed observation next is [1.0, 1.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5819142336966218, 0.6263841725599011, 0.0, 1.0, 0.20695721660617217], 
reward next is 0.7930, 
noisyNet noise sample is [array([0.67758065], dtype=float32), -1.700047]. 
=============================================
[2019-04-04 13:30:37,740] A3C_AGENT_WORKER-Thread-13 INFO:Local step 90000, global step 1435938: loss 0.3913
[2019-04-04 13:30:37,749] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 90000, global step 1435939: learning rate 0.0000
[2019-04-04 13:30:38,325] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90000, global step 1436235: loss 0.3533
[2019-04-04 13:30:38,325] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90000, global step 1436235: learning rate 0.0000
[2019-04-04 13:30:43,004] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90000, global step 1438316: loss 0.3866
[2019-04-04 13:30:43,005] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90000, global step 1438316: learning rate 0.0000
[2019-04-04 13:30:45,676] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90000, global step 1439493: loss 0.2913
[2019-04-04 13:30:45,677] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90000, global step 1439493: learning rate 0.0000
[2019-04-04 13:30:45,785] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90000, global step 1439543: loss 0.3333
[2019-04-04 13:30:45,788] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90000, global step 1439543: learning rate 0.0000
[2019-04-04 13:30:46,366] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90000, global step 1439840: loss 0.3111
[2019-04-04 13:30:46,370] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90000, global step 1439840: learning rate 0.0000
[2019-04-04 13:30:47,436] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90000, global step 1440308: loss 0.3277
[2019-04-04 13:30:47,438] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90000, global step 1440308: learning rate 0.0000
[2019-04-04 13:30:47,829] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0767860e-09 1.8054425e-10 5.8591103e-15 1.1381403e-13 1.0000000e+00
 4.7514936e-10 6.6629899e-15], sum to 1.0000
[2019-04-04 13:30:47,830] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5926
[2019-04-04 13:30:47,845] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 65.0, 0.0, 0.0, 26.0, 25.34119734426739, 0.3646299935157103, 0.0, 1.0, 40644.63843442687], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3012000.0000, 
sim time next is 3012600.0000, 
raw observation next is [-3.416666666666667, 65.0, 0.0, 0.0, 26.0, 25.32532667243676, 0.3618595483213258, 0.0, 1.0, 40196.93932327895], 
processed observation next is [0.0, 0.8695652173913043, 0.36795937211449675, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6104438893697299, 0.6206198494404419, 0.0, 1.0, 0.19141399677751883], 
reward next is 0.8086, 
noisyNet noise sample is [array([1.4366118], dtype=float32), -1.7537793]. 
=============================================
[2019-04-04 13:30:48,149] A3C_AGENT_WORKER-Thread-11 INFO:Local step 90000, global step 1440595: loss 0.3263
[2019-04-04 13:30:48,150] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 90000, global step 1440595: learning rate 0.0000
[2019-04-04 13:30:48,349] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90000, global step 1440681: loss 0.3118
[2019-04-04 13:30:48,356] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90000, global step 1440682: learning rate 0.0000
[2019-04-04 13:30:48,988] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90000, global step 1440951: loss 0.3008
[2019-04-04 13:30:48,989] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90000, global step 1440951: learning rate 0.0000
[2019-04-04 13:30:49,656] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90000, global step 1441265: loss 0.3294
[2019-04-04 13:30:49,657] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90000, global step 1441265: learning rate 0.0000
[2019-04-04 13:30:50,023] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90000, global step 1441428: loss 0.3357
[2019-04-04 13:30:50,023] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90000, global step 1441428: learning rate 0.0000
[2019-04-04 13:30:50,276] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90000, global step 1441549: loss 0.3615
[2019-04-04 13:30:50,278] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90000, global step 1441550: learning rate 0.0000
[2019-04-04 13:30:50,723] A3C_AGENT_WORKER-Thread-6 INFO:Local step 90000, global step 1441773: loss 0.3670
[2019-04-04 13:30:50,726] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 90000, global step 1441774: learning rate 0.0000
[2019-04-04 13:30:51,304] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90000, global step 1442082: loss 0.3362
[2019-04-04 13:30:51,307] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90000, global step 1442084: learning rate 0.0000
[2019-04-04 13:30:51,930] A3C_AGENT_WORKER-Thread-12 INFO:Local step 90000, global step 1442390: loss 0.3305
[2019-04-04 13:30:51,938] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 90000, global step 1442390: learning rate 0.0000
[2019-04-04 13:30:53,388] A3C_AGENT_WORKER-Thread-13 INFO:Local step 90500, global step 1443073: loss 0.1488
[2019-04-04 13:30:53,389] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 90500, global step 1443073: learning rate 0.0000
[2019-04-04 13:30:53,934] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90500, global step 1443361: loss 0.1776
[2019-04-04 13:30:53,934] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90500, global step 1443361: learning rate 0.0000
[2019-04-04 13:30:58,279] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90500, global step 1445882: loss 0.1345
[2019-04-04 13:30:58,281] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90500, global step 1445882: learning rate 0.0000
[2019-04-04 13:30:58,667] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.17864435e-09 1.10942984e-10 5.47367725e-16 1.01888048e-13
 1.00000000e+00 8.33251246e-11 4.04463907e-15], sum to 1.0000
[2019-04-04 13:30:58,677] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1413
[2019-04-04 13:30:58,702] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.333333333333333, 75.0, 0.0, 0.0, 26.0, 25.09432860842811, 0.4514649525827606, 1.0, 1.0, 92167.60119952037], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3436800.0000, 
sim time next is 3437400.0000, 
raw observation next is [1.166666666666667, 77.0, 0.0, 0.0, 26.0, 25.17686605181428, 0.4678758360979512, 1.0, 1.0, 18682.27693586107], 
processed observation next is [1.0, 0.782608695652174, 0.49492151431209613, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5980721709845233, 0.6559586120326504, 1.0, 1.0, 0.08896322350410034], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.05574068], dtype=float32), -1.1095184]. 
=============================================
[2019-04-04 13:31:01,182] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90500, global step 1447379: loss 0.1029
[2019-04-04 13:31:01,184] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90500, global step 1447380: learning rate 0.0000
[2019-04-04 13:31:01,806] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90500, global step 1447681: loss 0.1039
[2019-04-04 13:31:01,808] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90500, global step 1447681: learning rate 0.0000
[2019-04-04 13:31:01,999] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90500, global step 1447766: loss 0.1321
[2019-04-04 13:31:02,000] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90500, global step 1447766: learning rate 0.0000
[2019-04-04 13:31:03,491] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90500, global step 1448452: loss 0.0966
[2019-04-04 13:31:03,495] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90500, global step 1448456: learning rate 0.0000
[2019-04-04 13:31:03,842] A3C_AGENT_WORKER-Thread-11 INFO:Local step 90500, global step 1448621: loss 0.1069
[2019-04-04 13:31:03,843] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 90500, global step 1448621: learning rate 0.0000
[2019-04-04 13:31:04,220] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90500, global step 1448816: loss 0.1244
[2019-04-04 13:31:04,222] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90500, global step 1448816: learning rate 0.0000
[2019-04-04 13:31:04,931] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90500, global step 1449158: loss 0.1186
[2019-04-04 13:31:04,933] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90500, global step 1449158: learning rate 0.0000
[2019-04-04 13:31:05,718] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90500, global step 1449490: loss 0.1111
[2019-04-04 13:31:05,719] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90500, global step 1449490: learning rate 0.0000
[2019-04-04 13:31:05,977] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90500, global step 1449604: loss 0.1050
[2019-04-04 13:31:05,980] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90500, global step 1449606: learning rate 0.0000
[2019-04-04 13:31:06,000] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90500, global step 1449618: loss 0.1019
[2019-04-04 13:31:06,003] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90500, global step 1449618: learning rate 0.0000
[2019-04-04 13:31:06,705] A3C_AGENT_WORKER-Thread-6 INFO:Local step 90500, global step 1449918: loss 0.1073
[2019-04-04 13:31:06,705] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 90500, global step 1449918: learning rate 0.0000
[2019-04-04 13:31:07,425] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90500, global step 1450282: loss 0.1210
[2019-04-04 13:31:07,426] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90500, global step 1450282: learning rate 0.0000
[2019-04-04 13:31:07,692] A3C_AGENT_WORKER-Thread-12 INFO:Local step 90500, global step 1450418: loss 0.0995
[2019-04-04 13:31:07,692] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 90500, global step 1450418: learning rate 0.0000
[2019-04-04 13:31:08,353] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0683421e-09 2.8927535e-10 5.8733874e-15 1.5904856e-14 1.0000000e+00
 6.3650654e-11 8.3123993e-15], sum to 1.0000
[2019-04-04 13:31:08,353] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2700
[2019-04-04 13:31:08,391] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.09750664945245, 0.4839227792317282, 0.0, 1.0, 183667.2044460197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3443400.0000, 
sim time next is 3444000.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.15815893315217, 0.5120168556426314, 0.0, 1.0, 101774.7258670429], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5965132444293474, 0.6706722852142105, 0.0, 1.0, 0.4846415517478233], 
reward next is 0.5154, 
noisyNet noise sample is [array([-0.15955922], dtype=float32), -0.26576]. 
=============================================
[2019-04-04 13:31:08,409] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.75207]
 [83.40929]
 [82.57051]
 [82.40436]
 [82.09926]], R is [[84.16059875]
 [83.44438934]
 [82.66427612]
 [82.74856567]
 [82.78636932]].
[2019-04-04 13:31:08,811] A3C_AGENT_WORKER-Thread-13 INFO:Local step 91000, global step 1450999: loss 0.0468
[2019-04-04 13:31:08,817] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5513327e-09 9.5895702e-10 3.1052399e-15 4.0440952e-13 1.0000000e+00
 7.5363021e-11 7.3976021e-15], sum to 1.0000
[2019-04-04 13:31:08,818] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 91000, global step 1451004: learning rate 0.0000
[2019-04-04 13:31:08,819] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9538
[2019-04-04 13:31:08,844] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.333333333333333, 73.0, 0.0, 0.0, 26.0, 24.81348013039971, 0.2443580212665535, 0.0, 1.0, 41986.61890123891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3386400.0000, 
sim time next is 3387000.0000, 
raw observation next is [-5.166666666666667, 72.0, 0.0, 0.0, 26.0, 24.75295392613556, 0.2400042235630568, 0.0, 1.0, 42152.51697801601], 
processed observation next is [1.0, 0.17391304347826086, 0.31948291782086796, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5627461605112968, 0.5800014078543523, 0.0, 1.0, 0.20072627132388574], 
reward next is 0.7993, 
noisyNet noise sample is [array([1.4972893], dtype=float32), -1.3851235]. 
=============================================
[2019-04-04 13:31:08,854] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[81.896416]
 [81.97253 ]
 [82.04596 ]
 [82.12409 ]
 [82.167   ]], R is [[81.79320526]
 [81.77533722]
 [81.75841522]
 [81.74236298]
 [81.72718811]].
[2019-04-04 13:31:09,527] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91000, global step 1451375: loss 0.0493
[2019-04-04 13:31:09,527] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91000, global step 1451375: learning rate 0.0000
[2019-04-04 13:31:11,334] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5479743e-08 2.7511404e-09 1.1208230e-15 4.8401458e-14 1.0000000e+00
 5.1516313e-10 1.0916735e-14], sum to 1.0000
[2019-04-04 13:31:11,334] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0483
[2019-04-04 13:31:11,356] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.833333333333333, 48.83333333333334, 111.0, 777.3333333333333, 26.0, 26.61396191741011, 0.6261011390720693, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3408600.0000, 
sim time next is 3409200.0000, 
raw observation next is [3.0, 49.0, 112.0, 784.0, 26.0, 26.62986718141547, 0.6194122282320811, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.49, 0.37333333333333335, 0.8662983425414365, 0.6666666666666666, 0.7191555984512892, 0.706470742744027, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2423125], dtype=float32), 0.1347883]. 
=============================================
[2019-04-04 13:31:13,171] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9020645e-10 1.1762924e-10 5.6908120e-16 3.7084153e-14 1.0000000e+00
 2.4831333e-11 5.4011432e-16], sum to 1.0000
[2019-04-04 13:31:13,172] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9409
[2019-04-04 13:31:13,185] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.432727467183, 0.4489465346454404, 0.0, 1.0, 50080.940901407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3459600.0000, 
sim time next is 3460200.0000, 
raw observation next is [1.0, 79.00000000000001, 0.0, 0.0, 26.0, 25.42106645523782, 0.4438457905587533, 0.0, 1.0, 49726.4062567006], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7900000000000001, 0.0, 0.0, 0.6666666666666666, 0.6184222046031517, 0.6479485968529177, 0.0, 1.0, 0.2367924107461933], 
reward next is 0.7632, 
noisyNet noise sample is [array([1.2636356], dtype=float32), -0.10943561]. 
=============================================
[2019-04-04 13:31:14,066] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91000, global step 1453783: loss 0.0421
[2019-04-04 13:31:14,068] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91000, global step 1453784: learning rate 0.0000
[2019-04-04 13:31:15,257] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.2693125e-10 3.3623349e-10 2.4498856e-16 1.7201439e-15 1.0000000e+00
 5.3672882e-11 1.1571255e-15], sum to 1.0000
[2019-04-04 13:31:15,258] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1371
[2019-04-04 13:31:15,282] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.166666666666667, 77.0, 103.6666666666667, 706.3333333333333, 26.0, 26.1704051127485, 0.505276513219615, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3750600.0000, 
sim time next is 3751200.0000, 
raw observation next is [-3.0, 77.0, 105.5, 722.0, 26.0, 26.22033809613153, 0.5171331481745999, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.77, 0.3516666666666667, 0.7977900552486188, 0.6666666666666666, 0.6850281746776276, 0.6723777160582, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8674301], dtype=float32), -0.27594507]. 
=============================================
[2019-04-04 13:31:17,241] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91000, global step 1455432: loss 0.0362
[2019-04-04 13:31:17,242] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91000, global step 1455432: learning rate 0.0000
[2019-04-04 13:31:17,738] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91000, global step 1455710: loss 0.0433
[2019-04-04 13:31:17,739] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91000, global step 1455710: learning rate 0.0000
[2019-04-04 13:31:17,748] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9299370e-09 8.3301488e-10 5.4637500e-15 3.3631555e-14 1.0000000e+00
 3.7816802e-10 1.3323572e-14], sum to 1.0000
[2019-04-04 13:31:17,748] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4337
[2019-04-04 13:31:17,764] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.833333333333333, 48.66666666666667, 115.3333333333333, 815.6666666666666, 26.0, 25.20991566940364, 0.4568180109414371, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3589800.0000, 
sim time next is 3590400.0000, 
raw observation next is [-1.666666666666667, 47.33333333333334, 114.6666666666667, 813.8333333333334, 26.0, 25.24170507813501, 0.4547367605016904, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.4164358264081256, 0.47333333333333344, 0.38222222222222235, 0.8992633517495396, 0.6666666666666666, 0.6034754231779175, 0.6515789201672301, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19278793], dtype=float32), 0.16697593]. 
=============================================
[2019-04-04 13:31:17,915] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91000, global step 1455799: loss 0.0430
[2019-04-04 13:31:17,916] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91000, global step 1455799: learning rate 0.0000
[2019-04-04 13:31:19,454] A3C_AGENT_WORKER-Thread-11 INFO:Local step 91000, global step 1456564: loss 0.0373
[2019-04-04 13:31:19,460] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 91000, global step 1456566: learning rate 0.0000
[2019-04-04 13:31:19,461] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91000, global step 1456566: loss 0.0466
[2019-04-04 13:31:19,463] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91000, global step 1456566: learning rate 0.0000
[2019-04-04 13:31:19,966] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91000, global step 1456834: loss 0.0364
[2019-04-04 13:31:19,967] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91000, global step 1456836: learning rate 0.0000
[2019-04-04 13:31:20,804] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91000, global step 1457338: loss 0.0309
[2019-04-04 13:31:20,810] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91000, global step 1457339: learning rate 0.0000
[2019-04-04 13:31:21,155] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.4698915e-09 4.8459381e-10 2.3058288e-14 2.1240194e-13 1.0000000e+00
 1.5847422e-09 3.9830132e-14], sum to 1.0000
[2019-04-04 13:31:21,155] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0903
[2019-04-04 13:31:21,172] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3333333333333334, 42.66666666666666, 82.16666666666667, 668.3333333333333, 26.0, 25.34741938859077, 0.4726643538198249, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3598800.0000, 
sim time next is 3599400.0000, 
raw observation next is [-0.1666666666666666, 42.83333333333334, 78.33333333333334, 637.6666666666667, 26.0, 25.35031939672623, 0.4666206986447364, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.4579870729455217, 0.42833333333333345, 0.2611111111111111, 0.7046040515653776, 0.6666666666666666, 0.6125266163938526, 0.6555402328815788, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0550905], dtype=float32), 0.7891042]. 
=============================================
[2019-04-04 13:31:21,696] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91000, global step 1457845: loss 0.0254
[2019-04-04 13:31:21,697] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91000, global step 1457846: learning rate 0.0000
[2019-04-04 13:31:21,774] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91000, global step 1457893: loss 0.0361
[2019-04-04 13:31:21,775] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91000, global step 1457894: learning rate 0.0000
[2019-04-04 13:31:21,924] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91000, global step 1457993: loss 0.0326
[2019-04-04 13:31:21,924] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91000, global step 1457993: learning rate 0.0000
[2019-04-04 13:31:22,416] A3C_AGENT_WORKER-Thread-6 INFO:Local step 91000, global step 1458267: loss 0.0367
[2019-04-04 13:31:22,418] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 91000, global step 1458267: learning rate 0.0000
[2019-04-04 13:31:22,980] A3C_AGENT_WORKER-Thread-13 INFO:Local step 91500, global step 1458582: loss 1.1057
[2019-04-04 13:31:22,981] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 91500, global step 1458583: learning rate 0.0000
[2019-04-04 13:31:23,019] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91000, global step 1458606: loss 0.0332
[2019-04-04 13:31:23,020] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91000, global step 1458606: learning rate 0.0000
[2019-04-04 13:31:23,662] A3C_AGENT_WORKER-Thread-12 INFO:Local step 91000, global step 1458938: loss 0.0256
[2019-04-04 13:31:23,662] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 91000, global step 1458938: learning rate 0.0000
[2019-04-04 13:31:23,905] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91500, global step 1459081: loss 1.0483
[2019-04-04 13:31:23,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91500, global step 1459081: learning rate 0.0000
[2019-04-04 13:31:26,315] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2137613e-09 9.6397390e-11 6.1825750e-16 4.4605056e-15 1.0000000e+00
 1.4514029e-10 2.5869165e-14], sum to 1.0000
[2019-04-04 13:31:26,316] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2430
[2019-04-04 13:31:26,334] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.166666666666667, 62.16666666666667, 0.0, 0.0, 26.0, 25.72361612973082, 0.4455015030432417, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3703800.0000, 
sim time next is 3704400.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 26.0, 25.67547097283514, 0.4376713002124799, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.518005540166205, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6396225810695949, 0.64589043340416, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5867607], dtype=float32), -1.1451539]. 
=============================================
[2019-04-04 13:31:27,412] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4133535e-08 2.9903360e-09 1.3693985e-14 1.7813767e-12 1.0000000e+00
 1.9150017e-09 8.6746148e-14], sum to 1.0000
[2019-04-04 13:31:27,416] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4881
[2019-04-04 13:31:27,485] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.5, 66.0, 0.0, 0.0, 26.0, 23.56233043235398, 0.07339330325314407, 1.0, 1.0, 202955.1771653006], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4001400.0000, 
sim time next is 4002000.0000, 
raw observation next is [-13.33333333333333, 65.0, 15.5, 73.99999999999999, 26.0, 24.12768308345118, 0.1932730319634381, 1.0, 1.0, 152490.8549105871], 
processed observation next is [1.0, 0.30434782608695654, 0.09325946445060027, 0.65, 0.051666666666666666, 0.08176795580110496, 0.6666666666666666, 0.5106402569542651, 0.5644243439878127, 1.0, 1.0, 0.7261469281456528], 
reward next is 0.2739, 
noisyNet noise sample is [array([-0.8223266], dtype=float32), -0.08007784]. 
=============================================
[2019-04-04 13:31:27,502] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[76.534325]
 [75.3169  ]
 [74.39536 ]
 [74.50559 ]
 [74.58516 ]], R is [[76.95362091]
 [76.21762848]
 [75.4916687 ]
 [75.53256989]
 [75.57258606]].
[2019-04-04 13:31:28,307] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91500, global step 1461487: loss 1.2686
[2019-04-04 13:31:28,307] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91500, global step 1461487: learning rate 0.0000
[2019-04-04 13:31:31,882] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91500, global step 1463281: loss 1.2763
[2019-04-04 13:31:31,883] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91500, global step 1463281: learning rate 0.0000
[2019-04-04 13:31:32,178] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91500, global step 1463436: loss 1.0629
[2019-04-04 13:31:32,182] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91500, global step 1463438: learning rate 0.0000
[2019-04-04 13:31:32,654] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91500, global step 1463687: loss 1.1376
[2019-04-04 13:31:32,654] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91500, global step 1463687: learning rate 0.0000
[2019-04-04 13:31:33,871] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91500, global step 1464222: loss 1.1094
[2019-04-04 13:31:33,881] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91500, global step 1464222: learning rate 0.0000
[2019-04-04 13:31:33,935] A3C_AGENT_WORKER-Thread-11 INFO:Local step 91500, global step 1464251: loss 1.1389
[2019-04-04 13:31:33,944] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 91500, global step 1464251: learning rate 0.0000
[2019-04-04 13:31:34,644] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91500, global step 1464600: loss 1.2032
[2019-04-04 13:31:34,644] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91500, global step 1464600: learning rate 0.0000
[2019-04-04 13:31:35,448] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91500, global step 1465037: loss 1.1971
[2019-04-04 13:31:35,451] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91500, global step 1465039: learning rate 0.0000
[2019-04-04 13:31:36,364] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91500, global step 1465454: loss 1.0933
[2019-04-04 13:31:36,365] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91500, global step 1465455: learning rate 0.0000
[2019-04-04 13:31:36,514] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91500, global step 1465516: loss 1.0926
[2019-04-04 13:31:36,515] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91500, global step 1465516: learning rate 0.0000
[2019-04-04 13:31:36,720] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91500, global step 1465618: loss 1.1179
[2019-04-04 13:31:36,721] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91500, global step 1465619: learning rate 0.0000
[2019-04-04 13:31:37,065] A3C_AGENT_WORKER-Thread-6 INFO:Local step 91500, global step 1465775: loss 1.2515
[2019-04-04 13:31:37,067] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 91500, global step 1465776: learning rate 0.0000
[2019-04-04 13:31:37,793] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91500, global step 1466093: loss 1.1343
[2019-04-04 13:31:37,796] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91500, global step 1466095: learning rate 0.0000
[2019-04-04 13:31:38,479] A3C_AGENT_WORKER-Thread-12 INFO:Local step 91500, global step 1466370: loss 1.1647
[2019-04-04 13:31:38,482] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 91500, global step 1466370: learning rate 0.0000
[2019-04-04 13:31:39,697] A3C_AGENT_WORKER-Thread-13 INFO:Local step 92000, global step 1466977: loss 0.3236
[2019-04-04 13:31:39,700] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 92000, global step 1466977: learning rate 0.0000
[2019-04-04 13:31:40,509] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92000, global step 1467414: loss 0.2996
[2019-04-04 13:31:40,511] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92000, global step 1467414: learning rate 0.0000
[2019-04-04 13:31:44,537] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92000, global step 1469529: loss 0.2761
[2019-04-04 13:31:44,542] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92000, global step 1469531: learning rate 0.0000
[2019-04-04 13:31:46,639] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.6269187e-09 1.0095970e-09 1.8814099e-14 3.7068471e-13 1.0000000e+00
 2.3995175e-10 2.2142437e-14], sum to 1.0000
[2019-04-04 13:31:46,640] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9463
[2019-04-04 13:31:46,656] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.5, 42.5, 0.0, 0.0, 26.0, 25.33984779726618, 0.4120785497642485, 0.0, 1.0, 46922.20661621026], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4152600.0000, 
sim time next is 4153200.0000, 
raw observation next is [-1.666666666666667, 43.66666666666666, 0.0, 0.0, 26.0, 25.34275746914723, 0.4086977492385745, 0.0, 1.0, 41767.24959952971], 
processed observation next is [0.0, 0.043478260869565216, 0.4164358264081256, 0.4366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6118964557622691, 0.6362325830795248, 0.0, 1.0, 0.19889166475966527], 
reward next is 0.8011, 
noisyNet noise sample is [array([0.3684678], dtype=float32), 0.7867814]. 
=============================================
[2019-04-04 13:31:47,623] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92000, global step 1471312: loss 0.2630
[2019-04-04 13:31:47,624] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92000, global step 1471312: learning rate 0.0000
[2019-04-04 13:31:48,118] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92000, global step 1471620: loss 0.2526
[2019-04-04 13:31:48,118] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92000, global step 1471620: learning rate 0.0000
[2019-04-04 13:31:48,142] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92000, global step 1471638: loss 0.2540
[2019-04-04 13:31:48,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92000, global step 1471639: learning rate 0.0000
[2019-04-04 13:31:49,122] A3C_AGENT_WORKER-Thread-11 INFO:Local step 92000, global step 1472240: loss 0.2480
[2019-04-04 13:31:49,124] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 92000, global step 1472240: learning rate 0.0000
[2019-04-04 13:31:49,649] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92000, global step 1472517: loss 0.2616
[2019-04-04 13:31:49,651] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92000, global step 1472518: learning rate 0.0000
[2019-04-04 13:31:50,191] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92000, global step 1472758: loss 0.2493
[2019-04-04 13:31:50,193] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92000, global step 1472759: learning rate 0.0000
[2019-04-04 13:31:50,272] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.5932560e-10 3.2369305e-11 1.1467549e-15 3.9426944e-15 1.0000000e+00
 8.7357128e-11 1.8629075e-16], sum to 1.0000
[2019-04-04 13:31:50,273] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0937
[2019-04-04 13:31:50,281] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 52.0, 131.3333333333333, 825.3333333333334, 26.0, 25.27466389502006, 0.4260634965989659, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4279200.0000, 
sim time next is 4279800.0000, 
raw observation next is [7.0, 52.0, 142.6666666666667, 803.6666666666667, 26.0, 25.3327703146651, 0.4310577853564427, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.47555555555555573, 0.8880294659300185, 0.6666666666666666, 0.6110641928887585, 0.6436859284521476, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8193685], dtype=float32), 0.31295443]. 
=============================================
[2019-04-04 13:31:51,175] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92000, global step 1473295: loss 0.2601
[2019-04-04 13:31:51,176] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92000, global step 1473297: learning rate 0.0000
[2019-04-04 13:31:51,712] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92000, global step 1473598: loss 0.2239
[2019-04-04 13:31:51,717] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92000, global step 1473601: learning rate 0.0000
[2019-04-04 13:31:52,146] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92000, global step 1473853: loss 0.2563
[2019-04-04 13:31:52,149] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92000, global step 1473854: learning rate 0.0000
[2019-04-04 13:31:52,437] A3C_AGENT_WORKER-Thread-6 INFO:Local step 92000, global step 1474021: loss 0.2371
[2019-04-04 13:31:52,439] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 92000, global step 1474021: learning rate 0.0000
[2019-04-04 13:31:52,567] A3C_AGENT_WORKER-Thread-13 INFO:Local step 92500, global step 1474102: loss 2.4222
[2019-04-04 13:31:52,569] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 92500, global step 1474103: learning rate 0.0000
[2019-04-04 13:31:52,811] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92000, global step 1474251: loss 0.2474
[2019-04-04 13:31:52,817] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92000, global step 1474255: learning rate 0.0000
[2019-04-04 13:31:53,289] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92000, global step 1474526: loss 0.2439
[2019-04-04 13:31:53,290] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92000, global step 1474526: learning rate 0.0000
[2019-04-04 13:31:54,129] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92500, global step 1474987: loss 2.4596
[2019-04-04 13:31:54,129] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92500, global step 1474988: learning rate 0.0000
[2019-04-04 13:31:54,602] A3C_AGENT_WORKER-Thread-12 INFO:Local step 92000, global step 1475251: loss 0.2278
[2019-04-04 13:31:54,604] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 92000, global step 1475251: learning rate 0.0000
[2019-04-04 13:31:54,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9815310e-09 2.4616170e-10 6.9521844e-16 4.0941740e-15 1.0000000e+00
 4.7729636e-11 5.9096806e-16], sum to 1.0000
[2019-04-04 13:31:54,838] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4970
[2019-04-04 13:31:54,845] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.73333333333333, 58.33333333333334, 0.0, 0.0, 26.0, 27.1031291407058, 0.8868187280289567, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4393200.0000, 
sim time next is 4393800.0000, 
raw observation next is [10.6, 58.5, 0.0, 0.0, 26.0, 27.02020892082966, 0.8760345453709993, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7562326869806094, 0.585, 0.0, 0.0, 0.6666666666666666, 0.751684076735805, 0.7920115151236664, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8738302], dtype=float32), -1.1622897]. 
=============================================
[2019-04-04 13:31:57,524] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8394875e-10 1.9265620e-11 9.9200548e-17 2.3855508e-16 1.0000000e+00
 2.7201441e-11 3.8216634e-17], sum to 1.0000
[2019-04-04 13:31:57,528] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4520
[2019-04-04 13:31:57,539] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.6, 42.0, 7.5, 0.0, 26.0, 27.44625831922675, 0.995073645829545, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4383600.0000, 
sim time next is 4384200.0000, 
raw observation next is [12.5, 43.0, 6.000000000000001, 0.0, 26.0, 27.54860821220122, 1.006409718239791, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.808864265927978, 0.43, 0.020000000000000004, 0.0, 0.6666666666666666, 0.7957173510167683, 0.8354699060799303, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2692683], dtype=float32), -0.3959519]. 
=============================================
[2019-04-04 13:31:57,971] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92500, global step 1477263: loss 2.4355
[2019-04-04 13:31:57,974] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92500, global step 1477264: learning rate 0.0000
[2019-04-04 13:31:58,493] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2560181e-09 6.1718221e-11 2.9056830e-17 2.0251556e-15 1.0000000e+00
 1.1926118e-11 1.6750163e-16], sum to 1.0000
[2019-04-04 13:31:58,496] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7274
[2019-04-04 13:31:58,532] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.73333333333333, 28.66666666666666, 117.5, 851.1666666666667, 26.0, 27.53320777215315, 0.9393604231275554, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4365600.0000, 
sim time next is 4366200.0000, 
raw observation next is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 26.0, 26.65923350831825, 0.8709434799912558, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8688827331486613, 0.2883333333333334, 0.39, 0.9384898710865562, 0.6666666666666666, 0.721602792359854, 0.7903144933304186, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1599215], dtype=float32), 0.50103456]. 
=============================================
[2019-04-04 13:32:01,392] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92500, global step 1479232: loss 2.3965
[2019-04-04 13:32:01,394] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92500, global step 1479234: learning rate 0.0000
[2019-04-04 13:32:02,002] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92500, global step 1479559: loss 2.4209
[2019-04-04 13:32:02,004] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92500, global step 1479559: learning rate 0.0000
[2019-04-04 13:32:02,167] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92500, global step 1479645: loss 2.4123
[2019-04-04 13:32:02,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92500, global step 1479645: learning rate 0.0000
[2019-04-04 13:32:02,838] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4852100e-10 1.0710382e-11 4.8463923e-17 6.4481908e-16 1.0000000e+00
 6.1182322e-12 9.5467224e-17], sum to 1.0000
[2019-04-04 13:32:02,838] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3819
[2019-04-04 13:32:02,898] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 80.33333333333334, 67.33333333333334, 0.0, 26.0, 25.62467209686397, 0.5060817002085417, 1.0, 1.0, 49529.51073090103], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4462800.0000, 
sim time next is 4463400.0000, 
raw observation next is [0.0, 79.16666666666666, 63.66666666666666, 0.0, 26.0, 24.92592258774474, 0.4677873725775983, 1.0, 1.0, 193626.1113100135], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.7916666666666665, 0.2122222222222222, 0.0, 0.6666666666666666, 0.577160215645395, 0.6559291241925328, 1.0, 1.0, 0.9220291014762548], 
reward next is 0.0780, 
noisyNet noise sample is [array([-0.09905749], dtype=float32), 1.2531954]. 
=============================================
[2019-04-04 13:32:03,557] A3C_AGENT_WORKER-Thread-11 INFO:Local step 92500, global step 1480352: loss 2.3671
[2019-04-04 13:32:03,557] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 92500, global step 1480352: learning rate 0.0000
[2019-04-04 13:32:03,693] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92500, global step 1480405: loss 2.4165
[2019-04-04 13:32:03,694] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92500, global step 1480405: learning rate 0.0000
[2019-04-04 13:32:04,108] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92500, global step 1480628: loss 2.4237
[2019-04-04 13:32:04,110] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92500, global step 1480628: learning rate 0.0000
[2019-04-04 13:32:05,110] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92500, global step 1481150: loss 2.4059
[2019-04-04 13:32:05,111] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92500, global step 1481150: learning rate 0.0000
[2019-04-04 13:32:06,156] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92500, global step 1481672: loss 2.4707
[2019-04-04 13:32:06,158] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92500, global step 1481673: learning rate 0.0000
[2019-04-04 13:32:06,434] A3C_AGENT_WORKER-Thread-6 INFO:Local step 92500, global step 1481796: loss 2.3994
[2019-04-04 13:32:06,435] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 92500, global step 1481796: learning rate 0.0000
[2019-04-04 13:32:06,807] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4806009e-09 4.0820514e-10 3.0107108e-16 3.0623032e-15 1.0000000e+00
 3.4471738e-11 1.2726383e-16], sum to 1.0000
[2019-04-04 13:32:06,807] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3025
[2019-04-04 13:32:06,819] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.333333333333333, 54.66666666666667, 127.8333333333333, 778.0, 26.0, 26.65927328948198, 0.6911695851155412, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4617600.0000, 
sim time next is 4618200.0000, 
raw observation next is [1.666666666666667, 53.33333333333334, 126.6666666666667, 789.0, 26.0, 26.71748024104239, 0.6982243478392177, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5087719298245615, 0.5333333333333334, 0.42222222222222233, 0.8718232044198895, 0.6666666666666666, 0.7264566867535326, 0.7327414492797392, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14470372], dtype=float32), 0.16703752]. 
=============================================
[2019-04-04 13:32:06,849] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92500, global step 1482016: loss 2.4719
[2019-04-04 13:32:06,852] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92500, global step 1482018: learning rate 0.0000
[2019-04-04 13:32:06,958] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92500, global step 1482082: loss 2.3464
[2019-04-04 13:32:06,960] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92500, global step 1482082: learning rate 0.0000
[2019-04-04 13:32:07,597] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92500, global step 1482415: loss 2.3464
[2019-04-04 13:32:07,598] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92500, global step 1482415: learning rate 0.0000
[2019-04-04 13:32:07,884] A3C_AGENT_WORKER-Thread-13 INFO:Local step 93000, global step 1482569: loss 3.4614
[2019-04-04 13:32:07,885] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 93000, global step 1482569: learning rate 0.0000
[2019-04-04 13:32:08,634] A3C_AGENT_WORKER-Thread-12 INFO:Local step 92500, global step 1482968: loss 2.4852
[2019-04-04 13:32:08,636] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 92500, global step 1482968: learning rate 0.0000
[2019-04-04 13:32:09,689] A3C_AGENT_WORKER-Thread-17 INFO:Local step 93000, global step 1483571: loss 3.4674
[2019-04-04 13:32:09,691] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 93000, global step 1483573: learning rate 0.0000
[2019-04-04 13:32:12,883] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8800912e-10 3.5724944e-11 3.2246303e-15 7.4478328e-15 1.0000000e+00
 1.4294817e-10 1.8484747e-15], sum to 1.0000
[2019-04-04 13:32:12,885] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4979
[2019-04-04 13:32:12,908] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.7, 49.0, 171.0, 706.0, 26.0, 26.61618705526236, 0.7598209118385543, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4629600.0000, 
sim time next is 4630200.0000, 
raw observation next is [4.75, 49.16666666666667, 181.6666666666667, 670.3333333333333, 26.0, 26.05928069938164, 0.733194641509579, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5941828254847646, 0.4916666666666667, 0.6055555555555557, 0.7406998158379373, 0.6666666666666666, 0.6716067249484702, 0.7443982138365263, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.67581904], dtype=float32), -0.81855035]. 
=============================================
[2019-04-04 13:32:13,610] A3C_AGENT_WORKER-Thread-2 INFO:Local step 93000, global step 1485616: loss 3.5468
[2019-04-04 13:32:13,611] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 93000, global step 1485616: learning rate 0.0000
[2019-04-04 13:32:15,441] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8206278e-09 1.4984411e-10 8.3472529e-16 1.3262720e-14 1.0000000e+00
 1.6921174e-10 2.1270782e-15], sum to 1.0000
[2019-04-04 13:32:15,442] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0599
[2019-04-04 13:32:15,468] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.95279705704615, 0.3594660233067281, 0.0, 1.0, 41141.27814097558], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4755600.0000, 
sim time next is 4756200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.92046243437145, 0.3590001553279968, 0.0, 1.0, 41050.63436024157], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5767052028642876, 0.6196667184426655, 0.0, 1.0, 0.19547921123924558], 
reward next is 0.8045, 
noisyNet noise sample is [array([0.41285974], dtype=float32), 0.09398223]. 
=============================================
[2019-04-04 13:32:16,767] A3C_AGENT_WORKER-Thread-15 INFO:Local step 93000, global step 1487168: loss 3.5645
[2019-04-04 13:32:16,770] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 93000, global step 1487170: learning rate 0.0000
[2019-04-04 13:32:17,727] A3C_AGENT_WORKER-Thread-16 INFO:Local step 93000, global step 1487646: loss 3.5558
[2019-04-04 13:32:17,728] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 93000, global step 1487646: learning rate 0.0000
[2019-04-04 13:32:17,814] A3C_AGENT_WORKER-Thread-14 INFO:Local step 93000, global step 1487689: loss 3.5378
[2019-04-04 13:32:17,817] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 93000, global step 1487691: learning rate 0.0000
[2019-04-04 13:32:18,894] A3C_AGENT_WORKER-Thread-11 INFO:Local step 93000, global step 1488246: loss 3.5570
[2019-04-04 13:32:18,897] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 93000, global step 1488247: learning rate 0.0000
[2019-04-04 13:32:19,221] A3C_AGENT_WORKER-Thread-3 INFO:Local step 93000, global step 1488418: loss 3.5502
[2019-04-04 13:32:19,225] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 93000, global step 1488423: learning rate 0.0000
[2019-04-04 13:32:19,697] A3C_AGENT_WORKER-Thread-20 INFO:Local step 93000, global step 1488675: loss 3.5690
[2019-04-04 13:32:19,699] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 93000, global step 1488675: learning rate 0.0000
[2019-04-04 13:32:20,313] A3C_AGENT_WORKER-Thread-5 INFO:Local step 93000, global step 1488952: loss 3.5609
[2019-04-04 13:32:20,314] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 93000, global step 1488952: learning rate 0.0000
[2019-04-04 13:32:21,308] A3C_AGENT_WORKER-Thread-18 INFO:Local step 93000, global step 1489421: loss 3.5710
[2019-04-04 13:32:21,309] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 93000, global step 1489421: learning rate 0.0000
[2019-04-04 13:32:21,857] A3C_AGENT_WORKER-Thread-6 INFO:Local step 93000, global step 1489693: loss 3.5572
[2019-04-04 13:32:21,857] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 93000, global step 1489693: learning rate 0.0000
[2019-04-04 13:32:22,109] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6904828e-09 4.5758508e-10 1.7243363e-14 4.7743405e-14 1.0000000e+00
 6.1040499e-11 1.1246139e-14], sum to 1.0000
[2019-04-04 13:32:22,111] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5975
[2019-04-04 13:32:22,126] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 37.0, 139.5, 739.5, 26.0, 25.13769797406943, 0.4369334092347794, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4804800.0000, 
sim time next is 4805400.0000, 
raw observation next is [3.0, 37.0, 131.0, 737.0, 26.0, 25.13322647925805, 0.4358410288113959, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.43666666666666665, 0.8143646408839779, 0.6666666666666666, 0.5944355399381708, 0.645280342937132, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9575907], dtype=float32), -2.238411]. 
=============================================
[2019-04-04 13:32:22,283] A3C_AGENT_WORKER-Thread-4 INFO:Local step 93000, global step 1489906: loss 3.5492
[2019-04-04 13:32:22,284] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 93000, global step 1489906: learning rate 0.0000
[2019-04-04 13:32:22,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:22,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:22,554] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run12
[2019-04-04 13:32:22,992] A3C_AGENT_WORKER-Thread-10 INFO:Local step 93000, global step 1490276: loss 3.5480
[2019-04-04 13:32:22,996] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 93000, global step 1490277: learning rate 0.0000
[2019-04-04 13:32:23,167] A3C_AGENT_WORKER-Thread-19 INFO:Local step 93000, global step 1490368: loss 3.5326
[2019-04-04 13:32:23,169] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 93000, global step 1490368: learning rate 0.0000
[2019-04-04 13:32:23,709] A3C_AGENT_WORKER-Thread-12 INFO:Local step 93000, global step 1490640: loss 3.5578
[2019-04-04 13:32:23,709] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 93000, global step 1490640: learning rate 0.0000
[2019-04-04 13:32:24,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:24,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:24,122] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run12
[2019-04-04 13:32:26,741] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.78349033e-09 1.27047400e-10 6.79266584e-16 1.10946146e-14
 1.00000000e+00 1.55030613e-10 4.11369598e-15], sum to 1.0000
[2019-04-04 13:32:26,741] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1866
[2019-04-04 13:32:26,766] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.333333333333333, 38.0, 0.0, 0.0, 26.0, 25.56361670766733, 0.474106263523172, 0.0, 1.0, 24070.94910037158], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5010000.0000, 
sim time next is 5010600.0000, 
raw observation next is [2.166666666666667, 39.0, 0.0, 0.0, 26.0, 25.53889872632751, 0.4672849876478825, 0.0, 1.0, 40705.9396728452], 
processed observation next is [1.0, 1.0, 0.5226223453370269, 0.39, 0.0, 0.0, 0.6666666666666666, 0.6282415605272925, 0.6557616625492941, 0.0, 1.0, 0.1938378079659295], 
reward next is 0.8062, 
noisyNet noise sample is [array([0.89918274], dtype=float32), 0.6567157]. 
=============================================
[2019-04-04 13:32:27,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:27,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:27,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run12
[2019-04-04 13:32:29,355] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.8700874e-09 8.3057006e-10 2.4239787e-15 9.2954988e-15 1.0000000e+00
 4.6258439e-10 5.7150283e-15], sum to 1.0000
[2019-04-04 13:32:29,358] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6411
[2019-04-04 13:32:29,379] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.666666666666667, 29.16666666666667, 118.6666666666667, 817.0, 26.0, 26.68348577401455, 0.601122933109103, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4963800.0000, 
sim time next is 4964400.0000, 
raw observation next is [3.0, 29.0, 119.5, 824.0, 26.0, 26.6781291983695, 0.6110984994318882, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.29, 0.3983333333333333, 0.9104972375690608, 0.6666666666666666, 0.7231774331974584, 0.7036994998106293, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.41714555], dtype=float32), 0.015827768]. 
=============================================
[2019-04-04 13:32:30,735] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:30,735] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:30,758] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run12
[2019-04-04 13:32:31,090] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5311539e-10 8.2355768e-11 5.1256325e-16 1.0058497e-15 1.0000000e+00
 1.4597819e-10 2.9432235e-16], sum to 1.0000
[2019-04-04 13:32:31,092] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2584
[2019-04-04 13:32:31,107] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.0, 17.0, 56.0, 438.5, 26.0, 29.1616742762178, 1.202275130380803, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5072400.0000, 
sim time next is 5073000.0000, 
raw observation next is [11.83333333333333, 17.0, 49.33333333333333, 389.6666666666666, 26.0, 28.86685225589014, 1.175832744026335, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7903970452446908, 0.17, 0.16444444444444442, 0.4305709023941067, 0.6666666666666666, 0.9055710213241784, 0.8919442480087784, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19493014], dtype=float32), -0.10863918]. 
=============================================
[2019-04-04 13:32:31,115] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[87.777695]
 [87.88432 ]
 [88.04329 ]
 [88.1386  ]
 [88.15387 ]], R is [[87.81858826]
 [87.9404068 ]
 [88.06100464]
 [88.18039703]
 [88.29859161]].
[2019-04-04 13:32:31,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:31,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:31,486] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.8321126e-09 6.9755030e-10 5.9841823e-15 8.5017225e-14 1.0000000e+00
 5.7521123e-11 6.9324014e-16], sum to 1.0000
[2019-04-04 13:32:31,487] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7846
[2019-04-04 13:32:31,491] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run12
[2019-04-04 13:32:31,546] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 26.66666666666666, 0.0, 26.0, 22.58269557935225, -0.2559084218261343, 0.0, 1.0, 64017.87044463765], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 31800.0000, 
sim time next is 32400.0000, 
raw observation next is [7.7, 93.0, 29.5, 0.0, 26.0, 22.75956685279883, -0.2164468579006162, 0.0, 1.0, 60540.43269351996], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.09833333333333333, 0.0, 0.6666666666666666, 0.3966305710665692, 0.42785104736646123, 0.0, 1.0, 0.28828777473104744], 
reward next is 0.7117, 
noisyNet noise sample is [array([-0.01311898], dtype=float32), 0.8860707]. 
=============================================
[2019-04-04 13:32:31,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:31,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:31,850] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run12
[2019-04-04 13:32:32,615] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:32,615] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:32,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run12
[2019-04-04 13:32:32,841] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:32,841] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:32,847] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run12
[2019-04-04 13:32:33,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:33,226] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:33,231] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run12
[2019-04-04 13:32:33,382] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:33,382] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:33,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run12
[2019-04-04 13:32:34,108] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:34,108] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:34,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run12
[2019-04-04 13:32:35,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:35,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:35,014] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run12
[2019-04-04 13:32:35,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:35,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:35,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run12
[2019-04-04 13:32:35,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:35,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:35,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run12
[2019-04-04 13:32:36,344] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:36,344] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:36,348] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run12
[2019-04-04 13:32:36,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:32:36,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:36,856] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run12
[2019-04-04 13:32:40,707] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3778390e-08 1.7104357e-09 3.3891713e-14 5.4369242e-13 1.0000000e+00
 1.5358412e-09 2.0738224e-14], sum to 1.0000
[2019-04-04 13:32:40,707] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1401
[2019-04-04 13:32:40,731] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 20.92322984340302, -0.655938313470911, 0.0, 1.0, 41090.33541276866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 16200.0000, 
sim time next is 16800.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 26.0, 20.9386440371717, -0.6503602251475509, 0.0, 1.0, 40998.29739163682], 
processed observation next is [0.0, 0.17391304347826086, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.24488700309764155, 0.2832132582841497, 0.0, 1.0, 0.19522998757922297], 
reward next is 0.8048, 
noisyNet noise sample is [array([-0.86273223], dtype=float32), -1.1086153]. 
=============================================
[2019-04-04 13:32:44,678] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.5931256e-10 3.4962401e-11 6.3712193e-16 2.1144668e-14 1.0000000e+00
 2.6927810e-10 6.7936246e-16], sum to 1.0000
[2019-04-04 13:32:44,679] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0271
[2019-04-04 13:32:44,762] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.200000000000001, 88.0, 0.0, 0.0, 26.0, 24.59942586604893, 0.2052845272901187, 0.0, 1.0, 36908.43060522222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 66000.0000, 
sim time next is 66600.0000, 
raw observation next is [4.1, 87.5, 0.0, 0.0, 26.0, 24.60387685203898, 0.2060029493106725, 0.0, 1.0, 38881.28373722572], 
processed observation next is [0.0, 0.782608695652174, 0.5761772853185596, 0.875, 0.0, 0.0, 0.6666666666666666, 0.5503230710032483, 0.5686676497702242, 0.0, 1.0, 0.18514897017726534], 
reward next is 0.8149, 
noisyNet noise sample is [array([1.1131843], dtype=float32), 1.1312695]. 
=============================================
[2019-04-04 13:32:51,570] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.82196860e-09 2.13286055e-10 1.22412778e-15 4.46501729e-15
 1.00000000e+00 1.14451594e-10 5.01425886e-15], sum to 1.0000
[2019-04-04 13:32:51,571] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9372
[2019-04-04 13:32:51,617] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 61.0, 146.5, 169.0, 26.0, 25.90399507582614, 0.4345066624267982, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 138000.0000, 
sim time next is 138600.0000, 
raw observation next is [-6.7, 61.0, 148.0, 106.0, 26.0, 25.86210635889546, 0.4232060104810013, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.49333333333333335, 0.11712707182320442, 0.6666666666666666, 0.655175529907955, 0.6410686701603338, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01583301], dtype=float32), -1.2752439]. 
=============================================
[2019-04-04 13:32:52,459] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0492934e-08 7.5735302e-09 1.8910769e-14 4.3933336e-13 1.0000000e+00
 3.5465209e-10 9.1684073e-14], sum to 1.0000
[2019-04-04 13:32:52,460] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5669
[2019-04-04 13:32:52,473] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.23333333333333, 68.0, 0.0, 0.0, 26.0, 23.16715631102397, -0.1421492418874739, 0.0, 1.0, 47258.32244631694], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 276000.0000, 
sim time next is 276600.0000, 
raw observation next is [-10.41666666666667, 67.5, 0.0, 0.0, 26.0, 23.0903580042391, -0.1562843895820646, 0.0, 1.0, 47407.59749602441], 
processed observation next is [1.0, 0.17391304347826086, 0.17405355493998145, 0.675, 0.0, 0.0, 0.6666666666666666, 0.42419650035325834, 0.4479052034726451, 0.0, 1.0, 0.2257504642667829], 
reward next is 0.7742, 
noisyNet noise sample is [array([-1.1071465], dtype=float32), -0.6984672]. 
=============================================
[2019-04-04 13:32:52,824] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.01611120e-09 6.21649190e-11 3.65943898e-15 1.17590425e-14
 1.00000000e+00 3.65474526e-11 3.47847315e-15], sum to 1.0000
[2019-04-04 13:32:52,824] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3561
[2019-04-04 13:32:52,892] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.1, 66.0, 0.0, 0.0, 26.0, 25.12364849276268, 0.3018580354304577, 1.0, 1.0, 34650.56219650761], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 156600.0000, 
sim time next is 157200.0000, 
raw observation next is [-8.2, 66.66666666666666, 0.0, 0.0, 26.0, 25.0344327172556, 0.2964390995483497, 1.0, 1.0, 97376.32747067169], 
processed observation next is [1.0, 0.8260869565217391, 0.23545706371191139, 0.6666666666666665, 0.0, 0.0, 0.6666666666666666, 0.5862027264379668, 0.5988130331827832, 1.0, 1.0, 0.463696797479389], 
reward next is 0.5363, 
noisyNet noise sample is [array([-0.4526987], dtype=float32), 0.033764087]. 
=============================================
[2019-04-04 13:32:53,273] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 13:32:53,274] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 13:32:53,275] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 13:32:53,275] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 13:32:53,276] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:53,276] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:53,277] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:32:54,573] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run16
[2019-04-04 13:32:54,743] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run16
[2019-04-04 13:32:54,744] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run16
[2019-04-04 13:33:44,116] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16979307], dtype=float32), 0.19800696]
[2019-04-04 13:33:44,116] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.9, 82.00000000000001, 79.33333333333333, 170.0, 26.0, 25.76541773031996, 0.5424740448154174, 1.0, 1.0, 0.0]
[2019-04-04 13:33:44,116] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 13:33:44,117] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.5663688e-10 7.4837185e-11 7.2127656e-16 6.3727053e-15 1.0000000e+00
 5.3354425e-11 9.4289918e-16], sampled 0.3668708644966222
[2019-04-04 13:33:45,781] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.16979307], dtype=float32), 0.19800696]
[2019-04-04 13:33:45,782] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.5, 77.66666666666667, 36.16666666666666, 0.0, 26.0, 25.00871621350885, 0.2638855247828268, 0.0, 1.0, 46279.63837388801]
[2019-04-04 13:33:45,782] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 13:33:45,782] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.6380966e-09 2.6659869e-10 4.8354096e-15 3.4623241e-14 1.0000000e+00
 1.0821807e-10 7.5873730e-15], sampled 0.9684873600465416
[2019-04-04 13:34:36,343] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 13:34:55,718] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6673 263409857.2690 1551.6144
[2019-04-04 13:34:58,258] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 13:34:59,281] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1500000, evaluation results [1500000.0, 7241.667346338161, 263409857.26898408, 1551.6144136320988, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 13:34:59,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.0540357e-09 1.6311155e-10 2.1758100e-15 6.0510738e-15 1.0000000e+00
 1.8839633e-10 3.7459897e-16], sum to 1.0000
[2019-04-04 13:34:59,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8744
[2019-04-04 13:34:59,454] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 61.0, 145.0, 231.9999999999999, 26.0, 25.92770600529045, 0.4455016254716599, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 137400.0000, 
sim time next is 138000.0000, 
raw observation next is [-6.700000000000001, 61.0, 146.5, 169.0, 26.0, 25.9043900322699, 0.4346097381268899, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.48833333333333334, 0.1867403314917127, 0.6666666666666666, 0.6586991693558252, 0.6448699127089633, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45497566], dtype=float32), -0.7003569]. 
=============================================
[2019-04-04 13:34:59,457] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[84.49117 ]
 [85.097725]
 [85.65809 ]
 [86.195274]
 [86.658745]], R is [[83.90917206]
 [84.07008362]
 [84.22938538]
 [84.38709259]
 [84.54322052]].
[2019-04-04 13:35:18,361] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5064517e-08 4.7279185e-09 1.3692582e-13 1.7772262e-12 1.0000000e+00
 8.4471115e-09 4.8322614e-13], sum to 1.0000
[2019-04-04 13:35:18,361] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9547
[2019-04-04 13:35:18,410] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.1, 42.83333333333334, 0.0, 0.0, 26.0, 25.12035995931175, 0.292379913358332, 0.0, 1.0, 55069.89938929815], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 418200.0000, 
sim time next is 418800.0000, 
raw observation next is [-10.2, 43.66666666666667, 0.0, 0.0, 26.0, 25.11315875277195, 0.2822728623002804, 0.0, 1.0, 38218.35190238915], 
processed observation next is [1.0, 0.8695652173913043, 0.1800554016620499, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5927632293976627, 0.5940909541000935, 0.0, 1.0, 0.1819921519161388], 
reward next is 0.8180, 
noisyNet noise sample is [array([1.0030537], dtype=float32), -0.564968]. 
=============================================
[2019-04-04 13:35:22,560] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1610825e-08 1.4607121e-10 2.0324149e-15 1.2067739e-14 1.0000000e+00
 7.5031459e-10 3.1817901e-15], sum to 1.0000
[2019-04-04 13:35:22,562] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9170
[2019-04-04 13:35:22,619] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3197921e-09 1.8863472e-10 5.9942319e-16 1.2398722e-14 1.0000000e+00
 9.3365468e-11 6.2238202e-16], sum to 1.0000
[2019-04-04 13:35:22,619] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4854
[2019-04-04 13:35:22,621] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 94.0, 0.0, 0.0, 26.0, 25.07482144544532, 0.2369662860027593, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 499800.0000, 
sim time next is 500400.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 25.03397639193323, 0.2163734678090988, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5861646993277692, 0.5721244892696996, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15588085], dtype=float32), 0.61060745]. 
=============================================
[2019-04-04 13:35:22,707] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 80.0, 135.3333333333333, 528.6666666666666, 26.0, 24.93676420521925, 0.3278864464451518, 0.0, 1.0, 47463.65194559583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 565800.0000, 
sim time next is 566400.0000, 
raw observation next is [-1.2, 80.0, 136.6666666666667, 561.8333333333334, 26.0, 24.9289562597189, 0.3402804795784267, 0.0, 1.0, 45473.46253993316], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.4555555555555557, 0.6208103130755065, 0.6666666666666666, 0.5774130216432415, 0.6134268265261422, 0.0, 1.0, 0.21654029780920553], 
reward next is 0.7835, 
noisyNet noise sample is [array([0.14558098], dtype=float32), -0.85178983]. 
=============================================
[2019-04-04 13:35:27,027] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.2882080e-09 1.3380295e-09 8.2006528e-15 1.9681435e-13 1.0000000e+00
 2.2408801e-09 1.2504740e-14], sum to 1.0000
[2019-04-04 13:35:27,027] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7940
[2019-04-04 13:35:27,098] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 25.03397639193323, 0.2163734678090988, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 500400.0000, 
sim time next is 501000.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 24.92949352611359, 0.1986937607693117, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.6666666666666666, 0.577457793842799, 0.5662312535897706, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.77143997], dtype=float32), 1.2593932]. 
=============================================
[2019-04-04 13:35:27,106] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[83.25605]
 [83.22735]
 [83.11645]
 [82.56128]
 [81.87232]], R is [[84.37677765]
 [84.53301239]
 [84.68768311]
 [84.84080505]
 [84.99240112]].
[2019-04-04 13:35:36,524] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0858420e-09 1.4457666e-10 1.5181036e-14 5.2237620e-14 1.0000000e+00
 5.3663771e-11 1.0252468e-14], sum to 1.0000
[2019-04-04 13:35:36,526] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2972
[2019-04-04 13:35:36,570] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 54.0, 83.0, 38.0, 26.0, 24.89313320801532, 0.2220593854809287, 0.0, 1.0, 41586.72512368692], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 660600.0000, 
sim time next is 661200.0000, 
raw observation next is [-0.6, 54.0, 73.66666666666667, 34.16666666666666, 26.0, 24.88214417469394, 0.2206704045038995, 0.0, 1.0, 46427.6329516197], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.24555555555555558, 0.03775322283609575, 0.6666666666666666, 0.5735120145578284, 0.5735568015012998, 0.0, 1.0, 0.22108396643628428], 
reward next is 0.7789, 
noisyNet noise sample is [array([1.1767337], dtype=float32), -0.40036753]. 
=============================================
[2019-04-04 13:35:41,356] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1408337e-09 3.3882779e-11 1.1647027e-15 2.2663424e-14 1.0000000e+00
 4.9263153e-11 3.8935786e-16], sum to 1.0000
[2019-04-04 13:35:41,358] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8768
[2019-04-04 13:35:41,400] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 25.06402759586089, 0.3060660049250876, 0.0, 1.0, 50999.98890726877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 852000.0000, 
sim time next is 852600.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 25.02617437150591, 0.2985938085926099, 0.0, 1.0, 45166.41277978431], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5855145309588258, 0.59953126953087, 0.0, 1.0, 0.215078156094211], 
reward next is 0.7849, 
noisyNet noise sample is [array([1.0731076], dtype=float32), 0.98943394]. 
=============================================
[2019-04-04 13:35:42,083] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7418871e-09 2.8000319e-11 2.3083480e-16 1.7164316e-14 1.0000000e+00
 2.0043585e-11 4.3434351e-16], sum to 1.0000
[2019-04-04 13:35:42,083] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4741
[2019-04-04 13:35:42,121] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 68.0, 120.0, 58.5, 26.0, 25.94305560447102, 0.3373113275027964, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 727200.0000, 
sim time next is 727800.0000, 
raw observation next is [-1.516666666666667, 67.66666666666667, 126.3333333333333, 61.66666666666666, 26.0, 25.93732787991429, 0.3318164891858978, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4205909510618652, 0.6766666666666667, 0.421111111111111, 0.06813996316758747, 0.6666666666666666, 0.6614439899928574, 0.6106054963952993, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.952401], dtype=float32), 0.077924974]. 
=============================================
[2019-04-04 13:35:52,302] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.73809034e-09 6.42596309e-10 2.49370311e-15 2.67811116e-14
 1.00000000e+00 1.00127746e-10 2.25242620e-15], sum to 1.0000
[2019-04-04 13:35:52,303] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4428
[2019-04-04 13:35:52,322] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.63440496241382, -0.03996504419086674, 0.0, 1.0, 41992.87390941266], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 797400.0000, 
sim time next is 798000.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.60391747283335, -0.04478558415745906, 0.0, 1.0, 42039.13517380296], 
processed observation next is [1.0, 0.21739130434782608, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.4669931227361124, 0.4850714719475136, 0.0, 1.0, 0.2001863579704903], 
reward next is 0.7998, 
noisyNet noise sample is [array([1.4647889], dtype=float32), 2.1422832]. 
=============================================
[2019-04-04 13:35:52,329] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[79.716644]
 [79.781685]
 [79.851875]
 [79.89284 ]
 [79.92344 ]], R is [[79.64903259]
 [79.65258026]
 [79.65644073]
 [79.66072845]
 [79.6654892 ]].
[2019-04-04 13:35:52,488] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3421721e-08 1.1092611e-09 9.5772626e-15 2.4532375e-13 1.0000000e+00
 5.9189853e-09 2.2158070e-14], sum to 1.0000
[2019-04-04 13:35:52,488] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5143
[2019-04-04 13:35:52,532] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.35, 73.0, 87.0, 0.0, 26.0, 25.62702874879837, 0.2882794976633045, 1.0, 1.0, 32934.35359470722], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 815400.0000, 
sim time next is 816000.0000, 
raw observation next is [-5.066666666666666, 72.33333333333333, 90.83333333333333, 0.0, 26.0, 25.58063502998993, 0.2901854604513605, 1.0, 1.0, 23044.15867594224], 
processed observation next is [1.0, 0.43478260869565216, 0.32225300092336107, 0.7233333333333333, 0.30277777777777776, 0.0, 0.6666666666666666, 0.6317195858324943, 0.5967284868171202, 1.0, 1.0, 0.10973408893305828], 
reward next is 0.8903, 
noisyNet noise sample is [array([2.61804], dtype=float32), -1.2115978]. 
=============================================
[2019-04-04 13:35:52,544] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.95429 ]
 [82.80644 ]
 [82.81018 ]
 [83.058426]
 [83.19503 ]], R is [[83.11914062]
 [83.13111877]
 [83.08315277]
 [83.25231934]
 [83.4197998 ]].
[2019-04-04 13:35:53,109] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8166682e-09 3.9686737e-10 7.2176376e-16 1.5997102e-14 1.0000000e+00
 9.4996337e-11 1.7181700e-15], sum to 1.0000
[2019-04-04 13:35:53,110] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5648
[2019-04-04 13:35:53,128] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.533333333333333, 78.0, 0.0, 0.0, 26.0, 24.57566399822124, 0.1843559917088464, 0.0, 1.0, 39268.00146015653], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 876000.0000, 
sim time next is 876600.0000, 
raw observation next is [-1.45, 77.5, 0.0, 0.0, 26.0, 24.61344081981355, 0.1847178269616885, 0.0, 1.0, 39216.82556991155], 
processed observation next is [1.0, 0.13043478260869565, 0.422437673130194, 0.775, 0.0, 0.0, 0.6666666666666666, 0.5511200683177959, 0.5615726089872295, 0.0, 1.0, 0.18674678842815023], 
reward next is 0.8133, 
noisyNet noise sample is [array([1.5858741], dtype=float32), 0.52747947]. 
=============================================
[2019-04-04 13:35:58,415] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5314386e-10 6.2747564e-11 6.3864001e-17 2.9868268e-15 1.0000000e+00
 1.2919137e-11 4.2194483e-16], sum to 1.0000
[2019-04-04 13:35:58,418] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5740
[2019-04-04 13:35:58,431] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.200000000000001, 83.0, 0.0, 0.0, 26.0, 25.70443283018223, 0.4640435899644521, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 973200.0000, 
sim time next is 973800.0000, 
raw observation next is [9.4, 83.0, 0.0, 0.0, 26.0, 25.62576375874747, 0.449001770363239, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7229916897506927, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6354803132289559, 0.6496672567877463, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6746316], dtype=float32), -0.70622075]. 
=============================================
[2019-04-04 13:36:07,799] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7665518e-10 2.6940230e-11 1.0260204e-16 2.4747499e-15 1.0000000e+00
 1.7322638e-11 1.3781357e-16], sum to 1.0000
[2019-04-04 13:36:07,800] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4592
[2019-04-04 13:36:07,812] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.91666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.63865565983128, 0.6291235313672331, 0.0, 1.0, 21317.35986908592], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1133400.0000, 
sim time next is 1134000.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.64374415641641, 0.6281131311378269, 0.0, 1.0, 20037.64428129068], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6369786797013676, 0.709371043712609, 0.0, 1.0, 0.09541735372043181], 
reward next is 0.9046, 
noisyNet noise sample is [array([-0.7933497], dtype=float32), -0.46502757]. 
=============================================
[2019-04-04 13:36:07,821] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[88.99255]
 [88.97162]
 [88.87942]
 [88.35815]
 [88.19185]], R is [[89.0118866 ]
 [89.02025604]
 [89.01617432]
 [88.99503326]
 [88.95833588]].
[2019-04-04 13:36:08,462] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6156556e-10 2.2163979e-11 2.6248955e-17 1.0175240e-15 1.0000000e+00
 1.5003851e-11 1.2917331e-16], sum to 1.0000
[2019-04-04 13:36:08,466] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0280
[2019-04-04 13:36:08,475] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 95.0, 92.0, 0.0, 26.0, 25.63836966361508, 0.4707148673578399, 1.0, 1.0, 18680.56354532288], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1425000.0000, 
sim time next is 1425600.0000, 
raw observation next is [0.0, 95.0, 93.0, 0.0, 26.0, 25.66879744707587, 0.476390560878333, 1.0, 1.0, 18681.24447575885], 
processed observation next is [1.0, 0.5217391304347826, 0.46260387811634357, 0.95, 0.31, 0.0, 0.6666666666666666, 0.6390664539229892, 0.658796853626111, 1.0, 1.0, 0.08895830702742309], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.9571741], dtype=float32), -0.49110308]. 
=============================================
[2019-04-04 13:36:10,887] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1625938e-08 2.0331190e-09 1.5872943e-14 2.3892520e-13 1.0000000e+00
 4.8624649e-10 4.6477384e-15], sum to 1.0000
[2019-04-04 13:36:10,888] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7879
[2019-04-04 13:36:10,924] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.9545876e-10 3.6354659e-11 6.1768673e-17 4.2324763e-15 1.0000000e+00
 1.9453560e-12 1.8796110e-16], sum to 1.0000
[2019-04-04 13:36:10,932] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.5, 100.0, 83.66666666666667, 0.0, 26.0, 24.45726140577369, 0.4259845501270266, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1248600.0000, 
sim time next is 1249200.0000, 
raw observation next is [14.4, 100.0, 86.5, 0.0, 26.0, 24.70938401756385, 0.4496120801522759, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 1.0, 0.28833333333333333, 0.0, 0.6666666666666666, 0.5591153347969874, 0.649870693384092, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0540932], dtype=float32), -0.41172674]. 
=============================================
[2019-04-04 13:36:10,933] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8558
[2019-04-04 13:36:10,958] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.9, 98.66666666666667, 0.0, 0.0, 26.0, 24.58417628438773, 0.4273594919843051, 0.0, 1.0, 45616.45569043479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1272000.0000, 
sim time next is 1272600.0000, 
raw observation next is [10.25, 98.0, 0.0, 0.0, 26.0, 24.59008738072372, 0.433018660174218, 0.0, 1.0, 45080.17363173732], 
processed observation next is [0.0, 0.7391304347826086, 0.7465373961218837, 0.98, 0.0, 0.0, 0.6666666666666666, 0.5491739483936433, 0.644339553391406, 0.0, 1.0, 0.21466749348446343], 
reward next is 0.7853, 
noisyNet noise sample is [array([-1.7788845], dtype=float32), 1.0796428]. 
=============================================
[2019-04-04 13:36:14,924] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.2043384e-11 1.0523598e-11 2.2627842e-17 1.0749614e-15 1.0000000e+00
 2.1282432e-11 1.0999995e-16], sum to 1.0000
[2019-04-04 13:36:14,924] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7033
[2019-04-04 13:36:14,958] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6, 92.0, 27.0, 0.0, 26.0, 26.06283110054968, 0.5932425739570528, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1327800.0000, 
sim time next is 1328400.0000, 
raw observation next is [0.5, 92.0, 31.5, 0.0, 26.0, 26.07973968565456, 0.5890791945889431, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.105, 0.0, 0.6666666666666666, 0.6733116404712133, 0.6963597315296477, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.69575214], dtype=float32), 0.7333685]. 
=============================================
[2019-04-04 13:36:18,768] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.4044227e-10 1.3712780e-10 1.3555997e-17 6.0082652e-16 1.0000000e+00
 8.7674911e-12 1.9272764e-17], sum to 1.0000
[2019-04-04 13:36:18,780] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7166
[2019-04-04 13:36:18,793] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.31165131804912, 0.4522536538331909, 0.0, 1.0, 38680.81799933822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1399800.0000, 
sim time next is 1400400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.26006022470771, 0.4408453673241695, 0.0, 1.0, 38630.41193281801], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6050050187256426, 0.6469484557747232, 0.0, 1.0, 0.18395434253722862], 
reward next is 0.8160, 
noisyNet noise sample is [array([0.21195239], dtype=float32), 0.15787758]. 
=============================================
[2019-04-04 13:36:24,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0634505e-09 4.8260767e-11 5.5972652e-16 7.5825703e-15 1.0000000e+00
 4.2956284e-11 3.1740421e-16], sum to 1.0000
[2019-04-04 13:36:24,619] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8839
[2019-04-04 13:36:24,660] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.35, 84.5, 30.0, 0.0, 26.0, 24.79973380805919, 0.4167432841003588, 1.0, 1.0, 196399.3894200014], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1701000.0000, 
sim time next is 1701600.0000, 
raw observation next is [1.266666666666667, 85.66666666666667, 25.16666666666667, 0.0, 26.0, 25.09917823841828, 0.4803341427273746, 1.0, 1.0, 6231.073815241366], 
processed observation next is [1.0, 0.6956521739130435, 0.4976915974145891, 0.8566666666666667, 0.0838888888888889, 0.0, 0.6666666666666666, 0.5915981865348566, 0.6601113809091249, 1.0, 1.0, 0.029671780072577935], 
reward next is 0.9703, 
noisyNet noise sample is [array([1.3962829], dtype=float32), 2.0499928]. 
=============================================
[2019-04-04 13:36:26,184] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.8519735e-10 4.7574125e-11 5.7875975e-17 1.2606224e-15 1.0000000e+00
 8.4398035e-12 1.2664732e-16], sum to 1.0000
[2019-04-04 13:36:26,184] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7228
[2019-04-04 13:36:26,198] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.333333333333334, 74.66666666666667, 0.0, 0.0, 26.0, 26.06781009380366, 0.7111484801124907, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1545600.0000, 
sim time next is 1546200.0000, 
raw observation next is [7.15, 75.0, 0.0, 0.0, 26.0, 26.14492530501775, 0.7149981858788759, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6606648199445985, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6787437754181459, 0.738332728626292, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8953583], dtype=float32), 2.380898]. 
=============================================
[2019-04-04 13:36:26,913] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9703849e-10 1.3027028e-10 5.3970859e-16 9.4493047e-15 1.0000000e+00
 4.2603952e-11 4.6959072e-16], sum to 1.0000
[2019-04-04 13:36:26,915] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1207
[2019-04-04 13:36:26,929] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.63333333333333, 56.33333333333334, 0.0, 0.0, 26.0, 26.76644283210832, 0.7373382951175461, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1617600.0000, 
sim time next is 1618200.0000, 
raw observation next is [11.35, 57.5, 0.0, 0.0, 26.0, 26.80886280212101, 0.7371826134265312, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7770083102493075, 0.575, 0.0, 0.0, 0.6666666666666666, 0.7340719001767507, 0.7457275378088437, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19855729], dtype=float32), 0.6133291]. 
=============================================
[2019-04-04 13:36:34,693] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.08178619e-10 3.46762966e-11 1.05666937e-16 6.61191970e-15
 1.00000000e+00 1.34637640e-11 5.90316902e-16], sum to 1.0000
[2019-04-04 13:36:34,697] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7141
[2019-04-04 13:36:34,710] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.37146142841024, 0.4634460614506387, 0.0, 1.0, 43979.22878230816], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1731600.0000, 
sim time next is 1732200.0000, 
raw observation next is [0.45, 91.83333333333334, 0.0, 0.0, 26.0, 25.35809563101984, 0.4595065076756915, 0.0, 1.0, 43608.27314715306], 
processed observation next is [0.0, 0.043478260869565216, 0.47506925207756234, 0.9183333333333334, 0.0, 0.0, 0.6666666666666666, 0.61317463591832, 0.6531688358918971, 0.0, 1.0, 0.2076584435578717], 
reward next is 0.7923, 
noisyNet noise sample is [array([-0.7060394], dtype=float32), 0.44357914]. 
=============================================
[2019-04-04 13:36:47,329] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3245712e-09 2.3215272e-09 1.3652857e-14 4.9874802e-13 1.0000000e+00
 3.8373371e-10 7.0261730e-15], sum to 1.0000
[2019-04-04 13:36:47,329] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5453
[2019-04-04 13:36:47,349] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.8, 78.0, 0.0, 0.0, 26.0, 24.02640381164913, 0.009417790715065433, 0.0, 1.0, 44998.04419484737], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1908000.0000, 
sim time next is 1908600.0000, 
raw observation next is [-7.9, 78.0, 0.0, 0.0, 26.0, 24.00397779681917, -0.0001618426589157831, 0.0, 1.0, 44927.05662071559], 
processed observation next is [1.0, 0.08695652173913043, 0.24376731301939059, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5003314830682642, 0.49994605244702806, 0.0, 1.0, 0.21393836486055046], 
reward next is 0.7861, 
noisyNet noise sample is [array([0.4277244], dtype=float32), -0.41833648]. 
=============================================
[2019-04-04 13:36:51,420] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.8253394e-09 1.2991347e-10 1.9689760e-14 2.0939672e-13 1.0000000e+00
 4.4620999e-10 4.6813330e-15], sum to 1.0000
[2019-04-04 13:36:51,423] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1446
[2019-04-04 13:36:51,442] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 62.0, 105.6666666666667, 0.0, 26.0, 25.75716312992649, 0.3361752346794511, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1953600.0000, 
sim time next is 1954200.0000, 
raw observation next is [-2.9, 62.0, 99.33333333333333, 0.0, 26.0, 25.73518376373348, 0.3298230594130844, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.38227146814404434, 0.62, 0.3311111111111111, 0.0, 0.6666666666666666, 0.6445986469777901, 0.6099410198043614, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9672658], dtype=float32), 0.55275726]. 
=============================================
[2019-04-04 13:37:01,424] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.6587342e-09 1.4743493e-10 3.1376993e-15 1.4110158e-13 1.0000000e+00
 2.0009829e-10 2.9619289e-14], sum to 1.0000
[2019-04-04 13:37:01,425] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5894
[2019-04-04 13:37:01,458] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.19878066841607, 0.0954996088496922, 0.0, 1.0, 43475.66202092754], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2091600.0000, 
sim time next is 2092200.0000, 
raw observation next is [-6.283333333333333, 86.33333333333333, 0.0, 0.0, 26.0, 24.11543423510931, 0.1041199567838592, 0.0, 1.0, 43822.83469925112], 
processed observation next is [1.0, 0.21739130434782608, 0.288550323176362, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5096195195924423, 0.5347066522612863, 0.0, 1.0, 0.20868016523452915], 
reward next is 0.7913, 
noisyNet noise sample is [array([-0.20324594], dtype=float32), -1.0751722]. 
=============================================
[2019-04-04 13:37:04,563] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.41467895e-08 7.74702302e-10 5.18249757e-14 9.00507899e-14
 1.00000000e+00 1.88443505e-09 9.61698423e-15], sum to 1.0000
[2019-04-04 13:37:04,563] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8601
[2019-04-04 13:37:04,608] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.22659273341205, 0.4172569899598983, 0.0, 1.0, 46124.5342409372], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2148600.0000, 
sim time next is 2149200.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.25893457831888, 0.4173763371289906, 0.0, 1.0, 43960.33239639812], 
processed observation next is [1.0, 0.9130434782608695, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6049112148599066, 0.6391254457096635, 0.0, 1.0, 0.20933491617332436], 
reward next is 0.7907, 
noisyNet noise sample is [array([1.4796481], dtype=float32), 1.0267361]. 
=============================================
[2019-04-04 13:37:09,918] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.1053659e-09 5.7151922e-11 1.3073174e-14 4.4523054e-14 1.0000000e+00
 1.5275639e-10 6.0350754e-15], sum to 1.0000
[2019-04-04 13:37:09,920] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9596
[2019-04-04 13:37:09,959] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.566666666666666, 66.0, 130.6666666666667, 0.0, 26.0, 25.51468982528215, 0.3166446529869447, 1.0, 1.0, 24442.95848047459], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2205600.0000, 
sim time next is 2206200.0000, 
raw observation next is [-3.483333333333333, 65.5, 133.3333333333333, 0.0, 26.0, 25.35679571497867, 0.3138157259517917, 1.0, 1.0, 24404.38718391196], 
processed observation next is [1.0, 0.5217391304347826, 0.3661126500461681, 0.655, 0.4444444444444443, 0.0, 0.6666666666666666, 0.6130663095815558, 0.6046052419839306, 1.0, 1.0, 0.11621136754243791], 
reward next is 0.8838, 
noisyNet noise sample is [array([-0.2530244], dtype=float32), 2.6832724]. 
=============================================
[2019-04-04 13:37:15,166] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.2725514e-09 1.8289741e-09 3.0093802e-15 1.5791630e-13 1.0000000e+00
 9.1386017e-11 9.6727978e-15], sum to 1.0000
[2019-04-04 13:37:15,166] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4558
[2019-04-04 13:37:15,205] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.3, 86.83333333333333, 0.0, 0.0, 26.0, 24.05852964535086, 0.07281397093592464, 0.0, 1.0, 43638.49779605116], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2260200.0000, 
sim time next is 2260800.0000, 
raw observation next is [-8.4, 87.0, 0.0, 0.0, 26.0, 24.05992540001927, 0.06347713894111033, 0.0, 1.0, 43610.77930646755], 
processed observation next is [1.0, 0.17391304347826086, 0.2299168975069252, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5049937833349393, 0.5211590463137035, 0.0, 1.0, 0.20767037764984547], 
reward next is 0.7923, 
noisyNet noise sample is [array([0.5901412], dtype=float32), -1.3881364]. 
=============================================
[2019-04-04 13:37:17,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3036015e-09 4.9874116e-10 1.1861989e-14 3.4216540e-14 1.0000000e+00
 2.7794306e-10 1.2472536e-14], sum to 1.0000
[2019-04-04 13:37:17,838] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2607
[2019-04-04 13:37:17,886] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.1, 48.16666666666667, 223.6666666666667, 384.6666666666666, 26.0, 24.94396065833581, 0.3300369403797581, 0.0, 1.0, 26438.57615188583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2376600.0000, 
sim time next is 2377200.0000, 
raw observation next is [-1.0, 49.33333333333334, 237.8333333333333, 404.3333333333334, 26.0, 24.97837100614647, 0.3381149771782591, 0.0, 1.0, 18729.02135934286], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.4933333333333334, 0.7927777777777776, 0.44677716390423583, 0.6666666666666666, 0.5815309171788726, 0.6127049923927531, 0.0, 1.0, 0.08918581599687075], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.3017322], dtype=float32), 0.18683366]. 
=============================================
[2019-04-04 13:37:19,587] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.24549629e-07 8.48980086e-09 1.04094366e-13 1.05546886e-12
 9.99999881e-01 5.49282486e-09 6.91767065e-13], sum to 1.0000
[2019-04-04 13:37:19,593] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9288
[2019-04-04 13:37:19,631] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.9, 27.5, 87.0, 832.0, 26.0, 24.96753887336582, 0.271784556789972, 0.0, 1.0, 18709.64948181378], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2467800.0000, 
sim time next is 2468400.0000, 
raw observation next is [2.0, 27.33333333333334, 85.5, 824.0, 26.0, 24.96007332535531, 0.2733529337645252, 0.0, 1.0, 18708.74471952302], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.2733333333333334, 0.285, 0.9104972375690608, 0.6666666666666666, 0.5800061104462758, 0.5911176445881751, 0.0, 1.0, 0.08908926056915724], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.2972001], dtype=float32), -1.6772978]. 
=============================================
[2019-04-04 13:37:20,796] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.5970591e-09 1.7071373e-09 5.6614351e-14 2.1392865e-13 1.0000000e+00
 9.7408019e-11 6.4074444e-14], sum to 1.0000
[2019-04-04 13:37:20,796] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7283
[2019-04-04 13:37:20,810] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 69.0, 7.833333333333332, 0.0, 26.0, 23.97076868943522, 0.05389492235578545, 0.0, 1.0, 41697.63096289954], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2360400.0000, 
sim time next is 2361000.0000, 
raw observation next is [-3.4, 69.0, 13.66666666666666, 0.0, 26.0, 23.93895299580001, 0.04874009040984626, 0.0, 1.0, 41787.86495943606], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.04555555555555554, 0.0, 0.6666666666666666, 0.4949127496500007, 0.5162466968032821, 0.0, 1.0, 0.1989898331401717], 
reward next is 0.8010, 
noisyNet noise sample is [array([1.5106894], dtype=float32), -0.100995935]. 
=============================================
[2019-04-04 13:37:20,814] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[75.32643 ]
 [75.329506]
 [75.37219 ]
 [75.41418 ]
 [75.43009 ]], R is [[75.39590454]
 [75.44338226]
 [75.49082184]
 [75.53820038]
 [75.58548737]].
[2019-04-04 13:37:28,457] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4233263e-08 1.9713862e-09 7.2546645e-13 5.7243067e-13 1.0000000e+00
 2.2323270e-09 4.6946431e-13], sum to 1.0000
[2019-04-04 13:37:28,458] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9755
[2019-04-04 13:37:28,484] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.7, 29.0, 0.0, 0.0, 26.0, 25.2399406698794, 0.2843621973500608, 0.0, 1.0, 42259.38276239834], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2491200.0000, 
sim time next is 2491800.0000, 
raw observation next is [-0.7833333333333333, 30.33333333333334, 0.0, 0.0, 26.0, 25.28946653986403, 0.2866569717933605, 0.0, 1.0, 40744.81278979175], 
processed observation next is [0.0, 0.8695652173913043, 0.44090489381348114, 0.3033333333333334, 0.0, 0.0, 0.6666666666666666, 0.6074555449886692, 0.5955523239311201, 0.0, 1.0, 0.19402291804662739], 
reward next is 0.8060, 
noisyNet noise sample is [array([0.27570784], dtype=float32), -0.23027502]. 
=============================================
[2019-04-04 13:37:30,718] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0086582e-08 6.5283645e-10 2.6225262e-15 7.0728064e-14 1.0000000e+00
 3.8991643e-10 5.8662444e-15], sum to 1.0000
[2019-04-04 13:37:30,720] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7824
[2019-04-04 13:37:30,735] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.41758326437765, 0.4224526536538475, 0.0, 1.0, 23610.06214076009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2581800.0000, 
sim time next is 2582400.0000, 
raw observation next is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.4326162201103, 0.418170958590986, 0.0, 1.0, 24187.10697979028], 
processed observation next is [1.0, 0.9130434782608695, 0.38504155124653744, 0.56, 0.0, 0.0, 0.6666666666666666, 0.6193846850091917, 0.6393903195303287, 0.0, 1.0, 0.11517669990376322], 
reward next is 0.8848, 
noisyNet noise sample is [array([-0.7987748], dtype=float32), -1.8637873]. 
=============================================
[2019-04-04 13:37:32,596] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9823679e-08 4.0118744e-09 1.9043401e-13 4.3849286e-13 1.0000000e+00
 3.0809901e-09 1.4026090e-13], sum to 1.0000
[2019-04-04 13:37:32,596] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9648
[2019-04-04 13:37:32,630] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.7833333333333333, 30.33333333333334, 0.0, 0.0, 26.0, 25.28946654023552, 0.2866569717867968, 0.0, 1.0, 40744.81278848728], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2491800.0000, 
sim time next is 2492400.0000, 
raw observation next is [-0.8666666666666667, 31.66666666666667, 0.0, 0.0, 26.0, 25.30695946473351, 0.2863798082047632, 0.0, 1.0, 40354.34948402391], 
processed observation next is [0.0, 0.8695652173913043, 0.4385964912280702, 0.3166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6089132887277925, 0.5954599360682544, 0.0, 1.0, 0.1921635689715424], 
reward next is 0.8078, 
noisyNet noise sample is [array([1.8330055], dtype=float32), -2.8995528]. 
=============================================
[2019-04-04 13:37:39,566] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0783702e-09 1.3387742e-10 3.1970471e-15 1.2690795e-13 1.0000000e+00
 2.3372318e-10 2.0016352e-14], sum to 1.0000
[2019-04-04 13:37:39,568] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4359
[2019-04-04 13:37:39,596] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 56.5, 106.6666666666667, 769.6666666666666, 26.0, 26.38522472032174, 0.6155653028316338, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2728200.0000, 
sim time next is 2728800.0000, 
raw observation next is [-4.8, 56.0, 105.5, 760.5, 26.0, 26.5075157747243, 0.6300000059182058, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3296398891966759, 0.56, 0.3516666666666667, 0.8403314917127072, 0.6666666666666666, 0.7089596478936917, 0.7100000019727353, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0686115], dtype=float32), 0.48955122]. 
=============================================
[2019-04-04 13:37:41,514] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.5003272e-10 5.2371907e-10 3.6176472e-15 9.8504163e-15 1.0000000e+00
 1.1866132e-10 1.6699449e-15], sum to 1.0000
[2019-04-04 13:37:41,515] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6652
[2019-04-04 13:37:41,538] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 64.0, 0.0, 0.0, 26.0, 25.41090260166997, 0.4575863757354614, 0.0, 1.0, 33180.00382094669], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2754000.0000, 
sim time next is 2754600.0000, 
raw observation next is [-6.0, 63.16666666666667, 0.0, 0.0, 26.0, 25.42311765631074, 0.4510484299141987, 0.0, 1.0, 35075.26884395409], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.6316666666666667, 0.0, 0.0, 0.6666666666666666, 0.618593138025895, 0.6503494766380662, 0.0, 1.0, 0.1670250897331147], 
reward next is 0.8330, 
noisyNet noise sample is [array([0.53625757], dtype=float32), 1.1311374]. 
=============================================
[2019-04-04 13:37:54,948] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0131233e-09 7.5473683e-10 2.6006984e-15 4.1087969e-14 1.0000000e+00
 1.8286103e-10 2.0016010e-14], sum to 1.0000
[2019-04-04 13:37:54,948] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1479
[2019-04-04 13:37:54,992] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 56.5, 99.0, 635.0, 26.0, 25.33916211697487, 0.3257630532219768, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3058200.0000, 
sim time next is 3058800.0000, 
raw observation next is [-4.666666666666666, 55.66666666666667, 100.1666666666667, 655.6666666666667, 26.0, 25.31958303626982, 0.3236640304274999, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.33333333333333337, 0.5566666666666668, 0.333888888888889, 0.7244935543278086, 0.6666666666666666, 0.6099652530224849, 0.6078880101425, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5946209], dtype=float32), -0.77215457]. 
=============================================
[2019-04-04 13:37:59,269] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.8958834e-09 7.7251994e-10 4.7220283e-15 1.6966287e-14 1.0000000e+00
 3.4617021e-11 4.9936458e-14], sum to 1.0000
[2019-04-04 13:37:59,271] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7018
[2019-04-04 13:37:59,349] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.666666666666667, 69.0, 174.0, 42.0, 26.0, 24.84692871960326, 0.3061027346643935, 0.0, 1.0, 97469.73404623813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2974800.0000, 
sim time next is 2975400.0000, 
raw observation next is [-3.5, 68.0, 178.0, 24.0, 26.0, 24.88819128498533, 0.3160545643893864, 0.0, 1.0, 43768.29940431967], 
processed observation next is [0.0, 0.43478260869565216, 0.36565096952908593, 0.68, 0.5933333333333334, 0.026519337016574586, 0.6666666666666666, 0.574015940415444, 0.6053515214631288, 0.0, 1.0, 0.20842047335390318], 
reward next is 0.7916, 
noisyNet noise sample is [array([-0.2843366], dtype=float32), 0.26586756]. 
=============================================
[2019-04-04 13:38:01,449] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4554734e-08 4.2775801e-09 5.4420704e-13 1.4654434e-12 1.0000000e+00
 1.6172873e-09 3.8707487e-13], sum to 1.0000
[2019-04-04 13:38:01,449] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3601
[2019-04-04 13:38:01,464] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8333333333333334, 39.83333333333334, 75.0, 610.6666666666667, 26.0, 25.20658056454624, 0.3728260397285167, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3081000.0000, 
sim time next is 3081600.0000, 
raw observation next is [1.0, 40.0, 70.5, 579.5, 26.0, 25.20857039812794, 0.3657771090882096, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.4903047091412743, 0.4, 0.235, 0.6403314917127072, 0.6666666666666666, 0.600714199843995, 0.6219257030294032, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7779161], dtype=float32), 0.8438005]. 
=============================================
[2019-04-04 13:38:01,884] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6492240e-08 2.7709032e-10 6.1028649e-14 1.4272536e-13 1.0000000e+00
 2.1095174e-09 7.8899547e-14], sum to 1.0000
[2019-04-04 13:38:01,897] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1865
[2019-04-04 13:38:01,943] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 60.66666666666666, 85.66666666666667, 405.0, 26.0, 24.93314892956775, 0.2891828606275618, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3055200.0000, 
sim time next is 3055800.0000, 
raw observation next is [-6.0, 59.83333333333334, 88.33333333333334, 451.0, 26.0, 25.29867911727391, 0.3167370158206188, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.5983333333333334, 0.29444444444444445, 0.4983425414364641, 0.6666666666666666, 0.6082232597728258, 0.6055790052735396, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0801612], dtype=float32), -0.2846617]. 
=============================================
[2019-04-04 13:38:05,418] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9818866e-08 7.0036084e-09 3.3400010e-14 1.9029819e-13 9.9999988e-01
 4.2878370e-10 6.0850108e-14], sum to 1.0000
[2019-04-04 13:38:05,420] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2300
[2019-04-04 13:38:05,460] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 54.0, 103.6666666666667, 717.6666666666667, 26.0, 25.20084722036248, 0.3180162870666377, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3060600.0000, 
sim time next is 3061200.0000, 
raw observation next is [-4.0, 54.00000000000001, 104.8333333333333, 738.3333333333333, 26.0, 25.17942165380774, 0.3142890382371964, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.34944444444444434, 0.8158379373848986, 0.6666666666666666, 0.5982851378173116, 0.6047630127457321, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.71949613], dtype=float32), -1.3678762]. 
=============================================
[2019-04-04 13:38:07,279] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5405228e-10 4.8289289e-12 1.2201491e-18 3.2433666e-17 1.0000000e+00
 1.6794151e-12 4.8532721e-18], sum to 1.0000
[2019-04-04 13:38:07,279] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0378
[2019-04-04 13:38:07,316] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.833333333333334, 100.0, 88.33333333333334, 477.0, 26.0, 26.07161792342535, 0.5026514803311587, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3142200.0000, 
sim time next is 3142800.0000, 
raw observation next is [7.0, 100.0, 91.0, 519.5, 26.0, 26.21137574347621, 0.5173822369116184, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.30333333333333334, 0.5740331491712707, 0.6666666666666666, 0.6842813119563509, 0.6724607456372061, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-3.6309583], dtype=float32), 0.62149173]. 
=============================================
[2019-04-04 13:38:07,341] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6048671e-09 9.0968989e-11 1.6829197e-16 5.1775744e-15 1.0000000e+00
 1.7151719e-10 4.9294798e-16], sum to 1.0000
[2019-04-04 13:38:07,342] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3779
[2019-04-04 13:38:07,355] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.45856657317333, 0.3090316015896832, 0.0, 1.0, 18760.79931049813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3129000.0000, 
sim time next is 3129600.0000, 
raw observation next is [3.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.4501431234014, 0.3202346826854798, 0.0, 1.0, 28877.47719290327], 
processed observation next is [1.0, 0.21739130434782608, 0.5549399815327793, 1.0, 0.0, 0.0, 0.6666666666666666, 0.62084526028345, 0.6067448942284933, 0.0, 1.0, 0.13751179615668224], 
reward next is 0.8625, 
noisyNet noise sample is [array([-0.9605137], dtype=float32), -0.5102304]. 
=============================================
[2019-04-04 13:38:12,551] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2068950e-08 7.9455154e-10 3.1197148e-14 6.3532174e-14 1.0000000e+00
 7.5256767e-10 4.0324528e-15], sum to 1.0000
[2019-04-04 13:38:12,552] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5898
[2019-04-04 13:38:12,606] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.72250047763334, 0.4709143202019426, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3262200.0000, 
sim time next is 3262800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.15043169609397, 0.4829246943980998, 1.0, 1.0, 196217.9094192962], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5958693080078309, 0.6609748981326999, 1.0, 1.0, 0.9343709972347438], 
reward next is 0.0656, 
noisyNet noise sample is [array([0.709102], dtype=float32), 0.58858603]. 
=============================================
[2019-04-04 13:38:17,339] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3062971e-09 1.4787236e-09 6.4821543e-15 6.8514083e-14 1.0000000e+00
 4.3126899e-10 3.7296085e-15], sum to 1.0000
[2019-04-04 13:38:17,347] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3210
[2019-04-04 13:38:17,370] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 64.0, 113.5, 769.0, 26.0, 26.37742800266834, 0.5952196809961973, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3322800.0000, 
sim time next is 3323400.0000, 
raw observation next is [-6.833333333333334, 62.33333333333334, 114.3333333333333, 778.6666666666667, 26.0, 26.3765146874999, 0.5983824960098195, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.27331486611265005, 0.6233333333333334, 0.381111111111111, 0.8604051565377533, 0.6666666666666666, 0.6980428906249916, 0.6994608320032731, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8602393], dtype=float32), 1.8099005]. 
=============================================
[2019-04-04 13:38:20,454] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.5144025e-09 1.2440392e-10 1.4703906e-16 4.1817144e-15 1.0000000e+00
 7.2735949e-12 2.9393189e-16], sum to 1.0000
[2019-04-04 13:38:20,455] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0771
[2019-04-04 13:38:20,485] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 58.0, 95.0, 579.3333333333334, 26.0, 26.19187614670055, 0.5225366766017524, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3402600.0000, 
sim time next is 3403200.0000, 
raw observation next is [-1.110223024625157e-16, 56.00000000000001, 97.0, 618.6666666666667, 26.0, 26.2608781830534, 0.5426670186051482, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.56, 0.3233333333333333, 0.6836095764272561, 0.6666666666666666, 0.68840651525445, 0.6808890062017161, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.398031], dtype=float32), 0.10202493]. 
=============================================
[2019-04-04 13:38:24,621] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0947252e-08 3.4322254e-09 1.2496544e-13 2.2535371e-13 1.0000000e+00
 1.3426311e-09 1.5169929e-13], sum to 1.0000
[2019-04-04 13:38:24,621] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8374
[2019-04-04 13:38:24,637] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.50920031367377, 0.369243914549107, 0.0, 1.0, 24864.82843023255], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3636000.0000, 
sim time next is 3636600.0000, 
raw observation next is [8.866666666666667, 25.33333333333334, 0.0, 0.0, 26.0, 25.51340192289777, 0.3670411789018859, 0.0, 1.0, 25407.10768437139], 
processed observation next is [0.0, 0.08695652173913043, 0.7082179132040629, 0.2533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6261168269081475, 0.622347059633962, 0.0, 1.0, 0.12098622706843519], 
reward next is 0.8790, 
noisyNet noise sample is [array([-0.20398371], dtype=float32), -0.102210864]. 
=============================================
[2019-04-04 13:38:24,836] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.0964550e-10 4.3133955e-11 1.9190241e-16 2.9435843e-15 1.0000000e+00
 1.4779721e-11 4.6901072e-16], sum to 1.0000
[2019-04-04 13:38:24,838] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9506
[2019-04-04 13:38:24,855] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23990498061263, 0.3792511575565634, 0.0, 1.0, 41600.04261423077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3480000.0000, 
sim time next is 3480600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.25566189036813, 0.3641922009399656, 0.0, 1.0, 45836.98058494465], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6046384908640109, 0.6213974003133219, 0.0, 1.0, 0.21827133611878405], 
reward next is 0.7817, 
noisyNet noise sample is [array([-1.6190661], dtype=float32), 1.3443259]. 
=============================================
[2019-04-04 13:38:26,601] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7815319e-09 8.1240148e-10 5.3674204e-15 1.1633672e-13 1.0000000e+00
 6.5696448e-11 2.2442610e-15], sum to 1.0000
[2019-04-04 13:38:26,603] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6309
[2019-04-04 13:38:26,617] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.0, 27.33333333333333, 102.6666666666667, 679.6666666666666, 26.0, 25.69897293037167, 0.4654153731585529, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3663600.0000, 
sim time next is 3664200.0000, 
raw observation next is [11.0, 27.66666666666667, 104.3333333333333, 696.3333333333334, 26.0, 25.6826578793462, 0.4661633076261403, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.2766666666666667, 0.3477777777777777, 0.7694290976058932, 0.6666666666666666, 0.6402214899455165, 0.6553877692087134, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.50460565], dtype=float32), -1.3158009]. 
=============================================
[2019-04-04 13:38:27,775] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.4901908e-09 1.8431501e-09 7.6760717e-13 8.4760801e-13 1.0000000e+00
 8.7471047e-10 4.7851436e-14], sum to 1.0000
[2019-04-04 13:38:27,783] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7011
[2019-04-04 13:38:27,798] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.666666666666667, 68.33333333333333, 0.0, 0.0, 26.0, 24.75280831963537, 0.2842567937110528, 0.0, 1.0, 40860.1343417923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3562800.0000, 
sim time next is 3563400.0000, 
raw observation next is [-5.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.71225248931748, 0.2728203898809609, 0.0, 1.0, 40856.06487943286], 
processed observation next is [0.0, 0.21739130434782608, 0.30101569713758086, 0.6916666666666668, 0.0, 0.0, 0.6666666666666666, 0.5593543741097898, 0.5909401299603203, 0.0, 1.0, 0.19455268990206123], 
reward next is 0.8054, 
noisyNet noise sample is [array([-1.452347], dtype=float32), 1.0282583]. 
=============================================
[2019-04-04 13:38:33,674] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1489598e-08 2.8956806e-09 3.8194812e-15 5.0207692e-14 1.0000000e+00
 3.6513328e-10 1.6633433e-14], sum to 1.0000
[2019-04-04 13:38:33,677] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2298
[2019-04-04 13:38:33,691] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 69.66666666666667, 0.0, 0.0, 26.0, 25.43213136401057, 0.4097778433212509, 0.0, 1.0, 33758.52328124882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3709200.0000, 
sim time next is 3709800.0000, 
raw observation next is [-1.5, 68.5, 0.0, 0.0, 26.0, 25.42899569122917, 0.4046751132321587, 0.0, 1.0, 37287.8279490995], 
processed observation next is [0.0, 0.9565217391304348, 0.4210526315789474, 0.685, 0.0, 0.0, 0.6666666666666666, 0.6190829742690974, 0.6348917044107195, 0.0, 1.0, 0.17756108547190236], 
reward next is 0.8224, 
noisyNet noise sample is [array([-1.5788585], dtype=float32), -0.8586731]. 
=============================================
[2019-04-04 13:38:39,561] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0189401e-08 4.7598930e-10 3.5564440e-14 2.0637053e-14 1.0000000e+00
 2.9997030e-10 1.5215415e-14], sum to 1.0000
[2019-04-04 13:38:39,562] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4948
[2019-04-04 13:38:39,575] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.166666666666667, 24.33333333333333, 35.66666666666666, 311.3333333333333, 26.0, 26.97992270867027, 0.7215309166776723, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4036200.0000, 
sim time next is 4036800.0000, 
raw observation next is [-2.333333333333333, 24.66666666666666, 27.83333333333333, 252.1666666666667, 26.0, 27.05480839998741, 0.6880881729440512, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3979686057248385, 0.24666666666666662, 0.09277777777777776, 0.2786372007366483, 0.6666666666666666, 0.7545673666656176, 0.7293627243146837, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.428119], dtype=float32), -0.4498461]. 
=============================================
[2019-04-04 13:38:45,313] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-04 13:38:45,315] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 13:38:45,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:38:45,318] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 13:38:45,318] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:38:45,319] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 13:38:45,326] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:38:45,331] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run17
[2019-04-04 13:38:45,347] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run17
[2019-04-04 13:38:45,367] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run17
[2019-04-04 13:39:15,910] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16778278], dtype=float32), 0.19709891]
[2019-04-04 13:39:15,910] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [15.35, 68.0, 0.0, 0.0, 26.0, 25.62533308784029, 0.5239610791809568, 0.0, 1.0, 23961.70158176993]
[2019-04-04 13:39:15,911] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 13:39:15,911] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.8186655e-10 4.9823888e-11 9.5406059e-17 1.9821331e-15 1.0000000e+00
 8.2523979e-12 1.6701544e-16], sampled 0.18412819269200975
[2019-04-04 13:39:57,640] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16778278], dtype=float32), 0.19709891]
[2019-04-04 13:39:57,640] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.070943938, 55.00746438333333, 0.0, 0.0, 26.0, 24.68322460946829, 0.1997783415279763, 0.0, 1.0, 41240.31261347729]
[2019-04-04 13:39:57,640] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 13:39:57,641] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.5219635e-09 1.0933808e-09 3.6032287e-14 3.4535932e-13 1.0000000e+00
 5.8786698e-10 5.6870849e-14], sampled 0.6408519484311266
[2019-04-04 13:39:59,310] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16778278], dtype=float32), 0.19709891]
[2019-04-04 13:39:59,311] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [12.43333333333333, 37.66666666666667, 95.16666666666667, 677.6666666666666, 26.0, 26.73324005091566, 0.6907946668650228, 1.0, 1.0, 0.0]
[2019-04-04 13:39:59,311] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 13:39:59,311] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.9192881e-09 3.7180123e-10 4.1608058e-15 2.4578917e-14 1.0000000e+00
 2.1655890e-10 5.3845444e-15], sampled 0.12964807979839876
[2019-04-04 13:40:26,066] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 13:40:46,381] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 13:40:49,202] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 13:40:50,225] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1600000, evaluation results [1600000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 13:40:53,941] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.8592146e-09 6.5278544e-10 1.6301037e-15 2.0231335e-14 1.0000000e+00
 5.6085751e-11 2.1701566e-15], sum to 1.0000
[2019-04-04 13:40:53,942] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6022
[2019-04-04 13:40:53,944] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2774337e-08 3.3002479e-09 2.3258262e-13 4.9306344e-13 1.0000000e+00
 2.1783704e-09 5.0007561e-14], sum to 1.0000
[2019-04-04 13:40:53,945] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9331
[2019-04-04 13:40:53,974] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.5, 41.66666666666667, 0.0, 0.0, 26.0, 25.01561833691231, 0.3013321924090221, 0.0, 1.0, 32460.32956892819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4214400.0000, 
sim time next is 4215000.0000, 
raw observation next is [1.45, 41.83333333333333, 0.0, 0.0, 26.0, 24.99465601367654, 0.3030455543340196, 0.0, 1.0, 40090.84878818825], 
processed observation next is [0.0, 0.782608695652174, 0.5027700831024932, 0.4183333333333333, 0.0, 0.0, 0.6666666666666666, 0.5828880011397116, 0.6010151847780065, 0.0, 1.0, 0.19090880375327737], 
reward next is 0.8091, 
noisyNet noise sample is [array([0.69279605], dtype=float32), 0.10421198]. 
=============================================
[2019-04-04 13:40:53,975] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.166666666666667, 35.5, 107.3333333333333, 709.0, 26.0, 26.35210976064098, 0.5425145574333192, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4096200.0000, 
sim time next is 4096800.0000, 
raw observation next is [-2.0, 35.0, 109.0, 724.0, 26.0, 26.44083596781659, 0.5639495359215754, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.40720221606648205, 0.35, 0.36333333333333334, 0.8, 0.6666666666666666, 0.7034029973180491, 0.6879831786405252, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00936517], dtype=float32), 0.77276963]. 
=============================================
[2019-04-04 13:40:53,981] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[71.749695]
 [71.60895 ]
 [71.60372 ]
 [71.778946]
 [72.14318 ]], R is [[72.13368225]
 [72.25777435]
 [72.42402649]
 [72.55498505]
 [72.70075989]].
[2019-04-04 13:40:55,716] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5765588e-09 1.3915560e-09 8.8654297e-15 1.1488359e-13 1.0000000e+00
 1.3868343e-09 1.9016386e-14], sum to 1.0000
[2019-04-04 13:40:55,717] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2291
[2019-04-04 13:40:55,735] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.5, 47.0, 0.0, 0.0, 26.0, 25.49056204831965, 0.5030846982897507, 0.0, 1.0, 49541.7128602095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3965400.0000, 
sim time next is 3966000.0000, 
raw observation next is [-7.666666666666666, 47.66666666666666, 0.0, 0.0, 26.0, 25.46825038584701, 0.499655145958188, 0.0, 1.0, 55011.92293209437], 
processed observation next is [1.0, 0.9130434782608695, 0.25023084025854114, 0.47666666666666657, 0.0, 0.0, 0.6666666666666666, 0.6223541988205842, 0.6665517153193959, 0.0, 1.0, 0.261961537771878], 
reward next is 0.7380, 
noisyNet noise sample is [array([0.9703031], dtype=float32), 1.5863159]. 
=============================================
[2019-04-04 13:40:55,744] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[76.82542]
 [76.69475]
 [76.48871]
 [76.53681]
 [76.48412]], R is [[77.08721924]
 [77.08042908]
 [77.1279068 ]
 [77.21572113]
 [77.27454376]].
[2019-04-04 13:40:57,324] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.7742136e-09 1.5779941e-10 4.7359552e-15 1.8788817e-14 1.0000000e+00
 1.9197988e-10 1.5031038e-14], sum to 1.0000
[2019-04-04 13:40:57,324] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1335
[2019-04-04 13:40:57,358] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.333333333333333, 30.33333333333333, 116.6666666666667, 837.3333333333334, 26.0, 26.26699966983347, 0.4746347457513183, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4020600.0000, 
sim time next is 4021200.0000, 
raw observation next is [-4.0, 29.0, 116.0, 835.5, 26.0, 25.68433246151058, 0.5175409814613686, 1.0, 1.0, 106285.9502175838], 
processed observation next is [1.0, 0.5652173913043478, 0.3518005540166205, 0.29, 0.38666666666666666, 0.9232044198895027, 0.6666666666666666, 0.6403610384592149, 0.6725136604871228, 1.0, 1.0, 0.5061235724646848], 
reward next is 0.4939, 
noisyNet noise sample is [array([0.00206752], dtype=float32), -0.6097353]. 
=============================================
[2019-04-04 13:41:12,574] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.57540300e-09 1.07645365e-10 3.49983399e-16 4.80798517e-15
 1.00000000e+00 2.04496715e-11 2.16424449e-16], sum to 1.0000
[2019-04-04 13:41:12,579] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8471
[2019-04-04 13:41:12,592] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.233333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.56878550224137, 0.381736390454162, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4342200.0000, 
sim time next is 4342800.0000, 
raw observation next is [3.166666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.54467112456315, 0.3698939641756421, 0.0, 1.0, 18745.2356612352], 
processed observation next is [1.0, 0.2608695652173913, 0.5503231763619576, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6287225937135957, 0.6232979880585473, 0.0, 1.0, 0.08926302695826287], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.60169286], dtype=float32), -0.028488992]. 
=============================================
[2019-04-04 13:41:17,126] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.1044370e-10 3.8366327e-11 2.2458785e-16 2.3872726e-15 1.0000000e+00
 8.1168509e-12 9.3776202e-17], sum to 1.0000
[2019-04-04 13:41:17,129] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7738
[2019-04-04 13:41:17,141] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.34722260244308, 0.4732377725825255, 0.0, 1.0, 41131.00736144852], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4576200.0000, 
sim time next is 4576800.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.39702843919788, 0.4756324234962985, 0.0, 1.0, 25536.69620509431], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6164190365998232, 0.6585441411654328, 0.0, 1.0, 0.12160331526235386], 
reward next is 0.8784, 
noisyNet noise sample is [array([-0.06275176], dtype=float32), 0.10932186]. 
=============================================
[2019-04-04 13:41:18,081] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1700214e-10 2.8093279e-12 4.0072391e-17 2.3245985e-16 1.0000000e+00
 1.2683034e-12 3.4010682e-17], sum to 1.0000
[2019-04-04 13:41:18,083] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4551
[2019-04-04 13:41:18,097] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.0, 58.0, 0.0, 0.0, 26.0, 27.06423185929403, 0.8790111098740477, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4392000.0000, 
sim time next is 4392600.0000, 
raw observation next is [10.86666666666667, 58.16666666666667, 0.0, 0.0, 26.0, 27.0139011530906, 0.8673136438698067, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7636195752539245, 0.5816666666666667, 0.0, 0.0, 0.6666666666666666, 0.7511584294242167, 0.7891045479566022, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8787462], dtype=float32), -0.39807564]. 
=============================================
[2019-04-04 13:41:19,196] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.1468022e-10 3.8388726e-11 8.9418747e-17 1.9865690e-15 1.0000000e+00
 6.0839268e-12 1.5743367e-16], sum to 1.0000
[2019-04-04 13:41:19,196] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4604
[2019-04-04 13:41:19,228] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5333333333333334, 72.66666666666667, 92.5, 55.0, 26.0, 25.31231526617833, 0.4535987872192781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4522800.0000, 
sim time next is 4523400.0000, 
raw observation next is [-0.4, 72.5, 111.0, 66.0, 26.0, 25.61584363934977, 0.4754735637743517, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.45152354570637127, 0.725, 0.37, 0.07292817679558011, 0.6666666666666666, 0.6346536366124808, 0.6584911879247839, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14401023], dtype=float32), 0.5348383]. 
=============================================
[2019-04-04 13:41:19,267] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3740916e-10 2.8333889e-11 4.7344870e-17 6.1988465e-16 1.0000000e+00
 1.3864283e-11 8.7260487e-16], sum to 1.0000
[2019-04-04 13:41:19,274] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9017
[2019-04-04 13:41:19,300] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 78.0, 37.0, 27.5, 26.0, 26.20457171626813, 0.5852508130801866, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4467600.0000, 
sim time next is 4468200.0000, 
raw observation next is [0.0, 77.0, 33.0, 36.66666666666667, 26.0, 26.22391505506488, 0.477261772490592, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.77, 0.11, 0.04051565377532229, 0.6666666666666666, 0.6853262545887399, 0.6590872574968639, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3175333], dtype=float32), 0.20398098]. 
=============================================
[2019-04-04 13:41:36,129] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1259137e-08 1.3713728e-09 1.4155201e-13 2.5702351e-13 1.0000000e+00
 2.4615748e-10 1.1941220e-13], sum to 1.0000
[2019-04-04 13:41:36,130] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6263
[2019-04-04 13:41:36,148] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 44.33333333333334, 0.0, 0.0, 26.0, 24.98800826892044, 0.3460628963284802, 0.0, 1.0, 198813.7730786239], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4821600.0000, 
sim time next is 4822200.0000, 
raw observation next is [1.0, 45.0, 0.0, 0.0, 26.0, 25.02339328897085, 0.3751598579566292, 0.0, 1.0, 165135.9522821027], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.45, 0.0, 0.0, 0.6666666666666666, 0.5852827740809042, 0.6250532859855431, 0.0, 1.0, 0.7863616775338224], 
reward next is 0.2136, 
noisyNet noise sample is [array([-0.97089696], dtype=float32), 0.3649515]. 
=============================================
[2019-04-04 13:41:36,203] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:36,203] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:36,252] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run13
[2019-04-04 13:41:41,675] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.53482585e-09 4.84274121e-10 1.14529976e-14 5.81804299e-14
 1.00000000e+00 1.14126354e-10 5.34753499e-15], sum to 1.0000
[2019-04-04 13:41:41,675] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6064
[2019-04-04 13:41:41,689] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.5, 40.0, 0.0, 0.0, 26.0, 25.5419160070632, 0.4685610272485598, 0.0, 1.0, 18747.01870359453], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5013000.0000, 
sim time next is 5013600.0000, 
raw observation next is [1.333333333333333, 40.0, 0.0, 0.0, 26.0, 25.54426910666815, 0.4665387969787447, 0.0, 1.0, 18745.34792524653], 
processed observation next is [1.0, 0.0, 0.4995383194829178, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6286890922223458, 0.6555129323262482, 0.0, 1.0, 0.089263561548793], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.3501784], dtype=float32), -0.039970994]. 
=============================================
[2019-04-04 13:41:42,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:42,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:42,556] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run13
[2019-04-04 13:41:42,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:42,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:42,899] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run13
[2019-04-04 13:41:46,202] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:46,202] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:46,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run13
[2019-04-04 13:41:48,258] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:48,259] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:48,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run13
[2019-04-04 13:41:49,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:49,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:49,118] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run13
[2019-04-04 13:41:49,465] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.3427737e-09 7.7280143e-10 5.6156819e-15 2.4387561e-13 1.0000000e+00
 3.7418593e-10 7.0486698e-15], sum to 1.0000
[2019-04-04 13:41:49,465] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0426
[2019-04-04 13:41:49,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:49,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:49,502] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run13
[2019-04-04 13:41:49,520] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.699999999999999, 93.0, 23.83333333333333, 0.0, 26.0, 22.25698552562193, -0.3080156962645022, 0.0, 1.0, 74474.1722198579], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 31200.0000, 
sim time next is 31800.0000, 
raw observation next is [7.7, 93.0, 26.66666666666666, 0.0, 26.0, 22.52853684982706, -0.2688973678655116, 0.0, 1.0, 64101.82628055422], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.08888888888888886, 0.0, 0.6666666666666666, 0.3773780708189216, 0.4103675440448295, 0.0, 1.0, 0.30524679181216297], 
reward next is 0.6948, 
noisyNet noise sample is [array([1.5139942], dtype=float32), -1.1114343]. 
=============================================
[2019-04-04 13:41:49,613] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.0042239e-09 5.2105376e-10 3.3651687e-15 1.3723453e-14 1.0000000e+00
 1.4967244e-10 5.4102192e-15], sum to 1.0000
[2019-04-04 13:41:49,613] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1586
[2019-04-04 13:41:49,618] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.4, 20.0, 0.0, 0.0, 26.0, 26.18469918276245, 0.6094780804387546, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5094000.0000, 
sim time next is 5094600.0000, 
raw observation next is [8.350000000000001, 22.5, 0.0, 0.0, 26.0, 26.09883060426606, 0.5907651039217402, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6939058171745154, 0.225, 0.0, 0.0, 0.6666666666666666, 0.674902550355505, 0.6969217013072467, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09339168], dtype=float32), 0.37269786]. 
=============================================
[2019-04-04 13:41:49,738] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:49,738] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:49,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run13
[2019-04-04 13:41:49,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:49,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:49,988] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:49,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:49,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run13
[2019-04-04 13:41:50,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run13
[2019-04-04 13:41:50,079] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:50,080] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:50,084] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run13
[2019-04-04 13:41:50,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:50,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:50,236] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run13
[2019-04-04 13:41:50,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:50,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:50,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run13
[2019-04-04 13:41:50,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:50,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:50,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run13
[2019-04-04 13:41:50,695] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:50,695] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:50,710] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run13
[2019-04-04 13:41:51,994] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:41:51,994] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:41:51,999] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run13
[2019-04-04 13:42:07,017] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5538513e-08 8.1527674e-09 1.7282942e-14 7.7892380e-13 1.0000000e+00
 1.0641059e-09 2.7560021e-14], sum to 1.0000
[2019-04-04 13:42:07,017] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9579
[2019-04-04 13:42:07,033] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 74.0, 0.0, 0.0, 26.0, 23.56308492242317, -0.03737832305699605, 0.0, 1.0, 44387.67438775727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 178800.0000, 
sim time next is 179400.0000, 
raw observation next is [-8.9, 74.0, 0.0, 0.0, 26.0, 23.5680730715018, -0.04240526419554228, 0.0, 1.0, 44336.85682656246], 
processed observation next is [1.0, 0.043478260869565216, 0.21606648199445982, 0.74, 0.0, 0.0, 0.6666666666666666, 0.4640060892918167, 0.48586491193481923, 0.0, 1.0, 0.21112788965029744], 
reward next is 0.7889, 
noisyNet noise sample is [array([-0.25523725], dtype=float32), 0.109564885]. 
=============================================
[2019-04-04 13:42:18,596] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.8332183e-10 2.0366101e-11 3.8894075e-16 7.2244999e-15 1.0000000e+00
 3.1312966e-11 9.5663033e-16], sum to 1.0000
[2019-04-04 13:42:18,597] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1587
[2019-04-04 13:42:18,610] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 80.83333333333334, 0.0, 0.0, 26.0, 24.3715619244595, 0.1466708100118445, 0.0, 1.0, 44204.05894099727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 255000.0000, 
sim time next is 255600.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 24.38570689878538, 0.1410276341777169, 0.0, 1.0, 44220.47196980369], 
processed observation next is [1.0, 1.0, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5321422415654483, 0.5470092113925723, 0.0, 1.0, 0.21057367604668425], 
reward next is 0.7894, 
noisyNet noise sample is [array([0.8863175], dtype=float32), -0.3963243]. 
=============================================
[2019-04-04 13:42:21,231] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6826079e-08 8.7164560e-11 9.0433249e-14 1.6404669e-13 1.0000000e+00
 8.2625329e-10 2.1456930e-14], sum to 1.0000
[2019-04-04 13:42:21,232] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7091
[2019-04-04 13:42:21,285] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 42.0, 62.33333333333334, 437.5, 26.0, 26.3121650236822, 0.5245980095628116, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 315600.0000, 
sim time next is 316200.0000, 
raw observation next is [-9.5, 42.0, 54.66666666666667, 398.0000000000001, 26.0, 26.37362854423891, 0.3729149313468537, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.18222222222222223, 0.439779005524862, 0.6666666666666666, 0.6978023786865757, 0.6243049771156178, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1107006], dtype=float32), 1.0937686]. 
=============================================
[2019-04-04 13:42:26,734] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6150433e-08 1.4021818e-08 6.4125333e-13 2.4237882e-12 9.9999988e-01
 2.0878137e-09 3.7417674e-13], sum to 1.0000
[2019-04-04 13:42:26,734] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9787
[2019-04-04 13:42:26,777] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.7, 73.83333333333334, 0.0, 0.0, 26.0, 22.18633230882471, -0.3753474598869028, 0.0, 1.0, 48669.64196309792], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 364200.0000, 
sim time next is 364800.0000, 
raw observation next is [-15.8, 74.66666666666667, 0.0, 0.0, 26.0, 22.18251480249224, -0.3910984003913705, 0.0, 1.0, 48635.32755367534], 
processed observation next is [1.0, 0.21739130434782608, 0.024930747922437636, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.3485429002076866, 0.36963386653620983, 0.0, 1.0, 0.2315967978746445], 
reward next is 0.7684, 
noisyNet noise sample is [array([0.5569], dtype=float32), 1.1326042]. 
=============================================
[2019-04-04 13:42:34,428] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0327851e-07 3.1001612e-08 1.3332355e-12 2.5981339e-12 9.9999988e-01
 3.1295279e-08 3.1216937e-12], sum to 1.0000
[2019-04-04 13:42:34,434] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5108
[2019-04-04 13:42:34,457] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.133333333333333, 43.66666666666667, 0.0, 0.0, 26.0, 22.50805324031094, -0.3303808763119229, 0.0, 1.0, 46479.74120513813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 454800.0000, 
sim time next is 455400.0000, 
raw observation next is [-8.95, 43.5, 0.0, 0.0, 26.0, 22.46711898121957, -0.3178266796129535, 0.0, 1.0, 46761.52203662106], 
processed observation next is [1.0, 0.2608695652173913, 0.21468144044321333, 0.435, 0.0, 0.0, 0.6666666666666666, 0.37225991510163087, 0.39405777346234877, 0.0, 1.0, 0.2226739144601003], 
reward next is 0.7773, 
noisyNet noise sample is [array([-0.14474748], dtype=float32), -1.2844044]. 
=============================================
[2019-04-04 13:42:37,251] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6614997e-08 1.8674180e-09 6.9523766e-14 4.8353015e-14 1.0000000e+00
 1.1447567e-09 2.3229785e-14], sum to 1.0000
[2019-04-04 13:42:37,251] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0301
[2019-04-04 13:42:37,297] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 41.5, 13.33333333333334, 0.0, 26.0, 25.32241548291304, 0.275885325326732, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 492600.0000, 
sim time next is 493200.0000, 
raw observation next is [1.1, 43.0, 10.0, 0.0, 26.0, 25.5525066541112, 0.2939854679455627, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.43, 0.03333333333333333, 0.0, 0.6666666666666666, 0.6293755545092665, 0.5979951559818543, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48328194], dtype=float32), 0.6377209]. 
=============================================
[2019-04-04 13:42:45,794] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4265021e-10 1.5430225e-10 2.1791574e-15 5.3315455e-15 1.0000000e+00
 2.5410691e-11 1.0652866e-15], sum to 1.0000
[2019-04-04 13:42:45,795] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9226
[2019-04-04 13:42:45,845] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 85.66666666666667, 88.16666666666666, 134.6666666666667, 26.0, 24.79969824170891, 0.2694723451616623, 0.0, 1.0, 44294.3443029293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 555600.0000, 
sim time next is 556200.0000, 
raw observation next is [-0.6, 85.0, 77.0, 141.0, 26.0, 24.82080702670569, 0.2764748136790167, 0.0, 1.0, 30557.67488505005], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.85, 0.25666666666666665, 0.1558011049723757, 0.6666666666666666, 0.5684005855588076, 0.5921582712263389, 0.0, 1.0, 0.1455127375478574], 
reward next is 0.8545, 
noisyNet noise sample is [array([0.63796026], dtype=float32), -1.3534563]. 
=============================================
[2019-04-04 13:42:59,912] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0200606e-10 3.5100158e-12 5.5045540e-19 6.1758304e-17 1.0000000e+00
 3.9607293e-12 5.3721237e-18], sum to 1.0000
[2019-04-04 13:42:59,912] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5945
[2019-04-04 13:42:59,929] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.86666666666667, 68.33333333333334, 230.6666666666667, 179.1666666666667, 26.0, 27.36413340530822, 0.9629049262604479, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1077600.0000, 
sim time next is 1078200.0000, 
raw observation next is [16.05, 67.5, 254.0, 215.0, 26.0, 27.48534224693627, 0.6458384081351656, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.9072022160664821, 0.675, 0.8466666666666667, 0.23756906077348067, 0.6666666666666666, 0.7904451872446892, 0.7152794693783885, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18896367], dtype=float32), 1.089559]. 
=============================================
[2019-04-04 13:43:14,038] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1306389e-09 4.9098992e-11 3.5115471e-17 7.5333840e-15 1.0000000e+00
 2.8094794e-11 7.6655952e-17], sum to 1.0000
[2019-04-04 13:43:14,039] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8487
[2019-04-04 13:43:14,048] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.2, 76.0, 0.0, 0.0, 26.0, 26.02642308212903, 0.6200842282169615, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1041600.0000, 
sim time next is 1042200.0000, 
raw observation next is [14.1, 76.5, 0.0, 0.0, 26.0, 25.98193663788978, 0.6082539880627348, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.8531855955678671, 0.765, 0.0, 0.0, 0.6666666666666666, 0.6651613864908151, 0.7027513293542449, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3192017], dtype=float32), 0.018343922]. 
=============================================
[2019-04-04 13:43:20,383] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5630747e-08 7.9459745e-09 9.9289338e-13 1.4731903e-11 1.0000000e+00
 8.9439534e-10 5.4905727e-13], sum to 1.0000
[2019-04-04 13:43:20,386] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8910
[2019-04-04 13:43:20,395] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.1, 64.33333333333334, 0.0, 0.0, 26.0, 24.77758372623977, 0.4267819298454026, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1189200.0000, 
sim time next is 1189800.0000, 
raw observation next is [18.0, 65.0, 0.0, 0.0, 26.0, 24.79336282601584, 0.4237380668089121, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.9612188365650972, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5661135688346534, 0.6412460222696373, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0781509], dtype=float32), -0.2942654]. 
=============================================
[2019-04-04 13:43:25,177] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8028760e-10 1.7036388e-11 3.3817013e-17 5.2013270e-16 1.0000000e+00
 4.5144691e-12 1.5149310e-17], sum to 1.0000
[2019-04-04 13:43:25,189] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5104
[2019-04-04 13:43:25,197] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.9000000000000001, 92.0, 106.1666666666667, 0.0, 26.0, 26.09982356488922, 0.5899413414194449, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1334400.0000, 
sim time next is 1335000.0000, 
raw observation next is [1.0, 92.0, 110.3333333333333, 0.0, 26.0, 26.10683034716817, 0.5911418007643302, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.92, 0.36777777777777765, 0.0, 0.6666666666666666, 0.6755691955973475, 0.6970472669214435, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.408753], dtype=float32), -0.9986469]. 
=============================================
[2019-04-04 13:43:25,203] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[92.530136]
 [92.30944 ]
 [92.196785]
 [92.12128 ]
 [92.15387 ]], R is [[92.76353455]
 [92.83589935]
 [92.90753937]
 [92.97846222]
 [93.04867554]].
[2019-04-04 13:43:25,681] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.5000383e-10 6.2046787e-12 6.6382217e-17 7.7829294e-16 1.0000000e+00
 2.8650374e-12 6.8791318e-17], sum to 1.0000
[2019-04-04 13:43:25,684] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0624
[2019-04-04 13:43:25,713] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.2, 92.0, 0.0, 0.0, 26.0, 25.34618827843938, 0.5733324722209688, 0.0, 1.0, 45025.72734461569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1303800.0000, 
sim time next is 1304400.0000, 
raw observation next is [3.1, 92.0, 0.0, 0.0, 26.0, 25.43231131410501, 0.5768327410718979, 0.0, 1.0, 18763.9462018173], 
processed observation next is [1.0, 0.08695652173913043, 0.5484764542936289, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6193592761754175, 0.6922775803572992, 0.0, 1.0, 0.08935212477055857], 
reward next is 0.9106, 
noisyNet noise sample is [array([0.24769835], dtype=float32), 0.03297043]. 
=============================================
[2019-04-04 13:43:35,020] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6844094e-10 6.1342667e-11 1.4013768e-16 8.7549613e-15 1.0000000e+00
 1.0268571e-11 1.7311694e-16], sum to 1.0000
[2019-04-04 13:43:35,023] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2155
[2019-04-04 13:43:35,033] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 79.5, 0.0, 0.0, 26.0, 25.57627969126791, 0.5124479884518996, 0.0, 1.0, 72043.14633587642], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1561800.0000, 
sim time next is 1562400.0000, 
raw observation next is [5.0, 79.0, 0.0, 0.0, 26.0, 25.43431944034985, 0.5186487904236444, 0.0, 1.0, 127072.3454255032], 
processed observation next is [1.0, 0.08695652173913043, 0.6011080332409973, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6195266200291542, 0.6728829301412148, 0.0, 1.0, 0.6051064067881105], 
reward next is 0.3949, 
noisyNet noise sample is [array([-0.09039168], dtype=float32), -1.9666584]. 
=============================================
[2019-04-04 13:43:35,856] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0236837e-10 7.3684349e-12 2.9388354e-17 5.6244046e-16 1.0000000e+00
 6.5342007e-12 8.6042020e-18], sum to 1.0000
[2019-04-04 13:43:35,858] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4577
[2019-04-04 13:43:35,870] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.733333333333333, 76.33333333333333, 95.0, 700.3333333333334, 26.0, 25.31016704588002, 0.5485828112583394, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1515000.0000, 
sim time next is 1515600.0000, 
raw observation next is [7.2, 73.0, 92.5, 700.5, 26.0, 25.61590407307169, 0.5717774301345769, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.662049861495845, 0.73, 0.30833333333333335, 0.7740331491712708, 0.6666666666666666, 0.6346586727559741, 0.6905924767115256, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33801606], dtype=float32), 0.6566942]. 
=============================================
[2019-04-04 13:43:37,754] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9561881e-11 2.2226114e-12 2.0285245e-17 1.3226532e-16 1.0000000e+00
 1.2343597e-11 3.1526937e-17], sum to 1.0000
[2019-04-04 13:43:37,755] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7645
[2019-04-04 13:43:37,781] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 83.33333333333334, 0.0, 0.0, 26.0, 25.71671463008461, 0.5972397836739035, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1636800.0000, 
sim time next is 1637400.0000, 
raw observation next is [7.1, 82.66666666666667, 0.0, 0.0, 26.0, 25.67220543756352, 0.5864123962167468, 0.0, 1.0, 70871.98227472739], 
processed observation next is [1.0, 0.9565217391304348, 0.6592797783933518, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6393504531302933, 0.6954707987389156, 0.0, 1.0, 0.33748562987965425], 
reward next is 0.6625, 
noisyNet noise sample is [array([0.31226447], dtype=float32), 0.18918063]. 
=============================================
[2019-04-04 13:43:40,014] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6590670e-10 6.6151876e-11 3.5518580e-16 2.4655721e-15 1.0000000e+00
 3.2699227e-12 3.2708768e-16], sum to 1.0000
[2019-04-04 13:43:40,018] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9688
[2019-04-04 13:43:40,043] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.083333333333334, 81.5, 13.0, 15.0, 26.0, 25.47023995409695, 0.47265818256443, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1583400.0000, 
sim time next is 1584000.0000, 
raw observation next is [5.0, 82.0, 19.0, 20.0, 26.0, 25.40751743398645, 0.45835243150723, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6011080332409973, 0.82, 0.06333333333333334, 0.022099447513812154, 0.6666666666666666, 0.6172931194988708, 0.6527841438357433, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19350767], dtype=float32), 0.7066177]. 
=============================================
[2019-04-04 13:43:40,064] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[91.60555]
 [91.42078]
 [91.37704]
 [90.92998]
 [90.49798]], R is [[91.99714661]
 [92.07717896]
 [92.15641022]
 [92.23484802]
 [92.3125    ]].
[2019-04-04 13:43:41,533] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5630913e-10 5.3468346e-10 6.6794260e-16 5.1263781e-15 1.0000000e+00
 8.9956168e-11 4.2261354e-15], sum to 1.0000
[2019-04-04 13:43:41,537] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2875
[2019-04-04 13:43:41,561] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 87.66666666666666, 0.0, 0.0, 26.0, 25.1252254411372, 0.3994501754000805, 0.0, 1.0, 43193.72462467068], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1741800.0000, 
sim time next is 1742400.0000, 
raw observation next is [-0.6, 87.0, 0.0, 0.0, 26.0, 25.1012244853093, 0.394235466713572, 0.0, 1.0, 43236.58309169972], 
processed observation next is [0.0, 0.17391304347826086, 0.44598337950138506, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5917687071091082, 0.6314118222378573, 0.0, 1.0, 0.20588849091285583], 
reward next is 0.7941, 
noisyNet noise sample is [array([-0.1658673], dtype=float32), 0.6176196]. 
=============================================
[2019-04-04 13:43:50,755] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.7304895e-09 3.1675201e-10 7.5331536e-15 1.5894613e-13 1.0000000e+00
 5.0717336e-10 4.2069784e-15], sum to 1.0000
[2019-04-04 13:43:50,755] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9404
[2019-04-04 13:43:50,835] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.1, 82.33333333333334, 0.0, 0.0, 26.0, 25.00122574244676, 0.3169910470954944, 0.0, 1.0, 48805.00385110931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1794000.0000, 
sim time next is 1794600.0000, 
raw observation next is [-4.2, 82.5, 0.0, 0.0, 26.0, 25.00170280392685, 0.3164940289048671, 0.0, 1.0, 49356.56456470709], 
processed observation next is [0.0, 0.782608695652174, 0.34626038781163443, 0.825, 0.0, 0.0, 0.6666666666666666, 0.5834752336605709, 0.6054980096349557, 0.0, 1.0, 0.23503125983193854], 
reward next is 0.7650, 
noisyNet noise sample is [array([0.02716053], dtype=float32), -0.8546694]. 
=============================================
[2019-04-04 13:43:57,609] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.1982299e-09 1.3283337e-09 6.6936880e-15 3.3182597e-14 1.0000000e+00
 2.6326766e-10 1.5664846e-14], sum to 1.0000
[2019-04-04 13:43:57,609] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8033
[2019-04-04 13:43:57,625] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.81440838793016, -0.04667589002071026, 0.0, 1.0, 44942.88705433586], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1914600.0000, 
sim time next is 1915200.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.80419309252449, -0.0602350061044847, 0.0, 1.0, 45039.67449872042], 
processed observation next is [1.0, 0.17391304347826086, 0.2299168975069252, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4836827577103741, 0.47992166463183844, 0.0, 1.0, 0.21447464047009726], 
reward next is 0.7855, 
noisyNet noise sample is [array([1.4372039], dtype=float32), 0.022251597]. 
=============================================
[2019-04-04 13:44:00,459] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.91081761e-09 9.62387170e-10 4.19318367e-15 7.69117775e-14
 1.00000000e+00 3.48235885e-10 1.01483416e-14], sum to 1.0000
[2019-04-04 13:44:00,461] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8411
[2019-04-04 13:44:00,483] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.3572016827451, 0.1337855057750104, 0.0, 1.0, 41581.57873920054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1996800.0000, 
sim time next is 1997400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.38251715464673, 0.1325592772473653, 0.0, 1.0, 41526.04802449163], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5318764295538942, 0.5441864257491218, 0.0, 1.0, 0.19774308583091255], 
reward next is 0.8023, 
noisyNet noise sample is [array([-1.8503587], dtype=float32), -1.2970017]. 
=============================================
[2019-04-04 13:44:01,430] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4477954e-09 9.4922806e-11 7.6521009e-16 1.2712810e-14 1.0000000e+00
 1.3552039e-10 2.7779606e-15], sum to 1.0000
[2019-04-04 13:44:01,430] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8293
[2019-04-04 13:44:01,498] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 65.0, 229.5, 7.0, 26.0, 25.68895625292454, 0.344945950807949, 1.0, 1.0, 38800.71880122769], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1944000.0000, 
sim time next is 1944600.0000, 
raw observation next is [-4.816666666666666, 65.0, 228.6666666666667, 6.0, 26.0, 25.76092810310038, 0.3513758082429669, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.32917820867959374, 0.65, 0.7622222222222224, 0.0066298342541436465, 0.6666666666666666, 0.6467440085916983, 0.6171252694143222, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5385299], dtype=float32), -2.270452]. 
=============================================
[2019-04-04 13:44:02,983] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9903538e-09 2.1501875e-10 5.4628958e-15 2.3865634e-14 1.0000000e+00
 1.2903881e-09 4.4366290e-15], sum to 1.0000
[2019-04-04 13:44:02,983] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7124
[2019-04-04 13:44:03,032] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.583333333333333, 72.33333333333334, 0.0, 0.0, 26.0, 25.04264334215789, 0.3585134182571215, 1.0, 1.0, 54349.03545144598], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1968600.0000, 
sim time next is 1969200.0000, 
raw observation next is [-4.5, 71.0, 0.0, 0.0, 26.0, 25.23223887201218, 0.3630433309419115, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3379501385041552, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6026865726676816, 0.6210144436473038, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29768947], dtype=float32), -0.57370234]. 
=============================================
[2019-04-04 13:44:03,181] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8231123e-09 6.3287840e-11 4.4230426e-15 1.2272892e-14 1.0000000e+00
 1.3664686e-10 2.7600986e-15], sum to 1.0000
[2019-04-04 13:44:03,181] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0431
[2019-04-04 13:44:03,278] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.05, 77.0, 0.0, 0.0, 26.0, 25.19497407561622, 0.3099923357601462, 1.0, 1.0, 25664.25676084928], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1971000.0000, 
sim time next is 1971600.0000, 
raw observation next is [-5.233333333333333, 79.0, 0.0, 0.0, 26.0, 24.97702893625252, 0.3044045898730742, 1.0, 1.0, 156875.2378486512], 
processed observation next is [1.0, 0.8260869565217391, 0.31763619575253926, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5814190780210433, 0.6014681966243581, 1.0, 1.0, 0.7470249421364342], 
reward next is 0.2530, 
noisyNet noise sample is [array([-0.30765486], dtype=float32), 1.1660535]. 
=============================================
[2019-04-04 13:44:07,087] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.3720242e-10 4.7232270e-11 1.5916007e-16 5.9581242e-15 1.0000000e+00
 5.8703979e-11 1.0388958e-15], sum to 1.0000
[2019-04-04 13:44:07,088] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8862
[2019-04-04 13:44:07,138] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.05, 79.0, 147.0, 0.0, 26.0, 25.61352566256837, 0.3350822865651957, 1.0, 1.0, 24356.0319550243], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2028600.0000, 
sim time next is 2029200.0000, 
raw observation next is [-4.866666666666667, 77.66666666666667, 148.5, 0.0, 26.0, 25.64371393787246, 0.34015867565911, 1.0, 1.0, 23531.22703847628], 
processed observation next is [1.0, 0.4782608695652174, 0.3277931671283472, 0.7766666666666667, 0.495, 0.0, 0.6666666666666666, 0.6369761614893715, 0.6133862252197033, 1.0, 1.0, 0.1120534620879823], 
reward next is 0.8879, 
noisyNet noise sample is [array([-0.29228473], dtype=float32), -0.7140014]. 
=============================================
[2019-04-04 13:44:19,144] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7093594e-09 5.3374405e-10 3.1837084e-15 2.8278400e-14 1.0000000e+00
 6.8573675e-11 1.0713880e-14], sum to 1.0000
[2019-04-04 13:44:19,145] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7869
[2019-04-04 13:44:19,172] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 54.0, 0.0, 0.0, 26.0, 25.38781645878541, 0.433993398371146, 0.0, 1.0, 33807.71702523048], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2322000.0000, 
sim time next is 2322600.0000, 
raw observation next is [-1.7, 54.33333333333333, 0.0, 0.0, 26.0, 25.44985309711561, 0.4321295632406643, 0.0, 1.0, 18762.95486184495], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5433333333333333, 0.0, 0.0, 0.6666666666666666, 0.6208210914263009, 0.6440431877468881, 0.0, 1.0, 0.08934740410402357], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.6622233], dtype=float32), 1.2883875]. 
=============================================
[2019-04-04 13:44:22,250] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6651125e-09 4.1457571e-10 2.3983446e-15 1.9693367e-14 1.0000000e+00
 4.9133869e-10 1.2658514e-14], sum to 1.0000
[2019-04-04 13:44:22,250] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3804
[2019-04-04 13:44:22,264] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.8, 72.33333333333334, 0.0, 0.0, 26.0, 25.16496980931746, 0.3266316087271897, 0.0, 1.0, 44971.07842639001], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2240400.0000, 
sim time next is 2241000.0000, 
raw observation next is [-5.9, 73.0, 0.0, 0.0, 26.0, 25.03924036224547, 0.3090821775264884, 0.0, 1.0, 44492.89273327334], 
processed observation next is [1.0, 0.9565217391304348, 0.2991689750692521, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5866033635204557, 0.6030273925088295, 0.0, 1.0, 0.2118709177774921], 
reward next is 0.7881, 
noisyNet noise sample is [array([-0.26406187], dtype=float32), -1.3115039]. 
=============================================
[2019-04-04 13:44:22,267] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[82.113945]
 [81.70815 ]
 [81.892166]
 [81.67812 ]
 [81.40427 ]], R is [[81.80426788]
 [81.77207947]
 [81.74078369]
 [81.71122742]
 [81.68138885]].
[2019-04-04 13:44:29,491] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5810595e-08 2.6820247e-08 1.0844629e-12 3.3757489e-12 1.0000000e+00
 1.8712910e-08 7.4071961e-13], sum to 1.0000
[2019-04-04 13:44:29,491] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9444
[2019-04-04 13:44:29,513] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.3, 25.33333333333333, 41.0, 239.6666666666667, 26.0, 25.05170966704432, 0.2615095305716432, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2479200.0000, 
sim time next is 2479800.0000, 
raw observation next is [3.3, 25.16666666666667, 34.0, 200.3333333333334, 26.0, 25.06727875301318, 0.2520628047858914, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.554016620498615, 0.2516666666666667, 0.11333333333333333, 0.2213627992633518, 0.6666666666666666, 0.5889398960844318, 0.5840209349286304, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3092145], dtype=float32), 0.14241041]. 
=============================================
[2019-04-04 13:44:32,464] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.71502273e-09 6.77731204e-10 9.39076714e-15 4.53474504e-14
 1.00000000e+00 2.12441162e-10 1.36893365e-14], sum to 1.0000
[2019-04-04 13:44:32,464] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9335
[2019-04-04 13:44:32,508] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.116666666666667, 59.50000000000001, 157.6666666666667, 354.0, 26.0, 24.99597742609051, 0.3210069424162291, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2373000.0000, 
sim time next is 2373600.0000, 
raw observation next is [-1.933333333333333, 57.0, 162.3333333333333, 330.0, 26.0, 25.03557440897085, 0.3150167105577149, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.40904893813481075, 0.57, 0.541111111111111, 0.36464088397790057, 0.6666666666666666, 0.5862978674142374, 0.605005570185905, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1371692], dtype=float32), -1.2506629]. 
=============================================
[2019-04-04 13:44:33,676] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7581507e-08 2.2998479e-09 3.6893103e-14 3.5194646e-14 1.0000000e+00
 8.9105462e-10 1.1235204e-14], sum to 1.0000
[2019-04-04 13:44:33,676] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7341
[2019-04-04 13:44:33,696] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.716666666666666, 26.5, 163.0, 344.0, 26.0, 25.73336736331238, 0.377844505248225, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2556600.0000, 
sim time next is 2557200.0000, 
raw observation next is [3.633333333333334, 27.0, 161.0, 309.5, 26.0, 25.80364328949536, 0.3863886431437154, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5632502308402586, 0.27, 0.5366666666666666, 0.3419889502762431, 0.6666666666666666, 0.6503036074579466, 0.6287962143812384, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5875304], dtype=float32), 0.05792834]. 
=============================================
[2019-04-04 13:44:34,339] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.9294738e-08 1.4576818e-09 2.5363050e-14 8.2631184e-13 9.9999988e-01
 1.3209737e-09 1.6003090e-13], sum to 1.0000
[2019-04-04 13:44:34,340] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5739
[2019-04-04 13:44:34,408] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 54.00000000000001, 0.0, 0.0, 26.0, 24.72557196749095, 0.1760451816855463, 1.0, 1.0, 88502.41405399455], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2532000.0000, 
sim time next is 2532600.0000, 
raw observation next is [-2.8, 54.0, 0.0, 0.0, 26.0, 25.01462190114459, 0.2109959148578787, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.38504155124653744, 0.54, 0.0, 0.0, 0.6666666666666666, 0.5845518250953825, 0.5703319716192928, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5188649], dtype=float32), 0.040922437]. 
=============================================
[2019-04-04 13:44:37,603] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1355043e-08 6.3057408e-09 2.1990720e-13 1.6598873e-12 1.0000000e+00
 5.2382223e-09 1.8562175e-13], sum to 1.0000
[2019-04-04 13:44:37,604] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4159
[2019-04-04 13:44:37,618] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.42834676795625, -0.1018728313016455, 0.0, 1.0, 44419.61308755147], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2436000.0000, 
sim time next is 2436600.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.39240037709386, -0.1097425176175304, 0.0, 1.0, 44407.42440962906], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.44936669809115504, 0.4634191607941565, 0.0, 1.0, 0.21146392576013837], 
reward next is 0.7885, 
noisyNet noise sample is [array([-0.28862807], dtype=float32), 0.7133553]. 
=============================================
[2019-04-04 13:44:42,771] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-04 13:44:42,775] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 13:44:42,776] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:44:42,776] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 13:44:42,778] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run18
[2019-04-04 13:44:42,797] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:44:42,799] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 13:44:42,800] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:44:42,806] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run18
[2019-04-04 13:44:42,829] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run18
[2019-04-04 13:45:25,086] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16822419], dtype=float32), 0.19878046]
[2019-04-04 13:45:25,087] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.3, 83.0, 0.0, 0.0, 26.0, 25.31664414100007, 0.5198585339234211, 0.0, 1.0, 58897.87470836083]
[2019-04-04 13:45:25,087] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 13:45:25,089] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.1541364e-10 1.7930596e-11 1.2705186e-16 1.4627547e-15 1.0000000e+00
 7.7840139e-12 2.0369079e-16], sampled 0.4024276295984752
[2019-04-04 13:46:24,893] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4172 239942393.0563 1605.0250
[2019-04-04 13:46:30,516] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16822419], dtype=float32), 0.19878046]
[2019-04-04 13:46:30,517] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.24747399, 41.4967995, 0.0, 0.0, 26.0, 25.49895986514828, 0.4446963181766748, 0.0, 1.0, 18750.67240055845]
[2019-04-04 13:46:30,517] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 13:46:30,518] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.4231555e-09 8.3624968e-10 2.4953267e-14 1.3555171e-13 1.0000000e+00
 4.4513551e-10 3.1696593e-14], sampled 0.2640386531162383
[2019-04-04 13:46:42,899] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 13:46:46,870] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 13:46:47,894] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1700000, evaluation results [1700000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.417175922201, 239942393.05634084, 1605.024971488376, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 13:46:51,420] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.94647895e-10 1.08155755e-10 1.03704277e-15 6.49420754e-15
 1.00000000e+00 9.31386912e-11 2.05433378e-15], sum to 1.0000
[2019-04-04 13:46:51,421] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7808
[2019-04-04 13:46:51,436] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.3, 61.0, 0.0, 0.0, 26.0, 24.89250553431097, 0.2745739458620014, 0.0, 1.0, 41888.35314208679], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2590800.0000, 
sim time next is 2591400.0000, 
raw observation next is [-4.399999999999999, 61.5, 0.0, 0.0, 26.0, 24.85116935423477, 0.2644473503913863, 0.0, 1.0, 41886.93867216104], 
processed observation next is [1.0, 1.0, 0.3407202216066483, 0.615, 0.0, 0.0, 0.6666666666666666, 0.5709307795195642, 0.5881491167971288, 0.0, 1.0, 0.19946161272457638], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.40850312], dtype=float32), 1.2914481]. 
=============================================
[2019-04-04 13:46:51,798] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9240180e-09 6.0980021e-10 6.4850727e-15 9.7056613e-14 1.0000000e+00
 3.5879633e-10 8.4423664e-15], sum to 1.0000
[2019-04-04 13:46:51,798] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4971
[2019-04-04 13:46:51,815] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.96495603882387, 0.2425397325970602, 0.0, 1.0, 41596.16613189179], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2599800.0000, 
sim time next is 2600400.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.92707507397319, 0.2334994421087802, 0.0, 1.0, 41630.43321410431], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5772562561644324, 0.5778331473695933, 0.0, 1.0, 0.19824015816240148], 
reward next is 0.8018, 
noisyNet noise sample is [array([-1.2430165], dtype=float32), 1.1508847]. 
=============================================
[2019-04-04 13:46:56,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4396964e-10 1.3054711e-10 8.3352251e-16 3.6780665e-15 1.0000000e+00
 5.5273650e-11 4.4402841e-16], sum to 1.0000
[2019-04-04 13:46:56,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4117
[2019-04-04 13:46:56,842] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.9999999999999999, 46.0, 133.8333333333333, 750.0, 26.0, 25.98502923890942, 0.4513831484146639, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2806800.0000, 
sim time next is 2807400.0000, 
raw observation next is [1.5, 45.0, 142.6666666666667, 737.0, 26.0, 25.96430756185514, 0.4453931654403472, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5041551246537397, 0.45, 0.47555555555555573, 0.8143646408839779, 0.6666666666666666, 0.6636922968212616, 0.6484643884801157, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4395587], dtype=float32), -0.041485555]. 
=============================================
[2019-04-04 13:47:06,526] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3108495e-09 2.3763030e-10 1.4991699e-14 7.0162395e-14 1.0000000e+00
 8.7096920e-11 2.1707693e-15], sum to 1.0000
[2019-04-04 13:47:06,526] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7786
[2019-04-04 13:47:06,536] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 51.66666666666666, 165.8333333333333, 550.5, 26.0, 26.02863145029203, 0.45587645292657, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2803200.0000, 
sim time next is 2803800.0000, 
raw observation next is [-1.333333333333333, 50.83333333333334, 157.6666666666667, 593.0, 26.0, 26.0231780871031, 0.4632928843119255, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.5083333333333334, 0.5255555555555557, 0.6552486187845303, 0.6666666666666666, 0.6685981739252584, 0.6544309614373085, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.80107003], dtype=float32), 0.11680959]. 
=============================================
[2019-04-04 13:47:09,415] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.5012262e-10 8.2116839e-12 2.2395551e-17 2.4465528e-15 1.0000000e+00
 2.5019776e-12 2.1239960e-16], sum to 1.0000
[2019-04-04 13:47:09,416] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3151
[2019-04-04 13:47:09,445] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333334, 97.66666666666666, 53.83333333333333, 0.0, 26.0, 25.4305890704838, 0.312769240083818, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2886000.0000, 
sim time next is 2886600.0000, 
raw observation next is [0.1666666666666666, 98.83333333333334, 58.66666666666666, 0.0, 26.0, 25.46391368836634, 0.307690770148055, 1.0, 1.0, 18682.08054216582], 
processed observation next is [1.0, 0.391304347826087, 0.4672206832871654, 0.9883333333333334, 0.1955555555555555, 0.0, 0.6666666666666666, 0.6219928073638616, 0.6025635900493517, 1.0, 1.0, 0.0889622882960277], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.8642364], dtype=float32), 0.5576873]. 
=============================================
[2019-04-04 13:47:11,252] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1156894e-10 1.4053473e-11 2.3239323e-17 1.9088766e-15 1.0000000e+00
 1.7255300e-11 4.8412555e-17], sum to 1.0000
[2019-04-04 13:47:11,252] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3167
[2019-04-04 13:47:11,282] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 100.0, 167.6666666666667, 0.0, 26.0, 25.2531741270713, 0.3260485361774192, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2898600.0000, 
sim time next is 2899200.0000, 
raw observation next is [2.0, 100.0, 165.8333333333333, 0.0, 26.0, 25.31378375510884, 0.3312690078045213, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.5527777777777776, 0.0, 0.6666666666666666, 0.6094819795924034, 0.6104230026015071, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23794097], dtype=float32), 1.1412139]. 
=============================================
[2019-04-04 13:47:12,793] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7516009e-10 7.7571578e-12 4.1923476e-17 2.5226168e-15 1.0000000e+00
 8.2052334e-12 6.1815578e-17], sum to 1.0000
[2019-04-04 13:47:12,794] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4894
[2019-04-04 13:47:12,827] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 92.0, 101.0, 653.0, 26.0, 26.17394809891289, 0.6516360758787385, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3231000.0000, 
sim time next is 3231600.0000, 
raw observation next is [-3.0, 92.0, 102.3333333333333, 669.5, 26.0, 26.27178583250776, 0.662875395004535, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.341111111111111, 0.7397790055248619, 0.6666666666666666, 0.6893154860423133, 0.7209584650015116, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.70250046], dtype=float32), -1.0770243]. 
=============================================
[2019-04-04 13:47:15,286] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.1932968e-09 1.7360097e-09 7.1269196e-14 1.6859551e-13 1.0000000e+00
 8.2834567e-10 5.2516544e-14], sum to 1.0000
[2019-04-04 13:47:15,286] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9066
[2019-04-04 13:47:15,309] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.333333333333334, 73.0, 0.0, 0.0, 26.0, 24.39573097336552, 0.1408401695223522, 0.0, 1.0, 38752.03795448194], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3032400.0000, 
sim time next is 3033000.0000, 
raw observation next is [-5.5, 74.0, 0.0, 0.0, 26.0, 24.36364506402482, 0.1327258081103722, 0.0, 1.0, 38846.11185922774], 
processed observation next is [0.0, 0.08695652173913043, 0.3102493074792244, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5303037553354016, 0.5442419360367907, 0.0, 1.0, 0.18498148504394163], 
reward next is 0.8150, 
noisyNet noise sample is [array([0.4988254], dtype=float32), -0.26277938]. 
=============================================
[2019-04-04 13:47:15,317] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[75.86777]
 [75.7469 ]
 [75.68673]
 [75.7983 ]
 [75.56643]], R is [[76.16763306]
 [76.22142792]
 [76.27520752]
 [76.32902527]
 [76.38300323]].
[2019-04-04 13:47:28,172] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.8204860e-10 1.6752441e-11 2.5190947e-17 2.2939967e-15 1.0000000e+00
 1.0408694e-11 1.3032605e-16], sum to 1.0000
[2019-04-04 13:47:28,175] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2999
[2019-04-04 13:47:28,185] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.7, 86.0, 109.0, 752.0, 26.0, 26.41784014021671, 0.6985350907655655, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3234600.0000, 
sim time next is 3235200.0000, 
raw observation next is [-2.6, 84.0, 109.6666666666667, 761.8333333333334, 26.0, 26.4105230403594, 0.7062100937429414, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3905817174515236, 0.84, 0.3655555555555557, 0.841804788213628, 0.6666666666666666, 0.7008769200299501, 0.7354033645809804, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09931967], dtype=float32), 0.5118582]. 
=============================================
[2019-04-04 13:47:29,387] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7312086e-10 1.7864099e-11 1.1443465e-16 5.9384935e-16 1.0000000e+00
 1.9129772e-11 1.3050415e-16], sum to 1.0000
[2019-04-04 13:47:29,389] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4891
[2019-04-04 13:47:29,410] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.5, 71.0, 85.0, 686.0, 26.0, 25.60753027918174, 0.7154418481255552, 1.0, 1.0, 115883.7933061315], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3252600.0000, 
sim time next is 3253200.0000, 
raw observation next is [-2.666666666666667, 71.0, 80.66666666666667, 656.8333333333334, 26.0, 26.18355889216301, 0.7733459111980344, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.38873499538319484, 0.71, 0.2688888888888889, 0.7257826887661142, 0.6666666666666666, 0.6819632410135842, 0.7577819703993448, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.54748917], dtype=float32), 1.303887]. 
=============================================
[2019-04-04 13:47:38,755] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.06331724e-09 9.79236484e-11 7.93307394e-16 2.18136666e-14
 1.00000000e+00 1.00372495e-10 1.79335361e-15], sum to 1.0000
[2019-04-04 13:47:38,767] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9248
[2019-04-04 13:47:38,784] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23059286378188, 0.3778970944892746, 0.0, 1.0, 41684.80530548666], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3479400.0000, 
sim time next is 3480000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23974316045054, 0.3792055114394144, 0.0, 1.0, 41599.79770966426], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.603311930037545, 0.6264018371464715, 0.0, 1.0, 0.19809427480792505], 
reward next is 0.8019, 
noisyNet noise sample is [array([-0.20766354], dtype=float32), -0.71346927]. 
=============================================
[2019-04-04 13:47:38,790] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.98817]
 [86.0325 ]
 [86.07626]
 [86.10202]
 [86.11389]], R is [[85.89385986]
 [85.83641815]
 [85.77922058]
 [85.72235107]
 [85.66584778]].
[2019-04-04 13:47:43,313] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1273448e-08 1.4301369e-09 2.9837535e-14 1.8788648e-13 1.0000000e+00
 2.8788277e-10 2.2049887e-14], sum to 1.0000
[2019-04-04 13:47:43,315] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3748
[2019-04-04 13:47:43,330] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 51.00000000000001, 0.0, 0.0, 26.0, 25.54391034745026, 0.5045311963995434, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3871200.0000, 
sim time next is 3871800.0000, 
raw observation next is [1.0, 51.0, 0.0, 0.0, 26.0, 25.4641803493708, 0.4885570690338629, 0.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.51, 0.0, 0.0, 0.6666666666666666, 0.6220150291142333, 0.662852356344621, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.7156763], dtype=float32), 0.34493205]. 
=============================================
[2019-04-04 13:47:47,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0169354e-09 1.7145210e-10 8.7770602e-16 4.3624170e-15 1.0000000e+00
 1.4906333e-10 2.0437961e-16], sum to 1.0000
[2019-04-04 13:47:47,198] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5270
[2019-04-04 13:47:47,214] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.0, 87.0, 711.0, 26.0, 26.82438405173616, 0.696058528854881, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3771000.0000, 
sim time next is 3771600.0000, 
raw observation next is [0.0, 60.0, 83.16666666666666, 682.3333333333333, 26.0, 26.80690857454585, 0.699896111447219, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.2772222222222222, 0.7539594843462246, 0.6666666666666666, 0.7339090478788209, 0.7332987038157396, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3969282], dtype=float32), 0.020614196]. 
=============================================
[2019-04-04 13:47:47,653] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9017243e-08 7.2471562e-09 1.2657604e-13 3.7860698e-13 1.0000000e+00
 1.4147672e-09 3.1368334e-14], sum to 1.0000
[2019-04-04 13:47:47,660] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2736
[2019-04-04 13:47:47,670] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.166666666666666, 27.33333333333333, 0.0, 0.0, 26.0, 25.47450123135517, 0.3782335464260771, 0.0, 1.0, 36726.58655489114], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3640200.0000, 
sim time next is 3640800.0000, 
raw observation next is [8.133333333333333, 27.66666666666667, 0.0, 0.0, 26.0, 25.58811054716416, 0.3783989453896491, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.687903970452447, 0.2766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6323425455970133, 0.6261329817965496, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03276637], dtype=float32), -0.14633025]. 
=============================================
[2019-04-04 13:47:52,632] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0956779e-08 2.9955635e-10 5.0817716e-15 1.5017686e-14 1.0000000e+00
 2.3498639e-10 1.8269114e-15], sum to 1.0000
[2019-04-04 13:47:52,635] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9842
[2019-04-04 13:47:52,650] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 58.33333333333334, 0.0, 0.0, 26.0, 25.71686191274123, 0.5729302177558057, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3878400.0000, 
sim time next is 3879000.0000, 
raw observation next is [-1.0, 57.5, 0.0, 0.0, 26.0, 25.7278478974217, 0.5694539652552745, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.575, 0.0, 0.0, 0.6666666666666666, 0.6439873247851416, 0.6898179884184249, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48580602], dtype=float32), 2.8489907]. 
=============================================
[2019-04-04 13:47:52,662] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[84.03812]
 [83.85087]
 [83.66089]
 [83.24024]
 [82.71996]], R is [[84.51447296]
 [84.66932678]
 [84.82263184]
 [84.8850708 ]
 [84.62717438]].
[2019-04-04 13:47:54,004] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.1579497e-10 1.5675727e-10 5.1733077e-16 5.2212890e-15 1.0000000e+00
 6.6417122e-11 6.8955825e-16], sum to 1.0000
[2019-04-04 13:47:54,007] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7761
[2019-04-04 13:47:54,040] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 48.0, 96.5, 749.5, 26.0, 26.33262172764936, 0.6767175441297953, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3855600.0000, 
sim time next is 3856200.0000, 
raw observation next is [2.166666666666667, 47.5, 93.33333333333334, 738.6666666666667, 26.0, 25.70808299438855, 0.6099491951463383, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5226223453370269, 0.475, 0.3111111111111111, 0.816206261510129, 0.6666666666666666, 0.6423402495323792, 0.7033163983821128, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7571029], dtype=float32), 0.85989815]. 
=============================================
[2019-04-04 13:47:54,148] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.81769233e-09 1.01602546e-10 3.65246578e-15 4.02243580e-15
 1.00000000e+00 5.28982379e-11 1.45280063e-15], sum to 1.0000
[2019-04-04 13:47:54,150] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1644
[2019-04-04 13:47:54,199] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 72.0, 32.99999999999999, 233.6666666666666, 26.0, 25.2583466959872, 0.3102668826573949, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3743400.0000, 
sim time next is 3744000.0000, 
raw observation next is [-4.0, 71.0, 47.0, 282.5, 26.0, 25.25311885055592, 0.3018313445135863, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.71, 0.15666666666666668, 0.31215469613259667, 0.6666666666666666, 0.6044265708796601, 0.6006104481711955, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0240962], dtype=float32), 1.551739]. 
=============================================
[2019-04-04 13:47:54,212] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[85.63245]
 [85.16008]
 [84.76862]
 [84.1082 ]
 [83.63485]], R is [[85.75173187]
 [85.89421844]
 [86.03527832]
 [86.17492676]
 [86.31317902]].
[2019-04-04 13:47:55,410] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.77519918e-09 2.28550803e-10 2.12321937e-15 3.73876785e-15
 1.00000000e+00 1.00382265e-10 4.66359559e-16], sum to 1.0000
[2019-04-04 13:47:55,411] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6030
[2019-04-04 13:47:55,419] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.1666666666666666, 60.0, 112.0, 804.0, 26.0, 26.48311869934629, 0.6415045810820222, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3765000.0000, 
sim time next is 3765600.0000, 
raw observation next is [0.0, 60.0, 110.5, 797.0, 26.0, 26.57166844082699, 0.653216851824443, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.6, 0.36833333333333335, 0.8806629834254144, 0.6666666666666666, 0.7143057034022492, 0.7177389506081476, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17496178], dtype=float32), -0.69524217]. 
=============================================
[2019-04-04 13:47:59,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7365230e-08 1.4426847e-09 2.0472716e-13 1.1282201e-12 1.0000000e+00
 2.4898847e-09 1.5088525e-13], sum to 1.0000
[2019-04-04 13:47:59,580] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7218
[2019-04-04 13:47:59,613] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.46752981463541, 0.4701228201380851, 1.0, 1.0, 26896.08834129809], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4045200.0000, 
sim time next is 4045800.0000, 
raw observation next is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.4202831775993, 0.4606552047505681, 0.0, 1.0, 26872.6727881748], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.31, 0.0, 0.0, 0.6666666666666666, 0.6183569314666082, 0.653551734916856, 0.0, 1.0, 0.1279651085151181], 
reward next is 0.8720, 
noisyNet noise sample is [array([0.08357438], dtype=float32), -0.8819807]. 
=============================================
[2019-04-04 13:48:03,725] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2431556e-08 2.8856793e-09 7.0136171e-14 2.9923497e-13 1.0000000e+00
 4.6757587e-10 3.0255197e-14], sum to 1.0000
[2019-04-04 13:48:03,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5464
[2019-04-04 13:48:03,752] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.18010926716924, 0.1137317576011574, 0.0, 1.0, 43643.97458171876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3988800.0000, 
sim time next is 3989400.0000, 
raw observation next is [-12.16666666666667, 64.0, 0.0, 0.0, 26.0, 24.11390088048125, 0.1050048334163131, 0.0, 1.0, 43686.45603234225], 
processed observation next is [1.0, 0.17391304347826086, 0.12557710064635264, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5094917400401041, 0.5350016111387711, 0.0, 1.0, 0.20803074301115357], 
reward next is 0.7920, 
noisyNet noise sample is [array([-0.11829711], dtype=float32), -0.7462997]. 
=============================================
[2019-04-04 13:48:04,712] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.1084689e-08 1.6848495e-09 4.1347561e-14 1.3672226e-13 1.0000000e+00
 7.0277224e-09 3.2033543e-14], sum to 1.0000
[2019-04-04 13:48:04,714] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3844
[2019-04-04 13:48:04,747] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.333333333333334, 39.0, 26.83333333333333, 230.1666666666667, 26.0, 26.47572727583142, 0.4668084704441686, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3950400.0000, 
sim time next is 3951000.0000, 
raw observation next is [-5.5, 39.5, 19.0, 169.0, 26.0, 26.33210758692647, 0.534040172294755, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3102493074792244, 0.395, 0.06333333333333334, 0.1867403314917127, 0.6666666666666666, 0.6943422989105391, 0.6780133907649183, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14462921], dtype=float32), 0.7671752]. 
=============================================
[2019-04-04 13:48:04,756] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[75.65985]
 [76.12863]
 [76.4708 ]
 [76.68928]
 [77.12484]], R is [[75.41629028]
 [75.66212463]
 [75.90550232]
 [76.14644623]
 [76.38497925]].
[2019-04-04 13:48:07,760] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.8943842e-09 7.4075041e-10 5.3832637e-14 8.8944985e-14 1.0000000e+00
 4.7760079e-10 8.7728134e-15], sum to 1.0000
[2019-04-04 13:48:07,763] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0910
[2019-04-04 13:48:07,795] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 40.33333333333334, 0.0, 0.0, 26.0, 24.95317912054168, 0.3141228010614477, 0.0, 1.0, 40657.5725994301], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4063800.0000, 
sim time next is 4064400.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 24.96778003632974, 0.3099412061108756, 0.0, 1.0, 40704.83314248282], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5806483363608116, 0.6033137353702919, 0.0, 1.0, 0.19383253877372772], 
reward next is 0.8062, 
noisyNet noise sample is [array([-0.01550744], dtype=float32), -0.6183643]. 
=============================================
[2019-04-04 13:48:08,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.6551962e-09 2.6868349e-10 5.8589985e-15 2.2213346e-13 1.0000000e+00
 2.8418823e-10 2.5402754e-14], sum to 1.0000
[2019-04-04 13:48:08,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8043
[2019-04-04 13:48:08,832] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.16666666666667, 64.0, 30.99999999999999, 148.0, 26.0, 24.69521671671652, 0.2813574238787795, 1.0, 1.0, 104669.3607810103], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4002600.0000, 
sim time next is 4003200.0000, 
raw observation next is [-13.0, 63.0, 46.5, 222.0, 26.0, 25.20851512688009, 0.328579998152102, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.10249307479224376, 0.63, 0.155, 0.24530386740331492, 0.6666666666666666, 0.6007095939066742, 0.6095266660507007, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34178945], dtype=float32), -1.106228]. 
=============================================
[2019-04-04 13:48:11,999] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2273476e-09 2.1460258e-09 4.7740308e-14 6.4661411e-14 1.0000000e+00
 9.0830975e-11 3.3891841e-14], sum to 1.0000
[2019-04-04 13:48:12,002] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3667
[2019-04-04 13:48:12,016] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 26.0, 25.33290802568012, 0.4149976893336213, 0.0, 1.0, 44753.07046064206], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4152000.0000, 
sim time next is 4152600.0000, 
raw observation next is [-1.5, 42.5, 0.0, 0.0, 26.0, 25.33235320175747, 0.4152835182088061, 0.0, 1.0, 40992.59873842094], 
processed observation next is [0.0, 0.043478260869565216, 0.4210526315789474, 0.425, 0.0, 0.0, 0.6666666666666666, 0.6110294334797892, 0.6384278394029353, 0.0, 1.0, 0.19520285113533778], 
reward next is 0.8048, 
noisyNet noise sample is [array([0.20088202], dtype=float32), 0.4111305]. 
=============================================
[2019-04-04 13:48:15,026] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2318614e-08 5.3072222e-09 1.3996580e-14 3.8446869e-13 1.0000000e+00
 2.2936350e-10 1.1311371e-13], sum to 1.0000
[2019-04-04 13:48:15,032] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4450
[2019-04-04 13:48:15,058] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.22077488221958, 0.3604469134789903, 0.0, 1.0, 39474.13809205234], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4158600.0000, 
sim time next is 4159200.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.19686639389106, 0.3612233873509911, 0.0, 1.0, 39490.42706737509], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5997388661575883, 0.6204077957836637, 0.0, 1.0, 0.18804965270178614], 
reward next is 0.8120, 
noisyNet noise sample is [array([0.90096235], dtype=float32), 0.28810483]. 
=============================================
[2019-04-04 13:48:16,261] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4112040e-09 9.3075159e-10 9.5468779e-15 4.6630580e-14 1.0000000e+00
 3.7747477e-10 4.0436053e-15], sum to 1.0000
[2019-04-04 13:48:16,261] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4588
[2019-04-04 13:48:16,276] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.666666666666667, 41.0, 0.0, 0.0, 26.0, 25.5967273904132, 0.4940939015067185, 0.0, 1.0, 32923.78100424021], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4131600.0000, 
sim time next is 4132200.0000, 
raw observation next is [1.333333333333333, 42.0, 0.0, 0.0, 26.0, 25.51277269273606, 0.482170186264569, 1.0, 1.0, 27121.62227446763], 
processed observation next is [1.0, 0.8260869565217391, 0.4995383194829178, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6260643910613384, 0.660723395421523, 1.0, 1.0, 0.12915058225936968], 
reward next is 0.8708, 
noisyNet noise sample is [array([-0.62873816], dtype=float32), -0.46724465]. 
=============================================
[2019-04-04 13:48:25,319] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1954022e-10 2.1805339e-11 6.2902700e-16 8.3688057e-16 1.0000000e+00
 4.2309168e-12 1.4107304e-17], sum to 1.0000
[2019-04-04 13:48:25,320] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1372
[2019-04-04 13:48:25,378] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.333333333333333, 81.66666666666667, 130.5, 0.0, 26.0, 24.91481206413645, 0.3906734964954694, 1.0, 1.0, 196217.9094192962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4713600.0000, 
sim time next is 4714200.0000, 
raw observation next is [1.5, 79.5, 135.0, 0.0, 26.0, 24.35242732746798, 0.3896350722946837, 1.0, 1.0, 196918.6673485938], 
processed observation next is [1.0, 0.5652173913043478, 0.5041551246537397, 0.795, 0.45, 0.0, 0.6666666666666666, 0.5293689439556649, 0.6298783574315613, 1.0, 1.0, 0.9377079397552085], 
reward next is 0.0623, 
noisyNet noise sample is [array([-0.9584096], dtype=float32), 0.6829916]. 
=============================================
[2019-04-04 13:48:27,984] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.9328255e-10 6.8921846e-10 7.1089104e-16 3.1361693e-14 1.0000000e+00
 1.0465654e-10 1.2793855e-15], sum to 1.0000
[2019-04-04 13:48:27,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7882
[2019-04-04 13:48:28,005] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.35, 65.33333333333334, 0.0, 0.0, 26.0, 25.47510669263736, 0.4410683924325924, 0.0, 1.0, 18755.78872383344], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4587000.0000, 
sim time next is 4587600.0000, 
raw observation next is [-0.5, 65.66666666666667, 0.0, 0.0, 26.0, 25.4565861546393, 0.4423125353956092, 0.0, 1.0, 30412.30899794392], 
processed observation next is [1.0, 0.08695652173913043, 0.44875346260387816, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.621382179553275, 0.6474375117985364, 0.0, 1.0, 0.1448205190378282], 
reward next is 0.8552, 
noisyNet noise sample is [array([0.7630065], dtype=float32), -1.4660279]. 
=============================================
[2019-04-04 13:48:30,278] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3555360e-10 1.8222448e-11 1.3760659e-16 4.3374747e-16 1.0000000e+00
 2.0687846e-11 1.9951868e-17], sum to 1.0000
[2019-04-04 13:48:30,279] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4116
[2019-04-04 13:48:30,322] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.26160022416901, 0.4355885800641183, 0.0, 1.0, 90406.01736711986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4498800.0000, 
sim time next is 4499400.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.29523454592449, 0.4376006386716727, 0.0, 1.0, 61183.77793757566], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6079362121603742, 0.6458668795572242, 0.0, 1.0, 0.29135132351226506], 
reward next is 0.7086, 
noisyNet noise sample is [array([-0.38066268], dtype=float32), -0.6746374]. 
=============================================
[2019-04-04 13:48:32,425] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1880994e-09 5.3723272e-11 1.0209838e-16 4.1564750e-15 1.0000000e+00
 2.8105995e-11 1.3400225e-15], sum to 1.0000
[2019-04-04 13:48:32,425] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6866
[2019-04-04 13:48:32,444] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6666666666666666, 97.33333333333333, 0.0, 0.0, 26.0, 25.59583385263272, 0.4539671738277717, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4682400.0000, 
sim time next is 4683000.0000, 
raw observation next is [-0.8333333333333334, 98.66666666666667, 0.0, 0.0, 26.0, 25.58976972574073, 0.4489385399349252, 0.0, 1.0, 18736.72369368624], 
processed observation next is [1.0, 0.17391304347826086, 0.43951985226223456, 0.9866666666666667, 0.0, 0.0, 0.6666666666666666, 0.6324808104783942, 0.6496461799783084, 0.0, 1.0, 0.08922249377945828], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.7441917], dtype=float32), -1.5043275]. 
=============================================
[2019-04-04 13:48:32,466] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[89.52177]
 [89.29093]
 [89.02759]
 [88.73269]
 [88.34536]], R is [[89.85202026]
 [89.95349884]
 [90.05396271]
 [90.15342712]
 [90.25189209]].
[2019-04-04 13:48:43,686] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.06335679e-10 2.41919401e-11 1.10588198e-15 2.83171373e-15
 1.00000000e+00 1.36745094e-11 6.03755622e-17], sum to 1.0000
[2019-04-04 13:48:43,687] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1989
[2019-04-04 13:48:43,733] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.01760069253731, 0.4456702865676885, 1.0, 1.0, 96097.3391542815], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4734000.0000, 
sim time next is 4734600.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.07614636153299, 0.4730804891406905, 1.0, 1.0, 26042.27007258084], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5896788634610827, 0.6576934963802302, 1.0, 1.0, 0.12401080986943257], 
reward next is 0.8760, 
noisyNet noise sample is [array([-1.088348], dtype=float32), -0.09128309]. 
=============================================
[2019-04-04 13:48:43,970] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.7896656e-10 1.7081680e-11 4.5871445e-16 7.8419115e-15 1.0000000e+00
 3.1971575e-11 3.4763754e-16], sum to 1.0000
[2019-04-04 13:48:43,973] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9432
[2019-04-04 13:48:43,990] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 79.66666666666667, 0.0, 0.0, 26.0, 25.0891125703758, 0.3911404619081342, 0.0, 1.0, 41416.53040525907], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4753200.0000, 
sim time next is 4753800.0000, 
raw observation next is [-4.0, 77.5, 0.0, 0.0, 26.0, 25.0592347747295, 0.3817160972515389, 0.0, 1.0, 41356.04529731512], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.775, 0.0, 0.0, 0.6666666666666666, 0.5882695645607917, 0.6272386990838462, 0.0, 1.0, 0.1969335490348339], 
reward next is 0.8031, 
noisyNet noise sample is [array([0.46499002], dtype=float32), 0.27593544]. 
=============================================
[2019-04-04 13:48:46,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:48:46,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:48:46,184] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run14
[2019-04-04 13:48:48,342] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.4278758e-09 1.1766049e-09 2.3457214e-14 1.8238564e-13 1.0000000e+00
 1.6212212e-10 5.9778198e-14], sum to 1.0000
[2019-04-04 13:48:48,343] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0856
[2019-04-04 13:48:48,358] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.5, 37.0, 33.0, 185.0, 26.0, 25.09871162964572, 0.3756473217492647, 0.0, 1.0, 28329.16110285446], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4815000.0000, 
sim time next is 4815600.0000, 
raw observation next is [2.333333333333333, 38.0, 27.5, 154.1666666666667, 26.0, 25.0779093492529, 0.3662465608061624, 0.0, 1.0, 33700.83357700866], 
processed observation next is [0.0, 0.7391304347826086, 0.5272391505078486, 0.38, 0.09166666666666666, 0.17034990791896876, 0.6666666666666666, 0.5898257791044083, 0.6220821869353875, 0.0, 1.0, 0.16048015989051745], 
reward next is 0.8395, 
noisyNet noise sample is [array([0.6058958], dtype=float32), -2.593371]. 
=============================================
[2019-04-04 13:48:52,256] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:48:52,256] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:48:52,350] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run14
[2019-04-04 13:48:53,947] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:48:53,947] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:48:53,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run14
[2019-04-04 13:48:54,380] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4781853e-09 1.2663756e-10 1.6972979e-15 2.5883182e-14 1.0000000e+00
 1.0885153e-10 1.0052399e-15], sum to 1.0000
[2019-04-04 13:48:54,382] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0840
[2019-04-04 13:48:54,399] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.30702113190092, 0.2984046438106353, 0.0, 1.0, 75984.04953804151], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4935000.0000, 
sim time next is 4935600.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.29346041779743, 0.2949726789157112, 0.0, 1.0, 54685.52696353821], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6077883681497859, 0.5983242263052371, 0.0, 1.0, 0.26040727125494384], 
reward next is 0.7396, 
noisyNet noise sample is [array([-0.05408798], dtype=float32), -1.8144699]. 
=============================================
[2019-04-04 13:48:57,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:48:57,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:48:57,466] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run14
[2019-04-04 13:48:58,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:48:58,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:48:58,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run14
[2019-04-04 13:48:58,732] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:48:58,733] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:48:58,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run14
[2019-04-04 13:48:59,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:48:59,224] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:48:59,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run14
[2019-04-04 13:49:00,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:49:00,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:49:00,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run14
[2019-04-04 13:49:01,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:49:01,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:49:01,204] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run14
[2019-04-04 13:49:01,668] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:49:01,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:49:01,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run14
[2019-04-04 13:49:01,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:49:01,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:49:01,841] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run14
[2019-04-04 13:49:01,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:49:01,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:49:01,908] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run14
[2019-04-04 13:49:02,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:49:02,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:49:02,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run14
[2019-04-04 13:49:02,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:49:02,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:49:02,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run14
[2019-04-04 13:49:02,634] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:49:02,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:49:02,639] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run14
[2019-04-04 13:49:02,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:49:02,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:49:02,940] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run14
[2019-04-04 13:49:07,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3183661e-09 2.2362337e-10 1.7211080e-16 1.5573744e-14 1.0000000e+00
 1.6562102e-11 2.7246652e-16], sum to 1.0000
[2019-04-04 13:49:07,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7126
[2019-04-04 13:49:07,901] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 35.16666666666666, 0.0, 26.0, 23.04718312764531, -0.1797697358941687, 0.0, 1.0, 58937.84089948938], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 33600.0000, 
sim time next is 34200.0000, 
raw observation next is [7.7, 93.0, 38.0, 0.0, 26.0, 23.14058771585637, -0.157264808138463, 0.0, 1.0, 58670.66147320053], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.12666666666666668, 0.0, 0.6666666666666666, 0.4283823096546975, 0.447578397287179, 0.0, 1.0, 0.2793841022533359], 
reward next is 0.7206, 
noisyNet noise sample is [array([-0.8643842], dtype=float32), 0.09569356]. 
=============================================
[2019-04-04 13:49:16,735] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.4528847e-09 2.4602231e-10 1.6268928e-14 8.3705981e-15 1.0000000e+00
 1.7717686e-10 2.8900768e-15], sum to 1.0000
[2019-04-04 13:49:16,738] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7441
[2019-04-04 13:49:16,831] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 26.0, 23.10619738486687, -0.1290619601495465, 0.0, 1.0, 45874.74106586212], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 112200.0000, 
sim time next is 112800.0000, 
raw observation next is [-7.300000000000001, 68.0, 0.0, 0.0, 26.0, 23.0369534543634, -0.05304413202746705, 1.0, 1.0, 202334.8138321356], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.6666666666666666, 0.41974612119694993, 0.48231862265751096, 1.0, 1.0, 0.9634991134863601], 
reward next is 0.0365, 
noisyNet noise sample is [array([-0.93434733], dtype=float32), -0.8337161]. 
=============================================
[2019-04-04 13:49:17,975] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.07943805e-08 1.48410062e-09 2.64287303e-15 2.08723521e-14
 1.00000000e+00 1.10409751e-10 1.71505950e-15], sum to 1.0000
[2019-04-04 13:49:17,975] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0271
[2019-04-04 13:49:18,038] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 68.0, 18.5, 4.5, 26.0, 24.78850633197773, 0.2282358149975416, 1.0, 1.0, 9408.013234103682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 115200.0000, 
sim time next is 115800.0000, 
raw observation next is [-7.383333333333333, 66.83333333333334, 24.66666666666667, 6.000000000000001, 26.0, 24.9729873472627, 0.2308999139553971, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.25807940904893817, 0.6683333333333334, 0.08222222222222224, 0.006629834254143647, 0.6666666666666666, 0.5810822789385582, 0.5769666379851324, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45757753], dtype=float32), -0.08365699]. 
=============================================
[2019-04-04 13:49:22,651] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9433238e-09 1.4928327e-10 1.3084169e-15 6.9393636e-15 1.0000000e+00
 1.4363773e-10 1.9129515e-15], sum to 1.0000
[2019-04-04 13:49:22,655] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2286
[2019-04-04 13:49:22,694] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.99268004266057, 0.06961103160991532, 0.0, 1.0, 44811.6906197619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 170400.0000, 
sim time next is 171000.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.95605063050559, 0.0598040208786553, 0.0, 1.0, 44755.52300635851], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.4963375525421325, 0.5199346736262184, 0.0, 1.0, 0.2131215381255167], 
reward next is 0.7869, 
noisyNet noise sample is [array([2.0934255], dtype=float32), 0.7807658]. 
=============================================
[2019-04-04 13:49:22,697] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[81.101685]
 [81.217224]
 [81.2175  ]
 [81.316536]
 [81.08618 ]], R is [[80.99495697]
 [80.97161865]
 [80.94826508]
 [80.9249115 ]
 [80.90155029]].
[2019-04-04 13:49:47,202] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1733157e-08 2.1130611e-09 7.2597554e-14 1.2446977e-12 1.0000000e+00
 7.7706162e-09 8.3785572e-14], sum to 1.0000
[2019-04-04 13:49:47,203] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6300
[2019-04-04 13:49:47,285] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 37.0, 26.0, 0.0, 26.0, 25.06583838558965, 0.178426747180553, 1.0, 1.0, 34638.81225628891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 490800.0000, 
sim time next is 491400.0000, 
raw observation next is [1.1, 38.5, 20.0, 0.0, 26.0, 24.34788797131685, 0.1720396030547403, 1.0, 1.0, 197728.5200991297], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.385, 0.06666666666666667, 0.0, 0.6666666666666666, 0.5289906642764043, 0.5573465343515801, 1.0, 1.0, 0.9415643814244271], 
reward next is 0.0584, 
noisyNet noise sample is [array([0.23339838], dtype=float32), -0.6904915]. 
=============================================
[2019-04-04 13:49:50,912] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.2566211e-08 8.6703150e-10 1.8903485e-14 3.8565996e-13 1.0000000e+00
 2.5528570e-09 9.4339304e-14], sum to 1.0000
[2019-04-04 13:49:50,913] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4554
[2019-04-04 13:49:50,956] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 41.5, 13.33333333333334, 0.0, 26.0, 25.32237987253877, 0.2758880029143071, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 492600.0000, 
sim time next is 493200.0000, 
raw observation next is [1.1, 43.0, 10.0, 0.0, 26.0, 25.55250935908012, 0.2939988507458328, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.43, 0.03333333333333333, 0.0, 0.6666666666666666, 0.6293757799233433, 0.5979996169152776, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.689309], dtype=float32), -0.43916085]. 
=============================================
[2019-04-04 13:49:52,857] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2891975e-09 9.5813926e-11 4.4606396e-16 2.6329712e-15 1.0000000e+00
 2.6260653e-11 8.9374459e-16], sum to 1.0000
[2019-04-04 13:49:52,858] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0127
[2019-04-04 13:49:52,913] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 85.0, 77.0, 141.0, 26.0, 24.82092960998211, 0.2764566841636998, 0.0, 1.0, 30455.11646541399], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 556200.0000, 
sim time next is 556800.0000, 
raw observation next is [-0.6, 84.33333333333333, 79.0, 140.0, 26.0, 24.87280953598691, 0.2777778816909988, 0.0, 1.0, 18742.402800978], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.8433333333333333, 0.2633333333333333, 0.15469613259668508, 0.6666666666666666, 0.5727341279989092, 0.5925926272303329, 0.0, 1.0, 0.0892495371475143], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.4845049], dtype=float32), 0.23333293]. 
=============================================
[2019-04-04 13:50:00,573] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.5876729e-09 2.0677599e-10 3.2171967e-15 1.2360500e-13 1.0000000e+00
 1.0259028e-09 1.6613616e-14], sum to 1.0000
[2019-04-04 13:50:00,575] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1285
[2019-04-04 13:50:00,615] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.533333333333333, 66.0, 0.0, 0.0, 26.0, 24.62260886483815, 0.2208487533927499, 0.0, 1.0, 42931.40759198285], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 772800.0000, 
sim time next is 773400.0000, 
raw observation next is [-6.616666666666667, 66.5, 0.0, 0.0, 26.0, 24.59085421402424, 0.2137156663884776, 0.0, 1.0, 42783.72000486489], 
processed observation next is [1.0, 0.9565217391304348, 0.2793167128347184, 0.665, 0.0, 0.0, 0.6666666666666666, 0.5492378511686867, 0.5712385554628259, 0.0, 1.0, 0.20373200002316613], 
reward next is 0.7963, 
noisyNet noise sample is [array([-0.2612214], dtype=float32), 0.28567207]. 
=============================================
[2019-04-04 13:50:04,862] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5168108e-09 5.7739014e-10 3.9621893e-15 8.6912712e-15 1.0000000e+00
 1.5827839e-10 8.5083536e-15], sum to 1.0000
[2019-04-04 13:50:04,862] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9228
[2019-04-04 13:50:04,877] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.3, 82.33333333333334, 0.0, 0.0, 26.0, 24.83370966923195, 0.2567677284626809, 0.0, 1.0, 41560.48373720986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 857400.0000, 
sim time next is 858000.0000, 
raw observation next is [-3.2, 81.66666666666667, 0.0, 0.0, 26.0, 24.81111438259661, 0.2513255749680333, 0.0, 1.0, 41484.55922090947], 
processed observation next is [1.0, 0.9565217391304348, 0.37396121883656513, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.5675928652163842, 0.5837751916560111, 0.0, 1.0, 0.1975455200995689], 
reward next is 0.8025, 
noisyNet noise sample is [array([-0.60838526], dtype=float32), 1.5959024]. 
=============================================
[2019-04-04 13:50:04,883] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[86.46607 ]
 [86.08289 ]
 [86.02522 ]
 [85.617516]
 [85.77259 ]], R is [[86.26280212]
 [86.20227051]
 [86.1420517 ]
 [86.08239746]
 [86.02229309]].
[2019-04-04 13:50:07,529] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3190106e-08 6.6005962e-10 6.7875664e-15 5.2131815e-14 1.0000000e+00
 9.5564845e-10 3.7752943e-14], sum to 1.0000
[2019-04-04 13:50:07,530] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1261
[2019-04-04 13:50:07,540] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3, 46.0, 85.0, 31.0, 26.0, 25.36734473242258, 0.3581935794195911, 1.0, 1.0, 45133.14843425989], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 747000.0000, 
sim time next is 747600.0000, 
raw observation next is [-0.4, 45.66666666666667, 82.16666666666667, 26.33333333333334, 26.0, 25.47857986545692, 0.3708559258451412, 1.0, 1.0, 29844.16607548798], 
processed observation next is [1.0, 0.6521739130434783, 0.45152354570637127, 0.4566666666666667, 0.2738888888888889, 0.02909760589318601, 0.6666666666666666, 0.6232149887880766, 0.6236186419483803, 1.0, 1.0, 0.14211507654994276], 
reward next is 0.8579, 
noisyNet noise sample is [array([-1.7484806], dtype=float32), -0.32932958]. 
=============================================
[2019-04-04 13:50:12,050] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1258233e-10 1.3662510e-12 1.8632895e-18 3.6452103e-17 1.0000000e+00
 1.9587296e-12 1.5596252e-17], sum to 1.0000
[2019-04-04 13:50:12,056] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2714
[2019-04-04 13:50:12,062] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.8, 86.0, 116.0, 0.0, 26.0, 26.65443562555125, 0.6782173170095579, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 991200.0000, 
sim time next is 991800.0000, 
raw observation next is [11.9, 86.0, 120.0, 0.0, 26.0, 26.67963924209105, 0.6810715269121513, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7922437673130196, 0.86, 0.4, 0.0, 0.6666666666666666, 0.7233032701742541, 0.7270238423040505, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15782529], dtype=float32), -0.7792481]. 
=============================================
[2019-04-04 13:50:15,532] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2116412e-09 9.7009754e-11 1.2811680e-15 2.3522537e-14 1.0000000e+00
 1.0631600e-10 8.9900710e-16], sum to 1.0000
[2019-04-04 13:50:15,532] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4553
[2019-04-04 13:50:15,550] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.49590724382161, 0.1544217570109317, 0.0, 1.0, 38818.5373947883], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 885600.0000, 
sim time next is 886200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.49303381843414, 0.1499621748600739, 0.0, 1.0, 38742.13486458772], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5410861515361782, 0.5499873916200246, 0.0, 1.0, 0.18448635649803677], 
reward next is 0.8155, 
noisyNet noise sample is [array([-0.42563462], dtype=float32), -1.8105125]. 
=============================================
[2019-04-04 13:50:19,125] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1541885e-09 9.7346679e-12 5.3556140e-17 2.4226185e-16 1.0000000e+00
 2.7028291e-11 7.9406996e-17], sum to 1.0000
[2019-04-04 13:50:19,128] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6288
[2019-04-04 13:50:19,176] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.4, 97.33333333333333, 0.0, 0.0, 26.0, 24.86680932910476, 0.2533696711480514, 1.0, 1.0, 98223.49020851924], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 930000.0000, 
sim time next is 930600.0000, 
raw observation next is [4.4, 98.0, 0.0, 0.0, 26.0, 24.66648503951793, 0.2631861137441727, 1.0, 1.0, 177275.1476311195], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.98, 0.0, 0.0, 0.6666666666666666, 0.5555404199598275, 0.5877287045813909, 1.0, 1.0, 0.8441673696719976], 
reward next is 0.1558, 
noisyNet noise sample is [array([-0.5069774], dtype=float32), 0.1250426]. 
=============================================
[2019-04-04 13:50:25,959] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4703420e-10 6.2037792e-12 4.6327941e-17 9.8695134e-17 1.0000000e+00
 1.0002634e-11 2.6286131e-17], sum to 1.0000
[2019-04-04 13:50:25,969] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5960
[2019-04-04 13:50:25,985] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.89340525661687, 0.6296936314933399, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1029600.0000, 
sim time next is 1030200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.96878742083924, 0.626947646284974, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6640656184032702, 0.708982548761658, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.65497404], dtype=float32), 0.57961506]. 
=============================================
[2019-04-04 13:50:28,883] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 13:50:28,885] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 13:50:28,885] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 13:50:28,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:50:28,885] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 13:50:28,886] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:50:28,887] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:50:28,895] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run19
[2019-04-04 13:50:28,913] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run19
[2019-04-04 13:50:28,933] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run19
[2019-04-04 13:50:45,070] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16937429], dtype=float32), 0.2010829]
[2019-04-04 13:50:45,070] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-16.40702023333333, 76.74657409000001, 0.0, 461.3572356833333, 26.0, 23.36573252007091, -0.14318595144212, 1.0, 1.0, 86634.82307468163]
[2019-04-04 13:50:45,071] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 13:50:45,071] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.7035279e-08 7.3528068e-09 2.1481879e-13 1.7841309e-12 1.0000000e+00
 1.8032132e-09 5.1216925e-13], sampled 0.5117210024216233
[2019-04-04 13:50:49,700] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16937429], dtype=float32), 0.2010829]
[2019-04-04 13:50:49,700] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-10.85, 55.0, 0.0, 0.0, 26.0, 25.12312177234843, 0.1968023015512051, 0.0, 1.0, 43472.23787257649]
[2019-04-04 13:50:49,700] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 13:50:49,701] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.5152761e-08 1.5802689e-09 8.0466138e-14 4.4215963e-13 1.0000000e+00
 8.4530544e-10 1.2409422e-13], sampled 0.36591470653415537
[2019-04-04 13:50:53,133] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.16937429], dtype=float32), 0.2010829]
[2019-04-04 13:50:53,134] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.9, 86.0, 0.0, 0.0, 26.0, 24.34696582118021, 0.1251373054911006, 0.0, 1.0, 42150.19900710219]
[2019-04-04 13:50:53,134] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 13:50:53,135] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.2166369e-09 3.9573958e-10 3.3173215e-15 5.3192581e-14 1.0000000e+00
 1.2328874e-10 7.0546952e-15], sampled 0.6916728323887531
[2019-04-04 13:50:56,579] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16937429], dtype=float32), 0.2010829]
[2019-04-04 13:50:56,579] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-0.45, 73.5, 99.0, 328.0, 26.0, 25.10009526555186, 0.1570232609584779, 1.0, 1.0, 0.0]
[2019-04-04 13:50:56,580] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 13:50:56,580] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.3221659e-09 1.0949476e-10 4.2477265e-16 8.1764751e-15 1.0000000e+00
 2.4457646e-11 8.4594800e-16], sampled 0.9402286294273351
[2019-04-04 13:51:04,834] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.16937429], dtype=float32), 0.2010829]
[2019-04-04 13:51:04,834] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [11.1, 77.0, 0.0, 0.0, 26.0, 25.64854955293799, 0.6257830795204946, 0.0, 1.0, 21703.97238401216]
[2019-04-04 13:51:04,834] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 13:51:04,836] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.9985953e-10 4.8947513e-11 8.3145786e-17 1.7857693e-15 1.0000000e+00
 6.9783836e-12 1.6488399e-16], sampled 0.27652824166722767
[2019-04-04 13:51:13,743] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16937429], dtype=float32), 0.2010829]
[2019-04-04 13:51:13,743] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [10.2, 88.0, 0.0, 0.0, 26.0, 25.65902100183393, 0.6010113720613307, 0.0, 1.0, 18726.28543170482]
[2019-04-04 13:51:13,744] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 13:51:13,745] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.2436379e-10 3.2786929e-11 5.2918463e-17 1.2263993e-15 1.0000000e+00
 5.0952992e-12 1.0956232e-16], sampled 0.6685224023844208
[2019-04-04 13:51:33,299] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16937429], dtype=float32), 0.2010829]
[2019-04-04 13:51:33,300] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.234339704666667, 44.12528414000001, 76.83299167499999, 0.0, 26.0, 25.3409522140384, 0.283286225625464, 1.0, 1.0, 0.0]
[2019-04-04 13:51:33,300] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 13:51:33,301] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [9.3739745e-09 1.2311373e-09 4.9674078e-14 2.2300456e-13 1.0000000e+00
 1.0186572e-09 6.3666337e-14], sampled 0.3997900334464516
[2019-04-04 13:52:08,196] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.6804 239887112.1494 1605.0696
[2019-04-04 13:52:29,216] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 13:52:32,287] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 13:52:33,310] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1800000, evaluation results [1800000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.680418336308, 239887112.1493781, 1605.0696192925661, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 13:52:39,515] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1494835e-10 1.3644547e-11 2.5938594e-17 1.1489136e-15 1.0000000e+00
 2.9641135e-12 8.1867349e-17], sum to 1.0000
[2019-04-04 13:52:39,516] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5314
[2019-04-04 13:52:39,530] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.283333333333333, 92.0, 0.0, 0.0, 26.0, 25.43766950919385, 0.5481021367375404, 0.0, 1.0, 42226.66573159455], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1309800.0000, 
sim time next is 1310400.0000, 
raw observation next is [2.2, 92.0, 0.0, 0.0, 26.0, 25.40704335467453, 0.5491318442803643, 0.0, 1.0, 55385.95442412047], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6172536128895443, 0.6830439480934548, 0.0, 1.0, 0.2637426401148594], 
reward next is 0.7363, 
noisyNet noise sample is [array([-0.89848125], dtype=float32), 1.0936561]. 
=============================================
[2019-04-04 13:52:47,366] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5492562e-10 5.6084574e-11 9.9003964e-17 4.3168904e-16 1.0000000e+00
 6.4356249e-11 1.8271465e-17], sum to 1.0000
[2019-04-04 13:52:47,368] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5463
[2019-04-04 13:52:47,383] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.183333333333333, 91.5, 0.0, 0.0, 26.0, 25.52563477443152, 0.5417827847678992, 0.0, 1.0, 40195.63361958473], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1461000.0000, 
sim time next is 1461600.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.52021887915963, 0.5393788726372656, 0.0, 1.0, 39371.42140416825], 
processed observation next is [1.0, 0.9565217391304348, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6266849065966357, 0.6797929575457552, 0.0, 1.0, 0.18748295906746787], 
reward next is 0.8125, 
noisyNet noise sample is [array([0.06343214], dtype=float32), -0.31346616]. 
=============================================
[2019-04-04 13:52:51,433] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.4621353e-10 5.4114276e-11 2.5373333e-16 8.8998904e-15 1.0000000e+00
 1.3651687e-10 1.1521878e-15], sum to 1.0000
[2019-04-04 13:52:51,434] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1042
[2019-04-04 13:52:51,448] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.0, 60.0, 0.0, 0.0, 26.0, 26.37772476731071, 0.7008215895399509, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1533600.0000, 
sim time next is 1534200.0000, 
raw observation next is [9.9, 60.16666666666666, 0.0, 0.0, 26.0, 26.40803978045779, 0.7032304066319144, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7368421052631581, 0.6016666666666666, 0.0, 0.0, 0.6666666666666666, 0.700669981704816, 0.7344101355439715, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12158605], dtype=float32), -0.35478628]. 
=============================================
[2019-04-04 13:52:53,533] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.95736555e-10 1.96074164e-11 1.44013653e-16 1.21123831e-14
 1.00000000e+00 3.56827207e-11 1.01956714e-16], sum to 1.0000
[2019-04-04 13:52:53,535] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8540
[2019-04-04 13:52:53,593] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.433333333333334, 83.33333333333334, 33.83333333333333, 0.0, 26.0, 25.58170828680451, 0.4656316078414872, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1700400.0000, 
sim time next is 1701000.0000, 
raw observation next is [1.35, 84.5, 30.0, 0.0, 26.0, 24.79970003728044, 0.416733544894248, 1.0, 1.0, 196399.3694745335], 
processed observation next is [1.0, 0.6956521739130435, 0.5000000000000001, 0.845, 0.1, 0.0, 0.6666666666666666, 0.5666416697733702, 0.638911181631416, 1.0, 1.0, 0.9352350927358738], 
reward next is 0.0648, 
noisyNet noise sample is [array([1.0319647], dtype=float32), -0.21795014]. 
=============================================
[2019-04-04 13:52:53,599] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[88.298035]
 [88.49966 ]
 [88.713326]
 [88.9434  ]
 [89.30182 ]], R is [[88.06784058]
 [88.18716431]
 [88.30529022]
 [88.42224121]
 [88.53801727]].
[2019-04-04 13:52:54,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8843334e-10 3.0932343e-11 7.7712957e-17 7.0961763e-16 1.0000000e+00
 6.3818135e-12 4.7918308e-17], sum to 1.0000
[2019-04-04 13:52:54,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4939
[2019-04-04 13:52:54,419] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.7, 82.5, 0.0, 0.0, 26.0, 25.53156218238269, 0.5611228143284356, 0.0, 1.0, 9374.618972180173], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1564200.0000, 
sim time next is 1564800.0000, 
raw observation next is [4.600000000000001, 83.66666666666666, 0.0, 0.0, 26.0, 25.68871134180208, 0.544984307765369, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5900277008310251, 0.8366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6407259451501733, 0.6816614359217897, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8166351], dtype=float32), -0.41797534]. 
=============================================
[2019-04-04 13:53:00,879] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.5408496e-10 6.7007055e-10 7.2672288e-16 5.0853012e-15 1.0000000e+00
 7.3405761e-12 1.7421143e-15], sum to 1.0000
[2019-04-04 13:53:00,880] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4938
[2019-04-04 13:53:00,894] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.94253383344526, 0.3663310270185016, 0.0, 1.0, 43610.34176028528], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1747200.0000, 
sim time next is 1747800.0000, 
raw observation next is [-0.8999999999999999, 85.0, 0.0, 0.0, 26.0, 24.9832334634332, 0.3633693183496208, 0.0, 1.0, 43639.16399918468], 
processed observation next is [0.0, 0.21739130434782608, 0.43767313019390586, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5819361219527668, 0.6211231061165402, 0.0, 1.0, 0.2078055428532604], 
reward next is 0.7922, 
noisyNet noise sample is [array([1.5081435], dtype=float32), 0.11625205]. 
=============================================
[2019-04-04 13:53:05,159] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8989468e-09 2.8359788e-11 4.1772184e-15 4.0570263e-14 1.0000000e+00
 2.1255668e-10 2.3611346e-15], sum to 1.0000
[2019-04-04 13:53:05,163] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7156
[2019-04-04 13:53:05,202] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.166666666666667, 66.33333333333334, 37.33333333333333, 0.0, 26.0, 25.50084851172112, 0.3160404774809554, 1.0, 1.0, 33520.62074970889], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1959600.0000, 
sim time next is 1960200.0000, 
raw observation next is [-3.35, 68.5, 30.0, 0.0, 26.0, 25.53503542476353, 0.3250331878569039, 1.0, 1.0, 34846.71876804351], 
processed observation next is [1.0, 0.6956521739130435, 0.3698060941828255, 0.685, 0.1, 0.0, 0.6666666666666666, 0.6279196187302943, 0.6083443959523013, 1.0, 1.0, 0.16593675603830244], 
reward next is 0.8341, 
noisyNet noise sample is [array([-0.8186509], dtype=float32), 0.73581606]. 
=============================================
[2019-04-04 13:53:10,827] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3305380e-09 9.2438557e-11 1.3229718e-15 6.8360018e-15 1.0000000e+00
 3.6789879e-10 1.6272582e-15], sum to 1.0000
[2019-04-04 13:53:10,827] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6444
[2019-04-04 13:53:10,880] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.199999999999999, 68.33333333333333, 231.1666666666667, 9.0, 26.0, 25.63237640140755, 0.3084910396866696, 1.0, 1.0, 90610.44652019453], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1942800.0000, 
sim time next is 1943400.0000, 
raw observation next is [-5.1, 66.66666666666667, 230.3333333333333, 8.0, 26.0, 25.6371152121438, 0.3284364688989669, 1.0, 1.0, 60061.79595703969], 
processed observation next is [1.0, 0.4782608695652174, 0.3213296398891967, 0.6666666666666667, 0.7677777777777777, 0.008839779005524863, 0.6666666666666666, 0.6364262676786501, 0.6094788229663223, 1.0, 1.0, 0.2860085521763795], 
reward next is 0.7140, 
noisyNet noise sample is [array([1.9126644], dtype=float32), 0.43319872]. 
=============================================
[2019-04-04 13:53:16,493] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3415121e-10 3.0091225e-11 2.7435638e-16 1.6905646e-15 1.0000000e+00
 3.1138581e-11 5.1407102e-16], sum to 1.0000
[2019-04-04 13:53:16,494] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5449
[2019-04-04 13:53:16,508] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.8, 83.0, 0.0, 0.0, 26.0, 24.81755632280268, 0.2524370352992329, 0.0, 1.0, 42935.35862838235], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1982400.0000, 
sim time next is 1983000.0000, 
raw observation next is [-5.7, 83.0, 0.0, 0.0, 26.0, 24.74841197852467, 0.2414485010030502, 0.0, 1.0, 42987.69677334322], 
processed observation next is [1.0, 0.9565217391304348, 0.30470914127423826, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5623676648770557, 0.5804828336676834, 0.0, 1.0, 0.20470331796830105], 
reward next is 0.7953, 
noisyNet noise sample is [array([0.9862708], dtype=float32), 0.20272191]. 
=============================================
[2019-04-04 13:53:16,512] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.998215]
 [82.964264]
 [82.95368 ]
 [82.74221 ]
 [82.70742 ]], R is [[82.92204285]
 [82.88837433]
 [82.85424042]
 [82.81858826]
 [82.78421783]].
[2019-04-04 13:53:21,840] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2947344e-09 6.6226490e-11 1.6400397e-15 2.6846984e-14 1.0000000e+00
 6.6883415e-11 2.1104941e-14], sum to 1.0000
[2019-04-04 13:53:21,841] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0819
[2019-04-04 13:53:21,859] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.40562485409641, 0.1705277419938761, 0.0, 1.0, 42430.47031028753], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1988400.0000, 
sim time next is 1989000.0000, 
raw observation next is [-5.9, 85.0, 0.0, 0.0, 26.0, 24.37616805592097, 0.1664428609149942, 0.0, 1.0, 42325.20683190491], 
processed observation next is [1.0, 0.0, 0.2991689750692521, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5313473379934143, 0.5554809536383314, 0.0, 1.0, 0.20154860396145194], 
reward next is 0.7985, 
noisyNet noise sample is [array([0.11070455], dtype=float32), -1.5135716]. 
=============================================
[2019-04-04 13:53:21,867] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.46621 ]
 [81.657234]
 [82.073944]
 [82.60293 ]
 [82.71529 ]], R is [[81.26829529]
 [81.25356293]
 [81.23845673]
 [81.22296906]
 [81.20715332]].
[2019-04-04 13:53:26,487] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0383260e-09 1.5156879e-10 8.0260353e-16 5.3074515e-14 1.0000000e+00
 7.0812946e-11 2.5987843e-15], sum to 1.0000
[2019-04-04 13:53:26,488] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8681
[2019-04-04 13:53:26,512] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.4, 89.33333333333334, 0.0, 0.0, 26.0, 24.38834479804079, 0.1460217952134076, 0.0, 1.0, 43453.19735968704], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2086800.0000, 
sim time next is 2087400.0000, 
raw observation next is [-5.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.33002248434763, 0.1566966317298603, 0.0, 1.0, 43830.69704155161], 
processed observation next is [1.0, 0.13043478260869565, 0.3102493074792244, 0.9016666666666667, 0.0, 0.0, 0.6666666666666666, 0.5275018736956358, 0.5522322105766201, 0.0, 1.0, 0.20871760495976957], 
reward next is 0.7913, 
noisyNet noise sample is [array([0.18010944], dtype=float32), -0.069518924]. 
=============================================
[2019-04-04 13:53:28,658] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5111653e-09 5.6421909e-11 6.5057837e-16 9.0596518e-15 1.0000000e+00
 4.9548397e-11 8.0786847e-16], sum to 1.0000
[2019-04-04 13:53:28,658] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7208
[2019-04-04 13:53:28,695] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.716666666666667, 80.83333333333334, 196.6666666666667, 79.33333333333333, 26.0, 25.87500926532973, 0.3994169128424724, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2110200.0000, 
sim time next is 2110800.0000, 
raw observation next is [-7.633333333333333, 79.66666666666667, 202.3333333333333, 69.66666666666666, 26.0, 25.84980115526944, 0.3975030288688169, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2511542012927055, 0.7966666666666667, 0.6744444444444443, 0.07697974217311233, 0.6666666666666666, 0.6541500962724532, 0.632501009622939, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8704901], dtype=float32), 0.15921378]. 
=============================================
[2019-04-04 13:53:45,837] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0776323e-07 1.3083864e-08 1.4975536e-12 6.8216318e-12 9.9999988e-01
 2.5838618e-08 1.7828857e-12], sum to 1.0000
[2019-04-04 13:53:45,839] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8219
[2019-04-04 13:53:45,901] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.833333333333333, 27.0, 0.0, 0.0, 26.0, 24.85017200928564, 0.2152601544235068, 0.0, 1.0, 69708.12731258849], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2482800.0000, 
sim time next is 2483400.0000, 
raw observation next is [1.466666666666667, 27.5, 0.0, 0.0, 26.0, 24.86582500217, 0.2207888875049812, 0.0, 1.0, 47170.51019908855], 
processed observation next is [0.0, 0.7391304347826086, 0.5032317636195753, 0.275, 0.0, 0.0, 0.6666666666666666, 0.5721520835141666, 0.5735962958349937, 0.0, 1.0, 0.2246214771385169], 
reward next is 0.7754, 
noisyNet noise sample is [array([0.29029176], dtype=float32), 0.92182416]. 
=============================================
[2019-04-04 13:53:51,135] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5435981e-08 6.6189143e-10 6.9081215e-14 1.9479206e-13 1.0000000e+00
 2.0502994e-10 1.8002671e-14], sum to 1.0000
[2019-04-04 13:53:51,135] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2039
[2019-04-04 13:53:51,193] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.85, 37.16666666666666, 79.33333333333333, 794.3333333333334, 26.0, 24.93356933005916, 0.2437915245191563, 0.0, 1.0, 40261.84156107344], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2458200.0000, 
sim time next is 2458800.0000, 
raw observation next is [-2.3, 36.0, 81.0, 803.0, 26.0, 24.95856482771127, 0.2495736910891196, 0.0, 1.0, 18741.67821719052], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.36, 0.27, 0.887292817679558, 0.6666666666666666, 0.5798804023092726, 0.5831912303630399, 0.0, 1.0, 0.0892460867485263], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.08958619], dtype=float32), -0.9086931]. 
=============================================
[2019-04-04 13:53:55,004] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.5690226e-09 1.1514203e-09 2.1768113e-14 6.8210147e-14 1.0000000e+00
 3.1278378e-09 2.2565769e-14], sum to 1.0000
[2019-04-04 13:53:55,004] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5466
[2019-04-04 13:53:55,045] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.9666666666666667, 38.66666666666667, 0.0, 0.0, 26.0, 25.17655960172304, 0.3569745048262317, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2575200.0000, 
sim time next is 2575800.0000, 
raw observation next is [-1.15, 40.0, 0.0, 0.0, 26.0, 25.16220214430262, 0.3402754367969547, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4307479224376732, 0.4, 0.0, 0.0, 0.6666666666666666, 0.5968501786918848, 0.6134251455989849, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6799342], dtype=float32), 0.40370035]. 
=============================================
[2019-04-04 13:53:55,510] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.6974506e-09 3.2178449e-09 5.7791235e-14 1.7425249e-13 1.0000000e+00
 2.0372648e-09 3.8372781e-14], sum to 1.0000
[2019-04-04 13:53:55,510] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3504
[2019-04-04 13:53:55,526] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 29.0, 70.0, 162.0, 26.0, 25.27933620936296, 0.3507715674062472, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2565000.0000, 
sim time next is 2565600.0000, 
raw observation next is [2.9, 29.0, 59.5, 135.8333333333333, 26.0, 25.50388861233261, 0.3797646713975489, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5429362880886427, 0.29, 0.19833333333333333, 0.1500920810313075, 0.6666666666666666, 0.6253240510277175, 0.626588223799183, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.41628605], dtype=float32), 0.30548626]. 
=============================================
[2019-04-04 13:53:56,155] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7916425e-08 6.8303923e-09 9.7333206e-14 6.9133187e-13 1.0000000e+00
 2.9493257e-09 2.6027157e-13], sum to 1.0000
[2019-04-04 13:53:56,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3438
[2019-04-04 13:53:56,196] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.233333333333333, 28.5, 89.0, 840.6666666666666, 26.0, 24.91871642282385, 0.2695952175682331, 0.0, 1.0, 18719.82033319522], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2465400.0000, 
sim time next is 2466000.0000, 
raw observation next is [1.6, 28.0, 88.5, 838.5, 26.0, 24.94164417691549, 0.2737597700736229, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5069252077562327, 0.28, 0.295, 0.9265193370165746, 0.6666666666666666, 0.5784703480762907, 0.5912532566912077, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3831404], dtype=float32), -1.6634805]. 
=============================================
[2019-04-04 13:53:56,212] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[72.81351 ]
 [72.85855 ]
 [72.87116 ]
 [72.867195]
 [72.90069 ]], R is [[72.996315  ]
 [73.17720795]
 [73.32109833]
 [73.43280029]
 [73.57977295]].
[2019-04-04 13:54:04,469] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.5662827e-09 2.8864813e-10 2.0921208e-15 1.4567034e-13 1.0000000e+00
 1.8209116e-10 1.7178816e-15], sum to 1.0000
[2019-04-04 13:54:04,470] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8208
[2019-04-04 13:54:04,528] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-14.5, 87.0, 80.0, 330.0, 26.0, 25.71447886680073, 0.3862299421952788, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2709000.0000, 
sim time next is 2709600.0000, 
raw observation next is [-14.33333333333333, 88.33333333333334, 82.83333333333334, 377.0, 26.0, 25.91862088338155, 0.4092565918935945, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.0655586334256695, 0.8833333333333334, 0.27611111111111114, 0.4165745856353591, 0.6666666666666666, 0.6598850736151292, 0.6364188639645315, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24242422], dtype=float32), 0.3715559]. 
=============================================
[2019-04-04 13:54:09,101] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3096125e-08 3.9112161e-10 7.6737661e-15 2.7903032e-14 1.0000000e+00
 3.8337963e-10 8.4514212e-15], sum to 1.0000
[2019-04-04 13:54:09,105] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5476
[2019-04-04 13:54:09,136] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.166666666666667, 50.66666666666667, 75.33333333333334, 560.6666666666666, 26.0, 26.66532039972703, 0.6296904717605069, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2735400.0000, 
sim time next is 2736000.0000, 
raw observation next is [-3.0, 50.0, 70.0, 534.0, 26.0, 26.64450107401681, 0.5997377892943883, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3795013850415513, 0.5, 0.23333333333333334, 0.5900552486187846, 0.6666666666666666, 0.7203750895014007, 0.6999125964314628, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7972354], dtype=float32), 1.4276953]. 
=============================================
[2019-04-04 13:54:09,138] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.81719]
 [80.91534]
 [81.048  ]
 [81.29032]
 [81.56226]], R is [[80.89355469]
 [81.08461761]
 [81.27377319]
 [81.46103668]
 [81.64642334]].
[2019-04-04 13:54:10,343] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7344740e-09 1.1896907e-10 4.1258872e-17 7.2114759e-15 1.0000000e+00
 6.9555937e-12 3.4077045e-17], sum to 1.0000
[2019-04-04 13:54:10,344] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2402
[2019-04-04 13:54:10,356] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 100.0, 131.0, 0.0, 26.0, 25.40471138930817, 0.3217266584652879, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2894400.0000, 
sim time next is 2895000.0000, 
raw observation next is [1.166666666666667, 100.0, 145.6666666666667, 0.0, 26.0, 25.39311518586644, 0.3264305214636978, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5217391304347826, 0.49492151431209613, 1.0, 0.48555555555555574, 0.0, 0.6666666666666666, 0.6160929321555365, 0.6088101738212326, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([2.4158037], dtype=float32), 0.7649073]. 
=============================================
[2019-04-04 13:54:10,359] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[91.9119 ]
 [91.41151]
 [90.95762]
 [90.68164]
 [90.4864 ]], R is [[92.38636017]
 [92.37354279]
 [92.3608551 ]
 [92.34829712]
 [92.33586121]].
[2019-04-04 13:54:14,566] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8132625e-09 3.7353276e-10 9.1635381e-16 1.8892167e-14 1.0000000e+00
 2.7854860e-10 5.0796202e-15], sum to 1.0000
[2019-04-04 13:54:14,567] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3085
[2019-04-04 13:54:14,615] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 64.0, 122.6666666666667, 215.6666666666667, 26.0, 25.97064170917981, 0.3958562032534883, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2796600.0000, 
sim time next is 2797200.0000, 
raw observation next is [-6.0, 64.0, 130.0, 220.0, 26.0, 25.94037392734818, 0.3824269670002607, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.296398891966759, 0.64, 0.43333333333333335, 0.2430939226519337, 0.6666666666666666, 0.6616978272790149, 0.6274756556667536, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.69537216], dtype=float32), 0.033413164]. 
=============================================
[2019-04-04 13:54:14,957] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4323497e-09 2.9166319e-10 9.4945058e-15 7.1188227e-14 1.0000000e+00
 1.4118713e-10 3.4521804e-15], sum to 1.0000
[2019-04-04 13:54:14,961] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5499
[2019-04-04 13:54:14,982] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 49.00000000000001, 141.3333333333333, 678.0, 26.0, 26.07266363983751, 0.4739101270678807, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2805000.0000, 
sim time next is 2805600.0000, 
raw observation next is [-1.110223024625157e-16, 48.0, 133.1666666666667, 720.5, 26.0, 26.06124552352647, 0.4605177145916466, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.48, 0.44388888888888905, 0.7961325966850828, 0.6666666666666666, 0.6717704602938724, 0.6535059048638822, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47028014], dtype=float32), 0.0122148665]. 
=============================================
[2019-04-04 13:54:19,172] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4194622e-09 1.9148537e-11 2.8928094e-17 1.2893804e-15 1.0000000e+00
 9.4528161e-12 6.6869949e-17], sum to 1.0000
[2019-04-04 13:54:19,172] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0210
[2019-04-04 13:54:19,202] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 94.16666666666666, 49.33333333333334, 49.66666666666667, 26.0, 25.76282719795004, 0.4623178886747386, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2911800.0000, 
sim time next is 2912400.0000, 
raw observation next is [2.0, 93.0, 38.5, 47.5, 26.0, 25.87509780162168, 0.4709980190474747, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.93, 0.12833333333333333, 0.052486187845303865, 0.6666666666666666, 0.6562581501351401, 0.6569993396824916, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0005108], dtype=float32), 0.75309014]. 
=============================================
[2019-04-04 13:54:24,131] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0375097e-09 2.8385833e-10 8.0042353e-16 5.5977084e-14 1.0000000e+00
 5.2081107e-11 1.0181612e-14], sum to 1.0000
[2019-04-04 13:54:24,131] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5925
[2019-04-04 13:54:24,177] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.333333333333333, 61.66666666666667, 108.5, 791.8333333333334, 26.0, 25.12884688685212, 0.4063783560909659, 0.0, 1.0, 18716.68195802214], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2986800.0000, 
sim time next is 2987400.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 107.0, 783.6666666666666, 26.0, 25.12197136666815, 0.4065574867164665, 0.0, 1.0, 18714.83354681669], 
processed observation next is [0.0, 0.5652173913043478, 0.4025854108956602, 0.6083333333333333, 0.3566666666666667, 0.8659300184162062, 0.6666666666666666, 0.5934976138890123, 0.6355191622388222, 0.0, 1.0, 0.08911825498484138], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.7926421], dtype=float32), 0.9219482]. 
=============================================
[2019-04-04 13:54:25,819] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1947656e-11 6.1079917e-13 2.1177075e-18 4.4502357e-17 1.0000000e+00
 6.6627167e-13 1.1298835e-18], sum to 1.0000
[2019-04-04 13:54:25,821] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8200
[2019-04-04 13:54:25,832] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.8, 99.5, 84.0, 679.0, 26.0, 26.80258496264469, 0.8941319710857801, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3166200.0000, 
sim time next is 3166800.0000, 
raw observation next is [6.733333333333333, 99.33333333333334, 79.66666666666666, 649.0, 26.0, 27.14109262129613, 0.9364680733938266, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.649122807017544, 0.9933333333333334, 0.26555555555555554, 0.7171270718232045, 0.6666666666666666, 0.7617577184413443, 0.8121560244646089, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.92827874], dtype=float32), -0.50844276]. 
=============================================
[2019-04-04 13:54:34,678] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3494784e-10 3.6299815e-13 3.0116280e-17 1.6788800e-16 1.0000000e+00
 4.8016853e-13 2.4974133e-17], sum to 1.0000
[2019-04-04 13:54:34,679] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7520
[2019-04-04 13:54:34,701] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 95.33333333333334, 0.0, 0.0, 26.0, 25.40293797967857, 0.5980495391907058, 0.0, 1.0, 186185.4753389534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3192000.0000, 
sim time next is 3192600.0000, 
raw observation next is [2.0, 94.16666666666666, 0.0, 0.0, 26.0, 25.37509846627718, 0.618558276285051, 0.0, 1.0, 118028.485978503], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.9416666666666665, 0.0, 0.0, 0.6666666666666666, 0.6145915388564317, 0.706186092095017, 0.0, 1.0, 0.5620404094214428], 
reward next is 0.4380, 
noisyNet noise sample is [array([-1.3825194], dtype=float32), 0.24466576]. 
=============================================
[2019-04-04 13:54:34,904] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0041799e-10 2.1544548e-12 2.3753145e-18 2.7690283e-17 1.0000000e+00
 5.2566003e-12 1.9347227e-18], sum to 1.0000
[2019-04-04 13:54:34,908] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2512
[2019-04-04 13:54:34,927] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.4, 99.33333333333334, 62.33333333333333, 529.0, 26.0, 27.85159892786753, 0.7941075403136707, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3169200.0000, 
sim time next is 3169800.0000, 
raw observation next is [6.3, 99.5, 58.0, 499.0, 26.0, 27.8380488919215, 1.013513428640633, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6371191135734073, 0.995, 0.19333333333333333, 0.5513812154696133, 0.6666666666666666, 0.8198374076601249, 0.8378378095468776, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37385407], dtype=float32), 0.2689852]. 
=============================================
[2019-04-04 13:54:44,649] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.45810960e-09 5.77960901e-10 1.78852430e-14 8.21433850e-14
 1.00000000e+00 4.62970107e-10 1.04269595e-14], sum to 1.0000
[2019-04-04 13:54:44,649] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4924
[2019-04-04 13:54:44,663] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 26.0, 23.85902625128349, 0.08821599636331312, 0.0, 1.0, 44023.85178377754], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3308400.0000, 
sim time next is 3309000.0000, 
raw observation next is [-11.0, 77.33333333333334, 0.0, 0.0, 26.0, 23.86391696277907, 0.07456777232398827, 0.0, 1.0, 44038.68543911678], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.4886597468982557, 0.5248559241079961, 0.0, 1.0, 0.2097080259005561], 
reward next is 0.7903, 
noisyNet noise sample is [array([1.3326408], dtype=float32), -0.54294395]. 
=============================================
[2019-04-04 13:54:44,677] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.514755]
 [77.68043 ]
 [77.88281 ]
 [78.08287 ]
 [78.29138 ]], R is [[77.36503601]
 [77.38174438]
 [77.39836121]
 [77.41499329]
 [77.43177795]].
[2019-04-04 13:54:55,564] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5707467e-08 1.2370927e-09 3.3545835e-14 4.7449111e-13 1.0000000e+00
 3.9586792e-10 5.4853989e-14], sum to 1.0000
[2019-04-04 13:54:55,567] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9782
[2019-04-04 13:54:55,595] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 43.0, 74.5, 607.0, 26.0, 25.33882690286542, 0.4605474376399811, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3600000.0000, 
sim time next is 3600600.0000, 
raw observation next is [0.0, 42.33333333333334, 70.66666666666667, 576.3333333333333, 26.0, 25.32340037463254, 0.453173526146937, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.42333333333333345, 0.23555555555555557, 0.6368324125230201, 0.6666666666666666, 0.6102833645527118, 0.651057842048979, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6633669], dtype=float32), 1.4456034]. 
=============================================
[2019-04-04 13:54:58,461] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.20320145e-08 3.46065576e-09 2.06441394e-14 5.04043896e-14
 1.00000000e+00 3.71636749e-10 4.72449694e-15], sum to 1.0000
[2019-04-04 13:54:58,462] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5114
[2019-04-04 13:54:58,495] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.0, 26.33333333333334, 97.0, 576.3333333333334, 26.0, 25.70280110244176, 0.4562748030967027, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3661800.0000, 
sim time next is 3662400.0000, 
raw observation next is [11.0, 26.66666666666667, 99.0, 619.6666666666666, 26.0, 25.73524725017157, 0.4608273317719875, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.2666666666666667, 0.33, 0.6847145488029466, 0.6666666666666666, 0.6446039375142977, 0.6536091105906625, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7804626], dtype=float32), 0.9974025]. 
=============================================
[2019-04-04 13:55:00,356] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.13301848e-08 2.84971646e-09 2.34677746e-14 8.88922386e-14
 1.00000000e+00 1.28062658e-10 1.05667706e-13], sum to 1.0000
[2019-04-04 13:55:00,357] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9341
[2019-04-04 13:55:00,367] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.0, 27.33333333333333, 102.6666666666667, 679.6666666666666, 26.0, 25.69892987080715, 0.4653971041095866, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3663600.0000, 
sim time next is 3664200.0000, 
raw observation next is [11.0, 27.66666666666667, 104.3333333333333, 696.3333333333334, 26.0, 25.68268544521284, 0.4662025267804206, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.2766666666666667, 0.3477777777777777, 0.7694290976058932, 0.6666666666666666, 0.64022378710107, 0.6554008422601402, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1409062], dtype=float32), 0.47002918]. 
=============================================
[2019-04-04 13:55:01,247] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4895095e-09 8.7061046e-11 4.9922502e-16 2.1089558e-15 1.0000000e+00
 5.6633014e-11 1.3741205e-15], sum to 1.0000
[2019-04-04 13:55:01,249] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7484
[2019-04-04 13:55:01,258] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.0, 79.33333333333334, 653.6666666666667, 26.0, 26.83465596721753, 0.7063820408184128, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3772200.0000, 
sim time next is 3772800.0000, 
raw observation next is [0.0, 60.0, 75.5, 625.0, 26.0, 26.87360174252905, 0.7122180522555975, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6, 0.25166666666666665, 0.6906077348066298, 0.6666666666666666, 0.7394668118774209, 0.7374060174185325, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5540277], dtype=float32), -0.99919164]. 
=============================================
[2019-04-04 13:55:05,247] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.7407193e-09 1.4036703e-11 8.5528988e-16 1.9020730e-15 1.0000000e+00
 3.3674421e-11 1.2406278e-16], sum to 1.0000
[2019-04-04 13:55:05,250] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9071
[2019-04-04 13:55:05,268] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8333333333333334, 60.0, 116.3333333333333, 821.6666666666666, 26.0, 26.18582019066001, 0.5621597143447198, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3762600.0000, 
sim time next is 3763200.0000, 
raw observation next is [-0.6666666666666667, 60.00000000000001, 115.6666666666667, 819.8333333333334, 26.0, 26.091903276316, 0.584898736377486, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.44413665743305636, 0.6000000000000001, 0.38555555555555565, 0.9058931860036833, 0.6666666666666666, 0.6743252730263333, 0.694966245459162, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9867938], dtype=float32), -0.55430317]. 
=============================================
[2019-04-04 13:55:07,787] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6607141e-08 7.7032758e-10 2.1145868e-15 5.5981034e-14 1.0000000e+00
 3.9510153e-10 8.2837196e-15], sum to 1.0000
[2019-04-04 13:55:07,789] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1141
[2019-04-04 13:55:07,843] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.66666666666667, 56.33333333333333, 94.33333333333333, 486.3333333333333, 26.0, 26.11420655380419, 0.4727424296826579, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4005600.0000, 
sim time next is 4006200.0000, 
raw observation next is [-11.33333333333333, 54.66666666666667, 95.66666666666666, 528.6666666666667, 26.0, 26.25893728922221, 0.4852759634285892, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.14866112650046176, 0.5466666666666667, 0.31888888888888883, 0.5841620626151014, 0.6666666666666666, 0.6882447741018508, 0.6617586544761964, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.5792959], dtype=float32), -2.0659392]. 
=============================================
[2019-04-04 13:55:08,540] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.3796694e-10 1.4195447e-11 3.1748897e-16 5.6039949e-16 1.0000000e+00
 2.6164411e-11 5.3630378e-16], sum to 1.0000
[2019-04-04 13:55:08,541] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1677
[2019-04-04 13:55:08,582] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.833333333333333, 60.0, 107.0, 743.3333333333334, 26.0, 26.46907263233111, 0.6025721555911477, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3838200.0000, 
sim time next is 3838800.0000, 
raw observation next is [-1.666666666666667, 60.00000000000001, 108.5, 759.1666666666667, 26.0, 26.51485811933072, 0.6138174563148057, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4164358264081256, 0.6000000000000001, 0.3616666666666667, 0.8388581952117865, 0.6666666666666666, 0.7095715099442268, 0.7046058187716019, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2620457], dtype=float32), 0.8712182]. 
=============================================
[2019-04-04 13:55:13,132] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1276517e-09 1.6449404e-10 1.0670436e-15 3.9018924e-15 1.0000000e+00
 7.2217871e-11 9.3206387e-15], sum to 1.0000
[2019-04-04 13:55:13,140] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6112
[2019-04-04 13:55:13,159] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.5, 36.0, 93.0, 437.0, 26.0, 27.37713713323366, 0.5745391604330627, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4120200.0000, 
sim time next is 4120800.0000, 
raw observation next is [3.333333333333333, 36.33333333333333, 81.33333333333333, 373.6666666666667, 26.0, 27.33040898927599, 0.7739767051327252, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5549399815327793, 0.3633333333333333, 0.2711111111111111, 0.41289134438305714, 0.6666666666666666, 0.7775340824396659, 0.7579922350442417, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.107325], dtype=float32), 0.050361205]. 
=============================================
[2019-04-04 13:55:15,622] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3378466e-08 2.1880269e-09 9.6484988e-14 7.3567899e-13 1.0000000e+00
 9.2077695e-10 2.2340517e-13], sum to 1.0000
[2019-04-04 13:55:15,622] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9739
[2019-04-04 13:55:15,643] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.166666666666667, 40.16666666666666, 0.0, 0.0, 26.0, 25.35959668934127, 0.4191459128292219, 0.0, 1.0, 51474.77941189879], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4151400.0000, 
sim time next is 4152000.0000, 
raw observation next is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 26.0, 25.33290802568012, 0.4149976893336213, 0.0, 1.0, 44753.07046064206], 
processed observation next is [0.0, 0.043478260869565216, 0.42566943674976926, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.6110756688066766, 0.6383325631112071, 0.0, 1.0, 0.21310985933639076], 
reward next is 0.7869, 
noisyNet noise sample is [array([-1.1242054], dtype=float32), 0.16515721]. 
=============================================
[2019-04-04 13:55:15,661] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.02168]
 [76.16322]
 [76.82335]
 [77.50388]
 [78.19484]], R is [[76.08583832]
 [76.0798645 ]
 [76.11057281]
 [76.24588776]
 [76.35752869]].
[2019-04-04 13:55:26,777] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.2327476e-10 5.4668287e-10 3.6926173e-15 5.5680922e-15 1.0000000e+00
 1.8692986e-10 8.8580556e-16], sum to 1.0000
[2019-04-04 13:55:26,779] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7084
[2019-04-04 13:55:26,799] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.0, 35.0, 93.83333333333334, 652.0, 26.0, 26.39612992105547, 0.7121850346888964, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4117200.0000, 
sim time next is 4117800.0000, 
raw observation next is [4.0, 35.0, 93.66666666666666, 609.0, 26.0, 26.76431163642936, 0.7429458689806013, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.3122222222222222, 0.6729281767955801, 0.6666666666666666, 0.7303593030357799, 0.7476486229935339, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9094013], dtype=float32), 0.91177803]. 
=============================================
[2019-04-04 13:55:28,482] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6750957e-09 1.5399391e-09 2.4417617e-14 3.2989572e-13 1.0000000e+00
 3.5828207e-10 2.3653730e-14], sum to 1.0000
[2019-04-04 13:55:28,482] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1715
[2019-04-04 13:55:28,501] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.50219447772328, 0.3772646349567366, 0.0, 1.0, 48502.4078066952], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4222800.0000, 
sim time next is 4223400.0000, 
raw observation next is [1.0, 43.66666666666667, 0.0, 0.0, 26.0, 25.4605922537011, 0.3754897795999305, 0.0, 1.0, 63657.87193689325], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6217160211417584, 0.6251632598666436, 0.0, 1.0, 0.3031327235090155], 
reward next is 0.6969, 
noisyNet noise sample is [array([-1.3568889], dtype=float32), -0.67003536]. 
=============================================
[2019-04-04 13:55:31,001] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5415177e-09 6.2497091e-11 2.2550471e-16 5.1980543e-16 1.0000000e+00
 3.5027547e-11 4.4610797e-17], sum to 1.0000
[2019-04-04 13:55:31,003] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8597
[2019-04-04 13:55:31,055] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.9333333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.21055773401685, 0.4304753313418897, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4519200.0000, 
sim time next is 4519800.0000, 
raw observation next is [-0.9, 72.0, 0.0, 0.0, 26.0, 25.52360645142301, 0.4339486057293669, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.43767313019390586, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6269672042852509, 0.6446495352431223, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.83674663], dtype=float32), -1.0699867]. 
=============================================
[2019-04-04 13:55:31,504] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0658192e-09 1.5677491e-10 1.2720178e-15 1.7375215e-15 1.0000000e+00
 1.5133048e-10 1.4242880e-15], sum to 1.0000
[2019-04-04 13:55:31,506] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4831
[2019-04-04 13:55:31,515] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 56.16666666666666, 201.6666666666667, 606.3333333333334, 26.0, 25.37189141583549, 0.4429090001489131, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4283400.0000, 
sim time next is 4284000.0000, 
raw observation next is [7.0, 57.0, 208.5, 551.0, 26.0, 25.40400240155363, 0.4433923069560957, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6565096952908588, 0.57, 0.695, 0.6088397790055249, 0.6666666666666666, 0.6170002001294691, 0.6477974356520318, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5574401], dtype=float32), 1.4855568]. 
=============================================
[2019-04-04 13:55:31,519] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.92652]
 [84.7317 ]
 [84.53248]
 [84.33899]
 [84.22296]], R is [[85.25906372]
 [85.40647125]
 [85.55240631]
 [85.69688416]
 [85.83991241]].
[2019-04-04 13:55:35,178] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.09538972e-11 1.26907105e-11 1.71660994e-16 2.78001587e-16
 1.00000000e+00 2.10101051e-11 7.20192940e-17], sum to 1.0000
[2019-04-04 13:55:35,185] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5932
[2019-04-04 13:55:35,205] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.0, 35.0, 71.0, 0.0, 26.0, 28.50158550134771, 0.9442936981100102, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4377600.0000, 
sim time next is 4378200.0000, 
raw observation next is [13.0, 35.5, 60.33333333333333, 0.0, 26.0, 28.23752376563645, 1.065268728752514, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.355, 0.2011111111111111, 0.0, 0.6666666666666666, 0.8531269804697041, 0.855089576250838, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1599512], dtype=float32), -0.17019258]. 
=============================================
[2019-04-04 13:55:41,695] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3124021e-10 5.9050535e-11 2.3999175e-16 4.9688620e-16 1.0000000e+00
 8.1122740e-11 1.4781233e-16], sum to 1.0000
[2019-04-04 13:55:41,696] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3266
[2019-04-04 13:55:41,723] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 72.0, 64.5, 19.5, 26.0, 25.42567177108764, 0.4967021836291559, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4726800.0000, 
sim time next is 4727400.0000, 
raw observation next is [0.8333333333333334, 73.0, 52.66666666666666, 22.33333333333334, 26.0, 25.80370960562644, 0.5220627368075648, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4856879039704525, 0.73, 0.17555555555555552, 0.024677716390423578, 0.6666666666666666, 0.6503091338022035, 0.6740209122691883, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6797745], dtype=float32), 0.5978269]. 
=============================================
[2019-04-04 13:55:41,795] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4817089e-10 2.1297793e-10 1.0277563e-15 1.7478856e-15 1.0000000e+00
 4.0223214e-11 4.2614246e-16], sum to 1.0000
[2019-04-04 13:55:41,795] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8217
[2019-04-04 13:55:41,819] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3333333333333333, 72.16666666666667, 0.0, 0.0, 26.0, 25.35009453182054, 0.4860300370268502, 0.0, 1.0, 44812.39953265363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4489800.0000, 
sim time next is 4490400.0000, 
raw observation next is [-0.3666666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.37654928967753, 0.4848944923534291, 0.0, 1.0, 41308.10500806948], 
processed observation next is [1.0, 1.0, 0.4524469067405356, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6147124408064609, 0.661631497451143, 0.0, 1.0, 0.196705261943188], 
reward next is 0.8033, 
noisyNet noise sample is [array([1.7922814], dtype=float32), 0.3256075]. 
=============================================
[2019-04-04 13:55:49,534] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.0101666e-09 2.0813785e-11 5.9147548e-16 2.4612306e-15 1.0000000e+00
 1.3700936e-10 4.8494833e-16], sum to 1.0000
[2019-04-04 13:55:49,537] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3243
[2019-04-04 13:55:49,549] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 56.16666666666666, 0.0, 0.0, 26.0, 25.96487217390907, 0.6367239788100469, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4657800.0000, 
sim time next is 4658400.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 26.0, 25.93488243562176, 0.6263067772008984, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.57, 0.0, 0.0, 0.6666666666666666, 0.66124020296848, 0.7087689257336328, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24164067], dtype=float32), 0.09652468]. 
=============================================
[2019-04-04 13:55:51,064] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3478010e-11 2.2318692e-11 2.1455003e-18 2.4944154e-16 1.0000000e+00
 1.4709517e-12 7.7860691e-18], sum to 1.0000
[2019-04-04 13:55:51,066] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0069
[2019-04-04 13:55:51,076] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 89.0, 213.0, 6.0, 26.0, 26.36554341509872, 0.5758968167881844, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4707000.0000, 
sim time next is 4707600.0000, 
raw observation next is [0.6666666666666666, 88.0, 195.5, 5.0, 26.0, 26.34886297553366, 0.5655578590931781, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4810710987996307, 0.88, 0.6516666666666666, 0.0055248618784530384, 0.6666666666666666, 0.6957385812944716, 0.6885192863643926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36274773], dtype=float32), 0.39383388]. 
=============================================
[2019-04-04 13:55:51,465] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.30427905e-10 1.29694536e-11 1.10512595e-17 3.28147472e-16
 1.00000000e+00 4.01702890e-12 3.18900315e-18], sum to 1.0000
[2019-04-04 13:55:51,467] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7770
[2019-04-04 13:55:51,511] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 92.0, 63.0, 0.0, 26.0, 25.96363973913056, 0.5208930878122416, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4696200.0000, 
sim time next is 4696800.0000, 
raw observation next is [0.0, 92.0, 71.66666666666666, 0.0, 26.0, 26.16048831232909, 0.5233289547440699, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.23888888888888885, 0.0, 0.6666666666666666, 0.6800406926940908, 0.6744429849146899, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5638529], dtype=float32), 1.6738588]. 
=============================================
[2019-04-04 13:55:53,782] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:55:53,783] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:55:53,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run15
[2019-04-04 13:55:54,842] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3414894e-09 2.7485533e-10 1.5545957e-15 1.0895226e-14 1.0000000e+00
 3.8958916e-11 2.5964954e-15], sum to 1.0000
[2019-04-04 13:55:54,845] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5846
[2019-04-04 13:55:54,859] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.066666666666666, 92.33333333333333, 20.66666666666666, 69.83333333333331, 26.0, 23.82888753201604, 0.09384425587689972, 0.0, 1.0, 41892.23510127194], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4779600.0000, 
sim time next is 4780200.0000, 
raw observation next is [-6.033333333333333, 92.16666666666667, 41.33333333333332, 139.6666666666666, 26.0, 23.8088011879158, 0.09990877959353417, 0.0, 1.0, 41789.8233482893], 
processed observation next is [0.0, 0.30434782608695654, 0.29547553093259465, 0.9216666666666667, 0.13777777777777775, 0.1543278084714548, 0.6666666666666666, 0.48406676565965007, 0.5333029265311781, 0.0, 1.0, 0.1989991588013776], 
reward next is 0.8010, 
noisyNet noise sample is [array([-0.59776294], dtype=float32), 0.049952798]. 
=============================================
[2019-04-04 13:55:57,040] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6743755e-09 5.7519617e-10 3.4116098e-15 1.4635760e-14 1.0000000e+00
 2.5390681e-11 2.0613438e-15], sum to 1.0000
[2019-04-04 13:55:57,040] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1781
[2019-04-04 13:55:57,071] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 45.0, 127.1666666666667, 820.8333333333334, 26.0, 25.1424724804528, 0.4150456441270038, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4792800.0000, 
sim time next is 4793400.0000, 
raw observation next is [-0.5, 44.5, 122.0, 839.0, 26.0, 25.07469522236305, 0.4085688414206312, 0.0, 1.0, 18707.90478214007], 
processed observation next is [0.0, 0.4782608695652174, 0.44875346260387816, 0.445, 0.4066666666666667, 0.9270718232044199, 0.6666666666666666, 0.5895579351969209, 0.6361896138068771, 0.0, 1.0, 0.08908526086733368], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.4137985], dtype=float32), -1.518135]. 
=============================================
[2019-04-04 13:56:00,817] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.91887250e-09 7.48155260e-10 1.76780198e-14 4.43258813e-14
 1.00000000e+00 1.15837381e-10 1.23990055e-14], sum to 1.0000
[2019-04-04 13:56:00,817] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2824
[2019-04-04 13:56:00,826] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 45.0, 142.6666666666667, 387.0, 26.0, 25.12260978334207, 0.3735794302677092, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4895400.0000, 
sim time next is 4896000.0000, 
raw observation next is [3.0, 45.0, 132.5, 369.5, 26.0, 25.12908680882906, 0.3747229771698656, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.44166666666666665, 0.40828729281767956, 0.6666666666666666, 0.5940905674024218, 0.6249076590566219, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48448223], dtype=float32), 1.3454803]. 
=============================================
[2019-04-04 13:56:00,830] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.21838 ]
 [78.49512 ]
 [78.748795]
 [78.99013 ]
 [79.21389 ]], R is [[78.20366669]
 [78.42163086]
 [78.63741302]
 [78.85103607]
 [79.06252289]].
[2019-04-04 13:56:00,926] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3135776e-08 1.7739979e-09 3.8639014e-14 2.9823556e-13 1.0000000e+00
 7.9334311e-10 1.8919501e-14], sum to 1.0000
[2019-04-04 13:56:00,926] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6138
[2019-04-04 13:56:00,936] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.833333333333333, 44.83333333333334, 56.0, 230.3333333333333, 26.0, 25.09803149334947, 0.3300048472493716, 0.0, 1.0, 33263.93825150133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4900200.0000, 
sim time next is 4900800.0000, 
raw observation next is [2.666666666666667, 44.66666666666667, 44.5, 208.6666666666667, 26.0, 25.05963035589607, 0.3244513672759091, 0.0, 1.0, 45069.0552599011], 
processed observation next is [0.0, 0.7391304347826086, 0.5364727608494922, 0.4466666666666667, 0.14833333333333334, 0.23057090239410685, 0.6666666666666666, 0.5883025296580057, 0.6081504557586364, 0.0, 1.0, 0.2146145488566719], 
reward next is 0.7854, 
noisyNet noise sample is [array([-0.94625026], dtype=float32), 0.07968521]. 
=============================================
[2019-04-04 13:56:01,102] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:56:01,103] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:56:01,107] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run15
[2019-04-04 13:56:01,632] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:56:01,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:56:01,649] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run15
[2019-04-04 13:56:03,772] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 13:56:03,776] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 13:56:03,777] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 13:56:03,777] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:56:03,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:56:03,777] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 13:56:03,778] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:56:03,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run20
[2019-04-04 13:56:03,801] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run20
[2019-04-04 13:56:03,817] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run20
[2019-04-04 13:56:54,978] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17191625], dtype=float32), 0.20432355]
[2019-04-04 13:56:54,978] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-7.3, 75.0, 0.0, 0.0, 26.0, 24.14768900928649, 0.04417063168156043, 0.0, 1.0, 45154.62853315799]
[2019-04-04 13:56:54,978] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 13:56:54,979] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.2550307e-09 4.7027249e-10 9.1641367e-15 1.3599325e-13 1.0000000e+00
 2.2563103e-10 1.5975787e-14], sampled 0.14512963687795744
[2019-04-04 13:57:30,982] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17191625], dtype=float32), 0.20432355]
[2019-04-04 13:57:30,982] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.0, 70.0, 0.0, 0.0, 26.0, 25.79176387535801, 0.5753833251803645, 1.0, 1.0, 0.0]
[2019-04-04 13:57:30,982] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 13:57:30,982] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.6551855e-09 1.3608271e-10 2.0122953e-15 1.7337353e-14 1.0000000e+00
 1.1902172e-10 2.2232991e-15], sampled 0.5364942108089167
[2019-04-04 13:57:41,281] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17191625], dtype=float32), 0.20432355]
[2019-04-04 13:57:41,281] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [5.021367788666668, 60.38710733333333, 0.0, 0.0, 26.0, 25.84982438304449, 0.4879300552781777, 0.0, 1.0, 0.0]
[2019-04-04 13:57:41,281] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 13:57:41,282] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.0375643e-09 8.0665412e-11 4.6407029e-16 5.9296681e-15 1.0000000e+00
 2.4520937e-11 8.3064037e-16], sampled 0.9031112389036208
[2019-04-04 13:57:45,050] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4465 239936225.3969 1605.1790
[2019-04-04 13:57:46,882] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17191625], dtype=float32), 0.20432355]
[2019-04-04 13:57:46,882] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-10.07580671, 57.83663219, 0.0, 0.0, 26.0, 23.73605451877173, 0.04064766451685724, 0.0, 1.0, 44386.96691520564]
[2019-04-04 13:57:46,882] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 13:57:46,883] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.0940534e-08 5.1551723e-09 1.3465959e-13 1.2166391e-12 1.0000000e+00
 1.4474972e-09 2.3127981e-13], sampled 0.03128730763500365
[2019-04-04 13:58:04,195] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 13:58:06,813] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7307 275786560.0346 1233.1784
[2019-04-04 13:58:07,837] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1900000, evaluation results [1900000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.446545728992, 239936225.3969133, 1605.1790028959388, 7182.730666501954, 275786560.0345853, 1233.1784155230887]
[2019-04-04 13:58:10,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0147792e-08 6.9297124e-10 4.2034654e-15 4.6213633e-14 1.0000000e+00
 1.8792645e-11 4.8407988e-15], sum to 1.0000
[2019-04-04 13:58:10,456] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1784
[2019-04-04 13:58:10,469] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 46.66666666666667, 0.0, 0.0, 26.0, 25.46163225691539, 0.3659891413513731, 0.0, 1.0, 42137.72684907135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5032200.0000, 
sim time next is 5032800.0000, 
raw observation next is [-1.0, 46.0, 0.0, 0.0, 26.0, 25.37732221081928, 0.3641561400090136, 0.0, 1.0, 82052.15940973008], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.46, 0.0, 0.0, 0.6666666666666666, 0.6147768509016066, 0.6213853800030046, 0.0, 1.0, 0.3907245686177623], 
reward next is 0.6093, 
noisyNet noise sample is [array([0.47029796], dtype=float32), 1.3148317]. 
=============================================
[2019-04-04 13:58:11,927] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:58:11,927] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:58:11,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run15
[2019-04-04 13:58:12,626] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:58:12,626] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:58:12,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run15
[2019-04-04 13:58:14,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:58:14,182] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:58:14,200] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run15
[2019-04-04 13:58:14,423] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:58:14,423] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:58:14,434] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run15
[2019-04-04 13:58:14,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:58:14,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:58:14,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run15
[2019-04-04 13:58:15,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:58:15,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:58:15,012] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run15
[2019-04-04 13:58:15,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:58:15,366] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:58:15,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run15
[2019-04-04 13:58:15,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:58:15,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:58:15,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run15
[2019-04-04 13:58:15,572] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:58:15,574] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:58:15,579] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run15
[2019-04-04 13:58:16,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:58:16,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:58:16,086] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run15
[2019-04-04 13:58:16,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:58:16,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:58:16,168] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run15
[2019-04-04 13:58:16,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:58:16,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:58:16,468] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run15
[2019-04-04 13:58:18,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 13:58:18,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 13:58:18,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run15
[2019-04-04 13:58:30,255] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.3234874e-10 1.9075921e-11 8.4068457e-17 5.3217314e-15 1.0000000e+00
 7.0837359e-12 9.5500364e-17], sum to 1.0000
[2019-04-04 13:58:30,255] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1085
[2019-04-04 13:58:30,277] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.516666666666667, 91.0, 0.0, 0.0, 26.0, 24.25139907738383, 0.1248595648405281, 0.0, 1.0, 41818.35562747416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 93000.0000, 
sim time next is 93600.0000, 
raw observation next is [-1.7, 91.0, 0.0, 0.0, 26.0, 24.24411452235943, 0.1233700044417482, 0.0, 1.0, 41973.5003449189], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5203428768632857, 0.5411233348139161, 0.0, 1.0, 0.1998738111662805], 
reward next is 0.8001, 
noisyNet noise sample is [array([0.56588995], dtype=float32), 1.874673]. 
=============================================
[2019-04-04 13:58:35,871] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.8333837e-09 4.3980439e-10 5.8646334e-15 3.2437489e-14 1.0000000e+00
 1.1658197e-09 5.1046199e-15], sum to 1.0000
[2019-04-04 13:58:35,871] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2375
[2019-04-04 13:58:35,935] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 64.33333333333333, 0.0, 0.0, 26.0, 24.89480252952488, 0.3312323105919537, 1.0, 1.0, 169544.1473052755], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 150000.0000, 
sim time next is 150600.0000, 
raw observation next is [-7.299999999999999, 62.66666666666666, 0.0, 0.0, 26.0, 25.08837852912905, 0.3535037538711989, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.2603878116343491, 0.6266666666666666, 0.0, 0.0, 0.6666666666666666, 0.590698210760754, 0.617834584623733, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9393764], dtype=float32), 0.5904856]. 
=============================================
[2019-04-04 13:58:46,741] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.07064246e-10 1.26959707e-10 1.61504368e-16 1.50420452e-15
 1.00000000e+00 7.37863156e-12 6.18469789e-16], sum to 1.0000
[2019-04-04 13:58:46,743] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2832
[2019-04-04 13:58:46,766] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.25, 84.0, 0.0, 0.0, 26.0, 24.72935489484033, 0.2144816085870439, 0.0, 1.0, 39972.32161812517], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 531000.0000, 
sim time next is 531600.0000, 
raw observation next is [3.066666666666667, 83.33333333333334, 0.0, 0.0, 26.0, 24.70953473411188, 0.2108946260276024, 0.0, 1.0, 40030.66404359531], 
processed observation next is [0.0, 0.13043478260869565, 0.5475530932594646, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.5591278945093233, 0.5702982086758674, 0.0, 1.0, 0.19062220973140623], 
reward next is 0.8094, 
noisyNet noise sample is [array([1.1863493], dtype=float32), -0.3253995]. 
=============================================
[2019-04-04 13:59:00,858] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1654301e-08 7.3582462e-10 8.2431701e-14 2.4023210e-13 1.0000000e+00
 1.6440819e-09 2.0175498e-14], sum to 1.0000
[2019-04-04 13:59:00,859] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1053
[2019-04-04 13:59:00,900] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.766666666666667, 30.66666666666667, 92.0, 0.0, 26.0, 25.51382912872108, 0.189998075081951, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 469200.0000, 
sim time next is 469800.0000, 
raw observation next is [-3.4, 30.0, 98.0, 0.0, 26.0, 25.47240956494794, 0.1736042507390298, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.368421052631579, 0.3, 0.32666666666666666, 0.0, 0.6666666666666666, 0.6227007970789952, 0.5578680835796767, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5939062], dtype=float32), -1.3966125]. 
=============================================
[2019-04-04 13:59:01,617] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3981161e-08 7.7117610e-09 3.2442948e-13 1.4147245e-12 9.9999988e-01
 5.0802944e-09 4.2191324e-13], sum to 1.0000
[2019-04-04 13:59:01,618] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2724
[2019-04-04 13:59:01,657] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 30.0, 98.0, 0.0, 26.0, 25.47240956494794, 0.1736042507390298, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 469800.0000, 
sim time next is 470400.0000, 
raw observation next is [-3.033333333333333, 29.33333333333333, 102.0, 0.0, 26.0, 25.35822583698339, 0.1671124352577991, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.37857802400738694, 0.2933333333333333, 0.34, 0.0, 0.6666666666666666, 0.6131854864152825, 0.555704145085933, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5362099], dtype=float32), -1.4684075]. 
=============================================
[2019-04-04 13:59:02,921] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.2242934e-09 1.6167938e-09 1.5649198e-14 4.4836747e-14 1.0000000e+00
 3.6955436e-10 8.6377967e-14], sum to 1.0000
[2019-04-04 13:59:02,921] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3535
[2019-04-04 13:59:02,983] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 28.0, 124.0, 0.0, 26.0, 24.98723743956625, 0.1569609369856965, 1.0, 1.0, 73047.47583006487], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 478800.0000, 
sim time next is 479400.0000, 
raw observation next is [-1.1, 29.16666666666667, 122.3333333333333, 0.0, 26.0, 24.97024412655644, 0.04144893452784474, 1.0, 1.0, 56026.22363327091], 
processed observation next is [1.0, 0.5652173913043478, 0.4321329639889197, 0.29166666666666674, 0.4077777777777777, 0.0, 0.6666666666666666, 0.5808536772130367, 0.5138163115092816, 1.0, 1.0, 0.2667915411108138], 
reward next is 0.7332, 
noisyNet noise sample is [array([-0.32492104], dtype=float32), -0.7189032]. 
=============================================
[2019-04-04 13:59:05,675] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8614960e-10 1.3812417e-11 1.4233531e-16 5.4276220e-16 1.0000000e+00
 4.9980627e-12 1.0855099e-16], sum to 1.0000
[2019-04-04 13:59:05,678] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6638
[2019-04-04 13:59:05,689] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.716666666666666, 96.83333333333334, 0.0, 0.0, 26.0, 24.88396069463973, 0.2412304247799812, 0.0, 1.0, 39846.87247087662], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 517800.0000, 
sim time next is 518400.0000, 
raw observation next is [3.8, 97.0, 0.0, 0.0, 26.0, 24.88323567034572, 0.2391238128371389, 0.0, 1.0, 39791.34591855376], 
processed observation next is [0.0, 0.0, 0.5678670360110805, 0.97, 0.0, 0.0, 0.6666666666666666, 0.57360297252881, 0.5797079376123796, 0.0, 1.0, 0.18948259961216074], 
reward next is 0.8105, 
noisyNet noise sample is [array([2.5236778], dtype=float32), 0.69267166]. 
=============================================
[2019-04-04 13:59:07,158] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.6925694e-10 7.1383316e-11 8.4676765e-17 4.9153987e-15 1.0000000e+00
 6.7441684e-11 2.7593708e-16], sum to 1.0000
[2019-04-04 13:59:07,159] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8095
[2019-04-04 13:59:07,187] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.516666666666667, 96.0, 0.0, 0.0, 26.0, 24.8626257931677, 0.2354410471362184, 0.0, 1.0, 47629.92227680673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 507000.0000, 
sim time next is 507600.0000, 
raw observation next is [1.6, 96.0, 0.0, 0.0, 26.0, 24.87176511017244, 0.2358926882494762, 0.0, 1.0, 43769.87701904616], 
processed observation next is [1.0, 0.9130434782608695, 0.5069252077562327, 0.96, 0.0, 0.0, 0.6666666666666666, 0.57264709251437, 0.5786308960831588, 0.0, 1.0, 0.2084279858049817], 
reward next is 0.7916, 
noisyNet noise sample is [array([-0.19595924], dtype=float32), -0.5261546]. 
=============================================
[2019-04-04 13:59:08,410] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6002586e-10 2.9207834e-11 1.1644444e-16 1.7579158e-15 1.0000000e+00
 2.5959757e-11 4.6748527e-16], sum to 1.0000
[2019-04-04 13:59:08,410] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2188
[2019-04-04 13:59:08,461] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 80.0, 134.3333333333333, 552.3333333333333, 26.0, 24.99726839148161, 0.3501328964923981, 0.0, 1.0, 18725.75373743529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 568200.0000, 
sim time next is 568800.0000, 
raw observation next is [-1.2, 80.0, 132.5, 531.0, 26.0, 25.00245289473588, 0.349198396560372, 0.0, 1.0, 18723.87818829856], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.8, 0.44166666666666665, 0.5867403314917127, 0.6666666666666666, 0.5835377412279902, 0.616399465520124, 0.0, 1.0, 0.08916132470618363], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.40359083], dtype=float32), -0.9789222]. 
=============================================
[2019-04-04 13:59:12,223] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4742349e-08 3.4435235e-09 1.2351850e-13 8.1079962e-13 1.0000000e+00
 9.4633967e-10 8.2708098e-14], sum to 1.0000
[2019-04-04 13:59:12,224] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4428
[2019-04-04 13:59:12,247] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 67.0, 0.0, 0.0, 26.0, 23.66699945000402, -0.03465637629531366, 0.0, 1.0, 43735.67684035289], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 628800.0000, 
sim time next is 629400.0000, 
raw observation next is [-4.5, 67.5, 0.0, 0.0, 26.0, 23.63981323334947, -0.04189625574924715, 0.0, 1.0, 43757.31735707481], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.675, 0.0, 0.0, 0.6666666666666666, 0.46998443611245594, 0.48603458141691763, 0.0, 1.0, 0.20836817789083242], 
reward next is 0.7916, 
noisyNet noise sample is [array([0.49264368], dtype=float32), -1.141195]. 
=============================================
[2019-04-04 13:59:12,315] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5556335e-08 1.6184075e-09 4.3805878e-14 5.0666692e-13 1.0000000e+00
 4.4417800e-10 2.3298919e-14], sum to 1.0000
[2019-04-04 13:59:12,316] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6903
[2019-04-04 13:59:12,337] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 67.0, 0.0, 0.0, 26.0, 23.77506389384082, -0.0005536446398306019, 0.0, 1.0, 44230.8868791183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 624000.0000, 
sim time next is 624600.0000, 
raw observation next is [-4.5, 66.5, 0.0, 0.0, 26.0, 23.75423209237816, -0.005786277350948717, 0.0, 1.0, 44146.15497804353], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.665, 0.0, 0.0, 0.6666666666666666, 0.47951934103151334, 0.4980712408830171, 0.0, 1.0, 0.2102197856097311], 
reward next is 0.7898, 
noisyNet noise sample is [array([-0.44264463], dtype=float32), 2.2036157]. 
=============================================
[2019-04-04 13:59:16,838] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.45512704e-08 7.82382381e-10 1.40769795e-14 5.13576920e-13
 1.00000000e+00 2.83336327e-10 2.44266074e-14], sum to 1.0000
[2019-04-04 13:59:16,838] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8783
[2019-04-04 13:59:16,895] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8, 55.00000000000001, 36.33333333333333, 18.83333333333333, 26.0, 24.89885127801561, 0.2153314713183464, 0.0, 1.0, 37877.71052384419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 663600.0000, 
sim time next is 664200.0000, 
raw observation next is [-0.8999999999999999, 55.5, 27.0, 15.0, 26.0, 24.8881848369501, 0.2118319664785527, 0.0, 1.0, 47393.28540702795], 
processed observation next is [0.0, 0.6956521739130435, 0.43767313019390586, 0.555, 0.09, 0.016574585635359115, 0.6666666666666666, 0.574015403079175, 0.5706106554928508, 0.0, 1.0, 0.22568231146203785], 
reward next is 0.7743, 
noisyNet noise sample is [array([-0.71648526], dtype=float32), 1.2063955]. 
=============================================
[2019-04-04 13:59:22,925] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.7935598e-09 1.0426006e-10 1.4056018e-15 2.1197081e-14 1.0000000e+00
 3.0244066e-10 4.4585813e-16], sum to 1.0000
[2019-04-04 13:59:22,926] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8981
[2019-04-04 13:59:22,973] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 71.0, 104.5, 0.0, 26.0, 25.63482878984945, 0.3142589978181249, 1.0, 1.0, 27507.23306636863], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 820800.0000, 
sim time next is 821400.0000, 
raw observation next is [-4.5, 72.33333333333334, 102.6666666666667, 0.0, 26.0, 25.61386364740836, 0.3119579178440714, 1.0, 1.0, 27607.86632318267], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7233333333333334, 0.3422222222222223, 0.0, 0.6666666666666666, 0.6344886372840298, 0.6039859726146904, 1.0, 1.0, 0.13146603011039365], 
reward next is 0.8685, 
noisyNet noise sample is [array([2.2376492], dtype=float32), 0.19725892]. 
=============================================
[2019-04-04 13:59:25,520] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3452755e-09 6.4405613e-11 3.6580973e-16 2.3860342e-15 1.0000000e+00
 1.3184459e-10 1.1926912e-15], sum to 1.0000
[2019-04-04 13:59:25,520] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1687
[2019-04-04 13:59:25,543] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.90767199412814, 0.275339706042576, 0.0, 1.0, 42407.0964362668], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 854400.0000, 
sim time next is 855000.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.8715488010336, 0.2682870784135584, 0.0, 1.0, 42276.88974882339], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5726290667528001, 0.5894290261378529, 0.0, 1.0, 0.20131852261344474], 
reward next is 0.7987, 
noisyNet noise sample is [array([0.5985844], dtype=float32), 0.07334494]. 
=============================================
[2019-04-04 13:59:25,553] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[85.16802 ]
 [85.231384]
 [85.33806 ]
 [85.49358 ]
 [85.765076]], R is [[85.27272797]
 [85.21806335]
 [85.16281891]
 [85.10520172]
 [85.03907013]].
[2019-04-04 13:59:33,799] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.1122199e-10 2.6582726e-11 2.4388826e-16 1.4657264e-15 1.0000000e+00
 5.9399668e-12 1.7430972e-16], sum to 1.0000
[2019-04-04 13:59:33,800] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2619
[2019-04-04 13:59:33,822] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.4, 93.0, 54.0, 0.0, 26.0, 25.78008776910712, 0.4083095766758851, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 919800.0000, 
sim time next is 920400.0000, 
raw observation next is [4.4, 93.0, 48.0, 0.0, 26.0, 25.78810792894519, 0.4090477259313788, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5844875346260389, 0.93, 0.16, 0.0, 0.6666666666666666, 0.6490089940787659, 0.6363492419771263, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.58237195], dtype=float32), 0.3941524]. 
=============================================
[2019-04-04 13:59:35,216] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1678909e-10 3.7360809e-11 2.8860308e-17 3.9451896e-16 1.0000000e+00
 7.1105656e-12 9.2412841e-18], sum to 1.0000
[2019-04-04 13:59:35,216] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3931
[2019-04-04 13:59:35,231] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.41666666666667, 92.83333333333333, 72.0, 0.0, 26.0, 26.44149902774766, 0.6189538362590746, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 985800.0000, 
sim time next is 986400.0000, 
raw observation next is [10.5, 93.0, 78.0, 0.0, 26.0, 26.47276495812752, 0.6299986621124463, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7534626038781165, 0.93, 0.26, 0.0, 0.6666666666666666, 0.7060637465106266, 0.709999554037482, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46752292], dtype=float32), 0.63710874]. 
=============================================
[2019-04-04 13:59:43,985] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.39445016e-08 9.08205067e-09 3.24095981e-14 1.97058233e-12
 1.00000000e+00 6.69379441e-10 8.09890349e-14], sum to 1.0000
[2019-04-04 13:59:43,990] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4601
[2019-04-04 13:59:43,995] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.08333333333333, 95.5, 0.0, 0.0, 26.0, 23.69219324876644, 0.1869189535862591, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1227000.0000, 
sim time next is 1227600.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.66923643627832, 0.1823068529155665, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.47243636968986014, 0.5607689509718555, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.65899265], dtype=float32), -0.7705365]. 
=============================================
[2019-04-04 13:59:49,581] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8074750e-10 6.8125602e-11 6.0995736e-17 1.5870000e-15 1.0000000e+00
 1.6564092e-11 9.9303535e-17], sum to 1.0000
[2019-04-04 13:59:49,585] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4717
[2019-04-04 13:59:49,623] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 62.0, 0.0, 26.0, 25.81545261671846, 0.520789084358961, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1349400.0000, 
sim time next is 1350000.0000, 
raw observation next is [1.1, 92.0, 57.5, 0.0, 26.0, 25.74016726660579, 0.5141834657957186, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.19166666666666668, 0.0, 0.6666666666666666, 0.645013938883816, 0.6713944885985729, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3162556], dtype=float32), 0.84326977]. 
=============================================
[2019-04-04 13:59:49,643] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[91.490944]
 [91.68305 ]
 [91.89868 ]
 [92.06948 ]
 [92.26653 ]], R is [[91.4159317 ]
 [91.50177002]
 [91.58675385]
 [91.67089081]
 [91.75418091]].
[2019-04-04 13:59:58,502] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.2341989e-10 2.6346142e-11 9.9491980e-17 6.5284323e-16 1.0000000e+00
 5.8619359e-12 4.4785229e-17], sum to 1.0000
[2019-04-04 13:59:58,505] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4503
[2019-04-04 13:59:58,527] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.65, 98.0, 0.0, 0.0, 26.0, 25.49115832681751, 0.4654603526865135, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1492200.0000, 
sim time next is 1492800.0000, 
raw observation next is [1.466666666666667, 98.66666666666666, 0.0, 0.0, 26.0, 25.51645211170326, 0.4679383317271704, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.5032317636195753, 0.9866666666666666, 0.0, 0.0, 0.6666666666666666, 0.6263710093086049, 0.6559794439090568, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65963733], dtype=float32), -0.65977436]. 
=============================================
[2019-04-04 14:00:04,227] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0334246e-09 3.4516889e-10 1.1904860e-16 5.4302757e-15 1.0000000e+00
 1.3915573e-11 4.7200270e-16], sum to 1.0000
[2019-04-04 14:00:04,228] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9437
[2019-04-04 14:00:04,241] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.65225849915074, 0.517977255540183, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1576800.0000, 
sim time next is 1577400.0000, 
raw observation next is [5.083333333333334, 81.50000000000001, 0.0, 0.0, 26.0, 25.6909287896839, 0.5051686714675562, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.6034164358264081, 0.8150000000000002, 0.0, 0.0, 0.6666666666666666, 0.6409107324736582, 0.668389557155852, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65261286], dtype=float32), -0.8401454]. 
=============================================
[2019-04-04 14:00:05,877] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.5741101e-10 2.6707900e-11 1.9847415e-17 3.8636177e-16 1.0000000e+00
 2.9682506e-11 3.8042672e-17], sum to 1.0000
[2019-04-04 14:00:05,879] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7674
[2019-04-04 14:00:05,900] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.283333333333333, 75.66666666666667, 0.0, 0.0, 26.0, 25.32524695307546, 0.6539328999289727, 0.0, 1.0, 193118.1180720251], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1630200.0000, 
sim time next is 1630800.0000, 
raw observation next is [7.2, 76.0, 0.0, 0.0, 26.0, 25.47609761202443, 0.6894605124571687, 0.0, 1.0, 56903.31298616351], 
processed observation next is [1.0, 0.9130434782608695, 0.662049861495845, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6230081343353691, 0.7298201708190563, 0.0, 1.0, 0.2709681570769691], 
reward next is 0.7290, 
noisyNet noise sample is [array([0.39410987], dtype=float32), -0.5334877]. 
=============================================
[2019-04-04 14:00:11,146] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1577700e-10 4.7702606e-11 2.0525234e-16 7.2337384e-15 1.0000000e+00
 4.6288688e-11 6.1959385e-16], sum to 1.0000
[2019-04-04 14:00:11,147] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8656
[2019-04-04 14:00:11,207] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 87.0, 100.6666666666667, 0.0, 26.0, 24.9547816724825, 0.333335327445405, 0.0, 1.0, 33970.08946800172], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1766400.0000, 
sim time next is 1767000.0000, 
raw observation next is [-2.3, 87.0, 104.3333333333333, 0.0, 26.0, 24.92229441355542, 0.3316072296496629, 0.0, 1.0, 56758.80928022896], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.3477777777777777, 0.0, 0.6666666666666666, 0.576857867796285, 0.6105357432165542, 0.0, 1.0, 0.2702800441915665], 
reward next is 0.7297, 
noisyNet noise sample is [array([-0.60136634], dtype=float32), -0.5958536]. 
=============================================
[2019-04-04 14:00:11,225] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.67432 ]
 [83.6274  ]
 [83.59688 ]
 [83.55049 ]
 [83.396576]], R is [[83.71656036]
 [83.71762848]
 [83.79122162]
 [83.86405945]
 [83.81544495]].
[2019-04-04 14:00:12,921] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0004896e-09 9.3864888e-11 2.7788705e-16 1.4358374e-14 1.0000000e+00
 2.5572209e-11 2.0738530e-15], sum to 1.0000
[2019-04-04 14:00:12,922] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8567
[2019-04-04 14:00:12,970] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 87.0, 86.33333333333333, 0.0, 26.0, 24.89780722281078, 0.3424605331151351, 0.0, 1.0, 44095.05157533227], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1764600.0000, 
sim time next is 1765200.0000, 
raw observation next is [-2.3, 87.0, 91.66666666666667, 0.0, 26.0, 24.9483436024968, 0.3450356323931392, 0.0, 1.0, 18741.81424530815], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.3055555555555556, 0.0, 0.6666666666666666, 0.5790286335413999, 0.6150118774643797, 0.0, 1.0, 0.08924673450146738], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.5212379], dtype=float32), -1.2710747]. 
=============================================
[2019-04-04 14:00:17,149] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0783268e-08 2.6648239e-09 1.2036890e-14 4.0251704e-14 1.0000000e+00
 1.0049361e-09 2.6487384e-14], sum to 1.0000
[2019-04-04 14:00:17,152] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9790
[2019-04-04 14:00:17,213] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.15, 78.0, 148.0, 94.0, 26.0, 25.18804859057562, 0.2623632594079757, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1848600.0000, 
sim time next is 1849200.0000, 
raw observation next is [-5.966666666666667, 78.0, 143.3333333333333, 86.83333333333334, 26.0, 25.15029351127778, 0.2491431242883055, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2973222530009234, 0.78, 0.47777777777777763, 0.09594843462246778, 0.6666666666666666, 0.5958577926064818, 0.5830477080961018, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8176129], dtype=float32), -0.17714225]. 
=============================================
[2019-04-04 14:00:24,788] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.5970180e-09 7.9889617e-10 9.1564143e-15 1.3822445e-13 1.0000000e+00
 4.2385942e-10 3.0526343e-15], sum to 1.0000
[2019-04-04 14:00:24,788] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5875
[2019-04-04 14:00:24,812] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.8, 83.0, 0.0, 0.0, 26.0, 24.81755632280268, 0.2524370352992329, 0.0, 1.0, 42935.35862838235], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1982400.0000, 
sim time next is 1983000.0000, 
raw observation next is [-5.7, 83.0, 0.0, 0.0, 26.0, 24.74841197852467, 0.2414485010030502, 0.0, 1.0, 42987.69677334322], 
processed observation next is [1.0, 0.9565217391304348, 0.30470914127423826, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5623676648770557, 0.5804828336676834, 0.0, 1.0, 0.20470331796830105], 
reward next is 0.7953, 
noisyNet noise sample is [array([1.217087], dtype=float32), 0.3897668]. 
=============================================
[2019-04-04 14:00:24,852] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.96626]
 [81.96271]
 [81.9731 ]
 [81.80678]
 [81.80316]], R is [[81.88628387]
 [81.86296844]
 [81.83908844]
 [81.813591  ]
 [81.78926849]].
[2019-04-04 14:00:25,910] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9917087e-08 3.9175272e-09 6.5685697e-14 3.8740875e-13 1.0000000e+00
 6.3454558e-10 6.2492791e-15], sum to 1.0000
[2019-04-04 14:00:25,911] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4115
[2019-04-04 14:00:25,929] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.65, 80.0, 0.0, 0.0, 26.0, 23.66569902036616, -0.07955487919851556, 0.0, 1.0, 45283.44142089177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1917000.0000, 
sim time next is 1917600.0000, 
raw observation next is [-8.733333333333334, 80.66666666666667, 0.0, 0.0, 26.0, 23.64073059030563, -0.08477321225039858, 0.0, 1.0, 45300.62290607415], 
processed observation next is [1.0, 0.17391304347826086, 0.22068328716528163, 0.8066666666666668, 0.0, 0.0, 0.6666666666666666, 0.4700608825254691, 0.47174226258320046, 0.0, 1.0, 0.21571725193368643], 
reward next is 0.7843, 
noisyNet noise sample is [array([-1.0453947], dtype=float32), 0.4966278]. 
=============================================
[2019-04-04 14:00:36,364] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3996883e-09 1.3060042e-10 2.2738610e-16 1.5175332e-14 1.0000000e+00
 3.1661310e-10 1.6370270e-15], sum to 1.0000
[2019-04-04 14:00:36,364] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7678
[2019-04-04 14:00:36,423] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.9666666666666668, 47.0, 204.3333333333333, 67.5, 26.0, 24.69597617636885, 0.3475088337999195, 1.0, 1.0, 198531.0323868092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2295600.0000, 
sim time next is 2296200.0000, 
raw observation next is [-0.7833333333333332, 46.0, 187.6666666666667, 66.0, 26.0, 25.27813311453504, 0.4151256246957483, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.44090489381348114, 0.46, 0.6255555555555558, 0.07292817679558011, 0.6666666666666666, 0.6065110928779202, 0.6383752082319161, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.66271526], dtype=float32), -0.19708197]. 
=============================================
[2019-04-04 14:00:37,327] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3750527e-09 3.3649059e-11 3.8991971e-16 8.6284061e-15 1.0000000e+00
 2.7526597e-11 5.8325254e-16], sum to 1.0000
[2019-04-04 14:00:37,328] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9362
[2019-04-04 14:00:37,392] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.80720439809514, 0.4543548385799438, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2056200.0000, 
sim time next is 2056800.0000, 
raw observation next is [-3.9, 83.33333333333334, 0.0, 0.0, 26.0, 25.70976880344432, 0.4179460400974593, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.64248073362036, 0.6393153466991531, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48705623], dtype=float32), -0.78915215]. 
=============================================
[2019-04-04 14:00:47,884] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.6377170e-09 7.7517670e-10 2.2099560e-15 4.1588877e-14 1.0000000e+00
 1.7477167e-10 1.5960650e-14], sum to 1.0000
[2019-04-04 14:00:47,885] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6261
[2019-04-04 14:00:47,903] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.1, 86.5, 0.0, 0.0, 26.0, 24.17360180711483, 0.08459785238658533, 0.0, 1.0, 43643.54090514162], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2259000.0000, 
sim time next is 2259600.0000, 
raw observation next is [-8.2, 86.66666666666667, 0.0, 0.0, 26.0, 24.09607287136721, 0.07463407924661536, 0.0, 1.0, 43646.29871100695], 
processed observation next is [1.0, 0.13043478260869565, 0.23545706371191139, 0.8666666666666667, 0.0, 0.0, 0.6666666666666666, 0.508006072613934, 0.5248780264155385, 0.0, 1.0, 0.20783951767146167], 
reward next is 0.7922, 
noisyNet noise sample is [array([-0.13177492], dtype=float32), 1.2409004]. 
=============================================
[2019-04-04 14:00:56,115] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.9511604e-09 3.9197898e-10 9.6702939e-14 6.0792490e-13 1.0000000e+00
 4.0221698e-10 2.7529706e-14], sum to 1.0000
[2019-04-04 14:00:56,115] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7249
[2019-04-04 14:00:56,168] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.9, 42.5, 0.0, 0.0, 26.0, 24.96470720644993, 0.2774038811183204, 0.0, 1.0, 86673.74046781595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2403000.0000, 
sim time next is 2403600.0000, 
raw observation next is [-3.066666666666666, 42.33333333333334, 0.0, 0.0, 26.0, 25.02048965392874, 0.2837701674063635, 0.0, 1.0, 58115.67200386018], 
processed observation next is [0.0, 0.8260869565217391, 0.3776546629732226, 0.42333333333333345, 0.0, 0.0, 0.6666666666666666, 0.5850408044940615, 0.5945900558021212, 0.0, 1.0, 0.27674129525647706], 
reward next is 0.7233, 
noisyNet noise sample is [array([-1.1768936], dtype=float32), 0.11103514]. 
=============================================
[2019-04-04 14:00:58,036] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2623737e-08 4.7906507e-10 3.3607684e-15 1.2279079e-13 1.0000000e+00
 2.1460535e-10 5.1514312e-15], sum to 1.0000
[2019-04-04 14:00:58,038] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7182
[2019-04-04 14:00:58,058] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.016666666666667, 52.83333333333334, 238.0, 155.0, 26.0, 25.75410879292788, 0.3978622046072235, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2635800.0000, 
sim time next is 2636400.0000, 
raw observation next is [-1.733333333333333, 51.66666666666667, 241.5, 151.0, 26.0, 25.77486686494223, 0.3909109622776804, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.41458910433979695, 0.5166666666666667, 0.805, 0.16685082872928178, 0.6666666666666666, 0.6479055720785191, 0.6303036540925602, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00530214], dtype=float32), -0.8180091]. 
=============================================
[2019-04-04 14:01:00,187] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.9791713e-09 8.5573471e-10 3.0682918e-14 4.2819118e-14 1.0000000e+00
 2.4916555e-10 1.5268938e-14], sum to 1.0000
[2019-04-04 14:01:00,188] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5985
[2019-04-04 14:01:00,232] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 50.33333333333334, 165.1666666666667, 0.0, 26.0, 24.9880857092111, 0.2974843562146589, 0.0, 1.0, 18718.78506401748], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2384400.0000, 
sim time next is 2385000.0000, 
raw observation next is [0.0, 49.5, 160.0, 0.0, 26.0, 24.97493198294284, 0.2913658895289368, 0.0, 1.0, 24170.04241641153], 
processed observation next is [0.0, 0.6086956521739131, 0.46260387811634357, 0.495, 0.5333333333333333, 0.0, 0.6666666666666666, 0.5812443319119035, 0.5971219631763123, 0.0, 1.0, 0.11509544007815013], 
reward next is 0.8849, 
noisyNet noise sample is [array([-0.12725504], dtype=float32), -0.3512992]. 
=============================================
[2019-04-04 14:01:00,247] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[75.208015]
 [75.55496 ]
 [75.88606 ]
 [76.210144]
 [76.588554]], R is [[75.05841827]
 [75.21870422]
 [75.37736511]
 [75.5286026 ]
 [75.62059021]].
[2019-04-04 14:01:02,362] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3213416e-07 2.0335039e-08 1.0510031e-12 8.6865602e-12 9.9999988e-01
 3.5686702e-09 1.9825016e-13], sum to 1.0000
[2019-04-04 14:01:02,365] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3785
[2019-04-04 14:01:02,387] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.46585376869224, -0.09378784246338023, 0.0, 1.0, 44418.19698059525], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2435400.0000, 
sim time next is 2436000.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.42834672533456, -0.1018728424185236, 0.0, 1.0, 44419.61312015242], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.4523622271112133, 0.4660423858604921, 0.0, 1.0, 0.21152196723882105], 
reward next is 0.7885, 
noisyNet noise sample is [array([-0.00833053], dtype=float32), 1.6176347]. 
=============================================
[2019-04-04 14:01:02,401] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[70.47761]
 [70.52547]
 [70.57982]
 [70.60916]
 [70.61578]], R is [[70.51269531]
 [70.59605408]
 [70.6786499 ]
 [70.76057434]
 [70.84190369]].
[2019-04-04 14:01:05,837] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2779284e-09 1.2091587e-10 2.6249183e-15 3.2576433e-14 1.0000000e+00
 1.4062060e-10 6.8950074e-15], sum to 1.0000
[2019-04-04 14:01:05,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1252
[2019-04-04 14:01:05,867] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.666666666666667, 62.33333333333333, 0.0, 0.0, 26.0, 25.2660438632528, 0.4543939147440315, 0.0, 1.0, 74458.39469174301], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2752800.0000, 
sim time next is 2753400.0000, 
raw observation next is [-5.833333333333333, 63.16666666666666, 0.0, 0.0, 26.0, 25.35405194355583, 0.4592018718693603, 0.0, 1.0, 59921.58703620604], 
processed observation next is [1.0, 0.8695652173913043, 0.30101569713758086, 0.6316666666666666, 0.0, 0.0, 0.6666666666666666, 0.612837661962986, 0.6530672906231201, 0.0, 1.0, 0.2853408906486002], 
reward next is 0.7147, 
noisyNet noise sample is [array([-0.17363316], dtype=float32), -0.33028698]. 
=============================================
[2019-04-04 14:01:07,295] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.3764533e-09 9.8765918e-10 2.3133042e-14 1.0422272e-13 1.0000000e+00
 1.0083692e-09 4.3266548e-14], sum to 1.0000
[2019-04-04 14:01:07,296] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5808
[2019-04-04 14:01:07,320] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 39.0, 0.0, 0.0, 26.0, 24.98778296993105, 0.2025172209535462, 0.0, 1.0, 39054.84183120468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2511000.0000, 
sim time next is 2511600.0000, 
raw observation next is [-1.7, 38.66666666666667, 0.0, 0.0, 26.0, 25.0228578503729, 0.2040245141752458, 0.0, 1.0, 38947.98409764707], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.3866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5852381541977417, 0.5680081713917486, 0.0, 1.0, 0.18546659094117654], 
reward next is 0.8145, 
noisyNet noise sample is [array([0.49442264], dtype=float32), 0.8262137]. 
=============================================
[2019-04-04 14:01:09,774] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3616014e-09 4.9346682e-10 9.6419997e-15 6.3680066e-14 1.0000000e+00
 2.3057543e-10 1.8491235e-15], sum to 1.0000
[2019-04-04 14:01:09,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7361
[2019-04-04 14:01:09,787] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.8, 79.66666666666667, 0.0, 0.0, 26.0, 24.36652836811835, 0.1309492584157022, 0.0, 1.0, 42645.60415755651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2607600.0000, 
sim time next is 2608200.0000, 
raw observation next is [-5.9, 80.5, 0.0, 0.0, 26.0, 24.36932788260516, 0.132629567513669, 0.0, 1.0, 42725.53684895711], 
processed observation next is [1.0, 0.17391304347826086, 0.2991689750692521, 0.805, 0.0, 0.0, 0.6666666666666666, 0.53077732355043, 0.5442098558378897, 0.0, 1.0, 0.20345493737598624], 
reward next is 0.7965, 
noisyNet noise sample is [array([1.1341708], dtype=float32), -0.6536227]. 
=============================================
[2019-04-04 14:01:26,806] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2636697e-08 9.6813502e-10 1.4405996e-14 7.5529982e-14 1.0000000e+00
 1.6258919e-09 2.1978255e-14], sum to 1.0000
[2019-04-04 14:01:26,806] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2399
[2019-04-04 14:01:26,818] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.38448137357232, 0.4321434539896289, 0.0, 1.0, 42575.65767306398], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2841600.0000, 
sim time next is 2842200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.43974332867863, 0.4360251709378224, 0.0, 1.0, 18761.70884112401], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.6199786107232192, 0.6453417236459408, 0.0, 1.0, 0.0893414706720191], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.07419329], dtype=float32), 0.53061175]. 
=============================================
[2019-04-04 14:01:27,350] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3594840e-10 1.6098994e-11 1.5616434e-17 6.0232996e-15 1.0000000e+00
 9.4438060e-12 2.1315284e-16], sum to 1.0000
[2019-04-04 14:01:27,365] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2081
[2019-04-04 14:01:27,402] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.166666666666667, 93.0, 86.33333333333334, 104.0, 26.0, 25.38220853070371, 0.316757872539834, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2883000.0000, 
sim time next is 2883600.0000, 
raw observation next is [1.0, 93.0, 77.0, 78.0, 26.0, 25.34198087030527, 0.3164297380402726, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4903047091412743, 0.93, 0.25666666666666665, 0.0861878453038674, 0.6666666666666666, 0.6118317391921059, 0.6054765793467575, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.91107625], dtype=float32), 1.1857561]. 
=============================================
[2019-04-04 14:01:33,906] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.8574154e-09 1.9839083e-10 6.9857907e-15 6.6361555e-14 1.0000000e+00
 9.3259567e-11 7.0284249e-15], sum to 1.0000
[2019-04-04 14:01:33,909] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9707
[2019-04-04 14:01:33,947] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.833333333333333, 78.16666666666667, 0.0, 0.0, 26.0, 24.20476849238491, 0.1644904449567867, 0.0, 1.0, 42637.41213110021], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2958600.0000, 
sim time next is 2959200.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 24.17075815815873, 0.1566572547741946, 0.0, 1.0, 42624.94234754599], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5142298465132275, 0.5522190849247316, 0.0, 1.0, 0.2029759159406952], 
reward next is 0.7970, 
noisyNet noise sample is [array([0.88186675], dtype=float32), 0.9786047]. 
=============================================
[2019-04-04 14:01:34,404] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.96934913e-09 5.30489888e-11 5.53760759e-16 1.18343284e-14
 1.00000000e+00 1.10311663e-10 1.65569120e-16], sum to 1.0000
[2019-04-04 14:01:34,405] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2533
[2019-04-04 14:01:34,427] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 76.0, 0.0, 0.0, 26.0, 25.44307187369419, 0.5609389308792617, 0.0, 1.0, 134383.1686525218], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3270600.0000, 
sim time next is 3271200.0000, 
raw observation next is [-4.666666666666666, 77.66666666666667, 0.0, 0.0, 26.0, 25.43694424102224, 0.57649970050355, 0.0, 1.0, 81517.70764344954], 
processed observation next is [1.0, 0.8695652173913043, 0.33333333333333337, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6197453534185199, 0.6921665668345166, 0.0, 1.0, 0.3881795602069026], 
reward next is 0.6118, 
noisyNet noise sample is [array([0.9767941], dtype=float32), 0.3385395]. 
=============================================
[2019-04-04 14:01:35,128] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.6365270e-09 6.3540667e-10 9.7307543e-15 8.6417351e-14 1.0000000e+00
 1.3512831e-10 4.6215727e-15], sum to 1.0000
[2019-04-04 14:01:35,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1316
[2019-04-04 14:01:35,198] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.833333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 25.31563443838453, 0.3785604459087589, 0.0, 1.0, 47608.1251173269], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3009000.0000, 
sim time next is 3009600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.36964288905974, 0.3793982508717016, 0.0, 1.0, 42947.54539133072], 
processed observation next is [0.0, 0.8695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6141369074216451, 0.6264660836239005, 0.0, 1.0, 0.20451212091109866], 
reward next is 0.7955, 
noisyNet noise sample is [array([-0.5122414], dtype=float32), -0.2962726]. 
=============================================
[2019-04-04 14:01:35,957] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0675807e-08 1.8566063e-10 2.2017097e-15 7.8353516e-14 1.0000000e+00
 2.0396783e-10 1.3225832e-15], sum to 1.0000
[2019-04-04 14:01:35,959] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0127
[2019-04-04 14:01:35,991] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.583333333333333, 65.0, 0.0, 0.0, 26.0, 25.2947108574301, 0.351665054067327, 0.0, 1.0, 39700.94489978601], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3013800.0000, 
sim time next is 3014400.0000, 
raw observation next is [-3.666666666666667, 65.0, 0.0, 0.0, 26.0, 25.25576569219465, 0.3442370683313176, 0.0, 1.0, 39495.32763583213], 
processed observation next is [0.0, 0.9130434782608695, 0.3610341643582641, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6046471410162209, 0.6147456894437725, 0.0, 1.0, 0.18807298874205777], 
reward next is 0.8119, 
noisyNet noise sample is [array([-0.748589], dtype=float32), -0.20473108]. 
=============================================
[2019-04-04 14:01:39,420] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.15163059e-09 4.84527307e-10 1.02082903e-14 2.71664015e-14
 1.00000000e+00 1.13309515e-10 1.03131106e-14], sum to 1.0000
[2019-04-04 14:01:39,425] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0075
[2019-04-04 14:01:39,447] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 39.5, 84.0, 673.0, 26.0, 25.12395478879895, 0.3661316267012695, 0.0, 1.0, 18696.50652402029], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3079800.0000, 
sim time next is 3080400.0000, 
raw observation next is [0.6666666666666666, 39.66666666666666, 79.5, 641.8333333333334, 26.0, 25.14455956200775, 0.3744789228370023, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.4810710987996307, 0.39666666666666656, 0.265, 0.7092081031307551, 0.6666666666666666, 0.5953799635006458, 0.624826307612334, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04399012], dtype=float32), 0.36906636]. 
=============================================
[2019-04-04 14:01:39,902] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9426778e-09 9.7703179e-10 1.6565078e-14 2.0232350e-13 1.0000000e+00
 1.3386932e-09 7.9785753e-14], sum to 1.0000
[2019-04-04 14:01:39,902] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3390
[2019-04-04 14:01:39,944] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6666666666666666, 83.66666666666667, 0.0, 0.0, 26.0, 24.98965336440444, 0.2849679159105007, 0.0, 1.0, 42175.39906365092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3089400.0000, 
sim time next is 3090000.0000, 
raw observation next is [-0.7333333333333334, 85.33333333333334, 0.0, 0.0, 26.0, 24.99513074219652, 0.2847377348755945, 0.0, 1.0, 34589.24043558438], 
processed observation next is [0.0, 0.782608695652174, 0.44228993536472766, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.58292756184971, 0.5949125782918648, 0.0, 1.0, 0.164710668740878], 
reward next is 0.8353, 
noisyNet noise sample is [array([0.7092164], dtype=float32), -0.07077064]. 
=============================================
[2019-04-04 14:01:39,959] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[75.37875 ]
 [74.80509 ]
 [74.278625]
 [73.94571 ]
 [73.762825]], R is [[76.02568054]
 [76.06459045]
 [76.06420898]
 [76.04930115]
 [76.05997467]].
[2019-04-04 14:01:42,683] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0872667e-09 9.2039723e-11 3.0592077e-16 6.0258275e-15 1.0000000e+00
 2.0937696e-11 7.8700171e-16], sum to 1.0000
[2019-04-04 14:01:42,686] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5223
[2019-04-04 14:01:42,705] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.24108715471849, 0.3795564095029905, 0.0, 1.0, 41593.40187038895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3480000.0000, 
sim time next is 3480600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.25682744824891, 0.364492334232384, 0.0, 1.0, 45832.11214013743], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6047356206874092, 0.621497444744128, 0.0, 1.0, 0.2182481530482735], 
reward next is 0.7818, 
noisyNet noise sample is [array([0.7658505], dtype=float32), -1.3142947]. 
=============================================
[2019-04-04 14:01:42,747] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2343174e-09 2.5280084e-10 2.2485629e-15 1.9250640e-14 1.0000000e+00
 2.7654543e-10 1.9056244e-15], sum to 1.0000
[2019-04-04 14:01:42,747] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6935
[2019-04-04 14:01:42,754] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.833333333333333, 69.0, 0.0, 0.0, 26.0, 25.36164299137058, 0.4461176489254406, 1.0, 1.0, 97268.85813151131], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3435000.0000, 
sim time next is 3435600.0000, 
raw observation next is [1.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.26278594934304, 0.4394506425340404, 1.0, 1.0, 53823.60447999981], 
processed observation next is [1.0, 0.782608695652174, 0.5087719298245615, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6052321624452534, 0.6464835475113468, 1.0, 1.0, 0.2563028784761896], 
reward next is 0.7437, 
noisyNet noise sample is [array([1.135972], dtype=float32), 0.048038978]. 
=============================================
[2019-04-04 14:01:43,157] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.0221724e-10 3.2562446e-11 2.6102586e-16 6.1291321e-15 1.0000000e+00
 4.6834415e-11 6.8620209e-16], sum to 1.0000
[2019-04-04 14:01:43,160] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6761
[2019-04-04 14:01:43,180] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.32053316015904, 0.3659675826460044, 0.0, 1.0, 50356.1177763095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3095400.0000, 
sim time next is 3096000.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.47427602433219, 0.3717586584951908, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6228563353610159, 0.6239195528317303, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18201402], dtype=float32), 2.1350296]. 
=============================================
[2019-04-04 14:01:43,185] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[84.24878 ]
 [83.53386 ]
 [82.65755 ]
 [81.441185]
 [80.05225 ]], R is [[84.79127502]
 [84.7035675 ]
 [84.52829742]
 [84.10018921]
 [83.30812836]].
[2019-04-04 14:01:43,745] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0120931e-10 2.8195488e-12 4.6563204e-18 1.3247841e-16 1.0000000e+00
 8.3581151e-13 6.3749325e-18], sum to 1.0000
[2019-04-04 14:01:43,746] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8638
[2019-04-04 14:01:43,776] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 92.0, 90.33333333333333, 464.3333333333333, 26.0, 25.90801138533077, 0.5948446471969432, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3228600.0000, 
sim time next is 3229200.0000, 
raw observation next is [-3.0, 92.0, 93.0, 511.5, 26.0, 26.00490193698459, 0.6065342366290055, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.31, 0.5651933701657459, 0.6666666666666666, 0.6670751614153826, 0.7021780788763352, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01501085], dtype=float32), 0.3503558]. 
=============================================
[2019-04-04 14:01:44,340] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5350526e-10 1.6861103e-12 2.0580260e-17 7.5283203e-17 1.0000000e+00
 1.2866261e-12 6.3402761e-18], sum to 1.0000
[2019-04-04 14:01:44,342] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4608
[2019-04-04 14:01:44,356] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 95.33333333333334, 0.0, 0.0, 26.0, 25.40295647485065, 0.5980507637520331, 0.0, 1.0, 186166.4554376069], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3192000.0000, 
sim time next is 3192600.0000, 
raw observation next is [2.0, 94.16666666666666, 0.0, 0.0, 26.0, 25.37511262982442, 0.6185558689948861, 0.0, 1.0, 118022.447802358], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.9416666666666665, 0.0, 0.0, 0.6666666666666666, 0.6145927191520352, 0.706185289664962, 0.0, 1.0, 0.5620116562017048], 
reward next is 0.4380, 
noisyNet noise sample is [array([0.61895555], dtype=float32), -1.0883381]. 
=============================================
[2019-04-04 14:01:55,722] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.0740109e-09 6.8315620e-10 2.1743797e-14 4.4787091e-14 1.0000000e+00
 4.2441275e-10 6.4534352e-15], sum to 1.0000
[2019-04-04 14:01:55,726] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3131
[2019-04-04 14:01:55,757] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.45717833224691, 0.5416062476856945, 1.0, 1.0, 46886.88281068599], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3348000.0000, 
sim time next is 3348600.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.68783996256776, 0.5681721895606836, 1.0, 1.0, 31986.62617014852], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6406533302139801, 0.6893907298535612, 1.0, 1.0, 0.15231726747689772], 
reward next is 0.8477, 
noisyNet noise sample is [array([-0.92979145], dtype=float32), -0.8636305]. 
=============================================
[2019-04-04 14:01:55,908] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1871531e-08 8.2789553e-10 7.4250535e-15 1.3053572e-13 1.0000000e+00
 5.2329074e-10 7.5942941e-15], sum to 1.0000
[2019-04-04 14:01:55,929] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8882
[2019-04-04 14:01:55,955] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 65.5, 0.0, 0.0, 26.0, 24.65512747211622, 0.2428287836980529, 0.0, 1.0, 42892.5875737943], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3389400.0000, 
sim time next is 3390000.0000, 
raw observation next is [-3.666666666666667, 63.66666666666667, 0.0, 0.0, 26.0, 24.79516633080456, 0.243341446881217, 0.0, 1.0, 42747.96900499349], 
processed observation next is [1.0, 0.21739130434782608, 0.3610341643582641, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.56626386090038, 0.5811138156270723, 0.0, 1.0, 0.20356175716663566], 
reward next is 0.7964, 
noisyNet noise sample is [array([-0.6215969], dtype=float32), 0.61888546]. 
=============================================
[2019-04-04 14:01:55,971] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.946045]
 [81.04709 ]
 [81.158264]
 [81.22552 ]
 [81.27246 ]], R is [[80.83940125]
 [80.82675171]
 [80.81532288]
 [80.80368805]
 [80.79428864]].
[2019-04-04 14:01:58,123] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8996051e-09 5.8825733e-10 2.2321551e-14 2.4917931e-14 1.0000000e+00
 1.7799147e-10 6.3089936e-15], sum to 1.0000
[2019-04-04 14:01:58,124] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5195
[2019-04-04 14:01:58,132] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 49.0, 115.0, 811.5, 26.0, 25.77438525763017, 0.5224423693518297, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3416400.0000, 
sim time next is 3417000.0000, 
raw observation next is [3.0, 49.0, 114.3333333333333, 809.6666666666666, 26.0, 25.85676745056856, 0.5256578404958036, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.381111111111111, 0.8946593001841621, 0.6666666666666666, 0.6547306208807134, 0.6752192801652678, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.511048], dtype=float32), 0.7046891]. 
=============================================
[2019-04-04 14:01:58,143] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[84.63245 ]
 [84.702965]
 [84.82795 ]
 [84.90015 ]
 [84.71463 ]], R is [[84.68039703]
 [84.83359528]
 [84.98526001]
 [85.13540649]
 [84.85228729]].
[2019-04-04 14:02:04,938] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 14:02:04,939] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 14:02:04,939] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:02:04,939] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 14:02:04,939] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 14:02:04,940] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:02:04,940] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:02:05,064] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run21
[2019-04-04 14:02:05,111] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run21
[2019-04-04 14:02:05,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run21
[2019-04-04 14:03:10,700] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1688055], dtype=float32), 0.20209664]
[2019-04-04 14:03:10,700] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.2, 67.0, 0.0, 0.0, 26.0, 24.95395936422239, 0.3884901830658674, 0.0, 1.0, 37382.96744646758]
[2019-04-04 14:03:10,701] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 14:03:10,702] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.9073468e-09 2.8814978e-10 3.8527285e-15 5.3024832e-14 1.0000000e+00
 2.4743646e-10 5.9546022e-15], sampled 0.2394529729041205
[2019-04-04 14:03:45,774] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1688055], dtype=float32), 0.20209664]
[2019-04-04 14:03:45,774] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.250227375666666, 84.75830750666668, 0.0, 0.0, 26.0, 24.576692704821, 0.226867429331719, 0.0, 1.0, 43711.51877541332]
[2019-04-04 14:03:45,775] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 14:03:45,776] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.0357122e-09 4.4034915e-10 3.4900900e-15 5.2430269e-14 1.0000000e+00
 1.1869754e-10 6.2607801e-15], sampled 0.7843349881888245
[2019-04-04 14:03:47,506] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 14:03:49,912] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.1688055], dtype=float32), 0.20209664]
[2019-04-04 14:03:49,912] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.0, 35.5, 23.0, 57.0, 26.0, 26.76008341885562, 0.6217365269497279, 1.0, 1.0, 0.0]
[2019-04-04 14:03:49,912] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 14:03:49,913] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.0040891e-09 6.4369293e-10 1.5690682e-14 7.1194746e-14 1.0000000e+00
 5.1484778e-10 1.9820131e-14], sampled 0.18398825718791922
[2019-04-04 14:04:05,985] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.8553 263370387.4884 1552.0634
[2019-04-04 14:04:09,677] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7307 275786560.0346 1233.1784
[2019-04-04 14:04:10,700] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2000000, evaluation results [2000000.0, 7241.855297674469, 263370387.48835877, 1552.0634488189448, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.730666501954, 275786560.0345853, 1233.1784155230887]
[2019-04-04 14:04:15,997] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5632996e-08 1.1309603e-09 1.5968332e-13 4.0622551e-13 1.0000000e+00
 7.9973012e-10 3.6230489e-14], sum to 1.0000
[2019-04-04 14:04:15,998] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0647
[2019-04-04 14:04:16,017] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.833333333333333, 48.66666666666667, 0.0, 0.0, 26.0, 25.39082604083899, 0.3933564594675532, 0.0, 1.0, 48811.06786709772], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3621000.0000, 
sim time next is 3621600.0000, 
raw observation next is [-2.0, 50.0, 0.0, 0.0, 26.0, 25.41404762913723, 0.3903356255396263, 0.0, 1.0, 29576.07260678842], 
processed observation next is [0.0, 0.9565217391304348, 0.40720221606648205, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6178373024281024, 0.6301118751798754, 0.0, 1.0, 0.14083844098470677], 
reward next is 0.8592, 
noisyNet noise sample is [array([-1.766595], dtype=float32), 1.0220783]. 
=============================================
[2019-04-04 14:04:20,889] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1744548e-09 4.4864557e-10 1.4605747e-15 4.4102739e-14 1.0000000e+00
 7.6303464e-11 6.0897423e-16], sum to 1.0000
[2019-04-04 14:04:20,891] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8887
[2019-04-04 14:04:20,918] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.12611084809078, 0.3484268146778178, 0.0, 1.0, 46257.0685683475], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3812400.0000, 
sim time next is 3813000.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.11805428492085, 0.3397329588196894, 0.0, 1.0, 44368.07542743854], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5931711904100707, 0.6132443196065631, 0.0, 1.0, 0.21127654965446924], 
reward next is 0.7887, 
noisyNet noise sample is [array([-0.36507776], dtype=float32), 1.6596518]. 
=============================================
[2019-04-04 14:04:20,935] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[85.10138]
 [85.12314]
 [85.2551 ]
 [85.09634]
 [84.84975]], R is [[85.00361633]
 [84.93331146]
 [84.8388443 ]
 [84.69774628]
 [84.58461761]].
[2019-04-04 14:04:24,384] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8091232e-09 1.5689008e-10 1.3302749e-14 1.3998989e-13 1.0000000e+00
 1.6461528e-09 2.7344333e-15], sum to 1.0000
[2019-04-04 14:04:24,384] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5076
[2019-04-04 14:04:24,406] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 37.16666666666666, 0.0, 0.0, 26.0, 25.47298311021645, 0.5703616190214101, 0.0, 1.0, 44474.78444768094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4135800.0000, 
sim time next is 4136400.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 26.0, 25.71966180432339, 0.5897693970202159, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6433051503602826, 0.6965897990067386, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0628475], dtype=float32), -0.3532635]. 
=============================================
[2019-04-04 14:04:24,622] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0637899e-08 4.9761517e-10 1.2250994e-15 2.4215204e-15 1.0000000e+00
 8.8933243e-11 2.5995577e-15], sum to 1.0000
[2019-04-04 14:04:24,623] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4027
[2019-04-04 14:04:24,656] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 43.66666666666667, 67.83333333333333, 578.6666666666666, 26.0, 26.92834833967888, 0.6413565629893527, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3860400.0000, 
sim time next is 3861000.0000, 
raw observation next is [3.0, 43.0, 64.0, 551.0, 26.0, 26.40236943220677, 0.678832453068794, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.43, 0.21333333333333335, 0.6088397790055249, 0.6666666666666666, 0.7001974526838973, 0.7262774843562646, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7940989], dtype=float32), 0.9904618]. 
=============================================
[2019-04-04 14:04:24,683] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.752396]
 [84.98457 ]
 [85.12767 ]
 [85.21607 ]
 [85.37327 ]], R is [[84.61808014]
 [84.77189636]
 [84.92417908]
 [85.07493591]
 [85.22418976]].
[2019-04-04 14:04:25,944] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3508155e-09 2.5767677e-10 5.3309154e-15 4.6113432e-14 1.0000000e+00
 6.0715288e-10 1.3803889e-14], sum to 1.0000
[2019-04-04 14:04:25,944] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9132
[2019-04-04 14:04:25,970] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.666666666666667, 47.66666666666667, 0.0, 0.0, 26.0, 25.42150644342807, 0.3671935199900369, 0.0, 1.0, 27294.23205276706], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4232400.0000, 
sim time next is 4233000.0000, 
raw observation next is [1.833333333333333, 47.83333333333333, 0.0, 0.0, 26.0, 25.41375980155376, 0.3641333087014311, 0.0, 1.0, 36283.46812821513], 
processed observation next is [0.0, 1.0, 0.5133887349953832, 0.4783333333333333, 0.0, 0.0, 0.6666666666666666, 0.6178133167961466, 0.6213777695671437, 0.0, 1.0, 0.1727784196581673], 
reward next is 0.8272, 
noisyNet noise sample is [array([0.39244315], dtype=float32), 0.83454126]. 
=============================================
[2019-04-04 14:04:25,973] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[78.25954]
 [78.24126]
 [78.15308]
 [78.04854]
 [77.89833]], R is [[78.32965851]
 [78.41638947]
 [78.44039917]
 [78.45014954]
 [78.44837952]].
[2019-04-04 14:04:32,949] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5840673e-08 4.1336072e-09 1.4777703e-13 1.2293607e-13 9.9999988e-01
 6.6829533e-09 6.4181584e-14], sum to 1.0000
[2019-04-04 14:04:32,949] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0689
[2019-04-04 14:04:32,960] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.833333333333334, 30.66666666666667, 0.0, 0.0, 26.0, 25.4833671865053, 0.4828031997091805, 0.0, 1.0, 74903.6534743932], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4053000.0000, 
sim time next is 4053600.0000, 
raw observation next is [-5.0, 31.0, 0.0, 0.0, 26.0, 25.46098558328369, 0.4853110379540038, 0.0, 1.0, 64010.81647600716], 
processed observation next is [1.0, 0.9565217391304348, 0.32409972299168976, 0.31, 0.0, 0.0, 0.6666666666666666, 0.6217487986069742, 0.6617703459846679, 0.0, 1.0, 0.3048134117905103], 
reward next is 0.6952, 
noisyNet noise sample is [array([-1.5923853], dtype=float32), 0.8637695]. 
=============================================
[2019-04-04 14:04:35,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7006468e-09 1.2370212e-10 4.0488840e-15 2.6451236e-14 1.0000000e+00
 3.8940215e-10 6.5157217e-15], sum to 1.0000
[2019-04-04 14:04:35,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9255
[2019-04-04 14:04:35,589] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 35.5, 23.0, 57.0, 26.0, 26.63506159990285, 0.6271666531953336, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4123800.0000, 
sim time next is 4124400.0000, 
raw observation next is [3.0, 35.0, 19.16666666666667, 47.5, 26.0, 26.50184801131321, 0.6801042521747661, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.35, 0.0638888888888889, 0.052486187845303865, 0.6666666666666666, 0.7084873342761009, 0.7267014173915887, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5323727], dtype=float32), -1.9962395]. 
=============================================
[2019-04-04 14:04:40,538] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.3763368e-10 9.8297911e-11 3.3383949e-15 2.4964312e-14 1.0000000e+00
 6.5761639e-11 2.1028423e-15], sum to 1.0000
[2019-04-04 14:04:40,540] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8117
[2019-04-04 14:04:40,552] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 56.16666666666666, 0.0, 0.0, 26.0, 25.27565681799963, 0.52962627724209, 0.0, 1.0, 61935.75624646682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4567800.0000, 
sim time next is 4568400.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 26.0, 25.42363258466178, 0.5518340340094146, 0.0, 1.0, 18763.03971510121], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6186360487218151, 0.6839446780031381, 0.0, 1.0, 0.08934780816714863], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.24862441], dtype=float32), -0.32167307]. 
=============================================
[2019-04-04 14:04:41,498] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1452203e-08 9.9113906e-10 3.6450769e-14 5.8713851e-14 1.0000000e+00
 7.4814527e-10 3.9253780e-14], sum to 1.0000
[2019-04-04 14:04:41,498] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5963
[2019-04-04 14:04:41,513] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 42.66666666666667, 182.5, 163.8333333333333, 26.0, 25.02764730633144, 0.3631125042790552, 0.0, 1.0, 55238.93127949699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4200000.0000, 
sim time next is 4200600.0000, 
raw observation next is [2.0, 43.33333333333334, 178.0, 238.6666666666666, 26.0, 24.98536623119227, 0.3733159513104011, 0.0, 1.0, 57375.59758415996], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.4333333333333334, 0.5933333333333334, 0.26372007366482497, 0.6666666666666666, 0.5821138525993558, 0.6244386504368004, 0.0, 1.0, 0.27321713135314263], 
reward next is 0.7268, 
noisyNet noise sample is [array([-0.38157877], dtype=float32), 0.8725128]. 
=============================================
[2019-04-04 14:04:55,631] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6278219e-09 1.4314706e-10 3.7332077e-16 1.2788445e-14 1.0000000e+00
 1.0571846e-10 4.6801699e-16], sum to 1.0000
[2019-04-04 14:04:55,631] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0190
[2019-04-04 14:04:55,657] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5833333333333333, 73.0, 0.0, 0.0, 26.0, 25.54287796509252, 0.4891181752416459, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4495800.0000, 
sim time next is 4496400.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.58525910557882, 0.4842134121350963, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6321049254649017, 0.6614044707116987, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9817982], dtype=float32), -0.84268063]. 
=============================================
[2019-04-04 14:04:56,880] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.9910160e-09 3.4310715e-10 6.4372814e-15 5.0896119e-14 1.0000000e+00
 3.5902135e-11 5.6598343e-15], sum to 1.0000
[2019-04-04 14:04:56,881] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4078
[2019-04-04 14:04:56,930] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 24.9835160789254, 0.4085711170836073, 1.0, 1.0, 56537.67331763697], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4563600.0000, 
sim time next is 4564200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 24.95916887256214, 0.4152746666732368, 0.0, 1.0, 52318.2285178832], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5799307393801785, 0.638424888891079, 0.0, 1.0, 0.24913442151372953], 
reward next is 0.7509, 
noisyNet noise sample is [array([0.62843275], dtype=float32), -0.7519912]. 
=============================================
[2019-04-04 14:04:57,044] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.0621342e-09 8.7407676e-10 2.7453674e-14 2.0051862e-13 1.0000000e+00
 3.4900322e-10 1.4083452e-14], sum to 1.0000
[2019-04-04 14:04:57,048] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1624
[2019-04-04 14:04:57,090] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 44.33333333333334, 0.0, 0.0, 26.0, 24.98782354921254, 0.3462395876146511, 0.0, 1.0, 198825.9572201984], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4821600.0000, 
sim time next is 4822200.0000, 
raw observation next is [1.0, 45.0, 0.0, 0.0, 26.0, 25.02368101859592, 0.3753852027088445, 0.0, 1.0, 164404.3176276435], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.45, 0.0, 0.0, 0.6666666666666666, 0.5853067515496599, 0.6251284009029482, 0.0, 1.0, 0.7828777029887786], 
reward next is 0.2171, 
noisyNet noise sample is [array([0.11031465], dtype=float32), -0.097459346]. 
=============================================
[2019-04-04 14:04:58,518] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.2075414e-10 3.0944752e-10 3.8681745e-16 6.4499403e-15 1.0000000e+00
 2.8432364e-11 1.3618053e-15], sum to 1.0000
[2019-04-04 14:04:58,519] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0719
[2019-04-04 14:04:58,531] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.366666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 25.47452706292945, 0.4242205780399735, 0.0, 1.0, 18758.56329306053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4592400.0000, 
sim time next is 4593000.0000, 
raw observation next is [-1.433333333333333, 68.66666666666666, 0.0, 0.0, 26.0, 25.46425954592219, 0.4143688397905742, 0.0, 1.0, 20597.59299952726], 
processed observation next is [1.0, 0.13043478260869565, 0.4228993536472761, 0.6866666666666665, 0.0, 0.0, 0.6666666666666666, 0.6220216288268491, 0.6381229465968581, 0.0, 1.0, 0.09808377618822504], 
reward next is 0.9019, 
noisyNet noise sample is [array([-0.07563507], dtype=float32), -0.2145516]. 
=============================================
[2019-04-04 14:04:58,543] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[84.37272 ]
 [84.467545]
 [84.621475]
 [84.63382 ]
 [84.50526 ]], R is [[84.3411026 ]
 [84.40837097]
 [84.56428528]
 [84.58943176]
 [84.50885773]].
[2019-04-04 14:05:02,744] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0702139e-09 1.5751284e-10 1.5943178e-15 2.9502296e-14 1.0000000e+00
 2.5763647e-10 5.3210614e-15], sum to 1.0000
[2019-04-04 14:05:02,745] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4579
[2019-04-04 14:05:02,758] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.933333333333333, 76.5, 0.0, 0.0, 26.0, 25.03352507864113, 0.3385662729295489, 0.0, 1.0, 36229.21310989136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4603800.0000, 
sim time next is 4604400.0000, 
raw observation next is [-3.0, 77.0, 0.0, 0.0, 26.0, 25.0343631796711, 0.3306908077493043, 0.0, 1.0, 36228.07150904273], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5861969316392583, 0.6102302692497681, 0.0, 1.0, 0.17251462623353683], 
reward next is 0.8275, 
noisyNet noise sample is [array([-0.3197534], dtype=float32), -0.10720028]. 
=============================================
[2019-04-04 14:05:03,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.1156221e-11 1.5414774e-11 3.9257293e-18 8.6904045e-16 1.0000000e+00
 7.7376820e-12 8.3172713e-18], sum to 1.0000
[2019-04-04 14:05:03,731] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6951
[2019-04-04 14:05:03,741] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 92.0, 208.8333333333333, 6.0, 26.0, 26.46950636015861, 0.5966741291643781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4704000.0000, 
sim time next is 4704600.0000, 
raw observation next is [0.0, 92.0, 209.6666666666667, 6.0, 26.0, 26.47835166792327, 0.597564631707998, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.92, 0.698888888888889, 0.0066298342541436465, 0.6666666666666666, 0.7065293056602725, 0.6991882105693327, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6137946], dtype=float32), 0.11483001]. 
=============================================
[2019-04-04 14:05:04,276] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2574084e-09 1.3229643e-11 4.1591991e-17 1.6637899e-15 1.0000000e+00
 1.1882795e-11 6.6682973e-17], sum to 1.0000
[2019-04-04 14:05:04,278] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7824
[2019-04-04 14:05:04,313] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 67.33333333333334, 157.1666666666667, 452.6666666666667, 26.0, 26.07036742996046, 0.5386747537588651, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4612800.0000, 
sim time next is 4613400.0000, 
raw observation next is [-1.0, 65.5, 164.0, 509.0, 26.0, 26.10788362214141, 0.5696095661545021, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.655, 0.5466666666666666, 0.5624309392265193, 0.6666666666666666, 0.6756569685117843, 0.689869855384834, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1032894], dtype=float32), 0.5082277]. 
=============================================
[2019-04-04 14:05:07,985] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.21840804e-10 1.39257685e-11 3.06812307e-17 1.15797768e-15
 1.00000000e+00 1.82706853e-11 8.14894069e-17], sum to 1.0000
[2019-04-04 14:05:07,985] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7790
[2019-04-04 14:05:08,004] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 79.33333333333333, 0.0, 0.0, 26.0, 25.27126788405979, 0.442724950837782, 0.0, 1.0, 48763.68416587656], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4747200.0000, 
sim time next is 4747800.0000, 
raw observation next is [-3.0, 78.16666666666667, 0.0, 0.0, 26.0, 25.23542472082837, 0.4362140308635721, 0.0, 1.0, 44497.17909562677], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.7816666666666667, 0.0, 0.0, 0.6666666666666666, 0.6029520600690308, 0.645404676954524, 0.0, 1.0, 0.21189132902679417], 
reward next is 0.7881, 
noisyNet noise sample is [array([-2.0228028], dtype=float32), 1.5370877]. 
=============================================
[2019-04-04 14:05:08,200] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:08,200] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:08,234] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run16
[2019-04-04 14:05:08,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7760963e-09 5.5201163e-11 1.9040103e-16 5.3204935e-15 1.0000000e+00
 5.0164310e-11 1.1672023e-15], sum to 1.0000
[2019-04-04 14:05:08,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9280
[2019-04-04 14:05:08,871] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.666666666666667, 36.83333333333333, 118.3333333333333, 821.0, 26.0, 27.40088727361006, 0.5883582008384368, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5050200.0000, 
sim time next is 5050800.0000, 
raw observation next is [5.0, 36.0, 119.5, 827.0, 26.0, 27.31158912825403, 0.8135071036138046, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6011080332409973, 0.36, 0.3983333333333333, 0.9138121546961326, 0.6666666666666666, 0.775965760687836, 0.7711690345379348, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5226954], dtype=float32), 0.08442129]. 
=============================================
[2019-04-04 14:05:11,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:11,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:11,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run16
[2019-04-04 14:05:13,302] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:13,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:13,522] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run16
[2019-04-04 14:05:16,311] A3C_AGENT_WORKER-Thread-13 INFO:Local step 127500, global step 2034796: loss 0.9509
[2019-04-04 14:05:16,313] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 127500, global step 2034796: learning rate 0.0000
[2019-04-04 14:05:19,700] A3C_AGENT_WORKER-Thread-17 INFO:Local step 127500, global step 2036446: loss 0.9438
[2019-04-04 14:05:19,700] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 127500, global step 2036446: learning rate 0.0000
[2019-04-04 14:05:21,493] A3C_AGENT_WORKER-Thread-2 INFO:Local step 127500, global step 2037454: loss 0.9483
[2019-04-04 14:05:21,494] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 127500, global step 2037455: learning rate 0.0000
[2019-04-04 14:05:23,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:23,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:23,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run16
[2019-04-04 14:05:23,085] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3202402e-08 1.1253591e-09 9.4670924e-15 2.0223360e-13 1.0000000e+00
 8.5504781e-10 5.6127188e-14], sum to 1.0000
[2019-04-04 14:05:23,085] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5660
[2019-04-04 14:05:23,103] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 47.5, 0.0, 0.0, 26.0, 25.55146121388911, 0.444285202140067, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5020200.0000, 
sim time next is 5020800.0000, 
raw observation next is [-0.3333333333333333, 50.0, 0.0, 0.0, 26.0, 25.58805845837018, 0.4273794791110769, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4533702677747, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6323382048641818, 0.6424598263703589, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4426726], dtype=float32), -1.1089561]. 
=============================================
[2019-04-04 14:05:23,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:23,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:23,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run16
[2019-04-04 14:05:24,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:24,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:24,008] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run16
[2019-04-04 14:05:24,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:24,482] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:24,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run16
[2019-04-04 14:05:25,520] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:25,520] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:25,524] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run16
[2019-04-04 14:05:25,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:25,554] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:25,564] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run16
[2019-04-04 14:05:25,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:25,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:25,916] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run16
[2019-04-04 14:05:26,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:26,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:26,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run16
[2019-04-04 14:05:27,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:27,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:27,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run16
[2019-04-04 14:05:27,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:27,606] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:27,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run16
[2019-04-04 14:05:27,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:27,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:27,912] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run16
[2019-04-04 14:05:28,514] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:28,515] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:28,518] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run16
[2019-04-04 14:05:29,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:05:29,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:05:29,078] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run16
[2019-04-04 14:05:31,851] A3C_AGENT_WORKER-Thread-15 INFO:Local step 127500, global step 2040180: loss 0.9410
[2019-04-04 14:05:31,851] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 127500, global step 2040180: learning rate 0.0000
[2019-04-04 14:05:33,093] A3C_AGENT_WORKER-Thread-11 INFO:Local step 127500, global step 2040309: loss 0.9149
[2019-04-04 14:05:33,093] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 127500, global step 2040309: learning rate 0.0000
[2019-04-04 14:05:33,194] A3C_AGENT_WORKER-Thread-16 INFO:Local step 127500, global step 2040320: loss 0.9451
[2019-04-04 14:05:33,195] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 127500, global step 2040320: learning rate 0.0000
[2019-04-04 14:05:33,814] A3C_AGENT_WORKER-Thread-18 INFO:Local step 127500, global step 2040425: loss 0.9318
[2019-04-04 14:05:33,814] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 127500, global step 2040425: learning rate 0.0000
[2019-04-04 14:05:34,272] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5392997e-08 4.9693347e-09 9.1298239e-14 5.3783708e-13 1.0000000e+00
 1.8540381e-09 1.3333951e-13], sum to 1.0000
[2019-04-04 14:05:34,273] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7342
[2019-04-04 14:05:34,294] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.616666666666667, 93.5, 0.0, 0.0, 26.0, 20.83965894635776, -0.6782833097557083, 0.0, 1.0, 41458.10627770027], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 13800.0000, 
sim time next is 14400.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 20.86199839164837, -0.672482147519553, 0.0, 1.0, 41338.98636973317], 
processed observation next is [0.0, 0.17391304347826086, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.23849986597069753, 0.27583928416014897, 0.0, 1.0, 0.19685231604634842], 
reward next is 0.8031, 
noisyNet noise sample is [array([0.44248235], dtype=float32), 0.12923034]. 
=============================================
[2019-04-04 14:05:34,852] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3479844e-08 2.6909750e-09 2.2919160e-14 5.9122120e-14 1.0000000e+00
 1.3203088e-09 1.5316921e-14], sum to 1.0000
[2019-04-04 14:05:34,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7522
[2019-04-04 14:05:34,878] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.12400658655707, -0.6016644284170832, 0.0, 1.0, 40492.28819485414], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 21000.0000, 
sim time next is 21600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.13873517842831, -0.5963807140702465, 0.0, 1.0, 40454.56801124432], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.26156126486902576, 0.30120642864325115, 0.0, 1.0, 0.1926408000535444], 
reward next is 0.8074, 
noisyNet noise sample is [array([1.1177013], dtype=float32), -0.14718777]. 
=============================================
[2019-04-04 14:05:35,632] A3C_AGENT_WORKER-Thread-19 INFO:Local step 127500, global step 2040886: loss 0.9266
[2019-04-04 14:05:35,633] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 127500, global step 2040886: learning rate 0.0000
[2019-04-04 14:05:35,880] A3C_AGENT_WORKER-Thread-14 INFO:Local step 127500, global step 2040955: loss 0.9325
[2019-04-04 14:05:35,881] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 127500, global step 2040955: learning rate 0.0000
[2019-04-04 14:05:35,969] A3C_AGENT_WORKER-Thread-20 INFO:Local step 127500, global step 2040984: loss 0.9244
[2019-04-04 14:05:35,971] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 127500, global step 2040984: learning rate 0.0000
[2019-04-04 14:05:36,336] A3C_AGENT_WORKER-Thread-3 INFO:Local step 127500, global step 2041113: loss 0.9509
[2019-04-04 14:05:36,337] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 127500, global step 2041113: learning rate 0.0000
[2019-04-04 14:05:37,254] A3C_AGENT_WORKER-Thread-10 INFO:Local step 127500, global step 2041414: loss 0.9153
[2019-04-04 14:05:37,255] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 127500, global step 2041414: learning rate 0.0000
[2019-04-04 14:05:37,463] A3C_AGENT_WORKER-Thread-6 INFO:Local step 127500, global step 2041483: loss 0.9208
[2019-04-04 14:05:37,464] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 127500, global step 2041483: learning rate 0.0000
[2019-04-04 14:05:37,468] A3C_AGENT_WORKER-Thread-5 INFO:Local step 127500, global step 2041485: loss 0.9173
[2019-04-04 14:05:37,469] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 127500, global step 2041485: learning rate 0.0000
[2019-04-04 14:05:38,074] A3C_AGENT_WORKER-Thread-4 INFO:Local step 127500, global step 2041675: loss 0.9139
[2019-04-04 14:05:38,075] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 127500, global step 2041675: learning rate 0.0000
[2019-04-04 14:05:38,530] A3C_AGENT_WORKER-Thread-12 INFO:Local step 127500, global step 2041821: loss 0.9488
[2019-04-04 14:05:38,531] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 127500, global step 2041821: learning rate 0.0000
[2019-04-04 14:05:41,583] A3C_AGENT_WORKER-Thread-13 INFO:Local step 128000, global step 2042891: loss 0.2675
[2019-04-04 14:05:41,584] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 128000, global step 2042891: learning rate 0.0000
[2019-04-04 14:05:45,075] A3C_AGENT_WORKER-Thread-17 INFO:Local step 128000, global step 2044005: loss 0.2721
[2019-04-04 14:05:45,075] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 128000, global step 2044005: learning rate 0.0000
[2019-04-04 14:05:47,509] A3C_AGENT_WORKER-Thread-2 INFO:Local step 128000, global step 2044715: loss 0.2471
[2019-04-04 14:05:47,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 128000, global step 2044717: learning rate 0.0000
[2019-04-04 14:05:51,442] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.8647308e-09 3.8488143e-11 1.5493246e-14 2.1494000e-14 1.0000000e+00
 8.0248620e-11 3.8486009e-15], sum to 1.0000
[2019-04-04 14:05:51,442] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0281
[2019-04-04 14:05:51,471] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 78.5, 0.0, 0.0, 26.0, 24.42799821525956, 0.1522550817241501, 0.0, 1.0, 44178.92922783081], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 253800.0000, 
sim time next is 254400.0000, 
raw observation next is [-3.899999999999999, 79.66666666666667, 0.0, 0.0, 26.0, 24.39487747930847, 0.1466998827211072, 0.0, 1.0, 44189.51689022614], 
processed observation next is [1.0, 0.9565217391304348, 0.35457063711911363, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5329064566090391, 0.5488999609070357, 0.0, 1.0, 0.21042627090583876], 
reward next is 0.7896, 
noisyNet noise sample is [array([1.8134941], dtype=float32), -0.94020367]. 
=============================================
[2019-04-04 14:05:52,520] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8179835e-09 1.9297036e-10 1.8909462e-15 2.6507701e-14 1.0000000e+00
 2.5408475e-10 2.7323062e-15], sum to 1.0000
[2019-04-04 14:05:52,520] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4037
[2019-04-04 14:05:52,555] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.83323716768868, 0.2339169010536495, 0.0, 1.0, 45333.91379784483], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 247800.0000, 
sim time next is 248400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.78831140330734, 0.2244223805490266, 0.0, 1.0, 44883.49586526852], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5656926169422783, 0.5748074601830089, 0.0, 1.0, 0.2137309326917549], 
reward next is 0.7863, 
noisyNet noise sample is [array([0.8410073], dtype=float32), 1.0321133]. 
=============================================
[2019-04-04 14:05:57,946] A3C_AGENT_WORKER-Thread-15 INFO:Local step 128000, global step 2048128: loss 0.2290
[2019-04-04 14:05:57,950] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 128000, global step 2048130: learning rate 0.0000
[2019-04-04 14:05:58,736] A3C_AGENT_WORKER-Thread-11 INFO:Local step 128000, global step 2048326: loss 0.2272
[2019-04-04 14:05:58,736] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 128000, global step 2048326: learning rate 0.0000
[2019-04-04 14:05:59,217] A3C_AGENT_WORKER-Thread-16 INFO:Local step 128000, global step 2048463: loss 0.2584
[2019-04-04 14:05:59,218] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 128000, global step 2048463: learning rate 0.0000
[2019-04-04 14:05:59,603] A3C_AGENT_WORKER-Thread-18 INFO:Local step 128000, global step 2048594: loss 0.2273
[2019-04-04 14:05:59,606] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 128000, global step 2048594: learning rate 0.0000
[2019-04-04 14:06:01,147] A3C_AGENT_WORKER-Thread-19 INFO:Local step 128000, global step 2049071: loss 0.2330
[2019-04-04 14:06:01,147] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 128000, global step 2049071: learning rate 0.0000
[2019-04-04 14:06:01,494] A3C_AGENT_WORKER-Thread-14 INFO:Local step 128000, global step 2049162: loss 0.2363
[2019-04-04 14:06:01,495] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 128000, global step 2049162: learning rate 0.0000
[2019-04-04 14:06:01,758] A3C_AGENT_WORKER-Thread-3 INFO:Local step 128000, global step 2049246: loss 0.2124
[2019-04-04 14:06:01,759] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 128000, global step 2049246: learning rate 0.0000
[2019-04-04 14:06:01,826] A3C_AGENT_WORKER-Thread-20 INFO:Local step 128000, global step 2049267: loss 0.2259
[2019-04-04 14:06:01,827] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 128000, global step 2049268: learning rate 0.0000
[2019-04-04 14:06:02,832] A3C_AGENT_WORKER-Thread-10 INFO:Local step 128000, global step 2049627: loss 0.2260
[2019-04-04 14:06:02,832] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 128000, global step 2049627: learning rate 0.0000
[2019-04-04 14:06:03,075] A3C_AGENT_WORKER-Thread-5 INFO:Local step 128000, global step 2049715: loss 0.2119
[2019-04-04 14:06:03,075] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 128000, global step 2049715: learning rate 0.0000
[2019-04-04 14:06:03,763] A3C_AGENT_WORKER-Thread-6 INFO:Local step 128000, global step 2049942: loss 0.2158
[2019-04-04 14:06:03,764] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 128000, global step 2049942: learning rate 0.0000
[2019-04-04 14:06:03,835] A3C_AGENT_WORKER-Thread-4 INFO:Local step 128000, global step 2049966: loss 0.2348
[2019-04-04 14:06:03,836] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 128000, global step 2049966: learning rate 0.0000
[2019-04-04 14:06:03,947] A3C_AGENT_WORKER-Thread-12 INFO:Local step 128000, global step 2050001: loss 0.2207
[2019-04-04 14:06:03,948] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 128000, global step 2050001: learning rate 0.0000
[2019-04-04 14:06:04,754] A3C_AGENT_WORKER-Thread-13 INFO:Local step 128500, global step 2050293: loss 1.2243
[2019-04-04 14:06:04,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 128500, global step 2050293: learning rate 0.0000
[2019-04-04 14:06:07,496] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5266067e-09 4.9993165e-10 1.3714399e-14 6.5241568e-14 1.0000000e+00
 2.0222542e-10 5.5078631e-15], sum to 1.0000
[2019-04-04 14:06:07,496] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8314
[2019-04-04 14:06:07,578] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.16666666666667, 44.0, 49.16666666666666, 841.1666666666667, 26.0, 25.89400550062027, 0.4246978011192106, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 397200.0000, 
sim time next is 397800.0000, 
raw observation next is [-10.0, 43.0, 48.0, 832.0, 26.0, 25.16343722392237, 0.4282634088666383, 1.0, 1.0, 183622.5172362765], 
processed observation next is [1.0, 0.6086956521739131, 0.18559556786703602, 0.43, 0.16, 0.9193370165745857, 0.6666666666666666, 0.5969531019935307, 0.6427544696222127, 1.0, 1.0, 0.8743929392203643], 
reward next is 0.1256, 
noisyNet noise sample is [array([1.3059478], dtype=float32), 0.46521154]. 
=============================================
[2019-04-04 14:06:08,038] A3C_AGENT_WORKER-Thread-17 INFO:Local step 128500, global step 2051282: loss 1.2210
[2019-04-04 14:06:08,039] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 128500, global step 2051282: learning rate 0.0000
[2019-04-04 14:06:10,135] A3C_AGENT_WORKER-Thread-2 INFO:Local step 128500, global step 2051991: loss 1.2229
[2019-04-04 14:06:10,135] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 128500, global step 2051991: learning rate 0.0000
[2019-04-04 14:06:18,636] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7917793e-09 6.4316372e-11 1.3064013e-16 2.7439016e-14 1.0000000e+00
 3.6577158e-11 3.5340716e-16], sum to 1.0000
[2019-04-04 14:06:18,637] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6019
[2019-04-04 14:06:18,659] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 89.0, 0.0, 0.0, 26.0, 24.84454364342294, 0.2557508989919627, 0.0, 1.0, 39906.45578289893], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 522000.0000, 
sim time next is 522600.0000, 
raw observation next is [4.883333333333334, 88.83333333333334, 0.0, 0.0, 26.0, 24.97097467198192, 0.259025063272781, 0.0, 1.0, 39658.65650971423], 
processed observation next is [0.0, 0.043478260869565216, 0.597876269621422, 0.8883333333333334, 0.0, 0.0, 0.6666666666666666, 0.5809145559984934, 0.5863416877575937, 0.0, 1.0, 0.18885074528435347], 
reward next is 0.8111, 
noisyNet noise sample is [array([0.87463397], dtype=float32), -1.4000201]. 
=============================================
[2019-04-04 14:06:20,633] A3C_AGENT_WORKER-Thread-15 INFO:Local step 128500, global step 2055825: loss 1.2238
[2019-04-04 14:06:20,634] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 128500, global step 2055825: learning rate 0.0000
[2019-04-04 14:06:21,169] A3C_AGENT_WORKER-Thread-11 INFO:Local step 128500, global step 2056009: loss 1.2343
[2019-04-04 14:06:21,170] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 128500, global step 2056009: learning rate 0.0000
[2019-04-04 14:06:21,559] A3C_AGENT_WORKER-Thread-16 INFO:Local step 128500, global step 2056146: loss 1.2245
[2019-04-04 14:06:21,561] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 128500, global step 2056149: learning rate 0.0000
[2019-04-04 14:06:22,101] A3C_AGENT_WORKER-Thread-18 INFO:Local step 128500, global step 2056344: loss 1.2041
[2019-04-04 14:06:22,108] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 128500, global step 2056344: learning rate 0.0000
[2019-04-04 14:06:23,434] A3C_AGENT_WORKER-Thread-14 INFO:Local step 128500, global step 2056836: loss 1.2264
[2019-04-04 14:06:23,435] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 128500, global step 2056836: learning rate 0.0000
[2019-04-04 14:06:23,713] A3C_AGENT_WORKER-Thread-19 INFO:Local step 128500, global step 2056923: loss 1.2161
[2019-04-04 14:06:23,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 128500, global step 2056923: learning rate 0.0000
[2019-04-04 14:06:23,728] A3C_AGENT_WORKER-Thread-3 INFO:Local step 128500, global step 2056927: loss 1.2184
[2019-04-04 14:06:23,729] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 128500, global step 2056928: learning rate 0.0000
[2019-04-04 14:06:23,984] A3C_AGENT_WORKER-Thread-20 INFO:Local step 128500, global step 2057013: loss 1.2306
[2019-04-04 14:06:23,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 128500, global step 2057013: learning rate 0.0000
[2019-04-04 14:06:24,811] A3C_AGENT_WORKER-Thread-10 INFO:Local step 128500, global step 2057343: loss 1.2306
[2019-04-04 14:06:24,818] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 128500, global step 2057346: learning rate 0.0000
[2019-04-04 14:06:25,617] A3C_AGENT_WORKER-Thread-6 INFO:Local step 128500, global step 2057676: loss 1.2192
[2019-04-04 14:06:25,618] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 128500, global step 2057676: learning rate 0.0000
[2019-04-04 14:06:25,860] A3C_AGENT_WORKER-Thread-5 INFO:Local step 128500, global step 2057768: loss 1.2144
[2019-04-04 14:06:25,861] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 128500, global step 2057768: learning rate 0.0000
[2019-04-04 14:06:25,892] A3C_AGENT_WORKER-Thread-4 INFO:Local step 128500, global step 2057781: loss 1.2321
[2019-04-04 14:06:25,892] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 128500, global step 2057781: learning rate 0.0000
[2019-04-04 14:06:26,295] A3C_AGENT_WORKER-Thread-13 INFO:Local step 129000, global step 2057905: loss 0.6827
[2019-04-04 14:06:26,296] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 129000, global step 2057905: learning rate 0.0000
[2019-04-04 14:06:26,315] A3C_AGENT_WORKER-Thread-12 INFO:Local step 128500, global step 2057918: loss 1.2408
[2019-04-04 14:06:26,330] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 128500, global step 2057918: learning rate 0.0000
[2019-04-04 14:06:26,690] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.8170371e-09 2.4259657e-09 3.5215312e-15 6.2716474e-14 1.0000000e+00
 8.1960760e-10 7.0692692e-15], sum to 1.0000
[2019-04-04 14:06:26,691] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7645
[2019-04-04 14:06:26,718] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.2, 75.0, 0.0, 0.0, 26.0, 24.29336834973198, 0.06340123676435756, 0.0, 1.0, 41551.70322841383], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 703200.0000, 
sim time next is 703800.0000, 
raw observation next is [-3.1, 75.0, 0.0, 0.0, 26.0, 24.36409219946399, 0.05835944554945066, 0.0, 1.0, 41572.47635116085], 
processed observation next is [1.0, 0.13043478260869565, 0.37673130193905824, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5303410166219992, 0.5194531485164836, 0.0, 1.0, 0.19796417310076594], 
reward next is 0.8020, 
noisyNet noise sample is [array([0.4961912], dtype=float32), 0.023410399]. 
=============================================
[2019-04-04 14:06:29,066] A3C_AGENT_WORKER-Thread-17 INFO:Local step 129000, global step 2059019: loss 0.6838
[2019-04-04 14:06:29,069] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 129000, global step 2059019: learning rate 0.0000
[2019-04-04 14:06:31,054] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.5561823e-09 4.4020806e-10 3.5722280e-15 1.5193383e-13 1.0000000e+00
 1.7682575e-10 7.7479781e-15], sum to 1.0000
[2019-04-04 14:06:31,057] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2711
[2019-04-04 14:06:31,071] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.2683288354067, 0.03818476197701701, 0.0, 1.0, 41565.76084855872], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 709200.0000, 
sim time next is 709800.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.2534483091343, 0.03337916308837224, 0.0, 1.0, 41575.89677115578], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5211206924278583, 0.5111263876961241, 0.0, 1.0, 0.19798046081502751], 
reward next is 0.8020, 
noisyNet noise sample is [array([0.4687569], dtype=float32), -1.9778768]. 
=============================================
[2019-04-04 14:06:31,463] A3C_AGENT_WORKER-Thread-2 INFO:Local step 129000, global step 2060189: loss 0.7002
[2019-04-04 14:06:31,472] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 129000, global step 2060193: learning rate 0.0000
[2019-04-04 14:06:36,189] A3C_AGENT_WORKER-Thread-13 INFO:Local step 129500, global step 2062419: loss 0.8417
[2019-04-04 14:06:36,194] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 129500, global step 2062423: learning rate 0.0000
[2019-04-04 14:06:36,344] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3858135e-10 2.0432867e-11 4.9283511e-16 2.2169135e-15 1.0000000e+00
 5.2410052e-11 1.0742890e-15], sum to 1.0000
[2019-04-04 14:06:36,347] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3291
[2019-04-04 14:06:36,357] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.5, 77.0, 0.0, 0.0, 26.0, 25.65045937549521, 0.6422828976168052, 0.0, 1.0, 27005.70634228583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1126800.0000, 
sim time next is 1127400.0000, 
raw observation next is [10.41666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.65309318862646, 0.6412903590704385, 0.0, 1.0, 23303.9696024605], 
processed observation next is [0.0, 0.043478260869565216, 0.7511542012927056, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.6377577657188717, 0.7137634530234794, 0.0, 1.0, 0.11097128382124048], 
reward next is 0.8890, 
noisyNet noise sample is [array([0.8753264], dtype=float32), -0.36162955]. 
=============================================
[2019-04-04 14:06:36,474] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.5276114e-10 3.3473984e-11 1.5572777e-16 1.6201414e-15 1.0000000e+00
 1.0079725e-10 2.1492871e-16], sum to 1.0000
[2019-04-04 14:06:36,475] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8667
[2019-04-04 14:06:36,526] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 24.93457773472177, 0.3115385520730899, 1.0, 1.0, 24845.97251932622], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 845400.0000, 
sim time next is 846000.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.04319846348974, 0.3204311190233171, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5869332052908117, 0.6068103730077724, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3417512], dtype=float32), 0.5127571]. 
=============================================
[2019-04-04 14:06:36,529] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[88.333244]
 [88.16097 ]
 [87.53186 ]
 [87.02755 ]
 [87.03503 ]], R is [[88.38801575]
 [88.38581848]
 [87.77209473]
 [87.27990723]
 [87.13471222]].
[2019-04-04 14:06:39,328] A3C_AGENT_WORKER-Thread-17 INFO:Local step 129500, global step 2063760: loss 0.8350
[2019-04-04 14:06:39,328] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 129500, global step 2063760: learning rate 0.0000
[2019-04-04 14:06:41,202] A3C_AGENT_WORKER-Thread-15 INFO:Local step 129000, global step 2064646: loss 0.7636
[2019-04-04 14:06:41,203] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 129000, global step 2064646: learning rate 0.0000
[2019-04-04 14:06:41,565] A3C_AGENT_WORKER-Thread-2 INFO:Local step 129500, global step 2064794: loss 0.8475
[2019-04-04 14:06:41,566] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 129500, global step 2064794: learning rate 0.0000
[2019-04-04 14:06:41,894] A3C_AGENT_WORKER-Thread-11 INFO:Local step 129000, global step 2064961: loss 0.7131
[2019-04-04 14:06:41,896] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 129000, global step 2064961: learning rate 0.0000
[2019-04-04 14:06:42,381] A3C_AGENT_WORKER-Thread-16 INFO:Local step 129000, global step 2065212: loss 0.7319
[2019-04-04 14:06:42,382] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 129000, global step 2065212: learning rate 0.0000
[2019-04-04 14:06:42,664] A3C_AGENT_WORKER-Thread-18 INFO:Local step 129000, global step 2065387: loss 0.7139
[2019-04-04 14:06:42,673] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 129000, global step 2065387: learning rate 0.0000
[2019-04-04 14:06:43,696] A3C_AGENT_WORKER-Thread-14 INFO:Local step 129000, global step 2065910: loss 0.6933
[2019-04-04 14:06:43,696] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 129000, global step 2065910: learning rate 0.0000
[2019-04-04 14:06:44,157] A3C_AGENT_WORKER-Thread-3 INFO:Local step 129000, global step 2066134: loss 0.7132
[2019-04-04 14:06:44,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 129000, global step 2066134: learning rate 0.0000
[2019-04-04 14:06:44,326] A3C_AGENT_WORKER-Thread-20 INFO:Local step 129000, global step 2066222: loss 0.6903
[2019-04-04 14:06:44,332] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 129000, global step 2066225: learning rate 0.0000
[2019-04-04 14:06:44,380] A3C_AGENT_WORKER-Thread-19 INFO:Local step 129000, global step 2066250: loss 0.7116
[2019-04-04 14:06:44,381] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 129000, global step 2066250: learning rate 0.0000
[2019-04-04 14:06:45,408] A3C_AGENT_WORKER-Thread-10 INFO:Local step 129000, global step 2066816: loss 0.7068
[2019-04-04 14:06:45,409] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 129000, global step 2066816: learning rate 0.0000
[2019-04-04 14:06:45,954] A3C_AGENT_WORKER-Thread-5 INFO:Local step 129000, global step 2067105: loss 0.7193
[2019-04-04 14:06:45,955] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 129000, global step 2067105: learning rate 0.0000
[2019-04-04 14:06:46,185] A3C_AGENT_WORKER-Thread-4 INFO:Local step 129000, global step 2067242: loss 0.6924
[2019-04-04 14:06:46,190] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 129000, global step 2067246: learning rate 0.0000
[2019-04-04 14:06:46,339] A3C_AGENT_WORKER-Thread-6 INFO:Local step 129000, global step 2067330: loss 0.7144
[2019-04-04 14:06:46,340] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 129000, global step 2067330: learning rate 0.0000
[2019-04-04 14:06:46,819] A3C_AGENT_WORKER-Thread-12 INFO:Local step 129000, global step 2067635: loss 0.6831
[2019-04-04 14:06:46,821] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 129000, global step 2067635: learning rate 0.0000
[2019-04-04 14:06:51,292] A3C_AGENT_WORKER-Thread-13 INFO:Local step 130000, global step 2070510: loss 0.6557
[2019-04-04 14:06:51,293] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 130000, global step 2070511: learning rate 0.0000
[2019-04-04 14:06:51,749] A3C_AGENT_WORKER-Thread-15 INFO:Local step 129500, global step 2070824: loss 0.8451
[2019-04-04 14:06:51,750] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 129500, global step 2070824: learning rate 0.0000
[2019-04-04 14:06:53,059] A3C_AGENT_WORKER-Thread-11 INFO:Local step 129500, global step 2071698: loss 0.8149
[2019-04-04 14:06:53,062] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 129500, global step 2071699: learning rate 0.0000
[2019-04-04 14:06:53,564] A3C_AGENT_WORKER-Thread-16 INFO:Local step 129500, global step 2072042: loss 0.8158
[2019-04-04 14:06:53,567] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 129500, global step 2072043: learning rate 0.0000
[2019-04-04 14:06:53,974] A3C_AGENT_WORKER-Thread-18 INFO:Local step 129500, global step 2072321: loss 0.8022
[2019-04-04 14:06:53,976] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 129500, global step 2072321: learning rate 0.0000
[2019-04-04 14:06:53,988] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0805787e-09 2.1673316e-11 9.0918750e-17 2.7683326e-16 1.0000000e+00
 3.5613350e-12 5.7711286e-17], sum to 1.0000
[2019-04-04 14:06:53,992] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2089
[2019-04-04 14:06:54,000] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.8, 69.33333333333333, 0.0, 0.0, 26.0, 25.74895419666925, 0.6625393549980526, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1122000.0000, 
sim time next is 1122600.0000, 
raw observation next is [11.7, 70.16666666666667, 0.0, 0.0, 26.0, 25.75656444253736, 0.6588107991078515, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.7867036011080333, 0.7016666666666667, 0.0, 0.0, 0.6666666666666666, 0.6463803702114467, 0.7196035997026171, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07638871], dtype=float32), -1.0458971]. 
=============================================
[2019-04-04 14:06:54,615] A3C_AGENT_WORKER-Thread-17 INFO:Local step 130000, global step 2072752: loss 0.6307
[2019-04-04 14:06:54,617] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 130000, global step 2072754: learning rate 0.0000
[2019-04-04 14:06:54,889] A3C_AGENT_WORKER-Thread-14 INFO:Local step 129500, global step 2072934: loss 0.8166
[2019-04-04 14:06:54,891] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 129500, global step 2072935: learning rate 0.0000
[2019-04-04 14:06:55,559] A3C_AGENT_WORKER-Thread-19 INFO:Local step 129500, global step 2073398: loss 0.8182
[2019-04-04 14:06:55,561] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 129500, global step 2073399: learning rate 0.0000
[2019-04-04 14:06:55,623] A3C_AGENT_WORKER-Thread-3 INFO:Local step 129500, global step 2073447: loss 0.8316
[2019-04-04 14:06:55,627] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 129500, global step 2073447: learning rate 0.0000
[2019-04-04 14:06:55,725] A3C_AGENT_WORKER-Thread-20 INFO:Local step 129500, global step 2073511: loss 0.8084
[2019-04-04 14:06:55,728] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 129500, global step 2073515: learning rate 0.0000
[2019-04-04 14:06:56,760] A3C_AGENT_WORKER-Thread-2 INFO:Local step 130000, global step 2074191: loss 0.6205
[2019-04-04 14:06:56,761] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 130000, global step 2074191: learning rate 0.0000
[2019-04-04 14:06:57,048] A3C_AGENT_WORKER-Thread-10 INFO:Local step 129500, global step 2074393: loss 0.7803
[2019-04-04 14:06:57,049] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 129500, global step 2074393: learning rate 0.0000
[2019-04-04 14:06:57,288] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6357659e-10 6.0955303e-12 2.9018489e-18 2.9773494e-16 1.0000000e+00
 3.7801043e-12 2.1716782e-17], sum to 1.0000
[2019-04-04 14:06:57,289] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2553
[2019-04-04 14:06:57,301] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.48054760573672, 0.586121873141182, 0.0, 1.0, 46095.19647687343], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1286400.0000, 
sim time next is 1287000.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.45001786479835, 0.5865211544908505, 0.0, 1.0, 56250.55285079154], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6208348220665293, 0.6955070514969502, 0.0, 1.0, 0.2678597754799597], 
reward next is 0.7321, 
noisyNet noise sample is [array([-0.01705449], dtype=float32), -0.13472266]. 
=============================================
[2019-04-04 14:06:57,324] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[93.48509 ]
 [93.53967 ]
 [93.70971 ]
 [93.97779 ]
 [94.287506]], R is [[93.30280304]
 [93.15027618]
 [93.09754181]
 [93.16656494]
 [93.23490143]].
[2019-04-04 14:06:57,392] A3C_AGENT_WORKER-Thread-5 INFO:Local step 129500, global step 2074622: loss 0.7904
[2019-04-04 14:06:57,393] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 129500, global step 2074622: learning rate 0.0000
[2019-04-04 14:06:57,783] A3C_AGENT_WORKER-Thread-4 INFO:Local step 129500, global step 2074862: loss 0.7916
[2019-04-04 14:06:57,785] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 129500, global step 2074862: learning rate 0.0000
[2019-04-04 14:06:57,815] A3C_AGENT_WORKER-Thread-6 INFO:Local step 129500, global step 2074878: loss 0.8226
[2019-04-04 14:06:57,816] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 129500, global step 2074878: learning rate 0.0000
[2019-04-04 14:06:58,422] A3C_AGENT_WORKER-Thread-12 INFO:Local step 129500, global step 2075239: loss 0.7954
[2019-04-04 14:06:58,426] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 129500, global step 2075243: learning rate 0.0000
[2019-04-04 14:07:02,519] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8836474e-10 1.0157866e-11 1.7930995e-16 7.3470611e-16 1.0000000e+00
 2.8804521e-11 4.7695433e-18], sum to 1.0000
[2019-04-04 14:07:02,522] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1649
[2019-04-04 14:07:02,606] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.20181319108664, 0.3301094876567798, 1.0, 1.0, 22332.01250905557], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1708200.0000, 
sim time next is 1708800.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 24.38057802908627, 0.3326488490059551, 1.0, 1.0, 196524.289557986], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5317148357571891, 0.6108829496686518, 1.0, 1.0, 0.9358299502761238], 
reward next is 0.0642, 
noisyNet noise sample is [array([1.8595172], dtype=float32), 1.8221589]. 
=============================================
[2019-04-04 14:07:07,757] A3C_AGENT_WORKER-Thread-15 INFO:Local step 130000, global step 2079756: loss 0.7441
[2019-04-04 14:07:07,758] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 130000, global step 2079756: learning rate 0.0000
[2019-04-04 14:07:07,963] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.2360334e-10 2.5523724e-11 1.6516854e-16 3.7898880e-15 1.0000000e+00
 2.9715702e-11 3.4632302e-17], sum to 1.0000
[2019-04-04 14:07:07,963] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6956
[2019-04-04 14:07:07,983] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.40001988161649, 0.4933298781323066, 0.0, 1.0, 25659.73749012882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1466400.0000, 
sim time next is 1467000.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.38690794060201, 0.4886755702235293, 0.0, 1.0, 38180.31718165169], 
processed observation next is [1.0, 1.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6155756617168343, 0.6628918567411765, 0.0, 1.0, 0.18181103419834138], 
reward next is 0.8182, 
noisyNet noise sample is [array([0.15946738], dtype=float32), -2.4696164]. 
=============================================
[2019-04-04 14:07:07,995] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[90.01789]
 [90.08409]
 [90.05516]
 [90.00009]
 [90.01959]], R is [[89.95310211]
 [89.93138123]
 [89.86470795]
 [89.76964569]
 [89.64993286]].
[2019-04-04 14:07:08,059] A3C_AGENT_WORKER-Thread-13 INFO:Local step 130500, global step 2079923: loss 0.2334
[2019-04-04 14:07:08,060] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 130500, global step 2079923: learning rate 0.0000
[2019-04-04 14:07:09,053] A3C_AGENT_WORKER-Thread-11 INFO:Local step 130000, global step 2080389: loss 0.7697
[2019-04-04 14:07:09,055] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 130000, global step 2080390: learning rate 0.0000
[2019-04-04 14:07:09,074] A3C_AGENT_WORKER-Thread-16 INFO:Local step 130000, global step 2080397: loss 0.7637
[2019-04-04 14:07:09,076] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 130000, global step 2080397: learning rate 0.0000
[2019-04-04 14:07:09,907] A3C_AGENT_WORKER-Thread-18 INFO:Local step 130000, global step 2080835: loss 0.7552
[2019-04-04 14:07:09,909] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 130000, global step 2080835: learning rate 0.0000
[2019-04-04 14:07:10,174] A3C_AGENT_WORKER-Thread-14 INFO:Local step 130000, global step 2081000: loss 0.7479
[2019-04-04 14:07:10,175] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 130000, global step 2081000: learning rate 0.0000
[2019-04-04 14:07:10,715] A3C_AGENT_WORKER-Thread-3 INFO:Local step 130000, global step 2081343: loss 0.7648
[2019-04-04 14:07:10,718] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 130000, global step 2081344: learning rate 0.0000
[2019-04-04 14:07:10,870] A3C_AGENT_WORKER-Thread-17 INFO:Local step 130500, global step 2081437: loss 0.2230
[2019-04-04 14:07:10,871] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 130500, global step 2081437: learning rate 0.0000
[2019-04-04 14:07:11,190] A3C_AGENT_WORKER-Thread-20 INFO:Local step 130000, global step 2081615: loss 0.7984
[2019-04-04 14:07:11,192] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 130000, global step 2081615: learning rate 0.0000
[2019-04-04 14:07:11,227] A3C_AGENT_WORKER-Thread-19 INFO:Local step 130000, global step 2081635: loss 0.7722
[2019-04-04 14:07:11,230] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 130000, global step 2081635: learning rate 0.0000
[2019-04-04 14:07:12,505] A3C_AGENT_WORKER-Thread-10 INFO:Local step 130000, global step 2082391: loss 0.8060
[2019-04-04 14:07:12,510] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 130000, global step 2082392: learning rate 0.0000
[2019-04-04 14:07:12,538] A3C_AGENT_WORKER-Thread-5 INFO:Local step 130000, global step 2082411: loss 0.7961
[2019-04-04 14:07:12,539] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 130000, global step 2082411: learning rate 0.0000
[2019-04-04 14:07:12,854] A3C_AGENT_WORKER-Thread-2 INFO:Local step 130500, global step 2082617: loss 0.1974
[2019-04-04 14:07:12,857] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 130500, global step 2082617: learning rate 0.0000
[2019-04-04 14:07:13,150] A3C_AGENT_WORKER-Thread-4 INFO:Local step 130000, global step 2082799: loss 0.8108
[2019-04-04 14:07:13,154] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 130000, global step 2082800: learning rate 0.0000
[2019-04-04 14:07:13,356] A3C_AGENT_WORKER-Thread-6 INFO:Local step 130000, global step 2082926: loss 0.8237
[2019-04-04 14:07:13,359] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 130000, global step 2082926: learning rate 0.0000
[2019-04-04 14:07:13,652] A3C_AGENT_WORKER-Thread-12 INFO:Local step 130000, global step 2083093: loss 0.8045
[2019-04-04 14:07:13,656] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 130000, global step 2083094: learning rate 0.0000
[2019-04-04 14:07:17,083] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1471466e-10 2.6900487e-11 1.5339534e-16 2.5540085e-15 1.0000000e+00
 1.1271474e-11 1.0014998e-16], sum to 1.0000
[2019-04-04 14:07:17,084] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0284
[2019-04-04 14:07:17,127] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 96.66666666666667, 0.0, 26.0, 26.02091027354357, 0.6067889095046485, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1690800.0000, 
sim time next is 1691400.0000, 
raw observation next is [1.1, 88.0, 93.33333333333333, 0.0, 26.0, 26.19253713042627, 0.6223947647666914, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3111111111111111, 0.0, 0.6666666666666666, 0.6827114275355225, 0.7074649215888972, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7879576], dtype=float32), -0.1854774]. 
=============================================
[2019-04-04 14:07:19,812] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1398459e-09 1.2237573e-10 2.0889946e-15 1.0539893e-14 1.0000000e+00
 3.7189060e-10 3.2620546e-16], sum to 1.0000
[2019-04-04 14:07:19,815] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4513
[2019-04-04 14:07:19,843] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 81.0, 41.5, 0.0, 26.0, 26.04587446566456, 0.564482699586775, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1699200.0000, 
sim time next is 1699800.0000, 
raw observation next is [1.516666666666667, 82.16666666666667, 37.66666666666667, 0.0, 26.0, 26.07920314543801, 0.4540562313181396, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5046168051708219, 0.8216666666666668, 0.12555555555555556, 0.0, 0.6666666666666666, 0.673266928786501, 0.6513520771060465, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1155412], dtype=float32), -0.20880271]. 
=============================================
[2019-04-04 14:07:23,182] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8871904e-09 1.0650191e-10 3.0087375e-15 8.9775667e-15 1.0000000e+00
 1.0749374e-10 1.7028947e-15], sum to 1.0000
[2019-04-04 14:07:23,182] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2272
[2019-04-04 14:07:23,209] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 87.0, 0.0, 0.0, 26.0, 24.75683393378661, 0.3139666278372978, 0.0, 1.0, 44023.1710937966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1753200.0000, 
sim time next is 1753800.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 26.0, 24.75132803783238, 0.3095296845501519, 0.0, 1.0, 44030.00307459341], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 0.6666666666666666, 0.562610669819365, 0.6031765615167173, 0.0, 1.0, 0.20966668130758767], 
reward next is 0.7903, 
noisyNet noise sample is [array([1.4406756], dtype=float32), -0.085162684]. 
=============================================
[2019-04-04 14:07:23,576] A3C_AGENT_WORKER-Thread-15 INFO:Local step 130500, global step 2087546: loss 0.1761
[2019-04-04 14:07:23,577] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 130500, global step 2087546: learning rate 0.0000
[2019-04-04 14:07:25,202] A3C_AGENT_WORKER-Thread-16 INFO:Local step 130500, global step 2088125: loss 0.2041
[2019-04-04 14:07:25,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 130500, global step 2088125: learning rate 0.0000
[2019-04-04 14:07:25,376] A3C_AGENT_WORKER-Thread-11 INFO:Local step 130500, global step 2088194: loss 0.1821
[2019-04-04 14:07:25,377] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 130500, global step 2088194: learning rate 0.0000
[2019-04-04 14:07:26,367] A3C_AGENT_WORKER-Thread-18 INFO:Local step 130500, global step 2088567: loss 0.1887
[2019-04-04 14:07:26,368] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 130500, global step 2088567: learning rate 0.0000
[2019-04-04 14:07:26,497] A3C_AGENT_WORKER-Thread-14 INFO:Local step 130500, global step 2088617: loss 0.1806
[2019-04-04 14:07:26,498] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 130500, global step 2088617: learning rate 0.0000
[2019-04-04 14:07:26,663] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.1899453e-10 3.9379619e-10 1.4298188e-15 1.3138743e-14 1.0000000e+00
 2.5606081e-11 3.3282859e-15], sum to 1.0000
[2019-04-04 14:07:26,664] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6420
[2019-04-04 14:07:26,733] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 75.5, 141.3333333333333, 61.0, 26.0, 24.95810617739415, 0.2482293913810818, 0.0, 1.0, 22428.61916245018], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1853400.0000, 
sim time next is 1854000.0000, 
raw observation next is [-5.6, 75.0, 152.0, 66.0, 26.0, 24.97494960660735, 0.2485409750475607, 0.0, 1.0, 27542.35881952843], 
processed observation next is [0.0, 0.4782608695652174, 0.30747922437673136, 0.75, 0.5066666666666667, 0.07292817679558011, 0.6666666666666666, 0.5812458005506125, 0.5828469916825202, 0.0, 1.0, 0.13115408961680203], 
reward next is 0.8688, 
noisyNet noise sample is [array([1.6622254], dtype=float32), -0.054206118]. 
=============================================
[2019-04-04 14:07:26,743] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.42734 ]
 [79.52984 ]
 [79.566086]
 [79.48088 ]
 [79.27281 ]], R is [[79.44904327]
 [79.54775238]
 [79.55146027]
 [79.39641571]
 [79.10203552]].
[2019-04-04 14:07:26,837] A3C_AGENT_WORKER-Thread-3 INFO:Local step 130500, global step 2088752: loss 0.1834
[2019-04-04 14:07:26,837] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 130500, global step 2088752: learning rate 0.0000
[2019-04-04 14:07:27,466] A3C_AGENT_WORKER-Thread-20 INFO:Local step 130500, global step 2088950: loss 0.1973
[2019-04-04 14:07:27,467] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 130500, global step 2088950: learning rate 0.0000
[2019-04-04 14:07:27,632] A3C_AGENT_WORKER-Thread-19 INFO:Local step 130500, global step 2088989: loss 0.1850
[2019-04-04 14:07:27,632] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 130500, global step 2088989: learning rate 0.0000
[2019-04-04 14:07:29,192] A3C_AGENT_WORKER-Thread-5 INFO:Local step 130500, global step 2089523: loss 0.1607
[2019-04-04 14:07:29,194] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 130500, global step 2089524: learning rate 0.0000
[2019-04-04 14:07:29,484] A3C_AGENT_WORKER-Thread-4 INFO:Local step 130500, global step 2089651: loss 0.1824
[2019-04-04 14:07:29,484] A3C_AGENT_WORKER-Thread-10 INFO:Local step 130500, global step 2089652: loss 0.1660
[2019-04-04 14:07:29,489] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 130500, global step 2089653: learning rate 0.0000
[2019-04-04 14:07:29,490] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 130500, global step 2089653: learning rate 0.0000
[2019-04-04 14:07:30,030] A3C_AGENT_WORKER-Thread-6 INFO:Local step 130500, global step 2089850: loss 0.1708
[2019-04-04 14:07:30,031] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 130500, global step 2089850: learning rate 0.0000
[2019-04-04 14:07:30,864] A3C_AGENT_WORKER-Thread-12 INFO:Local step 130500, global step 2090127: loss 0.1653
[2019-04-04 14:07:30,864] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 130500, global step 2090127: learning rate 0.0000
[2019-04-04 14:07:31,010] A3C_AGENT_WORKER-Thread-13 INFO:Local step 131000, global step 2090176: loss 0.0676
[2019-04-04 14:07:31,010] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 131000, global step 2090176: learning rate 0.0000
[2019-04-04 14:07:34,209] A3C_AGENT_WORKER-Thread-17 INFO:Local step 131000, global step 2091300: loss 0.0656
[2019-04-04 14:07:34,210] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 131000, global step 2091300: learning rate 0.0000
[2019-04-04 14:07:34,795] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.3600291e-09 6.3460848e-10 1.0714201e-15 7.3799047e-14 1.0000000e+00
 1.1088713e-10 2.1936364e-15], sum to 1.0000
[2019-04-04 14:07:34,795] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8503
[2019-04-04 14:07:34,852] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 75.0, 61.33333333333333, 325.0, 26.0, 25.90937684072691, 0.403240780970924, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2191800.0000, 
sim time next is 2192400.0000, 
raw observation next is [-5.6, 75.0, 71.5, 356.5, 26.0, 25.96174273952661, 0.4075381386047334, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.30747922437673136, 0.75, 0.23833333333333334, 0.3939226519337017, 0.6666666666666666, 0.6634785616272175, 0.6358460462015778, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9036659], dtype=float32), -0.53267986]. 
=============================================
[2019-04-04 14:07:36,279] A3C_AGENT_WORKER-Thread-2 INFO:Local step 131000, global step 2092032: loss 0.0741
[2019-04-04 14:07:36,280] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 131000, global step 2092032: learning rate 0.0000
[2019-04-04 14:07:47,506] A3C_AGENT_WORKER-Thread-15 INFO:Local step 131000, global step 2095922: loss 0.0721
[2019-04-04 14:07:47,506] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 131000, global step 2095922: learning rate 0.0000
[2019-04-04 14:07:48,392] A3C_AGENT_WORKER-Thread-11 INFO:Local step 131000, global step 2096197: loss 0.0728
[2019-04-04 14:07:48,395] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 131000, global step 2096199: learning rate 0.0000
[2019-04-04 14:07:48,414] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5233511e-09 7.7434142e-11 1.9934770e-15 1.7490070e-14 1.0000000e+00
 1.4786748e-10 3.4390628e-15], sum to 1.0000
[2019-04-04 14:07:48,417] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9063
[2019-04-04 14:07:48,450] A3C_AGENT_WORKER-Thread-16 INFO:Local step 131000, global step 2096217: loss 0.0737
[2019-04-04 14:07:48,451] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 87.66666666666667, 0.0, 0.0, 26.0, 24.87484169124182, 0.2871141941427749, 0.0, 1.0, 42494.93064017963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2071200.0000, 
sim time next is 2071800.0000, 
raw observation next is [-4.5, 88.5, 0.0, 0.0, 26.0, 24.83281229805121, 0.2783354862622058, 0.0, 1.0, 42570.37436777331], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.885, 0.0, 0.0, 0.6666666666666666, 0.5694010248376008, 0.5927784954207352, 0.0, 1.0, 0.20271606841796813], 
reward next is 0.7973, 
noisyNet noise sample is [array([-0.5211861], dtype=float32), -0.28430295]. 
=============================================
[2019-04-04 14:07:48,456] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 131000, global step 2096218: learning rate 0.0000
[2019-04-04 14:07:49,821] A3C_AGENT_WORKER-Thread-14 INFO:Local step 131000, global step 2096770: loss 0.0662
[2019-04-04 14:07:49,821] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 131000, global step 2096770: learning rate 0.0000
[2019-04-04 14:07:50,292] A3C_AGENT_WORKER-Thread-18 INFO:Local step 131000, global step 2096944: loss 0.0665
[2019-04-04 14:07:50,298] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 131000, global step 2096949: learning rate 0.0000
[2019-04-04 14:07:50,317] A3C_AGENT_WORKER-Thread-3 INFO:Local step 131000, global step 2096956: loss 0.0600
[2019-04-04 14:07:50,318] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 131000, global step 2096956: learning rate 0.0000
[2019-04-04 14:07:50,815] A3C_AGENT_WORKER-Thread-20 INFO:Local step 131000, global step 2097127: loss 0.0612
[2019-04-04 14:07:50,816] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 131000, global step 2097127: learning rate 0.0000
[2019-04-04 14:07:51,506] A3C_AGENT_WORKER-Thread-19 INFO:Local step 131000, global step 2097385: loss 0.0662
[2019-04-04 14:07:51,507] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 131000, global step 2097385: learning rate 0.0000
[2019-04-04 14:07:52,162] A3C_AGENT_WORKER-Thread-13 INFO:Local step 131500, global step 2097612: loss 0.0319
[2019-04-04 14:07:52,164] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 131500, global step 2097613: learning rate 0.0000
[2019-04-04 14:07:52,913] A3C_AGENT_WORKER-Thread-10 INFO:Local step 131000, global step 2097899: loss 0.0572
[2019-04-04 14:07:52,913] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 131000, global step 2097899: learning rate 0.0000
[2019-04-04 14:07:53,081] A3C_AGENT_WORKER-Thread-5 INFO:Local step 131000, global step 2097960: loss 0.0615
[2019-04-04 14:07:53,087] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 131000, global step 2097961: learning rate 0.0000
[2019-04-04 14:07:53,215] A3C_AGENT_WORKER-Thread-4 INFO:Local step 131000, global step 2098007: loss 0.0671
[2019-04-04 14:07:53,215] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 131000, global step 2098007: learning rate 0.0000
[2019-04-04 14:07:53,763] A3C_AGENT_WORKER-Thread-6 INFO:Local step 131000, global step 2098169: loss 0.0675
[2019-04-04 14:07:53,766] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 131000, global step 2098169: learning rate 0.0000
[2019-04-04 14:07:54,700] A3C_AGENT_WORKER-Thread-12 INFO:Local step 131000, global step 2098446: loss 0.0690
[2019-04-04 14:07:54,701] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 131000, global step 2098446: learning rate 0.0000
[2019-04-04 14:07:56,027] A3C_AGENT_WORKER-Thread-17 INFO:Local step 131500, global step 2098980: loss 0.0339
[2019-04-04 14:07:56,029] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 131500, global step 2098980: learning rate 0.0000
[2019-04-04 14:07:57,846] A3C_AGENT_WORKER-Thread-2 INFO:Local step 131500, global step 2099676: loss 0.0330
[2019-04-04 14:07:57,861] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 131500, global step 2099678: learning rate 0.0000
[2019-04-04 14:07:58,832] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-04 14:07:58,834] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 14:07:58,834] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:07:58,834] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 14:07:58,840] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:07:58,841] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 14:07:58,842] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:07:58,845] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run22
[2019-04-04 14:07:58,864] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run22
[2019-04-04 14:07:58,880] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run22
[2019-04-04 14:08:47,493] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.16778727], dtype=float32), 0.20203514]
[2019-04-04 14:08:47,493] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-5.0, 84.66666666666667, 0.0, 0.0, 26.0, 24.85938706059062, 0.2590799948638596, 0.0, 1.0, 45786.21052867754]
[2019-04-04 14:08:47,493] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 14:08:47,494] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.6315683e-09 2.2044466e-10 3.9093120e-15 3.5433763e-14 1.0000000e+00
 1.1948620e-10 5.9260953e-15], sampled 0.8541274602833588
[2019-04-04 14:09:37,500] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16778727], dtype=float32), 0.20203514]
[2019-04-04 14:09:37,500] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [5.021367788666668, 60.38710733333333, 0.0, 0.0, 26.0, 25.84645094575325, 0.4874484378336115, 0.0, 1.0, 0.0]
[2019-04-04 14:09:37,500] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 14:09:37,501] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.3482296e-09 2.0538454e-10 1.9311347e-15 1.9520145e-14 1.0000000e+00
 7.9239601e-11 3.1013688e-15], sampled 0.9285564925713412
[2019-04-04 14:09:39,704] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 14:09:58,857] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 14:10:02,872] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6735 275798561.6764 1233.0993
[2019-04-04 14:10:03,897] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 2100000, evaluation results [2100000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.673515826508, 275798561.67643094, 1233.0993326628943]
[2019-04-04 14:10:06,803] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.3268511e-09 6.5987837e-10 9.5730258e-15 8.0611150e-14 1.0000000e+00
 1.2985383e-09 2.1920398e-14], sum to 1.0000
[2019-04-04 14:10:06,803] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8522
[2019-04-04 14:10:06,812] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.2, 60.5, 0.0, 0.0, 26.0, 24.91214516404119, 0.2826125498883554, 0.0, 1.0, 41907.5882365092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2590200.0000, 
sim time next is 2590800.0000, 
raw observation next is [-4.3, 61.0, 0.0, 0.0, 26.0, 24.89252769421293, 0.2745799668202532, 0.0, 1.0, 41888.39473283564], 
processed observation next is [1.0, 1.0, 0.34349030470914127, 0.61, 0.0, 0.0, 0.6666666666666666, 0.5743773078510775, 0.5915266556067511, 0.0, 1.0, 0.19946854634683636], 
reward next is 0.8005, 
noisyNet noise sample is [array([0.22428223], dtype=float32), 2.2904522]. 
=============================================
[2019-04-04 14:10:07,241] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3352697e-07 1.6552399e-08 6.5055947e-13 4.4074670e-13 9.9999988e-01
 8.1620390e-09 1.6891738e-13], sum to 1.0000
[2019-04-04 14:10:07,245] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5406
[2019-04-04 14:10:07,266] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 54.0, 0.0, 0.0, 26.0, 24.82373501339787, 0.1303699978441772, 0.0, 1.0, 38706.77086051065], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2530800.0000, 
sim time next is 2531400.0000, 
raw observation next is [-2.8, 54.0, 0.0, 0.0, 26.0, 24.79765363059029, 0.1201437387683369, 0.0, 1.0, 38808.52509859225], 
processed observation next is [1.0, 0.30434782608695654, 0.38504155124653744, 0.54, 0.0, 0.0, 0.6666666666666666, 0.5664711358825242, 0.540047912922779, 0.0, 1.0, 0.1848025004694869], 
reward next is 0.8152, 
noisyNet noise sample is [array([0.12071159], dtype=float32), -0.67633784]. 
=============================================
[2019-04-04 14:10:07,574] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1308265e-09 3.3718850e-10 3.7437200e-15 5.5034318e-15 1.0000000e+00
 7.5634863e-12 9.7869247e-16], sum to 1.0000
[2019-04-04 14:10:07,576] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6289
[2019-04-04 14:10:07,598] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 51.0, 241.5, 71.5, 26.0, 25.22074535226704, 0.3205781990042434, 1.0, 1.0, 18696.98945455949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2293200.0000, 
sim time next is 2293800.0000, 
raw observation next is [-1.516666666666667, 50.0, 234.6666666666667, 70.66666666666667, 26.0, 25.24706514379746, 0.3251476190350691, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4205909510618652, 0.5, 0.7822222222222224, 0.07808471454880295, 0.6666666666666666, 0.603922095316455, 0.6083825396783563, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.90533614], dtype=float32), -1.2167046]. 
=============================================
[2019-04-04 14:10:08,726] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.8909073e-09 3.7886039e-10 5.6144372e-16 1.8619051e-13 1.0000000e+00
 5.1906372e-11 1.1578148e-14], sum to 1.0000
[2019-04-04 14:10:08,727] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7621
[2019-04-04 14:10:08,789] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.550000000000001, 82.5, 101.0, 39.0, 26.0, 25.59322600225234, 0.3054496590995077, 1.0, 1.0, 18737.60918250461], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2280600.0000, 
sim time next is 2281200.0000, 
raw observation next is [-7.266666666666667, 81.0, 113.8333333333333, 40.83333333333333, 26.0, 25.60375487134927, 0.3160806935622308, 1.0, 1.0, 18734.99220254585], 
processed observation next is [1.0, 0.391304347826087, 0.26131117266851345, 0.81, 0.3794444444444443, 0.04511970534069981, 0.6666666666666666, 0.6336462392791059, 0.6053602311874102, 1.0, 1.0, 0.08921424858355168], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.2131833], dtype=float32), -0.27180383]. 
=============================================
[2019-04-04 14:10:12,900] A3C_AGENT_WORKER-Thread-15 INFO:Local step 131500, global step 2103597: loss 0.0286
[2019-04-04 14:10:12,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 131500, global step 2103597: learning rate 0.0000
[2019-04-04 14:10:13,895] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4475214e-10 1.3629599e-10 3.0058809e-15 2.1761181e-14 1.0000000e+00
 2.2489204e-10 2.5812768e-15], sum to 1.0000
[2019-04-04 14:10:13,896] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8836
[2019-04-04 14:10:13,904] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 61.0, 0.0, 0.0, 26.0, 25.00356004434604, 0.3120401214039759, 0.0, 1.0, 38568.45856083541], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2330400.0000, 
sim time next is 2331000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.9542824599899, 0.3052051172338371, 0.0, 1.0, 38543.25728097511], 
processed observation next is [1.0, 1.0, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5795235383324918, 0.6017350390779457, 0.0, 1.0, 0.18353932038559576], 
reward next is 0.8165, 
noisyNet noise sample is [array([1.0495594], dtype=float32), 0.3013193]. 
=============================================
[2019-04-04 14:10:13,913] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[78.68936 ]
 [78.766045]
 [78.7039  ]
 [78.81814 ]
 [78.71695 ]], R is [[78.74646759]
 [78.77534485]
 [78.80380249]
 [78.83171082]
 [78.85869598]].
[2019-04-04 14:10:14,244] A3C_AGENT_WORKER-Thread-11 INFO:Local step 131500, global step 2104140: loss 0.0225
[2019-04-04 14:10:14,244] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 131500, global step 2104140: learning rate 0.0000
[2019-04-04 14:10:14,531] A3C_AGENT_WORKER-Thread-16 INFO:Local step 131500, global step 2104256: loss 0.0202
[2019-04-04 14:10:14,531] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 131500, global step 2104256: learning rate 0.0000
[2019-04-04 14:10:15,660] A3C_AGENT_WORKER-Thread-14 INFO:Local step 131500, global step 2104652: loss 0.0184
[2019-04-04 14:10:15,660] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 131500, global step 2104652: learning rate 0.0000
[2019-04-04 14:10:16,129] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132000, global step 2104849: loss 0.0146
[2019-04-04 14:10:16,130] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132000, global step 2104849: learning rate 0.0000
[2019-04-04 14:10:16,515] A3C_AGENT_WORKER-Thread-3 INFO:Local step 131500, global step 2105004: loss 0.0248
[2019-04-04 14:10:16,515] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 131500, global step 2105004: learning rate 0.0000
[2019-04-04 14:10:16,817] A3C_AGENT_WORKER-Thread-20 INFO:Local step 131500, global step 2105123: loss 0.0174
[2019-04-04 14:10:16,818] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 131500, global step 2105123: learning rate 0.0000
[2019-04-04 14:10:16,841] A3C_AGENT_WORKER-Thread-18 INFO:Local step 131500, global step 2105129: loss 0.0233
[2019-04-04 14:10:16,843] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 131500, global step 2105129: learning rate 0.0000
[2019-04-04 14:10:17,512] A3C_AGENT_WORKER-Thread-19 INFO:Local step 131500, global step 2105420: loss 0.0230
[2019-04-04 14:10:17,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 131500, global step 2105421: learning rate 0.0000
[2019-04-04 14:10:18,978] A3C_AGENT_WORKER-Thread-5 INFO:Local step 131500, global step 2106035: loss 0.0220
[2019-04-04 14:10:18,978] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 131500, global step 2106035: learning rate 0.0000
[2019-04-04 14:10:19,100] A3C_AGENT_WORKER-Thread-10 INFO:Local step 131500, global step 2106093: loss 0.0170
[2019-04-04 14:10:19,101] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 131500, global step 2106093: learning rate 0.0000
[2019-04-04 14:10:19,255] A3C_AGENT_WORKER-Thread-4 INFO:Local step 131500, global step 2106167: loss 0.0167
[2019-04-04 14:10:19,265] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 131500, global step 2106167: learning rate 0.0000
[2019-04-04 14:10:19,914] A3C_AGENT_WORKER-Thread-6 INFO:Local step 131500, global step 2106466: loss 0.0211
[2019-04-04 14:10:19,915] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 131500, global step 2106466: learning rate 0.0000
[2019-04-04 14:10:20,214] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132000, global step 2106599: loss 0.0091
[2019-04-04 14:10:20,217] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132000, global step 2106599: learning rate 0.0000
[2019-04-04 14:10:20,699] A3C_AGENT_WORKER-Thread-12 INFO:Local step 131500, global step 2106796: loss 0.0218
[2019-04-04 14:10:20,711] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 131500, global step 2106799: learning rate 0.0000
[2019-04-04 14:10:21,340] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132000, global step 2107072: loss 0.0103
[2019-04-04 14:10:21,341] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132000, global step 2107073: learning rate 0.0000
[2019-04-04 14:10:22,405] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3584635e-08 5.6990740e-10 8.4673945e-15 5.9035574e-14 1.0000000e+00
 3.8519446e-10 2.7049033e-14], sum to 1.0000
[2019-04-04 14:10:22,405] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4388
[2019-04-04 14:10:22,414] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.016666666666667, 31.5, 202.3333333333333, 175.3333333333333, 26.0, 25.66112831902601, 0.3144622173999219, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2551800.0000, 
sim time next is 2552400.0000, 
raw observation next is [2.2, 30.0, 194.5, 252.0, 26.0, 25.59140749155384, 0.2982492460361343, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5235457063711911, 0.3, 0.6483333333333333, 0.27845303867403315, 0.6666666666666666, 0.6326172909628202, 0.5994164153453782, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6726768], dtype=float32), 0.49501783]. 
=============================================
[2019-04-04 14:10:23,499] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4170851e-08 1.9343700e-09 8.7494853e-15 2.3982097e-13 1.0000000e+00
 2.2151787e-09 2.2381365e-14], sum to 1.0000
[2019-04-04 14:10:23,508] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1733
[2019-04-04 14:10:23,524] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.80873738397342, 0.237053059124621, 0.0, 1.0, 41810.85104890822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2771400.0000, 
sim time next is 2772000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.76105796464427, 0.2224831998535401, 0.0, 1.0, 41709.76996661215], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5634214970536892, 0.5741610666178467, 0.0, 1.0, 0.1986179522219626], 
reward next is 0.8014, 
noisyNet noise sample is [array([-2.5346837], dtype=float32), 1.0750552]. 
=============================================
[2019-04-04 14:10:23,530] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[77.47737 ]
 [77.25628 ]
 [77.318535]
 [77.32283 ]
 [77.036064]], R is [[77.39754486]
 [77.42447662]
 [77.45051575]
 [77.47566223]
 [77.49986267]].
[2019-04-04 14:10:25,759] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6474281e-09 1.9312760e-10 1.3211367e-14 1.1630605e-14 1.0000000e+00
 7.3424981e-11 3.7170241e-15], sum to 1.0000
[2019-04-04 14:10:25,760] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1312
[2019-04-04 14:10:25,774] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.399999999999999, 61.5, 0.0, 0.0, 26.0, 24.85116935409194, 0.2644473503477804, 0.0, 1.0, 41886.93867204994], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2591400.0000, 
sim time next is 2592000.0000, 
raw observation next is [-4.5, 62.0, 0.0, 0.0, 26.0, 24.80601395099826, 0.2561451597010266, 0.0, 1.0, 41898.09662120678], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.62, 0.0, 0.0, 0.6666666666666666, 0.567167829249855, 0.5853817199003423, 0.0, 1.0, 0.19951474581527037], 
reward next is 0.8005, 
noisyNet noise sample is [array([2.694119], dtype=float32), -0.93298435]. 
=============================================
[2019-04-04 14:10:25,814] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[79.43854 ]
 [79.45926 ]
 [79.50522 ]
 [79.505424]
 [79.53632 ]], R is [[78.99319458]
 [79.00379944]
 [79.01428986]
 [79.02458191]
 [79.03456116]].
[2019-04-04 14:10:29,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1052314e-10 9.6725461e-10 1.0955491e-14 6.9750063e-15 1.0000000e+00
 4.6348345e-10 3.9526936e-15], sum to 1.0000
[2019-04-04 14:10:29,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5818
[2019-04-04 14:10:29,045] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1333333333333333, 44.33333333333333, 172.6666666666667, 197.5, 26.0, 26.31448595858363, 0.5290896416875615, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2641200.0000, 
sim time next is 2641800.0000, 
raw observation next is [0.3166666666666667, 43.66666666666666, 181.3333333333333, 184.0, 26.0, 26.39272986476748, 0.5379487524911645, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.47137580794090495, 0.4366666666666666, 0.6044444444444443, 0.20331491712707184, 0.6666666666666666, 0.6993941553972901, 0.6793162508303882, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5424465], dtype=float32), -1.0554546]. 
=============================================
[2019-04-04 14:10:31,739] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132000, global step 2111789: loss 0.0080
[2019-04-04 14:10:31,741] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132000, global step 2111789: learning rate 0.0000
[2019-04-04 14:10:32,351] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132000, global step 2112080: loss 0.0069
[2019-04-04 14:10:32,351] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132000, global step 2112080: learning rate 0.0000
[2019-04-04 14:10:33,096] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132000, global step 2112351: loss 0.0069
[2019-04-04 14:10:33,097] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132000, global step 2112351: learning rate 0.0000
[2019-04-04 14:10:33,836] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132000, global step 2112652: loss 0.0072
[2019-04-04 14:10:33,842] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132000, global step 2112652: learning rate 0.0000
[2019-04-04 14:10:34,325] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132500, global step 2112889: loss 0.0095
[2019-04-04 14:10:34,327] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132500, global step 2112890: learning rate 0.0000
[2019-04-04 14:10:35,013] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132000, global step 2113186: loss 0.0080
[2019-04-04 14:10:35,013] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132000, global step 2113186: learning rate 0.0000
[2019-04-04 14:10:35,485] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132000, global step 2113367: loss 0.0073
[2019-04-04 14:10:35,486] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132000, global step 2113367: learning rate 0.0000
[2019-04-04 14:10:35,494] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132000, global step 2113368: loss 0.0075
[2019-04-04 14:10:35,494] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132000, global step 2113368: learning rate 0.0000
[2019-04-04 14:10:35,669] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132000, global step 2113429: loss 0.0076
[2019-04-04 14:10:35,669] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132000, global step 2113429: learning rate 0.0000
[2019-04-04 14:10:36,896] A3C_AGENT_WORKER-Thread-5 INFO:Local step 132000, global step 2113862: loss 0.0089
[2019-04-04 14:10:36,901] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 132000, global step 2113864: learning rate 0.0000
[2019-04-04 14:10:37,654] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132000, global step 2114198: loss 0.0062
[2019-04-04 14:10:37,656] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132000, global step 2114198: learning rate 0.0000
[2019-04-04 14:10:37,953] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2114344: loss 0.0115
[2019-04-04 14:10:37,954] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2114344: learning rate 0.0000
[2019-04-04 14:10:37,967] A3C_AGENT_WORKER-Thread-4 INFO:Local step 132000, global step 2114348: loss 0.0075
[2019-04-04 14:10:37,969] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 132000, global step 2114351: learning rate 0.0000
[2019-04-04 14:10:38,027] A3C_AGENT_WORKER-Thread-6 INFO:Local step 132000, global step 2114376: loss 0.0067
[2019-04-04 14:10:38,027] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 132000, global step 2114376: learning rate 0.0000
[2019-04-04 14:10:39,375] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2114958: loss 0.0086
[2019-04-04 14:10:39,379] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2114958: learning rate 0.0000
[2019-04-04 14:10:39,529] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132000, global step 2115031: loss 0.0063
[2019-04-04 14:10:39,530] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132000, global step 2115031: learning rate 0.0000
[2019-04-04 14:10:40,508] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2465919e-09 1.8246246e-10 3.9355116e-15 5.9289697e-14 1.0000000e+00
 1.3726570e-10 1.9725085e-15], sum to 1.0000
[2019-04-04 14:10:40,509] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0141
[2019-04-04 14:10:40,523] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 26.0, 25.20929204782011, 0.3546035895142817, 0.0, 1.0, 46079.42479346092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2847600.0000, 
sim time next is 2848200.0000, 
raw observation next is [1.833333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 25.21311329681663, 0.353324291975483, 0.0, 1.0, 43999.38419926983], 
processed observation next is [1.0, 1.0, 0.5133887349953832, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6010927747347191, 0.6177747639918277, 0.0, 1.0, 0.20952087713938014], 
reward next is 0.7905, 
noisyNet noise sample is [array([0.41698], dtype=float32), -0.022585448]. 
=============================================
[2019-04-04 14:10:50,010] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2119917: loss 0.0128
[2019-04-04 14:10:50,011] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2119919: learning rate 0.0000
[2019-04-04 14:10:50,025] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133000, global step 2119925: loss 0.0961
[2019-04-04 14:10:50,026] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133000, global step 2119925: learning rate 0.0000
[2019-04-04 14:10:50,593] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132500, global step 2120196: loss 0.0088
[2019-04-04 14:10:50,594] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132500, global step 2120197: learning rate 0.0000
[2019-04-04 14:10:50,645] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4335725e-09 3.2344002e-11 1.0305554e-15 4.1914405e-15 1.0000000e+00
 9.4631761e-11 1.5673496e-16], sum to 1.0000
[2019-04-04 14:10:50,647] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0163
[2019-04-04 14:10:50,656] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 73.0, 17.33333333333333, 171.8333333333333, 26.0, 26.45365457940749, 0.6222183359480268, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3259200.0000, 
sim time next is 3259800.0000, 
raw observation next is [-4.0, 71.0, 9.0, 104.0, 26.0, 26.12885382479114, 0.6050779709533128, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.71, 0.03, 0.11491712707182321, 0.6666666666666666, 0.6774044853992617, 0.7016926569844376, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1203378], dtype=float32), 0.047805034]. 
=============================================
[2019-04-04 14:10:51,471] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2120577: loss 0.0144
[2019-04-04 14:10:51,472] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2120577: learning rate 0.0000
[2019-04-04 14:10:52,214] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2120879: loss 0.0163
[2019-04-04 14:10:52,215] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2120879: learning rate 0.0000
[2019-04-04 14:10:52,889] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2121156: loss 0.0155
[2019-04-04 14:10:52,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2121156: learning rate 0.0000
[2019-04-04 14:10:53,646] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2121481: loss 0.0149
[2019-04-04 14:10:53,652] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2121483: learning rate 0.0000
[2019-04-04 14:10:53,665] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2121486: loss 0.0980
[2019-04-04 14:10:53,667] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2121487: learning rate 0.0000
[2019-04-04 14:10:53,786] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2121537: loss 0.0151
[2019-04-04 14:10:53,787] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2121537: learning rate 0.0000
[2019-04-04 14:10:53,826] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2121551: loss 0.0180
[2019-04-04 14:10:53,827] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2121551: learning rate 0.0000
[2019-04-04 14:10:55,221] A3C_AGENT_WORKER-Thread-5 INFO:Local step 132500, global step 2122199: loss 0.0133
[2019-04-04 14:10:55,226] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 132500, global step 2122200: learning rate 0.0000
[2019-04-04 14:10:55,261] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2122213: loss 0.1067
[2019-04-04 14:10:55,263] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2122215: learning rate 0.0000
[2019-04-04 14:10:55,710] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2122445: loss 0.0121
[2019-04-04 14:10:55,710] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2122445: learning rate 0.0000
[2019-04-04 14:10:55,793] A3C_AGENT_WORKER-Thread-4 INFO:Local step 132500, global step 2122493: loss 0.0124
[2019-04-04 14:10:55,793] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 132500, global step 2122493: learning rate 0.0000
[2019-04-04 14:10:55,960] A3C_AGENT_WORKER-Thread-6 INFO:Local step 132500, global step 2122578: loss 0.0096
[2019-04-04 14:10:55,964] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 132500, global step 2122580: learning rate 0.0000
[2019-04-04 14:10:56,636] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0498433e-08 8.8490193e-09 6.7016115e-14 1.6531125e-13 1.0000000e+00
 6.5981792e-10 2.3722644e-13], sum to 1.0000
[2019-04-04 14:10:56,639] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1136
[2019-04-04 14:10:56,675] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1666666666666667, 39.16666666666666, 89.0, 707.0, 26.0, 25.11040482432199, 0.3644961581491949, 0.0, 1.0, 18700.86234736077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3078600.0000, 
sim time next is 3079200.0000, 
raw observation next is [0.3333333333333333, 39.33333333333334, 86.5, 690.0, 26.0, 25.11491235184653, 0.3661562496453227, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.4718374884579871, 0.3933333333333334, 0.28833333333333333, 0.7624309392265194, 0.6666666666666666, 0.5929093626538776, 0.6220520832151076, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.461361], dtype=float32), 0.94636226]. 
=============================================
[2019-04-04 14:10:57,003] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132500, global step 2123108: loss 0.0141
[2019-04-04 14:10:57,004] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132500, global step 2123108: learning rate 0.0000
[2019-04-04 14:10:57,934] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.0345164e-08 3.2958507e-09 2.3373562e-14 2.3040760e-13 1.0000000e+00
 2.1073538e-09 3.4757560e-14], sum to 1.0000
[2019-04-04 14:10:57,935] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7037
[2019-04-04 14:10:57,951] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 40.5, 99.0, 775.0, 26.0, 25.11443049952329, 0.3633826185924978, 0.0, 1.0, 18704.79652051203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3076200.0000, 
sim time next is 3076800.0000, 
raw observation next is [-0.3333333333333334, 40.0, 96.5, 758.0, 26.0, 25.11631530750642, 0.3630074511325337, 0.0, 1.0, 18703.96295320016], 
processed observation next is [0.0, 0.6086956521739131, 0.4533702677747, 0.4, 0.32166666666666666, 0.8375690607734807, 0.6666666666666666, 0.5930262756255349, 0.6210024837108445, 0.0, 1.0, 0.0890664902533341], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.8867135], dtype=float32), -0.26674116]. 
=============================================
[2019-04-04 14:11:02,898] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8750457e-11 8.3320225e-14 1.7959044e-18 2.3709342e-17 1.0000000e+00
 5.4269680e-13 2.4795560e-18], sum to 1.0000
[2019-04-04 14:11:02,898] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2585
[2019-04-04 14:11:02,903] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.333333333333333, 100.0, 0.0, 0.0, 26.0, 26.05818267898679, 0.7036096481318066, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3181200.0000, 
sim time next is 3181800.0000, 
raw observation next is [3.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.93355090874136, 0.6830744519518244, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5503231763619576, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6611292423951133, 0.7276914839839415, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0688775], dtype=float32), 2.134363]. 
=============================================
[2019-04-04 14:11:04,755] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133500, global step 2127499: loss 1.3002
[2019-04-04 14:11:04,756] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133500, global step 2127499: learning rate 0.0000
[2019-04-04 14:11:05,319] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2127771: loss 0.1012
[2019-04-04 14:11:05,320] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2127771: learning rate 0.0000
[2019-04-04 14:11:05,846] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133000, global step 2128046: loss 0.0987
[2019-04-04 14:11:05,847] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133000, global step 2128048: learning rate 0.0000
[2019-04-04 14:11:06,990] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2128646: loss 0.0909
[2019-04-04 14:11:06,991] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2128646: learning rate 0.0000
[2019-04-04 14:11:07,121] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2128718: loss 0.1115
[2019-04-04 14:11:07,123] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2128718: learning rate 0.0000
[2019-04-04 14:11:08,324] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2129328: loss 0.0997
[2019-04-04 14:11:08,326] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2129328: learning rate 0.0000
[2019-04-04 14:11:08,546] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2129459: loss 1.3275
[2019-04-04 14:11:08,547] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2129459: learning rate 0.0000
[2019-04-04 14:11:08,864] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2129629: loss 0.1035
[2019-04-04 14:11:08,866] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2129629: learning rate 0.0000
[2019-04-04 14:11:09,250] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2129811: loss 0.1060
[2019-04-04 14:11:09,252] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2129811: learning rate 0.0000
[2019-04-04 14:11:09,712] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2130036: loss 0.1007
[2019-04-04 14:11:09,713] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2130036: learning rate 0.0000
[2019-04-04 14:11:10,135] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2130250: loss 1.3275
[2019-04-04 14:11:10,144] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2130254: learning rate 0.0000
[2019-04-04 14:11:10,525] A3C_AGENT_WORKER-Thread-4 INFO:Local step 133000, global step 2130482: loss 0.0989
[2019-04-04 14:11:10,525] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 133000, global step 2130483: learning rate 0.0000
[2019-04-04 14:11:10,569] A3C_AGENT_WORKER-Thread-5 INFO:Local step 133000, global step 2130501: loss 0.0955
[2019-04-04 14:11:10,572] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 133000, global step 2130501: learning rate 0.0000
[2019-04-04 14:11:10,935] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2130702: loss 0.0988
[2019-04-04 14:11:10,937] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2130702: learning rate 0.0000
[2019-04-04 14:11:11,409] A3C_AGENT_WORKER-Thread-6 INFO:Local step 133000, global step 2130961: loss 0.0932
[2019-04-04 14:11:11,411] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 133000, global step 2130962: learning rate 0.0000
[2019-04-04 14:11:11,635] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.75609746e-09 8.26770374e-10 6.02435667e-15 1.10199407e-13
 1.00000000e+00 2.66998673e-10 3.14460880e-14], sum to 1.0000
[2019-04-04 14:11:11,635] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6814
[2019-04-04 14:11:11,650] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.5, 74.0, 0.0, 0.0, 26.0, 24.83594507660889, 0.2562206750145548, 0.0, 1.0, 41826.74450961807], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3385800.0000, 
sim time next is 3386400.0000, 
raw observation next is [-5.333333333333333, 73.0, 0.0, 0.0, 26.0, 24.8137801307313, 0.2444386726910734, 0.0, 1.0, 41986.5454498295], 
processed observation next is [1.0, 0.17391304347826086, 0.3148661126500462, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5678150108942749, 0.5814795575636912, 0.0, 1.0, 0.19993593071347382], 
reward next is 0.8001, 
noisyNet noise sample is [array([-1.7979631], dtype=float32), 1.3637713]. 
=============================================
[2019-04-04 14:11:12,245] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133000, global step 2131434: loss 0.0940
[2019-04-04 14:11:12,246] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133000, global step 2131435: learning rate 0.0000
[2019-04-04 14:11:18,045] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3815949e-09 1.8365028e-10 3.5420771e-15 3.7101412e-14 1.0000000e+00
 1.4685143e-10 9.8074108e-15], sum to 1.0000
[2019-04-04 14:11:18,046] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4073
[2019-04-04 14:11:18,086] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 54.66666666666667, 114.6666666666667, 817.1666666666666, 26.0, 25.14814601620839, 0.4418916090341927, 0.0, 1.0, 45381.06728484336], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3584400.0000, 
sim time next is 3585000.0000, 
raw observation next is [-3.166666666666667, 54.83333333333334, 115.3333333333333, 818.3333333333334, 26.0, 25.17215342826735, 0.443948947712318, 0.0, 1.0, 18712.8915262109], 
processed observation next is [0.0, 0.4782608695652174, 0.3748845798707295, 0.5483333333333335, 0.3844444444444443, 0.9042357274401474, 0.6666666666666666, 0.5976794523556125, 0.6479829825707727, 0.0, 1.0, 0.08910900726767096], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.6999468], dtype=float32), 0.18458477]. 
=============================================
[2019-04-04 14:11:18,097] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[79.395836]
 [79.35818 ]
 [79.394394]
 [79.58863 ]
 [79.90681 ]], R is [[79.43569946]
 [79.42523956]
 [79.46920776]
 [79.5854187 ]
 [79.78956604]].
[2019-04-04 14:11:18,151] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5643506e-09 8.3918927e-11 1.6132457e-16 7.3713474e-15 1.0000000e+00
 7.3842807e-11 7.1686029e-16], sum to 1.0000
[2019-04-04 14:11:18,154] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0975
[2019-04-04 14:11:18,161] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 48.0, 108.3333333333333, 796.0, 26.0, 26.5024438199111, 0.6999219954270933, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3852600.0000, 
sim time next is 3853200.0000, 
raw observation next is [2.0, 48.0, 107.1666666666667, 789.0, 26.0, 26.68150953648774, 0.7203686286834277, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.35722222222222233, 0.8718232044198895, 0.6666666666666666, 0.7234591280406452, 0.7401228762278093, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19037452], dtype=float32), 0.5364339]. 
=============================================
[2019-04-04 14:11:18,528] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134000, global step 2134919: loss 0.2993
[2019-04-04 14:11:18,529] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134000, global step 2134919: learning rate 0.0000
[2019-04-04 14:11:19,946] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2135737: loss 1.2941
[2019-04-04 14:11:19,949] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2135737: learning rate 0.0000
[2019-04-04 14:11:20,159] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133500, global step 2135860: loss 1.2936
[2019-04-04 14:11:20,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133500, global step 2135860: learning rate 0.0000
[2019-04-04 14:11:21,521] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2136639: loss 1.2946
[2019-04-04 14:11:21,523] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2136640: learning rate 0.0000
[2019-04-04 14:11:21,717] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2136752: loss 1.2748
[2019-04-04 14:11:21,730] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2136754: learning rate 0.0000
[2019-04-04 14:11:22,039] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2136940: loss 0.0866
[2019-04-04 14:11:22,040] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2136940: learning rate 0.0000
[2019-04-04 14:11:22,881] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.1738094e-09 1.1953891e-09 1.6957066e-14 2.2387567e-13 1.0000000e+00
 4.1499582e-10 1.9270120e-13], sum to 1.0000
[2019-04-04 14:11:22,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4132
[2019-04-04 14:11:22,893] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.49356618068028, 0.3703431666527917, 0.0, 1.0, 35239.8414951219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3634800.0000, 
sim time next is 3635400.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.50140525775011, 0.3705212414496584, 0.0, 1.0, 27981.31026770125], 
processed observation next is [0.0, 0.043478260869565216, 0.7119113573407203, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6251171048125093, 0.6235070804832195, 0.0, 1.0, 0.1332443346081012], 
reward next is 0.8668, 
noisyNet noise sample is [array([-1.8063525], dtype=float32), 0.17807779]. 
=============================================
[2019-04-04 14:11:22,921] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2137387: loss 1.2571
[2019-04-04 14:11:22,922] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2137387: learning rate 0.0000
[2019-04-04 14:11:23,415] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2137654: loss 0.2731
[2019-04-04 14:11:23,416] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2137654: learning rate 0.0000
[2019-04-04 14:11:23,634] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2137774: loss 1.2540
[2019-04-04 14:11:23,635] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2137776: learning rate 0.0000
[2019-04-04 14:11:23,916] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2137937: loss 1.2472
[2019-04-04 14:11:23,917] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2137937: learning rate 0.0000
[2019-04-04 14:11:24,114] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2138060: loss 1.2598
[2019-04-04 14:11:24,117] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2138060: learning rate 0.0000
[2019-04-04 14:11:25,298] A3C_AGENT_WORKER-Thread-4 INFO:Local step 133500, global step 2138727: loss 1.2522
[2019-04-04 14:11:25,302] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 133500, global step 2138728: learning rate 0.0000
[2019-04-04 14:11:25,568] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7585419e-09 2.5353057e-09 3.5598372e-14 1.3489891e-13 1.0000000e+00
 1.5563560e-10 9.4900575e-14], sum to 1.0000
[2019-04-04 14:11:25,570] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9186
[2019-04-04 14:11:25,583] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.5, 46.0, 0.0, 0.0, 26.0, 25.41743510637882, 0.3914754121291195, 0.0, 1.0, 58923.60321642882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3619800.0000, 
sim time next is 3620400.0000, 
raw observation next is [-1.666666666666667, 47.33333333333333, 0.0, 0.0, 26.0, 25.39988581435511, 0.3907567922256303, 0.0, 1.0, 54854.63546977209], 
processed observation next is [0.0, 0.9130434782608695, 0.4164358264081256, 0.4733333333333333, 0.0, 0.0, 0.6666666666666666, 0.6166571511962591, 0.6302522640752101, 0.0, 1.0, 0.2612125498560576], 
reward next is 0.7388, 
noisyNet noise sample is [array([0.7223325], dtype=float32), -1.2635534]. 
=============================================
[2019-04-04 14:11:25,687] A3C_AGENT_WORKER-Thread-5 INFO:Local step 133500, global step 2138983: loss 1.2375
[2019-04-04 14:11:25,688] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 133500, global step 2138984: learning rate 0.0000
[2019-04-04 14:11:25,957] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2139146: loss 1.2419
[2019-04-04 14:11:25,959] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2139147: learning rate 0.0000
[2019-04-04 14:11:26,263] A3C_AGENT_WORKER-Thread-6 INFO:Local step 133500, global step 2139318: loss 1.2434
[2019-04-04 14:11:26,273] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 133500, global step 2139326: learning rate 0.0000
[2019-04-04 14:11:26,468] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133500, global step 2139428: loss 1.2357
[2019-04-04 14:11:26,470] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133500, global step 2139429: learning rate 0.0000
[2019-04-04 14:11:29,855] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1317657e-09 2.3006716e-10 3.2817519e-15 7.0651717e-15 1.0000000e+00
 1.4928497e-10 2.1416849e-15], sum to 1.0000
[2019-04-04 14:11:29,858] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0444
[2019-04-04 14:11:29,897] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 73.0, 19.0, 184.8333333333333, 26.0, 25.20968755229383, 0.3100796989575785, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3742800.0000, 
sim time next is 3743400.0000, 
raw observation next is [-4.0, 72.0, 32.99999999999999, 233.6666666666666, 26.0, 25.25838864699619, 0.3102797900131691, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3518005540166205, 0.72, 0.10999999999999997, 0.25819521178637195, 0.6666666666666666, 0.6048657205830157, 0.6034265966710564, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0135843], dtype=float32), -0.19872092]. 
=============================================
[2019-04-04 14:11:32,102] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5144187e-10 6.4519223e-11 9.8754311e-16 1.9297354e-15 1.0000000e+00
 4.4227649e-11 1.2590652e-15], sum to 1.0000
[2019-04-04 14:11:32,105] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0634
[2019-04-04 14:11:32,116] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.0, 96.5, 743.5, 26.0, 26.72304438214999, 0.6784529305272439, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3769200.0000, 
sim time next is 3769800.0000, 
raw observation next is [0.0, 60.0, 93.33333333333334, 732.6666666666667, 26.0, 26.75971001514225, 0.688528672702239, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.3111111111111111, 0.8095764272559853, 0.6666666666666666, 0.7299758345951876, 0.729509557567413, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27898997], dtype=float32), -0.2929729]. 
=============================================
[2019-04-04 14:11:34,114] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2143532: loss 0.0702
[2019-04-04 14:11:34,115] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2143533: learning rate 0.0000
[2019-04-04 14:11:34,218] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134500, global step 2143593: loss 0.0584
[2019-04-04 14:11:34,222] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134500, global step 2143593: learning rate 0.0000
[2019-04-04 14:11:34,421] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134000, global step 2143700: loss 0.0664
[2019-04-04 14:11:34,424] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134000, global step 2143700: learning rate 0.0000
[2019-04-04 14:11:35,888] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2144375: loss 0.0748
[2019-04-04 14:11:35,891] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2144375: learning rate 0.0000
[2019-04-04 14:11:36,317] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2144614: loss 0.2658
[2019-04-04 14:11:36,321] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2144614: learning rate 0.0000
[2019-04-04 14:11:37,305] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2145171: loss 0.2654
[2019-04-04 14:11:37,306] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2145171: learning rate 0.0000
[2019-04-04 14:11:37,939] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2145521: loss 0.0731
[2019-04-04 14:11:37,941] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2145522: learning rate 0.0000
[2019-04-04 14:11:38,128] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2145614: loss 0.0569
[2019-04-04 14:11:38,129] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2145615: learning rate 0.0000
[2019-04-04 14:11:38,400] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2145767: loss 0.2716
[2019-04-04 14:11:38,403] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2145767: learning rate 0.0000
[2019-04-04 14:11:38,652] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2145891: loss 0.2790
[2019-04-04 14:11:38,653] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2145891: learning rate 0.0000
[2019-04-04 14:11:39,426] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2146262: loss 0.0558
[2019-04-04 14:11:39,427] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2146262: learning rate 0.0000
[2019-04-04 14:11:39,473] A3C_AGENT_WORKER-Thread-4 INFO:Local step 134000, global step 2146281: loss 0.2701
[2019-04-04 14:11:39,473] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 134000, global step 2146281: learning rate 0.0000
[2019-04-04 14:11:39,991] A3C_AGENT_WORKER-Thread-5 INFO:Local step 134000, global step 2146528: loss 0.0764
[2019-04-04 14:11:39,993] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 134000, global step 2146528: learning rate 0.0000
[2019-04-04 14:11:40,231] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2146650: loss 0.2566
[2019-04-04 14:11:40,232] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2146650: learning rate 0.0000
[2019-04-04 14:11:40,714] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134000, global step 2146893: loss 0.0802
[2019-04-04 14:11:40,716] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134000, global step 2146895: learning rate 0.0000
[2019-04-04 14:11:40,784] A3C_AGENT_WORKER-Thread-6 INFO:Local step 134000, global step 2146924: loss 0.2654
[2019-04-04 14:11:40,786] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 134000, global step 2146924: learning rate 0.0000
[2019-04-04 14:11:43,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.8437593e-09 6.9138567e-10 3.5779580e-14 9.1869583e-15 1.0000000e+00
 2.5431018e-10 5.0294309e-15], sum to 1.0000
[2019-04-04 14:11:43,046] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9487
[2019-04-04 14:11:43,073] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.5, 23.0, 67.0, 548.0, 26.0, 25.86443621311437, 0.6311043471999677, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4033800.0000, 
sim time next is 4034400.0000, 
raw observation next is [-1.666666666666667, 23.33333333333334, 59.16666666666667, 488.8333333333334, 26.0, 26.31124021279003, 0.6763062182722073, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4164358264081256, 0.2333333333333334, 0.19722222222222224, 0.5401473296500922, 0.6666666666666666, 0.6926033510658357, 0.7254354060907358, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2458272], dtype=float32), 1.5333986]. 
=============================================
[2019-04-04 14:11:47,490] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135000, global step 2150469: loss 1.3311
[2019-04-04 14:11:47,491] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135000, global step 2150469: learning rate 0.0000
[2019-04-04 14:11:49,447] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134500, global step 2151551: loss 0.0619
[2019-04-04 14:11:49,448] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134500, global step 2151551: learning rate 0.0000
[2019-04-04 14:11:49,885] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2151805: loss 0.0608
[2019-04-04 14:11:49,887] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2151805: learning rate 0.0000
[2019-04-04 14:11:50,227] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3607118e-08 5.2305249e-09 3.0309258e-14 4.5755121e-13 1.0000000e+00
 7.2730749e-10 1.7122569e-13], sum to 1.0000
[2019-04-04 14:11:50,229] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4152
[2019-04-04 14:11:50,244] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 39.5, 0.0, 0.0, 26.0, 25.4438640481191, 0.4323147635321296, 0.0, 1.0, 18762.2501754726], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4150200.0000, 
sim time next is 4150800.0000, 
raw observation next is [-1.0, 39.0, 0.0, 0.0, 26.0, 25.42321543683164, 0.422688266580518, 0.0, 1.0, 30961.44162364881], 
processed observation next is [0.0, 0.043478260869565216, 0.4349030470914128, 0.39, 0.0, 0.0, 0.6666666666666666, 0.6186012864026367, 0.6408960888601727, 0.0, 1.0, 0.14743543630308958], 
reward next is 0.8526, 
noisyNet noise sample is [array([0.24950217], dtype=float32), -0.31985307]. 
=============================================
[2019-04-04 14:11:51,035] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2152403: loss 1.3228
[2019-04-04 14:11:51,036] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2152404: learning rate 0.0000
[2019-04-04 14:11:51,258] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2152515: loss 0.0636
[2019-04-04 14:11:51,259] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2152515: learning rate 0.0000
[2019-04-04 14:11:52,060] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2152976: loss 0.0623
[2019-04-04 14:11:52,061] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2152976: learning rate 0.0000
[2019-04-04 14:11:52,842] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2153423: loss 1.3196
[2019-04-04 14:11:52,846] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2153425: learning rate 0.0000
[2019-04-04 14:11:53,124] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2153568: loss 0.0579
[2019-04-04 14:11:53,125] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2153568: learning rate 0.0000
[2019-04-04 14:11:53,419] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2153713: loss 0.0652
[2019-04-04 14:11:53,424] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2153715: learning rate 0.0000
[2019-04-04 14:11:53,958] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2153987: loss 0.0605
[2019-04-04 14:11:53,961] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2153990: learning rate 0.0000
[2019-04-04 14:11:54,516] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2154310: loss 0.0609
[2019-04-04 14:11:54,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2154310: learning rate 0.0000
[2019-04-04 14:11:55,292] A3C_AGENT_WORKER-Thread-4 INFO:Local step 134500, global step 2154728: loss 0.0669
[2019-04-04 14:11:55,293] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 134500, global step 2154728: learning rate 0.0000
[2019-04-04 14:11:56,015] A3C_AGENT_WORKER-Thread-5 INFO:Local step 134500, global step 2155137: loss 0.0661
[2019-04-04 14:11:56,016] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 134500, global step 2155138: learning rate 0.0000
[2019-04-04 14:11:56,250] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134500, global step 2155276: loss 0.0708
[2019-04-04 14:11:56,251] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134500, global step 2155276: learning rate 0.0000
[2019-04-04 14:11:56,333] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2155318: loss 0.0658
[2019-04-04 14:11:56,335] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2155318: learning rate 0.0000
[2019-04-04 14:11:56,581] A3C_AGENT_WORKER-Thread-6 INFO:Local step 134500, global step 2155464: loss 0.0619
[2019-04-04 14:11:56,583] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 134500, global step 2155465: learning rate 0.0000
[2019-04-04 14:11:59,272] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.8430295e-11 1.7504914e-11 3.3346223e-18 2.6095105e-17 1.0000000e+00
 1.4287933e-12 2.3945655e-18], sum to 1.0000
[2019-04-04 14:11:59,273] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4257
[2019-04-04 14:11:59,285] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.56666666666667, 29.66666666666667, 115.5, 843.8333333333334, 26.0, 27.49145432974843, 0.9959787286129412, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4368000.0000, 
sim time next is 4368600.0000, 
raw observation next is [14.55, 30.0, 115.0, 842.0, 26.0, 27.8984589068453, 1.048981002225204, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.865650969529086, 0.3, 0.38333333333333336, 0.9303867403314917, 0.6666666666666666, 0.8248715755704416, 0.849660334075068, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2225251], dtype=float32), 0.03439836]. 
=============================================
[2019-04-04 14:12:00,248] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0613856e-09 9.5076461e-11 4.9612292e-16 4.0653821e-15 1.0000000e+00
 2.1852303e-11 3.3669774e-16], sum to 1.0000
[2019-04-04 14:12:00,248] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0656
[2019-04-04 14:12:00,268] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.416666666666667, 75.16666666666667, 0.0, 0.0, 26.0, 25.59557156455107, 0.4194222820225336, 0.0, 1.0, 18738.6737007483], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4317000.0000, 
sim time next is 4317600.0000, 
raw observation next is [4.433333333333334, 75.33333333333334, 0.0, 0.0, 26.0, 25.60242991886164, 0.4113851691670372, 0.0, 1.0, 18736.643410131], 
processed observation next is [0.0, 1.0, 0.5854108956602032, 0.7533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6335358265718035, 0.6371283897223458, 0.0, 1.0, 0.08922211147681429], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.027389], dtype=float32), -1.0641123]. 
=============================================
[2019-04-04 14:12:02,879] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135500, global step 2159112: loss 0.0345
[2019-04-04 14:12:02,882] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135500, global step 2159112: learning rate 0.0000
[2019-04-04 14:12:03,602] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135000, global step 2159532: loss 1.3260
[2019-04-04 14:12:03,603] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135000, global step 2159532: learning rate 0.0000
[2019-04-04 14:12:03,739] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2159622: loss 1.3239
[2019-04-04 14:12:03,741] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2159622: learning rate 0.0000
[2019-04-04 14:12:05,441] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2160529: loss 1.3187
[2019-04-04 14:12:05,448] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2160529: learning rate 0.0000
[2019-04-04 14:12:05,695] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2160669: loss 1.2927
[2019-04-04 14:12:05,696] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2160669: learning rate 0.0000
[2019-04-04 14:12:06,258] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2160963: loss 0.0366
[2019-04-04 14:12:06,264] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2160966: learning rate 0.0000
[2019-04-04 14:12:07,333] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2161468: loss 1.2750
[2019-04-04 14:12:07,334] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2161468: learning rate 0.0000
[2019-04-04 14:12:07,874] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2161731: loss 1.2448
[2019-04-04 14:12:07,879] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2161733: learning rate 0.0000
[2019-04-04 14:12:08,014] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2161813: loss 1.2820
[2019-04-04 14:12:08,017] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2161814: learning rate 0.0000
[2019-04-04 14:12:08,312] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2161962: loss 0.0413
[2019-04-04 14:12:08,314] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2161962: learning rate 0.0000
[2019-04-04 14:12:08,803] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2162233: loss 1.2570
[2019-04-04 14:12:08,804] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2162233: learning rate 0.0000
[2019-04-04 14:12:09,360] A3C_AGENT_WORKER-Thread-4 INFO:Local step 135000, global step 2162516: loss 1.2894
[2019-04-04 14:12:09,361] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 135000, global step 2162516: learning rate 0.0000
[2019-04-04 14:12:09,426] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5609855e-10 3.4853738e-11 1.1025111e-17 1.6422622e-15 1.0000000e+00
 4.5232887e-11 3.6669832e-16], sum to 1.0000
[2019-04-04 14:12:09,427] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8943
[2019-04-04 14:12:09,465] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 71.0, 143.5, 340.0, 26.0, 26.09286465191337, 0.5234987403976342, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4611600.0000, 
sim time next is 4612200.0000, 
raw observation next is [-1.666666666666667, 69.16666666666667, 150.3333333333333, 396.3333333333334, 26.0, 26.10165183933735, 0.5266879543119197, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4164358264081256, 0.6916666666666668, 0.501111111111111, 0.437937384898711, 0.6666666666666666, 0.6751376532781125, 0.6755626514373065, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5074605], dtype=float32), 0.039656818]. 
=============================================
[2019-04-04 14:12:10,363] A3C_AGENT_WORKER-Thread-5 INFO:Local step 135000, global step 2163031: loss 1.2843
[2019-04-04 14:12:10,368] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 135000, global step 2163034: learning rate 0.0000
[2019-04-04 14:12:10,582] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135000, global step 2163158: loss 1.2791
[2019-04-04 14:12:10,583] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135000, global step 2163158: learning rate 0.0000
[2019-04-04 14:12:10,653] A3C_AGENT_WORKER-Thread-6 INFO:Local step 135000, global step 2163194: loss 1.3521
[2019-04-04 14:12:10,655] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 135000, global step 2163194: learning rate 0.0000
[2019-04-04 14:12:10,668] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2163205: loss 1.2643
[2019-04-04 14:12:10,669] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2163205: learning rate 0.0000
[2019-04-04 14:12:11,640] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3339401e-09 6.0195937e-10 1.7265809e-14 1.2173720e-13 1.0000000e+00
 1.7077781e-10 1.5531323e-14], sum to 1.0000
[2019-04-04 14:12:11,644] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5363
[2019-04-04 14:12:11,673] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 50.0, 0.0, 0.0, 26.0, 25.21884477492299, 0.2749679792216833, 0.0, 1.0, 38379.18182650388], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4939200.0000, 
sim time next is 4939800.0000, 
raw observation next is [-2.0, 49.33333333333334, 0.0, 0.0, 26.0, 25.18527449957824, 0.2708463256932336, 0.0, 1.0, 38352.83013380942], 
processed observation next is [1.0, 0.17391304347826086, 0.40720221606648205, 0.4933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5987728749648534, 0.5902821085644112, 0.0, 1.0, 0.18263252444671152], 
reward next is 0.8174, 
noisyNet noise sample is [array([1.3524079], dtype=float32), 0.8016576]. 
=============================================
[2019-04-04 14:12:11,679] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5777176e-09 3.6365289e-10 5.3037552e-15 6.1778660e-14 1.0000000e+00
 3.3027020e-10 1.0417418e-14], sum to 1.0000
[2019-04-04 14:12:11,682] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6852
[2019-04-04 14:12:11,737] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.83333333333334, 0.0, 0.0, 26.0, 25.04379351114835, 0.4343885133840173, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4565400.0000, 
sim time next is 4566000.0000, 
raw observation next is [2.0, 53.66666666666667, 0.0, 0.0, 26.0, 25.0872367961163, 0.4460199672968195, 0.0, 1.0, 198407.0329511942], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.5366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5906030663430251, 0.6486733224322732, 0.0, 1.0, 0.9447953950056868], 
reward next is 0.0552, 
noisyNet noise sample is [array([-1.1516948], dtype=float32), 0.4824701]. 
=============================================
[2019-04-04 14:12:11,744] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[83.41542 ]
 [82.882805]
 [82.74478 ]
 [83.2754  ]
 [82.42679 ]], R is [[82.95271301]
 [83.1231842 ]
 [83.20288849]
 [83.1061554 ]
 [82.82337189]].
[2019-04-04 14:12:12,936] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.1009945e-09 5.8036465e-10 3.0131381e-14 2.9368465e-13 1.0000000e+00
 6.4142563e-10 6.9944870e-14], sum to 1.0000
[2019-04-04 14:12:12,938] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7507
[2019-04-04 14:12:12,954] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 41.5, 0.0, 0.0, 26.0, 25.35221289784098, 0.3568336571693267, 0.0, 1.0, 43619.80711946682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4926600.0000, 
sim time next is 4927200.0000, 
raw observation next is [0.3333333333333334, 42.0, 0.0, 0.0, 26.0, 25.49357540568045, 0.3696033559593552, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.4718374884579871, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6244646171400374, 0.6232011186531184, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9813347], dtype=float32), 0.90448284]. 
=============================================
[2019-04-04 14:12:16,912] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:16,913] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:16,920] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run17
[2019-04-04 14:12:19,079] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2167567: loss 0.0337
[2019-04-04 14:12:19,080] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2167568: learning rate 0.0000
[2019-04-04 14:12:19,193] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135500, global step 2167624: loss 0.0310
[2019-04-04 14:12:19,195] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135500, global step 2167625: learning rate 0.0000
[2019-04-04 14:12:19,657] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3102071e-08 3.4489462e-09 6.5640859e-14 1.1361671e-12 1.0000000e+00
 2.1040845e-09 1.7255842e-13], sum to 1.0000
[2019-04-04 14:12:19,658] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0176
[2019-04-04 14:12:19,668] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.5, 37.0, 33.0, 185.0, 26.0, 25.09915268001102, 0.3754088633214609, 0.0, 1.0, 29793.45459127244], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4815000.0000, 
sim time next is 4815600.0000, 
raw observation next is [2.333333333333333, 38.0, 27.5, 154.1666666666667, 26.0, 25.0772930976747, 0.3661567703602815, 0.0, 1.0, 34811.7333187587], 
processed observation next is [0.0, 0.7391304347826086, 0.5272391505078486, 0.38, 0.09166666666666666, 0.17034990791896876, 0.6666666666666666, 0.5897744248062251, 0.6220522567867605, 0.0, 1.0, 0.1657701586607557], 
reward next is 0.8342, 
noisyNet noise sample is [array([0.42365715], dtype=float32), 1.3719487]. 
=============================================
[2019-04-04 14:12:20,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:20,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:20,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run17
[2019-04-04 14:12:20,753] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2168353: loss 0.0347
[2019-04-04 14:12:20,754] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2168353: learning rate 0.0000
[2019-04-04 14:12:20,762] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2168357: loss 0.0350
[2019-04-04 14:12:20,770] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2168357: learning rate 0.0000
[2019-04-04 14:12:21,365] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.14045404e-09 1.07339315e-09 1.36172733e-15 5.21012981e-14
 1.00000000e+00 4.98468211e-10 1.00611505e-14], sum to 1.0000
[2019-04-04 14:12:21,366] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9839
[2019-04-04 14:12:21,373] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.666666666666667, 38.0, 208.0, 597.0, 26.0, 25.14066472159273, 0.4325282351290337, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4801200.0000, 
sim time next is 4801800.0000, 
raw observation next is [2.833333333333333, 37.5, 196.0, 626.0, 26.0, 25.13272959796612, 0.433335654127982, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.541089566020314, 0.375, 0.6533333333333333, 0.6917127071823205, 0.6666666666666666, 0.5943941331638433, 0.6444452180426606, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5552183], dtype=float32), -1.0552734]. 
=============================================
[2019-04-04 14:12:21,850] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:21,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:21,862] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run17
[2019-04-04 14:12:22,497] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2169100: loss 0.0331
[2019-04-04 14:12:22,497] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2169100: learning rate 0.0000
[2019-04-04 14:12:22,884] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2169224: loss 0.0339
[2019-04-04 14:12:22,885] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2169224: learning rate 0.0000
[2019-04-04 14:12:23,478] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2169488: loss 0.0328
[2019-04-04 14:12:23,478] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2169488: learning rate 0.0000
[2019-04-04 14:12:24,084] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6721119e-08 3.6076047e-09 4.6602306e-14 3.5685953e-13 1.0000000e+00
 1.0878103e-09 4.7630072e-14], sum to 1.0000
[2019-04-04 14:12:24,086] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1662
[2019-04-04 14:12:24,103] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 37.0, 92.0, 667.6666666666667, 26.0, 25.19045397920935, 0.4413461210810746, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4809000.0000, 
sim time next is 4809600.0000, 
raw observation next is [3.0, 37.0, 89.5, 638.0, 26.0, 25.19355527690842, 0.4376853654359418, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.37, 0.29833333333333334, 0.7049723756906078, 0.6666666666666666, 0.5994629397423683, 0.6458951218119806, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2503555], dtype=float32), 0.35509482]. 
=============================================
[2019-04-04 14:12:24,156] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2169856: loss 0.0362
[2019-04-04 14:12:24,159] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2169856: learning rate 0.0000
[2019-04-04 14:12:24,380] A3C_AGENT_WORKER-Thread-4 INFO:Local step 135500, global step 2169963: loss 0.0342
[2019-04-04 14:12:24,383] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 135500, global step 2169965: learning rate 0.0000
[2019-04-04 14:12:24,982] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135500, global step 2170224: loss 0.0390
[2019-04-04 14:12:24,983] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135500, global step 2170224: learning rate 0.0000
[2019-04-04 14:12:25,307] A3C_AGENT_WORKER-Thread-5 INFO:Local step 135500, global step 2170362: loss 0.0304
[2019-04-04 14:12:25,310] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 135500, global step 2170363: learning rate 0.0000
[2019-04-04 14:12:25,697] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2170545: loss 0.0377
[2019-04-04 14:12:25,702] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2170546: learning rate 0.0000
[2019-04-04 14:12:25,831] A3C_AGENT_WORKER-Thread-6 INFO:Local step 135500, global step 2170611: loss 0.0371
[2019-04-04 14:12:25,832] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 135500, global step 2170611: learning rate 0.0000
[2019-04-04 14:12:27,532] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.9982520e-10 1.2431378e-10 3.1217235e-16 4.5835261e-15 1.0000000e+00
 1.5651281e-11 2.9113984e-16], sum to 1.0000
[2019-04-04 14:12:27,533] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0010
[2019-04-04 14:12:27,555] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.166666666666667, 24.33333333333333, 119.0, 861.6666666666666, 26.0, 26.53027666877308, 0.6754128119891402, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4972200.0000, 
sim time next is 4972800.0000, 
raw observation next is [7.333333333333334, 24.66666666666666, 118.0, 860.8333333333334, 26.0, 25.93988088924794, 0.6458985539775358, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6657433056325024, 0.24666666666666662, 0.3933333333333333, 0.9511970534069982, 0.6666666666666666, 0.6616567407706618, 0.7152995179925119, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02212828], dtype=float32), 0.30720598]. 
=============================================
[2019-04-04 14:12:28,821] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4576473e-09 1.0934846e-10 1.4723270e-16 7.9252138e-15 1.0000000e+00
 6.0818066e-11 5.1442021e-16], sum to 1.0000
[2019-04-04 14:12:28,822] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5103
[2019-04-04 14:12:28,840] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3, 93.0, 0.0, 0.0, 26.0, 24.28492571971337, 0.1580529208664903, 0.0, 1.0, 40275.01272569638], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 88200.0000, 
sim time next is 88800.0000, 
raw observation next is [-0.4, 92.33333333333334, 0.0, 0.0, 26.0, 24.37578912248127, 0.1540189445591419, 0.0, 1.0, 40378.18677994086], 
processed observation next is [1.0, 0.0, 0.45152354570637127, 0.9233333333333335, 0.0, 0.0, 0.6666666666666666, 0.5313157602067724, 0.5513396481863806, 0.0, 1.0, 0.19227707990448029], 
reward next is 0.8077, 
noisyNet noise sample is [array([0.6448109], dtype=float32), -0.78718245]. 
=============================================
[2019-04-04 14:12:29,869] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.8820559e-08 2.1772903e-09 1.8842614e-14 2.0779255e-13 1.0000000e+00
 1.7236470e-09 7.4832359e-14], sum to 1.0000
[2019-04-04 14:12:29,870] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9155
[2019-04-04 14:12:29,900] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 46.66666666666667, 0.0, 0.0, 26.0, 25.46157893030178, 0.3657566042275362, 0.0, 1.0, 42168.19762941906], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5032200.0000, 
sim time next is 5032800.0000, 
raw observation next is [-1.0, 46.0, 0.0, 0.0, 26.0, 25.37729293625344, 0.3639265664325442, 0.0, 1.0, 82054.75479782475], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.46, 0.0, 0.0, 0.6666666666666666, 0.6147744113544533, 0.6213088554775147, 0.0, 1.0, 0.3907369276086893], 
reward next is 0.6093, 
noisyNet noise sample is [array([-0.98036754], dtype=float32), -0.5217589]. 
=============================================
[2019-04-04 14:12:31,982] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:31,982] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:31,995] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run17
[2019-04-04 14:12:32,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:32,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:32,238] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run17
[2019-04-04 14:12:33,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:33,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:33,498] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run17
[2019-04-04 14:12:33,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:33,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:33,650] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run17
[2019-04-04 14:12:35,057] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2507873e-09 9.8013903e-11 2.6359747e-16 1.2014543e-15 1.0000000e+00
 4.1681616e-11 1.2385338e-15], sum to 1.0000
[2019-04-04 14:12:35,057] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8745
[2019-04-04 14:12:35,063] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 26.0, 28.15290953597379, 1.036213532542182, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5079000.0000, 
sim time next is 5079600.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 26.0, 28.02730338485365, 1.024717335362187, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7673130193905818, 0.17, 0.0, 0.0, 0.6666666666666666, 0.8356086154044707, 0.8415724451207289, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4668539], dtype=float32), 0.8911014]. 
=============================================
[2019-04-04 14:12:35,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:35,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:35,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run17
[2019-04-04 14:12:35,719] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:35,719] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:35,728] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run17
[2019-04-04 14:12:36,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:36,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:36,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run17
[2019-04-04 14:12:36,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:36,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:36,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run17
[2019-04-04 14:12:36,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:36,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:36,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run17
[2019-04-04 14:12:37,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:37,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:37,341] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run17
[2019-04-04 14:12:38,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:38,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:38,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run17
[2019-04-04 14:12:38,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:38,642] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:38,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run17
[2019-04-04 14:12:38,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:12:38,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:12:38,881] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run17
[2019-04-04 14:12:51,259] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6347726e-09 1.4258274e-10 4.7701121e-16 2.1908861e-14 1.0000000e+00
 2.6206214e-11 5.0045514e-15], sum to 1.0000
[2019-04-04 14:12:51,261] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3747
[2019-04-04 14:12:51,291] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.433333333333333, 88.33333333333334, 0.0, 0.0, 26.0, 24.18989314442501, 0.09671057249713455, 0.0, 1.0, 42596.5075930851], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 96000.0000, 
sim time next is 96600.0000, 
raw observation next is [-2.616666666666667, 87.66666666666666, 0.0, 0.0, 26.0, 24.11085984821583, 0.09513901856886396, 0.0, 1.0, 42780.78592670171], 
processed observation next is [1.0, 0.08695652173913043, 0.3901200369344414, 0.8766666666666666, 0.0, 0.0, 0.6666666666666666, 0.5092383206846526, 0.5317130061896213, 0.0, 1.0, 0.2037180282223891], 
reward next is 0.7963, 
noisyNet noise sample is [array([-1.6065648], dtype=float32), -1.676009]. 
=============================================
[2019-04-04 14:13:00,924] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.6046033e-09 1.8785604e-09 2.1502077e-14 6.2833967e-13 1.0000000e+00
 1.8269449e-09 3.3648624e-14], sum to 1.0000
[2019-04-04 14:13:00,925] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5160
[2019-04-04 14:13:00,972] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.98580292501664, -0.1861251265536724, 0.0, 1.0, 44404.53751724536], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 189000.0000, 
sim time next is 189600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.92170499213055, -0.1943196800491551, 0.0, 1.0, 44494.59199034655], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.41014208267754587, 0.4352267733169483, 0.0, 1.0, 0.2118790094778407], 
reward next is 0.7881, 
noisyNet noise sample is [array([0.19223046], dtype=float32), -1.2186629]. 
=============================================
[2019-04-04 14:13:03,628] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.66198985e-10 1.10906596e-10 5.45556240e-16 8.25753362e-15
 1.00000000e+00 6.75721146e-11 8.19852136e-16], sum to 1.0000
[2019-04-04 14:13:03,628] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9027
[2019-04-04 14:13:03,674] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666667, 65.0, 132.3333333333333, 0.0, 26.0, 25.12224283928952, 0.2229395350880611, 1.0, 1.0, 70236.27673660373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 218400.0000, 
sim time next is 219000.0000, 
raw observation next is [-4.583333333333333, 65.0, 135.6666666666667, 0.0, 26.0, 25.15029394298659, 0.2289764034374926, 1.0, 1.0, 37205.15541572539], 
processed observation next is [1.0, 0.5217391304347826, 0.3356417359187443, 0.65, 0.45222222222222236, 0.0, 0.6666666666666666, 0.5958578285822158, 0.5763254678124975, 1.0, 1.0, 0.1771674067415495], 
reward next is 0.8228, 
noisyNet noise sample is [array([-1.5664014], dtype=float32), 0.030004337]. 
=============================================
[2019-04-04 14:13:03,677] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[84.26273 ]
 [84.192116]
 [84.28269 ]
 [84.3843  ]
 [84.31273 ]], R is [[84.20326233]
 [84.02677155]
 [83.97942352]
 [84.0137558 ]
 [83.91680908]].
[2019-04-04 14:13:10,583] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3072876e-10 2.4703125e-11 4.6099842e-16 1.7100097e-15 1.0000000e+00
 1.2571529e-11 1.7462528e-15], sum to 1.0000
[2019-04-04 14:13:10,583] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0990
[2019-04-04 14:13:10,636] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.533333333333333, 85.66666666666667, 34.16666666666667, 37.5, 26.0, 24.95597492303515, 0.3027256914002152, 0.0, 1.0, 50157.07093248341], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 578400.0000, 
sim time next is 579000.0000, 
raw observation next is [-1.616666666666667, 86.33333333333333, 27.33333333333334, 30.0, 26.0, 24.94905836547446, 0.3073061200855076, 0.0, 1.0, 48624.31608821817], 
processed observation next is [0.0, 0.6956521739130435, 0.4178208679593721, 0.8633333333333333, 0.09111111111111113, 0.03314917127071823, 0.6666666666666666, 0.5790881971228717, 0.6024353733618358, 0.0, 1.0, 0.23154436232484843], 
reward next is 0.7685, 
noisyNet noise sample is [array([0.5406932], dtype=float32), 0.44550145]. 
=============================================
[2019-04-04 14:13:10,656] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[82.16553]
 [82.18138]
 [82.27934]
 [82.47724]
 [82.69251]], R is [[82.17137909]
 [82.11081696]
 [82.07269287]
 [82.08384705]
 [82.10072327]].
[2019-04-04 14:13:13,627] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7053905e-08 6.1624181e-09 4.8122449e-14 1.1613150e-12 1.0000000e+00
 9.3094332e-10 1.7703403e-13], sum to 1.0000
[2019-04-04 14:13:13,627] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7797
[2019-04-04 14:13:13,672] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 73.5, 0.0, 0.0, 26.0, 23.53165600853431, -0.06408252711428958, 0.0, 1.0, 43922.30914430658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 631800.0000, 
sim time next is 632400.0000, 
raw observation next is [-4.5, 75.33333333333333, 0.0, 0.0, 26.0, 23.50682628383731, -0.07009600558223357, 0.0, 1.0, 43947.69062612936], 
processed observation next is [0.0, 0.30434782608695654, 0.3379501385041552, 0.7533333333333333, 0.0, 0.0, 0.6666666666666666, 0.4589021903197758, 0.47663466480592215, 0.0, 1.0, 0.20927471726728267], 
reward next is 0.7907, 
noisyNet noise sample is [array([1.2120749], dtype=float32), -0.101006754]. 
=============================================
[2019-04-04 14:13:15,866] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0752116e-07 8.9385166e-09 5.3487640e-13 4.0679942e-12 9.9999988e-01
 1.2799171e-08 2.2202795e-13], sum to 1.0000
[2019-04-04 14:13:15,866] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2909
[2019-04-04 14:13:15,897] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.45, 54.5, 0.0, 0.0, 26.0, 23.68868439868848, -0.04508037131799873, 0.0, 1.0, 45279.71573130663], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 433800.0000, 
sim time next is 434400.0000, 
raw observation next is [-11.36666666666667, 54.66666666666667, 0.0, 0.0, 26.0, 23.62876296702031, -0.05684902742534851, 0.0, 1.0, 45346.32814149685], 
processed observation next is [1.0, 0.0, 0.14773776546629722, 0.5466666666666667, 0.0, 0.0, 0.6666666666666666, 0.4690635805850259, 0.48105032419155047, 0.0, 1.0, 0.21593489591188977], 
reward next is 0.7841, 
noisyNet noise sample is [array([-1.3238453], dtype=float32), 0.16709363]. 
=============================================
[2019-04-04 14:13:26,589] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.8887575e-09 1.6491249e-10 6.9671313e-16 1.8106910e-14 1.0000000e+00
 8.1695081e-11 1.3215695e-15], sum to 1.0000
[2019-04-04 14:13:26,590] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4522
[2019-04-04 14:13:26,609] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.4, 93.0, 0.0, 0.0, 26.0, 24.85528104441976, 0.2365404518559804, 0.0, 1.0, 39681.03164831681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 520200.0000, 
sim time next is 520800.0000, 
raw observation next is [4.6, 91.66666666666666, 0.0, 0.0, 26.0, 24.84957717163045, 0.2362185858828774, 0.0, 1.0, 39656.24512632528], 
processed observation next is [0.0, 0.0, 0.590027700831025, 0.9166666666666665, 0.0, 0.0, 0.6666666666666666, 0.5707980976358709, 0.5787395286276258, 0.0, 1.0, 0.18883926250631086], 
reward next is 0.8112, 
noisyNet noise sample is [array([-0.5569416], dtype=float32), 1.0714868]. 
=============================================
[2019-04-04 14:13:28,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1461033e-09 4.7268189e-09 7.0115808e-15 4.2881746e-13 1.0000000e+00
 3.5752110e-09 5.5473365e-15], sum to 1.0000
[2019-04-04 14:13:28,835] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1394
[2019-04-04 14:13:28,850] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.716666666666667, 71.0, 0.0, 0.0, 26.0, 24.14209805443095, 0.08899242538617069, 0.0, 1.0, 41541.13202432424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 784200.0000, 
sim time next is 784800.0000, 
raw observation next is [-7.8, 71.0, 0.0, 0.0, 26.0, 24.08229847963073, 0.08038198137193156, 0.0, 1.0, 41540.44557095467], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5068582066358941, 0.5267939937906438, 0.0, 1.0, 0.1978116455759746], 
reward next is 0.8022, 
noisyNet noise sample is [array([-1.5905929], dtype=float32), -0.60030353]. 
=============================================
[2019-04-04 14:13:30,791] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.2747714e-10 2.5082334e-11 7.0990731e-16 7.3592382e-15 1.0000000e+00
 4.2512147e-11 1.0567747e-15], sum to 1.0000
[2019-04-04 14:13:30,791] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2095
[2019-04-04 14:13:30,823] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 84.33333333333334, 0.0, 0.0, 26.0, 24.7475769178937, 0.2131038890630265, 0.0, 1.0, 42475.87851865776], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 602400.0000, 
sim time next is 603000.0000, 
raw observation next is [-3.4, 85.0, 0.0, 0.0, 26.0, 24.70934294002463, 0.2055256669381302, 0.0, 1.0, 42423.99444961869], 
processed observation next is [0.0, 1.0, 0.368421052631579, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5591119116687192, 0.5685085556460434, 0.0, 1.0, 0.2020190211886604], 
reward next is 0.7980, 
noisyNet noise sample is [array([-0.88382214], dtype=float32), -1.4657421]. 
=============================================
[2019-04-04 14:13:30,829] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.22697]
 [82.35226]
 [82.45623]
 [82.54412]
 [82.66786]], R is [[82.09738922]
 [82.07415009]
 [82.0508194 ]
 [82.02734375]
 [82.00373077]].
[2019-04-04 14:13:38,026] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6537844e-09 7.7986111e-11 2.7210311e-15 2.3264150e-14 1.0000000e+00
 6.7856387e-11 2.9587034e-15], sum to 1.0000
[2019-04-04 14:13:38,027] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3328
[2019-04-04 14:13:38,084] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.383333333333333, 59.83333333333333, 148.3333333333333, 80.66666666666667, 26.0, 24.93134049805266, 0.2372700141708145, 0.0, 1.0, 26322.5567632336], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 654600.0000, 
sim time next is 655200.0000, 
raw observation next is [-1.2, 60.0, 131.5, 74.5, 26.0, 24.91552600134002, 0.2350485507018495, 0.0, 1.0, 36940.09373392157], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.6, 0.43833333333333335, 0.08232044198895028, 0.6666666666666666, 0.5762938334450016, 0.5783495169006164, 0.0, 1.0, 0.17590520825676936], 
reward next is 0.8241, 
noisyNet noise sample is [array([-0.7850517], dtype=float32), -2.0829282]. 
=============================================
[2019-04-04 14:13:40,349] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1979108e-09 5.1045163e-10 3.4684428e-15 9.8388501e-15 1.0000000e+00
 1.1726595e-09 8.5894579e-15], sum to 1.0000
[2019-04-04 14:13:40,351] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8663
[2019-04-04 14:13:40,405] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 48.0, 70.83333333333334, 7.666666666666665, 26.0, 24.69713185005305, 0.3357531959427991, 1.0, 1.0, 197075.4340365627], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 750000.0000, 
sim time next is 750600.0000, 
raw observation next is [-1.7, 49.5, 68.0, 3.0, 26.0, 25.22742185878894, 0.3958711915639211, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4155124653739613, 0.495, 0.22666666666666666, 0.0033149171270718232, 0.6666666666666666, 0.6022851548990783, 0.6319570638546403, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7800998], dtype=float32), 1.313096]. 
=============================================
[2019-04-04 14:13:49,197] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-04 14:13:49,209] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 14:13:49,209] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 14:13:49,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:13:49,209] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:13:49,210] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 14:13:49,210] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:13:49,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run23
[2019-04-04 14:13:49,233] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run23
[2019-04-04 14:13:49,249] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run23
[2019-04-04 14:15:08,094] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1722638], dtype=float32), 0.20735173]
[2019-04-04 14:15:08,094] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.224546467000001, 63.19529199333333, 0.0, 0.0, 26.0, 24.52820656732963, 0.1854215142340782, 0.0, 1.0, 41710.93272912635]
[2019-04-04 14:15:08,094] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 14:15:08,095] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.0033690e-09 1.0007319e-09 2.9993992e-14 2.8177946e-13 1.0000000e+00
 5.9643701e-10 4.2295730e-14], sampled 0.6476788139757961
[2019-04-04 14:15:30,797] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.3589 239954635.8279 1605.1656
[2019-04-04 14:15:49,065] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.5698 263430344.9894 1551.9755
[2019-04-04 14:15:54,769] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 14:15:55,793] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2200000, evaluation results [2200000.0, 7241.569785764876, 263430344.989374, 1551.9755349129598, 7353.358877010041, 239954635.82789436, 1605.1655887482925, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 14:15:55,911] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3985828e-10 1.9612643e-12 4.5159710e-18 3.6435855e-16 1.0000000e+00
 1.9302392e-12 9.3389535e-18], sum to 1.0000
[2019-04-04 14:15:55,915] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7689
[2019-04-04 14:15:55,934] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 100.0, 77.0, 0.0, 26.0, 24.73242245057791, 0.4468600329811203, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1261800.0000, 
sim time next is 1262400.0000, 
raw observation next is [13.8, 100.0, 72.66666666666667, 0.0, 26.0, 24.75232287207088, 0.4452029026257652, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.24222222222222223, 0.0, 0.6666666666666666, 0.5626935726725734, 0.6484009675419217, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6190179], dtype=float32), -1.9346273]. 
=============================================
[2019-04-04 14:16:01,119] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5133567e-10 2.1189999e-11 2.2914131e-18 2.3872530e-16 1.0000000e+00
 2.1856043e-12 1.9309633e-17], sum to 1.0000
[2019-04-04 14:16:01,123] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5164
[2019-04-04 14:16:01,180] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.5, 100.0, 0.0, 0.0, 26.0, 25.01040611665172, 0.343445083538449, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 933000.0000, 
sim time next is 933600.0000, 
raw observation next is [4.600000000000001, 100.0, 0.0, 0.0, 26.0, 25.05600377936384, 0.320446825386543, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5900277008310251, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5880003149469868, 0.6068156084621811, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7398642], dtype=float32), 0.2090957]. 
=============================================
[2019-04-04 14:16:02,450] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5036415e-10 1.8540362e-11 2.6197238e-17 9.6879287e-17 1.0000000e+00
 3.4434263e-12 2.4902213e-17], sum to 1.0000
[2019-04-04 14:16:02,454] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6143
[2019-04-04 14:16:02,462] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.38333333333333, 82.5, 7.999999999999999, 27.66666666666666, 26.0, 25.89521334693137, 0.62399479401778, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1065000.0000, 
sim time next is 1065600.0000, 
raw observation next is [12.2, 83.0, 11.5, 38.0, 26.0, 25.92046998226742, 0.6230094401981775, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.03833333333333333, 0.041988950276243095, 0.6666666666666666, 0.6600391651889517, 0.7076698133993925, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2109656], dtype=float32), -1.1952208]. 
=============================================
[2019-04-04 14:16:02,962] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.1946651e-11 6.9947611e-12 1.6383372e-17 2.7535244e-16 1.0000000e+00
 5.8792021e-12 3.5821179e-17], sum to 1.0000
[2019-04-04 14:16:02,969] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2078
[2019-04-04 14:16:02,980] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.2, 76.0, 0.0, 0.0, 26.0, 26.02642313364284, 0.6200842218272266, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1041600.0000, 
sim time next is 1042200.0000, 
raw observation next is [14.1, 76.5, 0.0, 0.0, 26.0, 25.98193668412504, 0.6082539812151335, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.8531855955678671, 0.765, 0.0, 0.0, 0.6666666666666666, 0.6651613903437532, 0.7027513270717112, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6412251], dtype=float32), -1.0274007]. 
=============================================
[2019-04-04 14:16:03,239] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2244864e-09 1.3938950e-11 9.8743342e-17 1.7501674e-15 1.0000000e+00
 1.5387396e-11 5.4533540e-17], sum to 1.0000
[2019-04-04 14:16:03,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1665
[2019-04-04 14:16:03,276] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 24.87646999421698, 0.427481837129132, 1.0, 1.0, 111018.8465255837], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1454400.0000, 
sim time next is 1455000.0000, 
raw observation next is [1.183333333333333, 91.5, 0.0, 0.0, 26.0, 24.89629089365809, 0.4403536237000081, 0.0, 1.0, 52429.14709841423], 
processed observation next is [1.0, 0.8695652173913043, 0.49538319482917825, 0.915, 0.0, 0.0, 0.6666666666666666, 0.5746909078048409, 0.6467845412333361, 0.0, 1.0, 0.24966260523054395], 
reward next is 0.7503, 
noisyNet noise sample is [array([0.761646], dtype=float32), -1.3388044]. 
=============================================
[2019-04-04 14:16:03,279] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[89.91153 ]
 [89.130066]
 [88.494316]
 [89.41275 ]
 [90.36647 ]], R is [[89.02445221]
 [88.60554504]
 [88.25310516]
 [88.27422333]
 [88.39147949]].
[2019-04-04 14:16:03,573] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.6340458e-10 4.4403207e-11 2.0010035e-16 5.5327963e-15 1.0000000e+00
 5.8639938e-12 1.1105910e-16], sum to 1.0000
[2019-04-04 14:16:03,576] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2623
[2019-04-04 14:16:03,588] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.4, 83.0, 0.0, 0.0, 26.0, 25.62576683429495, 0.4490021001777955, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 973800.0000, 
sim time next is 974400.0000, 
raw observation next is [9.6, 83.0, 0.0, 0.0, 26.0, 25.55045684801548, 0.4361015205451014, 0.0, 1.0, 41672.52567504188], 
processed observation next is [1.0, 0.2608695652173913, 0.7285318559556787, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6292047373346232, 0.6453671735150338, 0.0, 1.0, 0.19844059845258036], 
reward next is 0.8016, 
noisyNet noise sample is [array([1.1214241], dtype=float32), 0.34122422]. 
=============================================
[2019-04-04 14:16:06,040] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4754245e-08 1.9350203e-08 5.6998555e-14 9.5196485e-13 1.0000000e+00
 9.3712349e-10 2.8635227e-13], sum to 1.0000
[2019-04-04 14:16:06,043] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9311
[2019-04-04 14:16:06,050] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.5366325689486, 0.1598954149203421, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1232400.0000, 
sim time next is 1233000.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.54077902750155, 0.156757431951029, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.46173158562512917, 0.5522524773170097, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9848846], dtype=float32), -0.6706892]. 
=============================================
[2019-04-04 14:16:06,061] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[71.72491 ]
 [71.78118 ]
 [71.800995]
 [71.79328 ]
 [71.80149 ]], R is [[71.96379089]
 [72.24415588]
 [72.52171326]
 [72.79649353]
 [73.06852722]].
[2019-04-04 14:16:10,615] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1721509e-09 1.1445356e-10 5.3967126e-17 2.5741772e-15 1.0000000e+00
 7.0021827e-12 4.4953161e-16], sum to 1.0000
[2019-04-04 14:16:10,619] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9454
[2019-04-04 14:16:10,640] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.2, 97.33333333333333, 100.0, 0.0, 26.0, 25.0426806690077, 0.4812326184089011, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1254000.0000, 
sim time next is 1254600.0000, 
raw observation next is [14.1, 98.0, 101.0, 0.0, 26.0, 25.01322982809008, 0.4778419001459819, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8531855955678671, 0.98, 0.33666666666666667, 0.0, 0.6666666666666666, 0.5844358190075066, 0.6592806333819939, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9953711], dtype=float32), 0.93536645]. 
=============================================
[2019-04-04 14:16:10,816] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7192058e-09 1.1281897e-09 1.2511277e-14 7.1448313e-14 1.0000000e+00
 1.2000420e-10 8.8549858e-15], sum to 1.0000
[2019-04-04 14:16:10,819] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1599
[2019-04-04 14:16:10,827] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.63333333333333, 52.0, 0.0, 0.0, 26.0, 25.9110767142228, 0.7509251459103837, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1100400.0000, 
sim time next is 1101000.0000, 
raw observation next is [16.36666666666667, 52.5, 0.0, 0.0, 26.0, 26.20048519943134, 0.7719064045420799, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.9159741458910436, 0.525, 0.0, 0.0, 0.6666666666666666, 0.6833737666192784, 0.75730213484736, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2606901], dtype=float32), 0.8482224]. 
=============================================
[2019-04-04 14:16:10,844] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[77.798965]
 [78.08261 ]
 [78.21501 ]
 [78.2742  ]
 [78.38578 ]], R is [[77.78385162]
 [78.00601196]
 [78.22595215]
 [78.44369507]
 [78.65925598]].
[2019-04-04 14:16:13,115] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0580748e-08 1.0970239e-09 1.0445098e-13 1.6492710e-12 1.0000000e+00
 3.1772196e-10 2.3591475e-13], sum to 1.0000
[2019-04-04 14:16:13,117] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8830
[2019-04-04 14:16:13,123] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.33333333333333, 94.0, 0.0, 0.0, 26.0, 23.75696686668212, 0.2010197303556063, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1225200.0000, 
sim time next is 1225800.0000, 
raw observation next is [15.25, 94.5, 0.0, 0.0, 26.0, 23.7408336412213, 0.196514944794371, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8850415512465375, 0.945, 0.0, 0.0, 0.6666666666666666, 0.4784028034351084, 0.5655049815981237, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35777515], dtype=float32), 1.8569037]. 
=============================================
[2019-04-04 14:16:14,913] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7576291e-07 1.0618253e-08 1.8680195e-13 1.7195032e-12 9.9999976e-01
 1.9464681e-09 2.4055207e-13], sum to 1.0000
[2019-04-04 14:16:14,920] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7814
[2019-04-04 14:16:14,929] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.33333333333333, 94.0, 0.0, 0.0, 26.0, 23.75696545071835, 0.201019387985982, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1225200.0000, 
sim time next is 1225800.0000, 
raw observation next is [15.25, 94.5, 0.0, 0.0, 26.0, 23.74083222145547, 0.1965146016475428, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8850415512465375, 0.945, 0.0, 0.0, 0.6666666666666666, 0.4784026851212892, 0.5655048672158476, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49860257], dtype=float32), 0.27164286]. 
=============================================
[2019-04-04 14:16:22,152] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3626285e-09 3.2896737e-11 1.9316652e-15 4.1523071e-15 1.0000000e+00
 9.6150594e-11 4.7105477e-16], sum to 1.0000
[2019-04-04 14:16:22,152] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3990
[2019-04-04 14:16:22,158] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.7, 60.5, 0.0, 0.0, 26.0, 26.30481245676888, 0.6686169002358465, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1535400.0000, 
sim time next is 1536000.0000, 
raw observation next is [9.600000000000001, 60.66666666666667, 0.0, 0.0, 26.0, 26.19168871381476, 0.6669971326627168, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7285318559556788, 0.6066666666666667, 0.0, 0.0, 0.6666666666666666, 0.68264072615123, 0.7223323775542388, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1885248], dtype=float32), 1.208481]. 
=============================================
[2019-04-04 14:16:22,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[88.1271  ]
 [87.91877 ]
 [87.837296]
 [87.83346 ]
 [87.897575]], R is [[88.39713287]
 [88.51316071]
 [88.62802887]
 [88.74175262]
 [88.8543396 ]].
[2019-04-04 14:16:28,351] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.2215346e-10 2.9627585e-11 2.2944593e-16 7.8470877e-15 1.0000000e+00
 9.8358301e-11 3.2802608e-16], sum to 1.0000
[2019-04-04 14:16:28,354] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2120
[2019-04-04 14:16:28,364] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.78333333333333, 56.66666666666667, 58.66666666666667, 20.66666666666667, 26.0, 26.00998556696514, 0.6790928760915187, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1529400.0000, 
sim time next is 1530000.0000, 
raw observation next is [10.5, 58.0, 44.5, 17.0, 26.0, 26.47132211160429, 0.7116535067905018, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7534626038781165, 0.58, 0.14833333333333334, 0.01878453038674033, 0.6666666666666666, 0.7059435093003575, 0.7372178355968338, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28497046], dtype=float32), -2.2485547]. 
=============================================
[2019-04-04 14:16:28,368] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[88.456474]
 [88.7472  ]
 [88.982956]
 [89.190834]
 [89.34647 ]], R is [[88.32350922]
 [88.4402771 ]
 [88.55587769]
 [88.6703186 ]
 [88.78361511]].
[2019-04-04 14:16:28,743] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.4994934e-11 5.9298356e-12 4.9022111e-17 6.3092070e-16 1.0000000e+00
 6.1773186e-12 4.5927450e-17], sum to 1.0000
[2019-04-04 14:16:28,748] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3582
[2019-04-04 14:16:28,763] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.5, 97.0, 0.0, 0.0, 26.0, 25.47897528680751, 0.5520907042157263, 0.0, 1.0, 48029.70158191529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1663200.0000, 
sim time next is 1663800.0000, 
raw observation next is [5.416666666666667, 96.16666666666667, 0.0, 0.0, 26.0, 25.54625838238306, 0.5581736765297644, 0.0, 1.0, 18747.05499238883], 
processed observation next is [1.0, 0.2608695652173913, 0.6126500461680519, 0.9616666666666667, 0.0, 0.0, 0.6666666666666666, 0.6288548651985882, 0.6860578921765881, 0.0, 1.0, 0.0892716904399468], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.1365335], dtype=float32), 0.2777896]. 
=============================================
[2019-04-04 14:16:29,285] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3714899e-10 3.4272175e-11 4.3071506e-17 4.5014078e-16 1.0000000e+00
 4.7379683e-12 1.5151853e-17], sum to 1.0000
[2019-04-04 14:16:29,290] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6979
[2019-04-04 14:16:29,314] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.966666666666667, 73.33333333333334, 102.0, 119.1666666666667, 26.0, 26.21822615641518, 0.6312192144959778, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1588800.0000, 
sim time next is 1589400.0000, 
raw observation next is [7.15, 72.0, 115.0, 136.0, 26.0, 26.33567795625265, 0.6556513410079319, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6606648199445985, 0.72, 0.38333333333333336, 0.15027624309392265, 0.6666666666666666, 0.6946398296877208, 0.7185504470026439, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0596331], dtype=float32), 0.2736862]. 
=============================================
[2019-04-04 14:16:29,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9812781e-11 8.1763936e-12 1.1843117e-17 1.9359205e-16 1.0000000e+00
 1.1326014e-12 5.0194759e-18], sum to 1.0000
[2019-04-04 14:16:29,803] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4489
[2019-04-04 14:16:29,823] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.56544470210051, 0.5512783009944342, 0.0, 1.0, 56366.05570448867], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1659600.0000, 
sim time next is 1660200.0000, 
raw observation next is [6.416666666666667, 97.0, 0.0, 0.0, 26.0, 25.57728452196256, 0.5653711671037019, 0.0, 1.0, 33008.92248484046], 
processed observation next is [1.0, 0.21739130434782608, 0.6403508771929826, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6314403768302134, 0.6884570557012339, 0.0, 1.0, 0.15718534516590696], 
reward next is 0.8428, 
noisyNet noise sample is [array([0.34845343], dtype=float32), 1.7930465]. 
=============================================
[2019-04-04 14:16:32,941] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.0095663e-10 2.8298461e-11 1.9600712e-16 4.3423275e-15 1.0000000e+00
 2.5466488e-11 6.5621644e-16], sum to 1.0000
[2019-04-04 14:16:32,942] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0831
[2019-04-04 14:16:32,971] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.37578047516058, 0.4684174549058479, 0.0, 1.0, 40720.70590089001], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1731000.0000, 
sim time next is 1731600.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.37146108092183, 0.4634460126658739, 0.0, 1.0, 43979.32055484036], 
processed observation next is [0.0, 0.043478260869565216, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6142884234101524, 0.654482004221958, 0.0, 1.0, 0.20942533597543028], 
reward next is 0.7906, 
noisyNet noise sample is [array([-0.20861052], dtype=float32), 0.73287284]. 
=============================================
[2019-04-04 14:16:33,138] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.6904639e-10 1.7385064e-11 1.3329556e-16 8.5267720e-16 1.0000000e+00
 8.4799659e-12 1.9045178e-17], sum to 1.0000
[2019-04-04 14:16:33,138] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0526
[2019-04-04 14:16:33,147] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.199999999999999, 85.33333333333334, 0.0, 0.0, 26.0, 25.66222865849925, 0.5989536516150303, 0.0, 1.0, 18727.16738031089], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1641000.0000, 
sim time next is 1641600.0000, 
raw observation next is [7.2, 86.0, 0.0, 0.0, 26.0, 25.66085953189688, 0.5914301763289657, 0.0, 1.0, 18725.65853537106], 
processed observation next is [1.0, 0.0, 0.662049861495845, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6384049609914065, 0.6971433921096551, 0.0, 1.0, 0.089169802549386], 
reward next is 0.9108, 
noisyNet noise sample is [array([2.281283], dtype=float32), -0.11937005]. 
=============================================
[2019-04-04 14:16:41,177] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.9451307e-10 2.2507742e-10 1.4466786e-15 8.3361196e-15 1.0000000e+00
 8.3493761e-11 2.3145654e-15], sum to 1.0000
[2019-04-04 14:16:41,177] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0701
[2019-04-04 14:16:41,227] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 85.0, 99.0, 0.0, 26.0, 24.96520802607402, 0.3382100959278303, 0.0, 1.0, 69163.63045656384], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1780200.0000, 
sim time next is 1780800.0000, 
raw observation next is [-2.8, 85.66666666666667, 93.5, 0.0, 26.0, 24.9544785821108, 0.3427841862004049, 0.0, 1.0, 60846.71762337897], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8566666666666667, 0.31166666666666665, 0.0, 0.6666666666666666, 0.5795398818425666, 0.6142613954001349, 0.0, 1.0, 0.2897462743970427], 
reward next is 0.7103, 
noisyNet noise sample is [array([0.14383759], dtype=float32), -0.31518403]. 
=============================================
[2019-04-04 14:16:46,701] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.9551735e-09 6.0507316e-10 3.7270503e-14 3.4111448e-13 1.0000000e+00
 2.4809819e-09 1.8856312e-14], sum to 1.0000
[2019-04-04 14:16:46,701] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8974
[2019-04-04 14:16:46,733] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.1, 85.0, 0.0, 0.0, 26.0, 24.10114037994057, 0.0920271492315935, 0.0, 1.0, 46559.27913495705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1823400.0000, 
sim time next is 1824000.0000, 
raw observation next is [-6.133333333333333, 85.66666666666667, 0.0, 0.0, 26.0, 24.06332762791116, 0.084031821312526, 0.0, 1.0, 46624.14417364175], 
processed observation next is [0.0, 0.08695652173913043, 0.2927054478301016, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.50527730232593, 0.5280106071041754, 0.0, 1.0, 0.2220197341601988], 
reward next is 0.7780, 
noisyNet noise sample is [array([-1.4335017], dtype=float32), -1.4459012]. 
=============================================
[2019-04-04 14:16:46,750] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[77.43561]
 [77.29969]
 [77.25415]
 [77.23201]
 [77.3439 ]], R is [[77.48847961]
 [77.49188232]
 [77.49554443]
 [77.49943542]
 [77.50360107]].
[2019-04-04 14:17:03,804] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.5006420e-09 3.5105685e-10 1.5443818e-15 1.8046374e-13 1.0000000e+00
 2.2040765e-10 2.2153071e-15], sum to 1.0000
[2019-04-04 14:17:03,804] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0581
[2019-04-04 14:17:03,821] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 82.66666666666667, 0.0, 0.0, 26.0, 25.3359497097801, 0.3735434330510017, 0.0, 1.0, 42711.16486122234], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2067000.0000, 
sim time next is 2067600.0000, 
raw observation next is [-4.1, 83.33333333333334, 0.0, 0.0, 26.0, 25.31404358123459, 0.3578993631303151, 0.0, 1.0, 42625.69274086929], 
processed observation next is [1.0, 0.9565217391304348, 0.3490304709141275, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6095036317695491, 0.6192997877101051, 0.0, 1.0, 0.20297948924223475], 
reward next is 0.7970, 
noisyNet noise sample is [array([1.2426382], dtype=float32), -1.0181636]. 
=============================================
[2019-04-04 14:17:10,334] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4927992e-09 8.0813252e-11 9.6687720e-16 6.1243176e-15 1.0000000e+00
 9.8027739e-11 1.2633038e-15], sum to 1.0000
[2019-04-04 14:17:10,335] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7879
[2019-04-04 14:17:10,368] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 71.0, 109.5, 225.5, 26.0, 26.01552172740385, 0.4176920561776444, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2196000.0000, 
sim time next is 2196600.0000, 
raw observation next is [-4.916666666666667, 71.0, 112.0, 150.3333333333333, 26.0, 26.02138990077521, 0.4083472874281832, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.32640812557710064, 0.71, 0.37333333333333335, 0.16611418047882132, 0.6666666666666666, 0.6684491583979341, 0.636115762476061, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49597827], dtype=float32), 1.3397843]. 
=============================================
[2019-04-04 14:17:10,388] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7778963e-08 3.1077261e-09 2.2500485e-14 1.5916305e-13 1.0000000e+00
 8.4014901e-10 1.7416343e-13], sum to 1.0000
[2019-04-04 14:17:10,388] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5765
[2019-04-04 14:17:10,415] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 27.33333333333334, 85.5, 824.0, 26.0, 24.96007330405834, 0.273352930592205, 0.0, 1.0, 18708.74471704271], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2468400.0000, 
sim time next is 2469000.0000, 
raw observation next is [2.1, 27.16666666666666, 84.0, 816.0, 26.0, 24.95792362850996, 0.2759195205289235, 0.0, 1.0, 18708.29397838229], 
processed observation next is [0.0, 0.5652173913043478, 0.5207756232686982, 0.2716666666666666, 0.28, 0.901657458563536, 0.6666666666666666, 0.5798269690424966, 0.5919731735096412, 0.0, 1.0, 0.08908711418277282], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.4260967], dtype=float32), 0.43486375]. 
=============================================
[2019-04-04 14:17:10,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[71.927315]
 [71.95709 ]
 [72.00227 ]
 [72.14814 ]
 [72.28839 ]], R is [[72.07654572]
 [72.26669312]
 [72.45493317]
 [72.73038483]
 [73.00308228]].
[2019-04-04 14:17:10,950] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.99345673e-09 4.95719576e-10 5.79648532e-15 5.93526083e-14
 1.00000000e+00 3.51725732e-10 1.05795304e-14], sum to 1.0000
[2019-04-04 14:17:10,953] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9320
[2019-04-04 14:17:10,967] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.3, 86.83333333333333, 0.0, 0.0, 26.0, 24.05852964535086, 0.07281397093592464, 0.0, 1.0, 43638.49779605116], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2260200.0000, 
sim time next is 2260800.0000, 
raw observation next is [-8.4, 87.0, 0.0, 0.0, 26.0, 24.05992540001927, 0.06347713894111033, 0.0, 1.0, 43610.77930646755], 
processed observation next is [1.0, 0.17391304347826086, 0.2299168975069252, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5049937833349393, 0.5211590463137035, 0.0, 1.0, 0.20767037764984547], 
reward next is 0.7923, 
noisyNet noise sample is [array([-0.75570333], dtype=float32), -0.50497097]. 
=============================================
[2019-04-04 14:17:12,718] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3564437e-08 6.7318856e-10 3.0586099e-14 3.1428056e-13 1.0000000e+00
 8.4119123e-10 2.1859273e-14], sum to 1.0000
[2019-04-04 14:17:12,718] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0701
[2019-04-04 14:17:12,734] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.65, 89.0, 0.0, 0.0, 26.0, 23.97351382773376, 0.04354473444138555, 0.0, 1.0, 43538.51014215021], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2262600.0000, 
sim time next is 2263200.0000, 
raw observation next is [-8.733333333333334, 89.66666666666667, 0.0, 0.0, 26.0, 23.9369191948702, 0.03040714814403973, 0.0, 1.0, 43500.83963723887], 
processed observation next is [1.0, 0.17391304347826086, 0.22068328716528163, 0.8966666666666667, 0.0, 0.0, 0.6666666666666666, 0.4947432662391833, 0.5101357160480132, 0.0, 1.0, 0.2071468554154232], 
reward next is 0.7929, 
noisyNet noise sample is [array([0.5987424], dtype=float32), 0.3958128]. 
=============================================
[2019-04-04 14:17:17,608] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8528159e-08 1.4155555e-09 2.0993329e-14 4.5276603e-13 1.0000000e+00
 1.6001933e-09 5.1831301e-14], sum to 1.0000
[2019-04-04 14:17:17,608] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5527
[2019-04-04 14:17:17,622] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.64195419516756, -0.02871173721943677, 0.0, 1.0, 43241.56119296171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2266800.0000, 
sim time next is 2267400.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.58457900591684, -0.03019884188586941, 0.0, 1.0, 43230.28110874936], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4653815838264033, 0.48993371937137686, 0.0, 1.0, 0.20585848147023506], 
reward next is 0.7941, 
noisyNet noise sample is [array([0.86527735], dtype=float32), -1.3434602]. 
=============================================
[2019-04-04 14:17:21,772] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0078659e-08 1.0945764e-09 2.1475517e-14 7.0403291e-14 1.0000000e+00
 7.1448597e-10 5.0850319e-14], sum to 1.0000
[2019-04-04 14:17:21,775] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4080
[2019-04-04 14:17:21,814] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 52.33333333333334, 0.0, 0.0, 26.0, 25.59389466081198, 0.4188468981392918, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2311800.0000, 
sim time next is 2312400.0000, 
raw observation next is [-1.2, 52.66666666666667, 0.0, 0.0, 26.0, 25.61565647178652, 0.3922011159901599, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.42936288088642666, 0.5266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6346380393155432, 0.6307337053300534, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6878362], dtype=float32), 1.633705]. 
=============================================
[2019-04-04 14:17:32,166] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.9240379e-08 7.0059731e-09 2.7446147e-13 1.0269975e-12 9.9999988e-01
 1.0927573e-09 1.3917257e-13], sum to 1.0000
[2019-04-04 14:17:32,169] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8700
[2019-04-04 14:17:32,182] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.2, 55.66666666666666, 0.0, 0.0, 26.0, 24.79494323634639, 0.1412547948469845, 0.0, 1.0, 38370.46576827975], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2523000.0000, 
sim time next is 2523600.0000, 
raw observation next is [-2.3, 57.0, 0.0, 0.0, 26.0, 24.76074188826838, 0.1412999191465761, 0.0, 1.0, 38370.52104918288], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.57, 0.0, 0.0, 0.6666666666666666, 0.5633951573556985, 0.5470999730488587, 0.0, 1.0, 0.18271676690087085], 
reward next is 0.8173, 
noisyNet noise sample is [array([-0.7110534], dtype=float32), -0.77670383]. 
=============================================
[2019-04-04 14:17:33,649] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8378807e-09 1.3123767e-10 1.3213477e-15 7.7275231e-15 1.0000000e+00
 1.3834177e-10 3.3171064e-15], sum to 1.0000
[2019-04-04 14:17:33,649] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7804
[2019-04-04 14:17:33,656] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8166666666666668, 40.33333333333333, 227.3333333333333, 54.66666666666667, 26.0, 25.74741459185379, 0.3264431407471187, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2548200.0000, 
sim time next is 2548800.0000, 
raw observation next is [1.1, 39.0, 225.0, 46.5, 26.0, 25.73664394078137, 0.3292070519885187, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.39, 0.75, 0.05138121546961326, 0.6666666666666666, 0.6447203283984475, 0.6097356839961728, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5208117], dtype=float32), 0.61271197]. 
=============================================
[2019-04-04 14:17:40,650] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4827417e-10 3.3252960e-12 5.6423670e-17 6.5158428e-16 1.0000000e+00
 3.0230308e-12 2.8595435e-17], sum to 1.0000
[2019-04-04 14:17:40,651] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2817
[2019-04-04 14:17:40,690] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 100.0, 86.66666666666666, 0.0, 26.0, 25.82629366828337, 0.4468724670667658, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2905800.0000, 
sim time next is 2906400.0000, 
raw observation next is [2.0, 100.0, 85.83333333333334, 0.0, 26.0, 25.85000565009864, 0.4496535647447541, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.28611111111111115, 0.0, 0.6666666666666666, 0.6541671375082201, 0.6498845215815847, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0790235], dtype=float32), 0.66968054]. 
=============================================
[2019-04-04 14:17:40,969] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.0983531e-09 3.9054165e-10 4.0826484e-15 2.5951104e-14 1.0000000e+00
 4.5046597e-10 1.5939598e-14], sum to 1.0000
[2019-04-04 14:17:40,969] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5126
[2019-04-04 14:17:41,021] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.33333333333333, 86.0, 94.16666666666667, 565.0, 26.0, 25.95344128287085, 0.4212757100768945, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2712000.0000, 
sim time next is 2712600.0000, 
raw observation next is [-13.0, 83.5, 97.0, 612.0, 26.0, 25.94188780073752, 0.4320249433653636, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.10249307479224376, 0.835, 0.3233333333333333, 0.6762430939226519, 0.6666666666666666, 0.6618239833947932, 0.6440083144551212, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29772264], dtype=float32), -0.52736366]. 
=============================================
[2019-04-04 14:17:43,493] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9263068e-09 2.8712877e-10 1.4636144e-15 3.0171866e-14 1.0000000e+00
 4.4956727e-10 1.2189600e-15], sum to 1.0000
[2019-04-04 14:17:43,493] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3522
[2019-04-04 14:17:43,519] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.783333333333333, 68.33333333333333, 0.0, 0.0, 26.0, 25.39660613236883, 0.4445530259197619, 0.0, 1.0, 63430.97901823181], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2670600.0000, 
sim time next is 2671200.0000, 
raw observation next is [-3.1, 69.0, 0.0, 0.0, 26.0, 25.41424802191314, 0.4430248998317579, 0.0, 1.0, 42680.76285183393], 
processed observation next is [1.0, 0.9565217391304348, 0.37673130193905824, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6178540018260948, 0.647674966610586, 0.0, 1.0, 0.20324172786587588], 
reward next is 0.7968, 
noisyNet noise sample is [array([1.5521026], dtype=float32), -0.037562434]. 
=============================================
[2019-04-04 14:17:46,270] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.2305843e-09 3.2777567e-10 1.3313233e-14 8.4596013e-14 1.0000000e+00
 7.9132861e-10 5.8898809e-14], sum to 1.0000
[2019-04-04 14:17:46,270] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3696
[2019-04-04 14:17:46,314] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 54.0, 103.6666666666667, 717.6666666666667, 26.0, 25.20018565604905, 0.3178488266382922, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3060600.0000, 
sim time next is 3061200.0000, 
raw observation next is [-4.0, 54.00000000000001, 104.8333333333333, 738.3333333333333, 26.0, 25.17872075009906, 0.3141546519922467, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.34944444444444434, 0.8158379373848986, 0.6666666666666666, 0.5982267291749217, 0.6047182173307489, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.6873794], dtype=float32), 0.47230977]. 
=============================================
[2019-04-04 14:17:52,446] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2101307e-09 5.6170246e-10 2.5142503e-14 1.6317097e-14 1.0000000e+00
 9.1623076e-10 4.4924557e-15], sum to 1.0000
[2019-04-04 14:17:52,447] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4366
[2019-04-04 14:17:52,521] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.666666666666667, 34.66666666666666, 0.0, 0.0, 26.0, 23.84201768133972, 0.2088335406845402, 1.0, 1.0, 198155.9279801271], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2832000.0000, 
sim time next is 2832600.0000, 
raw observation next is [3.333333333333333, 35.83333333333334, 0.0, 0.0, 26.0, 24.14699469236728, 0.2841723304560529, 1.0, 1.0, 199715.2682109395], 
processed observation next is [1.0, 0.782608695652174, 0.5549399815327793, 0.35833333333333345, 0.0, 0.0, 0.6666666666666666, 0.5122495576972733, 0.5947241101520176, 1.0, 1.0, 0.9510250867187596], 
reward next is 0.0490, 
noisyNet noise sample is [array([-0.4700263], dtype=float32), -1.1645395]. 
=============================================
[2019-04-04 14:17:55,874] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.4491024e-10 2.7684240e-11 6.0909504e-16 3.0525761e-15 1.0000000e+00
 7.3294017e-11 3.4828399e-16], sum to 1.0000
[2019-04-04 14:17:55,875] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0200
[2019-04-04 14:17:55,890] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.833333333333333, 94.16666666666666, 0.0, 0.0, 26.0, 24.82826854479635, 0.2347525435085972, 0.0, 1.0, 53904.06326436297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2875800.0000, 
sim time next is 2876400.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 24.80325140160042, 0.2324805487627906, 0.0, 1.0, 52862.80370236039], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.566937616800035, 0.5774935162542635, 0.0, 1.0, 0.2517276366779066], 
reward next is 0.7483, 
noisyNet noise sample is [array([-0.4205882], dtype=float32), 0.025747545]. 
=============================================
[2019-04-04 14:18:01,427] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2730240e-09 3.2701897e-09 1.7216023e-14 1.8888862e-13 1.0000000e+00
 4.3297593e-10 8.4539079e-14], sum to 1.0000
[2019-04-04 14:18:01,428] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0037
[2019-04-04 14:18:01,479] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.666666666666667, 63.33333333333333, 0.0, 0.0, 26.0, 25.16035637049238, 0.3718977954411314, 0.0, 1.0, 63048.927130315], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3008400.0000, 
sim time next is 3009000.0000, 
raw observation next is [-2.833333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 25.31563447688944, 0.3785604574208137, 0.0, 1.0, 47608.12581111841], 
processed observation next is [0.0, 0.8260869565217391, 0.3841181902123731, 0.6416666666666667, 0.0, 0.0, 0.6666666666666666, 0.6096362064074533, 0.6261868191402712, 0.0, 1.0, 0.22670536100532573], 
reward next is 0.7733, 
noisyNet noise sample is [array([0.32248518], dtype=float32), 2.251598]. 
=============================================
[2019-04-04 14:18:01,486] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.19714]
 [77.73024]
 [76.99069]
 [76.08603]
 [75.86101]], R is [[78.54177856]
 [78.45613098]
 [78.16190338]
 [77.55304718]
 [77.60341644]].
[2019-04-04 14:18:09,052] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9979862e-09 7.5391160e-12 3.8749534e-16 1.1587554e-15 1.0000000e+00
 9.2060089e-11 4.1073591e-16], sum to 1.0000
[2019-04-04 14:18:09,054] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8367
[2019-04-04 14:18:09,090] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.62603199400968, 0.4647106498214083, 0.0, 1.0, 26646.07216546434], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3449400.0000, 
sim time next is 3450000.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.52007861668139, 0.4558429605449961, 0.0, 1.0, 88732.61916906358], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6266732180567823, 0.6519476535149987, 0.0, 1.0, 0.4225362817574456], 
reward next is 0.5775, 
noisyNet noise sample is [array([-0.16831867], dtype=float32), 0.6988948]. 
=============================================
[2019-04-04 14:18:09,096] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[86.2138 ]
 [85.94039]
 [86.32568]
 [86.19432]
 [86.53419]], R is [[85.93091583]
 [85.94472504]
 [86.08528137]
 [86.22442627]
 [86.36218262]].
[2019-04-04 14:18:09,875] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.8281507e-10 2.0887131e-10 6.5996729e-15 9.4540998e-15 1.0000000e+00
 7.8879507e-11 2.0229936e-15], sum to 1.0000
[2019-04-04 14:18:09,878] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2214
[2019-04-04 14:18:09,924] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 26.0, 24.99820592090267, 0.2753266142761597, 0.0, 1.0, 37387.66488556704], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3092400.0000, 
sim time next is 3093000.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 26.0, 24.99350177419, 0.2783292002798468, 0.0, 1.0, 37479.47311418958], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5827918145158334, 0.5927764000932823, 0.0, 1.0, 0.17847368149614087], 
reward next is 0.8215, 
noisyNet noise sample is [array([0.7209081], dtype=float32), -0.86305803]. 
=============================================
[2019-04-04 14:18:09,928] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[78.81701 ]
 [78.067154]
 [77.38248 ]
 [76.732574]
 [76.129425]], R is [[79.53070831]
 [79.55736542]
 [79.59916687]
 [79.65619659]
 [79.71498108]].
[2019-04-04 14:18:13,340] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3440903e-09 6.7890205e-10 6.6519424e-15 1.9598338e-14 1.0000000e+00
 1.6629978e-10 3.1784664e-15], sum to 1.0000
[2019-04-04 14:18:13,340] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2683
[2019-04-04 14:18:13,355] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.93397171459103, 0.3468236844270435, 0.0, 1.0, 43831.2793370566], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3291600.0000, 
sim time next is 3292200.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.95738267513963, 0.346324439594184, 0.0, 1.0, 43832.43231868131], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5797818895949692, 0.615441479864728, 0.0, 1.0, 0.20872586818419672], 
reward next is 0.7913, 
noisyNet noise sample is [array([-2.5474894], dtype=float32), -0.15225206]. 
=============================================
[2019-04-04 14:18:15,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9832889e-09 5.2356799e-11 1.8553971e-16 2.9387823e-15 1.0000000e+00
 6.5204717e-11 4.2571026e-16], sum to 1.0000
[2019-04-04 14:18:15,514] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3527
[2019-04-04 14:18:15,558] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.5, 74.0, 59.0, 511.0, 26.0, 26.9231564903653, 0.7846631091435676, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3256200.0000, 
sim time next is 3256800.0000, 
raw observation next is [-3.666666666666667, 75.0, 50.66666666666667, 443.1666666666667, 26.0, 26.27174159537525, 0.727645812329234, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3610341643582641, 0.75, 0.1688888888888889, 0.48968692449355433, 0.6666666666666666, 0.6893117996146042, 0.7425486041097447, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5366369], dtype=float32), -0.5202812]. 
=============================================
[2019-04-04 14:18:16,204] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7332943e-10 7.8641399e-11 4.5172336e-16 2.4792963e-14 1.0000000e+00
 2.9168265e-10 6.9674770e-16], sum to 1.0000
[2019-04-04 14:18:16,206] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5311
[2019-04-04 14:18:16,222] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 79.33333333333334, 0.0, 0.0, 26.0, 25.08947494594756, 0.4037403428312611, 0.0, 1.0, 43564.83996773622], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3284400.0000, 
sim time next is 3285000.0000, 
raw observation next is [-7.0, 77.0, 0.0, 0.0, 26.0, 25.04911610701066, 0.3998283620740503, 0.0, 1.0, 43632.12855199828], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5874263422508884, 0.6332761206913501, 0.0, 1.0, 0.20777204072380132], 
reward next is 0.7922, 
noisyNet noise sample is [array([0.52808726], dtype=float32), 0.4709615]. 
=============================================
[2019-04-04 14:18:16,227] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.35218 ]
 [83.587685]
 [83.99161 ]
 [84.49267 ]
 [84.602135]], R is [[83.04904938]
 [83.0111084 ]
 [82.97389984]
 [82.93742371]
 [82.90167236]].
[2019-04-04 14:18:23,144] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0080519e-09 1.1132708e-10 4.6460874e-16 1.0818607e-14 1.0000000e+00
 3.8351110e-11 2.2363643e-15], sum to 1.0000
[2019-04-04 14:18:23,145] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8045
[2019-04-04 14:18:23,187] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 60.00000000000001, 72.83333333333333, 369.5, 26.0, 25.49433370361218, 0.4223484903349826, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3399600.0000, 
sim time next is 3400200.0000, 
raw observation next is [-1.5, 60.0, 87.0, 422.0, 26.0, 25.73141218046219, 0.4458879237422879, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4210526315789474, 0.6, 0.29, 0.4662983425414365, 0.6666666666666666, 0.6442843483718491, 0.648629307914096, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15266211], dtype=float32), -0.68673325]. 
=============================================
[2019-04-04 14:18:27,308] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1900017e-09 4.8158425e-11 7.2764387e-16 1.9066070e-14 1.0000000e+00
 6.0847188e-11 1.8891212e-16], sum to 1.0000
[2019-04-04 14:18:27,308] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0873
[2019-04-04 14:18:27,324] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 26.0, 25.34175921253728, 0.4290168360731326, 0.0, 1.0, 47807.67356126707], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3466800.0000, 
sim time next is 3467400.0000, 
raw observation next is [1.0, 71.16666666666667, 0.0, 0.0, 26.0, 25.38547058686921, 0.438442694287536, 0.0, 1.0, 36075.1559132279], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.7116666666666667, 0.0, 0.0, 0.6666666666666666, 0.6154558822391009, 0.646147564762512, 0.0, 1.0, 0.17178645672965667], 
reward next is 0.8282, 
noisyNet noise sample is [array([-0.50048995], dtype=float32), 0.75817996]. 
=============================================
[2019-04-04 14:18:27,991] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.4784748e-09 6.4256933e-10 2.4015959e-14 5.5954559e-14 1.0000000e+00
 7.7407009e-10 1.6717432e-14], sum to 1.0000
[2019-04-04 14:18:27,991] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4764
[2019-04-04 14:18:28,003] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.46515550888763, 0.495297474908708, 1.0, 1.0, 51194.53002790745], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3524400.0000, 
sim time next is 3525000.0000, 
raw observation next is [1.666666666666667, 55.33333333333334, 0.0, 0.0, 26.0, 25.4144266133524, 0.4844496958027862, 1.0, 1.0, 34544.17399593336], 
processed observation next is [1.0, 0.8260869565217391, 0.5087719298245615, 0.5533333333333335, 0.0, 0.0, 0.6666666666666666, 0.6178688844460334, 0.6614832319342621, 1.0, 1.0, 0.1644960666473017], 
reward next is 0.8355, 
noisyNet noise sample is [array([-1.0401661], dtype=float32), 0.22191466]. 
=============================================
[2019-04-04 14:18:28,016] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[81.53722]
 [81.11248]
 [80.56579]
 [80.53968]
 [80.62214]], R is [[81.89319611]
 [81.83048248]
 [81.58174896]
 [81.76593018]
 [81.94827271]].
[2019-04-04 14:18:28,431] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7449741e-09 1.0636853e-10 2.1018398e-15 6.8956681e-14 1.0000000e+00
 9.2489399e-10 2.6389341e-15], sum to 1.0000
[2019-04-04 14:18:28,436] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8970
[2019-04-04 14:18:28,448] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6666666666666666, 76.0, 0.0, 0.0, 26.0, 25.87608920868532, 0.5954081327904511, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3534000.0000, 
sim time next is 3534600.0000, 
raw observation next is [-0.8333333333333334, 77.0, 0.0, 0.0, 26.0, 25.85910933267354, 0.5809824722325941, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.43951985226223456, 0.77, 0.0, 0.0, 0.6666666666666666, 0.654925777722795, 0.6936608240775314, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2513425], dtype=float32), -0.5685613]. 
=============================================
[2019-04-04 14:18:34,703] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7203702e-08 2.1407807e-09 6.1767350e-14 2.3706858e-13 1.0000000e+00
 5.4366006e-10 4.5296276e-14], sum to 1.0000
[2019-04-04 14:18:34,705] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9902
[2019-04-04 14:18:34,722] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.333333333333333, 53.33333333333334, 0.0, 0.0, 26.0, 25.38964750505538, 0.3793990042757614, 0.0, 1.0, 45743.76644619827], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3622800.0000, 
sim time next is 3623400.0000, 
raw observation next is [-2.5, 55.0, 0.0, 0.0, 26.0, 25.36773645822509, 0.3777332361764192, 0.0, 1.0, 51125.95159442589], 
processed observation next is [0.0, 0.9565217391304348, 0.39335180055401664, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6139780381854241, 0.625911078725473, 0.0, 1.0, 0.243456912354409], 
reward next is 0.7565, 
noisyNet noise sample is [array([-1.020151], dtype=float32), 0.11602083]. 
=============================================
[2019-04-04 14:18:36,181] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.3279215e-09 2.8503019e-10 1.3257763e-14 3.0526474e-14 1.0000000e+00
 1.4983353e-10 2.9435956e-15], sum to 1.0000
[2019-04-04 14:18:36,185] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4166
[2019-04-04 14:18:36,214] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 67.33333333333333, 14.16666666666667, 122.5, 26.0, 25.81188806331881, 0.3728203982950038, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3778800.0000, 
sim time next is 3779400.0000, 
raw observation next is [-1.666666666666667, 69.16666666666667, 0.0, 0.0, 26.0, 25.55645233927689, 0.4889623702787121, 1.0, 1.0, 187265.6294137096], 
processed observation next is [1.0, 0.7391304347826086, 0.4164358264081256, 0.6916666666666668, 0.0, 0.0, 0.6666666666666666, 0.6297043616064076, 0.6629874567595707, 1.0, 1.0, 0.8917410924462362], 
reward next is 0.1083, 
noisyNet noise sample is [array([-1.94634], dtype=float32), -1.2243065]. 
=============================================
[2019-04-04 14:18:38,515] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6812867e-09 1.4703361e-10 1.9876225e-15 2.2466949e-14 1.0000000e+00
 6.4177039e-11 2.6512339e-15], sum to 1.0000
[2019-04-04 14:18:38,518] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1376
[2019-04-04 14:18:38,534] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6666666666666667, 68.66666666666667, 0.0, 0.0, 26.0, 25.45377332163844, 0.4048452691155539, 0.0, 1.0, 89483.33968345194], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3706800.0000, 
sim time next is 3707400.0000, 
raw observation next is [0.3333333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 25.42027686022217, 0.4093116291745429, 0.0, 1.0, 77008.02812778679], 
processed observation next is [0.0, 0.9130434782608695, 0.4718374884579871, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.6183564050185142, 0.6364372097248476, 0.0, 1.0, 0.36670489584660376], 
reward next is 0.6333, 
noisyNet noise sample is [array([0.95161605], dtype=float32), -0.13718773]. 
=============================================
[2019-04-04 14:18:43,364] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.9283875e-09 1.5992069e-10 1.3974112e-15 8.4135274e-15 1.0000000e+00
 6.3423669e-11 3.8137574e-16], sum to 1.0000
[2019-04-04 14:18:43,365] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2874
[2019-04-04 14:18:43,401] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 67.33333333333333, 0.0, 0.0, 26.0, 25.41439505268708, 0.4020171365893308, 0.0, 1.0, 45440.98523291413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3710400.0000, 
sim time next is 3711000.0000, 
raw observation next is [-2.5, 66.16666666666667, 0.0, 0.0, 26.0, 25.41026685667869, 0.39698047646368, 0.0, 1.0, 44194.22331109627], 
processed observation next is [0.0, 0.9565217391304348, 0.39335180055401664, 0.6616666666666667, 0.0, 0.0, 0.6666666666666666, 0.6175222380565574, 0.6323268254878933, 0.0, 1.0, 0.21044868243379175], 
reward next is 0.7896, 
noisyNet noise sample is [array([-0.20158274], dtype=float32), 1.6831264]. 
=============================================
[2019-04-04 14:18:43,435] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[83.04554]
 [83.13568]
 [83.18609]
 [83.2415 ]
 [83.21595]], R is [[82.89073944]
 [82.84545135]
 [82.83948517]
 [82.85025024]
 [82.83181   ]].
[2019-04-04 14:18:46,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5127753e-09 6.9962758e-10 1.7192788e-14 6.5818255e-14 1.0000000e+00
 2.9471198e-10 4.2719086e-15], sum to 1.0000
[2019-04-04 14:18:46,626] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4477
[2019-04-04 14:18:46,640] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333334, 54.00000000000001, 0.0, 0.0, 26.0, 25.09843658111749, 0.4479625010620185, 0.0, 1.0, 197174.018733413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3874800.0000, 
sim time next is 3875400.0000, 
raw observation next is [0.0, 55.5, 0.0, 0.0, 26.0, 25.06955424277707, 0.4704378707263102, 0.0, 1.0, 198686.1745785816], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.555, 0.0, 0.0, 0.6666666666666666, 0.5891295202314225, 0.6568126235754367, 0.0, 1.0, 0.9461246408503885], 
reward next is 0.0539, 
noisyNet noise sample is [array([0.86860573], dtype=float32), -0.33800372]. 
=============================================
[2019-04-04 14:18:50,015] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8745052e-09 1.2479033e-10 5.0753563e-16 8.8487728e-15 1.0000000e+00
 1.5507172e-10 1.7930936e-15], sum to 1.0000
[2019-04-04 14:18:50,016] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2246
[2019-04-04 14:18:50,054] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.666666666666666, 42.66666666666667, 116.5, 825.8333333333334, 26.0, 25.72538801491692, 0.6518016156106955, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3936000.0000, 
sim time next is 3936600.0000, 
raw observation next is [-5.5, 41.5, 116.0, 824.0, 26.0, 26.30913480052656, 0.6939995124403638, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3102493074792244, 0.415, 0.38666666666666666, 0.9104972375690608, 0.6666666666666666, 0.6924279000438801, 0.7313331708134546, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3681813], dtype=float32), 1.4064449]. 
=============================================
[2019-04-04 14:18:59,367] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.34024205e-08 2.47376652e-09 1.28018012e-13 3.33707400e-13
 1.00000000e+00 2.94326763e-09 6.88757116e-14], sum to 1.0000
[2019-04-04 14:18:59,368] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3574
[2019-04-04 14:18:59,379] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.833333333333334, 30.66666666666667, 0.0, 0.0, 26.0, 25.49572863067396, 0.4711192020354771, 0.0, 1.0, 71657.07467373225], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4053000.0000, 
sim time next is 4053600.0000, 
raw observation next is [-5.0, 31.0, 0.0, 0.0, 26.0, 25.44516351167429, 0.4721105449063848, 0.0, 1.0, 79042.68709346045], 
processed observation next is [1.0, 0.9565217391304348, 0.32409972299168976, 0.31, 0.0, 0.0, 0.6666666666666666, 0.6204302926395243, 0.6573701816354616, 0.0, 1.0, 0.37639374806409737], 
reward next is 0.6236, 
noisyNet noise sample is [array([-1.3027899], dtype=float32), -0.06633964]. 
=============================================
[2019-04-04 14:19:01,773] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1555560e-09 1.5082073e-10 2.1933853e-15 5.5481403e-15 1.0000000e+00
 7.1669261e-11 3.5356782e-15], sum to 1.0000
[2019-04-04 14:19:01,775] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1912
[2019-04-04 14:19:01,790] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8166666666666667, 71.33333333333334, 0.0, 0.0, 26.0, 25.31310932262422, 0.4165383865768246, 0.0, 1.0, 41572.21568794777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4510200.0000, 
sim time next is 4510800.0000, 
raw observation next is [-0.8, 71.0, 0.0, 0.0, 26.0, 25.29368501946821, 0.4193079278954636, 0.0, 1.0, 41463.81249980471], 
processed observation next is [1.0, 0.21739130434782608, 0.4404432132963989, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6078070849556841, 0.6397693092984879, 0.0, 1.0, 0.19744672618954626], 
reward next is 0.8026, 
noisyNet noise sample is [array([-0.54080164], dtype=float32), -1.8734322]. 
=============================================
[2019-04-04 14:19:02,796] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.3745907e-09 1.0751737e-09 1.5051408e-14 4.6697623e-13 1.0000000e+00
 2.6883676e-10 3.9418389e-14], sum to 1.0000
[2019-04-04 14:19:02,796] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8233
[2019-04-04 14:19:02,807] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 51.33333333333333, 0.0, 0.0, 26.0, 24.88846088098413, 0.2708406701187112, 0.0, 1.0, 39493.55192067858], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4167600.0000, 
sim time next is 4168200.0000, 
raw observation next is [-4.0, 50.66666666666667, 0.0, 0.0, 26.0, 24.85208107044593, 0.2633030714628557, 0.0, 1.0, 39545.59575188368], 
processed observation next is [0.0, 0.21739130434782608, 0.3518005540166205, 0.5066666666666667, 0.0, 0.0, 0.6666666666666666, 0.5710067558704942, 0.5877676904876186, 0.0, 1.0, 0.18831236072325563], 
reward next is 0.8117, 
noisyNet noise sample is [array([-0.8948831], dtype=float32), -1.1757851]. 
=============================================
[2019-04-04 14:19:10,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.7475801e-10 1.2820367e-10 6.4637269e-17 2.9709520e-15 1.0000000e+00
 9.5789574e-12 5.6337458e-16], sum to 1.0000
[2019-04-04 14:19:10,460] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9265
[2019-04-04 14:19:10,481] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.5, 54.5, 204.0, 604.0, 26.0, 25.4238525532873, 0.4055423158754672, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4271400.0000, 
sim time next is 4272000.0000, 
raw observation next is [4.666666666666666, 54.66666666666667, 190.1666666666667, 640.3333333333334, 26.0, 25.35570033315041, 0.4006337403883345, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.5918744228993538, 0.5466666666666667, 0.6338888888888891, 0.7075506445672192, 0.6666666666666666, 0.6129750277625341, 0.6335445801294448, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6627283], dtype=float32), 0.6754776]. 
=============================================
[2019-04-04 14:19:10,492] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[86.72932]
 [86.41173]
 [86.09559]
 [85.7446 ]
 [85.36302]], R is [[86.98275757]
 [87.1129303 ]
 [87.2417984 ]
 [87.36938477]
 [87.49568939]].
[2019-04-04 14:19:18,637] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5359722e-09 4.4620657e-10 2.5775869e-15 2.5218099e-14 1.0000000e+00
 5.6843419e-10 9.1089296e-15], sum to 1.0000
[2019-04-04 14:19:18,639] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6763
[2019-04-04 14:19:18,656] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8333333333333334, 47.66666666666667, 0.0, 0.0, 26.0, 25.42882112562595, 0.4388468548562333, 0.0, 1.0, 18764.58402176093], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4824600.0000, 
sim time next is 4825200.0000, 
raw observation next is [0.6666666666666667, 48.33333333333334, 0.0, 0.0, 26.0, 25.50509392588227, 0.4389176182284046, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4810710987996307, 0.48333333333333345, 0.0, 0.0, 0.6666666666666666, 0.6254244938235226, 0.6463058727428015, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6611142], dtype=float32), 1.5179268]. 
=============================================
[2019-04-04 14:19:19,579] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1351970e-10 1.0134687e-10 3.6524500e-16 3.5478106e-15 1.0000000e+00
 1.6774599e-11 1.6385880e-16], sum to 1.0000
[2019-04-04 14:19:19,580] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7207
[2019-04-04 14:19:19,587] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 57.0, 169.5, 9.0, 26.0, 26.26714157214334, 0.5499694676878092, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4532400.0000, 
sim time next is 4533000.0000, 
raw observation next is [2.0, 55.5, 152.3333333333333, 5.999999999999998, 26.0, 26.27042982709248, 0.5494213026457123, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.518005540166205, 0.555, 0.5077777777777777, 0.006629834254143645, 0.6666666666666666, 0.6892024855910401, 0.6831404342152374, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8692916], dtype=float32), 0.9679202]. 
=============================================
[2019-04-04 14:19:19,601] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[89.11273 ]
 [89.37665 ]
 [89.70064 ]
 [89.527885]
 [89.53345 ]], R is [[88.76757812]
 [88.8799057 ]
 [88.99110413]
 [89.10119629]
 [89.21018219]].
[2019-04-04 14:19:22,028] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.1148466e-10 1.1936797e-10 2.9427071e-16 2.0380836e-14 1.0000000e+00
 6.8826306e-11 9.2835520e-16], sum to 1.0000
[2019-04-04 14:19:22,033] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6095
[2019-04-04 14:19:22,063] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3666666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.34360186840387, 0.4828900281676521, 0.0, 1.0, 43818.89048200356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4490400.0000, 
sim time next is 4491000.0000, 
raw observation next is [-0.4, 72.5, 0.0, 0.0, 26.0, 25.34834238370953, 0.4799658054092319, 0.0, 1.0, 43633.5091928421], 
processed observation next is [1.0, 1.0, 0.45152354570637127, 0.725, 0.0, 0.0, 0.6666666666666666, 0.6123618653091274, 0.6599886018030773, 0.0, 1.0, 0.20777861520401], 
reward next is 0.7922, 
noisyNet noise sample is [array([-1.3034878], dtype=float32), -0.6731055]. 
=============================================
[2019-04-04 14:19:22,081] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[85.80057]
 [85.77927]
 [85.79668]
 [85.70504]
 [85.68088]], R is [[85.69181061]
 [85.62622833]
 [85.55900574]
 [85.48537445]
 [85.39055634]].
[2019-04-04 14:19:24,399] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-04 14:19:24,400] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 14:19:24,400] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:19:24,401] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 14:19:24,401] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:19:24,401] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 14:19:24,402] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:19:24,406] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run24
[2019-04-04 14:19:24,427] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run24
[2019-04-04 14:19:24,446] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run24
[2019-04-04 14:20:59,185] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17350066], dtype=float32), 0.2092474]
[2019-04-04 14:20:59,185] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [5.5, 48.5, 87.0, 705.0, 26.0, 25.5168452757637, 0.4879658518173212, 0.0, 1.0, 0.0]
[2019-04-04 14:20:59,185] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 14:20:59,186] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.1312759e-09 4.5275464e-10 3.4759795e-15 2.4260614e-14 1.0000000e+00
 1.4231022e-10 5.0181434e-15], sampled 0.4402463727000817
[2019-04-04 14:21:03,272] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17350066], dtype=float32), 0.2092474]
[2019-04-04 14:21:03,273] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.21442428862093, 0.3415776827169387, 0.0, 1.0, 41100.13441992427]
[2019-04-04 14:21:03,273] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 14:21:03,275] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.9095546e-09 3.7327849e-10 2.5487429e-15 3.2088149e-14 1.0000000e+00
 8.4821539e-11 4.5558789e-15], sampled 0.9773214871623555
[2019-04-04 14:21:07,865] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 14:21:15,907] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17350066], dtype=float32), 0.2092474]
[2019-04-04 14:21:15,908] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.475393743, 64.50222940833333, 0.0, 0.0, 26.0, 25.43293356316129, 0.4011851630802188, 0.0, 1.0, 40695.95393100064]
[2019-04-04 14:21:15,908] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 14:21:15,909] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2904079e-09 1.7111298e-10 5.6768487e-16 9.0714450e-15 1.0000000e+00
 3.2725576e-11 9.9083362e-16], sampled 0.962188260571578
[2019-04-04 14:21:24,404] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 14:21:29,712] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6651 275800324.5281 1233.1507
[2019-04-04 14:21:30,736] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 2300000, evaluation results [2300000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.665121294629, 275800324.5281271, 1233.1506611100626]
[2019-04-04 14:21:33,907] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.3422159e-10 2.1569733e-10 9.8632709e-16 3.8546102e-15 1.0000000e+00
 5.7747137e-11 1.4348840e-15], sum to 1.0000
[2019-04-04 14:21:33,917] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5673
[2019-04-04 14:21:33,950] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.5, 45.5, 132.3333333333333, 802.6666666666666, 26.0, 25.18784131070507, 0.4224880177364347, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4792200.0000, 
sim time next is 4792800.0000, 
raw observation next is [-1.0, 45.0, 127.1666666666667, 820.8333333333334, 26.0, 25.14243448514788, 0.4150316274213748, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4349030470914128, 0.45, 0.423888888888889, 0.9069981583793739, 0.6666666666666666, 0.5952028737623234, 0.6383438758071249, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21549426], dtype=float32), 0.713097]. 
=============================================
[2019-04-04 14:21:34,471] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:34,472] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:34,488] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run18
[2019-04-04 14:21:36,523] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.45414283e-09 6.14980400e-10 3.91552034e-15 2.46660805e-14
 1.00000000e+00 3.00412750e-11 1.39126856e-14], sum to 1.0000
[2019-04-04 14:21:36,523] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4929
[2019-04-04 14:21:36,545] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 60.00000000000001, 0.0, 0.0, 26.0, 24.95514218673161, 0.2693676877633909, 0.0, 1.0, 39209.79084567583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4850400.0000, 
sim time next is 4851000.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.92700014521915, 0.2658313413502723, 0.0, 1.0, 39215.05481711665], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5772500121015959, 0.5886104471167575, 0.0, 1.0, 0.18673835627198407], 
reward next is 0.8133, 
noisyNet noise sample is [array([1.139741], dtype=float32), 0.0011898269]. 
=============================================
[2019-04-04 14:21:36,557] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[79.472466]
 [79.5942  ]
 [79.67548 ]
 [79.76014 ]
 [79.96545 ]], R is [[79.37483978]
 [79.39437866]
 [79.41378784]
 [79.43309784]
 [79.45230103]].
[2019-04-04 14:21:38,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:38,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:38,588] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run18
[2019-04-04 14:21:39,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:39,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:39,192] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run18
[2019-04-04 14:21:40,442] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3374728e-10 8.5072019e-11 7.1513954e-16 3.1273390e-15 1.0000000e+00
 1.4091668e-11 9.9153306e-16], sum to 1.0000
[2019-04-04 14:21:40,442] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4332
[2019-04-04 14:21:40,495] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.4, 52.0, 291.5, 236.0, 26.0, 24.99026842844929, 0.3281487839978406, 0.0, 1.0, 38023.56750074068], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4878000.0000, 
sim time next is 4878600.0000, 
raw observation next is [-0.1666666666666667, 51.16666666666667, 288.6666666666666, 260.0, 26.0, 24.98107374785664, 0.3353746523211783, 0.0, 1.0, 30571.12215054053], 
processed observation next is [0.0, 0.4782608695652174, 0.4579870729455217, 0.5116666666666667, 0.9622222222222219, 0.287292817679558, 0.6666666666666666, 0.5817561456547201, 0.611791550773726, 0.0, 1.0, 0.1455767721454311], 
reward next is 0.8544, 
noisyNet noise sample is [array([0.2943451], dtype=float32), 1.1537402]. 
=============================================
[2019-04-04 14:21:45,087] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6853766e-09 1.7321589e-10 3.1534905e-15 1.0928860e-14 1.0000000e+00
 9.0529341e-11 2.8120361e-15], sum to 1.0000
[2019-04-04 14:21:45,088] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8648
[2019-04-04 14:21:45,100] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.4, 45.0, 276.5, 389.0, 26.0, 25.03287757683555, 0.353932023894495, 0.0, 1.0, 31889.33204157083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4885200.0000, 
sim time next is 4885800.0000, 
raw observation next is [1.5, 44.83333333333334, 275.0, 388.6666666666666, 26.0, 25.03669436759947, 0.3590309517171126, 0.0, 1.0, 20252.89748689447], 
processed observation next is [0.0, 0.5652173913043478, 0.5041551246537397, 0.4483333333333334, 0.9166666666666666, 0.4294659300184161, 0.6666666666666666, 0.5863911972999558, 0.6196769839057042, 0.0, 1.0, 0.09644236898521177], 
reward next is 0.9036, 
noisyNet noise sample is [array([1.110839], dtype=float32), -0.32657254]. 
=============================================
[2019-04-04 14:21:49,164] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:49,164] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:49,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run18
[2019-04-04 14:21:50,736] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5621056e-09 1.2866126e-10 1.0794692e-15 5.9491308e-15 1.0000000e+00
 1.5120526e-10 1.4014785e-16], sum to 1.0000
[2019-04-04 14:21:50,739] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7425
[2019-04-04 14:21:50,764] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.833333333333334, 24.16666666666666, 121.0, 863.3333333333334, 26.0, 27.0724507836845, 0.7330368138682734, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4971000.0000, 
sim time next is 4971600.0000, 
raw observation next is [7.0, 24.0, 120.0, 862.5, 26.0, 27.06856101633506, 0.6264462167803562, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 0.24, 0.4, 0.9530386740331491, 0.6666666666666666, 0.7557134180279217, 0.708815405593452, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.72692066], dtype=float32), -1.3742739]. 
=============================================
[2019-04-04 14:21:51,511] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:51,512] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:51,526] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run18
[2019-04-04 14:21:52,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:52,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:52,755] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run18
[2019-04-04 14:21:53,030] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:53,030] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:53,039] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run18
[2019-04-04 14:21:53,254] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.61920728e-11 1.13163454e-11 2.11647295e-17 2.84313147e-16
 1.00000000e+00 8.15043779e-12 1.07330895e-17], sum to 1.0000
[2019-04-04 14:21:53,254] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7494
[2019-04-04 14:21:53,277] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.666666666666667, 42.0, 113.3333333333333, 735.0, 26.0, 26.91768281942381, 0.6926869678016346, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5046600.0000, 
sim time next is 5047200.0000, 
raw observation next is [3.0, 41.0, 114.0, 753.5, 26.0, 26.96945510205893, 0.719194433574519, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5457063711911359, 0.41, 0.38, 0.8325966850828729, 0.6666666666666666, 0.7474545918382441, 0.739731477858173, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-3.1661406], dtype=float32), 1.9819285]. 
=============================================
[2019-04-04 14:21:53,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:53,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:53,305] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run18
[2019-04-04 14:21:53,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:53,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:53,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run18
[2019-04-04 14:21:53,847] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:53,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:53,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run18
[2019-04-04 14:21:54,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:54,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:54,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run18
[2019-04-04 14:21:54,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:54,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:54,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run18
[2019-04-04 14:21:56,257] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:56,257] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:56,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run18
[2019-04-04 14:21:56,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:56,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:56,302] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run18
[2019-04-04 14:21:56,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:56,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:56,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run18
[2019-04-04 14:21:58,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:21:58,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:21:58,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run18
[2019-04-04 14:22:14,780] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4533517e-08 1.5930945e-09 6.6271627e-14 2.2616841e-13 1.0000000e+00
 3.7758872e-09 2.5614702e-13], sum to 1.0000
[2019-04-04 14:22:14,781] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4215
[2019-04-04 14:22:14,805] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 69.5, 0.0, 0.0, 26.0, 24.44995581229495, 0.1770245572416284, 0.0, 1.0, 45819.75970041726], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 163800.0000, 
sim time next is 164400.0000, 
raw observation next is [-8.4, 70.0, 0.0, 0.0, 26.0, 24.40197368745733, 0.1658307954239472, 0.0, 1.0, 45767.17588713873], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5334978072881107, 0.5552769318079824, 0.0, 1.0, 0.2179389327958987], 
reward next is 0.7821, 
noisyNet noise sample is [array([-0.3343975], dtype=float32), -0.57507133]. 
=============================================
[2019-04-04 14:22:15,170] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8627927e-08 1.3199410e-09 1.9590564e-14 3.1332771e-13 1.0000000e+00
 1.6749551e-09 2.3972162e-14], sum to 1.0000
[2019-04-04 14:22:15,178] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6452
[2019-04-04 14:22:15,197] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.95609095355573, 0.05982941513401185, 0.0, 1.0, 44755.93585768347], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 171000.0000, 
sim time next is 171600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.91261870301451, 0.05040659934045044, 0.0, 1.0, 44700.71208088408], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.49271822525120906, 0.5168021997801501, 0.0, 1.0, 0.21286053371849561], 
reward next is 0.7871, 
noisyNet noise sample is [array([2.188821], dtype=float32), 0.39131072]. 
=============================================
[2019-04-04 14:22:17,958] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.18788066e-07 5.24696153e-09 1.14518231e-13 2.73545943e-12
 9.99999881e-01 1.04529763e-08 2.45676770e-13], sum to 1.0000
[2019-04-04 14:22:17,959] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2557
[2019-04-04 14:22:18,062] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.100000000000001, 37.33333333333334, 0.0, 0.0, 26.0, 25.85797262783007, 0.2129125012504537, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 408000.0000, 
sim time next is 408600.0000, 
raw observation next is [-9.2, 38.0, 0.0, 0.0, 26.0, 25.542799583968, 0.2921496416828831, 1.0, 1.0, 184854.7162698521], 
processed observation next is [1.0, 0.7391304347826086, 0.20775623268698065, 0.38, 0.0, 0.0, 0.6666666666666666, 0.6285666319973334, 0.5973832138942944, 1.0, 1.0, 0.8802605536659623], 
reward next is 0.1197, 
noisyNet noise sample is [array([0.20153162], dtype=float32), 1.2760208]. 
=============================================
[2019-04-04 14:22:19,084] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0943919e-08 7.2436834e-10 2.0494731e-15 2.7504737e-13 1.0000000e+00
 2.0479933e-10 7.2337105e-15], sum to 1.0000
[2019-04-04 14:22:19,084] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1696
[2019-04-04 14:22:19,158] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.666666666666666, 76.0, 86.5, 0.0, 26.0, 25.39277262474626, 0.2186697964266428, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 207600.0000, 
sim time next is 208200.0000, 
raw observation next is [-7.483333333333333, 75.5, 94.0, 0.0, 26.0, 25.39066981908058, 0.211145755858431, 1.0, 1.0, 18738.8136536837], 
processed observation next is [1.0, 0.391304347826087, 0.25530932594644506, 0.755, 0.31333333333333335, 0.0, 0.6666666666666666, 0.6158891515900482, 0.5703819186194771, 1.0, 1.0, 0.08923244596992239], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.1871032], dtype=float32), 0.12667254]. 
=============================================
[2019-04-04 14:22:37,307] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.7144810e-08 2.7333842e-08 2.9028592e-12 1.7777117e-11 9.9999988e-01
 7.3377029e-09 2.9046801e-13], sum to 1.0000
[2019-04-04 14:22:37,307] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1243
[2019-04-04 14:22:37,319] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.34864128213042, -0.1233058883135493, 0.0, 1.0, 45747.46696418092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 439200.0000, 
sim time next is 439800.0000, 
raw observation next is [-11.1, 54.0, 0.0, 0.0, 26.0, 23.33213529098965, -0.1345155214524445, 0.0, 1.0, 45794.18912860256], 
processed observation next is [1.0, 0.08695652173913043, 0.1551246537396122, 0.54, 0.0, 0.0, 0.6666666666666666, 0.4443446075824709, 0.45516149284918517, 0.0, 1.0, 0.2180675672790598], 
reward next is 0.7819, 
noisyNet noise sample is [array([0.179541], dtype=float32), -0.34409437]. 
=============================================
[2019-04-04 14:22:40,542] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.0443229e-09 2.0459641e-09 4.8521713e-14 4.4854734e-13 1.0000000e+00
 1.7756769e-09 3.8454035e-14], sum to 1.0000
[2019-04-04 14:22:40,543] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2902
[2019-04-04 14:22:40,602] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.1, 29.16666666666667, 122.3333333333333, 0.0, 26.0, 24.9702799260386, 0.04143409645559641, 1.0, 1.0, 55947.86924155812], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 479400.0000, 
sim time next is 480000.0000, 
raw observation next is [-1.0, 30.33333333333334, 120.6666666666667, 0.0, 26.0, 24.97876597219559, 0.2004749971783204, 1.0, 1.0, 35618.09149799782], 
processed observation next is [1.0, 0.5652173913043478, 0.4349030470914128, 0.3033333333333334, 0.4022222222222223, 0.0, 0.6666666666666666, 0.581563831016299, 0.5668249990594402, 1.0, 1.0, 0.16960995951427532], 
reward next is 0.8304, 
noisyNet noise sample is [array([-0.7417343], dtype=float32), 0.01451222]. 
=============================================
[2019-04-04 14:22:40,619] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.87817]
 [75.72654]
 [75.41975]
 [75.05335]
 [74.89419]], R is [[76.01259613]
 [75.98605347]
 [75.87844849]
 [75.68837738]
 [75.68817139]].
[2019-04-04 14:22:52,299] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6286615e-09 2.1486460e-10 5.5153902e-15 1.6585827e-13 1.0000000e+00
 4.4691353e-10 3.7141611e-15], sum to 1.0000
[2019-04-04 14:22:52,299] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0783
[2019-04-04 14:22:52,325] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.3, 75.0, 0.0, 0.0, 26.0, 24.13657741827112, 0.06734081222012471, 0.0, 1.0, 43531.62785442648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 618000.0000, 
sim time next is 618600.0000, 
raw observation next is [-4.399999999999999, 75.0, 0.0, 0.0, 26.0, 24.08503548858216, 0.05779724294182507, 0.0, 1.0, 43748.53409306304], 
processed observation next is [0.0, 0.13043478260869565, 0.3407202216066483, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5070862907151801, 0.519265747647275, 0.0, 1.0, 0.20832635282410972], 
reward next is 0.7917, 
noisyNet noise sample is [array([1.776043], dtype=float32), 0.6966899]. 
=============================================
[2019-04-04 14:22:52,386] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.6332619e-09 4.1759940e-10 1.1665485e-14 2.3045449e-14 1.0000000e+00
 4.3656388e-11 4.4772657e-14], sum to 1.0000
[2019-04-04 14:22:52,386] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7785
[2019-04-04 14:22:52,454] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.566666666666666, 65.0, 98.16666666666667, 6.333333333333332, 26.0, 24.82906869176254, 0.2069654829727473, 0.0, 1.0, 51423.24734345513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 643200.0000, 
sim time next is 643800.0000, 
raw observation next is [-3.483333333333333, 65.0, 96.33333333333333, 12.66666666666666, 26.0, 24.8733482155206, 0.2107165854792935, 0.0, 1.0, 23409.64282720065], 
processed observation next is [0.0, 0.43478260869565216, 0.3661126500461681, 0.65, 0.32111111111111107, 0.013996316758747691, 0.6666666666666666, 0.57277901796005, 0.5702388618264312, 0.0, 1.0, 0.11147448965333642], 
reward next is 0.8885, 
noisyNet noise sample is [array([1.2036173], dtype=float32), 0.17468472]. 
=============================================
[2019-04-04 14:22:54,290] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.9516628e-10 1.2174389e-10 5.4803630e-16 6.9667796e-14 1.0000000e+00
 3.3763624e-11 4.2627598e-15], sum to 1.0000
[2019-04-04 14:22:54,292] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7076
[2019-04-04 14:22:54,347] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.899999999999999, 78.66666666666667, 0.0, 0.0, 26.0, 24.21732096989514, 0.0972993389224915, 0.0, 1.0, 42541.64545377057], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 614400.0000, 
sim time next is 615000.0000, 
raw observation next is [-3.9, 76.83333333333333, 0.0, 0.0, 26.0, 24.18816278300215, 0.09002357898694911, 0.0, 1.0, 42681.16269888985], 
processed observation next is [0.0, 0.08695652173913043, 0.3545706371191136, 0.7683333333333333, 0.0, 0.0, 0.6666666666666666, 0.5156802319168458, 0.5300078596623163, 0.0, 1.0, 0.20324363189947547], 
reward next is 0.7968, 
noisyNet noise sample is [array([1.0158587], dtype=float32), 0.9692868]. 
=============================================
[2019-04-04 14:22:54,376] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.06354 ]
 [82.14102 ]
 [82.106445]
 [82.11832 ]
 [82.115105]], R is [[81.73851013]
 [81.71855164]
 [81.69931793]
 [81.6807251 ]
 [81.66270447]].
[2019-04-04 14:22:55,272] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.1624511e-09 3.6418946e-10 9.4327385e-15 2.8725706e-13 1.0000000e+00
 4.1907475e-10 2.7758434e-14], sum to 1.0000
[2019-04-04 14:22:55,276] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2025
[2019-04-04 14:22:55,336] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.283333333333334, 64.33333333333334, 92.66666666666667, 25.33333333333334, 26.0, 24.8876003768321, 0.205179293738819, 0.0, 1.0, 35922.41951014377], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 645000.0000, 
sim time next is 645600.0000, 
raw observation next is [-3.166666666666667, 63.66666666666667, 90.83333333333334, 31.66666666666667, 26.0, 24.86616452646616, 0.2045549497761443, 0.0, 1.0, 52322.55031305398], 
processed observation next is [0.0, 0.4782608695652174, 0.3748845798707295, 0.6366666666666667, 0.3027777777777778, 0.03499079189686925, 0.6666666666666666, 0.5721803772055134, 0.5681849832587148, 0.0, 1.0, 0.24915500149073325], 
reward next is 0.7508, 
noisyNet noise sample is [array([0.10578835], dtype=float32), 1.2576092]. 
=============================================
[2019-04-04 14:22:56,284] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.7604062e-09 1.6484465e-09 1.9510393e-14 1.9748218e-13 1.0000000e+00
 8.9842356e-10 1.7318283e-14], sum to 1.0000
[2019-04-04 14:22:56,285] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6052
[2019-04-04 14:22:56,326] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 57.00000000000001, 0.0, 0.0, 26.0, 24.9246958661111, 0.2141381454927738, 0.0, 1.0, 18736.21248390825], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 667200.0000, 
sim time next is 667800.0000, 
raw observation next is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.9231088149762, 0.2064432911009109, 0.0, 1.0, 28454.38030635476], 
processed observation next is [0.0, 0.7391304347826086, 0.42936288088642666, 0.57, 0.0, 0.0, 0.6666666666666666, 0.57692573458135, 0.5688144303669703, 0.0, 1.0, 0.13549704907787982], 
reward next is 0.8645, 
noisyNet noise sample is [array([-0.27646834], dtype=float32), -0.3143245]. 
=============================================
[2019-04-04 14:23:12,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.3778440e-09 2.9933034e-09 1.2913944e-14 3.1785059e-13 1.0000000e+00
 8.6998841e-10 1.0105698e-13], sum to 1.0000
[2019-04-04 14:23:12,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6437
[2019-04-04 14:23:12,650] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.55, 49.5, 35.0, 0.0, 26.0, 27.8585004786326, 1.014135547135516, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1096200.0000, 
sim time next is 1096800.0000, 
raw observation next is [18.26666666666667, 49.66666666666666, 29.33333333333334, 0.4999999999999999, 26.0, 27.90586012927346, 1.021586954240065, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.968605724838412, 0.4966666666666666, 0.0977777777777778, 0.0005524861878453037, 0.6666666666666666, 0.8254883441061217, 0.8405289847466882, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.252805], dtype=float32), 1.3344831]. 
=============================================
[2019-04-04 14:23:16,169] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5290623e-10 3.5913059e-13 6.9915919e-18 4.8820957e-16 1.0000000e+00
 3.7866351e-12 1.5183676e-17], sum to 1.0000
[2019-04-04 14:23:16,170] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5445
[2019-04-04 14:23:16,178] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.2, 76.0, 0.0, 0.0, 26.0, 26.02685737638963, 0.6200976258975024, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1041600.0000, 
sim time next is 1042200.0000, 
raw observation next is [14.1, 76.5, 0.0, 0.0, 26.0, 25.98234102483946, 0.6082702892773578, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.8531855955678671, 0.765, 0.0, 0.0, 0.6666666666666666, 0.6651950854032883, 0.7027567630924526, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4309345], dtype=float32), -0.49914217]. 
=============================================
[2019-04-04 14:23:17,723] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.6164994e-12 3.5419454e-13 1.5733864e-18 2.0359668e-17 1.0000000e+00
 7.5676238e-13 6.4220860e-19], sum to 1.0000
[2019-04-04 14:23:17,726] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1544
[2019-04-04 14:23:17,739] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [17.45, 60.5, 181.0, 317.0, 26.0, 27.00579459331561, 0.7243764466633813, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1081800.0000, 
sim time next is 1082400.0000, 
raw observation next is [17.73333333333333, 59.0, 179.3333333333333, 264.1666666666667, 26.0, 27.06157515326514, 0.860843506027371, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.9538319482917822, 0.59, 0.5977777777777776, 0.2918968692449356, 0.6666666666666666, 0.7551312627720949, 0.786947835342457, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.981961], dtype=float32), 0.79331213]. 
=============================================
[2019-04-04 14:23:26,762] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8384849e-08 2.6478070e-08 7.5343648e-14 7.1011326e-12 1.0000000e+00
 6.5433364e-10 2.3885787e-12], sum to 1.0000
[2019-04-04 14:23:26,764] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2440
[2019-04-04 14:23:26,771] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.7, 89.66666666666667, 0.0, 0.0, 26.0, 23.97651280687598, 0.2394108551602085, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1219200.0000, 
sim time next is 1219800.0000, 
raw observation next is [15.6, 91.33333333333333, 0.0, 0.0, 26.0, 23.94489308636353, 0.2355858082030159, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.8947368421052633, 0.9133333333333333, 0.0, 0.0, 0.6666666666666666, 0.4954077571969607, 0.5785286027343387, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19823451], dtype=float32), -0.79515016]. 
=============================================
[2019-04-04 14:23:36,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.8990446e-10 3.5851480e-12 8.4267486e-18 5.5327091e-16 1.0000000e+00
 1.1961839e-11 2.7820830e-17], sum to 1.0000
[2019-04-04 14:23:36,553] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9459
[2019-04-04 14:23:36,568] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.666666666666666, 71.33333333333334, 90.0, 700.6666666666666, 26.0, 25.82762122247348, 0.5912764098797948, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1516200.0000, 
sim time next is 1516800.0000, 
raw observation next is [8.133333333333333, 69.66666666666667, 87.5, 700.8333333333334, 26.0, 25.96050588516658, 0.6304923257046268, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.687903970452447, 0.6966666666666668, 0.2916666666666667, 0.774401473296501, 0.6666666666666666, 0.6633754904305483, 0.7101641085682089, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.59775746], dtype=float32), 0.8637184]. 
=============================================
[2019-04-04 14:23:36,631] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.7572769e-11 1.0650186e-11 1.1548092e-17 2.6196352e-16 1.0000000e+00
 4.1662598e-12 8.8865783e-18], sum to 1.0000
[2019-04-04 14:23:36,634] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9693
[2019-04-04 14:23:36,648] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.066666666666666, 66.33333333333334, 83.33333333333334, 694.6666666666667, 26.0, 26.3815014363219, 0.6844327144911636, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1518000.0000, 
sim time next is 1518600.0000, 
raw observation next is [9.533333333333333, 64.66666666666666, 81.66666666666666, 688.3333333333333, 26.0, 26.49890361730313, 0.7083654752217229, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.7266851338873501, 0.6466666666666666, 0.2722222222222222, 0.7605893186003683, 0.6666666666666666, 0.7082419681085943, 0.7361218250739077, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.6557312], dtype=float32), -1.0079106]. 
=============================================
[2019-04-04 14:23:38,708] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.1946245e-10 8.3766841e-11 1.5933076e-16 6.5674513e-15 1.0000000e+00
 1.4254167e-10 2.5399675e-16], sum to 1.0000
[2019-04-04 14:23:38,708] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3097
[2019-04-04 14:23:38,730] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.08333333333333, 59.66666666666666, 0.0, 0.0, 26.0, 26.43657216134831, 0.699008151533376, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1533000.0000, 
sim time next is 1533600.0000, 
raw observation next is [10.0, 60.0, 0.0, 0.0, 26.0, 26.37819819672055, 0.7009120966102659, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.739612188365651, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6981831830600459, 0.7336373655367553, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0093364], dtype=float32), 0.22033751]. 
=============================================
[2019-04-04 14:23:45,134] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1279439e-10 1.2080868e-10 4.5910986e-17 2.3525038e-15 1.0000000e+00
 3.6586231e-11 8.1098978e-17], sum to 1.0000
[2019-04-04 14:23:45,135] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4909
[2019-04-04 14:23:45,148] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.1, 77.66666666666667, 0.0, 0.0, 26.0, 25.76568888041058, 0.713670960749111, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1631400.0000, 
sim time next is 1632000.0000, 
raw observation next is [7.0, 79.33333333333334, 0.0, 0.0, 26.0, 25.94041648641229, 0.7220149342927714, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6565096952908588, 0.7933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6617013738676908, 0.7406716447642571, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.88261575], dtype=float32), -1.7354432]. 
=============================================
[2019-04-04 14:23:45,164] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[90.056755]
 [90.26411 ]
 [90.42292 ]
 [89.63704 ]
 [88.90916 ]], R is [[89.85411835]
 [89.95558167]
 [89.78410339]
 [88.96665192]
 [88.13526154]].
[2019-04-04 14:23:58,638] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9756434e-09 2.2384355e-10 1.4913213e-14 2.3977329e-14 1.0000000e+00
 3.2371467e-11 4.4452857e-15], sum to 1.0000
[2019-04-04 14:23:58,642] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4261
[2019-04-04 14:23:58,715] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 77.0, 124.6666666666667, 58.16666666666666, 26.0, 24.87151544972755, 0.2201045389012294, 0.0, 1.0, 105087.6176323189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1851600.0000, 
sim time next is 1852200.0000, 
raw observation next is [-5.6, 76.5, 120.0, 51.0, 26.0, 24.87667942572153, 0.2338474405125723, 0.0, 1.0, 75501.8149314007], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.765, 0.4, 0.056353591160221, 0.6666666666666666, 0.5730566188101275, 0.5779491468375241, 0.0, 1.0, 0.3595324520542891], 
reward next is 0.6405, 
noisyNet noise sample is [array([-1.6134374], dtype=float32), -0.55231386]. 
=============================================
[2019-04-04 14:24:03,602] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1238942e-09 3.9740594e-10 4.0577130e-15 1.1458853e-14 1.0000000e+00
 1.0102693e-09 3.6974094e-15], sum to 1.0000
[2019-04-04 14:24:03,603] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6619
[2019-04-04 14:24:03,652] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 65.0, 182.0, 2.0, 26.0, 25.57564236153672, 0.3137924469025294, 1.0, 1.0, 21269.50049420416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1947600.0000, 
sim time next is 1948200.0000, 
raw observation next is [-3.816666666666666, 64.5, 167.0, 1.333333333333333, 26.0, 25.50874423812982, 0.3024772170244631, 1.0, 1.0, 22270.88834670523], 
processed observation next is [1.0, 0.5652173913043478, 0.3568790397045245, 0.645, 0.5566666666666666, 0.00147329650092081, 0.6666666666666666, 0.6257286865108185, 0.6008257390081544, 1.0, 1.0, 0.1060518492700249], 
reward next is 0.8939, 
noisyNet noise sample is [array([0.59270823], dtype=float32), 0.21808752]. 
=============================================
[2019-04-04 14:24:04,312] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.0826373e-09 4.3880644e-10 4.7084660e-15 1.8647234e-13 1.0000000e+00
 5.4987005e-11 1.9581150e-14], sum to 1.0000
[2019-04-04 14:24:04,312] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4399
[2019-04-04 14:24:04,330] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.300000000000001, 80.0, 0.0, 0.0, 26.0, 24.33114289286431, 0.0909315577487456, 0.0, 1.0, 44918.02198733226], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1898400.0000, 
sim time next is 1899000.0000, 
raw observation next is [-7.3, 80.5, 0.0, 0.0, 26.0, 24.2908558424652, 0.08285150487210079, 0.0, 1.0, 44932.73833031488], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5242379868720999, 0.5276171682907003, 0.0, 1.0, 0.21396542062054705], 
reward next is 0.7860, 
noisyNet noise sample is [array([1.9211304], dtype=float32), 0.62274605]. 
=============================================
[2019-04-04 14:24:04,335] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[76.118675]
 [76.33921 ]
 [76.550575]
 [76.745964]
 [76.97613 ]], R is [[75.94429779]
 [75.9709549 ]
 [75.99739075]
 [76.02358246]
 [76.04950714]].
[2019-04-04 14:24:08,603] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.4304018e-09 1.8079753e-10 3.8389672e-15 4.0923021e-14 1.0000000e+00
 3.8272208e-10 4.1879562e-15], sum to 1.0000
[2019-04-04 14:24:08,605] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0333
[2019-04-04 14:24:08,651] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 78.83333333333333, 0.0, 0.0, 26.0, 25.23703589312017, 0.3783330793683652, 0.0, 1.0, 46125.39961711605], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1975800.0000, 
sim time next is 1976400.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 26.0, 25.221838046186, 0.3792861522835144, 0.0, 1.0, 44447.24493865737], 
processed observation next is [1.0, 0.9130434782608695, 0.30747922437673136, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6018198371821667, 0.6264287174278381, 0.0, 1.0, 0.21165354732693986], 
reward next is 0.7883, 
noisyNet noise sample is [array([-0.85508484], dtype=float32), 0.7930001]. 
=============================================
[2019-04-04 14:24:16,216] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.3516145e-09 1.0995294e-09 7.3412827e-14 1.7479106e-13 1.0000000e+00
 1.1330092e-09 3.8143361e-14], sum to 1.0000
[2019-04-04 14:24:16,223] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2678
[2019-04-04 14:24:16,238] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.716666666666667, 64.5, 0.0, 0.0, 26.0, 24.45850960274979, 0.1716241815177589, 0.0, 1.0, 40099.56514045438], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2346600.0000, 
sim time next is 2347200.0000, 
raw observation next is [-2.8, 65.0, 0.0, 0.0, 26.0, 24.42486269751895, 0.1717044081771102, 0.0, 1.0, 40215.70989972396], 
processed observation next is [0.0, 0.17391304347826086, 0.38504155124653744, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5354052247932458, 0.5572348027257034, 0.0, 1.0, 0.191503380474876], 
reward next is 0.8085, 
noisyNet noise sample is [array([-0.58962], dtype=float32), 0.6416598]. 
=============================================
[2019-04-04 14:24:23,948] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2294643e-08 1.1142045e-08 3.9671388e-13 1.7094384e-12 1.0000000e+00
 1.0486341e-09 6.7077794e-13], sum to 1.0000
[2019-04-04 14:24:23,949] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6892
[2019-04-04 14:24:23,971] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.1, 33.33333333333334, 0.0, 0.0, 26.0, 25.24645774795019, 0.2587084250813152, 0.0, 1.0, 40278.22151086324], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2499000.0000, 
sim time next is 2499600.0000, 
raw observation next is [-1.0, 33.66666666666667, 0.0, 0.0, 26.0, 25.21981269825888, 0.2544184401003516, 0.0, 1.0, 40306.0855778851], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.3366666666666667, 0.0, 0.0, 0.6666666666666666, 0.60165105818824, 0.5848061467001172, 0.0, 1.0, 0.19193374084707188], 
reward next is 0.8081, 
noisyNet noise sample is [array([-2.338983], dtype=float32), -0.96930295]. 
=============================================
[2019-04-04 14:24:26,064] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2601195e-09 2.7179672e-10 2.3397862e-15 4.9123269e-14 1.0000000e+00
 1.0193175e-10 1.2662710e-15], sum to 1.0000
[2019-04-04 14:24:26,064] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8590
[2019-04-04 14:24:26,105] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 68.5, 132.3333333333333, 94.99999999999999, 26.0, 26.37663961686793, 0.5143729098062053, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2213400.0000, 
sim time next is 2214000.0000, 
raw observation next is [-3.9, 68.0, 138.5, 142.5, 26.0, 26.36612639059554, 0.5135013657991426, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.68, 0.46166666666666667, 0.1574585635359116, 0.6666666666666666, 0.697177199216295, 0.6711671219330476, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38250452], dtype=float32), -1.2895038]. 
=============================================
[2019-04-04 14:24:26,110] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[83.06634 ]
 [83.07834 ]
 [83.120964]
 [83.305786]
 [83.48866 ]], R is [[83.35930634]
 [83.52571106]
 [83.69045258]
 [83.85354614]
 [84.01501465]].
[2019-04-04 14:24:31,411] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1975323e-09 4.6223425e-10 1.2613878e-14 6.2913806e-14 1.0000000e+00
 4.0709483e-10 3.0386128e-14], sum to 1.0000
[2019-04-04 14:24:31,412] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8816
[2019-04-04 14:24:31,459] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.2500000000000001, 44.0, 121.0, 60.0, 26.0, 26.29147636531823, 0.4924958042285339, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2298600.0000, 
sim time next is 2299200.0000, 
raw observation next is [0.5333333333333334, 43.66666666666666, 123.8333333333333, 57.0, 26.0, 26.2919565442068, 0.4982077105083971, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4773776546629733, 0.4366666666666666, 0.4127777777777777, 0.06298342541436464, 0.6666666666666666, 0.6909963786839001, 0.6660692368361324, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1383431], dtype=float32), -0.5167904]. 
=============================================
[2019-04-04 14:24:38,883] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6862331e-08 2.6546121e-09 3.3345608e-13 2.8976762e-12 1.0000000e+00
 4.2418153e-09 9.8349941e-13], sum to 1.0000
[2019-04-04 14:24:38,884] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4664
[2019-04-04 14:24:38,900] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.933333333333334, 51.33333333333333, 0.0, 0.0, 26.0, 24.23664155416683, 0.06711914130482592, 0.0, 1.0, 43492.93544108437], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2425200.0000, 
sim time next is 2425800.0000, 
raw observation next is [-7.116666666666666, 52.16666666666667, 0.0, 0.0, 26.0, 24.18704178230952, 0.05603941050862624, 0.0, 1.0, 43527.38275082677], 
processed observation next is [0.0, 0.043478260869565216, 0.265466297322253, 0.5216666666666667, 0.0, 0.0, 0.6666666666666666, 0.5155868151924601, 0.5186798035028753, 0.0, 1.0, 0.2072732511944132], 
reward next is 0.7927, 
noisyNet noise sample is [array([1.2924608], dtype=float32), -0.057837773]. 
=============================================
[2019-04-04 14:24:40,740] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0380865e-07 8.4939300e-09 5.3666468e-13 1.4435879e-12 9.9999976e-01
 5.3578413e-09 5.9645984e-13], sum to 1.0000
[2019-04-04 14:24:40,741] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0999
[2019-04-04 14:24:40,804] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.166666666666666, 45.33333333333333, 63.5, 683.6666666666667, 26.0, 25.26752740531755, 0.251806577468374, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2454000.0000, 
sim time next is 2454600.0000, 
raw observation next is [-5.883333333333333, 44.16666666666667, 66.0, 702.3333333333333, 26.0, 25.22071208431357, 0.246525655030038, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2996306555863343, 0.4416666666666667, 0.22, 0.7760589318600367, 0.6666666666666666, 0.6017260070261307, 0.5821752183433461, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2673551], dtype=float32), 0.5822639]. 
=============================================
[2019-04-04 14:24:45,041] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.4718792e-09 7.3986084e-10 9.7580432e-14 1.3496996e-13 1.0000000e+00
 1.0454706e-09 1.1515080e-13], sum to 1.0000
[2019-04-04 14:24:45,044] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4041
[2019-04-04 14:24:45,059] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.75, 65.0, 0.0, 0.0, 26.0, 24.68531176109004, 0.2434331457581347, 0.0, 1.0, 41906.76953959572], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2593800.0000, 
sim time next is 2594400.0000, 
raw observation next is [-4.833333333333333, 66.0, 0.0, 0.0, 26.0, 24.73916665143571, 0.2406575047036787, 0.0, 1.0, 41867.59815669498], 
processed observation next is [1.0, 0.0, 0.3287165281625116, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5615972209529758, 0.5802191682345595, 0.0, 1.0, 0.19936951503188086], 
reward next is 0.8006, 
noisyNet noise sample is [array([1.5558612], dtype=float32), 0.22904763]. 
=============================================
[2019-04-04 14:24:50,531] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8637032e-09 1.4517351e-10 1.6390765e-15 4.6406543e-14 1.0000000e+00
 1.5298295e-10 2.6103296e-15], sum to 1.0000
[2019-04-04 14:24:50,531] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5915
[2019-04-04 14:24:50,565] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.82723225959847, 0.3150186255528192, 0.0, 1.0, 43291.41711377919], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2942400.0000, 
sim time next is 2943000.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.78938074951459, 0.3111650301254395, 0.0, 1.0, 43279.13190315336], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5657817291262157, 0.6037216767084798, 0.0, 1.0, 0.20609110430073027], 
reward next is 0.7939, 
noisyNet noise sample is [array([1.0044882], dtype=float32), -0.28616506]. 
=============================================
[2019-04-04 14:24:50,575] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[82.93448]
 [82.60772]
 [82.44489]
 [82.77275]
 [83.10688]], R is [[82.93009949]
 [82.89465332]
 [82.85956573]
 [82.82479095]
 [82.79024506]].
[2019-04-04 14:24:56,063] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6877720e-09 2.7402840e-09 2.4186043e-14 4.4660778e-13 1.0000000e+00
 2.1366240e-09 7.7977365e-14], sum to 1.0000
[2019-04-04 14:24:56,064] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8104
[2019-04-04 14:24:56,082] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.33333333333333, 83.0, 0.0, 0.0, 26.0, 22.92215275007061, -0.1648746747003849, 0.0, 1.0, 43169.03260313197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2702400.0000, 
sim time next is 2703000.0000, 
raw observation next is [-15.16666666666667, 83.0, 0.0, 0.0, 26.0, 22.84563160356182, -0.1804970333884562, 0.0, 1.0, 43191.48721350184], 
processed observation next is [1.0, 0.2608695652173913, 0.04247460757156039, 0.83, 0.0, 0.0, 0.6666666666666666, 0.4038026336301517, 0.43983432220384794, 0.0, 1.0, 0.20567374863572305], 
reward next is 0.7943, 
noisyNet noise sample is [array([-1.2755958], dtype=float32), 2.1500938]. 
=============================================
[2019-04-04 14:24:56,090] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[72.99968 ]
 [73.221855]
 [73.45175 ]
 [73.70172 ]
 [73.94461 ]], R is [[72.84635162]
 [72.912323  ]
 [72.97749329]
 [73.04180145]
 [73.10526276]].
[2019-04-04 14:24:58,584] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.8129566e-09 2.1327712e-10 7.2463872e-15 7.7517365e-14 1.0000000e+00
 2.0256021e-09 1.4771890e-14], sum to 1.0000
[2019-04-04 14:24:58,585] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3232
[2019-04-04 14:24:58,610] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.833333333333333, 63.16666666666666, 0.0, 0.0, 26.0, 25.35430852950711, 0.459249668858962, 0.0, 1.0, 59921.06795191867], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2753400.0000, 
sim time next is 2754000.0000, 
raw observation next is [-6.0, 64.0, 0.0, 0.0, 26.0, 25.41106819662743, 0.4576132299946069, 0.0, 1.0, 33090.03849065255], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.64, 0.0, 0.0, 0.6666666666666666, 0.6175890163856191, 0.6525377433315357, 0.0, 1.0, 0.15757161186025023], 
reward next is 0.8424, 
noisyNet noise sample is [array([0.40117547], dtype=float32), 0.29788718]. 
=============================================
[2019-04-04 14:24:58,616] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[79.85167 ]
 [80.04719 ]
 [80.03826 ]
 [79.91933 ]
 [79.539566]], R is [[79.74245453]
 [79.65969086]
 [79.50853729]
 [79.17850494]
 [78.52716064]].
[2019-04-04 14:25:00,674] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.6516030e-09 5.0867499e-10 9.4519729e-15 4.3532262e-14 1.0000000e+00
 8.6602331e-10 8.1317145e-15], sum to 1.0000
[2019-04-04 14:25:00,675] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6794
[2019-04-04 14:25:00,733] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.05547765824042, 0.4105037993891509, 1.0, 1.0, 87055.91005170991], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2749800.0000, 
sim time next is 2750400.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.11392675087005, 0.4170734548615414, 0.0, 1.0, 37701.46491945855], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5928272292391709, 0.6390244849538471, 0.0, 1.0, 0.179530785330755], 
reward next is 0.8205, 
noisyNet noise sample is [array([0.8824994], dtype=float32), 1.2432421]. 
=============================================
[2019-04-04 14:25:01,633] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5073987e-09 1.5162893e-10 1.9792242e-15 4.5639130e-16 1.0000000e+00
 1.4706980e-10 2.3850514e-15], sum to 1.0000
[2019-04-04 14:25:01,637] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6432
[2019-04-04 14:25:01,644] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.5, 45.0, 142.6666666666667, 737.0, 26.0, 25.9645013946296, 0.4454358280046358, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2807400.0000, 
sim time next is 2808000.0000, 
raw observation next is [2.0, 44.0, 151.5, 724.0, 26.0, 25.91331734419889, 0.4422682660112945, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.44, 0.505, 0.8, 0.6666666666666666, 0.6594431120165742, 0.6474227553370981, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.92130697], dtype=float32), 0.16710052]. 
=============================================
[2019-04-04 14:25:01,645] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[84.53084 ]
 [84.6084  ]
 [84.70195 ]
 [84.82505 ]
 [84.950584]], R is [[84.65080261]
 [84.8042984 ]
 [84.95625305]
 [85.10668945]
 [85.25562286]].
[2019-04-04 14:25:05,848] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.3857211e-10 8.3228549e-11 1.1466543e-15 2.3245021e-15 1.0000000e+00
 4.1167500e-11 2.6438095e-16], sum to 1.0000
[2019-04-04 14:25:05,848] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1794
[2019-04-04 14:25:05,868] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 32.5, 249.0, 173.0, 26.0, 25.9739741741503, 0.4757889419953043, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2813400.0000, 
sim time next is 2814000.0000, 
raw observation next is [5.333333333333334, 31.66666666666667, 227.1666666666667, 144.1666666666667, 26.0, 25.99381198379177, 0.3212843284077092, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6103416435826409, 0.3166666666666667, 0.7572222222222224, 0.15930018416206268, 0.6666666666666666, 0.666150998649314, 0.607094776135903, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.494744], dtype=float32), -0.058125474]. 
=============================================
[2019-04-04 14:25:05,872] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[85.08815 ]
 [85.33397 ]
 [85.52931 ]
 [85.713715]
 [85.93704 ]], R is [[84.74932861]
 [84.90183258]
 [85.0528183 ]
 [85.2022934 ]
 [85.35027313]].
[2019-04-04 14:25:07,142] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2646401e-11 1.8910727e-11 1.1929448e-17 8.8652176e-17 1.0000000e+00
 9.7060001e-13 1.3478747e-18], sum to 1.0000
[2019-04-04 14:25:07,148] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7374
[2019-04-04 14:25:07,172] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.0, 100.0, 0.0, 0.0, 26.0, 26.44680416915366, 0.7892383868744702, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3178800.0000, 
sim time next is 3179400.0000, 
raw observation next is [3.833333333333333, 100.0, 0.0, 0.0, 26.0, 26.42581890626111, 0.7717274012889189, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5687903970452447, 1.0, 0.0, 0.0, 0.6666666666666666, 0.7021515755217592, 0.7572424670963063, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49609783], dtype=float32), 0.8067916]. 
=============================================
[2019-04-04 14:25:26,819] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.8806577e-09 1.2365312e-09 7.5985242e-15 1.2065192e-13 1.0000000e+00
 1.8574423e-10 1.5594937e-14], sum to 1.0000
[2019-04-04 14:25:26,823] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0148
[2019-04-04 14:25:26,839] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 70.0, 0.0, 0.0, 26.0, 25.28242739333457, 0.4221982988694841, 0.0, 1.0, 40966.93758590077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3550200.0000, 
sim time next is 3550800.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 26.0, 25.26733544140851, 0.4137446134859193, 0.0, 1.0, 40859.30093095285], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6056112867840424, 0.6379148711619731, 0.0, 1.0, 0.19456809967120406], 
reward next is 0.8054, 
noisyNet noise sample is [array([0.40000448], dtype=float32), -2.068877]. 
=============================================
[2019-04-04 14:25:27,450] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1202045e-09 2.9150564e-11 3.9707060e-16 1.4216607e-15 1.0000000e+00
 5.7167406e-11 9.8359504e-17], sum to 1.0000
[2019-04-04 14:25:27,455] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2578
[2019-04-04 14:25:27,471] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.56793898159808, 0.5397638437472871, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3207600.0000, 
sim time next is 3208200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.47035170242948, 0.541184807430805, 0.0, 1.0, 64893.47568422951], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6225293085357899, 0.6803949358102684, 0.0, 1.0, 0.30901655087728336], 
reward next is 0.6910, 
noisyNet noise sample is [array([-1.1849315], dtype=float32), 1.8837872]. 
=============================================
[2019-04-04 14:25:28,353] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-04 14:25:28,355] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 14:25:28,356] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 14:25:28,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:25:28,357] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:25:28,357] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 14:25:28,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run25
[2019-04-04 14:25:28,364] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:25:28,390] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run25
[2019-04-04 14:25:28,407] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run25
[2019-04-04 14:25:37,905] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17204645], dtype=float32), 0.20882624]
[2019-04-04 14:25:37,905] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.9, 74.0, 0.0, 0.0, 26.0, 23.56705823215545, -0.04264286243128258, 0.0, 1.0, 44337.5753793882]
[2019-04-04 14:25:37,905] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 14:25:37,907] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.6138124e-08 3.9071844e-09 9.2757033e-14 9.7681791e-13 1.0000000e+00
 1.7508172e-09 1.7064534e-13], sampled 0.814151310532361
[2019-04-04 14:26:24,738] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17204645], dtype=float32), 0.20882624]
[2019-04-04 14:26:24,738] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.008229913, 76.99406712666666, 73.34437846666665, 0.0, 26.0, 25.73915927609089, 0.3518602921951079, 1.0, 1.0, 0.0]
[2019-04-04 14:26:24,738] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 14:26:24,739] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.8755147e-09 3.5281422e-10 5.4556689e-15 4.5136034e-14 1.0000000e+00
 2.2195422e-10 7.6759320e-15], sampled 0.16908127075616652
[2019-04-04 14:26:29,820] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17204645], dtype=float32), 0.20882624]
[2019-04-04 14:26:29,820] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.65, 65.83333333333334, 43.00000000000001, 207.6666666666667, 26.0, 25.43666271378608, 0.2866046385272911, 1.0, 1.0, 0.0]
[2019-04-04 14:26:29,820] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 14:26:29,821] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.0501518e-09 2.7609851e-10 1.1265928e-15 1.5248943e-14 1.0000000e+00
 4.4986900e-11 2.5469351e-15], sampled 0.9381701192438417
[2019-04-04 14:27:08,767] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17204645], dtype=float32), 0.20882624]
[2019-04-04 14:27:08,768] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [12.45, 68.5, 119.0, 224.0, 26.0, 27.69619188107155, 1.044593325733075, 1.0, 1.0, 0.0]
[2019-04-04 14:27:08,768] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 14:27:08,769] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.4878826e-11 8.4217130e-12 1.7436484e-17 1.5263186e-16 1.0000000e+00
 3.5475388e-12 2.8899527e-17], sampled 0.9243222206472916
[2019-04-04 14:27:08,995] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 14:27:29,480] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6673 263409857.2690 1551.6144
[2019-04-04 14:27:31,887] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 14:27:32,910] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2400000, evaluation results [2400000.0, 7241.667346338161, 263409857.26898408, 1551.6144136320988, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 14:27:34,285] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8607215e-09 5.4894816e-10 7.9933161e-15 4.5885493e-14 1.0000000e+00
 2.8381772e-10 1.0202023e-14], sum to 1.0000
[2019-04-04 14:27:34,286] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3830
[2019-04-04 14:27:34,309] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.333333333333333, 47.33333333333333, 82.5, 645.1666666666667, 26.0, 26.05409280471896, 0.6324314941277125, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3339600.0000, 
sim time next is 3340200.0000, 
raw observation next is [-2.166666666666667, 46.66666666666667, 78.0, 616.3333333333334, 26.0, 25.49904625516827, 0.5996700960516473, 1.0, 1.0, 62550.48407309456], 
processed observation next is [1.0, 0.6521739130434783, 0.4025854108956602, 0.46666666666666673, 0.26, 0.6810313075506446, 0.6666666666666666, 0.6249205212640225, 0.6998900320172158, 1.0, 1.0, 0.29785944796711694], 
reward next is 0.7021, 
noisyNet noise sample is [array([-1.3563874], dtype=float32), 1.3267479]. 
=============================================
[2019-04-04 14:27:36,918] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6817897e-09 1.2814207e-10 9.1811373e-16 7.3583116e-15 1.0000000e+00
 1.2034022e-10 3.4667478e-16], sum to 1.0000
[2019-04-04 14:27:36,919] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2337
[2019-04-04 14:27:36,927] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 54.0, 117.0, 804.5, 26.0, 26.35448423151227, 0.6045648337521516, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3326400.0000, 
sim time next is 3327000.0000, 
raw observation next is [-5.833333333333333, 54.0, 117.3333333333333, 806.6666666666667, 26.0, 26.35550501629152, 0.607940064866371, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.30101569713758086, 0.54, 0.391111111111111, 0.8913443830570903, 0.6666666666666666, 0.69629208469096, 0.7026466882887904, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36813834], dtype=float32), -0.23703879]. 
=============================================
[2019-04-04 14:27:36,948] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.963326]
 [83.12327 ]
 [83.287254]
 [83.44143 ]
 [83.60263 ]], R is [[82.96370697]
 [83.13407135]
 [83.30273438]
 [83.4697113 ]
 [83.6350174 ]].
[2019-04-04 14:27:40,842] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.3972657e-09 1.7267960e-09 5.6476057e-15 2.6604344e-14 1.0000000e+00
 7.3958957e-11 8.2929210e-15], sum to 1.0000
[2019-04-04 14:27:40,844] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8057
[2019-04-04 14:27:40,858] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.88698515116743, 0.2995273415626319, 0.0, 1.0, 41173.55440655138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3376800.0000, 
sim time next is 3377400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.82983299856915, 0.2939363395130125, 0.0, 1.0, 41179.91457802296], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5691527498807624, 0.5979787798376709, 0.0, 1.0, 0.19609483132391886], 
reward next is 0.8039, 
noisyNet noise sample is [array([0.6560156], dtype=float32), -2.0678089]. 
=============================================
[2019-04-04 14:27:47,011] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7994280e-08 1.1468394e-09 1.3180359e-14 2.9652469e-14 1.0000000e+00
 5.6688482e-10 3.4153993e-15], sum to 1.0000
[2019-04-04 14:27:47,016] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2536
[2019-04-04 14:27:47,028] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.833333333333333, 49.5, 29.33333333333333, 275.6666666666666, 26.0, 26.36490452054876, 0.6631397716465705, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3517800.0000, 
sim time next is 3518400.0000, 
raw observation next is [2.666666666666667, 50.0, 21.16666666666666, 213.3333333333333, 26.0, 26.57709801526504, 0.4912168576601348, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5364727608494922, 0.5, 0.07055555555555554, 0.2357274401473296, 0.6666666666666666, 0.7147581679387534, 0.6637389525533782, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1916134], dtype=float32), 0.14727928]. 
=============================================
[2019-04-04 14:27:53,333] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0057343e-08 2.0852589e-09 6.7069280e-15 1.7821982e-13 1.0000000e+00
 2.0199759e-10 3.5401674e-14], sum to 1.0000
[2019-04-04 14:27:53,333] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4979
[2019-04-04 14:27:53,345] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.5, 28.0, 0.0, 0.0, 26.0, 25.44934092498961, 0.3583153117916711, 0.0, 1.0, 40135.20461730696], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3645000.0000, 
sim time next is 3645600.0000, 
raw observation next is [8.666666666666668, 27.66666666666667, 0.0, 0.0, 26.0, 25.46172754025783, 0.3599238954009019, 0.0, 1.0, 28116.51384934841], 
processed observation next is [0.0, 0.17391304347826086, 0.7026777469990768, 0.2766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6218106283548192, 0.6199746318003007, 0.0, 1.0, 0.13388816118737337], 
reward next is 0.8661, 
noisyNet noise sample is [array([-0.48666215], dtype=float32), -0.34974647]. 
=============================================
[2019-04-04 14:27:57,882] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.8773079e-10 4.9453913e-10 3.2286535e-16 5.3484532e-15 1.0000000e+00
 6.0847535e-11 3.6673469e-16], sum to 1.0000
[2019-04-04 14:27:57,884] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9555
[2019-04-04 14:27:57,907] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.39059652857783, 0.4139640429357392, 0.0, 1.0, 34009.93648668156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3808200.0000, 
sim time next is 3808800.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.44882924358662, 0.4055350732857777, 0.0, 1.0, 18761.75813817242], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6207357702988849, 0.6351783577619259, 0.0, 1.0, 0.08934170541986866], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.25758144], dtype=float32), -1.2088913]. 
=============================================
[2019-04-04 14:28:01,220] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.4723644e-09 1.0311991e-10 3.9013863e-15 8.2350102e-15 1.0000000e+00
 6.3992378e-10 1.3252505e-14], sum to 1.0000
[2019-04-04 14:28:01,221] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5628
[2019-04-04 14:28:01,266] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 38.0, 110.5, 806.0, 26.0, 26.54990034139361, 0.7261187948117197, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3938400.0000, 
sim time next is 3939000.0000, 
raw observation next is [-4.833333333333334, 38.0, 108.6666666666667, 800.0, 26.0, 26.74868216053559, 0.749968257844214, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.32871652816251157, 0.38, 0.36222222222222233, 0.8839779005524862, 0.6666666666666666, 0.7290568467112992, 0.7499894192814046, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12920675], dtype=float32), 1.8727928]. 
=============================================
[2019-04-04 14:28:01,282] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[82.74674]
 [82.87107]
 [83.01446]
 [83.04767]
 [82.12226]], R is [[82.84709167]
 [83.01862335]
 [83.18843842]
 [83.35655212]
 [82.5827179 ]].
[2019-04-04 14:28:03,456] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7291757e-09 8.0027596e-11 9.9669435e-16 5.8106320e-15 1.0000000e+00
 1.5670517e-11 1.1050006e-15], sum to 1.0000
[2019-04-04 14:28:03,461] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2418
[2019-04-04 14:28:03,474] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.933333333333334, 57.33333333333333, 108.3333333333333, 638.5, 26.0, 25.4784161573279, 0.4667672244351762, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4290000.0000, 
sim time next is 4290600.0000, 
raw observation next is [6.966666666666667, 56.66666666666667, 100.6666666666667, 622.0, 26.0, 25.49058147326769, 0.4713064937781775, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6555863342566944, 0.5666666666666668, 0.33555555555555566, 0.687292817679558, 0.6666666666666666, 0.6242151227723074, 0.6571021645927259, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51949924], dtype=float32), 1.0709298]. 
=============================================
[2019-04-04 14:28:22,007] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3146270e-09 3.3263206e-10 5.2000996e-15 1.7953566e-14 1.0000000e+00
 1.4577507e-10 1.9633423e-15], sum to 1.0000
[2019-04-04 14:28:22,008] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0409
[2019-04-04 14:28:22,020] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.366666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 25.47445816724103, 0.4242013266788408, 0.0, 1.0, 18758.56838164789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4592400.0000, 
sim time next is 4593000.0000, 
raw observation next is [-1.433333333333333, 68.66666666666666, 0.0, 0.0, 26.0, 25.46418989963333, 0.4143496945764844, 0.0, 1.0, 20622.73150169575], 
processed observation next is [1.0, 0.13043478260869565, 0.4228993536472761, 0.6866666666666665, 0.0, 0.0, 0.6666666666666666, 0.6220158249694441, 0.6381165648588282, 0.0, 1.0, 0.09820348334140834], 
reward next is 0.9018, 
noisyNet noise sample is [array([-0.4445657], dtype=float32), 2.035789]. 
=============================================
[2019-04-04 14:28:22,028] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[84.79921 ]
 [84.911674]
 [85.103226]
 [85.13561 ]
 [85.02503 ]], R is [[84.75093079]
 [84.81409454]
 [84.96595764]
 [84.98698425]
 [84.90249634]].
[2019-04-04 14:28:41,988] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9641724e-09 5.4587820e-11 5.3957710e-15 1.8492232e-14 1.0000000e+00
 1.2111992e-10 6.5051669e-15], sum to 1.0000
[2019-04-04 14:28:41,988] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6734
[2019-04-04 14:28:42,009] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.5, 37.0, 0.0, 0.0, 26.0, 25.57483512236173, 0.482185998871794, 0.0, 1.0, 18740.90683787737], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5009400.0000, 
sim time next is 5010000.0000, 
raw observation next is [2.333333333333333, 38.0, 0.0, 0.0, 26.0, 25.56403446798198, 0.4742234200105113, 0.0, 1.0, 23753.86255469996], 
processed observation next is [1.0, 1.0, 0.5272391505078486, 0.38, 0.0, 0.0, 0.6666666666666666, 0.630336205665165, 0.6580744733368371, 0.0, 1.0, 0.11311363121285696], 
reward next is 0.8869, 
noisyNet noise sample is [array([2.2826521], dtype=float32), 0.248061]. 
=============================================
[2019-04-04 14:28:42,025] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.445496]
 [80.710945]
 [81.01935 ]
 [81.17452 ]
 [81.027306]], R is [[80.34447479]
 [80.45178986]
 [80.6472702 ]
 [80.75151825]
 [80.70350647]].
[2019-04-04 14:28:42,786] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:28:42,786] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:28:42,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run19
[2019-04-04 14:28:46,173] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.2716412e-09 4.1601678e-09 6.4643779e-14 8.6484463e-14 1.0000000e+00
 1.5739129e-09 9.7401179e-14], sum to 1.0000
[2019-04-04 14:28:46,174] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7290
[2019-04-04 14:28:46,209] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.5, 41.5, 0.0, 0.0, 26.0, 25.0140403366012, 0.3459280844658901, 0.0, 1.0, 18829.33830445702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4818600.0000, 
sim time next is 4819200.0000, 
raw observation next is [1.333333333333333, 42.0, 0.0, 0.0, 26.0, 25.02059882337139, 0.3395432466773383, 0.0, 1.0, 19464.95223028817], 
processed observation next is [0.0, 0.782608695652174, 0.4995383194829178, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5850499019476159, 0.6131810822257794, 0.0, 1.0, 0.09269024871565795], 
reward next is 0.9073, 
noisyNet noise sample is [array([0.52267873], dtype=float32), 0.46116248]. 
=============================================
[2019-04-04 14:28:47,008] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:28:47,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:28:47,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run19
[2019-04-04 14:28:48,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:28:48,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:28:48,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run19
[2019-04-04 14:28:48,495] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2872523e-10 1.2304230e-10 8.2966143e-17 2.1965767e-14 1.0000000e+00
 2.3385684e-11 1.5967706e-15], sum to 1.0000
[2019-04-04 14:28:48,497] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7265
[2019-04-04 14:28:48,547] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 67.0, 167.0, 524.0, 26.0, 25.61342056665548, 0.4665691048603778, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4787400.0000, 
sim time next is 4788000.0000, 
raw observation next is [-3.0, 65.0, 163.5, 575.5, 26.0, 25.5772522600556, 0.4615874991846047, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3795013850415513, 0.65, 0.545, 0.6359116022099448, 0.6666666666666666, 0.6314376883379668, 0.6538624997282015, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54894733], dtype=float32), -0.53158945]. 
=============================================
[2019-04-04 14:28:48,555] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[87.37527]
 [87.68487]
 [87.89851]
 [88.10281]
 [88.15453]], R is [[87.21759033]
 [87.34541321]
 [87.47196198]
 [87.59724426]
 [87.72127533]].
[2019-04-04 14:28:53,941] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.4373789e-09 1.0103367e-09 1.7709001e-14 1.7714373e-14 1.0000000e+00
 1.2712158e-10 1.5320952e-14], sum to 1.0000
[2019-04-04 14:28:53,941] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1368
[2019-04-04 14:28:53,951] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.833333333333333, 44.83333333333333, 211.6666666666667, 390.0, 26.0, 25.07000751412204, 0.3688169092386249, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4891800.0000, 
sim time next is 4892400.0000, 
raw observation next is [3.0, 45.0, 199.5, 398.0, 26.0, 25.07362425504066, 0.3696922100479521, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.665, 0.4397790055248619, 0.6666666666666666, 0.5894686879200549, 0.6232307366826507, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35440147], dtype=float32), 1.4589618]. 
=============================================
[2019-04-04 14:28:54,003] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5144417e-09 6.2307992e-10 3.7253568e-15 2.4716107e-13 1.0000000e+00
 5.8062372e-10 6.0237622e-14], sum to 1.0000
[2019-04-04 14:28:54,003] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0393
[2019-04-04 14:28:54,013] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 45.0, 163.0, 422.0, 26.0, 25.09876997127406, 0.3748292913463989, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4894200.0000, 
sim time next is 4894800.0000, 
raw observation next is [3.0, 45.0, 152.8333333333333, 404.5, 26.0, 25.10926890338354, 0.3734002365907008, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.5094444444444443, 0.44696132596685084, 0.6666666666666666, 0.5924390752819617, 0.6244667455302336, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8163561], dtype=float32), 0.12575778]. 
=============================================
[2019-04-04 14:28:54,184] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.9968890e-09 4.7645254e-10 1.0277725e-14 9.8742168e-14 1.0000000e+00
 4.7874632e-10 1.4016964e-14], sum to 1.0000
[2019-04-04 14:28:54,185] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8008
[2019-04-04 14:28:54,198] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 25.80009581152848, 0.5672165665247344, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5004000.0000, 
sim time next is 5004600.0000, 
raw observation next is [3.0, 36.5, 0.0, 0.0, 26.0, 25.79380883789021, 0.5085169282003934, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.365, 0.0, 0.0, 0.6666666666666666, 0.6494840698241843, 0.6695056427334644, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.71449775], dtype=float32), 1.0378206]. 
=============================================
[2019-04-04 14:28:54,610] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1711745e-08 1.4835233e-09 5.3692253e-14 1.9980775e-13 1.0000000e+00
 2.7562825e-09 2.6645376e-14], sum to 1.0000
[2019-04-04 14:28:54,611] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1203
[2019-04-04 14:28:54,628] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 26.0, 26.04825061175525, 0.5869701194684995, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4996800.0000, 
sim time next is 4997400.0000, 
raw observation next is [5.666666666666667, 24.0, 0.0, 0.0, 26.0, 25.98623294766524, 0.5709703732078398, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6195752539242845, 0.24, 0.0, 0.0, 0.6666666666666666, 0.6655194123054367, 0.6903234577359466, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2154305], dtype=float32), -0.122388124]. 
=============================================
[2019-04-04 14:28:55,628] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0993370e-08 8.2844048e-10 5.1436160e-15 7.3405129e-14 1.0000000e+00
 2.6896960e-10 3.0411625e-15], sum to 1.0000
[2019-04-04 14:28:55,628] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0998
[2019-04-04 14:28:55,641] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.452674224518, 0.3951338045374639, 0.0, 1.0, 40047.29715442541], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5025000.0000, 
sim time next is 5025600.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.39251251975259, 0.3900993081308528, 0.0, 1.0, 68066.99162862741], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6160427099793825, 0.6300331027102842, 0.0, 1.0, 0.3241285315648924], 
reward next is 0.6759, 
noisyNet noise sample is [array([0.34061152], dtype=float32), -0.25696298]. 
=============================================
[2019-04-04 14:28:55,802] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:28:55,802] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:28:55,806] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run19
[2019-04-04 14:28:56,861] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5829887e-09 8.5192547e-10 5.6339418e-15 1.8001056e-14 1.0000000e+00
 3.0461281e-10 1.1092456e-14], sum to 1.0000
[2019-04-04 14:28:56,862] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5095
[2019-04-04 14:28:56,877] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.333333333333333, 58.66666666666666, 0.0, 0.0, 26.0, 25.24584210148786, 0.3471523943494745, 0.0, 1.0, 38023.62823913692], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5035200.0000, 
sim time next is 5035800.0000, 
raw observation next is [-2.666666666666667, 61.83333333333334, 0.0, 0.0, 26.0, 25.26128147837948, 0.352157614171095, 0.0, 1.0, 37406.38032557078], 
processed observation next is [1.0, 0.2608695652173913, 0.38873499538319484, 0.6183333333333334, 0.0, 0.0, 0.6666666666666666, 0.6051067898649567, 0.6173858713903649, 0.0, 1.0, 0.1781256205979561], 
reward next is 0.8219, 
noisyNet noise sample is [array([0.6158624], dtype=float32), 0.30696395]. 
=============================================
[2019-04-04 14:28:59,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:28:59,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:28:59,255] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run19
[2019-04-04 14:28:59,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:28:59,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:28:59,982] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run19
[2019-04-04 14:29:00,611] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:29:00,611] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:29:00,634] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run19
[2019-04-04 14:29:02,511] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:29:02,511] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:29:02,538] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run19
[2019-04-04 14:29:03,495] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.8436032e-09 4.2992535e-10 4.1455825e-14 4.2514755e-14 1.0000000e+00
 3.1308445e-10 1.6657149e-14], sum to 1.0000
[2019-04-04 14:29:03,496] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3511
[2019-04-04 14:29:03,510] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.30362909883017, 0.3454390863878603, 0.0, 1.0, 37139.7881715102], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5036400.0000, 
sim time next is 5037000.0000, 
raw observation next is [-2.833333333333333, 65.0, 39.33333333333334, 67.33333333333334, 26.0, 25.24773799979513, 0.3381752959093828, 0.0, 1.0, 37077.03274984133], 
processed observation next is [1.0, 0.30434782608695654, 0.3841181902123731, 0.65, 0.13111111111111115, 0.07440147329650093, 0.6666666666666666, 0.6039781666495943, 0.612725098636461, 0.0, 1.0, 0.17655729880876825], 
reward next is 0.8234, 
noisyNet noise sample is [array([-0.68903005], dtype=float32), -1.69563]. 
=============================================
[2019-04-04 14:29:03,516] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[81.48955]
 [81.43529]
 [81.41945]
 [81.45815]
 [81.49716]], R is [[81.88011169]
 [81.88445282]
 [81.88748932]
 [81.88755035]
 [81.87828827]].
[2019-04-04 14:29:03,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:29:03,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:29:03,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run19
[2019-04-04 14:29:04,063] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:29:04,063] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:29:04,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:29:04,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:29:04,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run19
[2019-04-04 14:29:04,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run19
[2019-04-04 14:29:04,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:29:04,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:29:04,397] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run19
[2019-04-04 14:29:04,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:29:04,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:29:04,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run19
[2019-04-04 14:29:05,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:29:05,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:29:05,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run19
[2019-04-04 14:29:06,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:29:06,226] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:29:06,274] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run19
[2019-04-04 14:29:07,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:29:07,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:29:07,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run19
[2019-04-04 14:29:16,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4215172e-07 1.5075644e-08 1.0175991e-12 9.1377591e-12 9.9999988e-01
 1.6267890e-08 3.3469795e-13], sum to 1.0000
[2019-04-04 14:29:16,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3941
[2019-04-04 14:29:16,589] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-14.66666666666667, 69.0, 0.0, 0.0, 26.0, 23.2560735640782, -0.1129821322728509, 0.0, 1.0, 47800.31586773927], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 350400.0000, 
sim time next is 351000.0000, 
raw observation next is [-14.75, 69.0, 0.0, 0.0, 26.0, 23.29844883517472, -0.1211822660966086, 0.0, 1.0, 47895.36230090888], 
processed observation next is [1.0, 0.043478260869565216, 0.05401662049861495, 0.69, 0.0, 0.0, 0.6666666666666666, 0.44153740293122673, 0.45960591130113043, 0.0, 1.0, 0.2280731538138518], 
reward next is 0.7719, 
noisyNet noise sample is [array([1.4153305], dtype=float32), 0.20155537]. 
=============================================
[2019-04-04 14:29:16,638] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[72.56942]
 [72.44298]
 [72.57751]
 [72.78266]
 [72.94807]], R is [[72.47203064]
 [72.51969147]
 [72.5672226 ]
 [72.61452484]
 [72.66156769]].
[2019-04-04 14:29:24,745] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8347419e-09 1.4459982e-10 1.2668943e-15 3.3621997e-14 1.0000000e+00
 5.0426030e-10 8.7265146e-16], sum to 1.0000
[2019-04-04 14:29:24,745] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8967
[2019-04-04 14:29:24,810] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.65, 78.0, 34.0, 296.0, 26.0, 25.09556755657278, 0.2229423058364831, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 203400.0000, 
sim time next is 204000.0000, 
raw observation next is [-8.566666666666666, 78.0, 41.5, 246.6666666666667, 26.0, 25.26528289199543, 0.2390955732121777, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.22530009233610343, 0.78, 0.13833333333333334, 0.27255985267034993, 0.6666666666666666, 0.6054402409996191, 0.5796985244040592, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3193748], dtype=float32), 0.2986293]. 
=============================================
[2019-04-04 14:29:24,817] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[84.02907 ]
 [83.400154]
 [82.76626 ]
 [81.65328 ]
 [80.27084 ]], R is [[84.44483948]
 [84.6003952 ]
 [84.70960236]
 [84.61557007]
 [84.34001923]].
[2019-04-04 14:29:39,168] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6089011e-08 3.0326683e-09 4.3113652e-14 2.5490956e-13 1.0000000e+00
 4.1136929e-09 8.8097410e-14], sum to 1.0000
[2019-04-04 14:29:39,168] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4047
[2019-04-04 14:29:39,195] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.1, 79.5, 0.0, 0.0, 26.0, 24.43393104398245, 0.152940336886477, 0.0, 1.0, 47549.92973720183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 336600.0000, 
sim time next is 337200.0000, 
raw observation next is [-13.2, 80.33333333333333, 0.0, 0.0, 26.0, 24.34949022395472, 0.1391501973980745, 0.0, 1.0, 47560.2114405668], 
processed observation next is [1.0, 0.9130434782608695, 0.09695290858725764, 0.8033333333333332, 0.0, 0.0, 0.6666666666666666, 0.5291241853295601, 0.5463833991326915, 0.0, 1.0, 0.2264771973360324], 
reward next is 0.7735, 
noisyNet noise sample is [array([0.38302514], dtype=float32), -0.35591924]. 
=============================================
[2019-04-04 14:29:48,409] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.3961459e-08 7.3169351e-09 1.1255592e-12 6.6853784e-12 9.9999988e-01
 2.7365181e-09 3.3692179e-13], sum to 1.0000
[2019-04-04 14:29:48,410] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7253
[2019-04-04 14:29:48,519] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.899999999999999, 40.5, 7.666666666666665, 0.0, 26.0, 23.99641419407673, 0.01451484752823044, 1.0, 1.0, 110191.8398357837], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 460200.0000, 
sim time next is 460800.0000, 
raw observation next is [-7.8, 40.0, 11.5, 0.0, 26.0, 24.43730608283598, 0.0611748285841375, 1.0, 1.0, 90981.36825798411], 
processed observation next is [1.0, 0.34782608695652173, 0.24653739612188366, 0.4, 0.03833333333333333, 0.0, 0.6666666666666666, 0.5364421735696651, 0.5203916095280459, 1.0, 1.0, 0.4332446107523053], 
reward next is 0.5668, 
noisyNet noise sample is [array([0.44806278], dtype=float32), 1.4642845]. 
=============================================
[2019-04-04 14:29:53,002] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4807585e-09 9.3856242e-12 3.2279886e-16 1.5700853e-15 1.0000000e+00
 8.2234275e-11 1.1768275e-16], sum to 1.0000
[2019-04-04 14:29:53,003] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8307
[2019-04-04 14:29:53,039] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.8, 92.66666666666667, 0.0, 0.0, 26.0, 24.85435368628566, 0.2336214126474328, 0.0, 1.0, 41010.99555803621], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 511800.0000, 
sim time next is 512400.0000, 
raw observation next is [2.9, 93.33333333333334, 0.0, 0.0, 26.0, 24.84927227119754, 0.2332516777018459, 0.0, 1.0, 40907.76234581547], 
processed observation next is [1.0, 0.9565217391304348, 0.5429362880886427, 0.9333333333333335, 0.0, 0.0, 0.6666666666666666, 0.5707726892664615, 0.5777505592339486, 0.0, 1.0, 0.194798868313407], 
reward next is 0.8052, 
noisyNet noise sample is [array([1.1271048], dtype=float32), -0.5732773]. 
=============================================
[2019-04-04 14:30:03,248] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6139285e-09 7.1475992e-10 2.4674349e-15 1.0610070e-13 1.0000000e+00
 2.4332772e-10 1.0122051e-14], sum to 1.0000
[2019-04-04 14:30:03,249] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5508
[2019-04-04 14:30:03,264] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.17126677928129, 0.005643474066822821, 0.0, 1.0, 42035.72282524779], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 716400.0000, 
sim time next is 717000.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.10660377076431, 0.002760061390367695, 0.0, 1.0, 42077.12796156071], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5088836475636924, 0.500920020463456, 0.0, 1.0, 0.20036727600743195], 
reward next is 0.7996, 
noisyNet noise sample is [array([0.540117], dtype=float32), -0.38427728]. 
=============================================
[2019-04-04 14:30:03,273] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.31197 ]
 [80.296   ]
 [80.302246]
 [80.28725 ]
 [80.26789 ]], R is [[80.30383301]
 [80.30062103]
 [80.29769897]
 [80.2949295 ]
 [80.29229736]].
[2019-04-04 14:30:04,470] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.41736520e-09 6.28533614e-10 1.26822220e-14 8.99520801e-14
 1.00000000e+00 1.07303846e-10 1.02231778e-14], sum to 1.0000
[2019-04-04 14:30:04,470] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7089
[2019-04-04 14:30:04,493] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 65.0, 0.0, 0.0, 26.0, 24.97453556769531, 0.2208420792101796, 0.0, 1.0, 44502.79225588075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 676800.0000, 
sim time next is 677400.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.97332930998702, 0.2275055753039838, 0.0, 1.0, 43633.0548042343], 
processed observation next is [0.0, 0.8695652173913043, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5811107758322516, 0.5758351917679946, 0.0, 1.0, 0.20777645144873477], 
reward next is 0.7922, 
noisyNet noise sample is [array([0.24690905], dtype=float32), -0.76868933]. 
=============================================
[2019-04-04 14:30:08,295] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4358645e-10 3.7355109e-11 1.9550305e-16 6.1260697e-15 1.0000000e+00
 3.5436317e-11 1.5513308e-16], sum to 1.0000
[2019-04-04 14:30:08,299] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2171
[2019-04-04 14:30:08,307] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 61.5, 84.0, 779.0, 26.0, 25.83383430522895, 0.3893265915730029, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 732600.0000, 
sim time next is 733200.0000, 
raw observation next is [-0.6, 60.0, 91.83333333333333, 724.0, 26.0, 25.85681979469038, 0.3917090782180775, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.6, 0.3061111111111111, 0.8, 0.6666666666666666, 0.6547349828908651, 0.6305696927393591, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.56976944], dtype=float32), -0.23916432]. 
=============================================
[2019-04-04 14:30:08,987] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3392456e-08 5.5756866e-09 4.8952457e-15 9.6887566e-14 1.0000000e+00
 1.7933796e-09 2.3812163e-14], sum to 1.0000
[2019-04-04 14:30:08,989] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9232
[2019-04-04 14:30:09,021] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.066666666666666, 51.0, 56.66666666666667, 2.833333333333333, 26.0, 25.67409862481582, 0.4271758652234433, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 751200.0000, 
sim time next is 751800.0000, 
raw observation next is [-2.433333333333334, 52.5, 45.33333333333334, 2.666666666666667, 26.0, 25.87441137400378, 0.4394515071458054, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3951985226223454, 0.525, 0.15111111111111114, 0.002946593001841621, 0.6666666666666666, 0.6562009478336485, 0.6464838357152685, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7527112], dtype=float32), -0.51981115]. 
=============================================
[2019-04-04 14:30:11,609] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.892167e-09 1.131566e-10 8.468553e-16 8.453936e-15 1.000000e+00
 6.882749e-11 8.956251e-16], sum to 1.0000
[2019-04-04 14:30:11,609] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8332
[2019-04-04 14:30:11,617] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.166666666666667, 92.66666666666666, 98.16666666666667, 0.0, 26.0, 25.35283886433297, 0.2767645800862847, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 906000.0000, 
sim time next is 906600.0000, 
raw observation next is [2.433333333333334, 94.83333333333334, 99.33333333333333, 0.0, 26.0, 25.32479868658363, 0.2756041551652125, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5300092336103417, 0.9483333333333335, 0.3311111111111111, 0.0, 0.6666666666666666, 0.6103998905486359, 0.5918680517217375, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7604043], dtype=float32), 1.0081569]. 
=============================================
[2019-04-04 14:30:15,843] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.6059926e-09 1.3560578e-09 1.3352701e-14 1.0528010e-12 1.0000000e+00
 3.1496875e-10 1.5239646e-13], sum to 1.0000
[2019-04-04 14:30:15,851] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3069
[2019-04-04 14:30:15,855] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.6068720559015, 0.1686602447566202, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1230600.0000, 
sim time next is 1231200.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.58367245195529, 0.16416340521854, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.46530603766294093, 0.5547211350728466, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.87223953], dtype=float32), -0.6836618]. 
=============================================
[2019-04-04 14:30:21,879] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.8574662e-09 3.0377614e-10 9.5072227e-16 1.3200787e-14 1.0000000e+00
 1.3135105e-11 8.8847615e-15], sum to 1.0000
[2019-04-04 14:30:21,879] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1410
[2019-04-04 14:30:21,883] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 65.0, 145.0, 0.0, 26.0, 25.23942585825207, 0.514923305453996, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1162800.0000, 
sim time next is 1163400.0000, 
raw observation next is [18.38333333333334, 64.66666666666667, 150.0, 0.0, 26.0, 25.20668647485015, 0.5111148184841271, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.9718374884579875, 0.6466666666666667, 0.5, 0.0, 0.6666666666666666, 0.6005572062375126, 0.6703716061613757, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3816668], dtype=float32), 0.35529867]. 
=============================================
[2019-04-04 14:30:27,037] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.2686197e-11 9.8798981e-12 2.8629147e-18 1.1610999e-16 1.0000000e+00
 6.5755301e-13 2.1202537e-18], sum to 1.0000
[2019-04-04 14:30:27,039] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0066
[2019-04-04 14:30:27,053] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 26.0, 25.82268626879889, 0.5983144068625639, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1058400.0000, 
sim time next is 1059000.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 26.0, 25.77317704660407, 0.589839824591971, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.8310249307479226, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6477647538836724, 0.6966132748639904, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42706048], dtype=float32), 0.03699158]. 
=============================================
[2019-04-04 14:30:27,067] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[95.10968 ]
 [95.204834]
 [95.29376 ]
 [95.39673 ]
 [95.44947 ]], R is [[95.05492401]
 [95.10437775]
 [95.15333557]
 [95.20180511]
 [95.24978638]].
[2019-04-04 14:30:32,126] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.6806552e-11 3.4823420e-12 1.3073428e-17 6.3874755e-16 1.0000000e+00
 1.4083627e-12 5.0306921e-17], sum to 1.0000
[2019-04-04 14:30:32,127] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2356
[2019-04-04 14:30:32,145] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.20020881970086, 0.4388543285841586, 0.0, 1.0, 38601.127304995], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1401000.0000, 
sim time next is 1401600.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.19241018278611, 0.4465236264244321, 0.0, 1.0, 38548.75284554109], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5993675152321757, 0.648841208808144, 0.0, 1.0, 0.18356548974067183], 
reward next is 0.8164, 
noisyNet noise sample is [array([0.8017367], dtype=float32), -1.8896745]. 
=============================================
[2019-04-04 14:30:34,021] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3056504e-10 5.4763645e-11 1.9327992e-16 3.5368904e-16 1.0000000e+00
 4.3047163e-11 1.6025180e-16], sum to 1.0000
[2019-04-04 14:30:34,026] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5583
[2019-04-04 14:30:34,074] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.96706534765517, 0.4576913461369636, 1.0, 1.0, 67410.58191113203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1367400.0000, 
sim time next is 1368000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.90458596037231, 0.4627027194062487, 1.0, 1.0, 84083.71554617138], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5753821633643591, 0.654234239802083, 1.0, 1.0, 0.400398645457959], 
reward next is 0.5996, 
noisyNet noise sample is [array([-0.3581619], dtype=float32), 0.4516107]. 
=============================================
[2019-04-04 14:30:34,088] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[90.01396]
 [89.5614 ]
 [90.28088]
 [91.41974]
 [92.60336]], R is [[90.37126923]
 [90.14656067]
 [90.15602875]
 [90.25447083]
 [90.35192871]].
[2019-04-04 14:30:34,188] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0057991e-09 1.4191170e-11 8.2984506e-17 4.6850107e-16 1.0000000e+00
 6.0629826e-12 1.7496526e-16], sum to 1.0000
[2019-04-04 14:30:34,189] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0580
[2019-04-04 14:30:34,243] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.91242491321606, 0.4922455442579014, 0.0, 1.0, 193928.8143927067], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1369200.0000, 
sim time next is 1369800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.95878138857479, 0.5260675042864814, 0.0, 1.0, 117152.109844733], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.579898449047899, 0.6753558347621604, 0.0, 1.0, 0.5578671897368238], 
reward next is 0.4421, 
noisyNet noise sample is [array([-0.26447695], dtype=float32), 2.208626]. 
=============================================
[2019-04-04 14:30:34,341] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4916657e-10 1.6378236e-11 7.1119447e-17 3.9492832e-17 1.0000000e+00
 1.7660581e-12 1.1311525e-16], sum to 1.0000
[2019-04-04 14:30:34,343] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7178
[2019-04-04 14:30:34,358] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.43356036915923, 0.5900047252661925, 0.0, 1.0, 53318.41740966364], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1287600.0000, 
sim time next is 1288200.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.43213842692758, 0.5930369771705806, 0.0, 1.0, 43798.48042823028], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6193448689106317, 0.6976789923901935, 0.0, 1.0, 0.20856419251538227], 
reward next is 0.7914, 
noisyNet noise sample is [array([0.08981705], dtype=float32), 0.66475165]. 
=============================================
[2019-04-04 14:30:45,338] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0051254e-10 3.2321847e-12 5.8994484e-18 1.3452827e-15 1.0000000e+00
 2.1068160e-12 7.4522836e-18], sum to 1.0000
[2019-04-04 14:30:45,339] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1981
[2019-04-04 14:30:45,353] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 68.0, 157.5, 112.0, 26.0, 26.60594582571135, 0.6990164461399156, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1591200.0000, 
sim time next is 1591800.0000, 
raw observation next is [7.983333333333333, 66.83333333333334, 171.6666666666667, 104.0, 26.0, 26.68247703995801, 0.7088023296087097, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6837488457987074, 0.6683333333333334, 0.5722222222222224, 0.11491712707182321, 0.6666666666666666, 0.7235397533298341, 0.7362674432029032, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.94194615], dtype=float32), 0.8460544]. 
=============================================
[2019-04-04 14:30:45,369] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1847871e-09 9.4792896e-11 1.1996859e-16 5.6345436e-15 1.0000000e+00
 1.2066106e-10 6.4909352e-16], sum to 1.0000
[2019-04-04 14:30:45,370] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2672
[2019-04-04 14:30:45,394] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.39293358116722, 0.4875454396096753, 0.0, 1.0, 35202.9810179666], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1467600.0000, 
sim time next is 1468200.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.3720316951291, 0.484659868942129, 0.0, 1.0, 46644.62763690638], 
processed observation next is [1.0, 1.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6143359745940916, 0.6615532896473764, 0.0, 1.0, 0.22211727446145893], 
reward next is 0.7779, 
noisyNet noise sample is [array([-0.6557553], dtype=float32), -1.2992487]. 
=============================================
[2019-04-04 14:30:48,076] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2482696e-09 7.0500147e-11 6.7220879e-16 1.0161048e-14 1.0000000e+00
 1.8462164e-11 7.6171524e-16], sum to 1.0000
[2019-04-04 14:30:48,076] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3120
[2019-04-04 14:30:48,087] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 26.0, 25.42849150742511, 0.4161167544926632, 0.0, 1.0, 65653.79111905598], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1494000.0000, 
sim time next is 1494600.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 26.0, 25.25421893190754, 0.4131166837266256, 0.0, 1.0, 82273.50207748594], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6045182443256284, 0.6377055612422086, 0.0, 1.0, 0.3917785813213616], 
reward next is 0.6082, 
noisyNet noise sample is [array([2.366864], dtype=float32), 0.4879169]. 
=============================================
[2019-04-04 14:30:48,832] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.6111051e-10 6.7053300e-11 4.9267698e-17 6.1053533e-15 1.0000000e+00
 3.5082038e-11 2.8144493e-16], sum to 1.0000
[2019-04-04 14:30:48,839] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3385
[2019-04-04 14:30:48,852] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.75, 83.66666666666667, 0.0, 0.0, 26.0, 25.5248564317123, 0.5273719492487118, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1573800.0000, 
sim time next is 1574400.0000, 
raw observation next is [4.800000000000001, 83.33333333333334, 0.0, 0.0, 26.0, 25.59104668342856, 0.5290139194196284, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5955678670360112, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6325872236190465, 0.6763379731398761, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30962873], dtype=float32), -1.6495285]. 
=============================================
[2019-04-04 14:30:55,453] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1059929e-09 4.3483619e-11 1.7999056e-15 4.2474474e-14 1.0000000e+00
 1.9954859e-11 2.7852292e-15], sum to 1.0000
[2019-04-04 14:30:55,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8635
[2019-04-04 14:30:55,462] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.0296086e-11 4.7014016e-12 7.9866547e-19 8.6864890e-17 1.0000000e+00
 7.4300478e-13 4.1313031e-18], sum to 1.0000
[2019-04-04 14:30:55,469] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6746
[2019-04-04 14:30:55,505] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.75, 92.0, 30.0, 0.0, 26.0, 25.61121932650918, 0.5147705210505017, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1672200.0000, 
sim time next is 1672800.0000, 
raw observation next is [2.566666666666667, 92.0, 33.83333333333333, 0.0, 26.0, 25.72070642675004, 0.5391370567125381, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5337026777469991, 0.92, 0.11277777777777777, 0.0, 0.6666666666666666, 0.6433922022291702, 0.6797123522375127, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0491025], dtype=float32), -0.22317846]. 
=============================================
[2019-04-04 14:30:55,538] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 85.0, 26.0, 0.0, 26.0, 24.88946424661335, 0.3682942680212445, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1758600.0000, 
sim time next is 1759200.0000, 
raw observation next is [-1.7, 84.33333333333333, 32.5, 0.0, 26.0, 25.14759382275716, 0.3772445116337981, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.4155124653739613, 0.8433333333333333, 0.10833333333333334, 0.0, 0.6666666666666666, 0.5956328185630966, 0.6257481705445994, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.725503], dtype=float32), 0.11970005]. 
=============================================
[2019-04-04 14:30:56,511] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0981481e-09 3.2886888e-11 3.3341933e-16 4.4088590e-15 1.0000000e+00
 8.6837655e-11 4.7165891e-16], sum to 1.0000
[2019-04-04 14:30:56,511] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5583
[2019-04-04 14:30:56,607] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 87.33333333333334, 104.6666666666667, 0.0, 26.0, 24.88493441162035, 0.4216428658683005, 1.0, 1.0, 196217.9094192962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1687800.0000, 
sim time next is 1688400.0000, 
raw observation next is [1.1, 88.0, 103.5, 0.0, 26.0, 24.1912855115808, 0.4096227234325464, 1.0, 1.0, 197185.7549368104], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.345, 0.0, 0.6666666666666666, 0.5159404592984002, 0.6365409078108488, 1.0, 1.0, 0.9389797854133829], 
reward next is 0.0610, 
noisyNet noise sample is [array([-0.10585637], dtype=float32), -0.95814204]. 
=============================================
[2019-04-04 14:31:12,632] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 14:31:12,637] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 14:31:12,638] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 14:31:12,639] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:31:12,639] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 14:31:12,646] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:31:12,646] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:31:12,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run26
[2019-04-04 14:31:13,354] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run26
[2019-04-04 14:31:13,498] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run26
[2019-04-04 14:31:22,175] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17099424], dtype=float32), 0.20902951]
[2019-04-04 14:31:22,176] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.4, 71.0, 0.0, 0.0, 26.0, 23.87452314386739, 0.04052335222094285, 0.0, 1.0, 44644.84152318563]
[2019-04-04 14:31:22,176] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 14:31:22,178] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.06334168e-08 1.91451965e-09 1.09303666e-13 6.28238997e-13
 1.00000000e+00 1.73183934e-09 1.47101000e-13], sampled 0.9041429563546239
[2019-04-04 14:31:50,264] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17099424], dtype=float32), 0.20902951]
[2019-04-04 14:31:50,264] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [14.85037236, 96.12720249, 0.0, 0.0, 26.0, 25.7292709145214, 0.6276098967864595, 0.0, 1.0, 0.0]
[2019-04-04 14:31:50,264] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 14:31:50,265] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.7958744e-10 1.1120590e-10 1.2935723e-16 6.0095294e-15 1.0000000e+00
 1.2796834e-11 2.8711201e-16], sampled 0.740715321554629
[2019-04-04 14:32:25,876] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17099424], dtype=float32), 0.20902951]
[2019-04-04 14:32:25,876] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.1424664971666666, 16.91523918166666, 158.1666536666667, 319.3862983666667, 26.0, 25.33943538259139, 0.2962077408446066, 1.0, 1.0, 0.0]
[2019-04-04 14:32:25,876] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 14:32:25,877] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.2835976e-09 8.9720775e-10 1.8271388e-14 9.1322274e-14 1.0000000e+00
 4.7425297e-10 3.0986386e-14], sampled 0.5544330573779347
[2019-04-04 14:32:29,289] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17099424], dtype=float32), 0.20902951]
[2019-04-04 14:32:29,290] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [12.8, 78.0, 0.0, 0.0, 26.0, 25.74148881575709, 0.6305777859193149, 0.0, 1.0, 0.0]
[2019-04-04 14:32:29,290] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 14:32:29,290] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.9216159e-10 3.2209731e-11 4.0828001e-17 1.0037301e-15 1.0000000e+00
 7.3151667e-12 8.2219443e-17], sampled 0.367954701012451
[2019-04-04 14:32:55,337] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 14:33:11,193] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 14:33:15,553] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17099424], dtype=float32), 0.20902951]
[2019-04-04 14:33:15,554] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.6801866025000001, 41.529750595, 0.0, 0.0, 26.0, 25.27465127551871, 0.3196428637303977, 0.0, 1.0, 37917.64458598733]
[2019-04-04 14:33:15,554] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 14:33:15,555] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.5998230e-09 1.9509623e-09 4.4348294e-14 2.9666853e-13 1.0000000e+00
 7.7908546e-10 7.1186465e-14], sampled 0.3321140856918917
[2019-04-04 14:33:18,285] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7881 275774498.6820 1233.0964
[2019-04-04 14:33:19,309] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2500000, evaluation results [2500000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.788101514333, 275774498.68198794, 1233.096368464637]
[2019-04-04 14:33:24,005] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1350755e-08 2.3474761e-10 6.1331547e-15 1.2416714e-13 1.0000000e+00
 2.3418738e-09 6.0617506e-14], sum to 1.0000
[2019-04-04 14:33:24,007] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8616
[2019-04-04 14:33:24,023] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.69257305439856, 0.2316152128561921, 0.0, 1.0, 43000.31823684725], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1983600.0000, 
sim time next is 1984200.0000, 
raw observation next is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.64378994817314, 0.2224094543363698, 0.0, 1.0, 42988.65755713489], 
processed observation next is [1.0, 1.0, 0.3074792243767313, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5536491623477616, 0.5741364847787899, 0.0, 1.0, 0.20470789312921375], 
reward next is 0.7953, 
noisyNet noise sample is [array([0.02283758], dtype=float32), -0.27865168]. 
=============================================
[2019-04-04 14:33:39,230] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.8656022e-09 6.3510497e-10 8.1671811e-16 3.6867217e-14 1.0000000e+00
 2.8145869e-10 4.5241508e-15], sum to 1.0000
[2019-04-04 14:33:39,230] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0680
[2019-04-04 14:33:39,271] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 71.0, 130.0, 0.0, 26.0, 25.74196843231881, 0.340030840573157, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2199600.0000, 
sim time next is 2200200.0000, 
raw observation next is [-4.4, 70.5, 134.3333333333333, 0.0, 26.0, 25.62609130626322, 0.3304065330061868, 1.0, 1.0, 75904.19869121828], 
processed observation next is [1.0, 0.4782608695652174, 0.3407202216066482, 0.705, 0.4477777777777776, 0.0, 0.6666666666666666, 0.6355076088552684, 0.6101355110020622, 1.0, 1.0, 0.3614485651962775], 
reward next is 0.6386, 
noisyNet noise sample is [array([0.2987962], dtype=float32), -0.7694766]. 
=============================================
[2019-04-04 14:33:52,025] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7880827e-08 5.1641352e-10 2.7638826e-14 2.1318243e-13 1.0000000e+00
 3.0364884e-09 6.8400452e-15], sum to 1.0000
[2019-04-04 14:33:52,026] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9232
[2019-04-04 14:33:52,047] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.09999999999999999, 44.83333333333334, 73.33333333333333, 14.0, 26.0, 25.881625337917, 0.4143582437411791, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2304600.0000, 
sim time next is 2305200.0000, 
raw observation next is [-0.2, 45.66666666666667, 57.16666666666667, 6.999999999999998, 26.0, 25.89612092359059, 0.4087784644292276, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4570637119113574, 0.4566666666666667, 0.19055555555555556, 0.007734806629834252, 0.6666666666666666, 0.6580100769658825, 0.6362594881430759, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.61273456], dtype=float32), 0.94050616]. 
=============================================
[2019-04-04 14:33:54,533] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0395380e-08 6.3658487e-09 7.9592264e-14 5.1653587e-13 1.0000000e+00
 3.4632772e-09 1.0670345e-13], sum to 1.0000
[2019-04-04 14:33:54,536] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1569
[2019-04-04 14:33:54,554] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.2, 27.0, 82.5, 808.0, 26.0, 24.96188757682368, 0.2787476920026817, 0.0, 1.0, 18707.86178358908], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2469600.0000, 
sim time next is 2470200.0000, 
raw observation next is [2.383333333333333, 27.0, 81.0, 800.0, 26.0, 24.96827841420013, 0.28004489695994, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.5286241920590952, 0.27, 0.27, 0.8839779005524862, 0.6666666666666666, 0.580689867850011, 0.5933482989866466, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2452494], dtype=float32), 1.8465111]. 
=============================================
[2019-04-04 14:33:57,436] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0331182e-10 2.0267760e-10 1.0535104e-15 3.4796145e-15 1.0000000e+00
 1.7775016e-11 1.9002091e-15], sum to 1.0000
[2019-04-04 14:33:57,437] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2977
[2019-04-04 14:33:57,491] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.566666666666666, 68.33333333333333, 102.8333333333333, 121.6666666666667, 26.0, 25.87453131449431, 0.3648260994312356, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2626800.0000, 
sim time next is 2627400.0000, 
raw observation next is [-5.283333333333333, 66.66666666666667, 112.6666666666667, 152.3333333333333, 26.0, 25.86916515468575, 0.3641132432542695, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.31625115420129274, 0.6666666666666667, 0.37555555555555564, 0.16832412523020251, 0.6666666666666666, 0.6557637628904791, 0.6213710810847565, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5416268], dtype=float32), 0.9012242]. 
=============================================
[2019-04-04 14:33:58,091] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.1735720e-08 1.4663913e-08 1.2432288e-12 4.3614747e-12 9.9999988e-01
 4.0338111e-09 1.2131400e-12], sum to 1.0000
[2019-04-04 14:33:58,093] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8013
[2019-04-04 14:33:58,123] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 58.0, 21.5, 228.0, 26.0, 22.83146151826167, -0.2198925688171398, 0.0, 1.0, 44076.79577140748], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2448000.0000, 
sim time next is 2448600.0000, 
raw observation next is [-9.133333333333333, 56.66666666666667, 27.66666666666667, 290.6666666666667, 26.0, 22.81929202740367, -0.1997237382140824, 0.0, 1.0, 44268.9102685885], 
processed observation next is [0.0, 0.34782608695652173, 0.20960295475530935, 0.5666666666666668, 0.09222222222222223, 0.3211786372007367, 0.6666666666666666, 0.4016076689503058, 0.43342542059530587, 0.0, 1.0, 0.2108043346123262], 
reward next is 0.7892, 
noisyNet noise sample is [array([1.2585742], dtype=float32), -1.0988692]. 
=============================================
[2019-04-04 14:34:03,500] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.6499072e-09 2.5797722e-10 1.0359120e-15 3.1616467e-14 1.0000000e+00
 1.9394301e-10 7.9721818e-15], sum to 1.0000
[2019-04-04 14:34:03,501] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2999
[2019-04-04 14:34:03,526] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.083333333333333, 62.5, 176.0, 240.3333333333333, 26.0, 25.73674000120192, 0.3659970881998338, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2631000.0000, 
sim time next is 2631600.0000, 
raw observation next is [-3.9, 62.0, 188.0, 223.0, 26.0, 25.72983903135655, 0.3624869343925809, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3545706371191136, 0.62, 0.6266666666666667, 0.24640883977900552, 0.6666666666666666, 0.6441532526130459, 0.6208289781308604, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4437467], dtype=float32), 0.08621815]. 
=============================================
[2019-04-04 14:34:07,107] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3069736e-11 4.3384350e-12 7.9447861e-18 5.2723846e-16 1.0000000e+00
 1.5877472e-12 1.2177643e-17], sum to 1.0000
[2019-04-04 14:34:07,107] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8453
[2019-04-04 14:34:07,142] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333334, 97.66666666666666, 53.83333333333333, 0.0, 26.0, 25.4306241172086, 0.3127764234312559, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2886000.0000, 
sim time next is 2886600.0000, 
raw observation next is [0.1666666666666666, 98.83333333333334, 58.66666666666666, 0.0, 26.0, 25.46394604930493, 0.3076971907968034, 1.0, 1.0, 18682.08334758304], 
processed observation next is [1.0, 0.391304347826087, 0.4672206832871654, 0.9883333333333334, 0.1955555555555555, 0.0, 0.6666666666666666, 0.6219955041087442, 0.6025657302656011, 1.0, 1.0, 0.08896230165515734], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.17837176], dtype=float32), 0.15046512]. 
=============================================
[2019-04-04 14:34:09,434] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.7304453e-10 1.2414987e-09 9.6432456e-16 5.8521409e-15 1.0000000e+00
 1.9940982e-10 3.7669558e-15], sum to 1.0000
[2019-04-04 14:34:09,434] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0267
[2019-04-04 14:34:09,466] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.666666666666668, 64.0, 112.8333333333333, 763.1666666666667, 26.0, 26.04548225753594, 0.4791464932187646, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2719200.0000, 
sim time next is 2719800.0000, 
raw observation next is [-8.5, 64.0, 112.0, 781.0, 26.0, 26.03364014067423, 0.4770869061091472, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.22714681440443216, 0.64, 0.37333333333333335, 0.8629834254143647, 0.6666666666666666, 0.6694700117228525, 0.6590289687030491, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6644796], dtype=float32), -1.1082066]. 
=============================================
[2019-04-04 14:34:10,474] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7725202e-10 3.9287268e-10 2.4177084e-15 2.3217117e-14 1.0000000e+00
 1.6873274e-11 1.4160210e-15], sum to 1.0000
[2019-04-04 14:34:10,485] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4560
[2019-04-04 14:34:10,508] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.79839559852112, 0.3073574057540323, 0.0, 1.0, 43185.62854987569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2944200.0000, 
sim time next is 2944800.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.77293840776252, 0.2987411381095923, 0.0, 1.0, 43143.86357743706], 
processed observation next is [0.0, 0.08695652173913043, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.56441153398021, 0.5995803793698641, 0.0, 1.0, 0.20544696941636698], 
reward next is 0.7946, 
noisyNet noise sample is [array([-1.1658515], dtype=float32), 0.07796558]. 
=============================================
[2019-04-04 14:34:17,627] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9730750e-09 6.4895433e-10 2.1588365e-15 1.2664407e-14 1.0000000e+00
 6.9063671e-11 8.1837147e-15], sum to 1.0000
[2019-04-04 14:34:17,631] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2196
[2019-04-04 14:34:17,672] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0320753e-11 1.6201849e-11 3.1341656e-18 2.7331791e-17 1.0000000e+00
 6.2828096e-13 3.9329543e-18], sum to 1.0000
[2019-04-04 14:34:17,672] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1448
[2019-04-04 14:34:17,692] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.66666666666667, 88.50000000000001, 91.33333333333333, 518.0, 26.0, 26.01044182235881, 0.4162407229482006, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2711400.0000, 
sim time next is 2712000.0000, 
raw observation next is [-13.33333333333333, 86.0, 94.16666666666667, 565.0, 26.0, 25.95356411511675, 0.4212913385993185, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.09325946445060027, 0.86, 0.3138888888888889, 0.6243093922651933, 0.6666666666666666, 0.6627970095930626, 0.6404304461997729, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2401937], dtype=float32), -0.13042097]. 
=============================================
[2019-04-04 14:34:17,696] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[83.43173]
 [83.23724]
 [82.89842]
 [82.42862]
 [81.76729]], R is [[83.7078476 ]
 [83.87077332]
 [84.03206635]
 [84.19174957]
 [84.34983063]].
[2019-04-04 14:34:17,705] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 100.0, 91.0, 519.5, 26.0, 26.2132157068446, 0.5176599646532251, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3142800.0000, 
sim time next is 3143400.0000, 
raw observation next is [7.0, 100.0, 93.66666666666667, 562.0, 26.0, 26.29283489905383, 0.5393070988859391, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.31222222222222223, 0.6209944751381216, 0.6666666666666666, 0.6910695749211525, 0.6797690329619797, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0573213], dtype=float32), 0.111880496]. 
=============================================
[2019-04-04 14:34:23,683] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6161590e-10 1.0403256e-11 7.9104018e-18 1.0003968e-15 1.0000000e+00
 9.0746776e-12 3.2876250e-17], sum to 1.0000
[2019-04-04 14:34:23,686] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4177
[2019-04-04 14:34:23,722] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.58494901479709, 0.6035375606880921, 0.0, 1.0, 18738.23433250855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3196200.0000, 
sim time next is 3196800.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.5478955230678, 0.594497062329948, 0.0, 1.0, 38886.76454748893], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6289912935889834, 0.698165687443316, 0.0, 1.0, 0.1851750692737568], 
reward next is 0.8148, 
noisyNet noise sample is [array([-0.7134822], dtype=float32), 0.6223664]. 
=============================================
[2019-04-04 14:34:30,664] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7843786e-08 6.4559664e-09 9.8239610e-15 9.7318360e-14 1.0000000e+00
 8.7584573e-10 3.4046821e-14], sum to 1.0000
[2019-04-04 14:34:30,665] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5816
[2019-04-04 14:34:30,710] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.666666666666667, 53.33333333333334, 113.5, 815.0, 26.0, 25.0592606945921, 0.3528175834947758, 0.0, 1.0, 18727.86206560603], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3068400.0000, 
sim time next is 3069000.0000, 
raw observation next is [-2.5, 52.5, 114.0, 817.0, 26.0, 25.08267976184641, 0.356771312795974, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.525, 0.38, 0.9027624309392265, 0.6666666666666666, 0.5902233134872009, 0.6189237709319914, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7835095], dtype=float32), 1.2049766]. 
=============================================
[2019-04-04 14:34:30,744] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[78.0194  ]
 [78.05276 ]
 [77.99775 ]
 [77.80698 ]
 [77.704025]], R is [[78.12380219]
 [78.25338745]
 [78.3351059 ]
 [78.33558655]
 [78.35823059]].
[2019-04-04 14:34:31,753] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2028768e-08 2.2126283e-09 2.5217521e-14 7.3144930e-13 1.0000000e+00
 1.2564231e-09 7.1974084e-14], sum to 1.0000
[2019-04-04 14:34:31,753] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1070
[2019-04-04 14:34:31,772] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 75.83333333333334, 0.0, 0.0, 26.0, 23.88467120933624, 0.02363079348458759, 0.0, 1.0, 40180.4150278407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3042600.0000, 
sim time next is 3043200.0000, 
raw observation next is [-6.0, 74.66666666666667, 0.0, 0.0, 26.0, 23.85991268526207, 0.02150102838018749, 0.0, 1.0, 40163.86805457473], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.4883260571051726, 0.5071670094600625, 0.0, 1.0, 0.19125651454559395], 
reward next is 0.8087, 
noisyNet noise sample is [array([0.245502], dtype=float32), -0.46493885]. 
=============================================
[2019-04-04 14:34:33,885] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.2288591e-09 3.1444833e-10 7.2226262e-15 2.5253237e-14 1.0000000e+00
 2.1105828e-10 9.9813685e-15], sum to 1.0000
[2019-04-04 14:34:33,885] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5609
[2019-04-04 14:34:33,935] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 67.0, 191.0, 67.33333333333331, 26.0, 24.94682898147651, 0.3259247687339995, 0.0, 1.0, 18751.98699253119], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2976000.0000, 
sim time next is 2976600.0000, 
raw observation next is [-3.166666666666667, 66.0, 204.0, 110.6666666666666, 26.0, 24.97636995703537, 0.3314560850655892, 0.0, 1.0, 18747.27090471123], 
processed observation next is [0.0, 0.43478260869565216, 0.3748845798707295, 0.66, 0.68, 0.12228360957642719, 0.6666666666666666, 0.5813641630862808, 0.6104853616885297, 0.0, 1.0, 0.089272718593863], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.31445253], dtype=float32), -0.7445159]. 
=============================================
[2019-04-04 14:34:40,916] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.1491034e-09 8.0783397e-10 2.7607108e-14 7.1485115e-14 1.0000000e+00
 3.6819645e-10 4.7237401e-15], sum to 1.0000
[2019-04-04 14:34:40,917] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3890
[2019-04-04 14:34:40,933] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.95689331222113, 0.3462205428574248, 0.0, 1.0, 43832.81470236457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3292200.0000, 
sim time next is 3292800.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.96837935469233, 0.3323898084000419, 0.0, 1.0, 43832.82544515742], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5806982795576943, 0.6107966028000139, 0.0, 1.0, 0.20872774021503532], 
reward next is 0.7913, 
noisyNet noise sample is [array([0.78455025], dtype=float32), -0.25937873]. 
=============================================
[2019-04-04 14:34:43,606] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2950775e-09 5.1814181e-11 1.3013435e-15 7.0701052e-15 1.0000000e+00
 2.1800609e-10 8.6164622e-16], sum to 1.0000
[2019-04-04 14:34:43,609] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7692
[2019-04-04 14:34:43,655] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 66.0, 0.0, 0.0, 26.0, 25.80498812305312, 0.6355446145197268, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3265800.0000, 
sim time next is 3266400.0000, 
raw observation next is [-4.0, 67.0, 0.0, 0.0, 26.0, 25.95109025282202, 0.6288949890083112, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.67, 0.0, 0.0, 0.6666666666666666, 0.662590854401835, 0.7096316630027704, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2176852], dtype=float32), -0.8995166]. 
=============================================
[2019-04-04 14:34:48,835] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2598000e-08 2.1463533e-09 3.9283440e-14 5.9484557e-13 1.0000000e+00
 2.9617162e-09 3.5139914e-14], sum to 1.0000
[2019-04-04 14:34:48,839] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6554
[2019-04-04 14:34:48,850] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83946724713513, 0.2829026761199563, 0.0, 1.0, 41086.01846527406], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3380400.0000, 
sim time next is 3381000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83059152870155, 0.2771886995981877, 0.0, 1.0, 41092.1123451376], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5692159607251291, 0.5923962331993958, 0.0, 1.0, 0.1956767254530362], 
reward next is 0.8043, 
noisyNet noise sample is [array([0.12391389], dtype=float32), 1.2154412]. 
=============================================
[2019-04-04 14:34:48,855] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.70305]
 [80.72725]
 [80.89198]
 [80.88613]
 [80.79812]], R is [[80.68732452]
 [80.68480682]
 [80.68222046]
 [80.6795578 ]
 [80.67686462]].
[2019-04-04 14:34:49,143] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9334008e-08 1.8553860e-09 5.8837699e-15 1.9214884e-13 1.0000000e+00
 4.5011039e-10 5.6876819e-14], sum to 1.0000
[2019-04-04 14:34:49,143] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3898
[2019-04-04 14:34:49,161] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.97929885047826, 0.3092620824177409, 0.0, 1.0, 43818.66154333748], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3816600.0000, 
sim time next is 3817200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.96310523091471, 0.3005925573177622, 0.0, 1.0, 43882.09218034998], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5802587692428925, 0.6001975191059207, 0.0, 1.0, 0.2089623437159523], 
reward next is 0.7910, 
noisyNet noise sample is [array([-2.4847965], dtype=float32), -1.1082102]. 
=============================================
[2019-04-04 14:35:02,714] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3354925e-09 3.8206713e-10 7.3246420e-17 1.1186408e-14 1.0000000e+00
 1.6857134e-10 2.3063058e-16], sum to 1.0000
[2019-04-04 14:35:02,718] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3141
[2019-04-04 14:35:02,733] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 60.00000000000001, 108.5, 759.1666666666667, 26.0, 26.51491274956609, 0.6138725280188523, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3838800.0000, 
sim time next is 3839400.0000, 
raw observation next is [-1.5, 60.0, 110.0, 775.0, 26.0, 26.54776256565594, 0.6210493737331625, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4210526315789474, 0.6, 0.36666666666666664, 0.856353591160221, 0.6666666666666666, 0.712313547137995, 0.7070164579110542, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8357709], dtype=float32), -0.16139653]. 
=============================================
[2019-04-04 14:35:03,939] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1944969e-09 8.8149360e-10 2.0703840e-14 7.9189015e-14 1.0000000e+00
 1.1182342e-09 8.9053580e-15], sum to 1.0000
[2019-04-04 14:35:03,939] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1693
[2019-04-04 14:35:03,963] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.166666666666667, 50.5, 0.0, 0.0, 26.0, 25.48801160519466, 0.5274738084517159, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3869400.0000, 
sim time next is 3870000.0000, 
raw observation next is [1.0, 51.0, 0.0, 0.0, 26.0, 25.51401788045739, 0.5324925561961621, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.51, 0.0, 0.0, 0.6666666666666666, 0.6261681567047827, 0.677497518732054, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2547293], dtype=float32), 1.7475425]. 
=============================================
[2019-04-04 14:35:03,974] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.2536  ]
 [81.22806 ]
 [81.275246]
 [81.20661 ]
 [80.416664]], R is [[81.51226044]
 [81.69713593]
 [81.8801651 ]
 [82.06136322]
 [81.36003113]].
[2019-04-04 14:35:12,797] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6279670e-09 1.6336218e-10 2.9231738e-15 5.5503026e-14 1.0000000e+00
 3.2681866e-10 1.9302583e-15], sum to 1.0000
[2019-04-04 14:35:12,799] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5933
[2019-04-04 14:35:12,813] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.91795479902657, 0.2684477286276469, 0.0, 1.0, 43252.57929680354], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3823800.0000, 
sim time next is 3824400.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.86947070972514, 0.251976299003063, 0.0, 1.0, 43147.02739957186], 
processed observation next is [1.0, 0.2608695652173913, 0.32409972299168976, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5724558924770949, 0.5839920996676876, 0.0, 1.0, 0.20546203523605647], 
reward next is 0.7945, 
noisyNet noise sample is [array([2.139763], dtype=float32), -0.1444707]. 
=============================================
[2019-04-04 14:35:14,779] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2684577e-09 8.4898581e-11 2.2374566e-15 1.1964109e-13 1.0000000e+00
 1.5307752e-10 1.9325791e-15], sum to 1.0000
[2019-04-04 14:35:14,781] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2188
[2019-04-04 14:35:14,793] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.36222651127257, 0.4368343603170533, 0.0, 1.0, 39754.21509358939], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3891600.0000, 
sim time next is 3892200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.40010778926091, 0.4444884189978862, 0.0, 1.0, 24692.22768624216], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6166756491050759, 0.6481628063326287, 0.0, 1.0, 0.11758203660115314], 
reward next is 0.8824, 
noisyNet noise sample is [array([-0.79374427], dtype=float32), 1.315926]. 
=============================================
[2019-04-04 14:35:16,991] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0030269e-09 3.2657749e-10 4.4795354e-15 7.7333024e-15 1.0000000e+00
 5.4780913e-10 1.0647178e-15], sum to 1.0000
[2019-04-04 14:35:16,992] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0932
[2019-04-04 14:35:17,011] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.333333333333334, 38.0, 102.1666666666667, 777.3333333333334, 26.0, 26.92003160807655, 0.7717574235088168, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3940800.0000, 
sim time next is 3941400.0000, 
raw observation next is [-4.166666666666667, 38.0, 99.33333333333334, 766.6666666666666, 26.0, 26.97891442334454, 0.7809870449067446, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3471837488457987, 0.38, 0.33111111111111113, 0.8471454880294659, 0.6666666666666666, 0.7482428686120451, 0.7603290149689149, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6995435], dtype=float32), 0.92554027]. 
=============================================
[2019-04-04 14:35:22,787] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5845367e-09 3.6398995e-11 4.1862131e-16 6.2132908e-15 1.0000000e+00
 8.0062408e-11 7.5063864e-16], sum to 1.0000
[2019-04-04 14:35:22,788] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5181
[2019-04-04 14:35:22,844] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.5, 41.5, 116.0, 824.0, 26.0, 25.01543527808224, 0.5615668180699301, 1.0, 1.0, 197455.185556544], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3936600.0000, 
sim time next is 3937200.0000, 
raw observation next is [-5.333333333333333, 40.33333333333334, 114.1666666666667, 818.0, 26.0, 25.73058690050912, 0.6525302792926416, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3148661126500462, 0.40333333333333343, 0.38055555555555565, 0.9038674033149171, 0.6666666666666666, 0.6442155750424267, 0.7175100930975472, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7502935], dtype=float32), -0.60733724]. 
=============================================
[2019-04-04 14:35:22,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2676281e-09 1.5849168e-10 1.2511700e-15 2.3329557e-14 1.0000000e+00
 4.6214234e-11 1.6605109e-16], sum to 1.0000
[2019-04-04 14:35:22,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5400
[2019-04-04 14:35:22,948] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.00732214993347, 0.4629934539619163, 1.0, 1.0, 45851.15365175533], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4478400.0000, 
sim time next is 4479000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.04367462954371, 0.4639409576811991, 0.0, 1.0, 18709.41619102495], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.586972885795309, 0.654646985893733, 0.0, 1.0, 0.08909245805249975], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.19863166], dtype=float32), 0.098034754]. 
=============================================
[2019-04-04 14:35:22,958] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[86.41324 ]
 [86.142975]
 [86.44095 ]
 [85.84111 ]
 [85.86406 ]], R is [[85.85984039]
 [85.78290558]
 [85.40332031]
 [85.09955597]
 [85.13481903]].
[2019-04-04 14:35:23,150] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3758958e-09 5.1598986e-11 6.9688588e-16 2.6381188e-15 1.0000000e+00
 3.2924170e-11 1.0467599e-16], sum to 1.0000
[2019-04-04 14:35:23,152] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8643
[2019-04-04 14:35:23,157] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.46666666666667, 58.66666666666666, 0.0, 0.0, 26.0, 26.838783392074, 0.8398746089571408, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4394400.0000, 
sim time next is 4395000.0000, 
raw observation next is [10.33333333333333, 58.83333333333334, 0.0, 0.0, 26.0, 26.81330132205868, 0.8296529527628537, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7488457987072946, 0.5883333333333334, 0.0, 0.0, 0.6666666666666666, 0.7344417768382234, 0.7765509842542846, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26650864], dtype=float32), 1.3048205]. 
=============================================
[2019-04-04 14:35:23,162] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[90.34075 ]
 [90.933334]
 [91.935905]
 [92.52195 ]
 [92.43046 ]], R is [[89.98680115]
 [90.08693695]
 [90.18606567]
 [90.28420258]
 [90.38136292]].
[2019-04-04 14:35:27,595] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.06981677e-08 1.42840306e-09 1.15669794e-14 4.56041929e-14
 1.00000000e+00 9.94771598e-10 8.24286724e-15], sum to 1.0000
[2019-04-04 14:35:27,595] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5977
[2019-04-04 14:35:27,619] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.2, 42.5, 0.0, 0.0, 26.0, 25.02373497762548, 0.346050897639957, 0.0, 1.0, 184272.9027565281], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4217400.0000, 
sim time next is 4218000.0000, 
raw observation next is [1.133333333333333, 42.66666666666666, 0.0, 0.0, 26.0, 25.09745496209449, 0.371998868737321, 0.0, 1.0, 100014.2036944], 
processed observation next is [0.0, 0.8260869565217391, 0.49399815327793173, 0.4266666666666666, 0.0, 0.0, 0.6666666666666666, 0.5914545801745407, 0.6239996229124404, 0.0, 1.0, 0.4762581128304762], 
reward next is 0.5237, 
noisyNet noise sample is [array([-0.73918414], dtype=float32), 0.7010827]. 
=============================================
[2019-04-04 14:35:27,623] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.7718 ]
 [76.58991]
 [75.42769]
 [75.19392]
 [74.97112]], R is [[78.21522522]
 [77.55558777]
 [76.83469391]
 [76.92692566]
 [77.05154419]].
[2019-04-04 14:35:28,199] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.5200273e-09 3.2870116e-09 6.9203566e-15 3.8747356e-14 1.0000000e+00
 2.8810471e-10 6.1289216e-15], sum to 1.0000
[2019-04-04 14:35:28,200] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4194
[2019-04-04 14:35:28,221] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3333333333333334, 30.66666666666667, 119.8333333333333, 808.1666666666666, 26.0, 26.72524082506046, 0.6433501295667442, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4101600.0000, 
sim time next is 4102200.0000, 
raw observation next is [0.0, 30.0, 121.0, 816.0, 26.0, 26.76042237113808, 0.4069448061468408, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.3, 0.4033333333333333, 0.901657458563536, 0.6666666666666666, 0.73003519759484, 0.6356482687156136, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.90588814], dtype=float32), -0.40478322]. 
=============================================
[2019-04-04 14:35:30,528] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.1187050e-10 2.4005142e-11 1.2757735e-16 6.9741017e-15 1.0000000e+00
 1.6551776e-11 4.8107718e-16], sum to 1.0000
[2019-04-04 14:35:30,528] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2787
[2019-04-04 14:35:30,573] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 117.0, 33.0, 26.0, 26.06674922177925, 0.5222399756803823, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4525200.0000, 
sim time next is 4525800.0000, 
raw observation next is [0.1666666666666667, 70.16666666666667, 119.0, 22.0, 26.0, 26.17725963045092, 0.528139334744303, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4672206832871654, 0.7016666666666667, 0.39666666666666667, 0.02430939226519337, 0.6666666666666666, 0.6814383025375766, 0.6760464449147676, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1103594], dtype=float32), -0.7577539]. 
=============================================
[2019-04-04 14:35:33,610] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.5315321e-10 8.8836993e-10 1.3647487e-15 5.4313040e-14 1.0000000e+00
 4.9076708e-11 1.2255020e-14], sum to 1.0000
[2019-04-04 14:35:33,612] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7450
[2019-04-04 14:35:33,630] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.5, 46.5, 0.0, 0.0, 26.0, 25.39121048968647, 0.3516849195488597, 0.0, 1.0, 40386.20671328501], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4239000.0000, 
sim time next is 4239600.0000, 
raw observation next is [2.666666666666667, 46.0, 0.0, 0.0, 26.0, 25.40692966867479, 0.3520361423256067, 0.0, 1.0, 30728.98830973974], 
processed observation next is [0.0, 0.043478260869565216, 0.5364727608494922, 0.46, 0.0, 0.0, 0.6666666666666666, 0.6172441390562323, 0.6173453807752022, 0.0, 1.0, 0.14632851576066544], 
reward next is 0.8537, 
noisyNet noise sample is [array([1.4886943], dtype=float32), 0.71533686]. 
=============================================
[2019-04-04 14:35:43,008] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0347636e-10 4.4983638e-11 4.3165917e-17 4.6013577e-16 1.0000000e+00
 1.0853222e-11 2.0039739e-17], sum to 1.0000
[2019-04-04 14:35:43,010] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2507
[2019-04-04 14:35:43,021] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8333333333333334, 87.0, 115.3333333333333, 0.0, 26.0, 26.41980998316163, 0.6157848908988196, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4450200.0000, 
sim time next is 4450800.0000, 
raw observation next is [0.6666666666666667, 88.0, 108.6666666666667, 0.0, 26.0, 26.37303888336253, 0.6034858849032799, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4810710987996307, 0.88, 0.36222222222222233, 0.0, 0.6666666666666666, 0.6977532402802108, 0.7011619616344267, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3059925], dtype=float32), -0.6780146]. 
=============================================
[2019-04-04 14:35:46,018] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4872409e-09 2.4342986e-10 2.5112571e-16 4.9810865e-15 1.0000000e+00
 7.0442888e-11 1.0155423e-15], sum to 1.0000
[2019-04-04 14:35:46,020] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8158
[2019-04-04 14:35:46,041] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.15, 72.0, 0.0, 0.0, 26.0, 25.39546664835329, 0.4832920733036319, 0.0, 1.0, 76774.62109910484], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4487400.0000, 
sim time next is 4488000.0000, 
raw observation next is [-0.2, 72.0, 0.0, 0.0, 26.0, 25.3600468464723, 0.4842922892494152, 0.0, 1.0, 70263.28243426225], 
processed observation next is [1.0, 0.9565217391304348, 0.4570637119113574, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6133372372060251, 0.6614307630831384, 0.0, 1.0, 0.3345870592107726], 
reward next is 0.6654, 
noisyNet noise sample is [array([-0.2667285], dtype=float32), -0.38175353]. 
=============================================
[2019-04-04 14:35:46,052] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[85.19943 ]
 [85.08587 ]
 [84.699844]
 [84.66317 ]
 [84.50484 ]], R is [[85.34131622]
 [85.12231445]
 [84.94169617]
 [84.71661377]
 [84.55157471]].
[2019-04-04 14:35:51,070] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4655373e-09 2.9809497e-10 6.3595428e-15 1.4557417e-14 1.0000000e+00
 1.6699302e-10 5.7621809e-15], sum to 1.0000
[2019-04-04 14:35:51,070] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0010
[2019-04-04 14:35:51,084] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.1, 67.0, 0.0, 0.0, 26.0, 25.33619686796398, 0.4221454875999464, 0.0, 1.0, 68884.40407900521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4590000.0000, 
sim time next is 4590600.0000, 
raw observation next is [-1.166666666666667, 67.33333333333334, 0.0, 0.0, 26.0, 25.35792806620313, 0.4294146309952279, 0.0, 1.0, 49369.43043509231], 
processed observation next is [1.0, 0.13043478260869565, 0.43028624192059095, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.6131606721835942, 0.6431382103317427, 0.0, 1.0, 0.23509252588139196], 
reward next is 0.7649, 
noisyNet noise sample is [array([0.4652721], dtype=float32), 0.005658171]. 
=============================================
[2019-04-04 14:35:54,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:35:54,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:35:54,591] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run20
[2019-04-04 14:35:58,650] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:35:58,652] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:35:58,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run20
[2019-04-04 14:35:59,891] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0913398e-08 8.9416169e-10 8.5731228e-15 6.5250275e-14 1.0000000e+00
 2.5984093e-10 1.6635536e-13], sum to 1.0000
[2019-04-04 14:35:59,891] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7369
[2019-04-04 14:35:59,906] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.166666666666667, 44.16666666666667, 248.0, 378.6666666666667, 26.0, 25.09325697950903, 0.3653188196861743, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4889400.0000, 
sim time next is 4890000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 242.0, 376.3333333333334, 26.0, 25.08014542898861, 0.3647486687896596, 0.0, 1.0, 18689.59600738501], 
processed observation next is [0.0, 0.6086956521739131, 0.5272391505078486, 0.4433333333333334, 0.8066666666666666, 0.4158379373848988, 0.6666666666666666, 0.5900121190823843, 0.6215828895965533, 0.0, 1.0, 0.08899807622564292], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.3976767], dtype=float32), 0.7855732]. 
=============================================
[2019-04-04 14:35:59,914] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[80.07732]
 [80.2913 ]
 [80.48921]
 [80.69647]
 [80.92424]], R is [[80.05399323]
 [80.25345612]
 [80.4509201 ]
 [80.64640808]
 [80.83994293]].
[2019-04-04 14:36:00,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:00,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:00,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run20
[2019-04-04 14:36:01,423] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6519788e-08 1.5528697e-09 1.9047823e-14 2.1057022e-13 1.0000000e+00
 1.8331021e-09 4.7755060e-14], sum to 1.0000
[2019-04-04 14:36:01,425] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0087
[2019-04-04 14:36:01,455] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 44.33333333333334, 0.0, 0.0, 26.0, 24.98793075074808, 0.3458915030987326, 0.0, 1.0, 198792.3548370029], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4821600.0000, 
sim time next is 4822200.0000, 
raw observation next is [1.0, 45.0, 0.0, 0.0, 26.0, 25.02237838021924, 0.3749123524897863, 0.0, 1.0, 166368.2020744562], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.45, 0.0, 0.0, 0.6666666666666666, 0.5851981983516034, 0.6249707841632621, 0.0, 1.0, 0.7922295336878867], 
reward next is 0.2078, 
noisyNet noise sample is [array([-0.47101587], dtype=float32), 0.5284775]. 
=============================================
[2019-04-04 14:36:06,308] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:06,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:06,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run20
[2019-04-04 14:36:06,479] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5937337e-10 8.6587251e-11 3.0585907e-15 5.4387960e-15 1.0000000e+00
 6.7859489e-11 2.1090604e-15], sum to 1.0000
[2019-04-04 14:36:06,480] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9864
[2019-04-04 14:36:06,499] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 34.0, 0.0, 0.0, 26.0, 25.47044470171203, 0.4960334515725138, 0.0, 1.0, 50725.56833580084], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5007600.0000, 
sim time next is 5008200.0000, 
raw observation next is [2.833333333333333, 35.0, 0.0, 0.0, 26.0, 25.52725108609411, 0.496359190088967, 0.0, 1.0, 18748.04628145459], 
processed observation next is [1.0, 1.0, 0.541089566020314, 0.35, 0.0, 0.0, 0.6666666666666666, 0.6272709238411759, 0.665453063362989, 0.0, 1.0, 0.08927641086406947], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.8128775], dtype=float32), 1.1559473]. 
=============================================
[2019-04-04 14:36:10,215] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:10,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:10,267] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run20
[2019-04-04 14:36:10,431] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:10,431] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:10,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run20
[2019-04-04 14:36:11,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:11,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:11,161] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run20
[2019-04-04 14:36:13,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:13,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:13,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run20
[2019-04-04 14:36:14,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:14,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:14,574] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run20
[2019-04-04 14:36:14,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:14,818] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:14,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:14,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:14,826] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run20
[2019-04-04 14:36:14,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run20
[2019-04-04 14:36:15,286] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:15,287] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:15,291] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run20
[2019-04-04 14:36:15,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:15,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:15,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run20
[2019-04-04 14:36:15,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:15,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:15,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run20
[2019-04-04 14:36:16,161] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:16,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:16,166] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run20
[2019-04-04 14:36:19,532] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.2026822e-09 2.8998742e-10 3.7206722e-14 9.6805962e-13 1.0000000e+00
 6.9821315e-10 3.8324873e-14], sum to 1.0000
[2019-04-04 14:36:19,532] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4530
[2019-04-04 14:36:19,550] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 74.0, 0.0, 0.0, 26.0, 23.6743446010575, -0.01489607997318048, 0.0, 1.0, 44425.98602926695], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 177000.0000, 
sim time next is 177600.0000, 
raw observation next is [-8.900000000000002, 74.0, 0.0, 0.0, 26.0, 23.65080153913102, -0.02123580632735929, 0.0, 1.0, 44424.10759721923], 
processed observation next is [1.0, 0.043478260869565216, 0.2160664819944598, 0.74, 0.0, 0.0, 0.6666666666666666, 0.47090012826091837, 0.49292139789088024, 0.0, 1.0, 0.21154336951056774], 
reward next is 0.7885, 
noisyNet noise sample is [array([-0.58519953], dtype=float32), 0.5037948]. 
=============================================
[2019-04-04 14:36:20,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:36:20,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:36:20,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run20
[2019-04-04 14:36:40,701] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7709502e-09 1.8635458e-10 6.1537308e-15 3.3508491e-14 1.0000000e+00
 6.9407542e-11 5.0428598e-15], sum to 1.0000
[2019-04-04 14:36:40,708] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9733
[2019-04-04 14:36:40,801] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.7754863450811, 0.2816835099545512, 1.0, 1.0, 136591.8306598813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 240600.0000, 
sim time next is 241200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.01098831388592, 0.3038389345456508, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5842490261571601, 0.6012796448485503, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3594158], dtype=float32), 1.0766021]. 
=============================================
[2019-04-04 14:36:44,256] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2516831e-08 7.7757010e-09 5.6382047e-13 6.9158546e-12 1.0000000e+00
 5.8630629e-09 4.1416648e-13], sum to 1.0000
[2019-04-04 14:36:44,256] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3028
[2019-04-04 14:36:44,270] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.23333333333333, 68.0, 0.0, 0.0, 26.0, 23.16715631102397, -0.1421492418874739, 0.0, 1.0, 47258.32244631694], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 276000.0000, 
sim time next is 276600.0000, 
raw observation next is [-10.41666666666667, 67.5, 0.0, 0.0, 26.0, 23.0903580042391, -0.1562843895820646, 0.0, 1.0, 47407.59749602441], 
processed observation next is [1.0, 0.17391304347826086, 0.17405355493998145, 0.675, 0.0, 0.0, 0.6666666666666666, 0.42419650035325834, 0.4479052034726451, 0.0, 1.0, 0.2257504642667829], 
reward next is 0.7742, 
noisyNet noise sample is [array([-0.08733398], dtype=float32), -0.73693764]. 
=============================================
[2019-04-04 14:36:51,665] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7036480e-07 1.2760343e-07 9.3829306e-13 2.6545575e-11 9.9999952e-01
 1.8826685e-07 1.7992313e-12], sum to 1.0000
[2019-04-04 14:36:51,668] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7923
[2019-04-04 14:36:51,686] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-16.0, 76.33333333333334, 0.0, 0.0, 26.0, 22.0801167179802, -0.4147324806918082, 0.0, 1.0, 48610.76102454025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 366000.0000, 
sim time next is 366600.0000, 
raw observation next is [-16.1, 77.16666666666666, 0.0, 0.0, 26.0, 21.9875273075273, -0.4363105681234463, 0.0, 1.0, 48660.89965763258], 
processed observation next is [1.0, 0.21739130434782608, 0.01662049861495839, 0.7716666666666666, 0.0, 0.0, 0.6666666666666666, 0.33229394229394177, 0.35456314395885125, 0.0, 1.0, 0.2317185697982504], 
reward next is 0.7683, 
noisyNet noise sample is [array([0.363758], dtype=float32), -0.26085603]. 
=============================================
[2019-04-04 14:37:08,590] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2063259e-09 4.8066932e-11 2.8656273e-16 5.9399693e-15 1.0000000e+00
 1.8597997e-10 2.6288250e-16], sum to 1.0000
[2019-04-04 14:37:08,590] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1508
[2019-04-04 14:37:08,645] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.899999999999999, 83.66666666666666, 57.33333333333334, 0.0, 26.0, 26.28180177736747, 0.4502941044036881, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 830400.0000, 
sim time next is 831000.0000, 
raw observation next is [-3.9, 84.83333333333334, 55.66666666666666, 0.0, 26.0, 26.260706404538, 0.4445739299599565, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.8483333333333334, 0.18555555555555553, 0.0, 0.6666666666666666, 0.6883922003781665, 0.6481913099866522, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.86384994], dtype=float32), -0.75013214]. 
=============================================
[2019-04-04 14:37:08,653] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[84.87121 ]
 [85.029655]
 [85.16486 ]
 [85.372154]
 [85.47503 ]], R is [[84.90964508]
 [85.06054688]
 [85.20994568]
 [85.35784912]
 [85.50427246]].
[2019-04-04 14:37:09,254] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-04 14:37:09,261] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 14:37:09,262] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:37:09,263] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 14:37:09,264] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 14:37:09,264] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:37:09,265] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:37:09,271] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run27
[2019-04-04 14:37:09,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run27
[2019-04-04 14:37:09,310] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run27
[2019-04-04 14:38:50,170] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 14:38:53,439] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17546524], dtype=float32), 0.21426074]
[2019-04-04 14:38:53,439] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 26.0, 24.76315762782176, 0.2120730104313277, 0.0, 1.0, 40423.68754907995]
[2019-04-04 14:38:53,439] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 14:38:53,440] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.7045454e-08 5.3101079e-09 1.2721063e-13 8.5772204e-13 1.0000000e+00
 1.5033687e-09 2.1035987e-13], sampled 0.35503427188209535
[2019-04-04 14:39:10,914] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 14:39:13,263] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17546524], dtype=float32), 0.21426074]
[2019-04-04 14:39:13,263] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.933952854333334, 34.44979452, 0.0, 0.0, 26.0, 25.61444015861635, 0.5684990310712611, 0.0, 1.0, 18734.86330247417]
[2019-04-04 14:39:13,263] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 14:39:13,264] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.39507378e-09 5.22105303e-10 7.10815480e-15 3.53288904e-14
 1.00000000e+00 3.54700103e-10 1.17430395e-14], sampled 0.45464741755291693
[2019-04-04 14:39:14,770] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 14:39:15,794] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 2600000, evaluation results [2600000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 14:39:20,103] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9284178e-08 2.0694013e-10 3.8024287e-15 8.5047556e-14 1.0000000e+00
 5.6198938e-11 8.1859627e-15], sum to 1.0000
[2019-04-04 14:39:20,103] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9457
[2019-04-04 14:39:20,142] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8999999999999999, 74.0, 0.0, 0.0, 26.0, 24.64429745188357, 0.1757677755794183, 0.0, 1.0, 39031.11774155078], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 880200.0000, 
sim time next is 880800.0000, 
raw observation next is [-0.8, 73.33333333333334, 0.0, 0.0, 26.0, 24.60562282934108, 0.1680301700492043, 0.0, 1.0, 39038.56473414176], 
processed observation next is [1.0, 0.17391304347826086, 0.4404432132963989, 0.7333333333333334, 0.0, 0.0, 0.6666666666666666, 0.5504685691117567, 0.5560100566830681, 0.0, 1.0, 0.18589792730543694], 
reward next is 0.8141, 
noisyNet noise sample is [array([-1.2223301], dtype=float32), -0.84671015]. 
=============================================
[2019-04-04 14:39:20,761] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5883124e-10 1.5635675e-11 1.6581816e-16 9.5368996e-16 1.0000000e+00
 2.1550929e-11 4.3935279e-16], sum to 1.0000
[2019-04-04 14:39:20,764] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3578
[2019-04-04 14:39:20,788] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.05, 85.5, 0.0, 0.0, 26.0, 25.38933093905168, 0.432888789048212, 0.0, 1.0, 28593.64349782226], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 955800.0000, 
sim time next is 956400.0000, 
raw observation next is [6.233333333333333, 84.33333333333334, 0.0, 0.0, 26.0, 25.36175001639029, 0.4492035419528292, 0.0, 1.0, 42696.3313622064], 
processed observation next is [1.0, 0.043478260869565216, 0.6352723915050786, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6134791680325241, 0.6497345139842764, 0.0, 1.0, 0.20331586362955428], 
reward next is 0.7967, 
noisyNet noise sample is [array([0.36617613], dtype=float32), -0.6109629]. 
=============================================
[2019-04-04 14:39:29,789] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.37983776e-08 3.14399906e-09 6.40189534e-15 4.38582357e-13
 1.00000000e+00 1.40119749e-09 7.92551619e-15], sum to 1.0000
[2019-04-04 14:39:29,789] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0677
[2019-04-04 14:39:29,814] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 72.33333333333333, 0.0, 0.0, 26.0, 23.78055889866079, 0.005174970032150547, 0.0, 1.0, 41570.62659995551], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 794400.0000, 
sim time next is 795000.0000, 
raw observation next is [-7.299999999999999, 71.66666666666667, 0.0, 0.0, 26.0, 23.83218583296544, 5.539652394715803e-05, 0.0, 1.0, 41624.39524490426], 
processed observation next is [1.0, 0.17391304347826086, 0.2603878116343491, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.4860154860804533, 0.5000184655079823, 0.0, 1.0, 0.19821140592811554], 
reward next is 0.8018, 
noisyNet noise sample is [array([0.15887241], dtype=float32), -1.1415714]. 
=============================================
[2019-04-04 14:39:29,822] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[76.29009 ]
 [76.349045]
 [76.41086 ]
 [76.474014]
 [76.512245]], R is [[76.26636505]
 [76.30574799]
 [76.34509277]
 [76.38442993]
 [76.42369843]].
[2019-04-04 14:39:31,298] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1261558e-09 1.3644606e-10 1.0096974e-16 8.7467197e-14 1.0000000e+00
 2.3325189e-11 4.0193815e-16], sum to 1.0000
[2019-04-04 14:39:31,299] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7551
[2019-04-04 14:39:31,317] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.2, 97.33333333333333, 100.0, 0.0, 26.0, 25.04158017824417, 0.480953880856043, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1254000.0000, 
sim time next is 1254600.0000, 
raw observation next is [14.1, 98.0, 101.0, 0.0, 26.0, 25.01209888551502, 0.4775563382115945, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8531855955678671, 0.98, 0.33666666666666667, 0.0, 0.6666666666666666, 0.5843415737929183, 0.6591854460705315, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07597033], dtype=float32), 0.57394797]. 
=============================================
[2019-04-04 14:39:36,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8899960e-10 2.6658228e-11 1.4173328e-17 1.7679765e-15 1.0000000e+00
 1.5643013e-11 1.7699046e-17], sum to 1.0000
[2019-04-04 14:39:36,241] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6861
[2019-04-04 14:39:36,252] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.0, 83.0, 0.0, 0.0, 26.0, 25.45047667527556, 0.4489051168845444, 0.0, 1.0, 61189.4293814892], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 975600.0000, 
sim time next is 976200.0000, 
raw observation next is [9.9, 84.66666666666667, 0.0, 0.0, 26.0, 25.43893600366097, 0.4590722358780368, 0.0, 1.0, 49398.11350260479], 
processed observation next is [1.0, 0.30434782608695654, 0.7368421052631581, 0.8466666666666667, 0.0, 0.0, 0.6666666666666666, 0.6199113336384142, 0.6530240786260123, 0.0, 1.0, 0.23522911191716567], 
reward next is 0.7648, 
noisyNet noise sample is [array([-0.05169632], dtype=float32), 0.29035556]. 
=============================================
[2019-04-04 14:39:36,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.0202048e-10 4.8131700e-11 8.7075894e-17 6.8083640e-15 1.0000000e+00
 8.6213129e-12 5.0456090e-16], sum to 1.0000
[2019-04-04 14:39:36,438] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0757
[2019-04-04 14:39:36,449] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.416666666666666, 83.16666666666667, 0.0, 0.0, 26.0, 25.45991181375604, 0.4564619513085434, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 957000.0000, 
sim time next is 957600.0000, 
raw observation next is [6.6, 82.0, 0.0, 0.0, 26.0, 25.55978519492045, 0.4567112556129091, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6454293628808865, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6299820995767043, 0.652237085204303, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.9372475], dtype=float32), -1.4522545]. 
=============================================
[2019-04-04 14:39:36,775] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4042579e-11 8.9760859e-13 3.1481371e-18 2.6042200e-17 1.0000000e+00
 2.1072933e-13 5.2186523e-18], sum to 1.0000
[2019-04-04 14:39:36,776] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8034
[2019-04-04 14:39:36,834] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 92.0, 12.0, 0.0, 26.0, 25.62589793806414, 0.5300214197819166, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1325400.0000, 
sim time next is 1326000.0000, 
raw observation next is [0.9000000000000001, 92.0, 15.0, 0.0, 26.0, 25.54524981917851, 0.5551176034109101, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.48753462603878117, 0.92, 0.05, 0.0, 0.6666666666666666, 0.6287708182648757, 0.68503920113697, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27829528], dtype=float32), 0.053736184]. 
=============================================
[2019-04-04 14:39:36,841] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[94.83041]
 [94.58518]
 [94.1587 ]
 [93.73226]
 [93.32317]], R is [[94.8707428 ]
 [94.92203522]
 [94.97281647]
 [95.02308655]
 [95.07285309]].
[2019-04-04 14:39:39,134] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1352238e-10 2.8943140e-11 1.4831211e-17 9.1096501e-17 1.0000000e+00
 1.5577034e-11 3.5625606e-18], sum to 1.0000
[2019-04-04 14:39:39,136] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1001
[2019-04-04 14:39:39,153] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.61666666666667, 86.0, 125.0, 0.0, 26.0, 25.65685939628831, 0.5572460580923078, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 996600.0000, 
sim time next is 997200.0000, 
raw observation next is [12.7, 86.0, 123.5, 0.0, 26.0, 25.95829776757688, 0.5819968365784806, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8144044321329641, 0.86, 0.4116666666666667, 0.0, 0.6666666666666666, 0.6631914806314066, 0.6939989455261601, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1321093], dtype=float32), -1.5834261]. 
=============================================
[2019-04-04 14:39:40,313] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.3167771e-11 1.2635924e-12 4.4297056e-18 6.3112981e-17 1.0000000e+00
 3.5201982e-13 1.3758185e-17], sum to 1.0000
[2019-04-04 14:39:40,314] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0617
[2019-04-04 14:39:40,338] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.08333333333333, 92.16666666666667, 49.00000000000001, 0.0, 26.0, 26.24191036606288, 0.5790427284364678, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 983400.0000, 
sim time next is 984000.0000, 
raw observation next is [10.16666666666667, 92.33333333333334, 54.5, 0.0, 26.0, 26.2638841170213, 0.5918602792300621, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.7442289935364729, 0.9233333333333335, 0.18166666666666667, 0.0, 0.6666666666666666, 0.6886570097517749, 0.6972867597433541, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0455316], dtype=float32), -1.664677]. 
=============================================
[2019-04-04 14:39:40,340] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[98.081764]
 [98.01342 ]
 [97.928406]
 [97.847565]
 [97.67303 ]], R is [[98.11950684]
 [98.13831329]
 [98.15692902]
 [98.17536163]
 [98.19361115]].
[2019-04-04 14:39:40,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2467858e-09 3.0723958e-11 4.9335027e-17 5.0149475e-15 1.0000000e+00
 2.8072725e-11 1.2129017e-16], sum to 1.0000
[2019-04-04 14:39:40,922] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2369
[2019-04-04 14:39:40,939] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.47792696063625, 0.5815497862324557, 0.0, 1.0, 18758.56211624264], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1372800.0000, 
sim time next is 1373400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.49357335421183, 0.5819091131432105, 0.0, 1.0, 18754.13002698783], 
processed observation next is [1.0, 0.9130434782608695, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6244644461843191, 0.6939697043810701, 0.0, 1.0, 0.08930538108089443], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.19673079], dtype=float32), -0.62659043]. 
=============================================
[2019-04-04 14:39:42,061] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5786383e-10 1.1266940e-11 1.3327312e-17 3.5830351e-16 1.0000000e+00
 8.0474351e-12 1.9251161e-17], sum to 1.0000
[2019-04-04 14:39:42,062] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2610
[2019-04-04 14:39:42,112] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 81.0, 0.0, 26.0, 26.24443215775859, 0.4799619183084223, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1432800.0000, 
sim time next is 1433400.0000, 
raw observation next is [1.1, 92.0, 78.0, 0.0, 26.0, 25.70373242590797, 0.5094875260518186, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.26, 0.0, 0.6666666666666666, 0.6419777021589974, 0.6698291753506062, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06795134], dtype=float32), -0.383358]. 
=============================================
[2019-04-04 14:39:44,842] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7014358e-09 1.1103068e-09 1.7592910e-15 1.6293585e-14 1.0000000e+00
 4.9101709e-11 1.4175991e-15], sum to 1.0000
[2019-04-04 14:39:44,849] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2204
[2019-04-04 14:39:44,867] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.2, 94.33333333333334, 0.0, 0.0, 26.0, 25.32692902076804, 0.4494697403356256, 0.0, 1.0, 57490.27264618797], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1480200.0000, 
sim time next is 1480800.0000, 
raw observation next is [2.2, 94.66666666666667, 0.0, 0.0, 26.0, 25.28492650066783, 0.470680030098912, 0.0, 1.0, 45268.54980717762], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9466666666666668, 0.0, 0.0, 0.6666666666666666, 0.6070772083889858, 0.656893343366304, 0.0, 1.0, 0.215564522891322], 
reward next is 0.7844, 
noisyNet noise sample is [array([-0.25393623], dtype=float32), -1.1898824]. 
=============================================
[2019-04-04 14:39:45,235] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7146581e-09 4.2779183e-11 3.7562065e-16 2.0366058e-15 1.0000000e+00
 3.6489221e-11 1.4616999e-16], sum to 1.0000
[2019-04-04 14:39:45,239] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5439
[2019-04-04 14:39:45,250] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.36666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.65813387414685, 0.6444432481740628, 0.0, 1.0, 114359.3825337915], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1118400.0000, 
sim time next is 1119000.0000, 
raw observation next is [12.28333333333333, 65.66666666666666, 0.0, 0.0, 26.0, 25.62839417932684, 0.6562997746954419, 0.0, 1.0, 84811.5041820994], 
processed observation next is [1.0, 0.9565217391304348, 0.8028624192059095, 0.6566666666666666, 0.0, 0.0, 0.6666666666666666, 0.6356995149439033, 0.7187665915651472, 0.0, 1.0, 0.40386430562904474], 
reward next is 0.5961, 
noisyNet noise sample is [array([-0.21016663], dtype=float32), -0.97204614]. 
=============================================
[2019-04-04 14:39:45,256] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[90.57357 ]
 [90.314316]
 [90.21767 ]
 [89.41804 ]
 [88.86747 ]], R is [[90.66606903]
 [90.21483612]
 [90.31269073]
 [90.40956116]
 [90.50546265]].
[2019-04-04 14:39:54,754] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6726768e-10 2.0369597e-11 1.0418043e-16 5.3674818e-15 1.0000000e+00
 2.6155066e-12 1.9711309e-16], sum to 1.0000
[2019-04-04 14:39:54,759] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7949
[2019-04-04 14:39:54,777] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.55, 92.5, 0.0, 0.0, 26.0, 25.49840342098388, 0.5544002975734662, 0.0, 1.0, 64969.9138923942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1301400.0000, 
sim time next is 1302000.0000, 
raw observation next is [3.466666666666667, 92.33333333333333, 0.0, 0.0, 26.0, 25.38970078195608, 0.552692464230826, 0.0, 1.0, 109330.7317154679], 
processed observation next is [1.0, 0.043478260869565216, 0.5586334256694367, 0.9233333333333333, 0.0, 0.0, 0.6666666666666666, 0.61580839849634, 0.6842308214102752, 0.0, 1.0, 0.5206225319784186], 
reward next is 0.4794, 
noisyNet noise sample is [array([-1.1220642], dtype=float32), -0.60005]. 
=============================================
[2019-04-04 14:39:54,795] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[94.17867 ]
 [94.095055]
 [94.02744 ]
 [94.17838 ]
 [93.98766 ]], R is [[93.64600372]
 [93.40016174]
 [93.46616364]
 [93.44224548]
 [93.41855621]].
[2019-04-04 14:39:55,151] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0525852e-10 5.4687657e-11 3.5733677e-18 3.7810196e-16 1.0000000e+00
 7.8269205e-12 4.7490662e-17], sum to 1.0000
[2019-04-04 14:39:55,151] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6518
[2019-04-04 14:39:55,164] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 106.0, 0.0, 26.0, 25.71353115963649, 0.5298969784318149, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1344600.0000, 
sim time next is 1345200.0000, 
raw observation next is [1.1, 92.0, 100.1666666666667, 0.0, 26.0, 25.69319787048423, 0.5343136338309148, 1.0, 1.0, 26896.59772116946], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.333888888888889, 0.0, 0.6666666666666666, 0.6410998225403525, 0.678104544610305, 1.0, 1.0, 0.12807903676747362], 
reward next is 0.8719, 
noisyNet noise sample is [array([0.5682324], dtype=float32), -0.6746281]. 
=============================================
[2019-04-04 14:40:00,703] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1675443e-09 5.1661814e-11 5.4711303e-16 1.6746282e-14 1.0000000e+00
 2.9210172e-11 1.1432345e-15], sum to 1.0000
[2019-04-04 14:40:00,703] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0851
[2019-04-04 14:40:00,759] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.9, 87.0, 77.0, 0.0, 26.0, 24.99574571146828, 0.343840161514373, 0.0, 1.0, 33141.51280101093], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1782600.0000, 
sim time next is 1783200.0000, 
raw observation next is [-3.0, 87.0, 71.5, 0.0, 26.0, 24.99933853186023, 0.3434790915999586, 0.0, 1.0, 38453.00263160095], 
processed observation next is [0.0, 0.6521739130434783, 0.3795013850415513, 0.87, 0.23833333333333334, 0.0, 0.6666666666666666, 0.5832782109883524, 0.6144930305333195, 0.0, 1.0, 0.18310953634095692], 
reward next is 0.8169, 
noisyNet noise sample is [array([0.41614354], dtype=float32), -0.46961892]. 
=============================================
[2019-04-04 14:40:02,650] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.2797089e-11 5.9277097e-12 3.0267849e-17 1.3585927e-16 1.0000000e+00
 6.7157196e-12 1.1660574e-17], sum to 1.0000
[2019-04-04 14:40:02,651] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6035
[2019-04-04 14:40:02,670] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.616666666666667, 92.0, 57.33333333333333, 0.0, 26.0, 25.97179361934777, 0.5604533260538078, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1677000.0000, 
sim time next is 1677600.0000, 
raw observation next is [1.5, 92.0, 59.5, 0.0, 26.0, 26.00486590525426, 0.5608528224277286, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5041551246537397, 0.92, 0.19833333333333333, 0.0, 0.6666666666666666, 0.6670721587711883, 0.6869509408092429, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49679857], dtype=float32), 0.26880762]. 
=============================================
[2019-04-04 14:40:10,980] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2052772e-09 2.1250277e-10 1.2379245e-15 8.0067433e-15 1.0000000e+00
 8.3228230e-11 1.2670103e-15], sum to 1.0000
[2019-04-04 14:40:10,980] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3213
[2019-04-04 14:40:11,022] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.68652467809071, 0.5629039060301487, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1558800.0000, 
sim time next is 1559400.0000, 
raw observation next is [5.0, 81.50000000000001, 0.0, 0.0, 26.0, 25.65077802836193, 0.5595889547030564, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6011080332409973, 0.8150000000000002, 0.0, 0.0, 0.6666666666666666, 0.6375648356968275, 0.6865296515676854, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4988782], dtype=float32), -0.23569958]. 
=============================================
[2019-04-04 14:40:12,235] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6124577e-09 4.4593749e-11 2.2935918e-17 2.0861028e-16 1.0000000e+00
 1.2996917e-11 4.1421777e-17], sum to 1.0000
[2019-04-04 14:40:12,237] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1766
[2019-04-04 14:40:12,248] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.716666666666667, 92.0, 0.0, 0.0, 26.0, 25.58343938406546, 0.534965568940391, 0.0, 1.0, 30748.8560770091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1667400.0000, 
sim time next is 1668000.0000, 
raw observation next is [4.433333333333334, 92.0, 0.0, 0.0, 26.0, 25.57239642037791, 0.5275154249210822, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.5854108956602032, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6310330350314924, 0.675838474973694, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4660132], dtype=float32), -0.13164102]. 
=============================================
[2019-04-04 14:40:12,264] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[91.42766 ]
 [91.54503 ]
 [91.65428 ]
 [91.80475 ]
 [91.975136]], R is [[92.21316528]
 [92.14460754]
 [92.08953857]
 [92.07943726]
 [92.06941986]].
[2019-04-04 14:40:12,775] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0230774e-09 7.2476086e-10 1.9010184e-14 6.5303850e-13 1.0000000e+00
 2.0015296e-09 1.8175143e-14], sum to 1.0000
[2019-04-04 14:40:12,777] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8055
[2019-04-04 14:40:12,814] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 62.0, 93.0, 0.0, 26.0, 25.70908362145991, 0.3162595324619021, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1954800.0000, 
sim time next is 1955400.0000, 
raw observation next is [-2.8, 62.0, 86.66666666666666, 0.0, 26.0, 25.62686280864538, 0.3159300481894435, 1.0, 1.0, 77712.3279617485], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.28888888888888886, 0.0, 0.6666666666666666, 0.6355719007204484, 0.6053100160631478, 1.0, 1.0, 0.37005870457975476], 
reward next is 0.6299, 
noisyNet noise sample is [array([-1.1428097], dtype=float32), -1.496421]. 
=============================================
[2019-04-04 14:40:15,635] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6867788e-09 1.0136407e-10 9.1932287e-16 4.8297527e-14 1.0000000e+00
 3.8264673e-11 9.3891866e-16], sum to 1.0000
[2019-04-04 14:40:15,635] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3850
[2019-04-04 14:40:15,680] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.35, 91.5, 0.0, 0.0, 26.0, 25.32528733129514, 0.456620992497031, 0.0, 1.0, 43096.09348582705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1733400.0000, 
sim time next is 1734000.0000, 
raw observation next is [0.3, 91.33333333333334, 0.0, 0.0, 26.0, 25.34037666456886, 0.4534682181767955, 0.0, 1.0, 42946.15967838938], 
processed observation next is [0.0, 0.043478260869565216, 0.47091412742382277, 0.9133333333333334, 0.0, 0.0, 0.6666666666666666, 0.6116980553807384, 0.6511560727255985, 0.0, 1.0, 0.20450552227804464], 
reward next is 0.7955, 
noisyNet noise sample is [array([-0.09337902], dtype=float32), 0.547066]. 
=============================================
[2019-04-04 14:40:15,696] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.39813 ]
 [85.3082  ]
 [84.93775 ]
 [84.693954]
 [84.895164]], R is [[85.69702911]
 [85.63483429]
 [85.57142639]
 [85.5051651 ]
 [85.44177246]].
[2019-04-04 14:40:20,609] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.7314523e-09 1.7438184e-09 1.5686672e-14 1.5467181e-13 1.0000000e+00
 3.9553660e-10 1.3845076e-14], sum to 1.0000
[2019-04-04 14:40:20,611] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6431
[2019-04-04 14:40:20,676] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 79.0, 29.0, 0.0, 26.0, 25.04282930916071, 0.2619376211325491, 0.0, 1.0, 26588.12462945441], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1873800.0000, 
sim time next is 1874400.0000, 
raw observation next is [-4.5, 80.33333333333333, 24.33333333333334, 0.0, 26.0, 25.04510862481879, 0.2554597085259551, 0.0, 1.0, 32612.01842596262], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.8033333333333332, 0.08111111111111113, 0.0, 0.6666666666666666, 0.5870923854015659, 0.5851532361753183, 0.0, 1.0, 0.15529532583791722], 
reward next is 0.8447, 
noisyNet noise sample is [array([0.09757876], dtype=float32), -0.5716332]. 
=============================================
[2019-04-04 14:40:21,716] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1763305e-09 1.7757540e-10 2.2586468e-15 3.2844085e-14 1.0000000e+00
 8.2653974e-11 7.7629189e-15], sum to 1.0000
[2019-04-04 14:40:21,716] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9929
[2019-04-04 14:40:21,736] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 26.0, 25.02944356982434, 0.2949050996298186, 0.0, 1.0, 45965.71833335696], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1804800.0000, 
sim time next is 1805400.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 26.0, 24.98629822425945, 0.2942398070869985, 0.0, 1.0, 45947.97256395272], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5821915186882874, 0.5980799356956662, 0.0, 1.0, 0.21879986935215578], 
reward next is 0.7812, 
noisyNet noise sample is [array([-1.8293815], dtype=float32), 2.0690944]. 
=============================================
[2019-04-04 14:40:22,283] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7660111e-08 8.4605913e-09 1.9360075e-14 1.8664252e-12 1.0000000e+00
 8.2070573e-10 1.2442270e-13], sum to 1.0000
[2019-04-04 14:40:22,292] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4089
[2019-04-04 14:40:22,305] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.45, 78.5, 0.0, 0.0, 26.0, 23.4884000189094, -0.05238630114860113, 0.0, 1.0, 47069.16265338177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1837800.0000, 
sim time next is 1838400.0000, 
raw observation next is [-6.533333333333333, 78.33333333333334, 0.0, 0.0, 26.0, 23.4531375677962, -0.05972279394562333, 0.0, 1.0, 47084.64292207297], 
processed observation next is [0.0, 0.2608695652173913, 0.2816251154201293, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.4544281306496834, 0.4800924020181256, 0.0, 1.0, 0.22421258534320462], 
reward next is 0.7758, 
noisyNet noise sample is [array([-0.94898975], dtype=float32), -0.6236432]. 
=============================================
[2019-04-04 14:40:27,984] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5313890e-09 7.7459844e-11 7.9007893e-16 5.1522493e-14 1.0000000e+00
 4.9606836e-11 2.0535816e-15], sum to 1.0000
[2019-04-04 14:40:27,986] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3293
[2019-04-04 14:40:28,022] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.366666666666667, 83.66666666666667, 76.33333333333334, 461.0, 26.0, 25.56360285144506, 0.2870699733305909, 1.0, 1.0, 18744.42614334656], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1934400.0000, 
sim time next is 1935000.0000, 
raw observation next is [-8.1, 82.5, 85.0, 549.0, 26.0, 25.64493418818277, 0.3004715397942315, 1.0, 1.0, 18740.78732631262], 
processed observation next is [1.0, 0.391304347826087, 0.23822714681440446, 0.825, 0.2833333333333333, 0.6066298342541436, 0.6666666666666666, 0.6370778490152308, 0.6001571799314105, 1.0, 1.0, 0.08924184441101247], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.9779618], dtype=float32), -1.6144947]. 
=============================================
[2019-04-04 14:40:28,038] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.352684]
 [81.98287 ]
 [81.728714]
 [81.47551 ]
 [81.33178 ]], R is [[82.86212158]
 [82.94424438]
 [83.1147995 ]
 [83.28365326]
 [83.45082092]].
[2019-04-04 14:40:28,519] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8290879e-09 1.9409216e-10 3.0579840e-15 3.5326803e-14 1.0000000e+00
 4.6530522e-11 2.2129085e-15], sum to 1.0000
[2019-04-04 14:40:28,521] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9462
[2019-04-04 14:40:28,563] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.5, 73.33333333333333, 211.6666666666667, 85.33333333333331, 26.0, 25.71001834758398, 0.3167115605944042, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1941000.0000, 
sim time next is 1941600.0000, 
raw observation next is [-5.4, 71.66666666666667, 221.8333333333333, 47.66666666666666, 26.0, 25.71286407239533, 0.3160264205447753, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.31301939058171746, 0.7166666666666667, 0.7394444444444442, 0.05267034990791896, 0.6666666666666666, 0.6427386726996108, 0.6053421401815918, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.74609196], dtype=float32), 2.1669626]. 
=============================================
[2019-04-04 14:40:28,896] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7138821e-09 5.4912197e-10 1.0471523e-14 1.3033917e-13 1.0000000e+00
 6.8216921e-10 7.1392426e-15], sum to 1.0000
[2019-04-04 14:40:28,898] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3001
[2019-04-04 14:40:28,912] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 26.0, 24.44085769567548, 0.1984223459772979, 0.0, 1.0, 42429.08378087801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2160000.0000, 
sim time next is 2160600.0000, 
raw observation next is [-7.3, 81.50000000000001, 0.0, 0.0, 26.0, 24.39418549685038, 0.1886706064577456, 0.0, 1.0, 42462.76129256444], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.8150000000000002, 0.0, 0.0, 0.6666666666666666, 0.5328487914041983, 0.5628902021525819, 0.0, 1.0, 0.2022036252026878], 
reward next is 0.7978, 
noisyNet noise sample is [array([1.0405725], dtype=float32), -0.217319]. 
=============================================
[2019-04-04 14:40:30,940] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2552852e-09 4.3666550e-11 1.5530656e-16 3.9089690e-15 1.0000000e+00
 3.2261811e-11 7.0359861e-16], sum to 1.0000
[2019-04-04 14:40:30,940] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3668
[2019-04-04 14:40:30,987] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.683333333333334, 73.0, 0.0, 0.0, 26.0, 25.29265682018623, 0.364383668875694, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1969800.0000, 
sim time next is 1970400.0000, 
raw observation next is [-4.866666666666667, 75.0, 0.0, 0.0, 26.0, 25.3303561972156, 0.3386404175193345, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3277931671283472, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6108630164346334, 0.6128801391731115, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10218719], dtype=float32), 0.37357754]. 
=============================================
[2019-04-04 14:40:35,600] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1263539e-08 3.0085801e-09 5.5398320e-14 3.1607198e-13 1.0000000e+00
 3.3116483e-09 2.2303848e-14], sum to 1.0000
[2019-04-04 14:40:35,600] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3882
[2019-04-04 14:40:35,620] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.15917594651401, 0.05870308307156864, 0.0, 1.0, 41111.63088577902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2008200.0000, 
sim time next is 2008800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.13806017937945, 0.05771813087306146, 0.0, 1.0, 41132.93127149757], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5115050149482876, 0.5192393769576872, 0.0, 1.0, 0.19587110129284557], 
reward next is 0.8041, 
noisyNet noise sample is [array([-0.40316337], dtype=float32), -1.1536113]. 
=============================================
[2019-04-04 14:40:39,343] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8003992e-08 2.6582905e-09 3.7855115e-14 2.4254334e-13 1.0000000e+00
 5.6617488e-10 4.1981951e-14], sum to 1.0000
[2019-04-04 14:40:39,346] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4376
[2019-04-04 14:40:39,360] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.1813748656308, 0.07387266340140326, 0.0, 1.0, 41282.91976536193], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2002200.0000, 
sim time next is 2002800.0000, 
raw observation next is [-5.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.10572648304938, 0.0882830716154946, 0.0, 1.0, 41600.01590546433], 
processed observation next is [1.0, 0.17391304347826086, 0.30193905817174516, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5088105402541151, 0.5294276905384981, 0.0, 1.0, 0.19809531383554443], 
reward next is 0.8019, 
noisyNet noise sample is [array([1.7684358], dtype=float32), -0.55519336]. 
=============================================
[2019-04-04 14:40:40,667] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.0513465e-09 2.1772882e-11 1.5372756e-15 2.9878881e-14 1.0000000e+00
 7.1546646e-10 4.5322522e-15], sum to 1.0000
[2019-04-04 14:40:40,667] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3607
[2019-04-04 14:40:40,686] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.399999999999999, 85.33333333333334, 0.0, 0.0, 26.0, 25.03984600545368, 0.3172658479630137, 0.0, 1.0, 42170.21440662239], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2069400.0000, 
sim time next is 2070000.0000, 
raw observation next is [-4.5, 86.0, 0.0, 0.0, 26.0, 24.98085207279602, 0.306160140129588, 0.0, 1.0, 42287.44611559393], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5817376727330016, 0.602053380043196, 0.0, 1.0, 0.20136879102663774], 
reward next is 0.7986, 
noisyNet noise sample is [array([0.41340387], dtype=float32), 1.563745]. 
=============================================
[2019-04-04 14:40:40,690] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[81.30321]
 [81.44705]
 [81.53668]
 [81.53374]
 [81.6965 ]], R is [[81.14710236]
 [81.13481903]
 [81.12324524]
 [81.11112976]
 [81.09703827]].
[2019-04-04 14:40:51,153] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.8179171e-09 5.9895877e-10 1.4977809e-14 7.6592053e-14 1.0000000e+00
 1.0979282e-09 2.3790191e-14], sum to 1.0000
[2019-04-04 14:40:51,153] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9583
[2019-04-04 14:40:51,189] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.09447048694729, 0.3992791467493133, 0.0, 1.0, 109835.5440945107], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2146800.0000, 
sim time next is 2147400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.13660494440588, 0.4101295452474757, 0.0, 1.0, 73821.4702849791], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5947170787004902, 0.6367098484158252, 0.0, 1.0, 0.35153081088085286], 
reward next is 0.6485, 
noisyNet noise sample is [array([-0.05200425], dtype=float32), -1.4227061]. 
=============================================
[2019-04-04 14:41:11,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2757106e-08 3.1154355e-09 5.1797108e-14 2.8886469e-13 1.0000000e+00
 8.8636937e-10 3.0767540e-14], sum to 1.0000
[2019-04-04 14:41:11,347] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9712
[2019-04-04 14:41:11,361] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 49.0, 0.0, 0.0, 26.0, 24.97477623761025, 0.1691429071736763, 0.0, 1.0, 38367.70237698045], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2520000.0000, 
sim time next is 2520600.0000, 
raw observation next is [-1.8, 50.33333333333334, 0.0, 0.0, 26.0, 24.89582860530006, 0.1571427908531271, 0.0, 1.0, 38390.8314039379], 
processed observation next is [1.0, 0.17391304347826086, 0.41274238227146814, 0.5033333333333334, 0.0, 0.0, 0.6666666666666666, 0.574652383775005, 0.5523809302843757, 0.0, 1.0, 0.18281348287589474], 
reward next is 0.8172, 
noisyNet noise sample is [array([-0.6411799], dtype=float32), 0.28596365]. 
=============================================
[2019-04-04 14:41:21,931] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.5611909e-08 3.1640917e-09 6.7832851e-14 1.7232167e-12 9.9999988e-01
 4.3474038e-09 4.1212704e-13], sum to 1.0000
[2019-04-04 14:41:21,931] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6058
[2019-04-04 14:41:21,945] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.16666666666667, 83.0, 0.0, 0.0, 26.0, 23.27238229590573, -0.06870903711591075, 0.0, 1.0, 43825.26137628922], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2697000.0000, 
sim time next is 2697600.0000, 
raw observation next is [-15.33333333333334, 83.0, 0.0, 0.0, 26.0, 23.23840890537035, -0.07448604775231732, 0.0, 1.0, 43693.18876091257], 
processed observation next is [1.0, 0.21739130434782608, 0.03785780240073851, 0.83, 0.0, 0.0, 0.6666666666666666, 0.4365340754475291, 0.47517131741589425, 0.0, 1.0, 0.2080628036233932], 
reward next is 0.7919, 
noisyNet noise sample is [array([0.3817152], dtype=float32), 1.5908177]. 
=============================================
[2019-04-04 14:41:33,510] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4845819e-09 1.7180891e-10 1.0655548e-15 5.0327127e-15 1.0000000e+00
 3.1425931e-11 1.1501853e-15], sum to 1.0000
[2019-04-04 14:41:33,510] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8655
[2019-04-04 14:41:33,537] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 55.0, 163.0, 370.5, 26.0, 25.93284345819335, 0.4203376078668271, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2800800.0000, 
sim time next is 2801400.0000, 
raw observation next is [-2.666666666666667, 54.16666666666667, 166.6666666666667, 416.3333333333333, 26.0, 25.96479666712283, 0.433458179134489, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.38873499538319484, 0.5416666666666667, 0.5555555555555557, 0.460036832412523, 0.6666666666666666, 0.6637330555935691, 0.6444860597114963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2040892], dtype=float32), 0.56624585]. 
=============================================
[2019-04-04 14:41:38,043] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.2937323e-10 3.2641434e-10 7.9103793e-16 1.2033676e-14 1.0000000e+00
 4.4482826e-10 6.6000977e-16], sum to 1.0000
[2019-04-04 14:41:38,044] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3958
[2019-04-04 14:41:38,063] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.666666666666667, 60.66666666666667, 115.1666666666667, 788.3333333333333, 26.0, 26.37428804959073, 0.5997390865500463, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3324000.0000, 
sim time next is 3324600.0000, 
raw observation next is [-6.5, 59.0, 116.0, 798.0, 26.0, 26.36456948058525, 0.6019609693873563, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.28254847645429365, 0.59, 0.38666666666666666, 0.881767955801105, 0.6666666666666666, 0.6970474567154374, 0.700653656462452, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.77363306], dtype=float32), 0.5611459]. 
=============================================
[2019-04-04 14:41:43,241] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7736979e-08 3.0308858e-10 2.6060178e-14 3.2559678e-13 1.0000000e+00
 3.0540923e-10 5.6358902e-14], sum to 1.0000
[2019-04-04 14:41:43,242] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1611
[2019-04-04 14:41:43,262] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93079208640976, 0.2752204650914976, 0.0, 1.0, 38150.93490819046], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3021600.0000, 
sim time next is 3022200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93795231173934, 0.2698984712044595, 0.0, 1.0, 38075.94099414045], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5781626926449451, 0.5899661570681531, 0.0, 1.0, 0.18131400473400217], 
reward next is 0.8187, 
noisyNet noise sample is [array([0.85912955], dtype=float32), 0.38012353]. 
=============================================
[2019-04-04 14:41:46,386] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9385435e-09 3.9344708e-10 1.2112147e-15 1.8371007e-15 1.0000000e+00
 1.1720776e-10 1.2039410e-15], sum to 1.0000
[2019-04-04 14:41:46,388] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1287
[2019-04-04 14:41:46,399] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.833333333333333, 48.83333333333334, 111.0, 777.3333333333333, 26.0, 26.61396191741011, 0.6261011390720693, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3408600.0000, 
sim time next is 3409200.0000, 
raw observation next is [3.0, 49.0, 112.0, 784.0, 26.0, 26.62986718141547, 0.6194122282320811, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.49, 0.37333333333333335, 0.8662983425414365, 0.6666666666666666, 0.7191555984512892, 0.706470742744027, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29105365], dtype=float32), -1.2443218]. 
=============================================
[2019-04-04 14:41:48,548] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.7247110e-10 3.4905832e-11 4.6935591e-17 3.3355562e-15 1.0000000e+00
 4.9866861e-11 8.9953834e-17], sum to 1.0000
[2019-04-04 14:41:48,549] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8990
[2019-04-04 14:41:48,561] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.7, 86.0, 109.0, 752.0, 26.0, 26.41785331153989, 0.6985376307552676, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3234600.0000, 
sim time next is 3235200.0000, 
raw observation next is [-2.6, 84.0, 109.6666666666667, 761.8333333333334, 26.0, 26.4105361340922, 0.7062125509066614, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3905817174515236, 0.84, 0.3655555555555557, 0.841804788213628, 0.6666666666666666, 0.7008780111743501, 0.7354041836355538, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3044071], dtype=float32), -1.3355942]. 
=============================================
[2019-04-04 14:41:51,878] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.1970940e-10 5.8602394e-11 5.9237419e-16 4.4932095e-15 1.0000000e+00
 2.8915479e-12 1.1422661e-16], sum to 1.0000
[2019-04-04 14:41:51,883] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8288
[2019-04-04 14:41:51,935] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.833333333333333, 60.0, 58.66666666666667, 317.0, 26.0, 25.556929485663, 0.3807230920686687, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3399000.0000, 
sim time next is 3399600.0000, 
raw observation next is [-1.666666666666667, 60.00000000000001, 72.83333333333333, 369.5, 26.0, 25.49433436695532, 0.4223487155406116, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4164358264081256, 0.6000000000000001, 0.24277777777777776, 0.40828729281767956, 0.6666666666666666, 0.6245278639129435, 0.6407829051802039, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9481651], dtype=float32), 1.9234873]. 
=============================================
[2019-04-04 14:41:54,911] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.0337221e-09 6.8883732e-10 4.7215061e-15 6.3020451e-14 1.0000000e+00
 3.0593453e-10 1.6133572e-15], sum to 1.0000
[2019-04-04 14:41:54,913] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7127
[2019-04-04 14:41:54,924] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 54.0, 117.0, 804.5, 26.0, 26.35694764918392, 0.605474173640689, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3326400.0000, 
sim time next is 3327000.0000, 
raw observation next is [-5.833333333333333, 54.0, 117.3333333333333, 806.6666666666667, 26.0, 26.36093311890187, 0.6085762519899556, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.30101569713758086, 0.54, 0.391111111111111, 0.8913443830570903, 0.6666666666666666, 0.6967444265751558, 0.7028587506633185, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14983667], dtype=float32), -0.57460576]. 
=============================================
[2019-04-04 14:41:54,933] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.51109]
 [83.66625]
 [83.8224 ]
 [83.96903]
 [84.12222]], R is [[83.51191711]
 [83.67679596]
 [83.84002686]
 [84.00162506]
 [84.16160583]].
[2019-04-04 14:41:55,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0366827e-09 3.0061753e-10 1.5236027e-15 3.6659589e-14 1.0000000e+00
 1.5503682e-10 1.4804038e-15], sum to 1.0000
[2019-04-04 14:41:55,283] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2528
[2019-04-04 14:41:55,296] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.833333333333333, 54.0, 117.3333333333333, 806.6666666666667, 26.0, 26.35549883364219, 0.6079404770137338, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3327000.0000, 
sim time next is 3327600.0000, 
raw observation next is [-5.666666666666666, 54.00000000000001, 117.6666666666667, 808.8333333333334, 26.0, 26.36037144181418, 0.5897192294474812, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3056325023084026, 0.54, 0.3922222222222223, 0.8937384898710866, 0.6666666666666666, 0.6966976201511818, 0.6965730764824937, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24875355], dtype=float32), -1.2874979]. 
=============================================
[2019-04-04 14:41:59,847] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4950474e-09 2.2746043e-11 3.9753133e-16 3.3159426e-15 1.0000000e+00
 4.7968386e-11 2.3667607e-16], sum to 1.0000
[2019-04-04 14:41:59,848] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9071
[2019-04-04 14:41:59,863] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.35414748331831, 0.4752441141724841, 0.0, 1.0, 47020.69152333237], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3452400.0000, 
sim time next is 3453000.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.38803121960945, 0.4822456331040841, 0.0, 1.0, 34916.04669879612], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6156692683007874, 0.660748544368028, 0.0, 1.0, 0.16626688904188627], 
reward next is 0.8337, 
noisyNet noise sample is [array([-0.348091], dtype=float32), 0.1489118]. 
=============================================
[2019-04-04 14:41:59,875] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[86.476585]
 [86.44648 ]
 [86.10563 ]
 [85.94039 ]
 [85.006966]], R is [[86.63488007]
 [86.54462433]
 [86.40499878]
 [86.12837982]
 [85.60125732]].
[2019-04-04 14:42:07,029] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0663063e-09 3.6819697e-11 5.6673472e-17 2.0408833e-15 1.0000000e+00
 1.5939196e-11 1.6313409e-16], sum to 1.0000
[2019-04-04 14:42:07,033] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7823
[2019-04-04 14:42:07,044] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8333333333333334, 60.83333333333334, 111.0, 783.3333333333333, 26.0, 26.27458422646531, 0.5802662089988812, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3495000.0000, 
sim time next is 3495600.0000, 
raw observation next is [1.0, 61.0, 112.0, 790.0, 26.0, 26.17833164058857, 0.5841821870726277, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.61, 0.37333333333333335, 0.8729281767955801, 0.6666666666666666, 0.6815276367157143, 0.6947273956908759, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9410158], dtype=float32), -0.8508379]. 
=============================================
[2019-04-04 14:42:07,531] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5844197e-08 2.1583242e-09 1.0855483e-14 5.2753070e-14 1.0000000e+00
 2.1229170e-10 1.5712903e-14], sum to 1.0000
[2019-04-04 14:42:07,532] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3373
[2019-04-04 14:42:07,544] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 43.0, 74.5, 607.0, 26.0, 25.33882690286542, 0.4605474376399811, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3600000.0000, 
sim time next is 3600600.0000, 
raw observation next is [0.0, 42.33333333333334, 70.66666666666667, 576.3333333333333, 26.0, 25.32340037463254, 0.453173526146937, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.42333333333333345, 0.23555555555555557, 0.6368324125230201, 0.6666666666666666, 0.6102833645527118, 0.651057842048979, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9681293], dtype=float32), -0.83882153]. 
=============================================
[2019-04-04 14:42:10,178] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.2873326e-09 1.6630145e-09 1.5524926e-14 2.7952343e-15 1.0000000e+00
 2.7986824e-10 5.0768114e-15], sum to 1.0000
[2019-04-04 14:42:10,180] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8295
[2019-04-04 14:42:10,194] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.166666666666667, 38.0, 99.33333333333334, 766.6666666666666, 26.0, 26.98018598998344, 0.7812585428730809, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3941400.0000, 
sim time next is 3942000.0000, 
raw observation next is [-4.0, 38.0, 96.5, 756.0, 26.0, 27.02160653260623, 0.7899100600301866, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.38, 0.32166666666666666, 0.8353591160220994, 0.6666666666666666, 0.7518005443838526, 0.7633033533433955, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.93754184], dtype=float32), -1.2280111]. 
=============================================
[2019-04-04 14:42:10,211] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[81.09425]
 [81.34246]
 [81.52215]
 [81.7383 ]
 [81.89312]], R is [[81.07098389]
 [81.26027679]
 [81.44767761]
 [81.6332016 ]
 [81.81687164]].
[2019-04-04 14:42:20,519] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7655664e-09 3.7089179e-10 6.9658825e-16 3.1333216e-15 1.0000000e+00
 1.6621891e-10 1.6205618e-15], sum to 1.0000
[2019-04-04 14:42:20,521] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7433
[2019-04-04 14:42:20,532] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 60.00000000000001, 115.8333333333333, 814.1666666666666, 26.0, 26.51699572194358, 0.6314028220490843, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3842400.0000, 
sim time next is 3843000.0000, 
raw observation next is [-1.0, 60.0, 117.0, 822.0, 26.0, 26.49443814597374, 0.6394944352453261, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.39, 0.9082872928176795, 0.6666666666666666, 0.7078698454978115, 0.713164811748442, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1542045], dtype=float32), 1.4009305]. 
=============================================
[2019-04-04 14:42:20,537] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[87.96842 ]
 [88.025444]
 [88.08548 ]
 [88.129326]
 [88.18808 ]], R is [[88.08836365]
 [88.20748138]
 [88.32540894]
 [88.44215393]
 [88.55773163]].
[2019-04-04 14:42:23,219] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2528389e-08 3.7420234e-10 1.3924314e-14 1.5865354e-13 1.0000000e+00
 2.3711416e-10 2.4930385e-14], sum to 1.0000
[2019-04-04 14:42:23,220] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2425
[2019-04-04 14:42:23,241] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666666, 75.0, 0.0, 0.0, 26.0, 24.85858022561867, 0.2739122606169821, 0.0, 1.0, 43674.54385779059], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3822000.0000, 
sim time next is 3822600.0000, 
raw observation next is [-4.833333333333334, 76.0, 0.0, 0.0, 26.0, 24.87227140648984, 0.2725578837625163, 0.0, 1.0, 43550.39764636278], 
processed observation next is [1.0, 0.21739130434782608, 0.32871652816251157, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5726892838741534, 0.5908526279208387, 0.0, 1.0, 0.20738284593506084], 
reward next is 0.7926, 
noisyNet noise sample is [array([-1.5087527], dtype=float32), 0.16449738]. 
=============================================
[2019-04-04 14:42:26,051] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.8473283e-09 5.6493787e-09 2.2944923e-14 1.5510195e-13 1.0000000e+00
 1.2169520e-09 2.0491624e-13], sum to 1.0000
[2019-04-04 14:42:26,054] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4068
[2019-04-04 14:42:26,064] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 36.0, 198.0, 698.6666666666666, 26.0, 25.09889013524507, 0.4099479840386862, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4195200.0000, 
sim time next is 4195800.0000, 
raw observation next is [2.0, 37.0, 214.0, 669.0, 26.0, 25.10903761597983, 0.4116165600380292, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.37, 0.7133333333333334, 0.7392265193370166, 0.6666666666666666, 0.5924198013316525, 0.6372055200126764, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46774703], dtype=float32), -1.4479715]. 
=============================================
[2019-04-04 14:42:26,287] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9677683e-08 2.6421035e-10 2.1795399e-15 2.0054989e-14 1.0000000e+00
 3.2030108e-11 5.8158875e-15], sum to 1.0000
[2019-04-04 14:42:26,289] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6720
[2019-04-04 14:42:26,308] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.833333333333333, 76.0, 0.0, 0.0, 26.0, 25.2733003088744, 0.358512821537721, 0.0, 1.0, 41068.69101384516], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3905400.0000, 
sim time next is 3906000.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.23069874136627, 0.3539996754560874, 0.0, 1.0, 40978.76647763071], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6025582284471893, 0.6179998918186959, 0.0, 1.0, 0.1951369832268129], 
reward next is 0.8049, 
noisyNet noise sample is [array([0.86093307], dtype=float32), -0.505545]. 
=============================================
[2019-04-04 14:42:26,325] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.66558 ]
 [82.647705]
 [82.59997 ]
 [82.56495 ]
 [82.53898 ]], R is [[82.67162323]
 [82.6493454 ]
 [82.62542725]
 [82.59976196]
 [82.5922699 ]].
[2019-04-04 14:42:46,483] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 14:42:46,484] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 14:42:46,484] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:42:46,485] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 14:42:46,485] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:42:46,485] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 14:42:46,489] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:42:46,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run28
[2019-04-04 14:42:46,515] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run28
[2019-04-04 14:42:46,516] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run28
[2019-04-04 14:43:15,496] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17782865], dtype=float32), 0.21727452]
[2019-04-04 14:43:15,496] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-7.8, 71.5, 0.0, 0.0, 26.0, 24.04395281701684, 0.0707676632662108, 0.0, 1.0, 41523.18761648718]
[2019-04-04 14:43:15,496] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 14:43:15,497] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.8544193e-09 1.9791924e-09 2.5682247e-14 3.3609064e-13 1.0000000e+00
 7.6165657e-10 5.2349130e-14], sampled 0.19018959555366133
[2019-04-04 14:43:27,476] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17782865], dtype=float32), 0.21727452]
[2019-04-04 14:43:27,477] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.1176028665, 92.702156505, 3.935090136, 0.0, 26.0, 25.81393489473578, 0.4622688140330247, 1.0, 1.0, 0.0]
[2019-04-04 14:43:27,477] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 14:43:27,477] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.8271844e-10 3.2262856e-11 1.3672753e-16 1.8273226e-15 1.0000000e+00
 1.7544155e-11 2.1156750e-16], sampled 0.6643655229020281
[2019-04-04 14:43:30,214] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17782865], dtype=float32), 0.21727452]
[2019-04-04 14:43:30,214] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [7.616666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 25.4876243526711, 0.5596168090216117, 1.0, 1.0, 0.0]
[2019-04-04 14:43:30,214] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 14:43:30,215] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.6627750e-10 2.1857974e-11 8.0755069e-17 8.7052027e-16 1.0000000e+00
 1.4688273e-11 9.2683675e-17], sampled 0.12150634530573534
[2019-04-04 14:44:27,200] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.2337 239980922.8040 1605.1966
[2019-04-04 14:44:46,721] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7242.0801 263323169.0646 1552.1606
[2019-04-04 14:44:49,237] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.8452 275762507.4591 1233.0324
[2019-04-04 14:44:50,260] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 2700000, evaluation results [2700000.0, 7242.080147311378, 263323169.06460783, 1552.1605816745282, 7353.233700933292, 239980922.80401036, 1605.1966122477527, 7182.845202575783, 275762507.4590914, 1233.0323884866036]
[2019-04-04 14:44:51,247] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2627533e-10 3.3296706e-11 2.0937643e-16 3.5301525e-15 1.0000000e+00
 3.3720245e-11 5.1402983e-16], sum to 1.0000
[2019-04-04 14:44:51,248] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2841
[2019-04-04 14:44:51,260] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.350000000000001, 73.0, 0.0, 0.0, 26.0, 25.5777582329219, 0.471558924589512, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4306200.0000, 
sim time next is 4306800.0000, 
raw observation next is [5.300000000000001, 73.0, 0.0, 0.0, 26.0, 25.73193123160505, 0.4773722078536491, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.6094182825484765, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6443276026337541, 0.6591240692845497, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.88067037], dtype=float32), 1.3343301]. 
=============================================
[2019-04-04 14:44:52,984] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0033804e-10 2.0265768e-12 4.5647089e-18 1.9459818e-17 1.0000000e+00
 4.3779945e-12 2.6643188e-18], sum to 1.0000
[2019-04-04 14:44:52,984] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4439
[2019-04-04 14:44:52,995] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 31.0, 120.0, 828.0, 26.0, 27.85674645113373, 0.9431463848533054, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4361400.0000, 
sim time next is 4362000.0000, 
raw observation next is [14.2, 30.0, 119.6666666666667, 832.1666666666666, 26.0, 27.95664068744026, 0.9694612540946957, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8559556786703602, 0.3, 0.398888888888889, 0.9195211786372007, 0.6666666666666666, 0.8297200572866883, 0.8231537513648987, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3577876], dtype=float32), -0.8948937]. 
=============================================
[2019-04-04 14:44:53,007] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[94.70293 ]
 [94.57797 ]
 [94.53651 ]
 [94.529526]
 [94.5459  ]], R is [[94.91860962]
 [94.96942139]
 [95.01972961]
 [95.0695343 ]
 [95.11884308]].
[2019-04-04 14:44:56,590] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7521426e-10 1.9734570e-11 3.7802697e-16 6.6666475e-16 1.0000000e+00
 7.8342090e-12 2.8113913e-16], sum to 1.0000
[2019-04-04 14:44:56,593] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8323
[2019-04-04 14:44:56,613] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.866666666666667, 43.5, 143.0, 141.0, 26.0, 26.97105969436376, 0.8104167131591296, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4637400.0000, 
sim time next is 4638000.0000, 
raw observation next is [5.733333333333333, 44.0, 130.0, 144.0, 26.0, 26.27162843429451, 0.7455275744500701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6214219759926132, 0.44, 0.43333333333333335, 0.1591160220994475, 0.6666666666666666, 0.6893023695245425, 0.7485091914833567, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9489597], dtype=float32), 0.96243674]. 
=============================================
[2019-04-04 14:44:56,618] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[88.770874]
 [89.08106 ]
 [89.25098 ]
 [89.41516 ]
 [89.70229 ]], R is [[88.4933548 ]
 [88.60842133]
 [88.72233582]
 [88.83511353]
 [88.94676208]].
[2019-04-04 14:45:07,608] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.71079295e-10 2.95901581e-10 1.05459149e-16 2.03618631e-15
 1.00000000e+00 1.11924074e-10 2.08218444e-15], sum to 1.0000
[2019-04-04 14:45:07,618] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5968
[2019-04-04 14:45:07,632] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 92.0, 0.0, 0.0, 26.0, 25.42255172835382, 0.4733608044964557, 0.0, 1.0, 71626.06241159666], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4680000.0000, 
sim time next is 4680600.0000, 
raw observation next is [-0.1666666666666667, 93.33333333333334, 0.0, 0.0, 26.0, 25.5626617792319, 0.4781399148857495, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.4579870729455217, 0.9333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6302218149359916, 0.6593799716285832, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.695822], dtype=float32), 1.0608503]. 
=============================================
[2019-04-04 14:45:09,348] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.4691033e-10 2.4229774e-10 4.9739257e-16 3.4976220e-14 1.0000000e+00
 1.5678807e-10 2.1830169e-16], sum to 1.0000
[2019-04-04 14:45:09,349] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7754
[2019-04-04 14:45:09,363] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 56.16666666666666, 0.0, 0.0, 26.0, 25.49761147809824, 0.5453209169046432, 0.0, 1.0, 22952.24736934149], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4662600.0000, 
sim time next is 4663200.0000, 
raw observation next is [2.0, 55.33333333333334, 0.0, 0.0, 26.0, 25.57427727835083, 0.5489877715870385, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.5533333333333335, 0.0, 0.0, 0.6666666666666666, 0.6311897731959025, 0.6829959238623462, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.95532924], dtype=float32), -0.07767668]. 
=============================================
[2019-04-04 14:45:10,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:10,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:10,058] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run21
[2019-04-04 14:45:12,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:12,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:12,511] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run21
[2019-04-04 14:45:14,074] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:14,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:14,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run21
[2019-04-04 14:45:14,273] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.3085133e-10 1.8884784e-10 9.9051186e-17 5.1904282e-15 1.0000000e+00
 7.1583429e-12 1.8628658e-15], sum to 1.0000
[2019-04-04 14:45:14,274] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7410
[2019-04-04 14:45:14,315] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.666666666666667, 58.66666666666667, 156.5, 678.5, 26.0, 25.47514339907931, 0.4567313617861095, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4789200.0000, 
sim time next is 4789800.0000, 
raw observation next is [-2.5, 55.5, 153.0, 730.0, 26.0, 25.42339568801107, 0.4520144503642296, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.39335180055401664, 0.555, 0.51, 0.8066298342541437, 0.6666666666666666, 0.6186163073342558, 0.6506714834547432, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.98791116], dtype=float32), -0.65913427]. 
=============================================
[2019-04-04 14:45:19,265] A3C_AGENT_WORKER-Thread-13 INFO:Local step 170000, global step 2714788: loss 0.1138
[2019-04-04 14:45:19,266] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 170000, global step 2714788: learning rate 0.0000
[2019-04-04 14:45:20,196] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:20,196] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:20,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run21
[2019-04-04 14:45:21,573] A3C_AGENT_WORKER-Thread-17 INFO:Local step 170000, global step 2715966: loss 0.1090
[2019-04-04 14:45:21,573] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 170000, global step 2715966: learning rate 0.0000
[2019-04-04 14:45:21,914] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3656772e-09 1.6129552e-10 4.6156149e-16 6.3596157e-15 1.0000000e+00
 2.4861901e-11 3.7484503e-15], sum to 1.0000
[2019-04-04 14:45:21,914] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2412
[2019-04-04 14:45:21,958] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 43.66666666666667, 77.5, 466.6666666666667, 26.0, 25.11934407536977, 0.2968361437051864, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4954800.0000, 
sim time next is 4955400.0000, 
raw observation next is [-1.5, 42.5, 93.0, 560.0, 26.0, 25.23826389180049, 0.329059180771462, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4210526315789474, 0.425, 0.31, 0.6187845303867403, 0.6666666666666666, 0.6031886576500408, 0.6096863935904874, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2429469], dtype=float32), 1.2177244]. 
=============================================
[2019-04-04 14:45:23,072] A3C_AGENT_WORKER-Thread-2 INFO:Local step 170000, global step 2716697: loss 0.1208
[2019-04-04 14:45:23,072] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 170000, global step 2716698: learning rate 0.0000
[2019-04-04 14:45:25,467] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:25,467] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:25,471] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run21
[2019-04-04 14:45:26,090] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:26,090] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:26,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run21
[2019-04-04 14:45:26,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:26,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:26,500] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run21
[2019-04-04 14:45:27,520] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:27,522] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:27,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run21
[2019-04-04 14:45:28,282] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:28,283] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:28,291] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run21
[2019-04-04 14:45:28,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:28,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:28,755] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run21
[2019-04-04 14:45:29,244] A3C_AGENT_WORKER-Thread-11 INFO:Local step 170000, global step 2719382: loss 0.1284
[2019-04-04 14:45:29,244] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 170000, global step 2719382: learning rate 0.0000
[2019-04-04 14:45:29,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:29,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:29,294] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run21
[2019-04-04 14:45:29,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:29,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:29,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run21
[2019-04-04 14:45:29,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:29,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:29,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run21
[2019-04-04 14:45:30,795] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:30,795] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:30,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run21
[2019-04-04 14:45:31,185] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3267896e-09 1.6869292e-10 4.2708799e-16 2.0781940e-14 1.0000000e+00
 6.4452804e-11 3.6471873e-16], sum to 1.0000
[2019-04-04 14:45:31,185] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2693
[2019-04-04 14:45:31,281] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.4, 89.0, 0.0, 0.0, 26.0, 24.57999953291704, 0.2028443548394402, 0.0, 1.0, 40478.05818483495], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 64800.0000, 
sim time next is 65400.0000, 
raw observation next is [4.300000000000001, 88.5, 0.0, 0.0, 26.0, 24.59115667896937, 0.2041710029799683, 0.0, 1.0, 36775.72272069693], 
processed observation next is [0.0, 0.782608695652174, 0.5817174515235458, 0.885, 0.0, 0.0, 0.6666666666666666, 0.5492630565807808, 0.5680570009933228, 0.0, 1.0, 0.17512248914617587], 
reward next is 0.8249, 
noisyNet noise sample is [array([-0.28402945], dtype=float32), 1.4817228]. 
=============================================
[2019-04-04 14:45:31,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:31,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:31,652] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run21
[2019-04-04 14:45:34,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:45:34,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:45:34,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run21
[2019-04-04 14:45:36,466] A3C_AGENT_WORKER-Thread-16 INFO:Local step 170000, global step 2720467: loss 0.1223
[2019-04-04 14:45:36,467] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 170000, global step 2720469: learning rate 0.0000
[2019-04-04 14:45:37,006] A3C_AGENT_WORKER-Thread-15 INFO:Local step 170000, global step 2720582: loss 0.1225
[2019-04-04 14:45:37,007] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 170000, global step 2720582: learning rate 0.0000
[2019-04-04 14:45:37,534] A3C_AGENT_WORKER-Thread-14 INFO:Local step 170000, global step 2720712: loss 0.1177
[2019-04-04 14:45:37,557] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 170000, global step 2720712: learning rate 0.0000
[2019-04-04 14:45:37,982] A3C_AGENT_WORKER-Thread-19 INFO:Local step 170000, global step 2720815: loss 0.1150
[2019-04-04 14:45:37,983] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 170000, global step 2720815: learning rate 0.0000
[2019-04-04 14:45:38,751] A3C_AGENT_WORKER-Thread-4 INFO:Local step 170000, global step 2721007: loss 0.1364
[2019-04-04 14:45:38,752] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 170000, global step 2721007: learning rate 0.0000
[2019-04-04 14:45:39,585] A3C_AGENT_WORKER-Thread-18 INFO:Local step 170000, global step 2721251: loss 0.1267
[2019-04-04 14:45:39,586] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 170000, global step 2721251: learning rate 0.0000
[2019-04-04 14:45:40,311] A3C_AGENT_WORKER-Thread-20 INFO:Local step 170000, global step 2721486: loss 0.1189
[2019-04-04 14:45:40,318] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 170000, global step 2721487: learning rate 0.0000
[2019-04-04 14:45:40,708] A3C_AGENT_WORKER-Thread-5 INFO:Local step 170000, global step 2721626: loss 0.1292
[2019-04-04 14:45:40,709] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 170000, global step 2721626: learning rate 0.0000
[2019-04-04 14:45:40,806] A3C_AGENT_WORKER-Thread-3 INFO:Local step 170000, global step 2721656: loss 0.1213
[2019-04-04 14:45:40,807] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 170000, global step 2721656: learning rate 0.0000
[2019-04-04 14:45:41,539] A3C_AGENT_WORKER-Thread-12 INFO:Local step 170000, global step 2721933: loss 0.1145
[2019-04-04 14:45:41,553] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 170000, global step 2721940: learning rate 0.0000
[2019-04-04 14:45:41,988] A3C_AGENT_WORKER-Thread-6 INFO:Local step 170000, global step 2722096: loss 0.1182
[2019-04-04 14:45:41,988] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 170000, global step 2722096: learning rate 0.0000
[2019-04-04 14:45:42,281] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5021362e-07 1.1897960e-08 7.4198921e-13 1.6998992e-11 9.9999976e-01
 1.7469915e-08 2.2536925e-12], sum to 1.0000
[2019-04-04 14:45:42,281] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1494
[2019-04-04 14:45:42,383] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.38333333333333, 67.5, 0.0, 0.0, 26.0, 22.49661687272651, -0.2951965512578943, 0.0, 1.0, 48023.19587564413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 285000.0000, 
sim time next is 285600.0000, 
raw observation next is [-12.46666666666667, 68.0, 0.0, 0.0, 26.0, 22.41811869162819, -0.2329287965690524, 1.0, 1.0, 202283.7440718212], 
processed observation next is [1.0, 0.30434782608695654, 0.11726685133887339, 0.68, 0.0, 0.0, 0.6666666666666666, 0.3681765576356826, 0.4223570678103159, 1.0, 1.0, 0.9632559241515295], 
reward next is 0.0367, 
noisyNet noise sample is [array([0.1347129], dtype=float32), -0.027741699]. 
=============================================
[2019-04-04 14:45:43,584] A3C_AGENT_WORKER-Thread-13 INFO:Local step 170500, global step 2722618: loss 0.7357
[2019-04-04 14:45:43,586] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 170500, global step 2722619: learning rate 0.0000
[2019-04-04 14:45:44,776] A3C_AGENT_WORKER-Thread-10 INFO:Local step 170000, global step 2723052: loss 0.1245
[2019-04-04 14:45:44,776] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 170000, global step 2723052: learning rate 0.0000
[2019-04-04 14:45:46,555] A3C_AGENT_WORKER-Thread-17 INFO:Local step 170500, global step 2723538: loss 0.7138
[2019-04-04 14:45:46,556] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 170500, global step 2723538: learning rate 0.0000
[2019-04-04 14:45:48,292] A3C_AGENT_WORKER-Thread-2 INFO:Local step 170500, global step 2724170: loss 0.6771
[2019-04-04 14:45:48,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 170500, global step 2724170: learning rate 0.0000
[2019-04-04 14:45:49,215] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6020461e-09 6.7807739e-11 4.0267807e-15 7.8016901e-15 1.0000000e+00
 2.7216712e-10 4.3159213e-15], sum to 1.0000
[2019-04-04 14:45:49,215] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2728
[2019-04-04 14:45:49,275] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 61.0, 145.0, 231.9999999999999, 26.0, 25.92748478462654, 0.4454476025629543, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 137400.0000, 
sim time next is 138000.0000, 
raw observation next is [-6.700000000000001, 61.0, 146.5, 169.0, 26.0, 25.90419524732235, 0.4345709921047169, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.48833333333333334, 0.1867403314917127, 0.6666666666666666, 0.6586829372768624, 0.644856997368239, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26544783], dtype=float32), -1.1089398]. 
=============================================
[2019-04-04 14:45:49,282] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.30423]
 [82.89913]
 [83.47917]
 [84.05312]
 [84.58379]], R is [[81.79666901]
 [81.97870636]
 [82.15892029]
 [82.33733368]
 [82.51396179]].
[2019-04-04 14:45:52,100] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2581760e-09 1.2193724e-10 6.4928451e-15 1.8812842e-14 1.0000000e+00
 4.9414028e-10 3.0542068e-15], sum to 1.0000
[2019-04-04 14:45:52,100] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1689
[2019-04-04 14:45:52,175] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.7, 61.0, 104.5, 75.5, 26.0, 25.60732300925154, 0.3894063847939735, 1.0, 1.0, 154356.3589756935], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 140400.0000, 
sim time next is 141000.0000, 
raw observation next is [-6.700000000000001, 61.5, 90.0, 65.33333333333333, 26.0, 25.574636971764, 0.3110214327191965, 1.0, 1.0, 100843.9331224681], 
processed observation next is [1.0, 0.6521739130434783, 0.2770083102493075, 0.615, 0.3, 0.0721915285451197, 0.6666666666666666, 0.6312197476470001, 0.6036738109063988, 1.0, 1.0, 0.48020920534508615], 
reward next is 0.5198, 
noisyNet noise sample is [array([0.49957228], dtype=float32), -0.7646754]. 
=============================================
[2019-04-04 14:45:52,183] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[80.10314]
 [79.80483]
 [80.38723]
 [80.99786]
 [81.65304]], R is [[79.88246155]
 [79.34860992]
 [79.55512238]
 [79.75957489]
 [79.96198273]].
[2019-04-04 14:45:53,073] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6969927e-08 6.9037531e-09 3.7055041e-13 7.1974674e-13 1.0000000e+00
 4.4841313e-09 3.1945391e-13], sum to 1.0000
[2019-04-04 14:45:53,073] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2754
[2019-04-04 14:45:53,120] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.1, 42.83333333333334, 0.0, 0.0, 26.0, 25.12043398854481, 0.2924124076657958, 0.0, 1.0, 55038.9237931314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 418200.0000, 
sim time next is 418800.0000, 
raw observation next is [-10.2, 43.66666666666667, 0.0, 0.0, 26.0, 25.11322632132154, 0.2823050429580097, 0.0, 1.0, 38238.00850748898], 
processed observation next is [1.0, 0.8695652173913043, 0.1800554016620499, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5927688601101284, 0.5941016809860032, 0.0, 1.0, 0.18208575479756656], 
reward next is 0.8179, 
noisyNet noise sample is [array([0.590376], dtype=float32), -0.58599204]. 
=============================================
[2019-04-04 14:45:54,634] A3C_AGENT_WORKER-Thread-11 INFO:Local step 170500, global step 2726107: loss 0.6705
[2019-04-04 14:45:54,636] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 170500, global step 2726107: learning rate 0.0000
[2019-04-04 14:46:01,756] A3C_AGENT_WORKER-Thread-15 INFO:Local step 170500, global step 2728370: loss 0.6208
[2019-04-04 14:46:01,756] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 170500, global step 2728370: learning rate 0.0000
[2019-04-04 14:46:01,828] A3C_AGENT_WORKER-Thread-16 INFO:Local step 170500, global step 2728394: loss 0.6410
[2019-04-04 14:46:01,828] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 170500, global step 2728394: learning rate 0.0000
[2019-04-04 14:46:01,942] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.5135250e-09 2.4803323e-10 1.6599427e-14 6.2929886e-14 1.0000000e+00
 1.0977398e-09 5.2850125e-15], sum to 1.0000
[2019-04-04 14:46:01,945] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9500
[2019-04-04 14:46:01,984] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 43.66666666666667, 86.33333333333333, 625.6666666666666, 26.0, 26.44105541154628, 0.5277560613474844, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 310200.0000, 
sim time next is 310800.0000, 
raw observation next is [-9.5, 43.33333333333334, 84.16666666666667, 624.3333333333334, 26.0, 26.38762331507234, 0.5203601362479456, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4333333333333334, 0.28055555555555556, 0.6898710865561695, 0.6666666666666666, 0.6989686095893616, 0.6734533787493152, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5153586], dtype=float32), -0.49489453]. 
=============================================
[2019-04-04 14:46:02,561] A3C_AGENT_WORKER-Thread-14 INFO:Local step 170500, global step 2728674: loss 0.6397
[2019-04-04 14:46:02,562] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 170500, global step 2728674: learning rate 0.0000
[2019-04-04 14:46:02,990] A3C_AGENT_WORKER-Thread-19 INFO:Local step 170500, global step 2728846: loss 0.6309
[2019-04-04 14:46:02,993] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 170500, global step 2728846: learning rate 0.0000
[2019-04-04 14:46:03,601] A3C_AGENT_WORKER-Thread-4 INFO:Local step 170500, global step 2729070: loss 0.6322
[2019-04-04 14:46:03,602] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 170500, global step 2729070: learning rate 0.0000
[2019-04-04 14:46:04,201] A3C_AGENT_WORKER-Thread-18 INFO:Local step 170500, global step 2729256: loss 0.6350
[2019-04-04 14:46:04,204] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 170500, global step 2729256: learning rate 0.0000
[2019-04-04 14:46:04,458] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7699769e-08 2.1009485e-09 4.0221292e-15 3.8702174e-13 1.0000000e+00
 7.2691642e-10 6.6319516e-15], sum to 1.0000
[2019-04-04 14:46:04,463] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2330
[2019-04-04 14:46:04,505] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.3, 75.0, 0.0, 0.0, 26.0, 24.13657741827112, 0.06734081222012471, 0.0, 1.0, 43531.62785442648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 618000.0000, 
sim time next is 618600.0000, 
raw observation next is [-4.399999999999999, 75.0, 0.0, 0.0, 26.0, 24.08503548858216, 0.05779724294182507, 0.0, 1.0, 43748.53409306304], 
processed observation next is [0.0, 0.13043478260869565, 0.3407202216066483, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5070862907151801, 0.519265747647275, 0.0, 1.0, 0.20832635282410972], 
reward next is 0.7917, 
noisyNet noise sample is [array([-0.33096176], dtype=float32), -0.4078958]. 
=============================================
[2019-04-04 14:46:04,974] A3C_AGENT_WORKER-Thread-20 INFO:Local step 170500, global step 2729499: loss 0.6358
[2019-04-04 14:46:04,977] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 170500, global step 2729499: learning rate 0.0000
[2019-04-04 14:46:05,174] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.1895302e-08 2.5613248e-08 8.0757379e-13 1.5041472e-11 9.9999988e-01
 1.9806471e-08 1.7456391e-12], sum to 1.0000
[2019-04-04 14:46:05,175] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3352
[2019-04-04 14:46:05,242] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-16.53333333333333, 80.0, 0.0, 0.0, 26.0, 22.68914621317499, -0.2505289343859654, 1.0, 1.0, 169295.3726917159], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 373200.0000, 
sim time next is 373800.0000, 
raw observation next is [-16.61666666666667, 80.5, 8.333333333333332, 192.3333333333333, 26.0, 23.0292743874449, -0.1737174884420432, 1.0, 1.0, 117085.9982290762], 
processed observation next is [1.0, 0.30434782608695654, 0.0023084025854107648, 0.805, 0.027777777777777773, 0.21252302025782682, 0.6666666666666666, 0.4191061989537417, 0.44209417051931893, 1.0, 1.0, 0.5575523725194105], 
reward next is 0.4424, 
noisyNet noise sample is [array([-0.9989962], dtype=float32), -0.36520737]. 
=============================================
[2019-04-04 14:46:05,511] A3C_AGENT_WORKER-Thread-5 INFO:Local step 170500, global step 2729675: loss 0.5942
[2019-04-04 14:46:05,513] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 170500, global step 2729677: learning rate 0.0000
[2019-04-04 14:46:05,589] A3C_AGENT_WORKER-Thread-3 INFO:Local step 170500, global step 2729700: loss 0.6029
[2019-04-04 14:46:05,589] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 170500, global step 2729700: learning rate 0.0000
[2019-04-04 14:46:05,955] A3C_AGENT_WORKER-Thread-12 INFO:Local step 170500, global step 2729817: loss 0.6042
[2019-04-04 14:46:05,956] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 170500, global step 2729817: learning rate 0.0000
[2019-04-04 14:46:06,914] A3C_AGENT_WORKER-Thread-13 INFO:Local step 171000, global step 2730124: loss 0.1451
[2019-04-04 14:46:06,916] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 171000, global step 2730124: learning rate 0.0000
[2019-04-04 14:46:07,165] A3C_AGENT_WORKER-Thread-6 INFO:Local step 170500, global step 2730191: loss 0.5908
[2019-04-04 14:46:07,171] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 170500, global step 2730191: learning rate 0.0000
[2019-04-04 14:46:09,286] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2986306e-08 9.0487434e-10 6.9776175e-14 7.4370767e-14 1.0000000e+00
 1.7447599e-09 1.4021109e-14], sum to 1.0000
[2019-04-04 14:46:09,287] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9942
[2019-04-04 14:46:09,346] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.88333333333333, 51.0, 57.0, 899.0, 26.0, 25.77000911030338, 0.2908828282801943, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 391800.0000, 
sim time next is 392400.0000, 
raw observation next is [-11.7, 51.0, 56.5, 896.0, 26.0, 25.86466956688175, 0.4067851378285785, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.13850415512465375, 0.51, 0.18833333333333332, 0.9900552486187846, 0.6666666666666666, 0.6553891305734793, 0.6355950459428595, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.55830383], dtype=float32), -2.2421603]. 
=============================================
[2019-04-04 14:46:09,795] A3C_AGENT_WORKER-Thread-10 INFO:Local step 170500, global step 2730995: loss 0.5696
[2019-04-04 14:46:09,795] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 170500, global step 2730995: learning rate 0.0000
[2019-04-04 14:46:10,040] A3C_AGENT_WORKER-Thread-17 INFO:Local step 171000, global step 2731093: loss 0.1471
[2019-04-04 14:46:10,041] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 171000, global step 2731093: learning rate 0.0000
[2019-04-04 14:46:12,122] A3C_AGENT_WORKER-Thread-2 INFO:Local step 171000, global step 2731782: loss 0.1443
[2019-04-04 14:46:12,123] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 171000, global step 2731782: learning rate 0.0000
[2019-04-04 14:46:13,336] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5199231e-08 1.5802358e-09 3.0889259e-14 9.6567329e-13 1.0000000e+00
 9.6612429e-10 5.0094816e-14], sum to 1.0000
[2019-04-04 14:46:13,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1448
[2019-04-04 14:46:13,376] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.45, 26.5, 129.0, 0.0, 26.0, 25.16601169557941, 0.154144857398555, 1.0, 1.0, 19878.01638510948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 477000.0000, 
sim time next is 477600.0000, 
raw observation next is [-1.366666666666667, 27.0, 127.3333333333333, 0.0, 26.0, 25.11029177217221, 0.1441641448079572, 1.0, 1.0, 51081.78628941248], 
processed observation next is [1.0, 0.5217391304347826, 0.42474607571560485, 0.27, 0.42444444444444435, 0.0, 0.6666666666666666, 0.5925243143476843, 0.5480547149359857, 1.0, 1.0, 0.24324660137815465], 
reward next is 0.7568, 
noisyNet noise sample is [array([-0.3175867], dtype=float32), 0.27595115]. 
=============================================
[2019-04-04 14:46:17,548] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7850867e-09 9.5583756e-11 3.0006897e-16 1.6089913e-14 1.0000000e+00
 1.9718241e-11 4.3877998e-16], sum to 1.0000
[2019-04-04 14:46:17,551] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6996
[2019-04-04 14:46:17,562] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.433333333333334, 84.66666666666667, 0.0, 0.0, 26.0, 24.74927466293748, 0.2180913858887323, 0.0, 1.0, 39917.03401161416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 530400.0000, 
sim time next is 531000.0000, 
raw observation next is [3.25, 84.0, 0.0, 0.0, 26.0, 24.73011230334215, 0.2146498659054199, 0.0, 1.0, 39971.88991511817], 
processed observation next is [0.0, 0.13043478260869565, 0.5526315789473685, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5608426919451791, 0.5715499553018066, 0.0, 1.0, 0.19034233292913413], 
reward next is 0.8097, 
noisyNet noise sample is [array([-1.6529146], dtype=float32), 0.11517406]. 
=============================================
[2019-04-04 14:46:17,575] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[88.90588 ]
 [89.08156 ]
 [88.885635]
 [88.89545 ]
 [88.887024]], R is [[88.65648651]
 [88.57984161]
 [88.50421143]
 [88.4295578 ]
 [88.35585785]].
[2019-04-04 14:46:18,429] A3C_AGENT_WORKER-Thread-11 INFO:Local step 171000, global step 2734047: loss 0.1566
[2019-04-04 14:46:18,430] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 171000, global step 2734048: learning rate 0.0000
[2019-04-04 14:46:18,723] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.24770677e-09 3.44937856e-10 3.29836694e-16 1.21355545e-14
 1.00000000e+00 1.05846790e-10 6.85134934e-16], sum to 1.0000
[2019-04-04 14:46:18,725] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4966
[2019-04-04 14:46:18,751] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 24.51839323486541, 0.1588533401385972, 0.0, 1.0, 40949.57928230616], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 544200.0000, 
sim time next is 544800.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 24.45549891165187, 0.1509735637973198, 0.0, 1.0, 41014.5215870986], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5379582426376558, 0.5503245212657732, 0.0, 1.0, 0.1953072456528505], 
reward next is 0.8047, 
noisyNet noise sample is [array([0.13146566], dtype=float32), 1.3742577]. 
=============================================
[2019-04-04 14:46:24,329] A3C_AGENT_WORKER-Thread-16 INFO:Local step 171000, global step 2736239: loss 0.1282
[2019-04-04 14:46:24,330] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 171000, global step 2736239: learning rate 0.0000
[2019-04-04 14:46:24,895] A3C_AGENT_WORKER-Thread-15 INFO:Local step 171000, global step 2736427: loss 0.1474
[2019-04-04 14:46:24,895] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 171000, global step 2736427: learning rate 0.0000
[2019-04-04 14:46:25,779] A3C_AGENT_WORKER-Thread-14 INFO:Local step 171000, global step 2736744: loss 0.1419
[2019-04-04 14:46:25,783] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 171000, global step 2736745: learning rate 0.0000
[2019-04-04 14:46:26,053] A3C_AGENT_WORKER-Thread-19 INFO:Local step 171000, global step 2736838: loss 0.1419
[2019-04-04 14:46:26,056] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 171000, global step 2736840: learning rate 0.0000
[2019-04-04 14:46:26,729] A3C_AGENT_WORKER-Thread-4 INFO:Local step 171000, global step 2737118: loss 0.1337
[2019-04-04 14:46:26,730] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 171000, global step 2737118: learning rate 0.0000
[2019-04-04 14:46:27,178] A3C_AGENT_WORKER-Thread-18 INFO:Local step 171000, global step 2737297: loss 0.1398
[2019-04-04 14:46:27,178] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 171000, global step 2737297: learning rate 0.0000
[2019-04-04 14:46:28,051] A3C_AGENT_WORKER-Thread-13 INFO:Local step 171500, global step 2737617: loss 0.1201
[2019-04-04 14:46:28,052] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 171500, global step 2737617: learning rate 0.0000
[2019-04-04 14:46:28,365] A3C_AGENT_WORKER-Thread-20 INFO:Local step 171000, global step 2737739: loss 0.1372
[2019-04-04 14:46:28,366] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 171000, global step 2737739: learning rate 0.0000
[2019-04-04 14:46:28,514] A3C_AGENT_WORKER-Thread-3 INFO:Local step 171000, global step 2737797: loss 0.1359
[2019-04-04 14:46:28,515] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 171000, global step 2737798: learning rate 0.0000
[2019-04-04 14:46:28,799] A3C_AGENT_WORKER-Thread-5 INFO:Local step 171000, global step 2737912: loss 0.1314
[2019-04-04 14:46:28,799] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 171000, global step 2737912: learning rate 0.0000
[2019-04-04 14:46:29,090] A3C_AGENT_WORKER-Thread-12 INFO:Local step 171000, global step 2738026: loss 0.1369
[2019-04-04 14:46:29,092] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 171000, global step 2738026: learning rate 0.0000
[2019-04-04 14:46:29,177] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.60609079e-09 3.81191656e-09 1.03032464e-13 3.98670138e-13
 1.00000000e+00 1.23720589e-09 8.53968724e-14], sum to 1.0000
[2019-04-04 14:46:29,178] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3833
[2019-04-04 14:46:29,236] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.86734074461355, 0.2034632942151095, 0.0, 1.0, 55076.40537130948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 669600.0000, 
sim time next is 670200.0000, 
raw observation next is [-1.383333333333333, 57.83333333333333, 0.0, 0.0, 26.0, 24.86993804982509, 0.2055638452849303, 0.0, 1.0, 51417.05721686484], 
processed observation next is [0.0, 0.782608695652174, 0.4242843951985227, 0.5783333333333333, 0.0, 0.0, 0.6666666666666666, 0.5724948374854243, 0.5685212817616434, 0.0, 1.0, 0.2448431296041183], 
reward next is 0.7552, 
noisyNet noise sample is [array([0.43105578], dtype=float32), 0.47092715]. 
=============================================
[2019-04-04 14:46:29,733] A3C_AGENT_WORKER-Thread-6 INFO:Local step 171000, global step 2738267: loss 0.1336
[2019-04-04 14:46:29,735] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 171000, global step 2738269: learning rate 0.0000
[2019-04-04 14:46:31,189] A3C_AGENT_WORKER-Thread-17 INFO:Local step 171500, global step 2738882: loss 0.1203
[2019-04-04 14:46:31,190] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 171500, global step 2738883: learning rate 0.0000
[2019-04-04 14:46:31,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.7608995e-09 1.6130353e-10 2.6021486e-14 2.8038875e-13 1.0000000e+00
 6.3240213e-10 1.5245453e-14], sum to 1.0000
[2019-04-04 14:46:31,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2967
[2019-04-04 14:46:31,861] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 72.5, 0.0, 0.0, 26.0, 24.40841491265353, 0.0937141516457492, 0.0, 1.0, 40981.58756347925], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 695400.0000, 
sim time next is 696000.0000, 
raw observation next is [-3.4, 73.0, 0.0, 0.0, 26.0, 24.43411856780081, 0.08813136429756986, 0.0, 1.0, 41006.2608660503], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.73, 0.0, 0.0, 0.6666666666666666, 0.536176547316734, 0.5293771214325232, 0.0, 1.0, 0.19526790888595383], 
reward next is 0.8047, 
noisyNet noise sample is [array([-0.1360555], dtype=float32), -0.8086417]. 
=============================================
[2019-04-04 14:46:31,868] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.33393 ]
 [80.306915]
 [79.93363 ]
 [79.484886]
 [78.94868 ]], R is [[80.68490601]
 [80.6829071 ]
 [80.68105316]
 [80.67930603]
 [80.67751312]].
[2019-04-04 14:46:32,204] A3C_AGENT_WORKER-Thread-10 INFO:Local step 171000, global step 2739327: loss 0.1365
[2019-04-04 14:46:32,206] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 171000, global step 2739329: learning rate 0.0000
[2019-04-04 14:46:32,798] A3C_AGENT_WORKER-Thread-2 INFO:Local step 171500, global step 2739636: loss 0.1228
[2019-04-04 14:46:32,800] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 171500, global step 2739637: learning rate 0.0000
[2019-04-04 14:46:35,967] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.8095987e-11 3.9245360e-12 1.8424176e-18 3.1271093e-17 1.0000000e+00
 7.6449497e-13 5.0198589e-18], sum to 1.0000
[2019-04-04 14:46:35,967] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4287
[2019-04-04 14:46:35,979] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.38333333333333, 82.5, 7.999999999999999, 27.66666666666666, 26.0, 25.89763246625111, 0.6247763553289857, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1065000.0000, 
sim time next is 1065600.0000, 
raw observation next is [12.2, 83.0, 11.5, 38.0, 26.0, 25.9228715062734, 0.623794698476482, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.03833333333333333, 0.041988950276243095, 0.6666666666666666, 0.6602392921894499, 0.7079315661588274, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9687064], dtype=float32), 0.7158448]. 
=============================================
[2019-04-04 14:46:37,987] A3C_AGENT_WORKER-Thread-13 INFO:Local step 172000, global step 2742170: loss 19.1627
[2019-04-04 14:46:37,987] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 172000, global step 2742170: learning rate 0.0000
[2019-04-04 14:46:38,632] A3C_AGENT_WORKER-Thread-11 INFO:Local step 171500, global step 2742450: loss 0.1376
[2019-04-04 14:46:38,633] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 171500, global step 2742450: learning rate 0.0000
[2019-04-04 14:46:38,830] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5683825e-10 2.9148396e-11 1.6770230e-17 2.1919707e-16 1.0000000e+00
 6.0692536e-12 2.0677772e-16], sum to 1.0000
[2019-04-04 14:46:38,832] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8038
[2019-04-04 14:46:38,852] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.7, 80.66666666666667, 0.0, 0.0, 26.0, 25.67375140188191, 0.6138252538840122, 0.0, 1.0, 18725.6451346694], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1149000.0000, 
sim time next is 1149600.0000, 
raw observation next is [12.7, 81.33333333333334, 0.0, 0.0, 26.0, 25.67758754463727, 0.6130385913483419, 0.0, 1.0, 18725.0069630218], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8133333333333335, 0.0, 0.0, 0.6666666666666666, 0.6397989620531058, 0.7043461971161139, 0.0, 1.0, 0.08916669982391333], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.02416139], dtype=float32), -0.2937982]. 
=============================================
[2019-04-04 14:46:41,801] A3C_AGENT_WORKER-Thread-17 INFO:Local step 172000, global step 2743907: loss 19.1617
[2019-04-04 14:46:41,801] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 172000, global step 2743907: learning rate 0.0000
[2019-04-04 14:46:42,950] A3C_AGENT_WORKER-Thread-2 INFO:Local step 172000, global step 2744464: loss 19.1887
[2019-04-04 14:46:42,956] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 172000, global step 2744464: learning rate 0.0000
[2019-04-04 14:46:44,216] A3C_AGENT_WORKER-Thread-16 INFO:Local step 171500, global step 2745077: loss 0.1469
[2019-04-04 14:46:44,218] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 171500, global step 2745078: learning rate 0.0000
[2019-04-04 14:46:44,876] A3C_AGENT_WORKER-Thread-15 INFO:Local step 171500, global step 2745417: loss 0.1551
[2019-04-04 14:46:44,878] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 171500, global step 2745417: learning rate 0.0000
[2019-04-04 14:46:45,308] A3C_AGENT_WORKER-Thread-14 INFO:Local step 171500, global step 2745656: loss 0.1399
[2019-04-04 14:46:45,315] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 171500, global step 2745656: learning rate 0.0000
[2019-04-04 14:46:45,947] A3C_AGENT_WORKER-Thread-19 INFO:Local step 171500, global step 2745968: loss 0.1403
[2019-04-04 14:46:45,947] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 171500, global step 2745968: learning rate 0.0000
[2019-04-04 14:46:45,989] A3C_AGENT_WORKER-Thread-4 INFO:Local step 171500, global step 2745996: loss 0.1382
[2019-04-04 14:46:45,990] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 171500, global step 2745996: learning rate 0.0000
[2019-04-04 14:46:46,857] A3C_AGENT_WORKER-Thread-18 INFO:Local step 171500, global step 2746490: loss 0.1317
[2019-04-04 14:46:46,858] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 171500, global step 2746490: learning rate 0.0000
[2019-04-04 14:46:47,572] A3C_AGENT_WORKER-Thread-20 INFO:Local step 171500, global step 2746898: loss 0.1385
[2019-04-04 14:46:47,577] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 171500, global step 2746898: learning rate 0.0000
[2019-04-04 14:46:47,718] A3C_AGENT_WORKER-Thread-3 INFO:Local step 171500, global step 2746986: loss 0.1334
[2019-04-04 14:46:47,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 171500, global step 2746986: learning rate 0.0000
[2019-04-04 14:46:48,396] A3C_AGENT_WORKER-Thread-5 INFO:Local step 171500, global step 2747394: loss 0.1308
[2019-04-04 14:46:48,403] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 171500, global step 2747394: learning rate 0.0000
[2019-04-04 14:46:48,468] A3C_AGENT_WORKER-Thread-11 INFO:Local step 172000, global step 2747429: loss 19.0654
[2019-04-04 14:46:48,470] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 172000, global step 2747430: learning rate 0.0000
[2019-04-04 14:46:48,594] A3C_AGENT_WORKER-Thread-12 INFO:Local step 171500, global step 2747507: loss 0.1246
[2019-04-04 14:46:48,594] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 171500, global step 2747507: learning rate 0.0000
[2019-04-04 14:46:49,522] A3C_AGENT_WORKER-Thread-6 INFO:Local step 171500, global step 2748119: loss 0.1232
[2019-04-04 14:46:49,523] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 171500, global step 2748119: learning rate 0.0000
[2019-04-04 14:46:51,496] A3C_AGENT_WORKER-Thread-10 INFO:Local step 171500, global step 2749331: loss 0.1190
[2019-04-04 14:46:51,498] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 171500, global step 2749332: learning rate 0.0000
[2019-04-04 14:46:51,662] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.1065633e-09 2.3383389e-09 3.6607395e-14 6.1928259e-14 1.0000000e+00
 1.4578265e-09 2.1691687e-14], sum to 1.0000
[2019-04-04 14:46:51,667] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2723
[2019-04-04 14:46:51,674] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.73333333333334, 54.33333333333334, 0.0, 0.0, 26.0, 26.49482871840005, 0.7746503049422818, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1102800.0000, 
sim time next is 1103400.0000, 
raw observation next is [15.55, 55.0, 0.0, 0.0, 26.0, 26.47225811067944, 0.7600300936241794, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8933518005540168, 0.55, 0.0, 0.0, 0.6666666666666666, 0.7060215092232868, 0.7533433645413932, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40108737], dtype=float32), -0.15153642]. 
=============================================
[2019-04-04 14:46:52,764] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8451488e-09 3.8593356e-10 5.6744482e-15 2.7633148e-13 1.0000000e+00
 1.5013220e-10 4.7297261e-15], sum to 1.0000
[2019-04-04 14:46:52,766] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4620
[2019-04-04 14:46:52,773] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.1, 53.0, 0.0, 0.0, 26.0, 26.42372466592091, 0.7749382414650489, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1101600.0000, 
sim time next is 1102200.0000, 
raw observation next is [15.91666666666667, 53.66666666666667, 0.0, 0.0, 26.0, 26.47470445471206, 0.7783586545663752, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.9035087719298247, 0.5366666666666667, 0.0, 0.0, 0.6666666666666666, 0.706225371226005, 0.7594528848554584, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.235469], dtype=float32), -1.1655859]. 
=============================================
[2019-04-04 14:46:53,091] A3C_AGENT_WORKER-Thread-13 INFO:Local step 172500, global step 2750380: loss 0.0418
[2019-04-04 14:46:53,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 172500, global step 2750382: learning rate 0.0000
[2019-04-04 14:46:54,751] A3C_AGENT_WORKER-Thread-16 INFO:Local step 172000, global step 2751496: loss 18.9608
[2019-04-04 14:46:54,753] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 172000, global step 2751497: learning rate 0.0000
[2019-04-04 14:46:55,549] A3C_AGENT_WORKER-Thread-15 INFO:Local step 172000, global step 2752041: loss 18.9452
[2019-04-04 14:46:55,551] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 172000, global step 2752041: learning rate 0.0000
[2019-04-04 14:46:56,430] A3C_AGENT_WORKER-Thread-14 INFO:Local step 172000, global step 2752654: loss 18.9661
[2019-04-04 14:46:56,433] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 172000, global step 2752654: learning rate 0.0000
[2019-04-04 14:46:56,724] A3C_AGENT_WORKER-Thread-19 INFO:Local step 172000, global step 2752846: loss 18.8861
[2019-04-04 14:46:56,727] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 172000, global step 2752847: learning rate 0.0000
[2019-04-04 14:46:57,003] A3C_AGENT_WORKER-Thread-4 INFO:Local step 172000, global step 2753023: loss 18.9445
[2019-04-04 14:46:57,006] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 172000, global step 2753023: learning rate 0.0000
[2019-04-04 14:46:57,372] A3C_AGENT_WORKER-Thread-17 INFO:Local step 172500, global step 2753283: loss 0.0338
[2019-04-04 14:46:57,374] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 172500, global step 2753284: learning rate 0.0000
[2019-04-04 14:46:57,755] A3C_AGENT_WORKER-Thread-18 INFO:Local step 172000, global step 2753532: loss 18.8832
[2019-04-04 14:46:57,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 172000, global step 2753534: learning rate 0.0000
[2019-04-04 14:46:57,960] A3C_AGENT_WORKER-Thread-2 INFO:Local step 172500, global step 2753672: loss 0.0314
[2019-04-04 14:46:57,962] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 172500, global step 2753673: learning rate 0.0000
[2019-04-04 14:46:58,740] A3C_AGENT_WORKER-Thread-20 INFO:Local step 172000, global step 2754183: loss 18.8598
[2019-04-04 14:46:58,742] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 172000, global step 2754183: learning rate 0.0000
[2019-04-04 14:46:58,772] A3C_AGENT_WORKER-Thread-3 INFO:Local step 172000, global step 2754201: loss 18.8861
[2019-04-04 14:46:58,773] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 172000, global step 2754201: learning rate 0.0000
[2019-04-04 14:46:59,707] A3C_AGENT_WORKER-Thread-5 INFO:Local step 172000, global step 2754773: loss 18.7914
[2019-04-04 14:46:59,708] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 172000, global step 2754773: learning rate 0.0000
[2019-04-04 14:46:59,790] A3C_AGENT_WORKER-Thread-12 INFO:Local step 172000, global step 2754816: loss 18.8629
[2019-04-04 14:46:59,792] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 172000, global step 2754817: learning rate 0.0000
[2019-04-04 14:47:00,623] A3C_AGENT_WORKER-Thread-6 INFO:Local step 172000, global step 2755310: loss 18.8020
[2019-04-04 14:47:00,625] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 172000, global step 2755310: learning rate 0.0000
[2019-04-04 14:47:02,006] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5462872e-09 1.0282452e-10 1.7652058e-16 4.3394959e-15 1.0000000e+00
 2.0178695e-10 4.1301726e-16], sum to 1.0000
[2019-04-04 14:47:02,011] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2930
[2019-04-04 14:47:02,027] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 57.5, 0.0, 26.0, 25.74016726660579, 0.5141834657957186, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1350000.0000, 
sim time next is 1350600.0000, 
raw observation next is [1.1, 92.16666666666667, 53.0, 0.0, 26.0, 25.70682445484368, 0.5131441649732681, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.9216666666666667, 0.17666666666666667, 0.0, 0.6666666666666666, 0.6422353712369734, 0.6710480549910893, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2907767], dtype=float32), -1.0535285]. 
=============================================
[2019-04-04 14:47:02,619] A3C_AGENT_WORKER-Thread-10 INFO:Local step 172000, global step 2756416: loss 18.7152
[2019-04-04 14:47:02,620] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 172000, global step 2756417: learning rate 0.0000
[2019-04-04 14:47:03,912] A3C_AGENT_WORKER-Thread-11 INFO:Local step 172500, global step 2757092: loss 0.0464
[2019-04-04 14:47:03,913] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 172500, global step 2757092: learning rate 0.0000
[2019-04-04 14:47:08,293] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4461423e-09 1.6294054e-10 8.5223495e-16 8.9191951e-15 1.0000000e+00
 5.1811808e-11 3.9815054e-16], sum to 1.0000
[2019-04-04 14:47:08,293] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3647
[2019-04-04 14:47:08,344] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.96706534765517, 0.4576913461369636, 1.0, 1.0, 67410.58191113203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1367400.0000, 
sim time next is 1368000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.90458596037231, 0.4627027194062487, 1.0, 1.0, 84083.71554617138], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5753821633643591, 0.654234239802083, 1.0, 1.0, 0.400398645457959], 
reward next is 0.5996, 
noisyNet noise sample is [array([-0.404929], dtype=float32), 0.070937395]. 
=============================================
[2019-04-04 14:47:08,361] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[88.293884]
 [87.77171 ]
 [88.58833 ]
 [89.82693 ]
 [91.12869 ]], R is [[88.72103882]
 [88.51283264]
 [88.53864288]
 [88.65325928]
 [88.76673126]].
[2019-04-04 14:47:09,483] A3C_AGENT_WORKER-Thread-13 INFO:Local step 173000, global step 2759763: loss 3.9919
[2019-04-04 14:47:09,484] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 173000, global step 2759763: learning rate 0.0000
[2019-04-04 14:47:10,245] A3C_AGENT_WORKER-Thread-16 INFO:Local step 172500, global step 2760113: loss 0.0722
[2019-04-04 14:47:10,248] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 172500, global step 2760114: learning rate 0.0000
[2019-04-04 14:47:11,213] A3C_AGENT_WORKER-Thread-15 INFO:Local step 172500, global step 2760566: loss 0.0772
[2019-04-04 14:47:11,213] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 172500, global step 2760566: learning rate 0.0000
[2019-04-04 14:47:11,578] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.0557534e-10 3.0285513e-10 3.2622913e-16 2.3853349e-14 1.0000000e+00
 5.5295372e-11 4.9941931e-16], sum to 1.0000
[2019-04-04 14:47:11,582] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5537
[2019-04-04 14:47:11,591] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.65, 98.0, 0.0, 0.0, 26.0, 25.49115832212148, 0.4654603521211467, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1492200.0000, 
sim time next is 1492800.0000, 
raw observation next is [1.466666666666667, 98.66666666666666, 0.0, 0.0, 26.0, 25.51645211048578, 0.4679383315106955, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.5032317636195753, 0.9866666666666666, 0.0, 0.0, 0.6666666666666666, 0.6263710092071483, 0.6559794438368985, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03477184], dtype=float32), 0.0958213]. 
=============================================
[2019-04-04 14:47:11,613] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.0163209e-10 8.7927998e-11 3.5174345e-16 1.0020582e-15 1.0000000e+00
 3.6275490e-11 1.0584612e-15], sum to 1.0000
[2019-04-04 14:47:11,614] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2883
[2019-04-04 14:47:11,628] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.54763398624428, 0.5095550577307714, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1471200.0000, 
sim time next is 1471800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.59205123901147, 0.5153228573977674, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6326709365842893, 0.6717742857992558, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34339592], dtype=float32), 0.08836971]. 
=============================================
[2019-04-04 14:47:11,830] A3C_AGENT_WORKER-Thread-14 INFO:Local step 172500, global step 2760920: loss 0.0869
[2019-04-04 14:47:11,831] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 172500, global step 2760921: learning rate 0.0000
[2019-04-04 14:47:12,459] A3C_AGENT_WORKER-Thread-4 INFO:Local step 172500, global step 2761286: loss 0.0923
[2019-04-04 14:47:12,461] A3C_AGENT_WORKER-Thread-19 INFO:Local step 172500, global step 2761288: loss 0.0794
[2019-04-04 14:47:12,463] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 172500, global step 2761288: learning rate 0.0000
[2019-04-04 14:47:12,464] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 172500, global step 2761286: learning rate 0.0000
[2019-04-04 14:47:12,613] A3C_AGENT_WORKER-Thread-18 INFO:Local step 172500, global step 2761376: loss 0.0836
[2019-04-04 14:47:12,617] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 172500, global step 2761376: learning rate 0.0000
[2019-04-04 14:47:13,561] A3C_AGENT_WORKER-Thread-20 INFO:Local step 172500, global step 2761950: loss 0.0893
[2019-04-04 14:47:13,563] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 172500, global step 2761950: learning rate 0.0000
[2019-04-04 14:47:13,942] A3C_AGENT_WORKER-Thread-3 INFO:Local step 172500, global step 2762183: loss 0.0907
[2019-04-04 14:47:13,943] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 172500, global step 2762184: learning rate 0.0000
[2019-04-04 14:47:14,048] A3C_AGENT_WORKER-Thread-17 INFO:Local step 173000, global step 2762243: loss 4.0234
[2019-04-04 14:47:14,049] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 173000, global step 2762243: learning rate 0.0000
[2019-04-04 14:47:14,550] A3C_AGENT_WORKER-Thread-5 INFO:Local step 172500, global step 2762530: loss 0.0896
[2019-04-04 14:47:14,551] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 172500, global step 2762530: learning rate 0.0000
[2019-04-04 14:47:14,626] A3C_AGENT_WORKER-Thread-2 INFO:Local step 173000, global step 2762572: loss 4.0279
[2019-04-04 14:47:14,628] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 173000, global step 2762574: learning rate 0.0000
[2019-04-04 14:47:15,027] A3C_AGENT_WORKER-Thread-12 INFO:Local step 172500, global step 2762819: loss 0.0858
[2019-04-04 14:47:15,029] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 172500, global step 2762820: learning rate 0.0000
[2019-04-04 14:47:15,259] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2875010e-10 3.1215884e-11 1.5803951e-17 1.6864286e-16 1.0000000e+00
 3.5548947e-12 4.0393767e-17], sum to 1.0000
[2019-04-04 14:47:15,261] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7812
[2019-04-04 14:47:15,276] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.433333333333334, 92.0, 0.0, 0.0, 26.0, 25.57239642037791, 0.5275154249210822, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1668000.0000, 
sim time next is 1668600.0000, 
raw observation next is [4.15, 92.0, 0.0, 0.0, 26.0, 25.54324292190781, 0.5269703781626764, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.5775623268698062, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6286035768256509, 0.6756567927208922, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7790513], dtype=float32), 0.47382122]. 
=============================================
[2019-04-04 14:47:15,619] A3C_AGENT_WORKER-Thread-6 INFO:Local step 172500, global step 2763129: loss 0.0907
[2019-04-04 14:47:15,622] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 172500, global step 2763130: learning rate 0.0000
[2019-04-04 14:47:17,832] A3C_AGENT_WORKER-Thread-10 INFO:Local step 172500, global step 2764273: loss 0.0985
[2019-04-04 14:47:17,834] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 172500, global step 2764273: learning rate 0.0000
[2019-04-04 14:47:20,363] A3C_AGENT_WORKER-Thread-11 INFO:Local step 173000, global step 2765518: loss 4.0450
[2019-04-04 14:47:20,387] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 173000, global step 2765518: learning rate 0.0000
[2019-04-04 14:47:21,805] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1986350e-10 6.4208014e-11 2.9764865e-16 1.5576290e-15 1.0000000e+00
 1.6587804e-11 2.3328166e-15], sum to 1.0000
[2019-04-04 14:47:21,807] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8428
[2019-04-04 14:47:21,823] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.94253116860676, 0.3663299308977526, 0.0, 1.0, 43610.34328726076], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1747200.0000, 
sim time next is 1747800.0000, 
raw observation next is [-0.8999999999999999, 85.0, 0.0, 0.0, 26.0, 24.98323077608004, 0.3633682238982506, 0.0, 1.0, 43639.16557852679], 
processed observation next is [0.0, 0.21739130434782608, 0.43767313019390586, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5819358980066699, 0.6211227412994168, 0.0, 1.0, 0.20780555037393708], 
reward next is 0.7922, 
noisyNet noise sample is [array([3.7222664], dtype=float32), -1.5652636]. 
=============================================
[2019-04-04 14:47:22,239] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.0780304e-10 1.2410269e-10 1.7351238e-15 2.4873483e-14 1.0000000e+00
 1.0676652e-10 7.3296768e-16], sum to 1.0000
[2019-04-04 14:47:22,239] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0476
[2019-04-04 14:47:22,258] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.45, 87.0, 0.0, 0.0, 26.0, 24.83080955898495, 0.3274670590722006, 0.0, 1.0, 43955.36259537904], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1751400.0000, 
sim time next is 1752000.0000, 
raw observation next is [-1.533333333333333, 87.0, 0.0, 0.0, 26.0, 24.8053163448324, 0.3220103225471669, 0.0, 1.0, 43985.94336815552], 
processed observation next is [0.0, 0.2608695652173913, 0.42012927054478305, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5671096954026998, 0.607336774182389, 0.0, 1.0, 0.20945687318169293], 
reward next is 0.7905, 
noisyNet noise sample is [array([-2.2414455], dtype=float32), -0.7516394]. 
=============================================
[2019-04-04 14:47:22,276] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[84.38939]
 [84.52266]
 [84.67302]
 [84.80678]
 [84.91922]], R is [[84.21350861]
 [84.1620636 ]
 [84.11131287]
 [84.06130219]
 [84.01204681]].
[2019-04-04 14:47:27,288] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.9097998e-09 2.8872962e-10 1.1029124e-14 5.5843039e-14 1.0000000e+00
 2.1791048e-10 6.3398502e-15], sum to 1.0000
[2019-04-04 14:47:27,288] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0032
[2019-04-04 14:47:27,300] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 88.33333333333334, 0.0, 0.0, 26.0, 24.33172768333688, 0.1258434383603503, 0.0, 1.0, 43419.98562756089], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2090400.0000, 
sim time next is 2091000.0000, 
raw observation next is [-6.1, 87.66666666666666, 0.0, 0.0, 26.0, 24.27568217279027, 0.1103162179627182, 0.0, 1.0, 43443.13814067842], 
processed observation next is [1.0, 0.17391304347826086, 0.29362880886426596, 0.8766666666666666, 0.0, 0.0, 0.6666666666666666, 0.522973514399189, 0.5367720726542394, 0.0, 1.0, 0.20687208638418295], 
reward next is 0.7931, 
noisyNet noise sample is [array([0.39834976], dtype=float32), -0.70192754]. 
=============================================
[2019-04-04 14:47:27,315] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[79.743225]
 [79.857376]
 [79.96956 ]
 [80.08703 ]
 [80.169846]], R is [[79.60336304]
 [79.60056305]
 [79.59791565]
 [79.59446716]
 [79.59046936]].
[2019-04-04 14:47:27,407] A3C_AGENT_WORKER-Thread-16 INFO:Local step 173000, global step 2768241: loss 4.0745
[2019-04-04 14:47:27,408] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 173000, global step 2768241: learning rate 0.0000
[2019-04-04 14:47:27,789] A3C_AGENT_WORKER-Thread-15 INFO:Local step 173000, global step 2768388: loss 4.0840
[2019-04-04 14:47:27,789] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 173000, global step 2768388: learning rate 0.0000
[2019-04-04 14:47:28,814] A3C_AGENT_WORKER-Thread-14 INFO:Local step 173000, global step 2768758: loss 4.1015
[2019-04-04 14:47:28,815] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 173000, global step 2768758: learning rate 0.0000
[2019-04-04 14:47:29,163] A3C_AGENT_WORKER-Thread-19 INFO:Local step 173000, global step 2768866: loss 4.0975
[2019-04-04 14:47:29,189] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 173000, global step 2768868: learning rate 0.0000
[2019-04-04 14:47:29,206] A3C_AGENT_WORKER-Thread-18 INFO:Local step 173000, global step 2768881: loss 4.0781
[2019-04-04 14:47:29,207] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 173000, global step 2768881: learning rate 0.0000
[2019-04-04 14:47:29,539] A3C_AGENT_WORKER-Thread-4 INFO:Local step 173000, global step 2768987: loss 4.0929
[2019-04-04 14:47:29,540] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 173000, global step 2768987: learning rate 0.0000
[2019-04-04 14:47:30,526] A3C_AGENT_WORKER-Thread-20 INFO:Local step 173000, global step 2769313: loss 4.1079
[2019-04-04 14:47:30,527] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 173000, global step 2769313: learning rate 0.0000
[2019-04-04 14:47:31,064] A3C_AGENT_WORKER-Thread-3 INFO:Local step 173000, global step 2769485: loss 4.1089
[2019-04-04 14:47:31,064] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 173000, global step 2769485: learning rate 0.0000
[2019-04-04 14:47:31,178] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4088922e-08 5.3540805e-10 1.1718516e-14 2.5113950e-13 1.0000000e+00
 5.6751120e-10 1.3524517e-13], sum to 1.0000
[2019-04-04 14:47:31,178] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0225
[2019-04-04 14:47:31,193] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.30937538728626, 0.1329307243608976, 0.0, 1.0, 41728.57022829013], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1993800.0000, 
sim time next is 1994400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.28262016577658, 0.1261326479948166, 0.0, 1.0, 41726.50911369605], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5235516804813818, 0.5420442159982722, 0.0, 1.0, 0.19869766244617165], 
reward next is 0.8013, 
noisyNet noise sample is [array([0.04687071], dtype=float32), -0.04667775]. 
=============================================
[2019-04-04 14:47:31,398] A3C_AGENT_WORKER-Thread-5 INFO:Local step 173000, global step 2769592: loss 4.0888
[2019-04-04 14:47:31,400] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 173000, global step 2769592: learning rate 0.0000
[2019-04-04 14:47:31,865] A3C_AGENT_WORKER-Thread-12 INFO:Local step 173000, global step 2769764: loss 4.1019
[2019-04-04 14:47:31,868] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 173000, global step 2769766: learning rate 0.0000
[2019-04-04 14:47:32,582] A3C_AGENT_WORKER-Thread-13 INFO:Local step 173500, global step 2770023: loss 1.5358
[2019-04-04 14:47:32,583] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 173500, global step 2770023: learning rate 0.0000
[2019-04-04 14:47:32,649] A3C_AGENT_WORKER-Thread-6 INFO:Local step 173000, global step 2770043: loss 4.1253
[2019-04-04 14:47:32,649] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 173000, global step 2770043: learning rate 0.0000
[2019-04-04 14:47:34,939] A3C_AGENT_WORKER-Thread-10 INFO:Local step 173000, global step 2770918: loss 4.1124
[2019-04-04 14:47:34,954] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 173000, global step 2770918: learning rate 0.0000
[2019-04-04 14:47:37,613] A3C_AGENT_WORKER-Thread-17 INFO:Local step 173500, global step 2771814: loss 1.5725
[2019-04-04 14:47:37,614] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 173500, global step 2771814: learning rate 0.0000
[2019-04-04 14:47:37,644] A3C_AGENT_WORKER-Thread-2 INFO:Local step 173500, global step 2771822: loss 1.6223
[2019-04-04 14:47:37,645] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 173500, global step 2771822: learning rate 0.0000
[2019-04-04 14:47:38,362] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0264709e-08 6.5663536e-10 4.1076609e-14 1.6570396e-13 1.0000000e+00
 3.6616565e-09 7.0250243e-14], sum to 1.0000
[2019-04-04 14:47:38,363] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4958
[2019-04-04 14:47:38,374] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.299999999999999, 79.5, 0.0, 0.0, 26.0, 24.46262998642965, 0.1903912873841104, 0.0, 1.0, 42453.99138023918], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2163000.0000, 
sim time next is 2163600.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.46131364518819, 0.1797361686243376, 0.0, 1.0, 42484.89415525682], 
processed observation next is [1.0, 0.043478260869565216, 0.26038781163434904, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5384428037656827, 0.5599120562081126, 0.0, 1.0, 0.20230901978693724], 
reward next is 0.7977, 
noisyNet noise sample is [array([-0.674205], dtype=float32), 0.7929982]. 
=============================================
[2019-04-04 14:47:43,568] A3C_AGENT_WORKER-Thread-11 INFO:Local step 173500, global step 2773961: loss 1.6318
[2019-04-04 14:47:43,568] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 173500, global step 2773961: learning rate 0.0000
[2019-04-04 14:47:46,714] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9294432e-08 8.8353036e-10 1.9556134e-15 1.6446901e-13 1.0000000e+00
 8.3932208e-11 4.3716793e-15], sum to 1.0000
[2019-04-04 14:47:46,714] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9382
[2019-04-04 14:47:46,767] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.95, 89.0, 45.0, 16.0, 26.0, 25.36050927937426, 0.2907238220668595, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2277000.0000, 
sim time next is 2277600.0000, 
raw observation next is [-8.766666666666666, 88.33333333333334, 54.33333333333333, 19.83333333333333, 26.0, 25.522221483693, 0.3077901469319679, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2197599261311173, 0.8833333333333334, 0.18111111111111108, 0.021915285451197048, 0.6666666666666666, 0.6268517903077498, 0.6025967156439893, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.63403946], dtype=float32), 0.20327185]. 
=============================================
[2019-04-04 14:47:47,903] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.4696974e-09 1.2510617e-09 9.1759604e-15 2.3564164e-14 1.0000000e+00
 9.7829557e-11 1.2263057e-15], sum to 1.0000
[2019-04-04 14:47:47,904] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7497
[2019-04-04 14:47:47,922] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 87.66666666666666, 0.0, 0.0, 26.0, 24.6442237643293, 0.2168401080445682, 0.0, 1.0, 42711.76525830389], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2079600.0000, 
sim time next is 2080200.0000, 
raw observation next is [-4.5, 86.83333333333333, 0.0, 0.0, 26.0, 24.59945381219009, 0.2088844710155276, 0.0, 1.0, 42726.77149472436], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.8683333333333333, 0.0, 0.0, 0.6666666666666666, 0.5499544843491743, 0.5696281570051759, 0.0, 1.0, 0.20346081664154456], 
reward next is 0.7965, 
noisyNet noise sample is [array([-0.11180364], dtype=float32), 0.104257196]. 
=============================================
[2019-04-04 14:47:49,972] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0905852e-08 3.1189493e-09 1.3433680e-14 5.1068649e-13 1.0000000e+00
 1.3414178e-09 6.2166486e-14], sum to 1.0000
[2019-04-04 14:47:49,972] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7267
[2019-04-04 14:47:49,987] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.7, 83.0, 0.0, 0.0, 26.0, 24.0624122030224, 0.07261119820174386, 0.0, 1.0, 43563.55716944933], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2095200.0000, 
sim time next is 2095800.0000, 
raw observation next is [-6.700000000000001, 82.16666666666667, 0.0, 0.0, 26.0, 24.05790031414959, 0.0612982477009678, 0.0, 1.0, 43596.21460866751], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.8216666666666668, 0.0, 0.0, 0.6666666666666666, 0.5048250261791326, 0.5204327492336559, 0.0, 1.0, 0.20760102194603577], 
reward next is 0.7924, 
noisyNet noise sample is [array([-0.26888254], dtype=float32), -1.6583941]. 
=============================================
[2019-04-04 14:47:50,892] A3C_AGENT_WORKER-Thread-16 INFO:Local step 173500, global step 2776513: loss 1.6545
[2019-04-04 14:47:50,897] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 173500, global step 2776513: learning rate 0.0000
[2019-04-04 14:47:51,332] A3C_AGENT_WORKER-Thread-15 INFO:Local step 173500, global step 2776692: loss 1.6380
[2019-04-04 14:47:51,334] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 173500, global step 2776692: learning rate 0.0000
[2019-04-04 14:47:52,432] A3C_AGENT_WORKER-Thread-14 INFO:Local step 173500, global step 2777113: loss 1.6671
[2019-04-04 14:47:52,432] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 173500, global step 2777113: learning rate 0.0000
[2019-04-04 14:47:52,588] A3C_AGENT_WORKER-Thread-19 INFO:Local step 173500, global step 2777167: loss 1.6366
[2019-04-04 14:47:52,589] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 173500, global step 2777167: learning rate 0.0000
[2019-04-04 14:47:52,767] A3C_AGENT_WORKER-Thread-18 INFO:Local step 173500, global step 2777233: loss 1.6636
[2019-04-04 14:47:52,768] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 173500, global step 2777233: learning rate 0.0000
[2019-04-04 14:47:52,826] A3C_AGENT_WORKER-Thread-13 INFO:Local step 174000, global step 2777257: loss 0.6078
[2019-04-04 14:47:52,858] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 174000, global step 2777258: learning rate 0.0000
[2019-04-04 14:47:53,648] A3C_AGENT_WORKER-Thread-4 INFO:Local step 173500, global step 2777550: loss 1.6825
[2019-04-04 14:47:53,649] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 173500, global step 2777550: learning rate 0.0000
[2019-04-04 14:47:53,687] A3C_AGENT_WORKER-Thread-20 INFO:Local step 173500, global step 2777564: loss 1.6698
[2019-04-04 14:47:53,691] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 173500, global step 2777564: learning rate 0.0000
[2019-04-04 14:47:54,543] A3C_AGENT_WORKER-Thread-3 INFO:Local step 173500, global step 2777863: loss 1.6679
[2019-04-04 14:47:54,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 173500, global step 2777863: learning rate 0.0000
[2019-04-04 14:47:54,983] A3C_AGENT_WORKER-Thread-5 INFO:Local step 173500, global step 2778016: loss 1.6207
[2019-04-04 14:47:54,983] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 173500, global step 2778016: learning rate 0.0000
[2019-04-04 14:47:55,226] A3C_AGENT_WORKER-Thread-12 INFO:Local step 173500, global step 2778103: loss 1.6681
[2019-04-04 14:47:55,227] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 173500, global step 2778103: learning rate 0.0000
[2019-04-04 14:47:55,654] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3427133e-09 4.6029167e-10 7.6776896e-15 6.6178528e-14 1.0000000e+00
 3.3060171e-10 1.2244039e-14], sum to 1.0000
[2019-04-04 14:47:55,654] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0126
[2019-04-04 14:47:55,704] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 71.0, 132.0, 0.0, 26.0, 25.54531504723665, 0.4550608091241881, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2210400.0000, 
sim time next is 2211000.0000, 
raw observation next is [-3.9, 70.5, 128.0, 0.0, 26.0, 26.07871872773686, 0.4904732208685048, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.705, 0.4266666666666667, 0.0, 0.6666666666666666, 0.6732265606447383, 0.6634910736228349, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1580789], dtype=float32), 0.012544673]. 
=============================================
[2019-04-04 14:47:55,710] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.17691]
 [83.28401]
 [82.47234]
 [81.95422]
 [81.82788]], R is [[83.15678406]
 [83.3252182 ]
 [82.56878662]
 [82.09342957]
 [82.0124588 ]].
[2019-04-04 14:47:56,634] A3C_AGENT_WORKER-Thread-6 INFO:Local step 173500, global step 2778609: loss 1.6877
[2019-04-04 14:47:56,636] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 173500, global step 2778610: learning rate 0.0000
[2019-04-04 14:47:58,146] A3C_AGENT_WORKER-Thread-2 INFO:Local step 174000, global step 2779207: loss 0.6065
[2019-04-04 14:47:58,150] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 174000, global step 2779210: learning rate 0.0000
[2019-04-04 14:47:58,156] A3C_AGENT_WORKER-Thread-17 INFO:Local step 174000, global step 2779212: loss 0.5960
[2019-04-04 14:47:58,158] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 174000, global step 2779213: learning rate 0.0000
[2019-04-04 14:47:58,696] A3C_AGENT_WORKER-Thread-10 INFO:Local step 173500, global step 2779369: loss 1.6873
[2019-04-04 14:47:58,700] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 173500, global step 2779369: learning rate 0.0000
[2019-04-04 14:48:01,884] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8301815e-09 4.8610183e-10 1.5574546e-14 1.9341668e-13 1.0000000e+00
 9.2901875e-10 3.8813408e-14], sum to 1.0000
[2019-04-04 14:48:01,884] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0922
[2019-04-04 14:48:01,916] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 68.0, 0.0, 0.0, 26.0, 24.76136850277294, 0.240152773989124, 0.0, 1.0, 41810.51589318649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2595600.0000, 
sim time next is 2596200.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 26.0, 24.77660116535049, 0.2452729850617457, 0.0, 1.0, 41773.92781756782], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5647167637792073, 0.5817576616872485, 0.0, 1.0, 0.19892346579794198], 
reward next is 0.8011, 
noisyNet noise sample is [array([-1.4167868], dtype=float32), -0.5064802]. 
=============================================
[2019-04-04 14:48:04,192] A3C_AGENT_WORKER-Thread-11 INFO:Local step 174000, global step 2781490: loss 0.5904
[2019-04-04 14:48:04,194] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 174000, global step 2781491: learning rate 0.0000
[2019-04-04 14:48:05,982] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5070931e-08 9.3867447e-10 2.6159181e-14 4.7312919e-13 1.0000000e+00
 8.2072454e-10 1.9798453e-13], sum to 1.0000
[2019-04-04 14:48:05,983] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9845
[2019-04-04 14:48:05,998] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.72690201793949, 0.2265364780359926, 0.0, 1.0, 39176.75122516284], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2341800.0000, 
sim time next is 2342400.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.68653063958725, 0.2182912949684541, 0.0, 1.0, 39302.48632714221], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5572108866322708, 0.5727637649894847, 0.0, 1.0, 0.18715469679591526], 
reward next is 0.8128, 
noisyNet noise sample is [array([-1.0451186], dtype=float32), -0.22500353]. 
=============================================
[2019-04-04 14:48:07,791] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0377910e-09 1.4731517e-10 2.1959136e-15 4.4634333e-15 1.0000000e+00
 8.3586173e-11 4.5162018e-15], sum to 1.0000
[2019-04-04 14:48:07,791] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4295
[2019-04-04 14:48:07,843] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.466666666666667, 63.0, 143.6666666666667, 426.0, 26.0, 24.89257780693902, 0.302233835534351, 0.0, 1.0, 68050.73852100108], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2371200.0000, 
sim time next is 2371800.0000, 
raw observation next is [-2.383333333333333, 62.5, 148.3333333333333, 402.0, 26.0, 24.92252457984495, 0.3121841811937132, 0.0, 1.0, 32935.9987864511], 
processed observation next is [0.0, 0.43478260869565216, 0.3965835641735919, 0.625, 0.4944444444444443, 0.44419889502762433, 0.6666666666666666, 0.5768770483204125, 0.6040613937312377, 0.0, 1.0, 0.15683808945929095], 
reward next is 0.8432, 
noisyNet noise sample is [array([1.3704886], dtype=float32), -0.01877151]. 
=============================================
[2019-04-04 14:48:11,263] A3C_AGENT_WORKER-Thread-16 INFO:Local step 174000, global step 2784297: loss 0.5663
[2019-04-04 14:48:11,265] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 174000, global step 2784298: learning rate 0.0000
[2019-04-04 14:48:11,732] A3C_AGENT_WORKER-Thread-15 INFO:Local step 174000, global step 2784495: loss 0.5760
[2019-04-04 14:48:11,733] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 174000, global step 2784495: learning rate 0.0000
[2019-04-04 14:48:12,534] A3C_AGENT_WORKER-Thread-13 INFO:Local step 174500, global step 2784837: loss 0.6435
[2019-04-04 14:48:12,535] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 174500, global step 2784837: learning rate 0.0000
[2019-04-04 14:48:12,687] A3C_AGENT_WORKER-Thread-19 INFO:Local step 174000, global step 2784907: loss 0.5636
[2019-04-04 14:48:12,689] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 174000, global step 2784908: learning rate 0.0000
[2019-04-04 14:48:12,797] A3C_AGENT_WORKER-Thread-14 INFO:Local step 174000, global step 2784957: loss 0.5703
[2019-04-04 14:48:12,803] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 174000, global step 2784961: learning rate 0.0000
[2019-04-04 14:48:12,859] A3C_AGENT_WORKER-Thread-18 INFO:Local step 174000, global step 2784988: loss 0.5746
[2019-04-04 14:48:12,860] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 174000, global step 2784988: learning rate 0.0000
[2019-04-04 14:48:13,579] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5769638e-08 8.3871168e-09 5.0741326e-14 6.9700528e-13 1.0000000e+00
 1.3827588e-09 6.7046934e-14], sum to 1.0000
[2019-04-04 14:48:13,584] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9969
[2019-04-04 14:48:13,605] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 73.0, 0.0, 0.0, 26.0, 24.90176689916004, 0.2551997886261475, 0.0, 1.0, 41645.68953060724], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2598600.0000, 
sim time next is 2599200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.97989800309158, 0.2524244878819699, 0.0, 1.0, 41599.97741536016], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5816581669242984, 0.5841414959606567, 0.0, 1.0, 0.19809513054933411], 
reward next is 0.8019, 
noisyNet noise sample is [array([-0.23364754], dtype=float32), -0.43886825]. 
=============================================
[2019-04-04 14:48:13,869] A3C_AGENT_WORKER-Thread-4 INFO:Local step 174000, global step 2785448: loss 0.5626
[2019-04-04 14:48:13,870] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 174000, global step 2785448: learning rate 0.0000
[2019-04-04 14:48:13,906] A3C_AGENT_WORKER-Thread-20 INFO:Local step 174000, global step 2785466: loss 0.5728
[2019-04-04 14:48:13,908] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 174000, global step 2785467: learning rate 0.0000
[2019-04-04 14:48:14,077] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.5219085e-09 1.4399933e-09 1.4514312e-13 8.1332738e-13 1.0000000e+00
 1.2947815e-09 1.1855684e-13], sum to 1.0000
[2019-04-04 14:48:14,078] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4269
[2019-04-04 14:48:14,090] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.75, 42.5, 0.0, 0.0, 26.0, 24.89207616200836, 0.2125057676373793, 0.0, 1.0, 42991.61512268484], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2413800.0000, 
sim time next is 2414400.0000, 
raw observation next is [-4.833333333333333, 42.0, 0.0, 0.0, 26.0, 24.84119687802228, 0.2029527566715419, 0.0, 1.0, 43015.9288663551], 
processed observation next is [0.0, 0.9565217391304348, 0.3287165281625116, 0.42, 0.0, 0.0, 0.6666666666666666, 0.57009973983519, 0.567650918890514, 0.0, 1.0, 0.20483775650645283], 
reward next is 0.7952, 
noisyNet noise sample is [array([-0.0839745], dtype=float32), -1.2222773]. 
=============================================
[2019-04-04 14:48:14,168] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7704394e-08 9.7622332e-10 7.4102589e-14 5.4914211e-13 1.0000000e+00
 3.3354646e-09 2.0523855e-13], sum to 1.0000
[2019-04-04 14:48:14,168] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6467
[2019-04-04 14:48:14,243] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 54.0, 40.0, 416.0, 26.0, 23.30715263988644, -0.005743958810927906, 0.0, 1.0, 203134.9941282969], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2449800.0000, 
sim time next is 2450400.0000, 
raw observation next is [-8.033333333333333, 52.66666666666666, 43.5, 457.5, 26.0, 24.00711317632287, 0.1052523159029845, 0.0, 1.0, 154200.2296521689], 
processed observation next is [0.0, 0.34782608695652173, 0.24007386888273316, 0.5266666666666666, 0.145, 0.505524861878453, 0.6666666666666666, 0.5005927646935726, 0.5350841053009948, 0.0, 1.0, 0.7342868078674709], 
reward next is 0.2657, 
noisyNet noise sample is [array([0.6476583], dtype=float32), 0.4132852]. 
=============================================
[2019-04-04 14:48:14,537] A3C_AGENT_WORKER-Thread-3 INFO:Local step 174000, global step 2785695: loss 0.5692
[2019-04-04 14:48:14,540] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 174000, global step 2785696: learning rate 0.0000
[2019-04-04 14:48:14,742] A3C_AGENT_WORKER-Thread-5 INFO:Local step 174000, global step 2785768: loss 0.5708
[2019-04-04 14:48:14,762] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 174000, global step 2785768: learning rate 0.0000
[2019-04-04 14:48:15,689] A3C_AGENT_WORKER-Thread-12 INFO:Local step 174000, global step 2786130: loss 0.5661
[2019-04-04 14:48:15,691] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 174000, global step 2786130: learning rate 0.0000
[2019-04-04 14:48:17,064] A3C_AGENT_WORKER-Thread-6 INFO:Local step 174000, global step 2786738: loss 0.5512
[2019-04-04 14:48:17,066] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 174000, global step 2786738: learning rate 0.0000
[2019-04-04 14:48:17,141] A3C_AGENT_WORKER-Thread-2 INFO:Local step 174500, global step 2786779: loss 0.6841
[2019-04-04 14:48:17,142] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 174500, global step 2786779: learning rate 0.0000
[2019-04-04 14:48:17,734] A3C_AGENT_WORKER-Thread-17 INFO:Local step 174500, global step 2787043: loss 0.6955
[2019-04-04 14:48:17,734] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 174500, global step 2787043: learning rate 0.0000
[2019-04-04 14:48:17,920] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1105446e-08 1.1423835e-09 6.0585604e-14 6.9860643e-13 1.0000000e+00
 5.8306304e-10 8.9612113e-15], sum to 1.0000
[2019-04-04 14:48:17,921] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2591
[2019-04-04 14:48:17,937] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 39.66666666666667, 0.0, 0.0, 26.0, 25.20723954372462, 0.2248363180449495, 0.0, 1.0, 39132.8486542027], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2508600.0000, 
sim time next is 2509200.0000, 
raw observation next is [-1.7, 40.0, 0.0, 0.0, 26.0, 25.14670300291356, 0.2158134236149967, 0.0, 1.0, 39139.22048356795], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.4, 0.0, 0.0, 0.6666666666666666, 0.5955585835761301, 0.5719378078716656, 0.0, 1.0, 0.18637724039794262], 
reward next is 0.8136, 
noisyNet noise sample is [array([0.6350753], dtype=float32), -0.650423]. 
=============================================
[2019-04-04 14:48:18,938] A3C_AGENT_WORKER-Thread-10 INFO:Local step 174000, global step 2787654: loss 0.5379
[2019-04-04 14:48:18,939] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 174000, global step 2787655: learning rate 0.0000
[2019-04-04 14:48:20,301] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8596684e-10 3.2296842e-11 9.3410602e-17 5.5061170e-16 1.0000000e+00
 3.1588170e-12 2.3798870e-17], sum to 1.0000
[2019-04-04 14:48:20,302] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8579
[2019-04-04 14:48:20,350] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.666666666666667, 93.0, 87.5, 134.5, 26.0, 24.80324729708973, 0.297618491782389, 1.0, 1.0, 88820.89384951346], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2881200.0000, 
sim time next is 2881800.0000, 
raw observation next is [1.5, 93.0, 105.0, 156.0, 26.0, 25.08078242628812, 0.3143476209099918, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5041551246537397, 0.93, 0.35, 0.1723756906077348, 0.6666666666666666, 0.5900652021906767, 0.6047825403033306, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11722732], dtype=float32), 0.11375185]. 
=============================================
[2019-04-04 14:48:22,532] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0381126e-10 7.8310761e-11 4.5319736e-16 4.0327607e-15 1.0000000e+00
 5.1411646e-11 5.3331905e-16], sum to 1.0000
[2019-04-04 14:48:22,535] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1709
[2019-04-04 14:48:22,572] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.4166666666666667, 46.33333333333334, 191.0, 189.6666666666667, 26.0, 25.46031412401701, 0.4455401773113885, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2639400.0000, 
sim time next is 2640000.0000, 
raw observation next is [-0.2333333333333334, 45.66666666666667, 177.5, 200.3333333333333, 26.0, 25.84649216947739, 0.4813745730640873, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.456140350877193, 0.4566666666666667, 0.5916666666666667, 0.2213627992633517, 0.6666666666666666, 0.6538743474564491, 0.6604581910213624, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07061766], dtype=float32), -0.37837154]. 
=============================================
[2019-04-04 14:48:22,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[85.01734 ]
 [85.56555 ]
 [85.68995 ]
 [85.142784]
 [84.886055]], R is [[84.62407684]
 [84.77783966]
 [84.61774445]
 [83.82984161]
 [83.35970306]].
[2019-04-04 14:48:23,362] A3C_AGENT_WORKER-Thread-11 INFO:Local step 174500, global step 2789720: loss 0.7564
[2019-04-04 14:48:23,363] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 174500, global step 2789720: learning rate 0.0000
[2019-04-04 14:48:29,247] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.9761933e-09 3.0512509e-10 2.0888561e-14 3.1325525e-14 1.0000000e+00
 1.7521792e-10 3.1953420e-14], sum to 1.0000
[2019-04-04 14:48:29,251] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7262
[2019-04-04 14:48:29,267] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.03708630966322, 0.291468683116379, 0.0, 1.0, 38372.66261427897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3019800.0000, 
sim time next is 3020400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.99787872516501, 0.2838739160015498, 0.0, 1.0, 38296.97878564578], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5831565604304174, 0.5946246386671833, 0.0, 1.0, 0.18236656564593226], 
reward next is 0.8176, 
noisyNet noise sample is [array([-1.2169908], dtype=float32), -1.2103393]. 
=============================================
[2019-04-04 14:48:29,533] A3C_AGENT_WORKER-Thread-16 INFO:Local step 174500, global step 2792402: loss 0.7339
[2019-04-04 14:48:29,533] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 174500, global step 2792402: learning rate 0.0000
[2019-04-04 14:48:30,088] A3C_AGENT_WORKER-Thread-15 INFO:Local step 174500, global step 2792611: loss 0.7508
[2019-04-04 14:48:30,090] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 174500, global step 2792611: learning rate 0.0000
[2019-04-04 14:48:30,468] A3C_AGENT_WORKER-Thread-13 INFO:Local step 175000, global step 2792766: loss 0.0974
[2019-04-04 14:48:30,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 175000, global step 2792767: learning rate 0.0000
[2019-04-04 14:48:31,500] A3C_AGENT_WORKER-Thread-18 INFO:Local step 174500, global step 2793156: loss 0.7486
[2019-04-04 14:48:31,502] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 174500, global step 2793157: learning rate 0.0000
[2019-04-04 14:48:31,534] A3C_AGENT_WORKER-Thread-14 INFO:Local step 174500, global step 2793168: loss 0.7404
[2019-04-04 14:48:31,537] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 174500, global step 2793168: learning rate 0.0000
[2019-04-04 14:48:31,596] A3C_AGENT_WORKER-Thread-19 INFO:Local step 174500, global step 2793191: loss 0.7613
[2019-04-04 14:48:31,596] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 174500, global step 2793191: learning rate 0.0000
[2019-04-04 14:48:32,254] A3C_AGENT_WORKER-Thread-4 INFO:Local step 174500, global step 2793452: loss 0.7532
[2019-04-04 14:48:32,262] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 174500, global step 2793452: learning rate 0.0000
[2019-04-04 14:48:32,624] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4500697e-10 6.4871893e-11 2.5694066e-17 7.7276663e-16 1.0000000e+00
 1.2661003e-10 3.5551384e-16], sum to 1.0000
[2019-04-04 14:48:32,627] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3085
[2019-04-04 14:48:32,661] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 100.0, 85.83333333333334, 0.0, 26.0, 25.84995102731052, 0.449796099563201, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2906400.0000, 
sim time next is 2907000.0000, 
raw observation next is [2.0, 100.0, 85.0, 0.0, 26.0, 25.87597122789581, 0.4547290946936417, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.2833333333333333, 0.0, 0.6666666666666666, 0.6563309356579842, 0.6515763648978806, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.58587295], dtype=float32), 0.20996656]. 
=============================================
[2019-04-04 14:48:32,670] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[92.41924]
 [92.5803 ]
 [92.7297 ]
 [92.96029]
 [93.30922]], R is [[92.34079742]
 [92.41738892]
 [92.49321747]
 [92.56828308]
 [92.64260101]].
[2019-04-04 14:48:33,007] A3C_AGENT_WORKER-Thread-20 INFO:Local step 174500, global step 2793752: loss 0.7465
[2019-04-04 14:48:33,008] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 174500, global step 2793752: learning rate 0.0000
[2019-04-04 14:48:33,226] A3C_AGENT_WORKER-Thread-5 INFO:Local step 174500, global step 2793856: loss 0.7369
[2019-04-04 14:48:33,227] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 174500, global step 2793856: learning rate 0.0000
[2019-04-04 14:48:33,303] A3C_AGENT_WORKER-Thread-3 INFO:Local step 174500, global step 2793889: loss 0.7597
[2019-04-04 14:48:33,305] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 174500, global step 2793891: learning rate 0.0000
[2019-04-04 14:48:34,477] A3C_AGENT_WORKER-Thread-2 INFO:Local step 175000, global step 2794432: loss 0.1012
[2019-04-04 14:48:34,478] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 175000, global step 2794432: learning rate 0.0000
[2019-04-04 14:48:34,503] A3C_AGENT_WORKER-Thread-12 INFO:Local step 174500, global step 2794446: loss 0.7730
[2019-04-04 14:48:34,504] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 174500, global step 2794446: learning rate 0.0000
[2019-04-04 14:48:35,826] A3C_AGENT_WORKER-Thread-17 INFO:Local step 175000, global step 2795027: loss 0.0929
[2019-04-04 14:48:35,828] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 175000, global step 2795027: learning rate 0.0000
[2019-04-04 14:48:36,100] A3C_AGENT_WORKER-Thread-6 INFO:Local step 174500, global step 2795146: loss 0.7534
[2019-04-04 14:48:36,100] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 174500, global step 2795146: learning rate 0.0000
[2019-04-04 14:48:38,089] A3C_AGENT_WORKER-Thread-10 INFO:Local step 174500, global step 2796038: loss 0.7560
[2019-04-04 14:48:38,090] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 174500, global step 2796038: learning rate 0.0000
[2019-04-04 14:48:41,248] A3C_AGENT_WORKER-Thread-11 INFO:Local step 175000, global step 2797573: loss 0.1022
[2019-04-04 14:48:41,249] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 175000, global step 2797573: learning rate 0.0000
[2019-04-04 14:48:45,998] A3C_AGENT_WORKER-Thread-13 INFO:Local step 175500, global step 2799724: loss 0.2167
[2019-04-04 14:48:46,000] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 175500, global step 2799725: learning rate 0.0000
[2019-04-04 14:48:46,623] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-04 14:48:46,624] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 14:48:46,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:48:46,626] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 14:48:46,626] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 14:48:46,628] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:48:46,628] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:48:46,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run29
[2019-04-04 14:48:46,653] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run29
[2019-04-04 14:48:46,655] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run29
[2019-04-04 14:50:05,899] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17585], dtype=float32), 0.2163252]
[2019-04-04 14:50:05,899] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [6.491805574000001, 16.99348981, 132.66624855, 183.8075243333333, 26.0, 25.21036317950874, 0.289912963036057, 1.0, 1.0, 0.0]
[2019-04-04 14:50:05,899] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 14:50:05,901] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.8307053e-09 8.4432250e-10 2.1786514e-14 1.1916646e-13 1.0000000e+00
 8.1003415e-10 2.4658085e-14], sampled 0.806746871915972
[2019-04-04 14:50:28,537] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5576 239912898.3460 1605.1887
[2019-04-04 14:50:48,211] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.4148 263462889.5000 1557.1326
[2019-04-04 14:50:51,551] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 14:50:52,575] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 2800000, evaluation results [2800000.0, 7241.414811904935, 263462889.49996805, 1557.1325719250624, 7353.557626923972, 239912898.345967, 1605.1886706289615, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 14:50:53,206] A3C_AGENT_WORKER-Thread-16 INFO:Local step 175000, global step 2800271: loss 0.0928
[2019-04-04 14:50:53,206] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 175000, global step 2800271: learning rate 0.0000
[2019-04-04 14:50:54,036] A3C_AGENT_WORKER-Thread-15 INFO:Local step 175000, global step 2800643: loss 0.0876
[2019-04-04 14:50:54,038] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 175000, global step 2800643: learning rate 0.0000
[2019-04-04 14:50:54,958] A3C_AGENT_WORKER-Thread-19 INFO:Local step 175000, global step 2801058: loss 0.0825
[2019-04-04 14:50:54,959] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 175000, global step 2801058: learning rate 0.0000
[2019-04-04 14:50:55,052] A3C_AGENT_WORKER-Thread-18 INFO:Local step 175000, global step 2801096: loss 0.0877
[2019-04-04 14:50:55,053] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 175000, global step 2801096: learning rate 0.0000
[2019-04-04 14:50:55,449] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.9839833e-09 3.2185368e-10 1.3687744e-14 2.1111633e-13 1.0000000e+00
 1.5467264e-10 3.7010803e-14], sum to 1.0000
[2019-04-04 14:50:55,451] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3267
[2019-04-04 14:50:55,480] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 71.16666666666667, 0.0, 0.0, 26.0, 23.76000552961344, -0.009181010160241948, 0.0, 1.0, 40215.71112227936], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3046200.0000, 
sim time next is 3046800.0000, 
raw observation next is [-6.0, 72.33333333333334, 0.0, 0.0, 26.0, 23.73304636665446, -0.0154576877729091, 0.0, 1.0, 40257.19456582331], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.4777538638878716, 0.4948474374090303, 0.0, 1.0, 0.1917009265039205], 
reward next is 0.8083, 
noisyNet noise sample is [array([1.2573789], dtype=float32), -0.8034378]. 
=============================================
[2019-04-04 14:50:55,546] A3C_AGENT_WORKER-Thread-14 INFO:Local step 175000, global step 2801346: loss 0.0884
[2019-04-04 14:50:55,549] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 175000, global step 2801349: learning rate 0.0000
[2019-04-04 14:50:55,863] A3C_AGENT_WORKER-Thread-4 INFO:Local step 175000, global step 2801509: loss 0.0860
[2019-04-04 14:50:55,863] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 175000, global step 2801509: learning rate 0.0000
[2019-04-04 14:50:56,018] A3C_AGENT_WORKER-Thread-2 INFO:Local step 175500, global step 2801580: loss 0.2023
[2019-04-04 14:50:56,020] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 175500, global step 2801581: learning rate 0.0000
[2019-04-04 14:50:56,766] A3C_AGENT_WORKER-Thread-20 INFO:Local step 175000, global step 2801894: loss 0.0847
[2019-04-04 14:50:56,767] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 175000, global step 2801894: learning rate 0.0000
[2019-04-04 14:50:56,843] A3C_AGENT_WORKER-Thread-5 INFO:Local step 175000, global step 2801932: loss 0.0895
[2019-04-04 14:50:56,843] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 175000, global step 2801932: learning rate 0.0000
[2019-04-04 14:50:56,979] A3C_AGENT_WORKER-Thread-3 INFO:Local step 175000, global step 2802001: loss 0.0859
[2019-04-04 14:50:56,989] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 175000, global step 2802001: learning rate 0.0000
[2019-04-04 14:50:58,266] A3C_AGENT_WORKER-Thread-17 INFO:Local step 175500, global step 2802588: loss 0.2092
[2019-04-04 14:50:58,266] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 175500, global step 2802588: learning rate 0.0000
[2019-04-04 14:50:58,316] A3C_AGENT_WORKER-Thread-12 INFO:Local step 175000, global step 2802612: loss 0.0849
[2019-04-04 14:50:58,318] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 175000, global step 2802612: learning rate 0.0000
[2019-04-04 14:50:59,401] A3C_AGENT_WORKER-Thread-6 INFO:Local step 175000, global step 2803137: loss 0.0939
[2019-04-04 14:50:59,403] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 175000, global step 2803137: learning rate 0.0000
[2019-04-04 14:51:01,660] A3C_AGENT_WORKER-Thread-10 INFO:Local step 175000, global step 2804387: loss 0.0939
[2019-04-04 14:51:01,662] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 175000, global step 2804387: learning rate 0.0000
[2019-04-04 14:51:02,823] A3C_AGENT_WORKER-Thread-11 INFO:Local step 175500, global step 2805084: loss 0.2122
[2019-04-04 14:51:02,829] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 175500, global step 2805084: learning rate 0.0000
[2019-04-04 14:51:03,714] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.6898384e-10 3.5181278e-11 3.5298007e-16 3.3927913e-15 1.0000000e+00
 3.6701833e-11 1.4242384e-16], sum to 1.0000
[2019-04-04 14:51:03,717] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7720
[2019-04-04 14:51:03,737] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 26.0, 25.29683675991418, 0.3086072830693904, 0.0, 1.0, 64817.07803413278], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3117000.0000, 
sim time next is 3117600.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 26.0, 25.25748780084532, 0.3152205471251593, 0.0, 1.0, 48289.93546032021], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6047906500704432, 0.6050735157083864, 0.0, 1.0, 0.22995207362057243], 
reward next is 0.7700, 
noisyNet noise sample is [array([-0.3063781], dtype=float32), 0.60940766]. 
=============================================
[2019-04-04 14:51:06,692] A3C_AGENT_WORKER-Thread-13 INFO:Local step 176000, global step 2807263: loss 0.0692
[2019-04-04 14:51:06,695] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 176000, global step 2807266: learning rate 0.0000
[2019-04-04 14:51:08,643] A3C_AGENT_WORKER-Thread-16 INFO:Local step 175500, global step 2808222: loss 0.2067
[2019-04-04 14:51:08,644] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 175500, global step 2808222: learning rate 0.0000
[2019-04-04 14:51:09,362] A3C_AGENT_WORKER-Thread-15 INFO:Local step 175500, global step 2808581: loss 0.1956
[2019-04-04 14:51:09,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 175500, global step 2808581: learning rate 0.0000
[2019-04-04 14:51:10,747] A3C_AGENT_WORKER-Thread-18 INFO:Local step 175500, global step 2809216: loss 0.1996
[2019-04-04 14:51:10,750] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 175500, global step 2809217: learning rate 0.0000
[2019-04-04 14:51:10,820] A3C_AGENT_WORKER-Thread-19 INFO:Local step 175500, global step 2809257: loss 0.2000
[2019-04-04 14:51:10,821] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 175500, global step 2809257: learning rate 0.0000
[2019-04-04 14:51:11,027] A3C_AGENT_WORKER-Thread-14 INFO:Local step 175500, global step 2809349: loss 0.1939
[2019-04-04 14:51:11,028] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 175500, global step 2809349: learning rate 0.0000
[2019-04-04 14:51:11,251] A3C_AGENT_WORKER-Thread-2 INFO:Local step 176000, global step 2809459: loss 0.0748
[2019-04-04 14:51:11,252] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 176000, global step 2809459: learning rate 0.0000
[2019-04-04 14:51:11,739] A3C_AGENT_WORKER-Thread-4 INFO:Local step 175500, global step 2809740: loss 0.1962
[2019-04-04 14:51:11,743] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 175500, global step 2809743: learning rate 0.0000
[2019-04-04 14:51:12,333] A3C_AGENT_WORKER-Thread-3 INFO:Local step 175500, global step 2810090: loss 0.2155
[2019-04-04 14:51:12,334] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 175500, global step 2810090: learning rate 0.0000
[2019-04-04 14:51:12,618] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.2047479e-09 4.1991008e-10 3.2111767e-15 6.7067012e-14 1.0000000e+00
 2.0029035e-10 1.3250912e-14], sum to 1.0000
[2019-04-04 14:51:12,629] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0810
[2019-04-04 14:51:12,648] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83092001720239, 0.2772759096991746, 0.0, 1.0, 41092.17558060888], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3381000.0000, 
sim time next is 3381600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.815426107001, 0.2758249883687985, 0.0, 1.0, 41115.58644341333], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5679521755834166, 0.5919416627895995, 0.0, 1.0, 0.19578850687339683], 
reward next is 0.8042, 
noisyNet noise sample is [array([-0.79504997], dtype=float32), -0.6967232]. 
=============================================
[2019-04-04 14:51:12,661] A3C_AGENT_WORKER-Thread-5 INFO:Local step 175500, global step 2810272: loss 0.2059
[2019-04-04 14:51:12,662] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 175500, global step 2810272: learning rate 0.0000
[2019-04-04 14:51:12,759] A3C_AGENT_WORKER-Thread-20 INFO:Local step 175500, global step 2810327: loss 0.2115
[2019-04-04 14:51:12,764] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 175500, global step 2810327: learning rate 0.0000
[2019-04-04 14:51:12,993] A3C_AGENT_WORKER-Thread-17 INFO:Local step 176000, global step 2810445: loss 0.0637
[2019-04-04 14:51:12,997] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 176000, global step 2810447: learning rate 0.0000
[2019-04-04 14:51:14,381] A3C_AGENT_WORKER-Thread-12 INFO:Local step 175500, global step 2811214: loss 0.2282
[2019-04-04 14:51:14,385] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 175500, global step 2811214: learning rate 0.0000
[2019-04-04 14:51:15,136] A3C_AGENT_WORKER-Thread-6 INFO:Local step 175500, global step 2811596: loss 0.2136
[2019-04-04 14:51:15,142] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 175500, global step 2811598: learning rate 0.0000
[2019-04-04 14:51:15,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9647849e-10 3.2568906e-11 1.0915017e-16 7.3254874e-15 1.0000000e+00
 2.1923819e-11 1.3444920e-16], sum to 1.0000
[2019-04-04 14:51:15,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1405
[2019-04-04 14:51:15,801] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 72.0, 112.3333333333333, 786.6666666666667, 26.0, 26.39147252689841, 0.5654794541060553, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3754200.0000, 
sim time next is 3754800.0000, 
raw observation next is [-3.0, 71.0, 113.0, 795.5, 26.0, 26.41876926476599, 0.5671711319988458, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3795013850415513, 0.71, 0.37666666666666665, 0.8790055248618784, 0.6666666666666666, 0.701564105397166, 0.6890570439996152, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2568771], dtype=float32), -0.32828113]. 
=============================================
[2019-04-04 14:51:16,168] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6350411e-09 7.6047023e-11 8.4641281e-16 1.3355120e-14 1.0000000e+00
 2.1441959e-10 5.0561506e-15], sum to 1.0000
[2019-04-04 14:51:16,170] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1449
[2019-04-04 14:51:16,176] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.0, 106.0, 776.0, 26.0, 26.61321560945434, 0.6560278422131415, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3767400.0000, 
sim time next is 3768000.0000, 
raw observation next is [0.0, 60.0, 102.8333333333333, 765.1666666666667, 26.0, 26.63520649663679, 0.6647615733084963, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.6, 0.3427777777777777, 0.8454880294659302, 0.6666666666666666, 0.7196005413863992, 0.7215871911028321, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19084404], dtype=float32), -0.26789048]. 
=============================================
[2019-04-04 14:51:16,178] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[85.79019]
 [85.84694]
 [85.92643]
 [85.98919]
 [86.03874]], R is [[85.86696625]
 [86.00830078]
 [86.14821625]
 [86.28673553]
 [86.42386627]].
[2019-04-04 14:51:17,155] A3C_AGENT_WORKER-Thread-10 INFO:Local step 175500, global step 2812720: loss 0.2422
[2019-04-04 14:51:17,156] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 175500, global step 2812720: learning rate 0.0000
[2019-04-04 14:51:18,130] A3C_AGENT_WORKER-Thread-11 INFO:Local step 176000, global step 2813262: loss 0.0670
[2019-04-04 14:51:18,135] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 176000, global step 2813262: learning rate 0.0000
[2019-04-04 14:51:19,976] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.83556831e-09 1.02852372e-10 1.13296692e-15 1.08402125e-14
 1.00000000e+00 3.04727701e-11 1.30205365e-15], sum to 1.0000
[2019-04-04 14:51:19,976] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2343
[2019-04-04 14:51:19,991] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 68.0, 0.0, 0.0, 26.0, 25.20352378334968, 0.4619336749546146, 0.0, 1.0, 83863.46925216838], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3538200.0000, 
sim time next is 3538800.0000, 
raw observation next is [-1.0, 66.0, 0.0, 0.0, 26.0, 25.22174359986482, 0.4760650222504151, 0.0, 1.0, 56881.41767349434], 
processed observation next is [1.0, 1.0, 0.4349030470914128, 0.66, 0.0, 0.0, 0.6666666666666666, 0.6018119666554016, 0.6586883407501384, 0.0, 1.0, 0.2708638936833064], 
reward next is 0.7291, 
noisyNet noise sample is [array([1.5341307], dtype=float32), 2.327273]. 
=============================================
[2019-04-04 14:51:20,039] A3C_AGENT_WORKER-Thread-13 INFO:Local step 176500, global step 2814343: loss 0.0181
[2019-04-04 14:51:20,040] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 176500, global step 2814343: learning rate 0.0000
[2019-04-04 14:51:22,618] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.2396955e-09 2.5987909e-10 2.4279952e-15 1.4741661e-14 1.0000000e+00
 3.2000202e-10 4.9463217e-15], sum to 1.0000
[2019-04-04 14:51:22,619] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7519
[2019-04-04 14:51:22,639] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 49.0, 90.33333333333334, 702.6666666666666, 26.0, 26.24757722632421, 0.6705267393969366, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3511200.0000, 
sim time next is 3511800.0000, 
raw observation next is [3.0, 49.0, 88.0, 687.0, 26.0, 25.65265648598464, 0.634679951597869, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.29333333333333333, 0.7591160220994475, 0.6666666666666666, 0.6377213738320533, 0.7115599838659564, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.5433548], dtype=float32), -0.7150783]. 
=============================================
[2019-04-04 14:51:22,869] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9816125e-08 3.4790544e-09 5.8703666e-14 2.3536999e-13 1.0000000e+00
 4.7971178e-09 5.9566731e-14], sum to 1.0000
[2019-04-04 14:51:22,870] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8478
[2019-04-04 14:51:22,909] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.13547123836307, 0.3633351715317261, 0.0, 1.0, 18704.6895958949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3607800.0000, 
sim time next is 3608400.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.12720918241889, 0.3548529982101298, 0.0, 1.0, 18704.47396987185], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5939340985349076, 0.6182843327367099, 0.0, 1.0, 0.08906892366605644], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.8513358], dtype=float32), 0.81019056]. 
=============================================
[2019-04-04 14:51:23,463] A3C_AGENT_WORKER-Thread-16 INFO:Local step 176000, global step 2816155: loss 0.0655
[2019-04-04 14:51:23,463] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 176000, global step 2816155: learning rate 0.0000
[2019-04-04 14:51:23,920] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1020514e-08 3.3204445e-10 1.2634917e-15 5.3829350e-14 1.0000000e+00
 4.3682899e-10 2.4980269e-14], sum to 1.0000
[2019-04-04 14:51:23,921] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3251
[2019-04-04 14:51:23,955] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.55312370862639, 0.3988013459295712, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3898200.0000, 
sim time next is 3898800.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.39971507181346, 0.3782439051942491, 0.0, 1.0, 89192.64395014996], 
processed observation next is [1.0, 0.13043478260869565, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6166429226511217, 0.6260813017314164, 0.0, 1.0, 0.42472687595309505], 
reward next is 0.5753, 
noisyNet noise sample is [array([0.39334258], dtype=float32), 0.42200956]. 
=============================================
[2019-04-04 14:51:24,646] A3C_AGENT_WORKER-Thread-15 INFO:Local step 176000, global step 2816760: loss 0.0647
[2019-04-04 14:51:24,648] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 176000, global step 2816761: learning rate 0.0000
[2019-04-04 14:51:25,137] A3C_AGENT_WORKER-Thread-19 INFO:Local step 176000, global step 2817061: loss 0.0656
[2019-04-04 14:51:25,139] A3C_AGENT_WORKER-Thread-2 INFO:Local step 176500, global step 2817062: loss 0.0175
[2019-04-04 14:51:25,141] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 176000, global step 2817062: learning rate 0.0000
[2019-04-04 14:51:25,146] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 176500, global step 2817062: learning rate 0.0000
[2019-04-04 14:51:25,532] A3C_AGENT_WORKER-Thread-18 INFO:Local step 176000, global step 2817299: loss 0.0603
[2019-04-04 14:51:25,533] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 176000, global step 2817299: learning rate 0.0000
[2019-04-04 14:51:25,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5241165e-09 6.7262895e-10 8.5274593e-15 4.2626808e-14 1.0000000e+00
 1.5235804e-10 3.5181761e-14], sum to 1.0000
[2019-04-04 14:51:25,789] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1327
[2019-04-04 14:51:25,823] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.0, 27.0, 0.0, 0.0, 26.0, 25.50374145883301, 0.3554415203496106, 0.0, 1.0, 30368.91805648798], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3654000.0000, 
sim time next is 3654600.0000, 
raw observation next is [8.833333333333334, 27.83333333333333, 0.0, 0.0, 26.0, 25.49015229943852, 0.3521984913830307, 0.0, 1.0, 35814.24313200195], 
processed observation next is [0.0, 0.30434782608695654, 0.7072945521698984, 0.27833333333333327, 0.0, 0.0, 0.6666666666666666, 0.6241793582865434, 0.6173994971276769, 0.0, 1.0, 0.17054401491429502], 
reward next is 0.8295, 
noisyNet noise sample is [array([0.4127375], dtype=float32), 0.8888791]. 
=============================================
[2019-04-04 14:51:26,007] A3C_AGENT_WORKER-Thread-14 INFO:Local step 176000, global step 2817562: loss 0.0583
[2019-04-04 14:51:26,007] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 176000, global step 2817562: learning rate 0.0000
[2019-04-04 14:51:26,382] A3C_AGENT_WORKER-Thread-4 INFO:Local step 176000, global step 2817767: loss 0.0599
[2019-04-04 14:51:26,405] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 176000, global step 2817767: learning rate 0.0000
[2019-04-04 14:51:26,817] A3C_AGENT_WORKER-Thread-17 INFO:Local step 176500, global step 2818016: loss 0.1192
[2019-04-04 14:51:26,822] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 176500, global step 2818018: learning rate 0.0000
[2019-04-04 14:51:26,951] A3C_AGENT_WORKER-Thread-3 INFO:Local step 176000, global step 2818098: loss 0.0620
[2019-04-04 14:51:26,954] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 176000, global step 2818098: learning rate 0.0000
[2019-04-04 14:51:27,330] A3C_AGENT_WORKER-Thread-5 INFO:Local step 176000, global step 2818324: loss 0.0558
[2019-04-04 14:51:27,331] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 176000, global step 2818325: learning rate 0.0000
[2019-04-04 14:51:27,650] A3C_AGENT_WORKER-Thread-20 INFO:Local step 176000, global step 2818513: loss 0.0542
[2019-04-04 14:51:27,652] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 176000, global step 2818515: learning rate 0.0000
[2019-04-04 14:51:29,352] A3C_AGENT_WORKER-Thread-12 INFO:Local step 176000, global step 2819452: loss 0.0623
[2019-04-04 14:51:29,353] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 176000, global step 2819452: learning rate 0.0000
[2019-04-04 14:51:29,929] A3C_AGENT_WORKER-Thread-6 INFO:Local step 176000, global step 2819758: loss 0.0539
[2019-04-04 14:51:29,932] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 176000, global step 2819763: learning rate 0.0000
[2019-04-04 14:51:31,060] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.8732283e-09 1.0727704e-10 2.8012121e-15 1.4266766e-13 1.0000000e+00
 5.8670652e-10 1.4298087e-14], sum to 1.0000
[2019-04-04 14:51:31,061] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6895
[2019-04-04 14:51:31,081] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 70.0, 0.0, 0.0, 26.0, 25.5722858202066, 0.4930419994036752, 1.0, 1.0, 74765.33457734213], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3784200.0000, 
sim time next is 3784800.0000, 
raw observation next is [-2.0, 69.0, 0.0, 0.0, 26.0, 25.43814747587798, 0.4772593069546264, 1.0, 1.0, 51257.83019201336], 
processed observation next is [1.0, 0.8260869565217391, 0.40720221606648205, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6198456229898316, 0.6590864356515421, 1.0, 1.0, 0.2440849056762541], 
reward next is 0.7559, 
noisyNet noise sample is [array([-0.15646297], dtype=float32), 1.0243406]. 
=============================================
[2019-04-04 14:51:32,490] A3C_AGENT_WORKER-Thread-11 INFO:Local step 176500, global step 2821153: loss 0.1576
[2019-04-04 14:51:32,491] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 176500, global step 2821153: learning rate 0.0000
[2019-04-04 14:51:32,811] A3C_AGENT_WORKER-Thread-10 INFO:Local step 176000, global step 2821330: loss 0.0500
[2019-04-04 14:51:32,811] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 176000, global step 2821330: learning rate 0.0000
[2019-04-04 14:51:33,506] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8879274e-09 6.4365457e-11 5.3864346e-15 3.4947946e-14 1.0000000e+00
 2.9945466e-10 4.6115697e-15], sum to 1.0000
[2019-04-04 14:51:33,508] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9144
[2019-04-04 14:51:33,532] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.97928141553391, 0.3092569595354092, 0.0, 1.0, 43818.65271024382], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3816600.0000, 
sim time next is 3817200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.96308679781025, 0.3005873065762337, 0.0, 1.0, 43882.08580443809], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5802572331508541, 0.6001957688587446, 0.0, 1.0, 0.20896231335446708], 
reward next is 0.7910, 
noisyNet noise sample is [array([1.4256039], dtype=float32), 0.61917216]. 
=============================================
[2019-04-04 14:51:34,439] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.4210384e-10 2.3426310e-11 5.4763507e-16 5.7361927e-15 1.0000000e+00
 1.7788999e-10 1.1678036e-15], sum to 1.0000
[2019-04-04 14:51:34,440] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4667
[2019-04-04 14:51:34,451] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.5, 62.5, 119.0, 829.0, 26.0, 26.52165830288088, 0.5952361440612909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3760200.0000, 
sim time next is 3760800.0000, 
raw observation next is [-1.333333333333333, 61.66666666666667, 118.3333333333333, 827.1666666666667, 26.0, 26.4133079894171, 0.5852227786634031, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.42566943674976926, 0.6166666666666667, 0.3944444444444443, 0.9139963167587478, 0.6666666666666666, 0.7011089991180915, 0.6950742595544677, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00361816], dtype=float32), -1.0598779]. 
=============================================
[2019-04-04 14:51:34,657] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.9377163e-09 1.0807704e-09 8.5791427e-15 5.6661991e-14 1.0000000e+00
 9.5142361e-10 9.1244405e-15], sum to 1.0000
[2019-04-04 14:51:34,658] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6683
[2019-04-04 14:51:34,676] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.78271792494645, 0.2434241498254154, 0.0, 1.0, 43077.44810549774], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3825000.0000, 
sim time next is 3825600.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.74315527452878, 0.2320711191616457, 0.0, 1.0, 43000.82566608274], 
processed observation next is [1.0, 0.2608695652173913, 0.32409972299168976, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5619296062107318, 0.5773570397205486, 0.0, 1.0, 0.2047658365051559], 
reward next is 0.7952, 
noisyNet noise sample is [array([0.40289947], dtype=float32), -0.40570974]. 
=============================================
[2019-04-04 14:51:35,720] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0858009e-09 1.0272106e-09 1.3470184e-15 1.0426164e-14 1.0000000e+00
 1.5828745e-10 1.6034238e-15], sum to 1.0000
[2019-04-04 14:51:35,720] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8456
[2019-04-04 14:51:35,727] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.833333333333333, 45.5, 79.33333333333334, 661.6666666666667, 26.0, 26.78211959357595, 0.7386067923711587, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3858600.0000, 
sim time next is 3859200.0000, 
raw observation next is [3.0, 45.0, 75.5, 634.0, 26.0, 26.8820596804276, 0.751604446453364, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.25166666666666665, 0.7005524861878453, 0.6666666666666666, 0.7401716400356332, 0.7505348154844547, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34991315], dtype=float32), -0.35419986]. 
=============================================
[2019-04-04 14:51:36,174] A3C_AGENT_WORKER-Thread-13 INFO:Local step 177000, global step 2823051: loss 0.0001
[2019-04-04 14:51:36,175] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 177000, global step 2823051: learning rate 0.0000
[2019-04-04 14:51:38,238] A3C_AGENT_WORKER-Thread-16 INFO:Local step 176500, global step 2824158: loss 0.0401
[2019-04-04 14:51:38,240] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 176500, global step 2824158: learning rate 0.0000
[2019-04-04 14:51:39,493] A3C_AGENT_WORKER-Thread-15 INFO:Local step 176500, global step 2824761: loss 0.1613
[2019-04-04 14:51:39,495] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 176500, global step 2824762: learning rate 0.0000
[2019-04-04 14:51:39,876] A3C_AGENT_WORKER-Thread-19 INFO:Local step 176500, global step 2824953: loss 0.1644
[2019-04-04 14:51:39,877] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 176500, global step 2824953: learning rate 0.0000
[2019-04-04 14:51:40,527] A3C_AGENT_WORKER-Thread-18 INFO:Local step 176500, global step 2825268: loss 0.0386
[2019-04-04 14:51:40,529] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 176500, global step 2825268: learning rate 0.0000
[2019-04-04 14:51:40,574] A3C_AGENT_WORKER-Thread-14 INFO:Local step 176500, global step 2825287: loss 0.0356
[2019-04-04 14:51:40,575] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 176500, global step 2825287: learning rate 0.0000
[2019-04-04 14:51:41,478] A3C_AGENT_WORKER-Thread-4 INFO:Local step 176500, global step 2825696: loss 0.0368
[2019-04-04 14:51:41,486] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 176500, global step 2825697: learning rate 0.0000
[2019-04-04 14:51:41,499] A3C_AGENT_WORKER-Thread-3 INFO:Local step 176500, global step 2825701: loss 0.1579
[2019-04-04 14:51:41,502] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 176500, global step 2825703: learning rate 0.0000
[2019-04-04 14:51:41,628] A3C_AGENT_WORKER-Thread-2 INFO:Local step 177000, global step 2825765: loss 0.0003
[2019-04-04 14:51:41,629] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 177000, global step 2825765: learning rate 0.0000
[2019-04-04 14:51:42,035] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2371433e-07 4.0515071e-09 3.1293230e-13 4.6577555e-12 9.9999988e-01
 9.7713846e-09 3.9185349e-13], sum to 1.0000
[2019-04-04 14:51:42,036] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3144
[2019-04-04 14:51:42,055] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.36948300291852, 0.1778193343275089, 0.0, 1.0, 43765.33986563783], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3983400.0000, 
sim time next is 3984000.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.3007688910482, 0.1647448591680873, 0.0, 1.0, 43762.06173687475], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5250640742540167, 0.5549149530560291, 0.0, 1.0, 0.20839077017559404], 
reward next is 0.7916, 
noisyNet noise sample is [array([-0.90915775], dtype=float32), 0.28023678]. 
=============================================
[2019-04-04 14:51:42,062] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.693054]
 [73.7297  ]
 [73.83438 ]
 [73.93097 ]
 [74.13427 ]], R is [[73.6420517 ]
 [73.69722748]
 [73.75180054]
 [73.80580902]
 [73.85922241]].
[2019-04-04 14:51:42,102] A3C_AGENT_WORKER-Thread-5 INFO:Local step 176500, global step 2825993: loss 0.1645
[2019-04-04 14:51:42,102] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 176500, global step 2825993: learning rate 0.0000
[2019-04-04 14:51:42,389] A3C_AGENT_WORKER-Thread-20 INFO:Local step 176500, global step 2826130: loss 0.1594
[2019-04-04 14:51:42,392] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 176500, global step 2826133: learning rate 0.0000
[2019-04-04 14:51:42,745] A3C_AGENT_WORKER-Thread-17 INFO:Local step 177000, global step 2826273: loss 0.0004
[2019-04-04 14:51:42,746] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 177000, global step 2826273: learning rate 0.0000
[2019-04-04 14:51:44,204] A3C_AGENT_WORKER-Thread-12 INFO:Local step 176500, global step 2826987: loss 0.1562
[2019-04-04 14:51:44,207] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 176500, global step 2826989: learning rate 0.0000
[2019-04-04 14:51:44,660] A3C_AGENT_WORKER-Thread-6 INFO:Local step 176500, global step 2827215: loss 0.1508
[2019-04-04 14:51:44,662] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 176500, global step 2827215: learning rate 0.0000
[2019-04-04 14:51:47,712] A3C_AGENT_WORKER-Thread-10 INFO:Local step 176500, global step 2828698: loss 0.0418
[2019-04-04 14:51:47,714] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 176500, global step 2828698: learning rate 0.0000
[2019-04-04 14:51:47,970] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0057838e-11 1.5776735e-12 3.4814948e-18 2.4522874e-17 1.0000000e+00
 1.2591008e-12 3.4683317e-18], sum to 1.0000
[2019-04-04 14:51:47,973] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6561
[2019-04-04 14:51:47,986] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.6, 34.0, 117.5, 804.0, 26.0, 27.72080974073229, 0.885921200023103, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4359600.0000, 
sim time next is 4360200.0000, 
raw observation next is [13.0, 33.0, 118.3333333333333, 812.0, 26.0, 27.74095489479588, 0.9011607738004491, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8227146814404434, 0.33, 0.3944444444444443, 0.8972375690607735, 0.6666666666666666, 0.81174624123299, 0.8003869246001497, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.368738], dtype=float32), -0.3831164]. 
=============================================
[2019-04-04 14:51:48,899] A3C_AGENT_WORKER-Thread-11 INFO:Local step 177000, global step 2829316: loss 0.0006
[2019-04-04 14:51:48,902] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 177000, global step 2829317: learning rate 0.0000
[2019-04-04 14:51:50,213] A3C_AGENT_WORKER-Thread-13 INFO:Local step 177500, global step 2830075: loss 3.7148
[2019-04-04 14:51:50,218] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 177500, global step 2830077: learning rate 0.0000
[2019-04-04 14:51:50,959] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7332017e-08 1.1994430e-09 2.2122007e-14 1.9067461e-13 1.0000000e+00
 4.3140989e-09 2.1254408e-14], sum to 1.0000
[2019-04-04 14:51:50,959] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9235
[2019-04-04 14:51:50,974] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 40.0, 0.0, 0.0, 26.0, 25.39789445426919, 0.4394017975461098, 0.0, 1.0, 26438.79088190107], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4149600.0000, 
sim time next is 4150200.0000, 
raw observation next is [-1.0, 39.5, 0.0, 0.0, 26.0, 25.41620301602092, 0.4332837988965233, 0.0, 1.0, 21751.1407871322], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.395, 0.0, 0.0, 0.6666666666666666, 0.6180169180017433, 0.6444279329655077, 0.0, 1.0, 0.10357686089110572], 
reward next is 0.8964, 
noisyNet noise sample is [array([-1.0903168], dtype=float32), 1.3788674]. 
=============================================
[2019-04-04 14:51:54,200] A3C_AGENT_WORKER-Thread-16 INFO:Local step 177000, global step 2832170: loss 0.0008
[2019-04-04 14:51:54,212] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 177000, global step 2832170: learning rate 0.0000
[2019-04-04 14:51:55,408] A3C_AGENT_WORKER-Thread-2 INFO:Local step 177500, global step 2832824: loss 3.7583
[2019-04-04 14:51:55,410] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 177500, global step 2832825: learning rate 0.0000
[2019-04-04 14:51:55,660] A3C_AGENT_WORKER-Thread-15 INFO:Local step 177000, global step 2832961: loss 0.0011
[2019-04-04 14:51:55,663] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 177000, global step 2832961: learning rate 0.0000
[2019-04-04 14:51:55,841] A3C_AGENT_WORKER-Thread-19 INFO:Local step 177000, global step 2833069: loss 0.0019
[2019-04-04 14:51:55,841] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 177000, global step 2833069: learning rate 0.0000
[2019-04-04 14:51:56,474] A3C_AGENT_WORKER-Thread-18 INFO:Local step 177000, global step 2833443: loss 0.0013
[2019-04-04 14:51:56,478] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 177000, global step 2833444: learning rate 0.0000
[2019-04-04 14:51:56,922] A3C_AGENT_WORKER-Thread-14 INFO:Local step 177000, global step 2833673: loss 0.0018
[2019-04-04 14:51:56,923] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 177000, global step 2833673: learning rate 0.0000
[2019-04-04 14:51:57,045] A3C_AGENT_WORKER-Thread-17 INFO:Local step 177500, global step 2833746: loss 4.6843
[2019-04-04 14:51:57,047] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 177500, global step 2833746: learning rate 0.0000
[2019-04-04 14:51:57,300] A3C_AGENT_WORKER-Thread-3 INFO:Local step 177000, global step 2833882: loss 0.0026
[2019-04-04 14:51:57,302] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 177000, global step 2833883: learning rate 0.0000
[2019-04-04 14:51:57,470] A3C_AGENT_WORKER-Thread-4 INFO:Local step 177000, global step 2833980: loss 0.0021
[2019-04-04 14:51:57,471] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 177000, global step 2833981: learning rate 0.0000
[2019-04-04 14:51:58,239] A3C_AGENT_WORKER-Thread-20 INFO:Local step 177000, global step 2834421: loss 0.0012
[2019-04-04 14:51:58,251] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 177000, global step 2834428: learning rate 0.0000
[2019-04-04 14:51:58,289] A3C_AGENT_WORKER-Thread-5 INFO:Local step 177000, global step 2834445: loss 0.0013
[2019-04-04 14:51:58,291] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 177000, global step 2834448: learning rate 0.0000
[2019-04-04 14:51:59,213] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.19343626e-09 3.66270236e-10 1.38583924e-14 1.60808002e-14
 1.00000000e+00 9.45346995e-11 1.29180334e-14], sum to 1.0000
[2019-04-04 14:51:59,215] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6767
[2019-04-04 14:51:59,231] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.666666666666667, 47.66666666666667, 0.0, 0.0, 26.0, 25.42150734939478, 0.3672004548423529, 0.0, 1.0, 27289.96398900482], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4232400.0000, 
sim time next is 4233000.0000, 
raw observation next is [1.833333333333333, 47.83333333333333, 0.0, 0.0, 26.0, 25.41376486019184, 0.3641398650635496, 0.0, 1.0, 36279.2637527503], 
processed observation next is [0.0, 1.0, 0.5133887349953832, 0.4783333333333333, 0.0, 0.0, 0.6666666666666666, 0.61781373834932, 0.6213799550211833, 0.0, 1.0, 0.17275839882262048], 
reward next is 0.8272, 
noisyNet noise sample is [array([-0.50639665], dtype=float32), 1.2868007]. 
=============================================
[2019-04-04 14:51:59,244] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[80.05847 ]
 [80.06598 ]
 [79.99509 ]
 [79.907585]
 [79.78745 ]], R is [[80.11284637]
 [80.18177032]
 [80.18813324]
 [80.18039703]
 [80.16129303]].
[2019-04-04 14:51:59,997] A3C_AGENT_WORKER-Thread-12 INFO:Local step 177000, global step 2835399: loss 0.0022
[2019-04-04 14:52:00,001] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 177000, global step 2835400: learning rate 0.0000
[2019-04-04 14:52:00,491] A3C_AGENT_WORKER-Thread-6 INFO:Local step 177000, global step 2835690: loss 0.0032
[2019-04-04 14:52:00,494] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 177000, global step 2835690: learning rate 0.0000
[2019-04-04 14:52:01,152] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.9507731e-09 8.5205387e-10 8.4995623e-15 8.5542122e-14 1.0000000e+00
 7.7517340e-11 1.4178243e-14], sum to 1.0000
[2019-04-04 14:52:01,152] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7507
[2019-04-04 14:52:01,163] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 37.0, 214.0, 669.0, 26.0, 25.11015128226857, 0.4021523512516984, 0.0, 1.0, 18688.23204216188], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4195800.0000, 
sim time next is 4196400.0000, 
raw observation next is [2.0, 38.0, 209.5, 572.3333333333334, 26.0, 25.11339744448698, 0.403284642309629, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.38, 0.6983333333333334, 0.6324125230202579, 0.6666666666666666, 0.5927831203739151, 0.6344282141032097, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35902134], dtype=float32), 1.2714317]. 
=============================================
[2019-04-04 14:52:01,943] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.2750000e-11 1.4288014e-12 7.3220398e-18 1.1751766e-16 1.0000000e+00
 2.4012639e-12 9.9934304e-18], sum to 1.0000
[2019-04-04 14:52:01,946] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2067
[2019-04-04 14:52:01,958] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.53333333333333, 30.33333333333333, 128.3333333333333, 806.5, 26.0, 28.19715874487028, 1.078779542597791, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4369200.0000, 
sim time next is 4369800.0000, 
raw observation next is [14.51666666666667, 30.66666666666667, 141.6666666666667, 771.0, 26.0, 28.37464325963791, 1.107415760905866, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8647276084949217, 0.3066666666666667, 0.4722222222222224, 0.8519337016574585, 0.6666666666666666, 0.8645536049698258, 0.869138586968622, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6094153], dtype=float32), 0.4752863]. 
=============================================
[2019-04-04 14:52:03,391] A3C_AGENT_WORKER-Thread-11 INFO:Local step 177500, global step 2837392: loss 4.8067
[2019-04-04 14:52:03,392] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 177500, global step 2837392: learning rate 0.0000
[2019-04-04 14:52:03,403] A3C_AGENT_WORKER-Thread-10 INFO:Local step 177000, global step 2837397: loss 0.0019
[2019-04-04 14:52:03,405] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 177000, global step 2837400: learning rate 0.0000
[2019-04-04 14:52:05,646] A3C_AGENT_WORKER-Thread-13 INFO:Local step 178000, global step 2838720: loss 0.0153
[2019-04-04 14:52:05,647] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 178000, global step 2838720: learning rate 0.0000
[2019-04-04 14:52:08,205] A3C_AGENT_WORKER-Thread-16 INFO:Local step 177500, global step 2840049: loss 3.7119
[2019-04-04 14:52:08,220] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 177500, global step 2840056: learning rate 0.0000
[2019-04-04 14:52:09,382] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4531405e-09 2.1734553e-10 2.2952484e-15 1.5650868e-14 1.0000000e+00
 3.6805112e-10 2.9981510e-15], sum to 1.0000
[2019-04-04 14:52:09,385] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3817
[2019-04-04 14:52:09,403] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.4666666666666667, 72.83333333333333, 0.0, 0.0, 26.0, 25.37204665996567, 0.4821396405822978, 0.0, 1.0, 47121.33052266898], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4492200.0000, 
sim time next is 4492800.0000, 
raw observation next is [-0.5, 73.0, 0.0, 0.0, 26.0, 25.41974647264299, 0.4788487345079511, 0.0, 1.0, 19265.3978823917], 
processed observation next is [1.0, 0.0, 0.44875346260387816, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6183122060535826, 0.6596162448359837, 0.0, 1.0, 0.09173998991615094], 
reward next is 0.9083, 
noisyNet noise sample is [array([1.2849315], dtype=float32), -0.33122766]. 
=============================================
[2019-04-04 14:52:09,802] A3C_AGENT_WORKER-Thread-15 INFO:Local step 177500, global step 2840886: loss 4.5112
[2019-04-04 14:52:09,803] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 177500, global step 2840886: learning rate 0.0000
[2019-04-04 14:52:10,167] A3C_AGENT_WORKER-Thread-19 INFO:Local step 177500, global step 2841079: loss 4.5677
[2019-04-04 14:52:10,168] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 177500, global step 2841079: learning rate 0.0000
[2019-04-04 14:52:10,386] A3C_AGENT_WORKER-Thread-18 INFO:Local step 177500, global step 2841197: loss 3.6365
[2019-04-04 14:52:10,393] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 177500, global step 2841201: learning rate 0.0000
[2019-04-04 14:52:10,511] A3C_AGENT_WORKER-Thread-2 INFO:Local step 178000, global step 2841269: loss 0.0099
[2019-04-04 14:52:10,515] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 178000, global step 2841270: learning rate 0.0000
[2019-04-04 14:52:11,256] A3C_AGENT_WORKER-Thread-14 INFO:Local step 177500, global step 2841621: loss 3.5962
[2019-04-04 14:52:11,258] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 177500, global step 2841624: learning rate 0.0000
[2019-04-04 14:52:11,472] A3C_AGENT_WORKER-Thread-4 INFO:Local step 177500, global step 2841738: loss 3.5958
[2019-04-04 14:52:11,473] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 177500, global step 2841739: learning rate 0.0000
[2019-04-04 14:52:11,835] A3C_AGENT_WORKER-Thread-3 INFO:Local step 177500, global step 2841932: loss 4.5878
[2019-04-04 14:52:11,836] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 177500, global step 2841932: learning rate 0.0000
[2019-04-04 14:52:12,230] A3C_AGENT_WORKER-Thread-20 INFO:Local step 177500, global step 2842160: loss 4.5932
[2019-04-04 14:52:12,239] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 177500, global step 2842160: learning rate 0.0000
[2019-04-04 14:52:12,309] A3C_AGENT_WORKER-Thread-17 INFO:Local step 178000, global step 2842201: loss 0.0082
[2019-04-04 14:52:12,312] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 178000, global step 2842201: learning rate 0.0000
[2019-04-04 14:52:12,334] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.1509809e-10 2.1596709e-09 1.2781739e-14 8.3069036e-14 1.0000000e+00
 4.7466109e-10 2.7205551e-14], sum to 1.0000
[2019-04-04 14:52:12,346] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3733
[2019-04-04 14:52:12,360] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.666666666666667, 60.0, 0.0, 0.0, 26.0, 25.07413068529036, 0.2951357437260663, 0.0, 1.0, 39145.44774014597], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4848000.0000, 
sim time next is 4848600.0000, 
raw observation next is [-2.833333333333333, 60.0, 0.0, 0.0, 26.0, 25.04539548476504, 0.2885676417745245, 0.0, 1.0, 39157.7768666097], 
processed observation next is [0.0, 0.08695652173913043, 0.3841181902123731, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5871162903970868, 0.5961892139248415, 0.0, 1.0, 0.18646560412671287], 
reward next is 0.8135, 
noisyNet noise sample is [array([1.0132089], dtype=float32), 1.1377153]. 
=============================================
[2019-04-04 14:52:12,538] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0956313e-09 7.6284584e-10 1.0128341e-15 3.0354370e-15 1.0000000e+00
 1.2349060e-11 5.0699216e-15], sum to 1.0000
[2019-04-04 14:52:12,538] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6871
[2019-04-04 14:52:12,552] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.9833333333333334, 73.0, 0.0, 0.0, 26.0, 25.46629362552658, 0.4422800389057584, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4504200.0000, 
sim time next is 4504800.0000, 
raw observation next is [-0.9666666666666668, 73.0, 0.0, 0.0, 26.0, 25.43811425725972, 0.4296217308623509, 0.0, 1.0, 32941.39579571734], 
processed observation next is [1.0, 0.13043478260869565, 0.43582640812557716, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6198428547716434, 0.6432072436207836, 0.0, 1.0, 0.1568637895034159], 
reward next is 0.8431, 
noisyNet noise sample is [array([0.68675435], dtype=float32), -1.330926]. 
=============================================
[2019-04-04 14:52:12,586] A3C_AGENT_WORKER-Thread-5 INFO:Local step 177500, global step 2842351: loss 4.6605
[2019-04-04 14:52:12,588] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 177500, global step 2842351: learning rate 0.0000
[2019-04-04 14:52:14,349] A3C_AGENT_WORKER-Thread-12 INFO:Local step 177500, global step 2843275: loss 4.4944
[2019-04-04 14:52:14,352] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 177500, global step 2843275: learning rate 0.0000
[2019-04-04 14:52:14,900] A3C_AGENT_WORKER-Thread-6 INFO:Local step 177500, global step 2843576: loss 4.5465
[2019-04-04 14:52:14,910] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 177500, global step 2843581: learning rate 0.0000
[2019-04-04 14:52:15,353] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3875805e-09 5.5650137e-11 1.2445533e-15 6.9270918e-15 1.0000000e+00
 1.6117035e-10 5.2870468e-16], sum to 1.0000
[2019-04-04 14:52:15,359] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5533
[2019-04-04 14:52:15,371] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.545521249444, 0.6436956238116619, 0.0, 1.0, 40165.55104516525], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4654200.0000, 
sim time next is 4654800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.7695827081558, 0.6586786161731017, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6474652256796499, 0.7195595387243673, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.8081558], dtype=float32), -0.89045364]. 
=============================================
[2019-04-04 14:52:17,240] A3C_AGENT_WORKER-Thread-10 INFO:Local step 177500, global step 2844837: loss 3.6963
[2019-04-04 14:52:17,243] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 177500, global step 2844839: learning rate 0.0000
[2019-04-04 14:52:18,273] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1639404e-09 1.3123692e-10 1.4336522e-16 7.8032673e-15 1.0000000e+00
 4.7431333e-11 8.9992317e-16], sum to 1.0000
[2019-04-04 14:52:18,273] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6866
[2019-04-04 14:52:18,318] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.166666666666667, 40.16666666666666, 98.0, 612.3333333333333, 26.0, 25.52267664240544, 0.3810526292777416, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4956600.0000, 
sim time next is 4957200.0000, 
raw observation next is [-1.0, 39.0, 100.5, 638.5, 26.0, 25.70442603465107, 0.410266860872363, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.39, 0.335, 0.7055248618784531, 0.6666666666666666, 0.6420355028875893, 0.6367556202907877, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2606013], dtype=float32), -1.9730135]. 
=============================================
[2019-04-04 14:52:18,799] A3C_AGENT_WORKER-Thread-11 INFO:Local step 178000, global step 2845661: loss 0.0139
[2019-04-04 14:52:18,800] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 178000, global step 2845661: learning rate 0.0000
[2019-04-04 14:52:18,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:18,840] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:18,855] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run22
[2019-04-04 14:52:23,474] A3C_AGENT_WORKER-Thread-16 INFO:Local step 178000, global step 2848000: loss 0.0133
[2019-04-04 14:52:23,475] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 178000, global step 2848000: learning rate 0.0000
[2019-04-04 14:52:23,683] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6553403e-09 6.2424302e-11 1.4097796e-15 2.0870032e-15 1.0000000e+00
 9.0135469e-11 9.9099234e-16], sum to 1.0000
[2019-04-04 14:52:23,684] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2395
[2019-04-04 14:52:23,695] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.333333333333334, 19.0, 0.0, 0.0, 26.0, 27.20472958487627, 0.8799126716425903, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5085600.0000, 
sim time next is 5086200.0000, 
raw observation next is [9.166666666666666, 19.0, 0.0, 0.0, 26.0, 27.17952451596737, 0.8681512344096975, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7165281625115422, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7649603763306141, 0.7893837448032325, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23681736], dtype=float32), -0.2704913]. 
=============================================
[2019-04-04 14:52:23,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:23,774] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:23,795] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run22
[2019-04-04 14:52:24,610] A3C_AGENT_WORKER-Thread-15 INFO:Local step 178000, global step 2848465: loss 0.0151
[2019-04-04 14:52:24,611] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 178000, global step 2848465: learning rate 0.0000
[2019-04-04 14:52:24,981] A3C_AGENT_WORKER-Thread-19 INFO:Local step 178000, global step 2848671: loss 0.0127
[2019-04-04 14:52:24,987] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 178000, global step 2848672: learning rate 0.0000
[2019-04-04 14:52:25,057] A3C_AGENT_WORKER-Thread-18 INFO:Local step 178000, global step 2848715: loss 0.0143
[2019-04-04 14:52:25,059] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 178000, global step 2848716: learning rate 0.0000
[2019-04-04 14:52:25,166] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:25,166] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:25,198] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run22
[2019-04-04 14:52:26,180] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3473060e-09 2.8802119e-10 2.3349713e-15 4.2462646e-14 1.0000000e+00
 5.0215238e-11 2.2847918e-15], sum to 1.0000
[2019-04-04 14:52:26,180] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1958
[2019-04-04 14:52:26,197] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.17913279593735, 0.3270825052428999, 0.0, 1.0, 39257.59662467085], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4843800.0000, 
sim time next is 4844400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.16256699116513, 0.3272705088100632, 0.0, 1.0, 39221.87645989926], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5968805825970941, 0.6090901696033544, 0.0, 1.0, 0.18677084028523455], 
reward next is 0.8132, 
noisyNet noise sample is [array([0.12612368], dtype=float32), -0.6134977]. 
=============================================
[2019-04-04 14:52:26,383] A3C_AGENT_WORKER-Thread-14 INFO:Local step 178000, global step 2849321: loss 0.0154
[2019-04-04 14:52:26,385] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 178000, global step 2849321: learning rate 0.0000
[2019-04-04 14:52:26,458] A3C_AGENT_WORKER-Thread-3 INFO:Local step 178000, global step 2849351: loss 0.0158
[2019-04-04 14:52:26,462] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 178000, global step 2849352: learning rate 0.0000
[2019-04-04 14:52:26,595] A3C_AGENT_WORKER-Thread-4 INFO:Local step 178000, global step 2849406: loss 0.0148
[2019-04-04 14:52:26,596] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 178000, global step 2849406: learning rate 0.0000
[2019-04-04 14:52:26,910] A3C_AGENT_WORKER-Thread-20 INFO:Local step 178000, global step 2849549: loss 0.0146
[2019-04-04 14:52:26,910] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 178000, global step 2849549: learning rate 0.0000
[2019-04-04 14:52:27,244] A3C_AGENT_WORKER-Thread-5 INFO:Local step 178000, global step 2849715: loss 0.0130
[2019-04-04 14:52:27,261] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 178000, global step 2849721: learning rate 0.0000
[2019-04-04 14:52:28,237] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7325174e-08 1.2746348e-09 1.9030901e-14 1.6126113e-13 1.0000000e+00
 1.5116698e-09 4.1657948e-14], sum to 1.0000
[2019-04-04 14:52:28,238] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2529
[2019-04-04 14:52:28,269] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 44.66666666666667, 0.0, 0.0, 26.0, 25.08356310506536, 0.3223472927039878, 0.0, 1.0, 199029.9776359275], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4908000.0000, 
sim time next is 4908600.0000, 
raw observation next is [1.0, 43.5, 0.0, 0.0, 26.0, 25.1273595740212, 0.3479729467206877, 0.0, 1.0, 152483.8535626254], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.435, 0.0, 0.0, 0.6666666666666666, 0.5939466311684333, 0.6159909822402292, 0.0, 1.0, 0.7261135883934542], 
reward next is 0.2739, 
noisyNet noise sample is [array([-0.3085103], dtype=float32), -1.0267296]. 
=============================================
[2019-04-04 14:52:29,217] A3C_AGENT_WORKER-Thread-12 INFO:Local step 178000, global step 2850568: loss 0.0167
[2019-04-04 14:52:29,217] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 178000, global step 2850568: learning rate 0.0000
[2019-04-04 14:52:29,614] A3C_AGENT_WORKER-Thread-6 INFO:Local step 178000, global step 2850769: loss 0.0141
[2019-04-04 14:52:29,614] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 178000, global step 2850769: learning rate 0.0000
[2019-04-04 14:52:31,279] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:31,280] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:31,287] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run22
[2019-04-04 14:52:31,788] A3C_AGENT_WORKER-Thread-10 INFO:Local step 178000, global step 2851937: loss 0.0147
[2019-04-04 14:52:31,790] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 178000, global step 2851937: learning rate 0.0000
[2019-04-04 14:52:32,494] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.5995041e-09 8.9040558e-10 1.9328750e-14 4.4719399e-14 1.0000000e+00
 1.9428559e-09 2.8209336e-14], sum to 1.0000
[2019-04-04 14:52:32,497] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9087
[2019-04-04 14:52:32,534] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 43.5, 0.0, 0.0, 26.0, 25.1273595740212, 0.3479729467206877, 0.0, 1.0, 152483.8535626254], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4908600.0000, 
sim time next is 4909200.0000, 
raw observation next is [1.0, 42.33333333333333, 0.0, 0.0, 26.0, 25.1776289685072, 0.3679485878425364, 0.0, 1.0, 82723.73819862817], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5981357473756, 0.6226495292808455, 0.0, 1.0, 0.39392256285061034], 
reward next is 0.6061, 
noisyNet noise sample is [array([0.51728946], dtype=float32), -0.9155686]. 
=============================================
[2019-04-04 14:52:32,814] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0553552e-09 6.9782441e-10 5.9081281e-15 1.4654448e-14 1.0000000e+00
 2.3539728e-10 1.0282975e-15], sum to 1.0000
[2019-04-04 14:52:32,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8491
[2019-04-04 14:52:32,840] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 34.5, 0.0, 0.0, 26.0, 25.45248938142927, 0.4858616626037445, 0.0, 1.0, 98184.25084607732], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5007000.0000, 
sim time next is 5007600.0000, 
raw observation next is [3.0, 34.0, 0.0, 0.0, 26.0, 25.47039796788152, 0.4961533300943957, 0.0, 1.0, 50500.06309161273], 
processed observation next is [1.0, 1.0, 0.5457063711911359, 0.34, 0.0, 0.0, 0.6666666666666666, 0.6225331639901267, 0.6653844433647985, 0.0, 1.0, 0.24047649091244155], 
reward next is 0.7595, 
noisyNet noise sample is [array([-0.3010009], dtype=float32), -0.80568284]. 
=============================================
[2019-04-04 14:52:35,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:35,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:35,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run22
[2019-04-04 14:52:36,484] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2392659e-08 1.4738945e-09 1.7756453e-14 4.4697593e-13 1.0000000e+00
 7.0065753e-09 3.5270919e-14], sum to 1.0000
[2019-04-04 14:52:36,485] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4877
[2019-04-04 14:52:36,522] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 26.0, 24.89818906104612, 0.2591329400580036, 0.0, 1.0, 42248.97912063688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 160800.0000, 
sim time next is 161400.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 26.0, 24.78196380954066, 0.237537150747084, 0.0, 1.0, 44877.90959167387], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5651636507950549, 0.579179050249028, 0.0, 1.0, 0.2137043313889232], 
reward next is 0.7863, 
noisyNet noise sample is [array([1.9430811], dtype=float32), -1.6051073]. 
=============================================
[2019-04-04 14:52:36,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:36,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:36,935] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run22
[2019-04-04 14:52:37,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:37,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:37,314] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run22
[2019-04-04 14:52:37,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:37,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:37,607] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run22
[2019-04-04 14:52:38,386] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:38,386] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:38,390] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run22
[2019-04-04 14:52:38,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:38,412] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:38,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run22
[2019-04-04 14:52:38,889] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:38,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:38,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run22
[2019-04-04 14:52:39,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:39,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:39,024] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run22
[2019-04-04 14:52:39,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:39,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:39,116] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run22
[2019-04-04 14:52:41,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:41,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:41,166] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run22
[2019-04-04 14:52:41,914] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:41,915] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:41,919] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run22
[2019-04-04 14:52:43,583] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:52:43,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:52:43,587] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run22
[2019-04-04 14:52:51,492] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.8240347e-10 5.4487231e-11 2.7096460e-16 2.8897903e-15 1.0000000e+00
 2.0132361e-11 2.3027806e-16], sum to 1.0000
[2019-04-04 14:52:51,492] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0394
[2019-04-04 14:52:51,545] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.683333333333334, 85.33333333333334, 12.0, 0.0, 26.0, 24.5469458818266, 0.1885009285325525, 0.0, 1.0, 34892.5448199301], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 60600.0000, 
sim time next is 61200.0000, 
raw observation next is [5.5, 86.0, 0.0, 0.0, 26.0, 24.54547892731366, 0.1942513450597224, 0.0, 1.0, 42178.2362889414], 
processed observation next is [0.0, 0.7391304347826086, 0.6149584487534627, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5454565772761383, 0.5647504483532408, 0.0, 1.0, 0.2008487442330543], 
reward next is 0.7992, 
noisyNet noise sample is [array([1.0808134], dtype=float32), 0.059626]. 
=============================================
[2019-04-04 14:52:52,559] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0826764e-09 9.1251902e-11 6.1670997e-16 6.2505936e-14 1.0000000e+00
 5.4712904e-11 8.6009287e-16], sum to 1.0000
[2019-04-04 14:52:52,559] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6604
[2019-04-04 14:52:52,613] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.8, 61.0, 41.0, 4.5, 26.0, 25.37777015165775, 0.2564031532495906, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 118800.0000, 
sim time next is 119400.0000, 
raw observation next is [-7.8, 63.16666666666667, 42.33333333333334, 2.999999999999999, 26.0, 25.31498845580347, 0.2533325595360833, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.6316666666666667, 0.14111111111111113, 0.0033149171270718224, 0.6666666666666666, 0.6095823713169558, 0.5844441865120278, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3589362], dtype=float32), 0.13885418]. 
=============================================
[2019-04-04 14:52:55,406] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9015190e-09 7.6343931e-11 3.1242747e-15 9.6219749e-15 1.0000000e+00
 4.4743123e-10 1.7247565e-15], sum to 1.0000
[2019-04-04 14:52:55,406] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7951
[2019-04-04 14:52:55,467] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.95, 63.5, 149.0, 0.0, 26.0, 24.38140286904173, 0.2867422225012413, 1.0, 1.0, 200978.3102003609], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 221400.0000, 
sim time next is 222000.0000, 
raw observation next is [-3.766666666666667, 63.0, 143.6666666666667, 0.0, 26.0, 25.30132555318309, 0.3540762950072152, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.358264081255771, 0.63, 0.47888888888888903, 0.0, 0.6666666666666666, 0.6084437960985909, 0.6180254316690718, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8787124], dtype=float32), -0.05480207]. 
=============================================
[2019-04-04 14:52:55,471] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[85.49646 ]
 [84.5193  ]
 [83.68477 ]
 [83.83757 ]
 [83.865326]], R is [[85.50041962]
 [84.68837738]
 [83.89131165]
 [83.96324921]
 [83.95184326]].
[2019-04-04 14:53:02,624] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1881363e-08 1.8196894e-10 1.0546288e-14 1.9583242e-14 1.0000000e+00
 4.0086676e-10 1.0265931e-14], sum to 1.0000
[2019-04-04 14:53:02,624] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4141
[2019-04-04 14:53:02,678] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.46750723716804, 0.3284270612536845, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 235800.0000, 
sim time next is 236400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.5810903305532, 0.2958926067004226, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6317575275460999, 0.5986308689001408, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11609463], dtype=float32), 1.4059933]. 
=============================================
[2019-04-04 14:53:07,328] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1722442e-08 8.5962976e-10 1.3546688e-14 2.4619139e-13 1.0000000e+00
 4.1639286e-10 9.2503985e-15], sum to 1.0000
[2019-04-04 14:53:07,329] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9832
[2019-04-04 14:53:07,341] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.233333333333334, 75.0, 0.0, 0.0, 26.0, 24.08030377765363, 0.08286188754839245, 0.0, 1.0, 44431.9436983023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 260400.0000, 
sim time next is 261000.0000, 
raw observation next is [-5.6, 73.0, 0.0, 0.0, 26.0, 24.09647015975025, 0.08485679099126786, 0.0, 1.0, 44487.28856982183], 
processed observation next is [1.0, 0.0, 0.30747922437673136, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5080391799791876, 0.5282855969970893, 0.0, 1.0, 0.21184423128486585], 
reward next is 0.7882, 
noisyNet noise sample is [array([0.5821456], dtype=float32), -0.096057974]. 
=============================================
[2019-04-04 14:53:07,344] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[78.97342 ]
 [78.93158 ]
 [79.06993 ]
 [79.31363 ]
 [79.330154]], R is [[78.97977448]
 [78.97840118]
 [78.9772644 ]
 [78.97624207]
 [78.97525787]].
[2019-04-04 14:53:07,969] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.85211633e-08 6.05402484e-09 1.10314014e-13 4.11853841e-12
 1.00000000e+00 9.66215108e-09 1.53760045e-12], sum to 1.0000
[2019-04-04 14:53:07,971] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5782
[2019-04-04 14:53:07,984] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.866666666666667, 69.0, 0.0, 0.0, 26.0, 23.29236422995781, -0.1181186900576482, 0.0, 1.0, 46905.92798448963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 274800.0000, 
sim time next is 275400.0000, 
raw observation next is [-10.05, 68.5, 0.0, 0.0, 26.0, 23.20707837092602, -0.1286163939974728, 0.0, 1.0, 47094.53639673702], 
processed observation next is [1.0, 0.17391304347826086, 0.18421052631578946, 0.685, 0.0, 0.0, 0.6666666666666666, 0.4339231975771683, 0.4571278686675091, 0.0, 1.0, 0.22425969712731914], 
reward next is 0.7757, 
noisyNet noise sample is [array([0.2961488], dtype=float32), 1.1255922]. 
=============================================
[2019-04-04 14:53:12,735] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.5491686e-08 6.3873999e-09 8.3686741e-13 1.4739546e-11 9.9999988e-01
 8.7587946e-09 3.9510314e-13], sum to 1.0000
[2019-04-04 14:53:12,735] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0391
[2019-04-04 14:53:12,754] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.7, 73.83333333333334, 0.0, 0.0, 26.0, 22.18612260685111, -0.3753777570848694, 0.0, 1.0, 48669.6979607547], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 364200.0000, 
sim time next is 364800.0000, 
raw observation next is [-15.8, 74.66666666666667, 0.0, 0.0, 26.0, 22.18231208342469, -0.3911271355350534, 0.0, 1.0, 48635.37910432401], 
processed observation next is [1.0, 0.21739130434782608, 0.024930747922437636, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.34852600695205754, 0.3696242881549822, 0.0, 1.0, 0.23159704335392386], 
reward next is 0.7684, 
noisyNet noise sample is [array([-1.2645772], dtype=float32), 0.966086]. 
=============================================
[2019-04-04 14:53:35,505] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.2665711e-10 5.8428956e-11 1.1885712e-16 4.9177996e-15 1.0000000e+00
 1.5605940e-11 1.1958660e-16], sum to 1.0000
[2019-04-04 14:53:35,507] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0547
[2019-04-04 14:53:35,543] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.066666666666667, 80.33333333333333, 131.3333333333333, 429.1666666666666, 26.0, 24.94283215970609, 0.3224784413216204, 0.0, 1.0, 33526.31549809976], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 564000.0000, 
sim time next is 564600.0000, 
raw observation next is [-1.133333333333333, 80.16666666666667, 132.6666666666667, 462.3333333333334, 26.0, 24.97541261543923, 0.3244839735495748, 0.0, 1.0, 18727.44136028136], 
processed observation next is [0.0, 0.5217391304347826, 0.43120960295475536, 0.8016666666666667, 0.4422222222222224, 0.5108655616942911, 0.6666666666666666, 0.5812843846199357, 0.6081613245165249, 0.0, 1.0, 0.08917829219181599], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.42183185], dtype=float32), 0.755599]. 
=============================================
[2019-04-04 14:53:36,846] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8722336e-09 1.0727730e-09 5.0667317e-15 2.0521272e-13 1.0000000e+00
 3.1390706e-11 1.1892164e-14], sum to 1.0000
[2019-04-04 14:53:36,847] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4633
[2019-04-04 14:53:36,892] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 66.0, 123.3333333333333, 34.00000000000001, 26.0, 25.05341158806157, 0.2119254359453454, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 640200.0000, 
sim time next is 640800.0000, 
raw observation next is [-3.9, 65.0, 117.5, 25.5, 26.0, 24.99688976226848, 0.192638267745959, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3545706371191136, 0.65, 0.39166666666666666, 0.0281767955801105, 0.6666666666666666, 0.5830741468557067, 0.5642127559153197, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3513853], dtype=float32), -0.095237255]. 
=============================================
[2019-04-04 14:53:44,075] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.4746060e-10 3.6216793e-11 2.1017747e-16 5.1144434e-15 1.0000000e+00
 2.8098437e-11 4.1939653e-16], sum to 1.0000
[2019-04-04 14:53:44,076] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7124
[2019-04-04 14:53:44,091] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.68333333333333, 76.0, 0.0, 0.0, 26.0, 25.64903596282402, 0.6424800235069441, 0.0, 1.0, 34436.71484656326], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1126200.0000, 
sim time next is 1126800.0000, 
raw observation next is [10.5, 77.0, 0.0, 0.0, 26.0, 25.65045937549521, 0.6422828976168052, 0.0, 1.0, 27005.70634228583], 
processed observation next is [0.0, 0.043478260869565216, 0.7534626038781165, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6375382812912674, 0.7140942992056017, 0.0, 1.0, 0.12859860162993253], 
reward next is 0.8714, 
noisyNet noise sample is [array([0.12333222], dtype=float32), -0.53905046]. 
=============================================
[2019-04-04 14:53:53,128] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5452106e-09 1.5290448e-10 2.9103991e-16 3.5595641e-15 1.0000000e+00
 9.8029418e-11 1.0112127e-15], sum to 1.0000
[2019-04-04 14:53:53,129] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8399
[2019-04-04 14:53:53,154] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 79.0, 0.0, 0.0, 26.0, 24.79141964176652, 0.241999522460024, 0.0, 1.0, 41034.75110482295], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 860400.0000, 
sim time next is 861000.0000, 
raw observation next is [-2.716666666666667, 79.16666666666667, 0.0, 0.0, 26.0, 24.76246307863278, 0.2359219821789847, 0.0, 1.0, 40916.21507934703], 
processed observation next is [1.0, 1.0, 0.3873499538319483, 0.7916666666666667, 0.0, 0.0, 0.6666666666666666, 0.5635385898860651, 0.5786406607263282, 0.0, 1.0, 0.19483911942546203], 
reward next is 0.8052, 
noisyNet noise sample is [array([0.27825442], dtype=float32), 1.2888811]. 
=============================================
[2019-04-04 14:53:53,167] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.53197]
 [82.54168]
 [82.52607]
 [82.63878]
 [82.55869]], R is [[82.54218292]
 [82.52135468]
 [82.50012207]
 [82.47847748]
 [82.45654297]].
[2019-04-04 14:53:57,141] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4238639e-08 2.5472442e-09 1.8479529e-15 7.9663645e-14 1.0000000e+00
 2.5350433e-10 1.2615226e-14], sum to 1.0000
[2019-04-04 14:53:57,141] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2652
[2019-04-04 14:53:57,221] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.57621626320504, 0.237173682257329, 1.0, 1.0, 95137.9096368824], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 890400.0000, 
sim time next is 891000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.99289755714832, 0.2482116820843802, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5827414630956934, 0.58273722736146, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40196037], dtype=float32), -0.90753657]. 
=============================================
[2019-04-04 14:53:57,232] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[83.32557 ]
 [81.845795]
 [81.860985]
 [81.862206]
 [81.89182 ]], R is [[84.44658661]
 [84.14907837]
 [84.12461853]
 [84.10031891]
 [84.07601929]].
[2019-04-04 14:53:58,331] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7150973e-10 1.7358854e-11 1.1630282e-16 3.6088927e-16 1.0000000e+00
 1.9606258e-11 9.1817286e-17], sum to 1.0000
[2019-04-04 14:53:58,336] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0959
[2019-04-04 14:53:58,360] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.8, 86.33333333333334, 0.0, 0.0, 26.0, 25.48746359145792, 0.4579219130374639, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 976800.0000, 
sim time next is 977400.0000, 
raw observation next is [9.7, 88.0, 0.0, 0.0, 26.0, 25.48273684387991, 0.4528171909322495, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.7313019390581719, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6235614036566591, 0.6509390636440832, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8030987], dtype=float32), -0.37439892]. 
=============================================
[2019-04-04 14:53:59,503] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9747715e-09 6.9279381e-11 1.6457922e-16 2.0193081e-15 1.0000000e+00
 8.4777818e-12 2.3117318e-16], sum to 1.0000
[2019-04-04 14:53:59,505] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2167
[2019-04-04 14:53:59,568] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 26.0, 25.20790694910162, 0.4588175174075822, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1495200.0000, 
sim time next is 1495800.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 26.0, 25.49721543462845, 0.4669732043185448, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6247679528857043, 0.6556577347728483, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12498412], dtype=float32), 0.09900059]. 
=============================================
[2019-04-04 14:54:02,104] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9042994e-10 6.3034343e-12 5.1957692e-18 9.0690526e-16 1.0000000e+00
 1.9195626e-12 1.2335493e-16], sum to 1.0000
[2019-04-04 14:54:02,105] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5245
[2019-04-04 14:54:02,118] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.45, 65.0, 0.0, 0.0, 26.0, 25.71063730697841, 0.6430273188157681, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1117800.0000, 
sim time next is 1118400.0000, 
raw observation next is [12.36666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.66313727793922, 0.6446114174363297, 0.0, 1.0, 111212.0262077389], 
processed observation next is [1.0, 0.9565217391304348, 0.8051708217913206, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6385947731616017, 0.7148704724787766, 0.0, 1.0, 0.529581077179709], 
reward next is 0.4704, 
noisyNet noise sample is [array([-0.44697392], dtype=float32), 0.38000488]. 
=============================================
[2019-04-04 14:54:27,459] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6881898e-10 4.3920981e-11 1.3919997e-16 7.5823637e-16 1.0000000e+00
 2.9218530e-11 2.7014102e-16], sum to 1.0000
[2019-04-04 14:54:27,460] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0791
[2019-04-04 14:54:27,475] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.08333333333333333, 94.5, 0.0, 0.0, 26.0, 25.37464872505003, 0.4818022825432731, 0.0, 1.0, 52911.2814797645], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1725000.0000, 
sim time next is 1725600.0000, 
raw observation next is [0.1666666666666667, 94.0, 0.0, 0.0, 26.0, 25.3617482416125, 0.4809118257551133, 0.0, 1.0, 49512.97652019529], 
processed observation next is [1.0, 1.0, 0.4672206832871654, 0.94, 0.0, 0.0, 0.6666666666666666, 0.6134790201343749, 0.660303941918371, 0.0, 1.0, 0.23577607866759662], 
reward next is 0.7642, 
noisyNet noise sample is [array([1.5883806], dtype=float32), -0.6986786]. 
=============================================
[2019-04-04 14:54:28,043] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-04 14:54:28,046] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 14:54:28,047] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:54:28,047] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 14:54:28,048] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 14:54:28,049] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:54:28,049] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:54:28,054] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run30
[2019-04-04 14:54:28,076] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run30
[2019-04-04 14:54:28,097] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run30
[2019-04-04 14:55:41,764] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17557575], dtype=float32), 0.21720548]
[2019-04-04 14:55:41,765] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-12.26666666666667, 80.66666666666666, 0.0, 0.0, 26.0, 24.06691909719467, 0.1011941984660853, 0.0, 1.0, 44448.26075201677]
[2019-04-04 14:55:41,765] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 14:55:41,766] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.46542405e-08 3.22762461e-09 4.74617599e-14 7.23609161e-13
 1.00000000e+00 1.26479760e-09 9.57631733e-14], sampled 0.3098534597016652
[2019-04-04 14:55:44,525] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17557575], dtype=float32), 0.21720548]
[2019-04-04 14:55:44,525] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-7.0, 64.0, 0.0, 0.0, 26.0, 23.6807649175081, -0.0415527995498681, 0.0, 1.0, 63098.7035846034]
[2019-04-04 14:55:44,526] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 14:55:44,527] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.3421698e-08 3.9017332e-09 8.6373020e-14 7.6326380e-13 1.0000000e+00
 1.5252821e-09 1.4194512e-13], sampled 0.5086910082487618
[2019-04-04 14:56:10,111] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 14:56:29,060] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6609 263411213.4411 1551.8501
[2019-04-04 14:56:33,114] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 14:56:34,143] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 2900000, evaluation results [2900000.0, 7241.660888375711, 263411213.4411044, 1551.8501036839202, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 14:56:37,014] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.1203276e-09 8.4955643e-10 2.7342037e-15 1.1240805e-13 1.0000000e+00
 1.0496005e-09 5.9124824e-14], sum to 1.0000
[2019-04-04 14:56:37,014] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3364
[2019-04-04 14:56:37,056] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 83.0, 109.0, 0.0, 26.0, 25.06023731627929, 0.3534792832365107, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1778400.0000, 
sim time next is 1779000.0000, 
raw observation next is [-2.8, 83.66666666666667, 105.6666666666667, 0.0, 26.0, 25.0435470227132, 0.3414249999664768, 0.0, 1.0, 27091.36919827135], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8366666666666667, 0.3522222222222223, 0.0, 0.6666666666666666, 0.5869622518927665, 0.6138083333221589, 0.0, 1.0, 0.12900651999176832], 
reward next is 0.8710, 
noisyNet noise sample is [array([0.45005932], dtype=float32), 0.35849026]. 
=============================================
[2019-04-04 14:56:37,059] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.870384]
 [83.240005]
 [83.511215]
 [83.61795 ]
 [83.692444]], R is [[82.67160797]
 [82.84489441]
 [82.92723846]
 [82.8610611 ]
 [82.77135468]].
[2019-04-04 14:56:47,769] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0251627e-09 9.2088581e-10 4.2936045e-15 1.2757699e-14 1.0000000e+00
 3.7409298e-11 1.4635928e-14], sum to 1.0000
[2019-04-04 14:56:47,769] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9795
[2019-04-04 14:56:47,815] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.416666666666667, 85.5, 0.0, 0.0, 26.0, 25.03946494283571, 0.2446656188896683, 0.0, 1.0, 47309.83112551711], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1885800.0000, 
sim time next is 1886400.0000, 
raw observation next is [-5.6, 86.0, 0.0, 0.0, 26.0, 25.02800880712413, 0.2376209972204582, 0.0, 1.0, 45327.08267313833], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5856674005936776, 0.579206999073486, 0.0, 1.0, 0.21584325082446823], 
reward next is 0.7842, 
noisyNet noise sample is [array([-0.95936286], dtype=float32), 1.347233]. 
=============================================
[2019-04-04 14:56:48,501] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4087084e-09 2.6882857e-10 2.4060249e-14 8.4475897e-14 1.0000000e+00
 6.1680944e-10 4.7060083e-14], sum to 1.0000
[2019-04-04 14:56:48,508] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7338
[2019-04-04 14:56:48,563] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 81.66666666666667, 19.66666666666667, 0.0, 26.0, 25.03009228933469, 0.2506010356942625, 0.0, 1.0, 46018.72325876683], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1875000.0000, 
sim time next is 1875600.0000, 
raw observation next is [-4.5, 83.0, 15.0, 0.0, 26.0, 25.01208707985355, 0.2497854512403863, 0.0, 1.0, 54554.82386625389], 
processed observation next is [0.0, 0.7391304347826086, 0.3379501385041552, 0.83, 0.05, 0.0, 0.6666666666666666, 0.5843405899877959, 0.5832618170801288, 0.0, 1.0, 0.25978487555359], 
reward next is 0.7402, 
noisyNet noise sample is [array([-1.6333383], dtype=float32), 0.85921055]. 
=============================================
[2019-04-04 14:56:53,501] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7686616e-07 2.0406214e-08 5.4218349e-13 1.3617810e-12 9.9999976e-01
 1.8777692e-08 7.4633106e-13], sum to 1.0000
[2019-04-04 14:56:53,502] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5523
[2019-04-04 14:56:53,519] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 26.0, 23.3619681558881, -0.1522893532927724, 0.0, 1.0, 44855.45430132446], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1922400.0000, 
sim time next is 1923000.0000, 
raw observation next is [-9.0, 83.50000000000001, 0.0, 0.0, 26.0, 23.28357726864437, -0.158506973822186, 0.0, 1.0, 44772.84372530266], 
processed observation next is [1.0, 0.2608695652173913, 0.21329639889196678, 0.8350000000000002, 0.0, 0.0, 0.6666666666666666, 0.44029810572036404, 0.4471643420592713, 0.0, 1.0, 0.21320401773953646], 
reward next is 0.7868, 
noisyNet noise sample is [array([1.2171534], dtype=float32), 1.2531229]. 
=============================================
[2019-04-04 14:56:53,546] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[72.834854]
 [72.899925]
 [72.97444 ]
 [73.03665 ]
 [73.105354]], R is [[72.81157684]
 [72.86986542]
 [72.92721558]
 [72.9836731 ]
 [73.03920746]].
[2019-04-04 14:57:02,445] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1573800e-08 2.6248759e-09 8.6836517e-14 3.2633415e-13 9.9999988e-01
 1.1971801e-09 2.2623644e-14], sum to 1.0000
[2019-04-04 14:57:02,448] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2816
[2019-04-04 14:57:02,461] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 65.0, 0.0, 0.0, 26.0, 24.12885988991154, 0.1025527852471642, 0.0, 1.0, 41193.98296353242], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2354400.0000, 
sim time next is 2355000.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.13852176511102, 0.1116472027223836, 0.0, 1.0, 41189.56664681783], 
processed observation next is [0.0, 0.2608695652173913, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5115434804259182, 0.5372157342407945, 0.0, 1.0, 0.1961407935562754], 
reward next is 0.8039, 
noisyNet noise sample is [array([0.9272119], dtype=float32), 0.5799559]. 
=============================================
[2019-04-04 14:57:02,464] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.824104]
 [75.88075 ]
 [75.95709 ]
 [76.02315 ]
 [76.08162 ]], R is [[75.81746674]
 [75.86313629]
 [75.90843964]
 [75.95345306]
 [75.99823761]].
[2019-04-04 14:57:18,313] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.2157608e-10 2.2427936e-11 1.6336708e-15 6.8646426e-15 1.0000000e+00
 3.3947936e-10 6.6110875e-15], sum to 1.0000
[2019-04-04 14:57:18,314] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6374
[2019-04-04 14:57:18,345] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 68.0, 116.3333333333333, 190.0, 26.0, 25.94634151291178, 0.4802028102548818, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2217000.0000, 
sim time next is 2217600.0000, 
raw observation next is [-3.9, 68.0, 96.0, 142.5, 26.0, 26.12841062569537, 0.4845066883743223, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.68, 0.32, 0.1574585635359116, 0.6666666666666666, 0.6773675521412809, 0.6615022294581074, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6951699], dtype=float32), 0.4668326]. 
=============================================
[2019-04-04 14:57:18,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1131828e-08 1.4276793e-08 4.9239990e-13 6.0751686e-13 1.0000000e+00
 2.4478393e-09 2.2717273e-13], sum to 1.0000
[2019-04-04 14:57:18,445] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9897
[2019-04-04 14:57:18,482] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.3, 25.5, 48.0, 279.0, 26.0, 24.96438678702562, 0.266874311260227, 0.0, 1.0, 18698.18917411887], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2478600.0000, 
sim time next is 2479200.0000, 
raw observation next is [3.3, 25.33333333333333, 41.0, 239.6666666666667, 26.0, 25.05171183743301, 0.2615098697912917, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.554016620498615, 0.2533333333333333, 0.13666666666666666, 0.2648250460405157, 0.6666666666666666, 0.5876426531194175, 0.5871699565970973, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7387387], dtype=float32), 0.14833441]. 
=============================================
[2019-04-04 14:57:19,315] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.3679255e-10 1.9846955e-10 8.3631570e-16 5.9689073e-15 1.0000000e+00
 6.0616907e-11 2.7117777e-15], sum to 1.0000
[2019-04-04 14:57:19,318] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9431
[2019-04-04 14:57:19,375] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.316666666666666, 90.33333333333334, 31.0, 17.33333333333333, 26.0, 25.13768839833951, 0.2469509821423297, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2275800.0000, 
sim time next is 2276400.0000, 
raw observation next is [-9.133333333333333, 89.66666666666667, 38.0, 16.66666666666667, 26.0, 25.17948808991788, 0.2729810924012594, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.20960295475530935, 0.8966666666666667, 0.12666666666666668, 0.018416206261510134, 0.6666666666666666, 0.5982906741598232, 0.5909936974670865, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48197618], dtype=float32), 0.84712994]. 
=============================================
[2019-04-04 14:57:21,536] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2670497e-08 8.6054514e-10 1.5757503e-14 4.1675612e-13 1.0000000e+00
 1.0761709e-09 1.0382471e-13], sum to 1.0000
[2019-04-04 14:57:21,536] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1536
[2019-04-04 14:57:21,557] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.8, 56.5, 0.0, 0.0, 26.0, 25.35341361404535, 0.3815293569146984, 0.0, 1.0, 42468.76921091427], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2326200.0000, 
sim time next is 2326800.0000, 
raw observation next is [-1.9, 57.0, 0.0, 0.0, 26.0, 25.3755546684334, 0.3752597440895485, 0.0, 1.0, 37860.41755255106], 
processed observation next is [1.0, 0.9565217391304348, 0.4099722991689751, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6146295557027832, 0.6250865813631828, 0.0, 1.0, 0.1802877026311955], 
reward next is 0.8197, 
noisyNet noise sample is [array([0.39678627], dtype=float32), 0.6853158]. 
=============================================
[2019-04-04 14:57:35,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0810502e-09 6.0685423e-10 4.2953434e-14 6.0527972e-14 1.0000000e+00
 2.0022666e-09 1.9194727e-14], sum to 1.0000
[2019-04-04 14:57:35,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0182
[2019-04-04 14:57:35,755] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.9, 29.0, 59.5, 135.8333333333333, 26.0, 25.50259660002002, 0.379605639125335, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2565600.0000, 
sim time next is 2566200.0000, 
raw observation next is [2.8, 29.0, 49.00000000000001, 109.6666666666667, 26.0, 25.66048932729607, 0.3880590228092285, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5401662049861496, 0.29, 0.16333333333333336, 0.12117863720073668, 0.6666666666666666, 0.6383741106080058, 0.6293530076030761, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2811435], dtype=float32), -0.7940079]. 
=============================================
[2019-04-04 14:57:36,475] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6735652e-10 5.3977456e-12 7.2128685e-18 1.3842746e-15 1.0000000e+00
 8.8991236e-12 1.2543419e-17], sum to 1.0000
[2019-04-04 14:57:36,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9420
[2019-04-04 14:57:36,495] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1666666666666667, 98.83333333333334, 68.33333333333334, 0.0, 26.0, 25.41907241417919, 0.3062062495894345, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2887800.0000, 
sim time next is 2888400.0000, 
raw observation next is [0.3333333333333333, 97.66666666666667, 73.16666666666666, 0.0, 26.0, 25.42012149460341, 0.3091400285669926, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.4718374884579871, 0.9766666666666667, 0.24388888888888885, 0.0, 0.6666666666666666, 0.6183434578836176, 0.6030466761889975, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.17977796], dtype=float32), -1.4866492]. 
=============================================
[2019-04-04 14:57:39,386] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.4729661e-10 5.2081901e-11 9.6227040e-16 2.4498507e-14 1.0000000e+00
 1.1608156e-10 7.6706303e-16], sum to 1.0000
[2019-04-04 14:57:39,387] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4790
[2019-04-04 14:57:39,395] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.466666666666667, 36.0, 220.3333333333333, 30.16666666666666, 26.0, 25.77237422866677, 0.3269630214220099, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2550000.0000, 
sim time next is 2550600.0000, 
raw observation next is [1.65, 34.5, 218.0, 22.0, 26.0, 25.73391524349543, 0.3141644975881102, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5083102493074793, 0.345, 0.7266666666666667, 0.02430939226519337, 0.6666666666666666, 0.6444929369579526, 0.6047214991960367, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34471133], dtype=float32), -1.0082737]. 
=============================================
[2019-04-04 14:57:43,601] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.3010168e-09 1.9275039e-10 9.3684402e-15 8.0881902e-14 1.0000000e+00
 7.4806539e-10 6.4304831e-15], sum to 1.0000
[2019-04-04 14:57:43,602] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3421
[2019-04-04 14:57:43,682] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.08338540564873, 0.1731902849977351, 1.0, 1.0, 203354.0294667885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2619000.0000, 
sim time next is 2619600.0000, 
raw observation next is [-7.3, 79.0, 18.66666666666666, 6.666666666666667, 26.0, 24.73504240433225, 0.2646438083191745, 0.0, 1.0, 88099.086109408], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.79, 0.0622222222222222, 0.007366482504604052, 0.6666666666666666, 0.5612535336943543, 0.5882146027730581, 0.0, 1.0, 0.41951945766384763], 
reward next is 0.5805, 
noisyNet noise sample is [array([-0.39768422], dtype=float32), -1.8507031]. 
=============================================
[2019-04-04 14:57:44,737] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.7429394e-09 3.9792095e-10 6.5013709e-15 8.3562457e-14 1.0000000e+00
 9.4718074e-11 1.8796703e-14], sum to 1.0000
[2019-04-04 14:57:44,738] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5219
[2019-04-04 14:57:44,759] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 65.0, 0.0, 0.0, 26.0, 25.34081735309641, 0.3643941535343675, 0.0, 1.0, 40590.78098485491], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3012000.0000, 
sim time next is 3012600.0000, 
raw observation next is [-3.416666666666667, 65.0, 0.0, 0.0, 26.0, 25.30424352046892, 0.3596199526588061, 0.0, 1.0, 40219.5276992467], 
processed observation next is [0.0, 0.8695652173913043, 0.36795937211449675, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6086869600390766, 0.6198733175529353, 0.0, 1.0, 0.19152156047260332], 
reward next is 0.8085, 
noisyNet noise sample is [array([1.2757115], dtype=float32), 0.6316469]. 
=============================================
[2019-04-04 14:57:46,741] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8122806e-09 5.1958698e-10 5.2381093e-14 1.1253505e-13 1.0000000e+00
 1.1870370e-09 1.8328666e-14], sum to 1.0000
[2019-04-04 14:57:46,741] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6315
[2019-04-04 14:57:46,793] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 62.5, 0.0, 0.0, 26.0, 24.9714935240008, 0.3584046472876619, 0.0, 1.0, 81202.30901040214], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2663400.0000, 
sim time next is 2664000.0000, 
raw observation next is [-1.2, 63.0, 0.0, 0.0, 26.0, 24.95541146598098, 0.372744739425137, 0.0, 1.0, 64368.91402166285], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5796176221650816, 0.6242482464750457, 0.0, 1.0, 0.30651863819839453], 
reward next is 0.6935, 
noisyNet noise sample is [array([-0.34471732], dtype=float32), -1.5119461]. 
=============================================
[2019-04-04 14:57:46,802] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[80.1104  ]
 [80.429985]
 [79.48782 ]
 [80.468895]
 [81.67096 ]], R is [[79.36440277]
 [79.18408203]
 [78.97733307]
 [79.06528473]
 [79.27463531]].
[2019-04-04 14:57:54,459] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.0884072e-10 3.7179822e-11 2.7465901e-16 2.1584331e-15 1.0000000e+00
 9.1654843e-11 4.3616328e-16], sum to 1.0000
[2019-04-04 14:57:54,459] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6472
[2019-04-04 14:57:54,480] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 85.0, 0.0, 0.0, 26.0, 25.38472892043007, 0.4873360889097043, 0.0, 1.0, 39295.17352968182], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2926800.0000, 
sim time next is 2927400.0000, 
raw observation next is [-1.0, 83.83333333333334, 0.0, 0.0, 26.0, 25.41388097072504, 0.4859555137164617, 0.0, 1.0, 25628.82130828057], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.8383333333333334, 0.0, 0.0, 0.6666666666666666, 0.6178234142270865, 0.6619851712388206, 0.0, 1.0, 0.12204200622990748], 
reward next is 0.8780, 
noisyNet noise sample is [array([-0.24470177], dtype=float32), -0.27728963]. 
=============================================
[2019-04-04 14:58:06,199] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.9600038e-10 1.3790365e-10 2.7804280e-17 1.7187149e-14 1.0000000e+00
 7.3103162e-11 2.6306607e-16], sum to 1.0000
[2019-04-04 14:58:06,200] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4537
[2019-04-04 14:58:06,215] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.3, 100.0, 0.0, 0.0, 26.0, 25.28184759918755, 0.2926945229583472, 0.0, 1.0, 62645.11110532508], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3123000.0000, 
sim time next is 3123600.0000, 
raw observation next is [2.4, 100.0, 0.0, 0.0, 26.0, 25.23396510280894, 0.2861296093204939, 0.0, 1.0, 56812.21408214215], 
processed observation next is [1.0, 0.13043478260869565, 0.5290858725761773, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6028304252340785, 0.5953765364401646, 0.0, 1.0, 0.27053435277210547], 
reward next is 0.7295, 
noisyNet noise sample is [array([-1.2300128], dtype=float32), -0.89621526]. 
=============================================
[2019-04-04 14:58:08,595] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3230469e-09 1.5809503e-09 1.7403217e-14 2.2941115e-14 1.0000000e+00
 8.7103114e-10 1.7589496e-14], sum to 1.0000
[2019-04-04 14:58:08,596] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3139
[2019-04-04 14:58:08,607] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 48.0, 60.0, 501.0, 26.0, 26.65204450166612, 0.6748029326196088, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3342600.0000, 
sim time next is 3343200.0000, 
raw observation next is [-2.0, 48.66666666666666, 51.83333333333334, 439.6666666666667, 26.0, 26.59953386540966, 0.5194392023937865, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.4866666666666666, 0.1727777777777778, 0.48581952117863725, 0.6666666666666666, 0.7166278221174718, 0.6731464007979288, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9279276], dtype=float32), -1.1349326]. 
=============================================
[2019-04-04 14:58:13,026] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.0477942e-10 4.9218164e-11 7.8506471e-16 1.6712450e-15 1.0000000e+00
 2.3620854e-11 5.1371426e-16], sum to 1.0000
[2019-04-04 14:58:13,026] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0592
[2019-04-04 14:58:13,044] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 25.35530897277023, 0.3151236837328787, 0.0, 1.0, 57221.16875474429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3133800.0000, 
sim time next is 3134400.0000, 
raw observation next is [5.333333333333334, 100.0, 0.0, 0.0, 26.0, 25.36794117606976, 0.3223340293665175, 0.0, 1.0, 53446.16133860867], 
processed observation next is [1.0, 0.2608695652173913, 0.6103416435826409, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6139950980058133, 0.6074446764555058, 0.0, 1.0, 0.2545055301838508], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.8907871], dtype=float32), -0.68033874]. 
=============================================
[2019-04-04 14:58:13,598] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.7011848e-09 3.3967107e-10 3.7275320e-15 3.5232661e-14 1.0000000e+00
 2.4014910e-10 6.0895596e-15], sum to 1.0000
[2019-04-04 14:58:13,599] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1801
[2019-04-04 14:58:13,621] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 84.0, 0.0, 0.0, 26.0, 25.13859801508339, 0.4207359596208319, 0.0, 1.0, 43414.50976217895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3283200.0000, 
sim time next is 3283800.0000, 
raw observation next is [-7.0, 81.66666666666667, 0.0, 0.0, 26.0, 25.1174928551908, 0.4135766580220039, 0.0, 1.0, 43491.05059411589], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.5931244045992333, 0.6378588860073346, 0.0, 1.0, 0.20710024092436138], 
reward next is 0.7929, 
noisyNet noise sample is [array([1.7029449], dtype=float32), 0.8806965]. 
=============================================
[2019-04-04 14:58:14,068] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0966542e-08 6.7130914e-09 3.4116248e-14 4.1139826e-13 1.0000000e+00
 1.1532951e-09 6.0402373e-14], sum to 1.0000
[2019-04-04 14:58:14,071] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6966
[2019-04-04 14:58:14,088] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 50.0, 116.0, 817.5, 26.0, 25.19923150197236, 0.4517134001026213, 0.0, 1.0, 18703.69111994542], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3589200.0000, 
sim time next is 3589800.0000, 
raw observation next is [-1.833333333333333, 48.66666666666667, 115.3333333333333, 815.6666666666666, 26.0, 25.20983452850533, 0.4567646748805766, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.41181902123730385, 0.4866666666666667, 0.3844444444444443, 0.9012891344383057, 0.6666666666666666, 0.6008195440421108, 0.6522548916268588, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8693606], dtype=float32), 0.4917749]. 
=============================================
[2019-04-04 14:58:15,898] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1255698e-10 4.0867179e-10 1.7243610e-16 8.4709808e-15 1.0000000e+00
 1.8412377e-10 5.8869325e-16], sum to 1.0000
[2019-04-04 14:58:15,898] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8684
[2019-04-04 14:58:15,914] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.666666666666667, 97.5, 107.6666666666667, 797.6666666666666, 26.0, 26.58372646744711, 0.7913281623780845, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3246600.0000, 
sim time next is 3247200.0000, 
raw observation next is [-4.0, 100.0, 106.0, 790.5, 26.0, 26.69081474203973, 0.8015375851617778, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3518005540166205, 1.0, 0.35333333333333333, 0.8734806629834254, 0.6666666666666666, 0.7242345618366443, 0.767179195053926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42033514], dtype=float32), -0.76396674]. 
=============================================
[2019-04-04 14:58:19,654] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0251248e-08 1.0613310e-09 5.8090807e-15 5.6638003e-14 1.0000000e+00
 6.5587180e-10 6.6767037e-15], sum to 1.0000
[2019-04-04 14:58:19,655] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6457
[2019-04-04 14:58:19,684] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.38606946151899, 0.4808139264237869, 0.0, 1.0, 65253.22067029626], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3277800.0000, 
sim time next is 3278400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.30924526711244, 0.4738241596721998, 0.0, 1.0, 61593.33307636363], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6091037722593701, 0.6579413865574, 0.0, 1.0, 0.29330158607792206], 
reward next is 0.7067, 
noisyNet noise sample is [array([0.05033864], dtype=float32), 0.8473002]. 
=============================================
[2019-04-04 14:58:20,833] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4572149e-08 9.0518509e-10 1.7583962e-14 2.7938087e-13 1.0000000e+00
 1.1747057e-09 1.0147529e-14], sum to 1.0000
[2019-04-04 14:58:20,834] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4324
[2019-04-04 14:58:20,866] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.75, 77.0, 0.0, 0.0, 26.0, 24.72040081842439, 0.2874614827958651, 0.0, 1.0, 43879.18453853389], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3297000.0000, 
sim time next is 3297600.0000, 
raw observation next is [-8.9, 77.0, 0.0, 0.0, 26.0, 24.76209344355879, 0.2665738756422346, 0.0, 1.0, 44206.37964100701], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5635077869632324, 0.5888579585474115, 0.0, 1.0, 0.21050656971908102], 
reward next is 0.7895, 
noisyNet noise sample is [array([-1.4633002], dtype=float32), 1.310541]. 
=============================================
[2019-04-04 14:58:26,381] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.7881916e-09 1.5932343e-09 7.2808330e-14 1.5840893e-13 1.0000000e+00
 6.7774963e-09 6.4163690e-15], sum to 1.0000
[2019-04-04 14:58:26,383] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4641
[2019-04-04 14:58:26,430] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.57171055093519, 0.5116365897872045, 1.0, 1.0, 31643.83781442364], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3352200.0000, 
sim time next is 3352800.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.54512288480654, 0.4952492178570551, 0.0, 1.0, 27162.38975319732], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6287602404005449, 0.6650830726190183, 0.0, 1.0, 0.12934471311046342], 
reward next is 0.8707, 
noisyNet noise sample is [array([-0.28394234], dtype=float32), 0.12200377]. 
=============================================
[2019-04-04 14:58:27,204] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4400573e-08 1.8859740e-09 1.4518603e-13 3.4238752e-13 1.0000000e+00
 6.5198907e-10 8.0529855e-14], sum to 1.0000
[2019-04-04 14:58:27,205] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0807
[2019-04-04 14:58:27,236] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.13544224658866, 0.3633313923654412, 0.0, 1.0, 18704.68788444652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3607800.0000, 
sim time next is 3608400.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.12718293125369, 0.3548594898511171, 0.0, 1.0, 18704.47277070349], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5939319109378074, 0.618286496617039, 0.0, 1.0, 0.08906891795573091], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.3494635], dtype=float32), -0.9782844]. 
=============================================
[2019-04-04 14:58:31,399] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.8649255e-09 5.0246904e-09 2.3599022e-14 1.4508914e-13 1.0000000e+00
 8.5747798e-10 1.8028544e-14], sum to 1.0000
[2019-04-04 14:58:31,400] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4854
[2019-04-04 14:58:31,416] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.333333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 25.31609199992329, 0.4377827125396521, 0.0, 1.0, 43878.46775088506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3547200.0000, 
sim time next is 3547800.0000, 
raw observation next is [-2.5, 65.5, 0.0, 0.0, 26.0, 25.29759628256678, 0.4333634725643812, 0.0, 1.0, 42249.19322023057], 
processed observation next is [0.0, 0.043478260869565216, 0.39335180055401664, 0.655, 0.0, 0.0, 0.6666666666666666, 0.6081330235472316, 0.6444544908547937, 0.0, 1.0, 0.20118663438205034], 
reward next is 0.7988, 
noisyNet noise sample is [array([-0.29004], dtype=float32), 1.7596123]. 
=============================================
[2019-04-04 14:58:35,581] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4437233e-08 4.7845443e-09 1.1615131e-14 3.1001300e-13 1.0000000e+00
 3.2358582e-10 5.1767253e-15], sum to 1.0000
[2019-04-04 14:58:35,581] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2444
[2019-04-04 14:58:35,680] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.833333333333334, 70.0, 59.66666666666667, 323.6666666666667, 26.0, 24.1953585809321, 0.2037576190043042, 0.0, 1.0, 41301.26763012353], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3571800.0000, 
sim time next is 3572400.0000, 
raw observation next is [-6.666666666666667, 70.0, 73.83333333333334, 374.3333333333334, 26.0, 24.19497273858665, 0.2921075383244111, 0.0, 1.0, 202438.3801351914], 
processed observation next is [0.0, 0.34782608695652173, 0.27793167128347185, 0.7, 0.24611111111111114, 0.4136279926335176, 0.6666666666666666, 0.5162477282155541, 0.5973691794414704, 0.0, 1.0, 0.9639922863580543], 
reward next is 0.0360, 
noisyNet noise sample is [array([-0.9605617], dtype=float32), 0.8978697]. 
=============================================
[2019-04-04 14:58:36,219] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0655017e-08 8.6048935e-10 4.0274104e-15 1.5966017e-13 1.0000000e+00
 7.0821221e-10 1.4099739e-14], sum to 1.0000
[2019-04-04 14:58:36,222] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4750
[2019-04-04 14:58:36,235] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.69352433985244, 0.4247576118361267, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3897600.0000, 
sim time next is 3898200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.55003712823476, 0.3982022252120083, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.62916976068623, 0.6327340750706695, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.201907], dtype=float32), -0.07754354]. 
=============================================
[2019-04-04 14:58:41,228] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5696409e-09 2.6358004e-11 2.1675163e-16 1.8492927e-15 1.0000000e+00
 1.2840874e-10 3.7263073e-16], sum to 1.0000
[2019-04-04 14:58:41,229] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1196
[2019-04-04 14:58:41,235] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.0, 87.0, 711.0, 26.0, 26.82429099838349, 0.6960415422592998, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3771000.0000, 
sim time next is 3771600.0000, 
raw observation next is [0.0, 60.0, 83.16666666666666, 682.3333333333333, 26.0, 26.80681693784898, 0.6998793511909208, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.2772222222222222, 0.7539594843462246, 0.6666666666666666, 0.7339014114874148, 0.7332931170636403, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.93060416], dtype=float32), 0.045094598]. 
=============================================
[2019-04-04 14:58:41,301] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0826543e-10 1.2574221e-10 3.1727107e-16 1.2510848e-14 1.0000000e+00
 1.4058843e-10 5.7832085e-16], sum to 1.0000
[2019-04-04 14:58:41,302] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1973
[2019-04-04 14:58:41,324] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.666666666666667, 35.66666666666667, 93.16666666666666, 480.0, 26.0, 27.35471124801103, 0.5718792829869824, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4119600.0000, 
sim time next is 4120200.0000, 
raw observation next is [3.5, 36.0, 93.0, 437.0, 26.0, 27.30289793289909, 0.7847117243341747, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5595567867036012, 0.36, 0.31, 0.48287292817679556, 0.6666666666666666, 0.7752414944082574, 0.7615705747780582, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1124184], dtype=float32), -0.9099709]. 
=============================================
[2019-04-04 14:58:47,604] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4527521e-08 2.0854340e-09 3.1312917e-14 2.9890410e-13 1.0000000e+00
 1.2676378e-09 6.2711094e-14], sum to 1.0000
[2019-04-04 14:58:47,604] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0780
[2019-04-04 14:58:47,620] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 51.5, 0.0, 0.0, 26.0, 24.69766311146042, 0.2103674170917568, 0.0, 1.0, 40243.07055591611], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4174200.0000, 
sim time next is 4174800.0000, 
raw observation next is [-5.0, 52.33333333333334, 15.33333333333333, 81.33333333333331, 26.0, 24.65220879499059, 0.2159994875558071, 0.0, 1.0, 40287.96960118788], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.5233333333333334, 0.0511111111111111, 0.0898710865561694, 0.6666666666666666, 0.5543507329158824, 0.571999829185269, 0.0, 1.0, 0.19184747429137083], 
reward next is 0.8082, 
noisyNet noise sample is [array([0.43401143], dtype=float32), -0.4631735]. 
=============================================
[2019-04-04 14:58:51,528] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.2660901e-09 3.4090364e-10 2.3820704e-14 7.6101607e-14 1.0000000e+00
 4.2184634e-10 2.0340214e-14], sum to 1.0000
[2019-04-04 14:58:51,529] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2878
[2019-04-04 14:58:51,602] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.333333333333334, 62.0, 5.0, 135.8333333333333, 26.0, 24.64155952035137, 0.3513635858725594, 1.0, 1.0, 202408.6114574343], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3914400.0000, 
sim time next is 3915000.0000, 
raw observation next is [-7.5, 61.0, 6.0, 163.0, 26.0, 25.15311531843441, 0.3759506934969081, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.2548476454293629, 0.61, 0.02, 0.18011049723756906, 0.6666666666666666, 0.5960929432028674, 0.6253168978323027, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6775971], dtype=float32), -0.8329863]. 
=============================================
[2019-04-04 14:58:51,616] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[81.093506]
 [79.05049 ]
 [79.37073 ]
 [79.64284 ]
 [79.93846 ]], R is [[82.30565643]
 [81.51874542]
 [81.49997711]
 [81.48195648]
 [81.46481323]].
[2019-04-04 14:58:54,972] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.04605355e-08 4.54668858e-10 1.19095966e-14 1.23627874e-13
 1.00000000e+00 5.11126752e-10 3.44025854e-14], sum to 1.0000
[2019-04-04 14:58:54,976] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4893
[2019-04-04 14:58:54,999] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.333333333333334, 39.0, 26.83333333333333, 230.1666666666667, 26.0, 26.64410959250903, 0.5046191386910184, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3950400.0000, 
sim time next is 3951000.0000, 
raw observation next is [-5.5, 39.5, 19.0, 169.0, 26.0, 26.4937787169621, 0.5734055217619869, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3102493074792244, 0.395, 0.06333333333333334, 0.1867403314917127, 0.6666666666666666, 0.7078148930801751, 0.6911351739206623, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6569666], dtype=float32), 1.7793106]. 
=============================================
[2019-04-04 14:58:55,006] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.53442 ]
 [78.12456 ]
 [78.661804]
 [79.09713 ]
 [79.54753 ]], R is [[77.21389008]
 [77.44174957]
 [77.66733551]
 [77.89066315]
 [78.11175537]].
[2019-04-04 14:58:58,318] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1443016e-07 3.0910787e-08 1.3273912e-12 1.1769958e-11 9.9999940e-01
 5.4278789e-08 3.2444989e-12], sum to 1.0000
[2019-04-04 14:58:58,319] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5985
[2019-04-04 14:58:58,338] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 36.66666666666666, 0.0, 0.0, 26.0, 24.7377637788037, 0.1966229081, 0.0, 1.0, 40218.53707121453], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4081200.0000, 
sim time next is 4081800.0000, 
raw observation next is [-4.0, 37.33333333333334, 0.0, 0.0, 26.0, 24.69186601038429, 0.1850584225248726, 0.0, 1.0, 40209.57556780105], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.3733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5576555008653576, 0.5616861408416242, 0.0, 1.0, 0.19147416937048117], 
reward next is 0.8085, 
noisyNet noise sample is [array([-0.28322622], dtype=float32), -0.52023005]. 
=============================================
[2019-04-04 14:58:59,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.45603207e-08 1.55947089e-09 1.07253555e-13 2.81694020e-13
 1.00000000e+00 1.39859484e-08 2.61782478e-14], sum to 1.0000
[2019-04-04 14:58:59,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7959
[2019-04-04 14:58:59,444] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 23.33333333333334, 59.16666666666667, 488.8333333333334, 26.0, 26.77828501003243, 0.43275204412083, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4034400.0000, 
sim time next is 4035000.0000, 
raw observation next is [-1.833333333333333, 23.66666666666666, 51.33333333333334, 429.6666666666667, 26.0, 26.76772258327252, 0.6629966007185396, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.41181902123730385, 0.2366666666666666, 0.17111111111111113, 0.47476979742173114, 0.6666666666666666, 0.7306435486060433, 0.7209988669061799, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6854382], dtype=float32), -0.9501113]. 
=============================================
[2019-04-04 14:58:59,448] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[77.16282 ]
 [77.45738 ]
 [77.54609 ]
 [77.664055]
 [77.80659 ]], R is [[77.08108521]
 [77.31027222]
 [77.53717041]
 [77.76180267]
 [77.98418427]].
[2019-04-04 14:59:02,434] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7980333e-10 7.7950823e-12 7.0042220e-17 3.6438357e-16 1.0000000e+00
 1.0835351e-11 7.8059089e-17], sum to 1.0000
[2019-04-04 14:59:02,435] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9478
[2019-04-04 14:59:02,449] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.5, 62.0, 0.0, 0.0, 26.0, 25.793500550164, 0.581662735629153, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4402800.0000, 
sim time next is 4403400.0000, 
raw observation next is [8.350000000000001, 62.16666666666667, 0.0, 0.0, 26.0, 25.71257797051774, 0.5648109419308583, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6939058171745154, 0.6216666666666667, 0.0, 0.0, 0.6666666666666666, 0.6427148308764782, 0.6882703139769527, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16183351], dtype=float32), 0.3944778]. 
=============================================
[2019-04-04 14:59:02,911] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.7189937e-10 3.7417291e-11 6.5941081e-16 1.5501839e-15 1.0000000e+00
 1.8614631e-11 1.6417477e-16], sum to 1.0000
[2019-04-04 14:59:02,912] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7118
[2019-04-04 14:59:02,925] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.466666666666667, 75.16666666666666, 0.0, 0.0, 26.0, 25.55945584113459, 0.4195844851402699, 0.0, 1.0, 18743.18560072287], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4315800.0000, 
sim time next is 4316400.0000, 
raw observation next is [4.4, 75.0, 0.0, 0.0, 26.0, 25.55542461431213, 0.421642394291338, 0.0, 1.0, 18741.44137506441], 
processed observation next is [0.0, 1.0, 0.5844875346260389, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6296187178593442, 0.6405474647637793, 0.0, 1.0, 0.08924495892887814], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.64486635], dtype=float32), -0.42963338]. 
=============================================
[2019-04-04 14:59:23,088] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.02652131e-09 1.04411306e-10 7.93534399e-16 4.47306368e-15
 1.00000000e+00 1.98178279e-10 1.38787005e-15], sum to 1.0000
[2019-04-04 14:59:23,090] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2706
[2019-04-04 14:59:23,104] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.5, 50.5, 247.0, 48.0, 26.0, 25.97643448414623, 0.5537490583512744, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4541400.0000, 
sim time next is 4542000.0000, 
raw observation next is [2.666666666666667, 50.0, 249.8333333333333, 58.83333333333333, 26.0, 26.07511605811671, 0.5723636960408872, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5364727608494922, 0.5, 0.8327777777777776, 0.06500920810313075, 0.6666666666666666, 0.6729263381763925, 0.6907878986802958, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04965599], dtype=float32), 0.743461]. 
=============================================
[2019-04-04 14:59:23,110] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[87.140915]
 [86.63972 ]
 [86.189095]
 [85.84623 ]
 [85.654724]], R is [[87.72164917]
 [87.84443665]
 [87.96599579]
 [88.08633423]
 [88.20547485]].
[2019-04-04 14:59:27,265] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0185658e-10 1.2664861e-11 3.8777187e-16 3.9999276e-15 1.0000000e+00
 3.2238004e-11 2.2310207e-16], sum to 1.0000
[2019-04-04 14:59:27,267] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5796
[2019-04-04 14:59:27,297] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 25.42390680917016, 0.5331419680617139, 0.0, 1.0, 18763.41331129596], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4741200.0000, 
sim time next is 4741800.0000, 
raw observation next is [-2.166666666666667, 84.83333333333334, 0.0, 0.0, 26.0, 25.48736208085061, 0.5310555826772063, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4025854108956602, 0.8483333333333334, 0.0, 0.0, 0.6666666666666666, 0.6239468400708841, 0.6770185275590688, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0757033], dtype=float32), -0.01370509]. 
=============================================
[2019-04-04 14:59:30,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:30,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:30,168] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run23
[2019-04-04 14:59:30,932] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.9079598e-10 5.1464527e-11 1.7843792e-16 3.4470747e-15 1.0000000e+00
 2.0597142e-10 5.8973843e-16], sum to 1.0000
[2019-04-04 14:59:30,932] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1496
[2019-04-04 14:59:30,949] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.2, 48.5, 26.66666666666667, 96.66666666666667, 26.0, 26.73049007624926, 0.7494901823288034, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4643400.0000, 
sim time next is 4644000.0000, 
raw observation next is [4.0, 49.0, 0.0, 0.0, 26.0, 26.26706104113046, 0.6962123187442159, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5734072022160666, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6889217534275384, 0.7320707729147387, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9802875], dtype=float32), 0.11799003]. 
=============================================
[2019-04-04 14:59:30,954] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.01011]
 [86.21195]
 [86.39953]
 [86.56281]
 [86.75167]], R is [[85.69832611]
 [85.84134674]
 [85.98293304]
 [86.12310791]
 [86.26187897]].
[2019-04-04 14:59:32,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:32,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:32,914] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run23
[2019-04-04 14:59:37,813] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:37,813] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:37,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run23
[2019-04-04 14:59:41,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:41,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:41,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run23
[2019-04-04 14:59:45,080] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:45,080] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:45,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run23
[2019-04-04 14:59:45,212] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5191676e-09 9.2697044e-11 5.9331047e-16 2.0252377e-14 1.0000000e+00
 1.7094041e-10 1.5577836e-15], sum to 1.0000
[2019-04-04 14:59:45,214] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2346
[2019-04-04 14:59:45,234] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6666666666666667, 31.5, 111.0, 745.6666666666667, 26.0, 26.3325336214837, 0.5174573959397804, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4960200.0000, 
sim time next is 4960800.0000, 
raw observation next is [1.0, 30.0, 112.5, 760.0, 26.0, 26.43352240571933, 0.5377219607700424, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.3, 0.375, 0.8397790055248618, 0.6666666666666666, 0.7027935338099441, 0.6792406535900142, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5211284], dtype=float32), -0.724438]. 
=============================================
[2019-04-04 14:59:47,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.3584289e-10 5.6699378e-11 2.6921823e-16 7.5081048e-16 1.0000000e+00
 3.4495677e-11 5.8968443e-16], sum to 1.0000
[2019-04-04 14:59:47,475] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0918
[2019-04-04 14:59:47,504] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.66666666666667, 17.0, 42.66666666666666, 340.8333333333333, 26.0, 29.05819799478462, 0.9868948916428573, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5073600.0000, 
sim time next is 5074200.0000, 
raw observation next is [11.5, 17.0, 36.0, 292.0, 26.0, 28.89479861437823, 1.126689178645199, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7811634349030472, 0.17, 0.12, 0.32265193370165746, 0.6666666666666666, 0.9078998845315193, 0.8755630595483996, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7672843], dtype=float32), -1.4869213]. 
=============================================
[2019-04-04 14:59:47,682] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.94432334e-09 6.23650687e-10 1.34561695e-14 8.24541919e-14
 1.00000000e+00 1.56014035e-09 7.28141200e-15], sum to 1.0000
[2019-04-04 14:59:47,682] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1088
[2019-04-04 14:59:47,690] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.3, 25.0, 0.0, 0.0, 26.0, 26.01536863312196, 0.5727434365154485, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5095200.0000, 
sim time next is 5095800.0000, 
raw observation next is [8.25, 27.5, 0.0, 0.0, 26.0, 25.93534602878092, 0.5558817985628172, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6911357340720222, 0.275, 0.0, 0.0, 0.6666666666666666, 0.6612788357317433, 0.6852939328542723, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.62470585], dtype=float32), -0.67808706]. 
=============================================
[2019-04-04 14:59:48,059] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:48,059] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:48,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run23
[2019-04-04 14:59:48,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:48,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:48,803] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run23
[2019-04-04 14:59:49,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:49,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:49,303] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run23
[2019-04-04 14:59:49,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:49,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:49,352] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run23
[2019-04-04 14:59:50,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:50,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:50,396] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run23
[2019-04-04 14:59:50,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:50,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:50,751] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run23
[2019-04-04 14:59:51,716] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:51,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:51,722] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run23
[2019-04-04 14:59:51,751] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:51,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:51,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run23
[2019-04-04 14:59:52,517] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:52,517] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:52,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run23
[2019-04-04 14:59:53,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:53,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:53,016] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run23
[2019-04-04 14:59:53,482] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 14:59:53,483] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 14:59:53,486] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run23
[2019-04-04 15:00:01,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2183907e-07 5.3449124e-08 4.4422239e-12 4.0469308e-11 9.9999988e-01
 1.4962634e-08 5.3940298e-12], sum to 1.0000
[2019-04-04 15:00:01,010] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6250
[2019-04-04 15:00:01,025] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.199999999999999, 96.0, 0.0, 0.0, 24.5, 20.35381700850962, -0.7752658355647378, 0.0, 1.0, 43101.2160926315], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 10200.0000, 
sim time next is 10800.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 25.0, 20.40630805730695, -0.76471619546731, 0.0, 1.0, 42851.22872283051], 
processed observation next is [0.0, 0.13043478260869565, 0.662049861495845, 0.96, 0.0, 0.0, 0.5833333333333334, 0.20052567144224595, 0.24509460151089668, 0.0, 1.0, 0.2040534701087167], 
reward next is 0.7959, 
noisyNet noise sample is [array([-0.503013], dtype=float32), -2.1836514]. 
=============================================
[2019-04-04 15:00:01,087] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.16976409e-09 1.58822810e-09 2.53265435e-15 1.41895312e-13
 1.00000000e+00 3.73712339e-10 1.05563505e-14], sum to 1.0000
[2019-04-04 15:00:01,087] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1701
[2019-04-04 15:00:01,103] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 20.95562332745192, -0.648651044293575, 0.0, 1.0, 40980.38791166097], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 16200.0000, 
sim time next is 16800.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 26.0, 20.96970392796214, -0.6430872139153895, 0.0, 1.0, 40896.12363472819], 
processed observation next is [0.0, 0.17391304347826086, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.24747532733017832, 0.2856375953615368, 0.0, 1.0, 0.19474344587965803], 
reward next is 0.8053, 
noisyNet noise sample is [array([-0.31692716], dtype=float32), -0.28676823]. 
=============================================
[2019-04-04 15:00:14,611] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2540008e-08 5.7018057e-09 9.4006820e-14 3.0855592e-13 1.0000000e+00
 9.8845254e-10 7.5156061e-14], sum to 1.0000
[2019-04-04 15:00:14,611] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5183
[2019-04-04 15:00:14,639] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 23.31624409276012, -0.1102817337009118, 0.0, 1.0, 44126.68170926346], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 184800.0000, 
sim time next is 185400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.28675285700236, -0.123421788529705, 0.0, 1.0, 44153.4848089223], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4405627380835299, 0.45885940382343166, 0.0, 1.0, 0.21025468956629667], 
reward next is 0.7897, 
noisyNet noise sample is [array([-1.334492], dtype=float32), -2.5370927]. 
=============================================
[2019-04-04 15:00:17,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.2021520e-08 4.4103188e-09 1.3178681e-13 1.5247009e-12 1.0000000e+00
 2.4756404e-09 1.3827006e-13], sum to 1.0000
[2019-04-04 15:00:17,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5315
[2019-04-04 15:00:17,550] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.41666666666667, 67.5, 0.0, 0.0, 26.0, 23.0903580042391, -0.1562843895820646, 0.0, 1.0, 47407.59749602441], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 276600.0000, 
sim time next is 277200.0000, 
raw observation next is [-10.6, 67.0, 0.0, 0.0, 26.0, 23.02386636169146, -0.1676489531889016, 0.0, 1.0, 47546.55665192018], 
processed observation next is [1.0, 0.21739130434782608, 0.1689750692520776, 0.67, 0.0, 0.0, 0.6666666666666666, 0.4186555301409551, 0.4441170156036995, 0.0, 1.0, 0.22641217453295326], 
reward next is 0.7736, 
noisyNet noise sample is [array([1.0280894], dtype=float32), -1.2870264]. 
=============================================
[2019-04-04 15:00:22,054] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-04 15:00:22,057] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 15:00:22,057] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 15:00:22,058] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:00:22,057] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:00:22,058] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 15:00:22,060] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:00:22,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run31
[2019-04-04 15:00:22,642] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run31
[2019-04-04 15:00:22,775] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run31
[2019-04-04 15:00:26,592] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17821376], dtype=float32), 0.22046669]
[2019-04-04 15:00:26,593] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [8.1289470015, 89.832914925, 0.0, 0.0, 26.0, 20.36682294449048, -0.7654894105893014, 0.0, 1.0, 43224.86283174113]
[2019-04-04 15:00:26,593] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 15:00:26,593] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.6167272e-07 3.7193047e-07 2.4103896e-11 3.9169978e-10 9.9999893e-01
 8.3283368e-08 4.4374879e-11], sampled 0.20043288396705483
[2019-04-04 15:01:17,963] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17821376], dtype=float32), 0.22046669]
[2019-04-04 15:01:17,963] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.1, 83.33333333333334, 64.5, 0.0, 26.0, 24.69863374440994, 0.3274181903487936, 1.0, 1.0, 198319.496989892]
[2019-04-04 15:01:17,963] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 15:01:17,964] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.9713396e-10 6.8777622e-11 5.6921366e-16 5.1713564e-15 1.0000000e+00
 3.6689794e-11 1.2023254e-15], sampled 0.0009525040768602056
[2019-04-04 15:01:55,273] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17821376], dtype=float32), 0.22046669]
[2019-04-04 15:01:55,273] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [13.8, 56.66666666666667, 183.6666666666667, 628.0, 26.0, 25.78161985193559, 0.6300539792025354, 0.0, 1.0, 0.0]
[2019-04-04 15:01:55,273] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 15:01:55,274] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.2977329e-10 1.5068580e-11 1.3170538e-17 2.0931015e-16 1.0000000e+00
 2.4792514e-12 3.1512147e-17], sampled 0.11210935940020561
[2019-04-04 15:02:03,463] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 15:02:21,718] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 15:02:26,697] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 15:02:27,722] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 3000000, evaluation results [3000000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 15:02:35,328] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4737575e-08 1.9978073e-09 3.2759873e-14 1.1843095e-13 1.0000000e+00
 5.3933080e-09 1.6333484e-13], sum to 1.0000
[2019-04-04 15:02:35,328] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5821
[2019-04-04 15:02:35,378] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.46104040857465, 0.3274052310751427, 1.0, 1.0, 46506.11511489948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 412800.0000, 
sim time next is 413400.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.37563639504797, 0.3056902924376846, 1.0, 1.0, 46975.4054826174], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6146363662539974, 0.6018967641458949, 1.0, 1.0, 0.22369240706008287], 
reward next is 0.7763, 
noisyNet noise sample is [array([-0.9701041], dtype=float32), -1.5601137]. 
=============================================
[2019-04-04 15:02:37,356] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8629946e-08 3.2246525e-09 5.5939194e-14 2.7793360e-13 1.0000000e+00
 4.2760138e-09 1.8343530e-14], sum to 1.0000
[2019-04-04 15:02:37,356] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7794
[2019-04-04 15:02:37,407] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-14.13333333333333, 64.0, 65.66666666666667, 730.5, 26.0, 25.87478233181663, 0.3561069395734673, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 382800.0000, 
sim time next is 383400.0000, 
raw observation next is [-13.95, 63.0, 71.0, 729.0, 26.0, 25.9307198761671, 0.3629454265369305, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.07617728531855956, 0.63, 0.23666666666666666, 0.8055248618784531, 0.6666666666666666, 0.660893323013925, 0.6209818088456435, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.57569134], dtype=float32), -0.55132425]. 
=============================================
[2019-04-04 15:02:43,582] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.8003643e-09 3.0621242e-10 6.6253520e-15 2.0058049e-14 1.0000000e+00
 3.1536851e-10 8.5025455e-15], sum to 1.0000
[2019-04-04 15:02:43,584] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7842
[2019-04-04 15:02:43,602] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 24.71867761717925, 0.1558110137431093, 0.0, 1.0, 41699.50157116403], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 686400.0000, 
sim time next is 687000.0000, 
raw observation next is [-3.816666666666666, 70.66666666666667, 0.0, 0.0, 26.0, 24.68520167067255, 0.1480111871376663, 0.0, 1.0, 41630.89268653015], 
processed observation next is [0.0, 0.9565217391304348, 0.3568790397045245, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.5571001392227126, 0.5493370623792221, 0.0, 1.0, 0.19824234612633404], 
reward next is 0.8018, 
noisyNet noise sample is [array([0.10858832], dtype=float32), -1.4922003]. 
=============================================
[2019-04-04 15:02:43,617] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.6752  ]
 [78.7351  ]
 [78.78394 ]
 [78.843925]
 [78.87681 ]], R is [[78.64179993]
 [78.65681458]
 [78.67128754]
 [78.68530273]
 [78.69891357]].
[2019-04-04 15:02:50,334] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3305632e-09 1.6201765e-10 1.8552351e-15 3.5946875e-15 1.0000000e+00
 1.7250110e-10 8.8801828e-16], sum to 1.0000
[2019-04-04 15:02:50,335] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2852
[2019-04-04 15:02:50,380] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 82.66666666666667, 42.33333333333334, 0.0, 26.0, 25.95459642510234, 0.4248324359864944, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 834600.0000, 
sim time next is 835200.0000, 
raw observation next is [-3.9, 82.0, 39.0, 0.0, 26.0, 26.01862950131956, 0.4054035633325223, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.13, 0.0, 0.6666666666666666, 0.6682191251099633, 0.6351345211108408, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.576451], dtype=float32), 0.6022321]. 
=============================================
[2019-04-04 15:03:01,073] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.5748173e-09 1.0856297e-09 7.3361422e-15 1.3120067e-13 1.0000000e+00
 5.3134486e-10 3.5257265e-14], sum to 1.0000
[2019-04-04 15:03:01,076] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9393
[2019-04-04 15:03:01,103] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 75.0, 0.0, 0.0, 26.0, 24.26489863165661, 0.03775188827597047, 0.0, 1.0, 41647.60692278988], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 705600.0000, 
sim time next is 706200.0000, 
raw observation next is [-2.716666666666667, 75.16666666666667, 0.0, 0.0, 26.0, 24.21687522351294, 0.04366107659049767, 0.0, 1.0, 41639.13605910561], 
processed observation next is [1.0, 0.17391304347826086, 0.3873499538319483, 0.7516666666666667, 0.0, 0.0, 0.6666666666666666, 0.518072935292745, 0.5145536921968326, 0.0, 1.0, 0.19828160028145528], 
reward next is 0.8017, 
noisyNet noise sample is [array([-1.0934873], dtype=float32), -0.47013894]. 
=============================================
[2019-04-04 15:03:01,390] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4129091e-09 2.6183095e-10 9.8011275e-15 5.2964489e-14 1.0000000e+00
 2.0204691e-10 1.3724867e-14], sum to 1.0000
[2019-04-04 15:03:01,390] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2427
[2019-04-04 15:03:01,403] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 47.0, 82.5, 372.5, 26.0, 25.94479421960897, 0.4267617651475511, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 745200.0000, 
sim time next is 745800.0000, 
raw observation next is [-0.09999999999999999, 46.66666666666667, 83.33333333333333, 258.6666666666666, 26.0, 25.86579965471508, 0.276112395169583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4598337950138504, 0.46666666666666673, 0.27777777777777773, 0.28581952117863707, 0.6666666666666666, 0.6554833045595899, 0.5920374650565277, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7533256], dtype=float32), 0.04773695]. 
=============================================
[2019-04-04 15:03:12,829] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.4293440e-10 7.6255408e-12 7.1686508e-18 2.4848233e-16 1.0000000e+00
 6.4839943e-12 1.6077954e-17], sum to 1.0000
[2019-04-04 15:03:12,830] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8758
[2019-04-04 15:03:12,839] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.16666666666667, 53.0, 0.0, 26.0, 25.70682433138328, 0.5131441326067806, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1350600.0000, 
sim time next is 1351200.0000, 
raw observation next is [1.1, 92.33333333333334, 48.5, 0.0, 26.0, 25.70665454624498, 0.5107668499167836, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.9233333333333335, 0.16166666666666665, 0.0, 0.6666666666666666, 0.6422212121870817, 0.6702556166389279, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22404033], dtype=float32), -0.29030097]. 
=============================================
[2019-04-04 15:03:19,116] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5192189e-10 7.5271473e-12 9.4180851e-19 6.6050564e-17 1.0000000e+00
 3.5045571e-12 7.4108368e-18], sum to 1.0000
[2019-04-04 15:03:19,118] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0108
[2019-04-04 15:03:19,128] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.7, 86.0, 112.0, 0.0, 26.0, 26.66259027324093, 0.6705093644361341, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 990600.0000, 
sim time next is 991200.0000, 
raw observation next is [11.8, 86.0, 116.0, 0.0, 26.0, 26.65443509988768, 0.6782171675273464, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7894736842105264, 0.86, 0.38666666666666666, 0.0, 0.6666666666666666, 0.7212029249906401, 0.7260723891757821, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.32179], dtype=float32), -0.77860963]. 
=============================================
[2019-04-04 15:03:23,428] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9098862e-10 3.2550692e-12 1.9355209e-17 3.4081350e-16 1.0000000e+00
 1.7304664e-12 2.4714737e-17], sum to 1.0000
[2019-04-04 15:03:23,432] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9128
[2019-04-04 15:03:23,441] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.0, 100.0, 19.0, 0.0, 26.0, 24.60079661398782, 0.4214553514662492, 0.0, 1.0, 26533.59884668281], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1269000.0000, 
sim time next is 1269600.0000, 
raw observation next is [12.73333333333333, 100.0, 15.83333333333333, 0.0, 26.0, 24.59580846656353, 0.4221732093195605, 0.0, 1.0, 27474.39249671723], 
processed observation next is [0.0, 0.6956521739130435, 0.8153277931671283, 1.0, 0.05277777777777777, 0.0, 0.6666666666666666, 0.5496507055469607, 0.6407244031065201, 0.0, 1.0, 0.13083044046055825], 
reward next is 0.8692, 
noisyNet noise sample is [array([0.9029961], dtype=float32), -1.8314708]. 
=============================================
[2019-04-04 15:03:26,842] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5473292e-09 2.0943229e-09 1.7217434e-14 6.9590140e-13 1.0000000e+00
 8.7899320e-11 7.2698839e-14], sum to 1.0000
[2019-04-04 15:03:26,843] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5828
[2019-04-04 15:03:26,847] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.33004331261119, 0.3182687923855447, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1206000.0000, 
sim time next is 1206600.0000, 
raw observation next is [16.51666666666667, 75.5, 0.0, 0.0, 26.0, 24.30237984529745, 0.3139660294351146, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 1.0, 0.9201292705447832, 0.755, 0.0, 0.0, 0.6666666666666666, 0.5251983204414543, 0.6046553431450382, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4450317], dtype=float32), 1.0621717]. 
=============================================
[2019-04-04 15:03:39,764] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.1659577e-10 5.2391563e-11 6.7351528e-16 1.6360968e-15 1.0000000e+00
 2.1152456e-10 3.7520963e-16], sum to 1.0000
[2019-04-04 15:03:39,768] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5233
[2019-04-04 15:03:39,785] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.616666666666667, 73.83333333333334, 0.0, 0.0, 26.0, 25.30940493897585, 0.6307638643150252, 0.0, 1.0, 173753.9425738378], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1543800.0000, 
sim time next is 1544400.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 26.0, 25.46048683418699, 0.6887371586476867, 0.0, 1.0, 42066.26284043533], 
processed observation next is [1.0, 0.9130434782608695, 0.6759002770083103, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6217072361822492, 0.7295790528825622, 0.0, 1.0, 0.20031553733540633], 
reward next is 0.7997, 
noisyNet noise sample is [array([1.5109466], dtype=float32), -0.543713]. 
=============================================
[2019-04-04 15:03:43,937] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.89393867e-10 1.53950880e-11 2.28222943e-16 8.15709351e-16
 1.00000000e+00 2.44308718e-12 1.06656284e-16], sum to 1.0000
[2019-04-04 15:03:43,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6315
[2019-04-04 15:03:43,950] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.54808609320038, 0.5774329812675162, 0.0, 1.0, 18746.80652177668], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1654200.0000, 
sim time next is 1654800.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.6217692004795, 0.5734413959577318, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6351474333732915, 0.6911471319859106, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6396367], dtype=float32), -0.80368483]. 
=============================================
[2019-04-04 15:03:44,461] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4832460e-09 1.2690983e-10 6.4525953e-16 4.3801594e-15 1.0000000e+00
 2.2951042e-11 5.7185365e-16], sum to 1.0000
[2019-04-04 15:03:44,467] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8172
[2019-04-04 15:03:44,477] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.65216075656508, 0.5179582790127665, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1576800.0000, 
sim time next is 1577400.0000, 
raw observation next is [5.083333333333334, 81.50000000000001, 0.0, 0.0, 26.0, 25.69082353304864, 0.5051451751469833, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.6034164358264081, 0.8150000000000002, 0.0, 0.0, 0.6666666666666666, 0.6409019610873866, 0.6683817250489944, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29175463], dtype=float32), 0.60610616]. 
=============================================
[2019-04-04 15:03:46,253] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2564877e-09 3.0286711e-11 3.2182613e-17 3.0691330e-15 1.0000000e+00
 1.0010172e-11 2.8102747e-17], sum to 1.0000
[2019-04-04 15:03:46,256] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0573
[2019-04-04 15:03:46,284] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.683333333333334, 97.0, 0.0, 0.0, 26.0, 25.46823762330927, 0.5386407554937126, 0.0, 1.0, 79180.09229462541], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1662600.0000, 
sim time next is 1663200.0000, 
raw observation next is [5.5, 97.0, 0.0, 0.0, 26.0, 25.47898549740312, 0.5520948038697597, 0.0, 1.0, 48030.23014706527], 
processed observation next is [1.0, 0.2608695652173913, 0.6149584487534627, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6232487914502599, 0.6840316012899198, 0.0, 1.0, 0.22871538165269176], 
reward next is 0.7713, 
noisyNet noise sample is [array([-0.6137137], dtype=float32), 0.947452]. 
=============================================
[2019-04-04 15:03:59,095] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1315542e-08 2.1366240e-09 3.2490493e-14 1.8862292e-13 1.0000000e+00
 9.1344587e-10 1.4149074e-13], sum to 1.0000
[2019-04-04 15:03:59,096] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2389
[2019-04-04 15:03:59,165] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 79.0, 72.0, 0.0, 26.0, 25.04401407067407, 0.2651805309553191, 0.0, 1.0, 51361.20431919419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1870200.0000, 
sim time next is 1870800.0000, 
raw observation next is [-4.5, 77.66666666666667, 64.83333333333333, 0.0, 26.0, 25.04667537643056, 0.2626051674198926, 0.0, 1.0, 45268.96821904967], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.7766666666666667, 0.2161111111111111, 0.0, 0.6666666666666666, 0.58722294803588, 0.5875350558066309, 0.0, 1.0, 0.21556651532880794], 
reward next is 0.7844, 
noisyNet noise sample is [array([0.76909655], dtype=float32), -0.69153666]. 
=============================================
[2019-04-04 15:04:02,107] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.6862822e-09 7.3285245e-10 1.4227899e-14 1.1611260e-13 1.0000000e+00
 1.6878022e-09 4.5443191e-15], sum to 1.0000
[2019-04-04 15:04:02,107] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8056
[2019-04-04 15:04:02,121] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.1, 82.16666666666667, 0.0, 0.0, 26.0, 25.10609661876841, 0.3523232675699173, 0.0, 1.0, 42928.06091424254], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1979400.0000, 
sim time next is 1980000.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 26.0, 25.08156396832157, 0.3479049735231395, 0.0, 1.0, 42926.56822680285], 
processed observation next is [1.0, 0.9565217391304348, 0.2908587257617729, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5901303306934643, 0.6159683245077131, 0.0, 1.0, 0.20441222965144212], 
reward next is 0.7956, 
noisyNet noise sample is [array([0.90373915], dtype=float32), 0.39788246]. 
=============================================
[2019-04-04 15:04:02,163] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.16892]
 [77.03228]
 [76.92458]
 [76.84214]
 [76.85533]], R is [[77.42251587]
 [77.44387054]
 [77.46400452]
 [77.48188019]
 [77.4990387 ]].
[2019-04-04 15:04:06,580] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3769480e-09 2.3463245e-11 1.9108264e-14 9.6647808e-14 1.0000000e+00
 2.7260769e-10 1.1397429e-14], sum to 1.0000
[2019-04-04 15:04:06,580] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1396
[2019-04-04 15:04:06,647] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.05, 77.0, 0.0, 0.0, 26.0, 25.19497457741178, 0.3099924329050099, 1.0, 1.0, 25664.1299326529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1971000.0000, 
sim time next is 1971600.0000, 
raw observation next is [-5.233333333333333, 79.0, 0.0, 0.0, 26.0, 24.97702940485933, 0.3044046632438874, 1.0, 1.0, 156875.1712695478], 
processed observation next is [1.0, 0.8260869565217391, 0.31763619575253926, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5814191170716109, 0.6014682210812957, 1.0, 1.0, 0.7470246250930848], 
reward next is 0.2530, 
noisyNet noise sample is [array([2.1040676], dtype=float32), -1.2517178]. 
=============================================
[2019-04-04 15:04:26,716] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3980859e-08 2.6233145e-09 4.6903535e-14 4.2846428e-13 1.0000000e+00
 3.6734786e-09 2.4373737e-14], sum to 1.0000
[2019-04-04 15:04:26,716] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7672
[2019-04-04 15:04:26,731] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.66666666666667, 0.0, 0.0, 26.0, 23.71107213461102, -0.007402652003734735, 0.0, 1.0, 41891.52741842767], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2182800.0000, 
sim time next is 2183400.0000, 
raw observation next is [-5.9, 77.0, 0.0, 0.0, 26.0, 23.68422284306744, -0.0109196994600774, 0.0, 1.0, 41897.64244274286], 
processed observation next is [1.0, 0.2608695652173913, 0.2991689750692521, 0.77, 0.0, 0.0, 0.6666666666666666, 0.4736852369222868, 0.49636010017997423, 0.0, 1.0, 0.19951258306068026], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.899997], dtype=float32), 0.259641]. 
=============================================
[2019-04-04 15:04:29,435] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0651816e-09 4.5138795e-10 1.1614733e-14 2.1069476e-13 1.0000000e+00
 6.2981886e-10 2.9192418e-14], sum to 1.0000
[2019-04-04 15:04:29,445] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6018
[2019-04-04 15:04:29,464] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 41.0, 0.0, 0.0, 26.0, 24.74325241613468, 0.1845365437709821, 0.0, 1.0, 43063.62156739495], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2415600.0000, 
sim time next is 2416200.0000, 
raw observation next is [-5.100000000000001, 41.33333333333334, 0.0, 0.0, 26.0, 24.69834686881405, 0.175741060173632, 0.0, 1.0, 43088.83347147018], 
processed observation next is [0.0, 1.0, 0.32132963988919666, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.5581955724011708, 0.5585803533912107, 0.0, 1.0, 0.20518492129271512], 
reward next is 0.7948, 
noisyNet noise sample is [array([-0.9311322], dtype=float32), -1.121732]. 
=============================================
[2019-04-04 15:04:36,473] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.2032775e-09 5.8370979e-09 5.1293540e-14 3.0858416e-13 1.0000000e+00
 2.1518831e-09 3.0633916e-14], sum to 1.0000
[2019-04-04 15:04:36,473] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2736
[2019-04-04 15:04:36,504] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 54.0, 0.0, 0.0, 26.0, 25.38781642736431, 0.4339933900713587, 0.0, 1.0, 33807.73329177557], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2322000.0000, 
sim time next is 2322600.0000, 
raw observation next is [-1.7, 54.33333333333333, 0.0, 0.0, 26.0, 25.44985306634089, 0.4321295563064934, 0.0, 1.0, 18762.95486425829], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5433333333333333, 0.0, 0.0, 0.6666666666666666, 0.6208210888617408, 0.6440431854354979, 0.0, 1.0, 0.08934740411551566], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.48769084], dtype=float32), -1.9936891]. 
=============================================
[2019-04-04 15:04:43,294] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.3941542e-09 5.5950622e-10 1.7877262e-14 2.8800757e-13 1.0000000e+00
 5.4767435e-10 2.2809264e-14], sum to 1.0000
[2019-04-04 15:04:43,296] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1148
[2019-04-04 15:04:43,308] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 48.0, 165.1666666666667, 193.3333333333333, 26.0, 25.95793604080808, 0.4724719212530251, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2647200.0000, 
sim time next is 2647800.0000, 
raw observation next is [0.5, 48.5, 155.0, 206.0, 26.0, 25.98739161014815, 0.4781647978307713, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4764542936288089, 0.485, 0.5166666666666667, 0.2276243093922652, 0.6666666666666666, 0.665615967512346, 0.6593882659435905, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14062187], dtype=float32), -0.5562077]. 
=============================================
[2019-04-04 15:04:45,246] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5363857e-08 6.5636021e-09 3.0121955e-14 3.2240557e-13 1.0000000e+00
 7.8348744e-10 7.2940094e-14], sum to 1.0000
[2019-04-04 15:04:45,253] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6658
[2019-04-04 15:04:45,264] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.1054926e-09 1.6570366e-09 3.7737608e-14 5.0786957e-13 1.0000000e+00
 3.1765730e-09 5.5239206e-14], sum to 1.0000
[2019-04-04 15:04:45,264] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9894
[2019-04-04 15:04:45,273] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 35.66666666666667, 0.0, 0.0, 26.0, 25.26894970300071, 0.2731017079596361, 0.0, 1.0, 40058.51176891803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2496000.0000, 
sim time next is 2496600.0000, 
raw observation next is [-1.2, 35.0, 0.0, 0.0, 26.0, 25.27499537919721, 0.2784485239118872, 0.0, 1.0, 40067.03978471497], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.35, 0.0, 0.0, 0.6666666666666666, 0.6062496149331009, 0.5928161746372957, 0.0, 1.0, 0.19079542754626178], 
reward next is 0.8092, 
noisyNet noise sample is [array([-0.1782668], dtype=float32), -1.9337914]. 
=============================================
[2019-04-04 15:04:45,276] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.92560882941622, 0.1766751921566819, 0.0, 1.0, 38626.28616648732], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2516400.0000, 
sim time next is 2517000.0000, 
raw observation next is [-1.7, 44.83333333333334, 0.0, 0.0, 26.0, 24.92735402918071, 0.1840188682211628, 0.0, 1.0, 38577.21825497227], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4483333333333334, 0.0, 0.0, 0.6666666666666666, 0.5772795024317258, 0.5613396227403876, 0.0, 1.0, 0.18370103930939177], 
reward next is 0.8163, 
noisyNet noise sample is [array([1.0674081], dtype=float32), -0.51059276]. 
=============================================
[2019-04-04 15:04:45,294] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.399475]
 [74.47008 ]
 [74.82702 ]
 [74.83314 ]
 [75.11449 ]], R is [[74.24269867]
 [74.31633759]
 [74.38917542]
 [74.46118927]
 [74.53226471]].
[2019-04-04 15:04:51,450] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.1471475e-08 2.8003440e-09 3.7238242e-14 6.6491327e-13 9.9999988e-01
 1.2542249e-09 1.8585944e-13], sum to 1.0000
[2019-04-04 15:04:51,452] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0868
[2019-04-04 15:04:51,466] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.80903793590719, 0.2371150264849592, 0.0, 1.0, 41810.80429834314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2771400.0000, 
sim time next is 2772000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.76134579883988, 0.2225438186184171, 0.0, 1.0, 41709.7176846165], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5634454832366567, 0.5741812728728056, 0.0, 1.0, 0.19861770326007858], 
reward next is 0.8014, 
noisyNet noise sample is [array([0.04651425], dtype=float32), 0.6946821]. 
=============================================
[2019-04-04 15:04:51,478] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.39713 ]
 [76.2938  ]
 [76.415245]
 [76.47008 ]
 [76.244064]], R is [[76.25183868]
 [76.29022217]
 [76.3276062 ]
 [76.36398315]
 [76.39929962]].
[2019-04-04 15:04:51,537] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1703961e-08 8.5135090e-09 1.9138475e-13 3.6541855e-12 1.0000000e+00
 1.0949717e-08 2.2252825e-13], sum to 1.0000
[2019-04-04 15:04:51,538] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8169
[2019-04-04 15:04:51,555] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.63606641391792, 0.205405489099103, 0.0, 1.0, 41387.07977304298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2773800.0000, 
sim time next is 2774400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.62933428337444, 0.2045694387435171, 0.0, 1.0, 41273.32870753755], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5524445236145367, 0.5681898129145057, 0.0, 1.0, 0.19653966051208357], 
reward next is 0.8035, 
noisyNet noise sample is [array([1.2544831], dtype=float32), -0.6023685]. 
=============================================
[2019-04-04 15:04:59,046] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6443672e-09 1.4394325e-10 1.9437803e-15 1.2770602e-14 1.0000000e+00
 8.8183161e-10 1.0203653e-15], sum to 1.0000
[2019-04-04 15:04:59,047] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6200
[2019-04-04 15:04:59,096] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.11092011024514, 0.3914111347524364, 1.0, 1.0, 84278.39588163915], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2748600.0000, 
sim time next is 2749200.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.09154887212524, 0.3884766177547226, 0.0, 1.0, 79261.93161942923], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5909624060104367, 0.6294922059182408, 0.0, 1.0, 0.3774377696163297], 
reward next is 0.6226, 
noisyNet noise sample is [array([-0.40183708], dtype=float32), -0.44686002]. 
=============================================
[2019-04-04 15:05:08,225] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.3621100e-11 2.2358901e-12 1.7138413e-17 8.9466518e-17 1.0000000e+00
 3.5336879e-12 6.5891765e-17], sum to 1.0000
[2019-04-04 15:05:08,226] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9912
[2019-04-04 15:05:08,237] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.333333333333333, 100.0, 160.3333333333333, 0.0, 26.0, 25.41094838258574, 0.3009388644704934, 1.0, 1.0, 9340.205835115268], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2895600.0000, 
sim time next is 2896200.0000, 
raw observation next is [1.5, 100.0, 175.0, 0.0, 26.0, 25.24585538241987, 0.3028196724364338, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5217391304347826, 0.5041551246537397, 1.0, 0.5833333333333334, 0.0, 0.6666666666666666, 0.6038212818683224, 0.6009398908121446, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([2.2324574], dtype=float32), 0.66593796]. 
=============================================
[2019-04-04 15:05:08,254] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2378620e-09 9.8603618e-11 5.1076533e-17 6.1412082e-15 1.0000000e+00
 3.1661535e-11 1.3029225e-16], sum to 1.0000
[2019-04-04 15:05:08,256] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6962
[2019-04-04 15:05:08,278] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 100.0, 80.33333333333333, 18.0, 26.0, 25.91280403627516, 0.4576766992669566, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2908200.0000, 
sim time next is 2908800.0000, 
raw observation next is [2.0, 100.0, 78.0, 27.0, 26.0, 25.92084114481721, 0.3538867630415987, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 1.0, 0.26, 0.02983425414364641, 0.6666666666666666, 0.6600700954014341, 0.6179622543471995, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07020335], dtype=float32), 0.3930626]. 
=============================================
[2019-04-04 15:05:20,900] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5660195e-09 3.3919328e-10 7.0850325e-16 6.3207733e-15 1.0000000e+00
 7.7051678e-11 1.3312185e-15], sum to 1.0000
[2019-04-04 15:05:20,901] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2874
[2019-04-04 15:05:20,918] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.54515375189463, 0.5496654494471679, 0.0, 1.0, 18745.81329052171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3209400.0000, 
sim time next is 3210000.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.58170032419861, 0.5345609653342093, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6318083603498842, 0.6781869884447365, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7420766], dtype=float32), -1.1318206]. 
=============================================
[2019-04-04 15:05:20,928] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[89.49929 ]
 [89.66482 ]
 [89.7269  ]
 [89.606316]
 [89.84225 ]], R is [[89.32938385]
 [89.34682465]
 [89.27886963]
 [89.07923126]
 [89.18843842]].
[2019-04-04 15:05:26,842] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.52264507e-10 7.87018367e-12 1.13823775e-17 1.42989369e-17
 1.00000000e+00 3.33348778e-12 1.82630933e-18], sum to 1.0000
[2019-04-04 15:05:26,842] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8487
[2019-04-04 15:05:26,850] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 100.0, 102.8333333333333, 770.1666666666667, 26.0, 27.66736197285451, 0.9328192040701039, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3162000.0000, 
sim time next is 3162600.0000, 
raw observation next is [7.0, 100.0, 101.0, 763.0, 26.0, 27.61623554628279, 0.9385737988916607, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6565096952908588, 1.0, 0.33666666666666667, 0.8430939226519337, 0.6666666666666666, 0.8013529621902326, 0.8128579329638869, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31089067], dtype=float32), -0.29240584]. 
=============================================
[2019-04-04 15:05:26,873] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9183052e-11 6.4680847e-12 8.7046253e-19 3.3475606e-17 1.0000000e+00
 9.2694830e-13 3.4081837e-18], sum to 1.0000
[2019-04-04 15:05:26,874] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3839
[2019-04-04 15:05:26,884] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.6, 99.0, 71.0, 589.0, 26.0, 27.56509045577113, 0.9884934699657698, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3168000.0000, 
sim time next is 3168600.0000, 
raw observation next is [6.5, 99.16666666666666, 66.66666666666666, 559.0, 26.0, 27.68423103700262, 1.005320923241025, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6426592797783934, 0.9916666666666666, 0.22222222222222218, 0.6176795580110497, 0.6666666666666666, 0.8070192530835515, 0.8351069744136751, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.545031], dtype=float32), -0.044832934]. 
=============================================
[2019-04-04 15:05:29,788] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5863226e-08 7.9160034e-10 1.4710906e-14 6.7604147e-13 1.0000000e+00
 2.4983478e-09 1.0073617e-13], sum to 1.0000
[2019-04-04 15:05:29,789] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5398
[2019-04-04 15:05:29,801] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.5, 86.5, 0.0, 0.0, 26.0, 25.61774625450629, 0.5522432455349203, 0.0, 1.0, 18732.31365390505], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3274200.0000, 
sim time next is 3274800.0000, 
raw observation next is [-5.666666666666667, 88.33333333333333, 0.0, 0.0, 26.0, 25.54293728127535, 0.5416550425477212, 0.0, 1.0, 61589.93179350624], 
processed observation next is [1.0, 0.9130434782608695, 0.30563250230840255, 0.8833333333333333, 0.0, 0.0, 0.6666666666666666, 0.6285781067729458, 0.6805516808492404, 0.0, 1.0, 0.29328538949288685], 
reward next is 0.7067, 
noisyNet noise sample is [array([0.31969368], dtype=float32), -1.5424726]. 
=============================================
[2019-04-04 15:05:30,567] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6780639e-09 3.7870576e-10 4.9089341e-15 2.3950453e-14 1.0000000e+00
 2.3903768e-10 1.1244466e-14], sum to 1.0000
[2019-04-04 15:05:30,571] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0128
[2019-04-04 15:05:30,588] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.333333333333334, 72.33333333333334, 0.0, 0.0, 26.0, 25.0490826320194, 0.3805876932746939, 0.0, 1.0, 43761.24534117637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3288000.0000, 
sim time next is 3288600.0000, 
raw observation next is [-7.5, 73.5, 0.0, 0.0, 26.0, 25.03109074748658, 0.3676041702920208, 0.0, 1.0, 43782.79497681736], 
processed observation next is [1.0, 0.043478260869565216, 0.2548476454293629, 0.735, 0.0, 0.0, 0.6666666666666666, 0.585924228957215, 0.6225347234306736, 0.0, 1.0, 0.20848949988960647], 
reward next is 0.7915, 
noisyNet noise sample is [array([-0.16657878], dtype=float32), -0.28847176]. 
=============================================
[2019-04-04 15:05:32,123] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8381098e-08 3.9334522e-09 5.9260309e-14 1.4662934e-12 1.0000000e+00
 4.7776867e-09 2.9242865e-13], sum to 1.0000
[2019-04-04 15:05:32,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1729
[2019-04-04 15:05:32,138] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 26.0, 24.08912799645007, 0.1227856246396574, 0.0, 1.0, 43844.69666732862], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3306000.0000, 
sim time next is 3306600.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 26.0, 24.00304307962309, 0.1044444893679641, 0.0, 1.0, 43905.13031427469], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5002535899685908, 0.5348148297893214, 0.0, 1.0, 0.20907204911559377], 
reward next is 0.7909, 
noisyNet noise sample is [array([0.16388424], dtype=float32), -1.0178126]. 
=============================================
[2019-04-04 15:05:35,666] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4144630e-08 3.6173942e-10 2.9170027e-15 4.1932176e-14 1.0000000e+00
 5.3242022e-10 3.5755272e-15], sum to 1.0000
[2019-04-04 15:05:35,666] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8497
[2019-04-04 15:05:35,692] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 64.0, 113.5, 769.0, 26.0, 26.37748861055563, 0.5952240273557649, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3322800.0000, 
sim time next is 3323400.0000, 
raw observation next is [-6.833333333333334, 62.33333333333334, 114.3333333333333, 778.6666666666667, 26.0, 26.37652131921206, 0.5984134677019484, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.27331486611265005, 0.6233333333333334, 0.381111111111111, 0.8604051565377533, 0.6666666666666666, 0.6980434432676716, 0.6994711559006493, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36597696], dtype=float32), -0.19378798]. 
=============================================
[2019-04-04 15:05:41,220] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9605469e-09 2.2925416e-10 4.4804413e-15 8.4865098e-14 1.0000000e+00
 5.1412263e-10 1.9900351e-15], sum to 1.0000
[2019-04-04 15:05:41,224] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5082
[2019-04-04 15:05:41,261] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 49.0, 45.66666666666667, 400.3333333333334, 26.0, 26.59626801615544, 0.529446507829432, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3516600.0000, 
sim time next is 3517200.0000, 
raw observation next is [3.0, 49.0, 37.5, 338.0, 26.0, 26.55116047450317, 0.6863391535460974, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.49, 0.125, 0.3734806629834254, 0.6666666666666666, 0.7125967062085975, 0.7287797178486991, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0434972], dtype=float32), 1.385377]. 
=============================================
[2019-04-04 15:05:56,433] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.2960427e-10 1.5321569e-10 3.1436521e-16 1.4254892e-15 1.0000000e+00
 9.1435533e-11 4.5715273e-16], sum to 1.0000
[2019-04-04 15:05:56,433] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6185
[2019-04-04 15:05:56,457] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.0, 102.8333333333333, 765.1666666666667, 26.0, 26.63662402940732, 0.6651231089442323, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3768000.0000, 
sim time next is 3768600.0000, 
raw observation next is [0.0, 60.0, 99.66666666666666, 754.3333333333333, 26.0, 26.67866088023206, 0.6741252954594135, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.6, 0.3322222222222222, 0.8335174953959483, 0.6666666666666666, 0.7232217400193383, 0.7247084318198045, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22179975], dtype=float32), 0.038456697]. 
=============================================
[2019-04-04 15:05:56,981] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8729108e-09 2.2382050e-10 2.2161851e-16 3.9637919e-15 1.0000000e+00
 9.7277311e-11 3.4414626e-16], sum to 1.0000
[2019-04-04 15:05:56,982] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6088
[2019-04-04 15:05:56,997] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.833333333333333, 64.16666666666667, 117.6666666666667, 826.6666666666667, 26.0, 26.49078522227996, 0.6073089545672509, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3759000.0000, 
sim time next is 3759600.0000, 
raw observation next is [-1.666666666666667, 63.33333333333334, 118.3333333333333, 827.8333333333334, 26.0, 26.52213526419407, 0.6115466160434041, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4164358264081256, 0.6333333333333334, 0.3944444444444443, 0.9147329650092082, 0.6666666666666666, 0.7101779386828392, 0.703848872014468, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6506451], dtype=float32), -0.036949445]. 
=============================================
[2019-04-04 15:05:58,342] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5470412e-09 8.1094620e-10 5.2004968e-15 4.4714622e-14 1.0000000e+00
 1.1126642e-09 2.6444261e-15], sum to 1.0000
[2019-04-04 15:05:58,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6888
[2019-04-04 15:05:58,361] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 26.0, 24.92799566016194, 0.2920715495732042, 0.0, 1.0, 42025.06065439757], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3910800.0000, 
sim time next is 3911400.0000, 
raw observation next is [-6.5, 61.5, 0.0, 0.0, 26.0, 24.91647843306871, 0.2834527412304775, 0.0, 1.0, 42172.9947809158], 
processed observation next is [1.0, 0.2608695652173913, 0.28254847645429365, 0.615, 0.0, 0.0, 0.6666666666666666, 0.5763732027557259, 0.5944842470768258, 0.0, 1.0, 0.2008237846710276], 
reward next is 0.7992, 
noisyNet noise sample is [array([-1.0249988], dtype=float32), 0.811397]. 
=============================================
[2019-04-04 15:06:00,580] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7934617e-09 7.5175854e-10 1.3389649e-15 6.5875528e-14 1.0000000e+00
 2.2714491e-10 3.9578690e-15], sum to 1.0000
[2019-04-04 15:06:00,593] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8022
[2019-04-04 15:06:00,617] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.333333333333334, 74.0, 0.0, 0.0, 26.0, 25.21222904062638, 0.3529531296142685, 0.0, 1.0, 41019.6012527259], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3906600.0000, 
sim time next is 3907200.0000, 
raw observation next is [-4.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.21442167566212, 0.3415768501379869, 0.0, 1.0, 41100.12623596632], 
processed observation next is [1.0, 0.21739130434782608, 0.3333333333333333, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6012018063051766, 0.6138589500459957, 0.0, 1.0, 0.19571488683793486], 
reward next is 0.8043, 
noisyNet noise sample is [array([-1.1095243], dtype=float32), -0.56293666]. 
=============================================
[2019-04-04 15:06:05,326] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.3525232e-09 1.1977879e-09 5.2630231e-15 1.1251852e-13 1.0000000e+00
 1.1408071e-09 2.1301730e-14], sum to 1.0000
[2019-04-04 15:06:05,330] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7740
[2019-04-04 15:06:05,362] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.5, 39.5, 19.0, 169.0, 26.0, 26.33107558949345, 0.5338173206632774, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3951000.0000, 
sim time next is 3951600.0000, 
raw observation next is [-5.666666666666667, 40.0, 15.83333333333333, 140.8333333333333, 26.0, 25.69708645369411, 0.5613352543673039, 1.0, 1.0, 196225.1200444002], 
processed observation next is [1.0, 0.7391304347826086, 0.30563250230840255, 0.4, 0.05277777777777777, 0.15561694290976053, 0.6666666666666666, 0.6414238711411757, 0.687111751455768, 1.0, 1.0, 0.9344053335447629], 
reward next is 0.0656, 
noisyNet noise sample is [array([-1.0682327], dtype=float32), 0.8748533]. 
=============================================
[2019-04-04 15:06:08,100] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 15:06:08,101] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 15:06:08,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:06:08,103] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run32
[2019-04-04 15:06:08,125] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 15:06:08,125] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 15:06:08,127] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:06:08,127] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:06:08,130] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run32
[2019-04-04 15:06:08,146] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run32
[2019-04-04 15:07:33,860] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17918776], dtype=float32), 0.22190252]
[2019-04-04 15:07:33,860] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.1, 65.16666666666667, 34.33333333333333, 336.0, 26.0, 25.35569567257776, 0.3848121132064675, 0.0, 1.0, 0.0]
[2019-04-04 15:07:33,860] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 15:07:33,860] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.4186358e-09 5.0504190e-10 2.5257464e-15 3.8115141e-14 1.0000000e+00
 1.3141326e-10 4.4500875e-15], sampled 0.819020810982031
[2019-04-04 15:07:34,917] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17918776], dtype=float32), 0.22190252]
[2019-04-04 15:07:34,917] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.3353875965, 86.67123716500001, 0.0, 0.0, 26.0, 25.04216600014013, 0.2830203148445808, 0.0, 1.0, 25594.59220999357]
[2019-04-04 15:07:34,917] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 15:07:34,918] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.8769854e-09 5.6170463e-10 6.1896103e-15 5.6596322e-14 1.0000000e+00
 2.6944663e-10 9.3795262e-15], sampled 0.21732794103249808
[2019-04-04 15:07:48,963] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5470 239915119.6159 1605.0337
[2019-04-04 15:08:08,667] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.5698 263430344.9894 1551.9755
[2019-04-04 15:08:12,963] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 15:08:13,987] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 3100000, evaluation results [3100000.0, 7241.569785764876, 263430344.989374, 1551.9755349129598, 7353.547049448098, 239915119.61590135, 1605.033734361032, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 15:08:17,254] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6844953e-09 8.7848584e-10 5.0046277e-15 5.3157458e-15 1.0000000e+00
 2.8260555e-10 1.9268273e-14], sum to 1.0000
[2019-04-04 15:08:17,255] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9565
[2019-04-04 15:08:17,267] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.833333333333333, 20.33333333333334, 94.0, 739.3333333333334, 26.0, 26.88903977727549, 0.7482908888315714, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4029000.0000, 
sim time next is 4029600.0000, 
raw observation next is [-1.666666666666667, 20.66666666666667, 91.5, 725.6666666666667, 26.0, 26.9905300581549, 0.7650979154336824, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4164358264081256, 0.20666666666666672, 0.305, 0.801841620626151, 0.6666666666666666, 0.7492108381795749, 0.7550326384778941, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.359866], dtype=float32), 1.8171325]. 
=============================================
[2019-04-04 15:08:25,248] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0768505e-09 3.2647102e-10 7.9450060e-16 4.8757168e-14 1.0000000e+00
 2.2947245e-10 1.5745957e-15], sum to 1.0000
[2019-04-04 15:08:25,253] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2214
[2019-04-04 15:08:25,291] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.833333333333333, 53.83333333333333, 189.3333333333333, 288.6666666666666, 26.0, 25.63672323938571, 0.4087722434559087, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4269000.0000, 
sim time next is 4269600.0000, 
raw observation next is [4.0, 54.0, 193.0, 367.5, 26.0, 25.62762144313424, 0.4094514685030994, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.5734072022160666, 0.54, 0.6433333333333333, 0.40607734806629836, 0.6666666666666666, 0.6356351202611868, 0.6364838228343664, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.414034], dtype=float32), 0.81671774]. 
=============================================
[2019-04-04 15:08:29,128] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4922232e-10 5.9541601e-11 4.9475059e-17 3.6266679e-16 1.0000000e+00
 6.0080628e-11 2.6293464e-16], sum to 1.0000
[2019-04-04 15:08:29,132] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6832
[2019-04-04 15:08:29,144] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.45, 72.5, 0.0, 0.0, 26.0, 25.75764765881978, 0.416628831264259, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4326600.0000, 
sim time next is 4327200.0000, 
raw observation next is [4.5, 72.0, 0.0, 0.0, 26.0, 25.75996148426364, 0.4042384615017142, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5872576177285319, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6466634570219701, 0.6347461538339048, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18476447], dtype=float32), -1.8124063]. 
=============================================
[2019-04-04 15:08:35,323] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.8109824e-09 9.8570901e-11 6.1169861e-15 9.1567507e-14 1.0000000e+00
 4.9965809e-10 7.6893549e-15], sum to 1.0000
[2019-04-04 15:08:35,323] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4773
[2019-04-04 15:08:35,348] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.20377811045935, 0.5477166381052251, 0.0, 1.0, 61797.19217690087], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4481400.0000, 
sim time next is 4482000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.39245166584374, 0.5660074290812441, 0.0, 1.0, 38476.85873349314], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6160376388203117, 0.6886691430270814, 0.0, 1.0, 0.18322313682615782], 
reward next is 0.8168, 
noisyNet noise sample is [array([0.03979287], dtype=float32), 1.9007969]. 
=============================================
[2019-04-04 15:08:35,356] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[85.25435]
 [85.48073]
 [85.7327 ]
 [85.6241 ]
 [85.73113]], R is [[84.76138306]
 [84.61949921]
 [84.2984848 ]
 [83.61592102]
 [82.83368683]].
[2019-04-04 15:08:41,173] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3686531e-09 2.8515254e-10 8.7933853e-15 1.4324728e-14 1.0000000e+00
 5.8229283e-10 3.0280469e-15], sum to 1.0000
[2019-04-04 15:08:41,174] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2093
[2019-04-04 15:08:41,184] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.0, 41.5, 34.66666666666666, 26.0, 25.77204713888094, 0.4873449694828345, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4555200.0000, 
sim time next is 4555800.0000, 
raw observation next is [2.0, 52.0, 28.0, 28.0, 26.0, 25.83258151928953, 0.4617869555935368, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.09333333333333334, 0.030939226519337018, 0.6666666666666666, 0.6527151266074608, 0.6539289851978456, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.94501716], dtype=float32), -0.3678052]. 
=============================================
[2019-04-04 15:08:42,694] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2314684e-09 5.0644450e-10 1.5241026e-15 7.3468860e-14 1.0000000e+00
 5.4891885e-10 4.8961795e-15], sum to 1.0000
[2019-04-04 15:08:42,696] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0612
[2019-04-04 15:08:42,721] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.5830897435604, 0.4188339325918073, 0.0, 1.0, 23945.44626980651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4683600.0000, 
sim time next is 4684200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.42787264414369, 0.4097084403791789, 0.0, 1.0, 108326.5479850814], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6189893870119741, 0.6365694801263929, 0.0, 1.0, 0.5158407046908638], 
reward next is 0.4842, 
noisyNet noise sample is [array([-0.19314955], dtype=float32), -0.019245628]. 
=============================================
[2019-04-04 15:08:43,079] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1668210e-09 2.6610271e-11 2.7262467e-15 1.2430262e-14 1.0000000e+00
 2.1992137e-10 2.3067295e-15], sum to 1.0000
[2019-04-04 15:08:43,090] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9398
[2019-04-04 15:08:43,106] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.7, 62.0, 0.0, 0.0, 26.0, 25.4372086035494, 0.4575492384960835, 0.0, 1.0, 44877.94352404569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4581000.0000, 
sim time next is 4581600.0000, 
raw observation next is [0.6000000000000001, 62.33333333333334, 0.0, 0.0, 26.0, 25.44949104116888, 0.4587839950303709, 0.0, 1.0, 31299.44606638633], 
processed observation next is [1.0, 0.0, 0.479224376731302, 0.6233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6207909200974067, 0.6529279983434569, 0.0, 1.0, 0.14904498126850632], 
reward next is 0.8510, 
noisyNet noise sample is [array([-1.6866031], dtype=float32), 0.91201985]. 
=============================================
[2019-04-04 15:08:47,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:08:47,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:08:47,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run24
[2019-04-04 15:08:49,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:08:49,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:08:49,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run24
[2019-04-04 15:08:49,951] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8650461e-09 3.3513928e-10 2.3731627e-15 1.5484264e-14 1.0000000e+00
 6.6246864e-10 6.3861871e-15], sum to 1.0000
[2019-04-04 15:08:49,952] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9791
[2019-04-04 15:08:49,984] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.5, 19.0, 0.0, 0.0, 26.0, 27.26932197662799, 0.887369831666908, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5085000.0000, 
sim time next is 5085600.0000, 
raw observation next is [9.333333333333334, 19.0, 0.0, 0.0, 26.0, 27.20477274652416, 0.8799240333733672, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7211449676823639, 0.19, 0.0, 0.0, 0.6666666666666666, 0.76706439554368, 0.7933080111244557, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44559315], dtype=float32), -0.49769956]. 
=============================================
[2019-04-04 15:08:51,154] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.93282415e-10 1.11659036e-10 1.31303681e-15 4.19404942e-14
 1.00000000e+00 2.12442685e-11 9.06456120e-15], sum to 1.0000
[2019-04-04 15:08:51,156] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0067
[2019-04-04 15:08:51,168] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666667, 78.0, 0.0, 0.0, 26.0, 24.84387083962324, 0.3220775500137752, 0.0, 1.0, 40576.26940792798], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4760400.0000, 
sim time next is 4761000.0000, 
raw observation next is [-5.0, 81.5, 0.0, 0.0, 26.0, 24.81723770468048, 0.3169860628961098, 0.0, 1.0, 40551.49441497983], 
processed observation next is [0.0, 0.08695652173913043, 0.32409972299168976, 0.815, 0.0, 0.0, 0.6666666666666666, 0.5681031420567066, 0.6056620209653699, 0.0, 1.0, 0.1931023543570468], 
reward next is 0.8069, 
noisyNet noise sample is [array([-1.9172994], dtype=float32), -2.005936]. 
=============================================
[2019-04-04 15:08:51,170] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[81.58539 ]
 [81.57132 ]
 [81.68002 ]
 [81.72452 ]
 [81.834076]], R is [[81.87807465]
 [81.86607361]
 [81.85398102]
 [81.84170532]
 [81.82928467]].
[2019-04-04 15:08:51,423] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:08:51,423] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:08:51,451] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run24
[2019-04-04 15:08:53,402] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.2396510e-09 6.9826378e-10 1.2347792e-14 2.6254252e-14 1.0000000e+00
 7.5903450e-10 1.4424664e-14], sum to 1.0000
[2019-04-04 15:08:53,403] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3994
[2019-04-04 15:08:53,421] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 37.0, 92.0, 667.6666666666667, 26.0, 25.1912201711568, 0.4396580218399413, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4809000.0000, 
sim time next is 4809600.0000, 
raw observation next is [3.0, 37.0, 89.5, 638.0, 26.0, 25.19429263062671, 0.4364635043255369, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.37, 0.29833333333333334, 0.7049723756906078, 0.6666666666666666, 0.5995243858855591, 0.645487834775179, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6796288], dtype=float32), 0.70910263]. 
=============================================
[2019-04-04 15:08:57,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:08:57,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:08:57,462] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run24
[2019-04-04 15:08:58,131] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4886631e-09 5.4392563e-10 7.1063379e-15 3.2730021e-14 1.0000000e+00
 1.3606610e-10 3.9831479e-15], sum to 1.0000
[2019-04-04 15:08:58,131] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4540
[2019-04-04 15:08:58,142] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.78149741914688, 0.2230238205461865, 0.0, 1.0, 39433.62836046446], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4857000.0000, 
sim time next is 4857600.0000, 
raw observation next is [-3.666666666666667, 67.33333333333334, 0.0, 0.0, 26.0, 24.77989551880392, 0.2174823293230662, 0.0, 1.0, 39493.65424150733], 
processed observation next is [0.0, 0.21739130434782608, 0.3610341643582641, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.5649912932336599, 0.5724941097743553, 0.0, 1.0, 0.18806502019765395], 
reward next is 0.8119, 
noisyNet noise sample is [array([-0.01735485], dtype=float32), -0.9595073]. 
=============================================
[2019-04-04 15:09:01,612] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:09:01,612] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:09:01,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run24
[2019-04-04 15:09:04,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:09:04,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:09:04,168] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run24
[2019-04-04 15:09:05,603] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:09:05,603] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:09:05,912] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run24
[2019-04-04 15:09:06,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:09:06,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:09:06,266] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run24
[2019-04-04 15:09:06,299] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:09:06,299] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:09:06,314] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run24
[2019-04-04 15:09:07,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:09:07,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:09:07,764] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:09:07,764] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:09:07,766] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run24
[2019-04-04 15:09:07,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run24
[2019-04-04 15:09:08,312] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:09:08,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:09:08,318] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:09:08,318] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:09:08,330] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run24
[2019-04-04 15:09:08,376] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run24
[2019-04-04 15:09:10,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:09:10,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:09:10,167] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run24
[2019-04-04 15:09:10,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:09:10,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:09:10,526] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run24
[2019-04-04 15:09:10,560] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:09:10,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:09:10,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run24
[2019-04-04 15:09:19,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2902528e-10 4.8917437e-12 5.8870193e-17 4.7489421e-16 1.0000000e+00
 3.9430161e-12 7.4756064e-17], sum to 1.0000
[2019-04-04 15:09:19,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7807
[2019-04-04 15:09:19,726] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.6, 82.0, 34.0, 0.0, 26.0, 24.55217378776486, 0.1824160254079449, 0.0, 1.0, 18736.73615302705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 57600.0000, 
sim time next is 58200.0000, 
raw observation next is [6.416666666666667, 82.66666666666667, 28.66666666666666, 0.0, 26.0, 24.54425347733297, 0.1774971767268634, 0.0, 1.0, 28652.25280020442], 
processed observation next is [0.0, 0.6956521739130435, 0.6403508771929826, 0.8266666666666667, 0.09555555555555553, 0.0, 0.6666666666666666, 0.545354456444414, 0.5591657255756212, 0.0, 1.0, 0.13643929904859248], 
reward next is 0.8636, 
noisyNet noise sample is [array([0.6983178], dtype=float32), 1.3203125]. 
=============================================
[2019-04-04 15:09:25,727] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1130360e-08 1.7737010e-08 2.2724597e-13 2.7624361e-12 1.0000000e+00
 4.8952327e-09 4.9688319e-13], sum to 1.0000
[2019-04-04 15:09:25,727] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7204
[2019-04-04 15:09:25,745] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.64423461643237, -0.2686205423049726, 0.0, 1.0, 44963.31874660851], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 196200.0000, 
sim time next is 196800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.59719105711297, -0.2739333097961998, 0.0, 1.0, 44975.45408805274], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3830992547594141, 0.40868889673460007, 0.0, 1.0, 0.21416882899072734], 
reward next is 0.7858, 
noisyNet noise sample is [array([-1.4660645], dtype=float32), -0.5947205]. 
=============================================
[2019-04-04 15:09:33,827] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4191284e-08 3.4486700e-09 8.2593966e-14 8.5966781e-13 1.0000000e+00
 6.9734178e-09 2.4802612e-14], sum to 1.0000
[2019-04-04 15:09:33,827] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0654
[2019-04-04 15:09:33,879] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.78816660034418, 0.224419554168731, 0.0, 1.0, 44883.81135981878], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 248400.0000, 
sim time next is 249000.0000, 
raw observation next is [-3.483333333333333, 66.66666666666667, 0.0, 0.0, 26.0, 24.73134698482353, 0.2137034001661745, 0.0, 1.0, 44754.75766822971], 
processed observation next is [1.0, 0.9130434782608695, 0.3661126500461681, 0.6666666666666667, 0.0, 0.0, 0.6666666666666666, 0.5609455820686277, 0.5712344667220582, 0.0, 1.0, 0.2131178936582367], 
reward next is 0.7869, 
noisyNet noise sample is [array([0.17494787], dtype=float32), 0.80053514]. 
=============================================
[2019-04-04 15:09:33,894] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[77.61755 ]
 [77.76868 ]
 [77.978615]
 [78.41149 ]
 [78.93295 ]], R is [[77.50331879]
 [77.51455688]
 [77.52353668]
 [77.52568817]
 [77.51056671]].
[2019-04-04 15:09:35,685] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.1342122e-09 2.9750424e-10 6.0798111e-15 8.2047087e-14 1.0000000e+00
 9.7976383e-10 6.5561625e-15], sum to 1.0000
[2019-04-04 15:09:35,686] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4428
[2019-04-04 15:09:35,715] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.483333333333333, 66.66666666666667, 0.0, 0.0, 26.0, 24.73144771209004, 0.2136858225065903, 0.0, 1.0, 44754.58327104349], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 249000.0000, 
sim time next is 249600.0000, 
raw observation next is [-3.566666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 24.67931413914277, 0.2037971931611094, 0.0, 1.0, 44714.0018859397], 
processed observation next is [1.0, 0.9130434782608695, 0.3638042474607572, 0.6833333333333335, 0.0, 0.0, 0.6666666666666666, 0.556609511595231, 0.5679323977203697, 0.0, 1.0, 0.21292381850447478], 
reward next is 0.7871, 
noisyNet noise sample is [array([0.58429784], dtype=float32), -0.39700907]. 
=============================================
[2019-04-04 15:09:56,017] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.6224660e-10 6.9910397e-12 7.1773284e-17 3.0680195e-16 1.0000000e+00
 1.4705374e-11 4.3193757e-17], sum to 1.0000
[2019-04-04 15:09:56,021] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0999
[2019-04-04 15:09:56,034] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.716666666666666, 96.83333333333334, 0.0, 0.0, 26.0, 24.88396069463973, 0.2412304247799812, 0.0, 1.0, 39846.87247087662], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 517800.0000, 
sim time next is 518400.0000, 
raw observation next is [3.8, 97.0, 0.0, 0.0, 26.0, 24.88323567034572, 0.2391238128371389, 0.0, 1.0, 39791.34591855376], 
processed observation next is [0.0, 0.0, 0.5678670360110805, 0.97, 0.0, 0.0, 0.6666666666666666, 0.57360297252881, 0.5797079376123796, 0.0, 1.0, 0.18948259961216074], 
reward next is 0.8105, 
noisyNet noise sample is [array([0.31397313], dtype=float32), 0.5925654]. 
=============================================
[2019-04-04 15:10:04,968] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2668448e-09 6.4781414e-10 8.7883217e-15 1.8375889e-14 1.0000000e+00
 5.5170668e-10 2.2068726e-15], sum to 1.0000
[2019-04-04 15:10:04,971] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6767
[2019-04-04 15:10:05,006] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 49.16666666666667, 103.0, 665.0, 26.0, 25.50997938792285, 0.3399924374455612, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 738600.0000, 
sim time next is 739200.0000, 
raw observation next is [0.5, 48.33333333333334, 96.0, 719.0, 26.0, 25.31673303315982, 0.3605706793463859, 1.0, 1.0, 18680.50499868238], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.48333333333333345, 0.32, 0.7944751381215469, 0.6666666666666666, 0.6097277527633184, 0.6201902264487953, 1.0, 1.0, 0.08895478570801134], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.66970843], dtype=float32), 2.0909307]. 
=============================================
[2019-04-04 15:10:22,336] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9853884e-10 5.2367586e-11 1.5433622e-16 2.9581957e-15 1.0000000e+00
 3.1954669e-12 6.9801629e-17], sum to 1.0000
[2019-04-04 15:10:22,336] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4230
[2019-04-04 15:10:22,360] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.600000000000001, 92.66666666666667, 24.0, 0.0, 26.0, 25.76920356991501, 0.3958484809577676, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 922800.0000, 
sim time next is 923400.0000, 
raw observation next is [4.7, 92.5, 18.0, 0.0, 26.0, 25.72357548480072, 0.2908680021155182, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.592797783933518, 0.925, 0.06, 0.0, 0.6666666666666666, 0.64363129040006, 0.5969560007051727, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.74407005], dtype=float32), 1.1048592]. 
=============================================
[2019-04-04 15:10:22,654] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.7542125e-10 2.1763247e-11 1.8403755e-16 2.8446083e-15 1.0000000e+00
 1.4370369e-11 3.9312314e-17], sum to 1.0000
[2019-04-04 15:10:22,656] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2923
[2019-04-04 15:10:22,672] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.7, 94.0, 0.0, 0.0, 26.0, 25.48897083306646, 0.3532077745564017, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 927000.0000, 
sim time next is 927600.0000, 
raw observation next is [4.600000000000001, 94.66666666666667, 0.0, 0.0, 26.0, 25.34320688888258, 0.327370925972723, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5900277008310251, 0.9466666666666668, 0.0, 0.0, 0.6666666666666666, 0.6119339074068817, 0.6091236419909077, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00182671], dtype=float32), 1.1971928]. 
=============================================
[2019-04-04 15:10:22,862] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8582520e-08 1.0506767e-08 1.8040997e-14 3.1771801e-12 1.0000000e+00
 2.7448860e-10 9.1377860e-14], sum to 1.0000
[2019-04-04 15:10:22,863] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1413
[2019-04-04 15:10:22,868] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.16666666666667, 95.0, 0.0, 0.0, 26.0, 23.7170114085833, 0.191493270372787, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1226400.0000, 
sim time next is 1227000.0000, 
raw observation next is [15.08333333333333, 95.5, 0.0, 0.0, 26.0, 23.69219324876644, 0.1869189535862591, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8804247460757156, 0.955, 0.0, 0.0, 0.6666666666666666, 0.4743494373972033, 0.5623063178620864, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.482577], dtype=float32), -0.09443832]. 
=============================================
[2019-04-04 15:10:22,883] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.627846]
 [75.58716 ]
 [75.54441 ]
 [75.51224 ]
 [75.458374]], R is [[75.90356445]
 [76.14453125]
 [76.38308716]
 [76.61925507]
 [76.85306549]].
[2019-04-04 15:10:24,600] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5173367e-10 3.3354229e-11 2.6154402e-17 7.5085056e-16 1.0000000e+00
 1.3921281e-11 3.5459129e-17], sum to 1.0000
[2019-04-04 15:10:24,603] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4967
[2019-04-04 15:10:24,651] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.82650111599638, 0.4746019627341261, 1.0, 1.0, 173374.309317247], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1363800.0000, 
sim time next is 1364400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.95394460889597, 0.4906961968056413, 1.0, 1.0, 25845.48700159445], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5794953840746642, 0.6635653989352138, 1.0, 1.0, 0.12307374762664024], 
reward next is 0.8769, 
noisyNet noise sample is [array([0.01314852], dtype=float32), -0.07639331]. 
=============================================
[2019-04-04 15:10:26,059] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6032750e-10 2.4535676e-11 2.1018618e-17 5.5548722e-16 1.0000000e+00
 1.5031292e-11 2.0585051e-17], sum to 1.0000
[2019-04-04 15:10:26,064] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7786
[2019-04-04 15:10:26,073] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.93643353753733, 0.6450209979118873, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1048800.0000, 
sim time next is 1049400.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 26.0234214506067, 0.6374973555410443, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6686184542172251, 0.7124991185136814, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25095987], dtype=float32), 1.6955055]. 
=============================================
[2019-04-04 15:10:36,621] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5767412e-08 9.9245945e-10 2.7592789e-14 2.1045791e-12 1.0000000e+00
 1.0253682e-10 2.5035599e-14], sum to 1.0000
[2019-04-04 15:10:36,624] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1007
[2019-04-04 15:10:36,629] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 96.0, 14.0, 0.0, 26.0, 23.43851452814444, 0.1348069236588776, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1238400.0000, 
sim time next is 1239000.0000, 
raw observation next is [15.0, 96.66666666666666, 18.66666666666667, 0.0, 26.0, 23.41746209946482, 0.1319165226970884, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.9666666666666666, 0.06222222222222224, 0.0, 0.6666666666666666, 0.4514551749554017, 0.5439721742323628, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.739436], dtype=float32), 0.20894551]. 
=============================================
[2019-04-04 15:10:36,639] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[75.60918]
 [75.53954]
 [75.51056]
 [75.55453]
 [75.60234]], R is [[75.92055511]
 [76.16135406]
 [76.39974213]
 [76.63574219]
 [76.86938477]].
[2019-04-04 15:10:42,487] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4911090e-10 9.8150793e-12 4.8123843e-17 4.4480000e-15 1.0000000e+00
 3.5420148e-12 2.1390618e-17], sum to 1.0000
[2019-04-04 15:10:42,492] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9009
[2019-04-04 15:10:42,528] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 31.5, 0.0, 26.0, 26.07992324149609, 0.5890901871270487, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1328400.0000, 
sim time next is 1329000.0000, 
raw observation next is [0.5, 92.0, 36.0, 0.0, 26.0, 26.05984138909694, 0.5781086598012113, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.12, 0.0, 0.6666666666666666, 0.6716534490914116, 0.6927028866004038, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4222186], dtype=float32), 0.80148244]. 
=============================================
[2019-04-04 15:10:42,534] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[93.07259]
 [93.29384]
 [93.48831]
 [93.60991]
 [93.65813]], R is [[92.92915344]
 [92.99986267]
 [93.06986237]
 [93.13916779]
 [93.20777893]].
[2019-04-04 15:10:44,281] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6566468e-10 7.9865307e-12 2.5977013e-17 2.3518397e-15 1.0000000e+00
 1.9387856e-11 4.9255485e-17], sum to 1.0000
[2019-04-04 15:10:44,281] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1926
[2019-04-04 15:10:44,292] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 71.0, 0.0, 26.0, 25.85235464632336, 0.5434953879497387, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1348200.0000, 
sim time next is 1348800.0000, 
raw observation next is [1.1, 92.0, 66.5, 0.0, 26.0, 25.85802911276725, 0.5349095939373768, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.22166666666666668, 0.0, 0.6666666666666666, 0.654835759397271, 0.6783031979791256, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5204733], dtype=float32), 0.2296364]. 
=============================================
[2019-04-04 15:10:46,003] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.0006891e-09 1.8935424e-10 5.4951427e-16 1.1824175e-14 1.0000000e+00
 2.5238828e-11 1.6842885e-15], sum to 1.0000
[2019-04-04 15:10:46,004] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2127
[2019-04-04 15:10:46,018] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.95, 82.33333333333334, 0.0, 0.0, 26.0, 25.63560340922874, 0.5172790510236501, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1576200.0000, 
sim time next is 1576800.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.65225850590992, 0.51796389238267, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.6011080332409973, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6376882088258267, 0.6726546307942233, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.53430134], dtype=float32), 0.9296106]. 
=============================================
[2019-04-04 15:10:51,103] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4253780e-09 3.9904050e-10 2.7239390e-15 5.8827184e-14 1.0000000e+00
 2.6063066e-10 6.2545230e-16], sum to 1.0000
[2019-04-04 15:10:51,103] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9342
[2019-04-04 15:10:51,129] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.4, 86.0, 0.0, 0.0, 26.0, 25.60156706412787, 0.5257040620735105, 0.0, 1.0, 18736.71446041186], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1566000.0000, 
sim time next is 1566600.0000, 
raw observation next is [4.433333333333334, 85.83333333333334, 0.0, 0.0, 26.0, 25.58568150761173, 0.5271026122787691, 0.0, 1.0, 24102.24390413803], 
processed observation next is [1.0, 0.13043478260869565, 0.5854108956602032, 0.8583333333333334, 0.0, 0.0, 0.6666666666666666, 0.632140125634311, 0.6757008707595897, 0.0, 1.0, 0.11477259001970491], 
reward next is 0.8852, 
noisyNet noise sample is [array([-0.37956753], dtype=float32), -0.8660993]. 
=============================================
[2019-04-04 15:10:55,365] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5319478e-09 4.3838816e-10 2.9163336e-16 3.3106594e-15 1.0000000e+00
 9.5616945e-11 3.4117513e-16], sum to 1.0000
[2019-04-04 15:10:55,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8883
[2019-04-04 15:10:55,382] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.533333333333333, 85.33333333333334, 0.0, 0.0, 26.0, 25.48338700527204, 0.5143972634559151, 0.0, 1.0, 72497.11947157135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1568400.0000, 
sim time next is 1569000.0000, 
raw observation next is [4.566666666666666, 85.16666666666667, 0.0, 0.0, 26.0, 25.50506664446357, 0.5237436402397214, 0.0, 1.0, 39447.80504307033], 
processed observation next is [1.0, 0.13043478260869565, 0.5891043397968606, 0.8516666666666667, 0.0, 0.0, 0.6666666666666666, 0.6254222203719643, 0.6745812134132404, 0.0, 1.0, 0.1878466906812873], 
reward next is 0.8122, 
noisyNet noise sample is [array([-1.7166605], dtype=float32), -1.0415332]. 
=============================================
[2019-04-04 15:10:55,389] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[88.05059 ]
 [87.79832 ]
 [87.692055]
 [87.72369 ]
 [87.690445]], R is [[88.05251312]
 [87.82676697]
 [87.71198273]
 [87.7279892 ]
 [87.73593903]].
[2019-04-04 15:10:59,220] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2546016e-09 1.7509533e-10 3.7848456e-15 1.6487723e-14 1.0000000e+00
 3.1956165e-10 5.3356734e-16], sum to 1.0000
[2019-04-04 15:10:59,222] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1535
[2019-04-04 15:10:59,241] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.48564855080937, 0.5398911094557138, 0.0, 1.0, 53866.41614994144], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1720800.0000, 
sim time next is 1721400.0000, 
raw observation next is [0.4166666666666667, 92.5, 0.0, 0.0, 26.0, 25.46671527503711, 0.4941724952729067, 0.0, 1.0, 55999.19084490597], 
processed observation next is [1.0, 0.9565217391304348, 0.47414589104339805, 0.925, 0.0, 0.0, 0.6666666666666666, 0.6222262729197592, 0.6647241650909689, 0.0, 1.0, 0.2666628135471713], 
reward next is 0.7333, 
noisyNet noise sample is [array([-0.98176837], dtype=float32), -1.9117689]. 
=============================================
[2019-04-04 15:11:03,547] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.2659884e-09 3.2520131e-10 1.6454540e-15 2.4206396e-14 1.0000000e+00
 1.1175385e-10 3.2710907e-15], sum to 1.0000
[2019-04-04 15:11:03,547] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4731
[2019-04-04 15:11:03,572] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.97097029862827, 0.3564493837303606, 0.0, 1.0, 43687.03440645937], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1748400.0000, 
sim time next is 1749000.0000, 
raw observation next is [-1.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.94479705201243, 0.3500979592379281, 0.0, 1.0, 43752.12185008932], 
processed observation next is [0.0, 0.21739130434782608, 0.4321329639889197, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5787330876677025, 0.616699319745976, 0.0, 1.0, 0.20834343738137773], 
reward next is 0.7917, 
noisyNet noise sample is [array([-0.23052382], dtype=float32), 0.7654774]. 
=============================================
[2019-04-04 15:11:03,579] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[84.87046 ]
 [85.00844 ]
 [85.15273 ]
 [85.32052 ]
 [85.483986]], R is [[84.68128967]
 [84.62644958]
 [84.57238007]
 [84.51898193]
 [84.46633148]].
[2019-04-04 15:11:16,309] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0942054e-08 5.4629096e-10 6.8067538e-15 2.1115782e-13 1.0000000e+00
 1.6075252e-10 8.6774892e-15], sum to 1.0000
[2019-04-04 15:11:16,310] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0351
[2019-04-04 15:11:16,366] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.683333333333334, 83.5, 0.0, 0.0, 26.0, 25.00312923333038, 0.2364678464526229, 0.0, 1.0, 55800.68009144457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1883400.0000, 
sim time next is 1884000.0000, 
raw observation next is [-4.866666666666667, 84.0, 0.0, 0.0, 26.0, 24.99505292210493, 0.2400432092444433, 0.0, 1.0, 115772.5200274078], 
processed observation next is [0.0, 0.8260869565217391, 0.3277931671283472, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5829210768420774, 0.5800144030814811, 0.0, 1.0, 0.5512977144162275], 
reward next is 0.4487, 
noisyNet noise sample is [array([0.531652], dtype=float32), 0.07025581]. 
=============================================
[2019-04-04 15:11:16,372] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.75197]
 [78.54158]
 [78.38001]
 [78.30602]
 [78.2085 ]], R is [[78.92166138]
 [78.86672974]
 [78.85868073]
 [78.8914566 ]
 [78.95570374]].
[2019-04-04 15:11:16,376] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4045830e-08 6.0934907e-10 1.6587209e-14 1.0376097e-13 1.0000000e+00
 5.1157445e-10 3.9713160e-14], sum to 1.0000
[2019-04-04 15:11:16,376] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4192
[2019-04-04 15:11:16,415] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.866666666666667, 84.0, 0.0, 0.0, 26.0, 24.99505292210493, 0.2400432092444433, 0.0, 1.0, 115772.5200274078], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1884000.0000, 
sim time next is 1884600.0000, 
raw observation next is [-5.05, 84.5, 0.0, 0.0, 26.0, 24.9830244114952, 0.2471645211193227, 0.0, 1.0, 74740.41001207838], 
processed observation next is [0.0, 0.8260869565217391, 0.32271468144044324, 0.845, 0.0, 0.0, 0.6666666666666666, 0.5819187009579334, 0.5823881737064409, 0.0, 1.0, 0.3559067143432304], 
reward next is 0.6441, 
noisyNet noise sample is [array([1.6131269], dtype=float32), -3.2212732]. 
=============================================
[2019-04-04 15:11:52,256] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.67549281e-08 3.53927665e-10 6.35791782e-15 3.97489252e-14
 1.00000000e+00 1.05299075e-10 1.83046310e-14], sum to 1.0000
[2019-04-04 15:11:52,263] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8842
[2019-04-04 15:11:52,315] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 52.0, 175.5, 0.0, 26.0, 24.94660948000242, 0.3057619257242475, 0.0, 1.0, 19947.55155383727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2383200.0000, 
sim time next is 2383800.0000, 
raw observation next is [0.0, 51.16666666666667, 170.3333333333333, 0.0, 26.0, 24.98782714809477, 0.3024683318389589, 0.0, 1.0, 18720.65590730978], 
processed observation next is [0.0, 0.6086956521739131, 0.46260387811634357, 0.5116666666666667, 0.5677777777777776, 0.0, 0.6666666666666666, 0.5823189290078975, 0.6008227772796529, 0.0, 1.0, 0.08914598051099897], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.20928691], dtype=float32), -1.9731022]. 
=============================================
[2019-04-04 15:11:56,213] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0291190e-09 5.4694541e-11 1.3778841e-15 7.9927977e-15 1.0000000e+00
 5.5307398e-11 5.5830837e-16], sum to 1.0000
[2019-04-04 15:11:56,213] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8258
[2019-04-04 15:11:56,257] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 61.0, 144.6666666666667, 228.6666666666667, 26.0, 25.85411381444265, 0.3890323068528931, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2798400.0000, 
sim time next is 2799000.0000, 
raw observation next is [-4.5, 59.5, 152.0, 233.0, 26.0, 25.87468273285439, 0.3960684279679203, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3379501385041552, 0.595, 0.5066666666666667, 0.2574585635359116, 0.6666666666666666, 0.6562235610711991, 0.6320228093226401, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21702501], dtype=float32), -0.441075]. 
=============================================
[2019-04-04 15:11:56,262] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.8471 ]
 [84.07067]
 [84.23833]
 [84.29056]
 [84.2348 ]], R is [[83.75606537]
 [83.91850281]
 [84.07931519]
 [84.23852539]
 [84.39614105]].
[2019-04-04 15:12:01,804] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2864041e-08 1.4734503e-09 3.6989349e-14 2.6994981e-13 1.0000000e+00
 5.7968980e-10 1.9951670e-14], sum to 1.0000
[2019-04-04 15:12:01,806] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2909
[2019-04-04 15:12:01,821] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 79.0, 0.0, 0.0, 26.0, 23.84301107417843, 0.01035204647957017, 0.0, 1.0, 44816.37177352951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2617200.0000, 
sim time next is 2617800.0000, 
raw observation next is [-7.3, 79.00000000000001, 0.0, 0.0, 26.0, 23.82375459192352, 0.00564730123107228, 0.0, 1.0, 44881.56385971064], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.7900000000000001, 0.0, 0.0, 0.6666666666666666, 0.48531288266029343, 0.5018824337436908, 0.0, 1.0, 0.21372173266528874], 
reward next is 0.7863, 
noisyNet noise sample is [array([0.9689882], dtype=float32), -0.54553515]. 
=============================================
[2019-04-04 15:12:03,719] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.6877282e-11 1.9147441e-11 2.9565566e-17 6.0535872e-16 1.0000000e+00
 2.9412108e-11 2.3908790e-17], sum to 1.0000
[2019-04-04 15:12:03,719] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1893
[2019-04-04 15:12:03,806] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.333333333333333, 93.0, 0.0, 0.0, 26.0, 25.42388963023659, 0.3927649236676361, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2914800.0000, 
sim time next is 2915400.0000, 
raw observation next is [1.166666666666667, 93.0, 0.0, 0.0, 26.0, 24.93217394120394, 0.3859159427665226, 1.0, 1.0, 150162.3771133921], 
processed observation next is [1.0, 0.7391304347826086, 0.49492151431209613, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5776811617669949, 0.6286386475888409, 1.0, 1.0, 0.7150589386352005], 
reward next is 0.2849, 
noisyNet noise sample is [array([0.9270776], dtype=float32), -2.0570452]. 
=============================================
[2019-04-04 15:12:04,404] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9019263e-09 2.8758368e-10 6.0183156e-15 3.9328123e-14 1.0000000e+00
 2.2794958e-10 1.0537280e-14], sum to 1.0000
[2019-04-04 15:12:04,405] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4438
[2019-04-04 15:12:04,422] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.833333333333333, 84.16666666666666, 0.0, 0.0, 26.0, 24.66953346760217, 0.2749367843015045, 0.0, 1.0, 42836.4864539621], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2947800.0000, 
sim time next is 2948400.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.64372066448442, 0.2656114594595926, 0.0, 1.0, 42759.22353337883], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.6666666666666666, 0.553643388707035, 0.5885371531531975, 0.0, 1.0, 0.2036153501589468], 
reward next is 0.7964, 
noisyNet noise sample is [array([-2.8936396], dtype=float32), 1.5709126]. 
=============================================
[2019-04-04 15:12:05,227] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0939278e-09 6.4699424e-10 6.2040776e-15 1.1665113e-13 1.0000000e+00
 5.7532124e-10 3.0737569e-14], sum to 1.0000
[2019-04-04 15:12:05,228] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3547
[2019-04-04 15:12:05,246] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.90434066450486, 0.3399474377342013, 0.0, 1.0, 43355.78700594074], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2938800.0000, 
sim time next is 2939400.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.8637901256391, 0.337160911326789, 0.0, 1.0, 43350.80398310196], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5719825104699249, 0.612386970442263, 0.0, 1.0, 0.20643239991953316], 
reward next is 0.7936, 
noisyNet noise sample is [array([-1.1012009], dtype=float32), -1.2936784]. 
=============================================
[2019-04-04 15:12:05,257] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0897213e-08 3.1259395e-10 4.8752870e-15 5.0870498e-14 1.0000000e+00
 1.2792960e-10 1.0765870e-14], sum to 1.0000
[2019-04-04 15:12:05,260] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5711
[2019-04-04 15:12:05,284] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.166666666666666, 64.0, 112.3333333333333, 787.0, 26.0, 25.98686026843815, 0.4714517048107085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2721000.0000, 
sim time next is 2721600.0000, 
raw observation next is [-8.0, 64.0, 112.5, 790.0, 26.0, 25.94380731560665, 0.4710848062256616, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.24099722991689754, 0.64, 0.375, 0.8729281767955801, 0.6666666666666666, 0.661983942967221, 0.6570282687418872, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41938964], dtype=float32), -1.2316822]. 
=============================================
[2019-04-04 15:12:06,989] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-04 15:12:06,990] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 15:12:06,990] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:12:06,992] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run33
[2019-04-04 15:12:07,019] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 15:12:07,019] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:12:07,020] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 15:12:07,020] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:12:07,023] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run33
[2019-04-04 15:12:07,038] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run33
[2019-04-04 15:12:10,739] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17935461], dtype=float32), 0.22337814]
[2019-04-04 15:12:10,740] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [7.2, 96.0, 0.0, 0.0, 19.5, 19.46480724927993, -0.9325486225473306, 0.0, 1.0, 0.0]
[2019-04-04 15:12:10,740] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 15:12:10,741] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.5646973e-03 5.0952835e-03 2.8198061e-05 1.2534963e-04 9.9064273e-01
 1.5145128e-03 2.9182336e-05], sampled 0.2552539103363912
[2019-04-04 15:12:52,866] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17935461], dtype=float32), 0.22337814]
[2019-04-04 15:12:52,866] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.2, 89.66666666666667, 0.0, 0.0, 26.0, 25.19614756311137, 0.4151683756996989, 0.0, 1.0, 43074.7018703397]
[2019-04-04 15:12:52,867] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 15:12:52,867] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.6661353e-10 8.0561169e-11 1.5200607e-16 3.4863908e-15 1.0000000e+00
 1.4430659e-11 3.5498666e-16], sampled 0.44299597490559695
[2019-04-04 15:13:46,793] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17935461], dtype=float32), 0.22337814]
[2019-04-04 15:13:46,793] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.75, 39.0, 0.0, 0.0, 26.0, 25.75093550107426, 0.5060351032716296, 0.0, 1.0, 0.0]
[2019-04-04 15:13:46,793] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 15:13:46,793] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.4604596e-09 5.5771943e-10 7.6310875e-15 5.3523134e-14 1.0000000e+00
 2.5888419e-10 1.5937774e-14], sampled 0.16927672830846807
[2019-04-04 15:13:49,044] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.6115 239901591.9745 1605.2062
[2019-04-04 15:13:55,872] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17935461], dtype=float32), 0.22337814]
[2019-04-04 15:13:55,872] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.416666666666667, 83.33333333333333, 135.0, 165.0, 26.0, 26.03915560466884, 0.599822331721645, 1.0, 1.0, 0.0]
[2019-04-04 15:13:55,872] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 15:13:55,873] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.0274512e-10 9.6005599e-12 7.8634370e-18 2.3103213e-16 1.0000000e+00
 1.4645270e-12 1.9496970e-17], sampled 0.09103509958330591
[2019-04-04 15:14:07,649] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6351 263416630.7723 1557.0695
[2019-04-04 15:14:09,056] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 15:14:10,080] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 3200000, evaluation results [3200000.0, 7241.6350915605335, 263416630.7722868, 1557.0695496036576, 7353.611466788218, 239901591.97446975, 1605.2062106696299, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 15:14:12,936] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9009379e-09 1.0912094e-10 5.2608551e-15 2.7667044e-14 1.0000000e+00
 5.2852783e-10 1.1382997e-15], sum to 1.0000
[2019-04-04 15:14:12,939] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2408
[2019-04-04 15:14:13,001] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.166666666666666, 64.0, 35.99999999999999, 68.99999999999999, 26.0, 24.92644148372333, 0.2996700987276067, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2793000.0000, 
sim time next is 2793600.0000, 
raw observation next is [-6.0, 64.0, 54.0, 103.5, 26.0, 25.38640044371159, 0.3333955925624055, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.18, 0.1143646408839779, 0.6666666666666666, 0.6155333703092992, 0.6111318641874685, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27373114], dtype=float32), 0.36090025]. 
=============================================
[2019-04-04 15:14:21,380] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5965379e-09 3.8366914e-11 8.2434607e-16 8.3305563e-15 1.0000000e+00
 1.1526194e-10 4.2332835e-15], sum to 1.0000
[2019-04-04 15:14:21,381] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6646
[2019-04-04 15:14:21,393] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.9860413662608, 0.3559384781744819, 0.0, 1.0, 43347.39777312176], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2937600.0000, 
sim time next is 2938200.0000, 
raw observation next is [-2.0, 85.00000000000001, 0.0, 0.0, 26.0, 24.94570141990914, 0.3473947106934908, 0.0, 1.0, 43352.70557267527], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.8500000000000001, 0.0, 0.0, 0.6666666666666666, 0.5788084516590949, 0.6157982368978302, 0.0, 1.0, 0.20644145510797748], 
reward next is 0.7936, 
noisyNet noise sample is [array([-0.21977624], dtype=float32), -0.7011614]. 
=============================================
[2019-04-04 15:14:32,430] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6619672e-10 7.7805410e-12 3.7069103e-17 6.1677350e-16 1.0000000e+00
 7.6821249e-12 3.6807155e-17], sum to 1.0000
[2019-04-04 15:14:32,430] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1554
[2019-04-04 15:14:32,451] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 26.0, 25.29684615272486, 0.3085980096283799, 0.0, 1.0, 64824.05082178712], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3117000.0000, 
sim time next is 3117600.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 26.0, 25.25749464265155, 0.3152122059527796, 0.0, 1.0, 48293.26837795327], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6047912202209625, 0.6050707353175931, 0.0, 1.0, 0.22996794465692033], 
reward next is 0.7700, 
noisyNet noise sample is [array([1.1146628], dtype=float32), 0.088464655]. 
=============================================
[2019-04-04 15:14:37,642] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7173823e-09 7.5618026e-11 1.1517566e-16 4.7047852e-15 1.0000000e+00
 2.2465783e-11 3.9809452e-15], sum to 1.0000
[2019-04-04 15:14:37,642] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4585
[2019-04-04 15:14:37,665] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 48.0, 104.0, 711.0, 26.0, 26.53790666784383, 0.6002541237615617, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3405600.0000, 
sim time next is 3406200.0000, 
raw observation next is [2.166666666666667, 48.16666666666666, 105.6666666666667, 728.6666666666666, 26.0, 26.57625097970191, 0.6127545354310061, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5226223453370269, 0.4816666666666666, 0.3522222222222223, 0.8051565377532228, 0.6666666666666666, 0.7146875816418259, 0.7042515118103353, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5130502], dtype=float32), -1.4134002]. 
=============================================
[2019-04-04 15:14:40,768] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.7967820e-10 4.4285218e-11 4.2507720e-17 3.8864855e-16 1.0000000e+00
 1.7018040e-11 6.9937830e-17], sum to 1.0000
[2019-04-04 15:14:40,769] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3954
[2019-04-04 15:14:40,776] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 73.33333333333334, 114.3333333333333, 819.0, 26.0, 26.66906271743749, 0.7655589425682305, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3240600.0000, 
sim time next is 3241200.0000, 
raw observation next is [-2.0, 75.66666666666667, 114.6666666666667, 821.0, 26.0, 26.69584129301002, 0.7607995940254341, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.7566666666666667, 0.38222222222222235, 0.907182320441989, 0.6666666666666666, 0.7246534410841683, 0.7535998646751447, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2615068], dtype=float32), 1.1888427]. 
=============================================
[2019-04-04 15:14:47,256] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.0461348e-08 6.1735710e-09 1.9417729e-14 3.5630998e-13 1.0000000e+00
 1.0241353e-09 8.8576593e-14], sum to 1.0000
[2019-04-04 15:14:47,258] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5256
[2019-04-04 15:14:47,270] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.815426107001, 0.2758249883687985, 0.0, 1.0, 41115.58644341333], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3381600.0000, 
sim time next is 3382200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.82780537830124, 0.2769812020430907, 0.0, 1.0, 41144.1849516783], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5689837815251032, 0.5923270673476969, 0.0, 1.0, 0.19592469024608714], 
reward next is 0.8041, 
noisyNet noise sample is [array([-0.4191313], dtype=float32), -1.2749829]. 
=============================================
[2019-04-04 15:14:47,624] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0572271e-08 1.1648928e-09 8.8174360e-15 4.2184409e-13 1.0000000e+00
 1.0118177e-09 1.3127020e-14], sum to 1.0000
[2019-04-04 15:14:47,625] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8017
[2019-04-04 15:14:47,642] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.90944652119394, 0.5414149296304905, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3349800.0000, 
sim time next is 3350400.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.76818602890177, 0.5272174978712773, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6473488357418141, 0.6757391659570925, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02951444], dtype=float32), 0.56383175]. 
=============================================
[2019-04-04 15:14:50,548] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0692879e-09 1.4456010e-10 4.1684770e-16 6.4221479e-15 1.0000000e+00
 8.3406532e-11 6.2886384e-16], sum to 1.0000
[2019-04-04 15:14:50,550] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1931
[2019-04-04 15:14:50,567] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.0, 59.0, 39.5, 343.5, 26.0, 25.44486223756817, 0.4365503331484107, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3690000.0000, 
sim time next is 3690600.0000, 
raw observation next is [4.0, 59.0, 31.33333333333333, 284.0, 26.0, 25.41548098322233, 0.4223696228265998, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.10444444444444442, 0.3138121546961326, 0.6666666666666666, 0.6179567486018609, 0.6407898742755332, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32673663], dtype=float32), -1.8174019]. 
=============================================
[2019-04-04 15:14:55,755] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9174219e-10 1.6055020e-11 2.6432939e-17 1.5140025e-15 1.0000000e+00
 4.6207422e-12 6.3295289e-16], sum to 1.0000
[2019-04-04 15:14:55,756] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2039
[2019-04-04 15:14:55,774] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6666666666666667, 67.33333333333334, 97.16666666666666, 624.8333333333334, 26.0, 25.71638604615123, 0.4766960604543884, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3489600.0000, 
sim time next is 3490200.0000, 
raw observation next is [-0.5, 65.5, 99.0, 670.0, 26.0, 25.72888106260263, 0.4989191371476983, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.44875346260387816, 0.655, 0.33, 0.7403314917127072, 0.6666666666666666, 0.6440734218835523, 0.6663063790492328, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3581568], dtype=float32), -1.2796423]. 
=============================================
[2019-04-04 15:15:03,669] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0580659e-09 1.6270180e-09 2.6949168e-15 1.8942067e-14 1.0000000e+00
 6.2935895e-10 7.4675763e-15], sum to 1.0000
[2019-04-04 15:15:03,673] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0390
[2019-04-04 15:15:03,698] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.166666666666666, 31.16666666666667, 32.33333333333333, 215.0, 26.0, 25.46169849063133, 0.3835480248315149, 0.0, 1.0, 29651.87678953702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3657000.0000, 
sim time next is 3657600.0000, 
raw observation next is [8.0, 32.0, 46.5, 262.0, 26.0, 25.48169022511965, 0.3969666850992357, 0.0, 1.0, 18753.17327691974], 
processed observation next is [0.0, 0.34782608695652173, 0.6842105263157896, 0.32, 0.155, 0.28950276243093925, 0.6666666666666666, 0.6234741854266375, 0.6323222283664119, 0.0, 1.0, 0.08930082512818924], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.5967578], dtype=float32), 1.0524337]. 
=============================================
[2019-04-04 15:15:03,978] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.11152785e-08 1.37316047e-09 1.02836474e-14 1.27802093e-13
 1.00000000e+00 5.47567824e-10 2.59194437e-14], sum to 1.0000
[2019-04-04 15:15:03,980] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0685
[2019-04-04 15:15:04,017] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 40.5, 14.0, 142.0, 26.0, 25.10712207788762, 0.379269494259753, 0.0, 1.0, 48596.91737097686], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3605400.0000, 
sim time next is 3606000.0000, 
raw observation next is [-0.6666666666666666, 41.0, 11.66666666666667, 118.3333333333333, 26.0, 25.1025425900159, 0.3763555962860612, 0.0, 1.0, 39784.42166779109], 
processed observation next is [0.0, 0.7391304347826086, 0.44413665743305636, 0.41, 0.038888888888888896, 0.1307550644567219, 0.6666666666666666, 0.5918785491679918, 0.625451865428687, 0.0, 1.0, 0.18944962698948137], 
reward next is 0.8106, 
noisyNet noise sample is [array([0.65391713], dtype=float32), -0.9556969]. 
=============================================
[2019-04-04 15:15:04,030] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.840096]
 [74.10474 ]
 [74.34866 ]
 [74.644936]
 [75.02523 ]], R is [[73.71630096]
 [73.74772644]
 [73.76233673]
 [73.83552551]
 [74.00610352]].
[2019-04-04 15:15:05,181] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5874660e-09 3.3467122e-09 3.5182147e-15 2.5238453e-14 1.0000000e+00
 1.1693065e-10 3.8713501e-15], sum to 1.0000
[2019-04-04 15:15:05,182] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3008
[2019-04-04 15:15:05,198] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.166666666666666, 27.33333333333333, 0.0, 0.0, 26.0, 25.47418383043089, 0.3781124192267014, 0.0, 1.0, 36744.725268706], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3640200.0000, 
sim time next is 3640800.0000, 
raw observation next is [8.133333333333333, 27.66666666666667, 0.0, 0.0, 26.0, 25.58775345365774, 0.3782749453839876, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.687903970452447, 0.2766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6323127878048117, 0.6260916484613291, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37765515], dtype=float32), 0.8535731]. 
=============================================
[2019-04-04 15:15:05,253] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.5343310e-10 6.5399894e-11 1.1457312e-16 1.5481274e-15 1.0000000e+00
 3.5682721e-11 1.4764053e-15], sum to 1.0000
[2019-04-04 15:15:05,254] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7204
[2019-04-04 15:15:05,274] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.0, 71.66666666666666, 596.3333333333333, 26.0, 26.9096799805421, 0.7158404930101775, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3773400.0000, 
sim time next is 3774000.0000, 
raw observation next is [0.0, 60.00000000000001, 67.83333333333333, 567.6666666666666, 26.0, 26.94264666283271, 0.4443733955049413, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6000000000000001, 0.2261111111111111, 0.627255985267035, 0.6666666666666666, 0.7452205552360592, 0.6481244651683138, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13954368], dtype=float32), -0.42127457]. 
=============================================
[2019-04-04 15:15:05,286] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[86.14847]
 [86.24848]
 [86.34161]
 [86.45078]
 [86.53663]], R is [[86.07950592]
 [86.21871185]
 [86.35652924]
 [86.4929657 ]
 [86.6280365 ]].
[2019-04-04 15:15:08,931] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6302983e-09 5.4777671e-10 1.0426676e-15 2.0974597e-14 1.0000000e+00
 1.7126481e-10 2.9441345e-15], sum to 1.0000
[2019-04-04 15:15:08,931] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6895
[2019-04-04 15:15:08,950] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.49576034770774, 0.4007598034632758, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3718200.0000, 
sim time next is 3718800.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.58347323414564, 0.3874643585629532, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.63195610284547, 0.6291547861876511, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8727365], dtype=float32), -0.93240666]. 
=============================================
[2019-04-04 15:15:10,280] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5562195e-10 3.4782680e-11 6.3539594e-17 1.8428703e-15 1.0000000e+00
 5.4927855e-12 1.9700259e-16], sum to 1.0000
[2019-04-04 15:15:10,281] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1696
[2019-04-04 15:15:10,296] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 76.0, 107.3333333333333, 737.6666666666667, 26.0, 26.26422059017912, 0.5274202677437123, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3751800.0000, 
sim time next is 3752400.0000, 
raw observation next is [-3.0, 75.0, 109.1666666666667, 753.3333333333334, 26.0, 26.30272796527059, 0.5386771680860801, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.75, 0.363888888888889, 0.8324125230202579, 0.6666666666666666, 0.6918939971058826, 0.6795590560286934, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6354282], dtype=float32), -1.1902016]. 
=============================================
[2019-04-04 15:15:11,282] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.6585150e-10 7.8096862e-11 1.9266672e-16 1.2080952e-15 1.0000000e+00
 7.9656323e-11 6.2484657e-16], sum to 1.0000
[2019-04-04 15:15:11,283] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0006
[2019-04-04 15:15:11,299] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 48.0, 109.5, 803.0, 26.0, 26.27062084609477, 0.6692793992220722, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3852000.0000, 
sim time next is 3852600.0000, 
raw observation next is [2.0, 48.0, 108.3333333333333, 796.0, 26.0, 26.50343369946635, 0.7001165219030187, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.361111111111111, 0.8795580110497238, 0.6666666666666666, 0.7086194749555291, 0.7333721739676728, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7281182], dtype=float32), -1.257804]. 
=============================================
[2019-04-04 15:15:16,582] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.7113440e-11 1.5413325e-12 2.1574171e-18 7.1463197e-17 1.0000000e+00
 3.0648234e-14 2.1909235e-18], sum to 1.0000
[2019-04-04 15:15:16,583] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5827
[2019-04-04 15:15:16,597] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.0, 42.0, 111.0, 728.5, 26.0, 27.232602144565, 0.7609039229163285, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4356000.0000, 
sim time next is 4356600.0000, 
raw observation next is [10.43333333333333, 40.66666666666666, 112.3333333333333, 745.6666666666667, 26.0, 27.33201617691212, 0.7863973937373115, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7516158818097877, 0.40666666666666657, 0.37444444444444436, 0.8239410681399633, 0.6666666666666666, 0.7776680147426767, 0.7621324645791039, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0675082], dtype=float32), 1.4501044]. 
=============================================
[2019-04-04 15:15:22,354] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6506527e-07 3.0342544e-08 3.0579280e-13 4.7922989e-12 9.9999976e-01
 1.7247755e-08 7.0565044e-13], sum to 1.0000
[2019-04-04 15:15:22,354] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1435
[2019-04-04 15:15:22,372] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 36.0, 0.0, 0.0, 26.0, 24.82899811774097, 0.2207144359255359, 0.0, 1.0, 40190.81714542563], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4080600.0000, 
sim time next is 4081200.0000, 
raw observation next is [-4.0, 36.66666666666666, 0.0, 0.0, 26.0, 24.81842458596958, 0.2128651160840288, 0.0, 1.0, 40176.57983001737], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.3666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5682020488307984, 0.5709550386946763, 0.0, 1.0, 0.1913170468096065], 
reward next is 0.8087, 
noisyNet noise sample is [array([-0.24464759], dtype=float32), 0.7047558]. 
=============================================
[2019-04-04 15:15:33,730] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9139947e-08 2.8627476e-09 2.9202609e-14 1.5511526e-13 1.0000000e+00
 5.1266552e-10 2.4581730e-14], sum to 1.0000
[2019-04-04 15:15:33,731] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2297
[2019-04-04 15:15:33,769] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 35.0, 114.8333333333333, 782.0, 26.0, 25.21173631433666, 0.3889708093590205, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4185600.0000, 
sim time next is 4186200.0000, 
raw observation next is [-1.166666666666667, 35.0, 115.6666666666667, 790.0, 26.0, 25.17301930936225, 0.3830636864091437, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.43028624192059095, 0.35, 0.38555555555555565, 0.8729281767955801, 0.6666666666666666, 0.5977516091135208, 0.6276878954697146, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2017175], dtype=float32), -1.0769846]. 
=============================================
[2019-04-04 15:15:39,302] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0903445e-09 7.9763050e-11 1.2337845e-16 8.0420029e-16 1.0000000e+00
 8.1355783e-11 6.4656788e-16], sum to 1.0000
[2019-04-04 15:15:39,303] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9603
[2019-04-04 15:15:39,325] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.4, 61.0, 0.0, 0.0, 26.0, 26.52151481157786, 0.777224637571886, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4399200.0000, 
sim time next is 4399800.0000, 
raw observation next is [9.25, 61.16666666666667, 0.0, 0.0, 26.0, 26.46540833035172, 0.7139046930272178, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.718836565096953, 0.6116666666666667, 0.0, 0.0, 0.6666666666666666, 0.7054506941959767, 0.7379682310090726, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49799114], dtype=float32), -0.5422482]. 
=============================================
[2019-04-04 15:15:40,236] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3532289e-09 8.6132351e-10 1.9678638e-15 2.8038005e-14 1.0000000e+00
 5.5810113e-11 4.8821171e-15], sum to 1.0000
[2019-04-04 15:15:40,237] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9157
[2019-04-04 15:15:40,253] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.5, 41.5, 170.0, 787.0, 26.0, 25.08925776170359, 0.4318014553906663, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4797000.0000, 
sim time next is 4797600.0000, 
raw observation next is [1.666666666666667, 41.0, 178.3333333333333, 750.5, 26.0, 25.10129847168619, 0.443575185111127, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5087719298245615, 0.41, 0.5944444444444443, 0.8292817679558011, 0.6666666666666666, 0.5917748726405158, 0.6478583950370423, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3825472], dtype=float32), 0.36624548]. 
=============================================
[2019-04-04 15:15:53,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:15:53,286] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:15:53,296] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run25
[2019-04-04 15:15:55,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:15:55,730] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:15:55,763] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run25
[2019-04-04 15:15:56,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:15:56,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:15:56,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run25
[2019-04-04 15:15:57,757] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7936333e-09 4.5367093e-10 2.3144860e-15 3.7662896e-14 1.0000000e+00
 1.1271222e-10 3.6174677e-15], sum to 1.0000
[2019-04-04 15:15:57,758] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3733
[2019-04-04 15:15:57,781] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8333333333333334, 48.83333333333334, 0.0, 0.0, 26.0, 25.33020418340369, 0.3209664800633216, 0.0, 1.0, 58929.16674712829], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4931400.0000, 
sim time next is 4932000.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.35108440801602, 0.3477412579762458, 0.0, 1.0, 45568.34704438726], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6125903673346684, 0.6159137526587486, 0.0, 1.0, 0.21699212878279647], 
reward next is 0.7830, 
noisyNet noise sample is [array([0.8852491], dtype=float32), -0.32459247]. 
=============================================
[2019-04-04 15:15:57,785] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.00441 ]
 [79.62013 ]
 [79.79272 ]
 [79.623665]
 [79.47051 ]], R is [[79.72176361]
 [79.64392853]
 [79.43566132]
 [79.27290344]
 [79.34606934]].
[2019-04-04 15:16:00,758] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:16:00,759] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:16:00,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run25
[2019-04-04 15:16:06,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:16:06,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:16:06,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run25
[2019-04-04 15:16:06,344] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.0895740e-09 7.6412113e-11 6.6429877e-16 1.4793771e-14 1.0000000e+00
 4.2187993e-11 5.9205787e-16], sum to 1.0000
[2019-04-04 15:16:06,347] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8626
[2019-04-04 15:16:06,394] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.95, 87.5, 0.0, 0.0, 26.0, 24.56837755939077, 0.1890348308112841, 0.0, 1.0, 53308.25968608323], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 63000.0000, 
sim time next is 63600.0000, 
raw observation next is [4.766666666666667, 88.0, 0.0, 0.0, 26.0, 24.55270538451181, 0.1952383687254217, 0.0, 1.0, 59872.17518422443], 
processed observation next is [0.0, 0.7391304347826086, 0.5946445060018468, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5460587820426509, 0.5650794562418072, 0.0, 1.0, 0.2851055961153544], 
reward next is 0.7149, 
noisyNet noise sample is [array([-0.47570962], dtype=float32), -0.5402759]. 
=============================================
[2019-04-04 15:16:07,300] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5439977e-09 9.4904334e-11 5.2068661e-16 3.6135230e-15 1.0000000e+00
 4.7231191e-11 3.8125938e-16], sum to 1.0000
[2019-04-04 15:16:07,300] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9105
[2019-04-04 15:16:07,324] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.0, 26.0, 110.3333333333333, 825.8333333333333, 26.0, 27.45616628454879, 0.8384583496162615, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4976400.0000, 
sim time next is 4977000.0000, 
raw observation next is [8.0, 26.0, 109.0, 819.0, 26.0, 27.55090251344456, 0.8557577913807152, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.36333333333333334, 0.9049723756906077, 0.6666666666666666, 0.7959085427870466, 0.7852525971269051, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51473504], dtype=float32), 2.0171175]. 
=============================================
[2019-04-04 15:16:07,336] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[87.66834 ]
 [87.48958 ]
 [87.28809 ]
 [87.06305 ]
 [86.902565]], R is [[87.97889709]
 [88.09910583]
 [88.21811676]
 [88.3359375 ]
 [88.45257568]].
[2019-04-04 15:16:07,566] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8629933e-08 7.2669915e-09 1.8630446e-14 1.9892630e-13 1.0000000e+00
 6.7323347e-10 4.4636400e-14], sum to 1.0000
[2019-04-04 15:16:07,567] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8607
[2019-04-04 15:16:07,595] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 36.66666666666666, 0.0, 0.0, 26.0, 25.51425919743516, 0.3759394758326453, 0.0, 1.0, 26716.74085927187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4913400.0000, 
sim time next is 4914000.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 26.0, 25.48249736690263, 0.3701125607247506, 0.0, 1.0, 47007.96326142089], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 0.6666666666666666, 0.623541447241886, 0.6233708535749168, 0.0, 1.0, 0.22384744410200424], 
reward next is 0.7762, 
noisyNet noise sample is [array([1.6322144], dtype=float32), 1.120435]. 
=============================================
[2019-04-04 15:16:07,601] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[78.8567  ]
 [79.01284 ]
 [79.16839 ]
 [79.424255]
 [79.614235]], R is [[78.82228088]
 [78.90683746]
 [79.02848816]
 [79.23820496]
 [79.44582367]].
[2019-04-04 15:16:07,775] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.3535898e-09 5.0668891e-10 5.7273590e-15 6.0683939e-13 1.0000000e+00
 1.2064173e-10 6.3575053e-15], sum to 1.0000
[2019-04-04 15:16:07,776] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3494
[2019-04-04 15:16:07,788] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.25092706119304, -0.564531496373125, 0.0, 1.0, 40345.80124233884], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 24600.0000, 
sim time next is 25200.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.27033401808782, -0.558808535824341, 0.0, 1.0, 40341.41858721609], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.27252783484065163, 0.313730488058553, 0.0, 1.0, 0.19210199327245756], 
reward next is 0.8079, 
noisyNet noise sample is [array([-0.27520955], dtype=float32), 0.58805543]. 
=============================================
[2019-04-04 15:16:10,272] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.8618332e-09 1.5364998e-10 5.3438414e-16 1.3819485e-14 1.0000000e+00
 2.7462843e-10 2.8070846e-15], sum to 1.0000
[2019-04-04 15:16:10,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5216
[2019-04-04 15:16:10,285] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.166666666666666, 24.83333333333334, 122.6666666666667, 858.3333333333334, 26.0, 27.05334646870244, 0.7241854512911137, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4968600.0000, 
sim time next is 4969200.0000, 
raw observation next is [6.333333333333333, 24.66666666666667, 122.8333333333333, 861.6666666666667, 26.0, 27.12936802674267, 0.7268317546542243, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6380424746075716, 0.2466666666666667, 0.40944444444444433, 0.9521178637200738, 0.6666666666666666, 0.7607806688952227, 0.7422772515514081, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30931827], dtype=float32), -0.40237367]. 
=============================================
[2019-04-04 15:16:10,671] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:16:10,671] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:16:10,676] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run25
[2019-04-04 15:16:12,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:16:12,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:16:12,216] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run25
[2019-04-04 15:16:12,248] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.11857451e-09 4.46743059e-10 1.19514672e-14 2.77160056e-14
 1.00000000e+00 4.81272300e-10 1.00698275e-14], sum to 1.0000
[2019-04-04 15:16:12,249] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7592
[2019-04-04 15:16:12,256] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.75, 19.0, 0.0, 0.0, 26.0, 26.82749983306238, 0.7916159092531267, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5089800.0000, 
sim time next is 5090400.0000, 
raw observation next is [8.7, 19.0, 0.0, 0.0, 26.0, 26.77371844576583, 0.7795968013663591, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.703601108033241, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7311432038138191, 0.759865600455453, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.95820403], dtype=float32), -0.47682312]. 
=============================================
[2019-04-04 15:16:13,390] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:16:13,390] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:16:13,406] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run25
[2019-04-04 15:16:13,542] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:16:13,543] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:16:13,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run25
[2019-04-04 15:16:14,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:16:14,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:16:14,224] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run25
[2019-04-04 15:16:14,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:16:14,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:16:14,763] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run25
[2019-04-04 15:16:15,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:16:15,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:16:15,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run25
[2019-04-04 15:16:16,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:16:16,838] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:16:16,842] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run25
[2019-04-04 15:16:17,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:16:17,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:16:17,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run25
[2019-04-04 15:16:17,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:16:17,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:16:17,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run25
[2019-04-04 15:16:18,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:16:18,142] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:16:18,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run25
[2019-04-04 15:16:19,762] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.84608440e-09 3.49921730e-10 1.02623674e-14 7.32976403e-15
 1.00000000e+00 6.25061836e-10 1.46850229e-15], sum to 1.0000
[2019-04-04 15:16:19,763] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4091
[2019-04-04 15:16:19,868] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.3518414524663, 0.2672231475544296, 1.0, 1.0, 199754.1745506152], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 234600.0000, 
sim time next is 235200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.09692202838873, 0.3074362602382417, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5914101690323941, 0.6024787534127473, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08274435], dtype=float32), 0.66515255]. 
=============================================
[2019-04-04 15:16:26,847] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0646716e-10 2.5676019e-11 9.8407540e-17 2.7781196e-15 1.0000000e+00
 8.5005197e-11 1.7984429e-16], sum to 1.0000
[2019-04-04 15:16:26,848] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6177
[2019-04-04 15:16:26,863] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.7833333333333333, 91.00000000000001, 0.0, 0.0, 26.0, 24.34449979527522, 0.1438531114456964, 0.0, 1.0, 40996.81853527307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 90600.0000, 
sim time next is 91200.0000, 
raw observation next is [-0.9666666666666667, 91.0, 0.0, 0.0, 26.0, 24.32756402323855, 0.1446939241920529, 0.0, 1.0, 41217.38791849679], 
processed observation next is [1.0, 0.043478260869565216, 0.43582640812557716, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5272970019365459, 0.5482313080640177, 0.0, 1.0, 0.19627327580236567], 
reward next is 0.8037, 
noisyNet noise sample is [array([0.91983765], dtype=float32), 0.771434]. 
=============================================
[2019-04-04 15:16:28,941] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2686567e-08 6.3521677e-09 4.1102452e-15 1.5844609e-13 1.0000000e+00
 5.6549498e-10 1.0810892e-14], sum to 1.0000
[2019-04-04 15:16:28,941] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7641
[2019-04-04 15:16:28,976] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.9, 85.66666666666667, 0.0, 0.0, 26.0, 24.05860520030572, 0.08599278635322184, 0.0, 1.0, 43086.98739205402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 97800.0000, 
sim time next is 98400.0000, 
raw observation next is [-3.0, 84.33333333333334, 0.0, 0.0, 26.0, 24.08479104639365, 0.08299541897174625, 0.0, 1.0, 43209.92480693283], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5070659205328042, 0.5276651396572488, 0.0, 1.0, 0.20576154669968014], 
reward next is 0.7942, 
noisyNet noise sample is [array([-2.228706], dtype=float32), 0.12289595]. 
=============================================
[2019-04-04 15:16:32,342] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.1004287e-08 6.2420784e-09 5.1116593e-13 1.9269282e-12 9.9999988e-01
 7.7187954e-09 7.6340350e-13], sum to 1.0000
[2019-04-04 15:16:32,342] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4587
[2019-04-04 15:16:32,374] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.87128701330758, -0.2036394317938104, 0.0, 1.0, 44740.27258133318], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 191400.0000, 
sim time next is 192000.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.88525387116927, -0.2071121000044054, 0.0, 1.0, 44806.86990742869], 
processed observation next is [1.0, 0.21739130434782608, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.40710448926410575, 0.43096263333186485, 0.0, 1.0, 0.21336604717823188], 
reward next is 0.7866, 
noisyNet noise sample is [array([1.61391], dtype=float32), 0.22610939]. 
=============================================
[2019-04-04 15:16:32,377] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[72.68393 ]
 [72.75535 ]
 [72.812386]
 [72.89276 ]
 [72.96799 ]], R is [[72.67008972]
 [72.73033905]
 [72.79039001]
 [72.85021973]
 [72.90984344]].
[2019-04-04 15:16:37,604] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3375738e-07 5.4195223e-08 3.6838284e-12 6.7266401e-12 9.9999976e-01
 2.2461741e-08 3.5755253e-12], sum to 1.0000
[2019-04-04 15:16:37,604] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7318
[2019-04-04 15:16:37,633] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.9, 50.5, 0.0, 0.0, 26.0, 23.11695265521644, -0.1991601218452452, 0.0, 1.0, 46083.16149843595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 444600.0000, 
sim time next is 445200.0000, 
raw observation next is [-11.0, 51.0, 0.0, 0.0, 26.0, 23.02687534577635, -0.2109419849880308, 0.0, 1.0, 46193.42858184229], 
processed observation next is [1.0, 0.13043478260869565, 0.15789473684210528, 0.51, 0.0, 0.0, 0.6666666666666666, 0.41890627881469583, 0.42968600500398973, 0.0, 1.0, 0.21996870753258232], 
reward next is 0.7800, 
noisyNet noise sample is [array([0.16347797], dtype=float32), -0.61388874]. 
=============================================
[2019-04-04 15:16:38,608] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.8707434e-08 3.0863141e-08 7.0386771e-14 1.1311332e-12 9.9999988e-01
 1.6668132e-08 1.2729801e-13], sum to 1.0000
[2019-04-04 15:16:38,609] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7987
[2019-04-04 15:16:38,628] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 74.0, 0.0, 0.0, 26.0, 23.54568417995201, -0.05218857429541362, 0.0, 1.0, 44276.98082193927], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 180000.0000, 
sim time next is 180600.0000, 
raw observation next is [-8.9, 74.66666666666667, 0.0, 0.0, 26.0, 23.5047852861759, -0.06334173371652944, 0.0, 1.0, 44234.28628601265], 
processed observation next is [1.0, 0.08695652173913043, 0.21606648199445982, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.4587321071813249, 0.47888608876115685, 0.0, 1.0, 0.21063945850482213], 
reward next is 0.7894, 
noisyNet noise sample is [array([-1.5797536], dtype=float32), -0.6683705]. 
=============================================
[2019-04-04 15:16:38,847] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9530855e-08 1.5134585e-09 5.7780874e-14 1.6960568e-13 1.0000000e+00
 8.1649632e-10 7.9260342e-14], sum to 1.0000
[2019-04-04 15:16:38,847] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2771
[2019-04-04 15:16:38,890] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.666666666666666, 41.0, 44.33333333333333, 790.3333333333334, 26.0, 26.06710840237766, 0.5017293087593989, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 399000.0000, 
sim time next is 399600.0000, 
raw observation next is [-9.5, 40.0, 42.5, 769.5, 26.0, 26.26907512288827, 0.5095117345193158, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.4, 0.14166666666666666, 0.8502762430939227, 0.6666666666666666, 0.6890895935740226, 0.6698372448397719, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1317469], dtype=float32), 0.54330426]. 
=============================================
[2019-04-04 15:16:39,575] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.4868741e-09 3.2721847e-10 2.0155526e-15 4.6432930e-14 1.0000000e+00
 5.8980548e-10 3.9254510e-15], sum to 1.0000
[2019-04-04 15:16:39,575] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6681
[2019-04-04 15:16:39,616] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.383333333333333, 72.5, 133.6666666666667, 0.0, 26.0, 25.26925709274394, 0.2169934369981968, 1.0, 1.0, 27891.69241117127], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 211800.0000, 
sim time next is 212400.0000, 
raw observation next is [-6.2, 72.0, 138.5, 0.0, 26.0, 25.28164193508536, 0.2221562103276054, 1.0, 1.0, 27527.82624561398], 
processed observation next is [1.0, 0.4782608695652174, 0.2908587257617729, 0.72, 0.46166666666666667, 0.0, 0.6666666666666666, 0.6068034945904467, 0.5740520701092018, 1.0, 1.0, 0.1310848868838761], 
reward next is 0.8689, 
noisyNet noise sample is [array([-0.4932589], dtype=float32), -0.16792955]. 
=============================================
[2019-04-04 15:16:47,460] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1364511e-08 1.3348839e-09 2.4829699e-13 5.5761006e-13 1.0000000e+00
 2.1168933e-09 2.0070334e-14], sum to 1.0000
[2019-04-04 15:16:47,464] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4347
[2019-04-04 15:16:47,526] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.78333333333333, 50.33333333333334, 0.0, 0.0, 26.0, 25.91357470410954, 0.4310719463442732, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 321000.0000, 
sim time next is 321600.0000, 
raw observation next is [-10.96666666666667, 51.66666666666667, 0.0, 0.0, 26.0, 25.69278922801104, 0.3921260784078251, 1.0, 1.0, 155988.2977982872], 
processed observation next is [1.0, 0.7391304347826086, 0.15881809787626952, 0.5166666666666667, 0.0, 0.0, 0.6666666666666666, 0.64106576900092, 0.6307086928026083, 1.0, 1.0, 0.742801418087082], 
reward next is 0.2572, 
noisyNet noise sample is [array([0.01107172], dtype=float32), 0.116304316]. 
=============================================
[2019-04-04 15:16:50,738] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4226549e-06 9.0331319e-08 2.4810888e-11 2.1493393e-10 9.9999833e-01
 1.6195088e-07 3.8070019e-11], sum to 1.0000
[2019-04-04 15:16:50,738] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0931
[2019-04-04 15:16:50,834] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-16.28333333333333, 78.5, 0.0, 0.0, 26.0, 21.58261355467166, -0.5246877354090199, 0.0, 1.0, 49481.77494666538], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 371400.0000, 
sim time next is 372000.0000, 
raw observation next is [-16.36666666666667, 79.0, 0.0, 0.0, 26.0, 21.49329037909963, -0.4405880206216344, 1.0, 1.0, 202242.6109366239], 
processed observation next is [1.0, 0.30434782608695654, 0.009233610341643453, 0.79, 0.0, 0.0, 0.6666666666666666, 0.2911075315916358, 0.3531373264594552, 1.0, 1.0, 0.9630600520791615], 
reward next is 0.0369, 
noisyNet noise sample is [array([0.7943682], dtype=float32), -1.9023226]. 
=============================================
[2019-04-04 15:16:50,841] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[63.899467]
 [64.00541 ]
 [64.09726 ]
 [64.20682 ]
 [64.31333 ]], R is [[65.38877106]
 [65.49925995]
 [65.60915375]
 [65.71838379]
 [65.82715607]].
[2019-04-04 15:16:53,838] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9529336e-10 1.4589961e-11 3.0838601e-16 2.4528700e-15 1.0000000e+00
 1.2430044e-11 4.4198886e-15], sum to 1.0000
[2019-04-04 15:16:53,838] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8342
[2019-04-04 15:16:53,888] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.2, 87.0, 0.0, 0.0, 26.0, 24.95153833814856, 0.2886347023293497, 0.0, 1.0, 44013.23053346942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 582600.0000, 
sim time next is 583200.0000, 
raw observation next is [-2.3, 87.0, 0.0, 0.0, 26.0, 24.94948167951006, 0.2873967225619177, 0.0, 1.0, 45096.01490511648], 
processed observation next is [0.0, 0.782608695652174, 0.3988919667590028, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5791234732925051, 0.5957989075206392, 0.0, 1.0, 0.21474292811960227], 
reward next is 0.7853, 
noisyNet noise sample is [array([0.16229148], dtype=float32), -1.629914]. 
=============================================
[2019-04-04 15:16:55,188] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.1351889e-10 5.4829700e-11 3.6591302e-16 1.1047441e-14 1.0000000e+00
 4.5261368e-11 8.6154760e-16], sum to 1.0000
[2019-04-04 15:16:55,188] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0334
[2019-04-04 15:16:55,237] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 83.66666666666667, 0.0, 0.0, 26.0, 24.77958918969759, 0.2208073580417505, 0.0, 1.0, 42543.81002311617], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 601800.0000, 
sim time next is 602400.0000, 
raw observation next is [-3.4, 84.33333333333334, 0.0, 0.0, 26.0, 24.74741732684791, 0.2130626133670838, 0.0, 1.0, 42475.97378393133], 
processed observation next is [0.0, 1.0, 0.368421052631579, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5622847772373257, 0.5710208711223613, 0.0, 1.0, 0.20226654182824444], 
reward next is 0.7977, 
noisyNet noise sample is [array([0.92331207], dtype=float32), -0.4659028]. 
=============================================
[2019-04-04 15:17:04,290] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0861439e-08 1.9132638e-09 1.8917697e-14 4.3522301e-14 1.0000000e+00
 1.5246829e-09 7.4359423e-14], sum to 1.0000
[2019-04-04 15:17:04,290] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5135
[2019-04-04 15:17:04,337] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 30.33333333333334, 120.6666666666667, 0.0, 26.0, 24.9787852441201, 0.2004634088518848, 1.0, 1.0, 35618.78386998985], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 480000.0000, 
sim time next is 480600.0000, 
raw observation next is [-0.8999999999999999, 31.5, 119.0, 0.0, 26.0, 24.95072583818543, 0.2028499643950034, 1.0, 1.0, 46493.88951282183], 
processed observation next is [1.0, 0.5652173913043478, 0.43767313019390586, 0.315, 0.39666666666666667, 0.0, 0.6666666666666666, 0.5792271531821193, 0.5676166547983345, 1.0, 1.0, 0.22139947387058012], 
reward next is 0.7786, 
noisyNet noise sample is [array([-0.29969773], dtype=float32), -0.9538165]. 
=============================================
[2019-04-04 15:17:12,681] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.4792804e-11 3.5244260e-12 1.4482500e-18 5.2714794e-16 1.0000000e+00
 2.8033329e-12 5.2831500e-18], sum to 1.0000
[2019-04-04 15:17:12,681] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1571
[2019-04-04 15:17:12,717] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.16666666666667, 92.33333333333334, 54.5, 0.0, 26.0, 26.2638841170213, 0.5918602792300621, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 984000.0000, 
sim time next is 984600.0000, 
raw observation next is [10.25, 92.5, 60.0, 0.0, 26.0, 26.35255201882364, 0.6070792772209891, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.7465373961218837, 0.925, 0.2, 0.0, 0.6666666666666666, 0.6960460015686367, 0.702359759073663, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.70583266], dtype=float32), 0.65899926]. 
=============================================
[2019-04-04 15:17:12,913] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.5001145e-09 1.3751997e-09 5.5898557e-14 3.0077536e-13 1.0000000e+00
 3.6873216e-09 5.0915245e-14], sum to 1.0000
[2019-04-04 15:17:12,929] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8044
[2019-04-04 15:17:12,969] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 61.0, 0.0, 0.0, 26.0, 24.96113926206539, 0.2975237555077312, 0.0, 1.0, 44598.20769748293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 766800.0000, 
sim time next is 767400.0000, 
raw observation next is [-5.7, 61.5, 0.0, 0.0, 26.0, 24.9190388811414, 0.2960682432452119, 0.0, 1.0, 44290.44143123835], 
processed observation next is [1.0, 0.9130434782608695, 0.30470914127423826, 0.615, 0.0, 0.0, 0.6666666666666666, 0.57658657342845, 0.5986894144150706, 0.0, 1.0, 0.21090686395827787], 
reward next is 0.7891, 
noisyNet noise sample is [array([-0.2845236], dtype=float32), 1.15342]. 
=============================================
[2019-04-04 15:17:13,616] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.22826537e-11 1.04796918e-11 1.10897795e-16 7.31517982e-16
 1.00000000e+00 2.30559044e-11 7.87604983e-17], sum to 1.0000
[2019-04-04 15:17:13,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6730
[2019-04-04 15:17:13,630] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.416666666666667, 90.16666666666667, 0.0, 0.0, 26.0, 25.31631538248588, 0.4205040396131806, 0.0, 1.0, 38043.44376785709], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 953400.0000, 
sim time next is 954000.0000, 
raw observation next is [5.5, 89.0, 0.0, 0.0, 26.0, 25.30785333973127, 0.4250028479713272, 0.0, 1.0, 38027.73945754909], 
processed observation next is [1.0, 0.043478260869565216, 0.6149584487534627, 0.89, 0.0, 0.0, 0.6666666666666666, 0.6089877783109392, 0.6416676159904424, 0.0, 1.0, 0.18108447360737662], 
reward next is 0.8189, 
noisyNet noise sample is [array([0.9235445], dtype=float32), -0.3448847]. 
=============================================
[2019-04-04 15:17:13,654] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[90.66273]
 [90.54622]
 [90.38884]
 [90.28307]
 [90.15212]], R is [[90.61220551]
 [90.52492523]
 [90.43855286]
 [90.35307312]
 [90.26804352]].
[2019-04-04 15:17:17,996] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6554782e-10 6.3560511e-11 2.1139163e-16 1.4805853e-14 1.0000000e+00
 4.2882607e-11 9.2786308e-16], sum to 1.0000
[2019-04-04 15:17:17,998] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4596
[2019-04-04 15:17:18,010] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 50.0, 110.0, 611.0, 26.0, 25.58458720462538, 0.3591352820367154, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 738000.0000, 
sim time next is 738600.0000, 
raw observation next is [0.5, 49.16666666666667, 103.0, 665.0, 26.0, 25.50997982553285, 0.3399926094800987, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.4916666666666667, 0.3433333333333333, 0.7348066298342542, 0.6666666666666666, 0.6258316521277374, 0.6133308698266996, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.04872979], dtype=float32), 0.08425575]. 
=============================================
[2019-04-04 15:17:18,428] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.5377092e-10 3.0291044e-11 1.1458492e-16 1.2518480e-15 1.0000000e+00
 5.2066407e-11 2.2198149e-16], sum to 1.0000
[2019-04-04 15:17:18,430] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7922
[2019-04-04 15:17:18,448] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.23555385785906, 0.4078372400358732, 0.0, 1.0, 38854.28177361815], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 946800.0000, 
sim time next is 947400.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.23764554974663, 0.4081317286697104, 0.0, 1.0, 38746.62599240625], 
processed observation next is [1.0, 1.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6031371291455526, 0.6360439095565701, 0.0, 1.0, 0.18450774282098212], 
reward next is 0.8155, 
noisyNet noise sample is [array([3.0537155], dtype=float32), -0.285563]. 
=============================================
[2019-04-04 15:17:19,128] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3362026e-09 3.5067940e-10 1.1408477e-14 2.4116117e-14 1.0000000e+00
 2.6144070e-10 7.3684802e-15], sum to 1.0000
[2019-04-04 15:17:19,129] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6833
[2019-04-04 15:17:19,143] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.483333333333333, 69.33333333333333, 0.0, 0.0, 26.0, 24.79255871472069, 0.1695037793280072, 0.0, 1.0, 41903.01177795805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 684600.0000, 
sim time next is 685200.0000, 
raw observation next is [-3.566666666666667, 69.66666666666667, 0.0, 0.0, 26.0, 24.75247442550711, 0.161867006934087, 0.0, 1.0, 41847.31573913733], 
processed observation next is [0.0, 0.9565217391304348, 0.3638042474607572, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.5627062021255925, 0.5539556689780291, 0.0, 1.0, 0.19927293209113014], 
reward next is 0.8007, 
noisyNet noise sample is [array([-0.55119956], dtype=float32), -0.6354328]. 
=============================================
[2019-04-04 15:17:20,453] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.00432016e-08 1.39831995e-08 6.70196930e-14 7.95836379e-13
 9.99999881e-01 4.03088052e-09 6.77045022e-14], sum to 1.0000
[2019-04-04 15:17:20,454] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0210
[2019-04-04 15:17:20,466] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 26.0, 24.39793993134773, 0.1668662856394149, 0.0, 1.0, 41965.66131990321], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 777000.0000, 
sim time next is 777600.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.36194490612996, 0.1583037248834525, 0.0, 1.0, 41881.33905127856], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5301620755108299, 0.5527679082944842, 0.0, 1.0, 0.19943494786323124], 
reward next is 0.8006, 
noisyNet noise sample is [array([0.47869474], dtype=float32), -0.7873281]. 
=============================================
[2019-04-04 15:17:28,691] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6990683e-09 5.7016912e-11 6.5283257e-18 4.1185929e-16 1.0000000e+00
 2.2328570e-11 3.1878128e-17], sum to 1.0000
[2019-04-04 15:17:28,691] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0211
[2019-04-04 15:17:28,700] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.74369927664782, 0.6333614575897234, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1048200.0000, 
sim time next is 1048800.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 25.93643353753733, 0.6450209979118873, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6613694614614442, 0.7150069993039624, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.66333], dtype=float32), 2.3493643]. 
=============================================
[2019-04-04 15:17:31,446] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.48108831e-10 1.33548606e-11 1.03162426e-17 5.92647671e-16
 1.00000000e+00 1.52412024e-11 1.07576070e-16], sum to 1.0000
[2019-04-04 15:17:31,447] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9344
[2019-04-04 15:17:31,490] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.95394460829484, 0.4906962046783849, 1.0, 1.0, 25845.48085264178], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1364400.0000, 
sim time next is 1365000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.06393646718314, 0.5054252139967061, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5886613722652617, 0.6684750713322355, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23595531], dtype=float32), 0.39605024]. 
=============================================
[2019-04-04 15:17:31,508] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[92.535545]
 [92.600586]
 [91.778305]
 [91.35478 ]
 [91.54768 ]], R is [[92.3792572 ]
 [92.33238983]
 [91.58347321]
 [91.1183548 ]
 [91.20716858]].
[2019-04-04 15:17:31,901] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5661680e-10 4.5663886e-11 1.3731549e-17 1.0539440e-16 1.0000000e+00
 6.4781843e-12 7.3258715e-17], sum to 1.0000
[2019-04-04 15:17:31,902] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1606
[2019-04-04 15:17:31,913] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.96666666666667, 82.0, 0.0, 0.0, 26.0, 25.6230904345656, 0.6105089025052972, 0.0, 1.0, 32283.6665585505], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1146000.0000, 
sim time next is 1146600.0000, 
raw observation next is [12.15, 81.5, 0.0, 0.0, 26.0, 25.62829114679203, 0.6127366466710383, 0.0, 1.0, 25008.24241707429], 
processed observation next is [0.0, 0.2608695652173913, 0.7991689750692522, 0.815, 0.0, 0.0, 0.6666666666666666, 0.6356909288993359, 0.7042455488903462, 0.0, 1.0, 0.11908686865273471], 
reward next is 0.8809, 
noisyNet noise sample is [array([1.1384633], dtype=float32), 0.23579895]. 
=============================================
[2019-04-04 15:17:35,953] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.3824502e-11 4.8447045e-12 8.7789597e-18 1.5021681e-16 1.0000000e+00
 9.0454337e-13 1.0269166e-17], sum to 1.0000
[2019-04-04 15:17:35,956] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4579
[2019-04-04 15:17:35,971] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.4, 93.0, 13.5, 0.0, 26.0, 25.4141685716254, 0.4374157691804261, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 979200.0000, 
sim time next is 979800.0000, 
raw observation next is [9.5, 92.83333333333333, 18.0, 0.0, 26.0, 25.40585924682344, 0.4252915335922909, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7257617728531857, 0.9283333333333332, 0.06, 0.0, 0.6666666666666666, 0.6171549372352866, 0.6417638445307636, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9652969], dtype=float32), 0.9600668]. 
=============================================
[2019-04-04 15:17:36,686] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.7404173e-10 3.9674986e-11 3.0196576e-17 8.0808420e-16 1.0000000e+00
 7.3340544e-12 2.6691843e-16], sum to 1.0000
[2019-04-04 15:17:36,690] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3595
[2019-04-04 15:17:36,702] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.8, 83.0, 0.0, 0.0, 26.0, 25.45489647919157, 0.4422814625568606, 0.0, 1.0, 87448.06362040089], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 975000.0000, 
sim time next is 975600.0000, 
raw observation next is [10.0, 83.0, 0.0, 0.0, 26.0, 25.45043610125441, 0.4488844861757624, 0.0, 1.0, 61194.93981076749], 
processed observation next is [1.0, 0.30434782608695654, 0.739612188365651, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6208696751045343, 0.6496281620585874, 0.0, 1.0, 0.291404475289369], 
reward next is 0.7086, 
noisyNet noise sample is [array([-0.8194222], dtype=float32), -0.4067759]. 
=============================================
[2019-04-04 15:17:39,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6175272e-10 3.6921017e-12 7.2364597e-18 8.4403293e-17 1.0000000e+00
 4.0203249e-12 6.5007909e-18], sum to 1.0000
[2019-04-04 15:17:39,576] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5489
[2019-04-04 15:17:39,583] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.31666666666667, 61.83333333333334, 0.0, 0.0, 26.0, 26.56742472329142, 0.7409429479163827, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1620600.0000, 
sim time next is 1621200.0000, 
raw observation next is [10.13333333333333, 62.66666666666667, 0.0, 0.0, 26.0, 26.51201388585027, 0.7318196203966508, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7433056325023084, 0.6266666666666667, 0.0, 0.0, 0.6666666666666666, 0.7093344904875224, 0.7439398734655502, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00705774], dtype=float32), 0.9500042]. 
=============================================
[2019-04-04 15:17:43,003] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.9299252e-09 1.1154138e-09 7.2794681e-15 1.1339393e-12 1.0000000e+00
 9.0050585e-11 1.7722382e-14], sum to 1.0000
[2019-04-04 15:17:43,005] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0585
[2019-04-04 15:17:43,021] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 100.0, 74.0, 0.0, 26.0, 23.31958731623961, 0.1237711212264981, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1243800.0000, 
sim time next is 1244400.0000, 
raw observation next is [15.0, 100.0, 74.66666666666667, 0.0, 26.0, 23.31415679927185, 0.1235505204152415, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.8781163434903049, 1.0, 0.2488888888888889, 0.0, 0.6666666666666666, 0.4428463999393208, 0.5411835068050804, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0209162], dtype=float32), -1.7882401]. 
=============================================
[2019-04-04 15:17:46,386] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 15:17:46,387] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 15:17:46,387] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 15:17:46,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:17:46,388] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 15:17:46,388] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:17:46,391] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run34
[2019-04-04 15:17:46,393] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:17:46,425] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run34
[2019-04-04 15:17:46,452] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run34
[2019-04-04 15:19:06,254] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17920639], dtype=float32), 0.22573777]
[2019-04-04 15:19:06,254] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.779800623, 94.36047013, 45.41494583, 37.2927081, 26.0, 25.0731090885681, 0.2380191872219272, 1.0, 1.0, 0.0]
[2019-04-04 15:19:06,254] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 15:19:06,255] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.0018158e-10 9.1124934e-11 1.6282385e-16 3.4178885e-15 1.0000000e+00
 1.8089851e-11 3.9396855e-16], sampled 0.7913295642631976
[2019-04-04 15:19:28,164] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 15:19:47,328] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.5698 263430344.9894 1551.9755
[2019-04-04 15:19:49,341] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 15:19:50,365] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 3300000, evaluation results [3300000.0, 7241.569785764876, 263430344.989374, 1551.9755349129598, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 15:19:51,171] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.17141380e-10 1.21328416e-11 2.64133841e-17 1.08019825e-15
 1.00000000e+00 2.01429620e-11 1.09150585e-16], sum to 1.0000
[2019-04-04 15:19:51,175] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1241
[2019-04-04 15:19:51,183] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 84.0, 95.0, 0.0, 26.0, 25.86053518993326, 0.5222310383418596, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1684800.0000, 
sim time next is 1685400.0000, 
raw observation next is [1.1, 84.66666666666667, 99.0, 0.0, 26.0, 25.85255911825676, 0.5212899245428887, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.8466666666666667, 0.33, 0.0, 0.6666666666666666, 0.6543799265213966, 0.673763308180963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8945223], dtype=float32), -0.5286899]. 
=============================================
[2019-04-04 15:19:53,089] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0561372e-09 1.4806052e-10 5.5698498e-17 1.0028236e-14 1.0000000e+00
 4.5415380e-11 2.6633548e-17], sum to 1.0000
[2019-04-04 15:19:53,089] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0224
[2019-04-04 15:19:53,108] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.266666666666667, 92.0, 0.0, 0.0, 26.0, 25.3541770853876, 0.5299962511917439, 0.0, 1.0, 45991.24918938802], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1320000.0000, 
sim time next is 1320600.0000, 
raw observation next is [1.183333333333333, 92.0, 0.0, 0.0, 26.0, 25.41026722667187, 0.5278991594501431, 0.0, 1.0, 21569.52710518911], 
processed observation next is [1.0, 0.2608695652173913, 0.49538319482917825, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6175222688893225, 0.675966386483381, 0.0, 1.0, 0.10271203383423386], 
reward next is 0.8973, 
noisyNet noise sample is [array([-0.04477418], dtype=float32), 0.7727564]. 
=============================================
[2019-04-04 15:19:53,965] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2642741e-10 5.7261692e-11 6.4536254e-17 2.6650242e-15 1.0000000e+00
 4.5560795e-11 1.1690376e-16], sum to 1.0000
[2019-04-04 15:19:53,965] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5180
[2019-04-04 15:19:54,167] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.35457301732419, 0.4735240581696719, 0.0, 1.0, 43295.09364771321], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1728000.0000, 
sim time next is 1728600.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.35190556326903, 0.4716885264051141, 0.0, 1.0, 43196.18831257158], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.612658796939086, 0.6572295088017047, 0.0, 1.0, 0.20569613482176943], 
reward next is 0.7943, 
noisyNet noise sample is [array([1.3563536], dtype=float32), 1.4102532]. 
=============================================
[2019-04-04 15:20:08,022] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9896683e-10 6.5924115e-12 4.9179633e-17 9.4669071e-16 1.0000000e+00
 8.5275701e-12 1.2644554e-16], sum to 1.0000
[2019-04-04 15:20:08,023] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4941
[2019-04-04 15:20:08,057] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 101.1666666666667, 0.0, 26.0, 25.36532487300239, 0.559427425863828, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1689600.0000, 
sim time next is 1690200.0000, 
raw observation next is [1.1, 88.0, 100.0, 0.0, 26.0, 25.79262818773086, 0.5837730798756605, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3333333333333333, 0.0, 0.6666666666666666, 0.6493856823109049, 0.6945910266252202, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3760194], dtype=float32), -1.0440527]. 
=============================================
[2019-04-04 15:20:19,571] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7350051e-09 3.6487707e-10 4.0703791e-15 4.7841305e-14 1.0000000e+00
 4.7019266e-10 1.8929643e-14], sum to 1.0000
[2019-04-04 15:20:19,571] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7351
[2019-04-04 15:20:19,632] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666667, 73.66666666666666, 0.0, 0.0, 26.0, 24.98143490055637, 0.3233261801785467, 1.0, 1.0, 148403.9682616459], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1968000.0000, 
sim time next is 1968600.0000, 
raw observation next is [-4.583333333333333, 72.33333333333334, 0.0, 0.0, 26.0, 25.04264959346462, 0.358513385565042, 1.0, 1.0, 54346.94329768897], 
processed observation next is [1.0, 0.782608695652174, 0.3356417359187443, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.5868874661220517, 0.619504461855014, 1.0, 1.0, 0.2587949680842332], 
reward next is 0.7412, 
noisyNet noise sample is [array([-1.0993967], dtype=float32), -0.010016863]. 
=============================================
[2019-04-04 15:20:21,844] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7016516e-09 2.6590461e-09 3.0340707e-15 1.7106181e-13 1.0000000e+00
 4.5441312e-10 7.4551097e-15], sum to 1.0000
[2019-04-04 15:20:21,845] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8954
[2019-04-04 15:20:21,936] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 78.0, 47.16666666666666, 15.66666666666666, 26.0, 24.1707305896439, 0.1630856528805252, 0.0, 1.0, 161283.0144685804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1845600.0000, 
sim time next is 1846200.0000, 
raw observation next is [-6.7, 78.0, 67.33333333333333, 31.33333333333333, 26.0, 24.63826088208528, 0.2261688013602329, 0.0, 1.0, 84617.47002491], 
processed observation next is [0.0, 0.34782608695652173, 0.2770083102493075, 0.78, 0.22444444444444442, 0.034622467771639034, 0.6666666666666666, 0.55318840684044, 0.575389600453411, 0.0, 1.0, 0.40294033345195235], 
reward next is 0.5971, 
noisyNet noise sample is [array([-0.83447725], dtype=float32), -0.020905858]. 
=============================================
[2019-04-04 15:20:26,326] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1263871e-09 5.5124988e-11 2.5259196e-15 3.2272531e-14 1.0000000e+00
 6.7902475e-11 1.3287071e-15], sum to 1.0000
[2019-04-04 15:20:26,327] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6530
[2019-04-04 15:20:26,364] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.899999999999999, 69.0, 126.1666666666667, 47.49999999999999, 26.0, 26.37473521540584, 0.512437486133272, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2212800.0000, 
sim time next is 2213400.0000, 
raw observation next is [-3.9, 68.5, 132.3333333333333, 94.99999999999999, 26.0, 26.37663902576772, 0.5143726410693632, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.685, 0.44111111111111095, 0.10497237569060772, 0.6666666666666666, 0.69805325214731, 0.6714575470231211, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02682764], dtype=float32), 0.20609643]. 
=============================================
[2019-04-04 15:20:37,263] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.3283772e-10 1.2413987e-10 1.2713185e-16 1.4556695e-14 1.0000000e+00
 2.2629551e-11 5.7911558e-16], sum to 1.0000
[2019-04-04 15:20:37,300] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9280
[2019-04-04 15:20:37,346] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.983333333333333, 79.5, 126.6666666666667, 42.66666666666667, 26.0, 25.64864785101044, 0.3179431621642445, 1.0, 1.0, 18732.41818113269], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2281800.0000, 
sim time next is 2282400.0000, 
raw observation next is [-6.7, 78.0, 139.5, 44.5, 26.0, 25.62990509070363, 0.3258439723348759, 1.0, 1.0, 18730.00623369189], 
processed observation next is [1.0, 0.43478260869565216, 0.2770083102493075, 0.78, 0.465, 0.049171270718232046, 0.6666666666666666, 0.6358254242253025, 0.6086146574449586, 1.0, 1.0, 0.08919050587472328], 
reward next is 0.9108, 
noisyNet noise sample is [array([1.2858568], dtype=float32), -0.883156]. 
=============================================
[2019-04-04 15:20:43,983] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6622700e-08 3.2595286e-09 2.3339751e-14 5.7467112e-13 1.0000000e+00
 4.6839438e-10 2.3136850e-13], sum to 1.0000
[2019-04-04 15:20:43,984] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8444
[2019-04-04 15:20:43,996] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 35.0, 0.0, 0.0, 26.0, 25.27499537877184, 0.2784485239245637, 0.0, 1.0, 40067.03978633757], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2496600.0000, 
sim time next is 2497200.0000, 
raw observation next is [-1.2, 34.33333333333334, 0.0, 0.0, 26.0, 25.32132116890053, 0.2763853623533309, 0.0, 1.0, 40086.4326701298], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.34333333333333343, 0.0, 0.0, 0.6666666666666666, 0.6101100974083774, 0.5921284541177769, 0.0, 1.0, 0.1908877746196657], 
reward next is 0.8091, 
noisyNet noise sample is [array([1.6131985], dtype=float32), -2.5246744]. 
=============================================
[2019-04-04 15:20:46,508] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.9950916e-09 9.9153985e-10 3.8195413e-14 1.3152539e-13 1.0000000e+00
 4.7490745e-10 8.5537880e-14], sum to 1.0000
[2019-04-04 15:20:46,510] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6026
[2019-04-04 15:20:46,525] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.583333333333333, 42.33333333333334, 0.0, 0.0, 26.0, 25.15375514677674, 0.2737868309012471, 0.0, 1.0, 43057.27392274513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2409000.0000, 
sim time next is 2409600.0000, 
raw observation next is [-3.766666666666667, 42.66666666666667, 0.0, 0.0, 26.0, 25.12761516155523, 0.2638840915628139, 0.0, 1.0, 43031.5293171833], 
processed observation next is [0.0, 0.9130434782608695, 0.358264081255771, 0.4266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5939679301296025, 0.5879613638542712, 0.0, 1.0, 0.20491204436753951], 
reward next is 0.7951, 
noisyNet noise sample is [array([-0.12564087], dtype=float32), 1.040702]. 
=============================================
[2019-04-04 15:20:51,572] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3864326e-08 6.4832610e-09 5.6834627e-14 2.1077925e-12 9.9999988e-01
 8.9338643e-09 7.0633691e-14], sum to 1.0000
[2019-04-04 15:20:51,575] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9524
[2019-04-04 15:20:51,616] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 54.33333333333333, 0.0, 0.0, 26.0, 25.46570188965627, 0.4327706489447382, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2322600.0000, 
sim time next is 2323200.0000, 
raw observation next is [-1.7, 54.66666666666667, 0.0, 0.0, 26.0, 25.46075111754251, 0.4262699320734433, 0.0, 1.0, 18757.07767634589], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5466666666666667, 0.0, 0.0, 0.6666666666666666, 0.6217292597952092, 0.6420899773578145, 0.0, 1.0, 0.089319417506409], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.14219865], dtype=float32), 1.5362475]. 
=============================================
[2019-04-04 15:21:03,130] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2023851e-08 2.3589541e-09 4.1039042e-13 2.0395400e-12 1.0000000e+00
 3.7848933e-09 6.1432061e-13], sum to 1.0000
[2019-04-04 15:21:03,131] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9763
[2019-04-04 15:21:03,153] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.5, 42.66666666666667, 0.0, 0.0, 26.0, 24.60975749525947, 0.1545323951410931, 0.0, 1.0, 43120.03568443811], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2418600.0000, 
sim time next is 2419200.0000, 
raw observation next is [-5.6, 43.0, 0.0, 0.0, 26.0, 24.58117730612978, 0.1483217832732406, 0.0, 1.0, 43131.57196876818], 
processed observation next is [0.0, 0.0, 0.30747922437673136, 0.43, 0.0, 0.0, 0.6666666666666666, 0.5484314421774817, 0.5494405944244135, 0.0, 1.0, 0.20538843794651512], 
reward next is 0.7946, 
noisyNet noise sample is [array([0.13009629], dtype=float32), -0.29954976]. 
=============================================
[2019-04-04 15:21:11,478] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5357687e-09 4.2774964e-10 2.0378581e-14 8.0463990e-14 1.0000000e+00
 2.1286026e-09 1.0124677e-14], sum to 1.0000
[2019-04-04 15:21:11,479] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4990
[2019-04-04 15:21:11,516] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 35.0, 0.0, 0.0, 26.0, 25.01429492970205, 0.3387048287539368, 1.0, 1.0, 66824.19703560811], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2570400.0000, 
sim time next is 2571000.0000, 
raw observation next is [0.3166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 25.17282555513436, 0.3556701129842708, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.47137580794090495, 0.35166666666666674, 0.0, 0.0, 0.6666666666666666, 0.5977354629278633, 0.6185567043280903, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30341607], dtype=float32), 0.31665015]. 
=============================================
[2019-04-04 15:21:11,542] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.50156 ]
 [77.20108 ]
 [76.39642 ]
 [76.661575]
 [77.15137 ]], R is [[77.57318878]
 [77.47924805]
 [76.76920319]
 [76.91255951]
 [77.09895325]].
[2019-04-04 15:21:14,796] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4560287e-09 1.0414225e-09 1.9965070e-14 1.9573880e-13 1.0000000e+00
 1.1685112e-09 2.5609314e-14], sum to 1.0000
[2019-04-04 15:21:14,799] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5386
[2019-04-04 15:21:14,821] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.583333333333333, 63.0, 0.0, 0.0, 26.0, 24.76962730006288, 0.2475255266532663, 0.0, 1.0, 41906.37437270804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2592600.0000, 
sim time next is 2593200.0000, 
raw observation next is [-4.666666666666667, 64.0, 0.0, 0.0, 26.0, 24.72718439652688, 0.2383937986305931, 0.0, 1.0, 41912.89061344854], 
processed observation next is [1.0, 0.0, 0.3333333333333333, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5605986997105733, 0.579464599543531, 0.0, 1.0, 0.199585193397374], 
reward next is 0.8004, 
noisyNet noise sample is [array([-0.5345319], dtype=float32), -0.14144169]. 
=============================================
[2019-04-04 15:21:19,479] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.7221391e-07 1.4635350e-07 2.2170581e-12 9.8532025e-12 9.9999893e-01
 3.7238745e-08 1.1724373e-12], sum to 1.0000
[2019-04-04 15:21:19,480] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4175
[2019-04-04 15:21:19,502] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.5, 83.0, 0.0, 0.0, 26.0, 23.23151621992661, -0.08624045724646244, 0.0, 1.0, 43575.31322947524], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2698200.0000, 
sim time next is 2698800.0000, 
raw observation next is [-15.66666666666667, 83.0, 0.0, 0.0, 26.0, 23.17916832911605, -0.1049998813659846, 0.0, 1.0, 43489.49963230586], 
processed observation next is [1.0, 0.21739130434782608, 0.02862419205909501, 0.83, 0.0, 0.0, 0.6666666666666666, 0.43159736075967076, 0.46500003954467184, 0.0, 1.0, 0.20709285539193267], 
reward next is 0.7929, 
noisyNet noise sample is [array([-1.0378783], dtype=float32), -0.869043]. 
=============================================
[2019-04-04 15:21:24,511] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.10729345e-09 2.15229654e-10 7.49031129e-16 1.17840545e-14
 1.00000000e+00 7.21228841e-11 8.27398882e-16], sum to 1.0000
[2019-04-04 15:21:24,512] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2374
[2019-04-04 15:21:24,542] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.666666666666667, 52.66666666666667, 88.66666666666667, 633.8333333333334, 26.0, 25.94745989398547, 0.5977342416611704, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2733600.0000, 
sim time next is 2734200.0000, 
raw observation next is [-3.5, 52.0, 86.0, 614.0, 26.0, 26.32949896040012, 0.6311462570544477, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.36565096952908593, 0.52, 0.2866666666666667, 0.6784530386740332, 0.6666666666666666, 0.6941249133666766, 0.7103820856848159, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24825804], dtype=float32), -0.19119118]. 
=============================================
[2019-04-04 15:21:25,788] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7883332e-08 2.3960567e-09 8.3620014e-14 3.6900160e-13 1.0000000e+00
 2.5822942e-09 1.0247136e-13], sum to 1.0000
[2019-04-04 15:21:25,789] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6725
[2019-04-04 15:21:25,802] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.72410976942034, 0.2562756064931525, 0.0, 1.0, 42694.63888888687], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2767800.0000, 
sim time next is 2768400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.75007254777581, 0.2486427517175544, 0.0, 1.0, 42516.00424077885], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5625060456479843, 0.5828809172391848, 0.0, 1.0, 0.20245716305132785], 
reward next is 0.7975, 
noisyNet noise sample is [array([0.02599585], dtype=float32), 0.7839695]. 
=============================================
[2019-04-04 15:21:29,608] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.9049321e-10 8.6517592e-11 4.1413741e-16 6.3615812e-15 1.0000000e+00
 3.8450281e-11 1.2153286e-16], sum to 1.0000
[2019-04-04 15:21:29,609] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5101
[2019-04-04 15:21:29,620] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.5, 88.0, 0.0, 0.0, 26.0, 25.23950214533208, 0.4465835227629723, 0.0, 1.0, 43287.05931535286], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3281400.0000, 
sim time next is 3282000.0000, 
raw observation next is [-6.666666666666666, 86.66666666666667, 0.0, 0.0, 26.0, 25.21507265107335, 0.4375896762040517, 0.0, 1.0, 43287.80854042621], 
processed observation next is [1.0, 1.0, 0.2779316712834719, 0.8666666666666667, 0.0, 0.0, 0.6666666666666666, 0.6012560542561124, 0.6458632254013505, 0.0, 1.0, 0.2061324216210772], 
reward next is 0.7939, 
noisyNet noise sample is [array([0.972773], dtype=float32), 0.15867679]. 
=============================================
[2019-04-04 15:21:29,634] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[83.49301 ]
 [83.571884]
 [83.63219 ]
 [83.492355]
 [83.32043 ]], R is [[83.31616211]
 [83.27687073]
 [83.2374115 ]
 [83.195961  ]
 [83.14677429]].
[2019-04-04 15:21:31,906] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9656340e-09 1.3528698e-09 2.5718607e-15 6.5000604e-14 1.0000000e+00
 4.5316910e-11 4.2103018e-15], sum to 1.0000
[2019-04-04 15:21:31,906] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2712
[2019-04-04 15:21:31,955] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666666, 55.66666666666667, 100.1666666666667, 655.6666666666667, 26.0, 25.31958303626982, 0.3236640304274999, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3058800.0000, 
sim time next is 3059400.0000, 
raw observation next is [-4.333333333333333, 54.83333333333333, 101.3333333333333, 676.3333333333333, 26.0, 25.28352094361804, 0.3205733132651369, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.342566943674977, 0.5483333333333333, 0.3377777777777777, 0.747329650092081, 0.6666666666666666, 0.6069600786348367, 0.606857771088379, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.64702344], dtype=float32), 0.27923873]. 
=============================================
[2019-04-04 15:21:34,708] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.5622609e-10 2.9462316e-10 3.4413068e-15 3.6350514e-14 1.0000000e+00
 1.1629963e-10 8.2371464e-15], sum to 1.0000
[2019-04-04 15:21:34,708] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4958
[2019-04-04 15:21:34,717] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.916666666666667, 65.0, 0.0, 0.0, 26.0, 25.16057679384285, 0.3263011913235064, 0.0, 1.0, 39034.00556176074], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3016200.0000, 
sim time next is 3016800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.14680190791097, 0.3208306069311901, 0.0, 1.0, 38897.16575481275], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5955668256592475, 0.60694353564373, 0.0, 1.0, 0.18522459883244166], 
reward next is 0.8148, 
noisyNet noise sample is [array([0.6342089], dtype=float32), 0.29409024]. 
=============================================
[2019-04-04 15:21:45,832] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.2887040e-09 3.9483483e-10 3.7012058e-15 2.2021979e-14 1.0000000e+00
 3.3136496e-10 2.2761099e-15], sum to 1.0000
[2019-04-04 15:21:45,832] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5954
[2019-04-04 15:21:45,842] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.5, 54.0, 118.0, 811.0, 26.0, 26.24678252315418, 0.5773457359469171, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3328200.0000, 
sim time next is 3328800.0000, 
raw observation next is [-5.333333333333333, 54.0, 117.3333333333333, 809.1666666666667, 26.0, 26.11295285546427, 0.5635301399598469, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3148661126500462, 0.54, 0.391111111111111, 0.8941068139963169, 0.6666666666666666, 0.6760794046220223, 0.6878433799866156, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1587638], dtype=float32), -0.69898504]. 
=============================================
[2019-04-04 15:21:50,401] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3351146e-09 3.4371567e-10 1.3077133e-15 5.1947861e-15 1.0000000e+00
 4.8485525e-11 8.4993353e-15], sum to 1.0000
[2019-04-04 15:21:50,404] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5627
[2019-04-04 15:21:50,443] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.666666666666667, 60.66666666666667, 115.1666666666667, 788.3333333333333, 26.0, 26.37428081764838, 0.5997372819867898, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3324000.0000, 
sim time next is 3324600.0000, 
raw observation next is [-6.5, 59.0, 116.0, 798.0, 26.0, 26.36456294842002, 0.6019592329310486, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.28254847645429365, 0.59, 0.38666666666666666, 0.881767955801105, 0.6666666666666666, 0.6970469123683349, 0.7006530776436829, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9800271], dtype=float32), 0.5175144]. 
=============================================
[2019-04-04 15:21:54,975] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.5382823e-09 3.0462921e-09 2.6059333e-14 5.5427093e-13 1.0000000e+00
 7.3854256e-10 1.3134884e-14], sum to 1.0000
[2019-04-04 15:21:54,976] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3087
[2019-04-04 15:21:54,992] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.5, 86.5, 0.0, 0.0, 26.0, 25.61774625450629, 0.5522432455349203, 0.0, 1.0, 18732.31365390505], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3274200.0000, 
sim time next is 3274800.0000, 
raw observation next is [-5.666666666666667, 88.33333333333333, 0.0, 0.0, 26.0, 25.54293728127535, 0.5416550425477212, 0.0, 1.0, 61589.93179350624], 
processed observation next is [1.0, 0.9130434782608695, 0.30563250230840255, 0.8833333333333333, 0.0, 0.0, 0.6666666666666666, 0.6285781067729458, 0.6805516808492404, 0.0, 1.0, 0.29328538949288685], 
reward next is 0.7067, 
noisyNet noise sample is [array([-0.96349525], dtype=float32), -0.23693584]. 
=============================================
[2019-04-04 15:21:55,449] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9528211e-09 1.6231551e-10 1.3240970e-16 1.8734853e-14 1.0000000e+00
 1.5518970e-11 1.0603155e-16], sum to 1.0000
[2019-04-04 15:21:55,449] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9657
[2019-04-04 15:21:55,480] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 47.0, 115.0, 804.0, 26.0, 26.61119369709709, 0.419385990759829, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3411000.0000, 
sim time next is 3411600.0000, 
raw observation next is [3.0, 46.33333333333334, 115.3333333333333, 806.1666666666666, 26.0, 26.59231894367101, 0.6201255550940191, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.46333333333333343, 0.3844444444444443, 0.8907918968692449, 0.6666666666666666, 0.7160265786392509, 0.706708518364673, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38260087], dtype=float32), 0.41882068]. 
=============================================
[2019-04-04 15:22:03,223] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7170895e-10 7.8974799e-11 1.1940883e-16 7.1847046e-15 1.0000000e+00
 7.4264567e-11 1.4499328e-15], sum to 1.0000
[2019-04-04 15:22:03,228] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6562
[2019-04-04 15:22:03,247] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 79.00000000000001, 0.0, 0.0, 26.0, 25.42104113657188, 0.4438307277997307, 0.0, 1.0, 49728.5877836527], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3460200.0000, 
sim time next is 3460800.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.38480998594849, 0.4631099262268282, 0.0, 1.0, 63628.44806702762], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6154008321623742, 0.6543699754089427, 0.0, 1.0, 0.30299260984298865], 
reward next is 0.6970, 
noisyNet noise sample is [array([-0.22502102], dtype=float32), -0.5261073]. 
=============================================
[2019-04-04 15:22:05,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8504809e-09 6.4271644e-10 1.3317804e-14 2.5987560e-14 1.0000000e+00
 1.8611456e-09 1.8278708e-14], sum to 1.0000
[2019-04-04 15:22:05,833] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6388
[2019-04-04 15:22:05,850] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 35.33333333333333, 84.33333333333334, 692.6666666666667, 26.0, 26.11124547100735, 0.665712880936084, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3944400.0000, 
sim time next is 3945000.0000, 
raw observation next is [-4.0, 34.66666666666667, 80.66666666666667, 661.3333333333334, 26.0, 26.34736277722889, 0.7042063303570112, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.34666666666666673, 0.2688888888888889, 0.730755064456722, 0.6666666666666666, 0.6956135647690743, 0.7347354434523371, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.95855474], dtype=float32), 2.1370375]. 
=============================================
[2019-04-04 15:22:05,855] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[79.511505]
 [79.85521 ]
 [80.227264]
 [80.60586 ]
 [80.85811 ]], R is [[79.38941193]
 [79.59552002]
 [79.79956818]
 [80.00157166]
 [80.20155334]].
[2019-04-04 15:22:09,873] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.2803837e-09 2.9752765e-09 1.4941627e-14 3.4973418e-14 1.0000000e+00
 7.5666931e-11 2.2076652e-14], sum to 1.0000
[2019-04-04 15:22:09,874] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0799
[2019-04-04 15:22:09,892] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 42.0, 99.33333333333333, 773.1666666666667, 26.0, 25.21868283827658, 0.4562126840804988, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3595200.0000, 
sim time next is 3595800.0000, 
raw observation next is [-1.0, 42.0, 96.66666666666667, 758.3333333333333, 26.0, 25.21557877554564, 0.4524056408336352, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.32222222222222224, 0.8379373848987108, 0.6666666666666666, 0.6012982312954701, 0.6508018802778784, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5825086], dtype=float32), -0.31853813]. 
=============================================
[2019-04-04 15:22:10,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4615801e-09 3.6331801e-10 2.0064084e-15 5.5606127e-14 1.0000000e+00
 3.2764438e-10 4.8485237e-15], sum to 1.0000
[2019-04-04 15:22:10,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9648
[2019-04-04 15:22:10,171] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.166666666666667, 38.0, 99.33333333333334, 766.6666666666666, 26.0, 26.98018598998344, 0.7812585428730809, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3941400.0000, 
sim time next is 3942000.0000, 
raw observation next is [-4.0, 38.0, 96.5, 756.0, 26.0, 27.02160653260623, 0.7899100600301866, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.38, 0.32166666666666666, 0.8353591160220994, 0.6666666666666666, 0.7518005443838526, 0.7633033533433955, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2565664], dtype=float32), -0.10024052]. 
=============================================
[2019-04-04 15:22:10,173] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.148   ]
 [81.37945 ]
 [81.57716 ]
 [81.787796]
 [81.94271 ]], R is [[81.13916016]
 [81.32776642]
 [81.51448822]
 [81.69934082]
 [81.88234711]].
[2019-04-04 15:22:12,216] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.1737851e-09 2.7462070e-09 5.5365363e-14 2.0081181e-13 1.0000000e+00
 1.0382611e-09 2.1275580e-14], sum to 1.0000
[2019-04-04 15:22:12,216] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7679
[2019-04-04 15:22:12,227] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 42.0, 96.66666666666667, 758.3333333333333, 26.0, 25.21476958614468, 0.4523112303929129, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3595800.0000, 
sim time next is 3596400.0000, 
raw observation next is [-1.0, 42.0, 94.0, 743.5, 26.0, 25.20321709672947, 0.4507287499785559, 0.0, 1.0, 18693.52366712677], 
processed observation next is [0.0, 0.6521739130434783, 0.4349030470914128, 0.42, 0.31333333333333335, 0.8215469613259668, 0.6666666666666666, 0.6002680913941226, 0.6502429166595186, 0.0, 1.0, 0.08901677936727033], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.0661441], dtype=float32), -0.14067747]. 
=============================================
[2019-04-04 15:22:16,634] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6970540e-09 8.2464702e-11 8.9216752e-16 2.3041670e-14 1.0000000e+00
 2.2750555e-11 1.5008286e-15], sum to 1.0000
[2019-04-04 15:22:16,634] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1186
[2019-04-04 15:22:16,692] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 72.0, 61.00000000000001, 331.3333333333334, 26.0, 25.19161835805996, 0.3035227619881227, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3744600.0000, 
sim time next is 3745200.0000, 
raw observation next is [-4.0, 73.0, 75.0, 380.1666666666667, 26.0, 25.18966495568438, 0.3319790735082151, 1.0, 1.0, 9360.018379773128], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.73, 0.25, 0.42007366482504604, 0.6666666666666666, 0.5991387463070316, 0.610659691169405, 1.0, 1.0, 0.04457151609415775], 
reward next is 0.9554, 
noisyNet noise sample is [array([-0.9058586], dtype=float32), -0.058493085]. 
=============================================
[2019-04-04 15:22:22,192] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4992342e-09 2.4970886e-10 1.2905220e-15 2.3366075e-14 1.0000000e+00
 7.2845389e-11 1.4263432e-15], sum to 1.0000
[2019-04-04 15:22:22,194] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6581
[2019-04-04 15:22:22,219] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 48.0, 99.66666666666666, 760.3333333333333, 26.0, 26.95831773453475, 0.6415824590269346, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3855000.0000, 
sim time next is 3855600.0000, 
raw observation next is [2.0, 48.0, 96.5, 749.5, 26.0, 26.33242765526969, 0.6766463042857912, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 0.48, 0.32166666666666666, 0.8281767955801105, 0.6666666666666666, 0.6943689712724742, 0.7255487680952637, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3013946], dtype=float32), 0.5911626]. 
=============================================
[2019-04-04 15:22:22,259] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7433540e-09 2.7511915e-10 1.5163202e-15 2.6594400e-14 1.0000000e+00
 8.1235768e-11 1.6475456e-15], sum to 1.0000
[2019-04-04 15:22:22,260] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2480
[2019-04-04 15:22:22,277] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.166666666666667, 47.5, 93.33333333333334, 738.6666666666667, 26.0, 25.70789908595633, 0.6098814342946847, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3856200.0000, 
sim time next is 3856800.0000, 
raw observation next is [2.333333333333333, 47.0, 90.16666666666666, 727.8333333333333, 26.0, 26.08771144655163, 0.6658102484577375, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5272391505078486, 0.47, 0.3005555555555555, 0.8042357274401473, 0.6666666666666666, 0.6739759538793025, 0.7219367494859125, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3013946], dtype=float32), 0.5911626]. 
=============================================
[2019-04-04 15:22:23,780] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4215843e-09 3.5802317e-10 9.4890026e-15 1.3340355e-14 1.0000000e+00
 2.1264224e-10 3.2835050e-15], sum to 1.0000
[2019-04-04 15:22:23,783] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7119
[2019-04-04 15:22:23,799] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 58.33333333333334, 0.0, 0.0, 26.0, 25.26587303829786, 0.4508119074885713, 0.0, 1.0, 91782.9292964003], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3883200.0000, 
sim time next is 3883800.0000, 
raw observation next is [-1.0, 59.16666666666666, 0.0, 0.0, 26.0, 25.25054319504078, 0.4553723000403219, 0.0, 1.0, 58579.33719536583], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.5916666666666666, 0.0, 0.0, 0.6666666666666666, 0.604211932920065, 0.6517907666801074, 0.0, 1.0, 0.2789492247398373], 
reward next is 0.7211, 
noisyNet noise sample is [array([-0.12320485], dtype=float32), -0.30878916]. 
=============================================
[2019-04-04 15:22:27,077] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7913202e-09 2.7959359e-09 2.0751253e-13 2.1212340e-13 1.0000000e+00
 1.9560298e-09 9.4383397e-14], sum to 1.0000
[2019-04-04 15:22:27,077] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3077
[2019-04-04 15:22:27,134] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.62475730424639, 0.5654991464940481, 1.0, 1.0, 83866.02276955031], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3954600.0000, 
sim time next is 3955200.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.6916681080471, 0.5862769587725998, 1.0, 1.0, 50258.70999634077], 
processed observation next is [1.0, 0.782608695652174, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.6409723423372583, 0.6954256529241999, 1.0, 1.0, 0.23932719045876555], 
reward next is 0.7607, 
noisyNet noise sample is [array([-0.3260441], dtype=float32), -1.0449164]. 
=============================================
[2019-04-04 15:22:28,793] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8320195e-08 6.9639106e-10 6.6323344e-14 7.2491004e-14 1.0000000e+00
 4.9305315e-09 6.5693843e-14], sum to 1.0000
[2019-04-04 15:22:28,794] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7409
[2019-04-04 15:22:28,837] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.65592691903439, 0.5308722181093252, 1.0, 1.0, 132724.479243397], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3955800.0000, 
sim time next is 3956400.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.61027928901198, 0.5408694495838785, 1.0, 1.0, 87122.30931127546], 
processed observation next is [1.0, 0.8260869565217391, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.6341899407509984, 0.6802898165279595, 1.0, 1.0, 0.4148681395775022], 
reward next is 0.5851, 
noisyNet noise sample is [array([-1.50277], dtype=float32), -0.46899003]. 
=============================================
[2019-04-04 15:22:42,768] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3108139e-09 8.1187439e-11 2.1850581e-16 2.1733629e-15 1.0000000e+00
 7.8726796e-11 6.0094832e-15], sum to 1.0000
[2019-04-04 15:22:42,769] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8916
[2019-04-04 15:22:42,808] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 71.0, 136.6666666666667, 283.6666666666666, 26.0, 26.07075539042388, 0.5183667539634536, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4611000.0000, 
sim time next is 4611600.0000, 
raw observation next is [-2.0, 71.0, 143.5, 340.0, 26.0, 26.09354763141553, 0.5248215425620012, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.40720221606648205, 0.71, 0.47833333333333333, 0.3756906077348066, 0.6666666666666666, 0.6744623026179607, 0.6749405141873338, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9897771], dtype=float32), 0.32444182]. 
=============================================
[2019-04-04 15:22:58,024] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.9090884e-09 6.5937256e-10 4.9856681e-15 9.2347482e-14 1.0000000e+00
 2.3016811e-10 5.4018260e-15], sum to 1.0000
[2019-04-04 15:22:58,026] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4892
[2019-04-04 15:22:58,041] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.42918353075378, 0.4481847635180026, 0.0, 1.0, 130741.9617671429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4573800.0000, 
sim time next is 4574400.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.29575730878954, 0.4517979051809522, 0.0, 1.0, 113004.4605589135], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6079797757324616, 0.6505993017269841, 0.0, 1.0, 0.538116478851969], 
reward next is 0.4619, 
noisyNet noise sample is [array([0.1834861], dtype=float32), -0.31240752]. 
=============================================
[2019-04-04 15:22:59,697] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:22:59,698] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:22:59,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run26
[2019-04-04 15:23:03,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:03,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:03,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run26
[2019-04-04 15:23:04,218] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:04,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:04,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run26
[2019-04-04 15:23:07,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:07,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:07,490] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run26
[2019-04-04 15:23:10,133] A3C_AGENT_WORKER-Thread-13 INFO:Local step 212500, global step 3393932: loss 0.5294
[2019-04-04 15:23:10,134] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 212500, global step 3393932: learning rate 0.0000
[2019-04-04 15:23:11,593] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5957616e-09 2.6200331e-10 4.5498870e-15 3.0067826e-14 1.0000000e+00
 1.4610743e-10 1.2540233e-14], sum to 1.0000
[2019-04-04 15:23:11,596] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3361
[2019-04-04 15:23:11,611] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.42076953727525, 0.3989128859617422, 0.0, 1.0, 25744.25807567277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4831800.0000, 
sim time next is 4832400.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.40297505794258, 0.3929613300150996, 0.0, 1.0, 40465.30279836569], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6169145881618817, 0.6309871100050332, 0.0, 1.0, 0.19269191808745564], 
reward next is 0.8073, 
noisyNet noise sample is [array([-1.0853121], dtype=float32), 0.19192806]. 
=============================================
[2019-04-04 15:23:13,354] A3C_AGENT_WORKER-Thread-2 INFO:Local step 212500, global step 3395502: loss 0.5398
[2019-04-04 15:23:13,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 212500, global step 3395505: learning rate 0.0000
[2019-04-04 15:23:13,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:13,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:13,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run26
[2019-04-04 15:23:14,533] A3C_AGENT_WORKER-Thread-17 INFO:Local step 212500, global step 3396049: loss 0.5378
[2019-04-04 15:23:14,535] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 212500, global step 3396049: learning rate 0.0000
[2019-04-04 15:23:16,529] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.08054214e-08 8.59777305e-10 6.47881695e-15 4.18891674e-14
 1.00000000e+00 2.92131958e-10 1.84074591e-14], sum to 1.0000
[2019-04-04 15:23:16,531] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9844
[2019-04-04 15:23:16,617] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.833333333333333, 65.0, 39.33333333333334, 67.33333333333334, 26.0, 25.24773799979513, 0.3381752959093828, 0.0, 1.0, 37077.03274984133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5037000.0000, 
sim time next is 5037600.0000, 
raw observation next is [-2.666666666666667, 65.0, 49.16666666666667, 84.16666666666667, 26.0, 25.21047004058141, 0.386900563261592, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.38873499538319484, 0.65, 0.16388888888888892, 0.09300184162062615, 0.6666666666666666, 0.6008725033817841, 0.6289668544205307, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.07134375], dtype=float32), 0.5191278]. 
=============================================
[2019-04-04 15:23:17,371] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.6541930e-09 2.7295748e-09 2.0423369e-14 6.9057634e-14 1.0000000e+00
 4.7298104e-10 3.5552233e-14], sum to 1.0000
[2019-04-04 15:23:17,373] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7333
[2019-04-04 15:23:17,392] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 51.66666666666666, 0.0, 0.0, 26.0, 25.40701204364291, 0.3933680615606672, 0.0, 1.0, 28384.51467155455], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5028000.0000, 
sim time next is 5028600.0000, 
raw observation next is [-1.0, 50.83333333333334, 0.0, 0.0, 26.0, 25.40576046550209, 0.3879413481717256, 0.0, 1.0, 33805.45743529902], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.5083333333333334, 0.0, 0.0, 0.6666666666666666, 0.6171467054585076, 0.6293137827239085, 0.0, 1.0, 0.16097836873951912], 
reward next is 0.8390, 
noisyNet noise sample is [array([0.5571878], dtype=float32), -0.13626294]. 
=============================================
[2019-04-04 15:23:17,643] A3C_AGENT_WORKER-Thread-11 INFO:Local step 212500, global step 3397614: loss 0.5310
[2019-04-04 15:23:17,646] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 212500, global step 3397615: learning rate 0.0000
[2019-04-04 15:23:17,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:17,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:17,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run26
[2019-04-04 15:23:18,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:18,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:18,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run26
[2019-04-04 15:23:20,076] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:20,076] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:20,079] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run26
[2019-04-04 15:23:21,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:21,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:21,434] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run26
[2019-04-04 15:23:21,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:21,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:21,479] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run26
[2019-04-04 15:23:22,119] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:22,119] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:22,123] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run26
[2019-04-04 15:23:22,895] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:22,895] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:22,900] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run26
[2019-04-04 15:23:23,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:23,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:23,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run26
[2019-04-04 15:23:24,213] A3C_AGENT_WORKER-Thread-16 INFO:Local step 212500, global step 3399800: loss 0.5451
[2019-04-04 15:23:24,215] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 212500, global step 3399800: learning rate 0.0000
[2019-04-04 15:23:24,802] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.6974330e-09 2.8528802e-10 3.6339124e-15 3.8282039e-15 1.0000000e+00
 6.4713949e-11 1.3002865e-15], sum to 1.0000
[2019-04-04 15:23:24,802] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7193
[2019-04-04 15:23:24,823] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.0, 19.0, 0.0, 0.0, 26.0, 27.12330518833017, 0.8543868026209204, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5086800.0000, 
sim time next is 5087400.0000, 
raw observation next is [8.95, 19.0, 0.0, 0.0, 26.0, 27.0611845561474, 0.8412303998037814, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7105263157894738, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7550987130122833, 0.7804101332679272, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6523375], dtype=float32), 0.95505196]. 
=============================================
[2019-04-04 15:23:24,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:24,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:24,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run26
[2019-04-04 15:23:25,232] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-04 15:23:25,236] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 15:23:25,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:25,239] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run35
[2019-04-04 15:23:25,268] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 15:23:25,270] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:25,273] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run35
[2019-04-04 15:23:25,273] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 15:23:25,273] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:25,306] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run35
[2019-04-04 15:23:25,446] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:25,446] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:25,450] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run26
[2019-04-04 15:23:26,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:23:26,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:23:26,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run26
[2019-04-04 15:23:38,640] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17997827], dtype=float32), 0.22735806]
[2019-04-04 15:23:38,640] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.4, 65.0, 0.0, 0.0, 26.0, 25.58077239967007, 0.2958068049067215, 1.0, 1.0, 0.0]
[2019-04-04 15:23:38,640] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 15:23:38,641] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.3371416e-09 1.7841395e-10 1.8392744e-15 1.7405309e-14 1.0000000e+00
 1.3845844e-10 3.0416962e-15], sampled 0.6315676807903408
[2019-04-04 15:24:13,845] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17997827], dtype=float32), 0.22735806]
[2019-04-04 15:24:13,845] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.322371319666667, 83.30934908333333, 0.0, 0.0, 26.0, 24.69704000371945, 0.3036239972788777, 0.0, 1.0, 44496.8166767792]
[2019-04-04 15:24:13,846] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 15:24:13,847] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.5291074e-10 1.1302287e-10 2.8584543e-16 6.2008123e-15 1.0000000e+00
 2.4547050e-11 6.3575990e-16], sampled 0.07951901071962342
[2019-04-04 15:24:20,371] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17997827], dtype=float32), 0.22735806]
[2019-04-04 15:24:20,372] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-5.6, 83.0, 0.0, 0.0, 26.0, 24.33854659735294, 0.1092330710769949, 0.0, 1.0, 41260.23623151728]
[2019-04-04 15:24:20,372] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 15:24:20,372] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.0145240e-09 8.9307595e-10 6.4233240e-15 9.8592372e-14 1.0000000e+00
 2.7495967e-10 1.1875119e-14], sampled 0.42484827400165137
[2019-04-04 15:24:24,753] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17997827], dtype=float32), 0.22735806]
[2019-04-04 15:24:24,753] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-0.01666666666666667, 45.0, 150.3333333333333, 207.3333333333333, 26.0, 25.02246236380546, 0.3190652601801245, 0.0, 1.0, 23984.89956399502]
[2019-04-04 15:24:24,753] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 15:24:24,754] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.6161599e-09 6.7635331e-10 9.5121249e-15 5.2212019e-14 1.0000000e+00
 2.9218880e-10 1.6808908e-14], sampled 0.7502311267101786
[2019-04-04 15:25:07,848] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.6115 239901591.9745 1605.2062
[2019-04-04 15:25:25,934] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.3231 263482140.3031 1556.9109
[2019-04-04 15:25:31,364] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 15:25:32,388] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 3400000, evaluation results [3400000.0, 7241.323141413718, 263482140.30311874, 1556.9109350937526, 7353.611466788218, 239901591.97446975, 1605.2062106696299, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 15:25:35,554] A3C_AGENT_WORKER-Thread-15 INFO:Local step 212500, global step 3401098: loss 0.5769
[2019-04-04 15:25:35,555] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 212500, global step 3401098: learning rate 0.0000
[2019-04-04 15:25:36,884] A3C_AGENT_WORKER-Thread-18 INFO:Local step 212500, global step 3401473: loss 0.5628
[2019-04-04 15:25:36,887] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 212500, global step 3401474: learning rate 0.0000
[2019-04-04 15:25:36,984] A3C_AGENT_WORKER-Thread-12 INFO:Local step 212500, global step 3401504: loss 0.5715
[2019-04-04 15:25:36,984] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 212500, global step 3401504: learning rate 0.0000
[2019-04-04 15:25:37,148] A3C_AGENT_WORKER-Thread-3 INFO:Local step 212500, global step 3401550: loss 0.5613
[2019-04-04 15:25:37,148] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 212500, global step 3401550: learning rate 0.0000
[2019-04-04 15:25:37,166] A3C_AGENT_WORKER-Thread-5 INFO:Local step 212500, global step 3401555: loss 0.5532
[2019-04-04 15:25:37,166] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 212500, global step 3401555: learning rate 0.0000
[2019-04-04 15:25:37,237] A3C_AGENT_WORKER-Thread-4 INFO:Local step 212500, global step 3401577: loss 0.5705
[2019-04-04 15:25:37,237] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 212500, global step 3401577: learning rate 0.0000
[2019-04-04 15:25:37,238] A3C_AGENT_WORKER-Thread-10 INFO:Local step 212500, global step 3401577: loss 0.5787
[2019-04-04 15:25:37,239] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 212500, global step 3401577: learning rate 0.0000
[2019-04-04 15:25:37,254] A3C_AGENT_WORKER-Thread-20 INFO:Local step 212500, global step 3401585: loss 0.5670
[2019-04-04 15:25:37,261] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 212500, global step 3401585: learning rate 0.0000
[2019-04-04 15:25:37,278] A3C_AGENT_WORKER-Thread-14 INFO:Local step 212500, global step 3401592: loss 0.5599
[2019-04-04 15:25:37,279] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 212500, global step 3401592: learning rate 0.0000
[2019-04-04 15:25:37,312] A3C_AGENT_WORKER-Thread-19 INFO:Local step 212500, global step 3401600: loss 0.5578
[2019-04-04 15:25:37,313] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 212500, global step 3401600: learning rate 0.0000
[2019-04-04 15:25:37,567] A3C_AGENT_WORKER-Thread-6 INFO:Local step 212500, global step 3401680: loss 0.5508
[2019-04-04 15:25:37,578] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 212500, global step 3401680: learning rate 0.0000
[2019-04-04 15:25:40,379] A3C_AGENT_WORKER-Thread-13 INFO:Local step 213000, global step 3402869: loss 0.0189
[2019-04-04 15:25:40,380] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 213000, global step 3402869: learning rate 0.0000
[2019-04-04 15:25:43,721] A3C_AGENT_WORKER-Thread-2 INFO:Local step 213000, global step 3403749: loss 0.0166
[2019-04-04 15:25:43,723] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 213000, global step 3403751: learning rate 0.0000
[2019-04-04 15:25:45,068] A3C_AGENT_WORKER-Thread-17 INFO:Local step 213000, global step 3404104: loss 0.0177
[2019-04-04 15:25:45,075] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 213000, global step 3404105: learning rate 0.0000
[2019-04-04 15:25:45,847] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2434900e-08 2.7506564e-10 2.0977198e-14 2.6920735e-13 1.0000000e+00
 7.4139956e-09 1.0446214e-13], sum to 1.0000
[2019-04-04 15:25:45,850] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4540
[2019-04-04 15:25:45,874] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 69.5, 0.0, 0.0, 26.0, 24.44995635935216, 0.1770119743063205, 0.0, 1.0, 45818.9281230924], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 163800.0000, 
sim time next is 164400.0000, 
raw observation next is [-8.4, 70.0, 0.0, 0.0, 26.0, 24.40198508068561, 0.1658208529164477, 0.0, 1.0, 45766.39566822183], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5334987567238008, 0.5552736176388159, 0.0, 1.0, 0.21793521746772299], 
reward next is 0.7821, 
noisyNet noise sample is [array([0.839735], dtype=float32), -0.31422955]. 
=============================================
[2019-04-04 15:25:48,204] A3C_AGENT_WORKER-Thread-11 INFO:Local step 213000, global step 3405294: loss 0.0165
[2019-04-04 15:25:48,208] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 213000, global step 3405294: learning rate 0.0000
[2019-04-04 15:25:52,690] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1841534e-08 9.7551274e-09 2.7014708e-13 2.8800217e-12 9.9999988e-01
 7.9137568e-09 3.1386062e-13], sum to 1.0000
[2019-04-04 15:25:52,691] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0005
[2019-04-04 15:25:52,751] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.266666666666667, 37.66666666666667, 19.16666666666667, 0.0, 26.0, 24.83994753184961, 0.141049530288135, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 462000.0000, 
sim time next is 462600.0000, 
raw observation next is [-7.0, 36.5, 23.0, 0.0, 26.0, 25.15166687503788, 0.1718064403962706, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2686980609418283, 0.365, 0.07666666666666666, 0.0, 0.6666666666666666, 0.5959722395864899, 0.5572688134654236, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2231811], dtype=float32), -0.05944313]. 
=============================================
[2019-04-04 15:25:53,121] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.2622920e-09 4.6141152e-10 1.0102455e-14 1.2131465e-13 1.0000000e+00
 4.4812046e-10 6.3971105e-15], sum to 1.0000
[2019-04-04 15:25:53,121] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0149
[2019-04-04 15:25:53,149] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.333333333333334, 69.0, 0.0, 0.0, 26.0, 24.14993192803506, 0.0812965499261787, 0.0, 1.0, 44683.84938549842], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 262200.0000, 
sim time next is 262800.0000, 
raw observation next is [-6.7, 67.0, 0.0, 0.0, 26.0, 24.14697953739614, 0.07460676864728757, 0.0, 1.0, 44846.46258689865], 
processed observation next is [1.0, 0.043478260869565216, 0.2770083102493075, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5122482947830115, 0.5248689228824291, 0.0, 1.0, 0.21355458374713643], 
reward next is 0.7864, 
noisyNet noise sample is [array([-0.04304445], dtype=float32), -0.47311297]. 
=============================================
[2019-04-04 15:25:53,726] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6434148e-08 1.7791993e-09 8.2329095e-14 2.2225424e-13 1.0000000e+00
 1.7460283e-09 1.4276205e-14], sum to 1.0000
[2019-04-04 15:25:53,727] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3620
[2019-04-04 15:25:53,743] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 69.0, 0.0, 0.0, 26.0, 24.01977553326479, 0.0487973991717717, 0.0, 1.0, 45326.76408626579], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 264600.0000, 
sim time next is 265200.0000, 
raw observation next is [-7.1, 69.66666666666666, 0.0, 0.0, 26.0, 24.00759741683281, 0.04662319573082763, 0.0, 1.0, 45414.31642967485], 
processed observation next is [1.0, 0.043478260869565216, 0.2659279778393352, 0.6966666666666665, 0.0, 0.0, 0.6666666666666666, 0.5006331180694007, 0.5155410652436092, 0.0, 1.0, 0.21625864966511835], 
reward next is 0.7837, 
noisyNet noise sample is [array([0.42281967], dtype=float32), 0.52590716]. 
=============================================
[2019-04-04 15:25:56,119] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0105299e-08 1.0035073e-09 3.7119887e-14 8.0578908e-13 1.0000000e+00
 1.1341660e-09 7.9151222e-15], sum to 1.0000
[2019-04-04 15:25:56,120] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9434
[2019-04-04 15:25:56,120] A3C_AGENT_WORKER-Thread-16 INFO:Local step 213000, global step 3407780: loss 0.0164
[2019-04-04 15:25:56,120] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 213000, global step 3407780: learning rate 0.0000
[2019-04-04 15:25:56,184] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.3, 67.0, 62.5, 384.5, 26.0, 25.70795429258868, 0.3036182209135867, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 291600.0000, 
sim time next is 292200.0000, 
raw observation next is [-12.2, 66.33333333333334, 73.33333333333334, 384.0, 26.0, 25.63348245076612, 0.2992575007909693, 1.0, 1.0, 18775.81002285966], 
processed observation next is [1.0, 0.391304347826087, 0.12465373961218838, 0.6633333333333334, 0.24444444444444446, 0.4243093922651934, 0.6666666666666666, 0.6361235375638433, 0.5997525002636565, 1.0, 1.0, 0.08940861915647456], 
reward next is 0.9106, 
noisyNet noise sample is [array([0.95430446], dtype=float32), 0.54497445]. 
=============================================
[2019-04-04 15:26:00,330] A3C_AGENT_WORKER-Thread-15 INFO:Local step 213000, global step 3408934: loss 0.0138
[2019-04-04 15:26:00,331] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 213000, global step 3408934: learning rate 0.0000
[2019-04-04 15:26:01,009] A3C_AGENT_WORKER-Thread-18 INFO:Local step 213000, global step 3409263: loss 0.0129
[2019-04-04 15:26:01,010] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 213000, global step 3409263: learning rate 0.0000
[2019-04-04 15:26:01,026] A3C_AGENT_WORKER-Thread-12 INFO:Local step 213000, global step 3409273: loss 0.0133
[2019-04-04 15:26:01,029] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 213000, global step 3409273: learning rate 0.0000
[2019-04-04 15:26:01,054] A3C_AGENT_WORKER-Thread-14 INFO:Local step 213000, global step 3409285: loss 0.0140
[2019-04-04 15:26:01,063] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 213000, global step 3409285: learning rate 0.0000
[2019-04-04 15:26:01,151] A3C_AGENT_WORKER-Thread-3 INFO:Local step 213000, global step 3409334: loss 0.0123
[2019-04-04 15:26:01,151] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 213000, global step 3409334: learning rate 0.0000
[2019-04-04 15:26:01,211] A3C_AGENT_WORKER-Thread-10 INFO:Local step 213000, global step 3409364: loss 0.0126
[2019-04-04 15:26:01,211] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 213000, global step 3409364: learning rate 0.0000
[2019-04-04 15:26:01,276] A3C_AGENT_WORKER-Thread-5 INFO:Local step 213000, global step 3409397: loss 0.0127
[2019-04-04 15:26:01,277] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 213000, global step 3409397: learning rate 0.0000
[2019-04-04 15:26:01,534] A3C_AGENT_WORKER-Thread-20 INFO:Local step 213000, global step 3409526: loss 0.0131
[2019-04-04 15:26:01,540] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 213000, global step 3409526: learning rate 0.0000
[2019-04-04 15:26:01,762] A3C_AGENT_WORKER-Thread-4 INFO:Local step 213000, global step 3409619: loss 0.0128
[2019-04-04 15:26:01,767] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 213000, global step 3409620: learning rate 0.0000
[2019-04-04 15:26:01,899] A3C_AGENT_WORKER-Thread-19 INFO:Local step 213000, global step 3409668: loss 0.0119
[2019-04-04 15:26:01,905] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 213000, global step 3409669: learning rate 0.0000
[2019-04-04 15:26:02,054] A3C_AGENT_WORKER-Thread-6 INFO:Local step 213000, global step 3409722: loss 0.0113
[2019-04-04 15:26:02,054] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 213000, global step 3409722: learning rate 0.0000
[2019-04-04 15:26:05,519] A3C_AGENT_WORKER-Thread-13 INFO:Local step 213500, global step 3410656: loss 0.5774
[2019-04-04 15:26:05,522] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 213500, global step 3410656: learning rate 0.0000
[2019-04-04 15:26:07,869] A3C_AGENT_WORKER-Thread-2 INFO:Local step 213500, global step 3411345: loss 0.5829
[2019-04-04 15:26:07,871] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 213500, global step 3411347: learning rate 0.0000
[2019-04-04 15:26:09,862] A3C_AGENT_WORKER-Thread-17 INFO:Local step 213500, global step 3412254: loss 0.6032
[2019-04-04 15:26:09,863] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 213500, global step 3412254: learning rate 0.0000
[2019-04-04 15:26:12,390] A3C_AGENT_WORKER-Thread-11 INFO:Local step 213500, global step 3413008: loss 0.6088
[2019-04-04 15:26:12,392] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 213500, global step 3413008: learning rate 0.0000
[2019-04-04 15:26:12,898] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1849537e-09 6.4496247e-10 2.3678997e-15 4.7354786e-14 1.0000000e+00
 3.8047163e-10 6.5295559e-15], sum to 1.0000
[2019-04-04 15:26:12,899] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0238
[2019-04-04 15:26:12,985] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 84.0, 0.0, 0.0, 26.0, 24.7870796034236, 0.1520487495956447, 1.0, 1.0, 181567.7533548503], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 496800.0000, 
sim time next is 497400.0000, 
raw observation next is [0.6000000000000001, 86.0, 0.0, 0.0, 26.0, 24.41274978989135, 0.1839365788657097, 1.0, 1.0, 199571.1242047812], 
processed observation next is [1.0, 0.782608695652174, 0.479224376731302, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5343958158242792, 0.5613121929552366, 1.0, 1.0, 0.9503386866894342], 
reward next is 0.0497, 
noisyNet noise sample is [array([1.6511563], dtype=float32), 0.4895422]. 
=============================================
[2019-04-04 15:26:17,871] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9974861e-10 4.9371139e-11 2.8834791e-16 2.8981141e-15 1.0000000e+00
 1.1479484e-10 5.1713543e-16], sum to 1.0000
[2019-04-04 15:26:17,871] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1186
[2019-04-04 15:26:17,909] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.899999999999999, 83.33333333333334, 45.66666666666667, 0.0, 26.0, 25.76830744805835, 0.4094999905062375, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 834000.0000, 
sim time next is 834600.0000, 
raw observation next is [-3.9, 82.66666666666667, 42.33333333333334, 0.0, 26.0, 25.95459888666594, 0.4248331313236604, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.8266666666666667, 0.14111111111111113, 0.0, 0.6666666666666666, 0.6628832405554951, 0.6416110437745535, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3727045], dtype=float32), -0.21535914]. 
=============================================
[2019-04-04 15:26:20,347] A3C_AGENT_WORKER-Thread-16 INFO:Local step 213500, global step 3415912: loss 0.6170
[2019-04-04 15:26:20,348] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 213500, global step 3415912: learning rate 0.0000
[2019-04-04 15:26:23,431] A3C_AGENT_WORKER-Thread-15 INFO:Local step 213500, global step 3417209: loss 0.5751
[2019-04-04 15:26:23,432] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 213500, global step 3417209: learning rate 0.0000
[2019-04-04 15:26:24,226] A3C_AGENT_WORKER-Thread-18 INFO:Local step 213500, global step 3417415: loss 0.5851
[2019-04-04 15:26:24,226] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 213500, global step 3417415: learning rate 0.0000
[2019-04-04 15:26:24,532] A3C_AGENT_WORKER-Thread-12 INFO:Local step 213500, global step 3417503: loss 0.5863
[2019-04-04 15:26:24,533] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 213500, global step 3417503: learning rate 0.0000
[2019-04-04 15:26:24,562] A3C_AGENT_WORKER-Thread-3 INFO:Local step 213500, global step 3417516: loss 0.5917
[2019-04-04 15:26:24,563] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 213500, global step 3417516: learning rate 0.0000
[2019-04-04 15:26:24,922] A3C_AGENT_WORKER-Thread-10 INFO:Local step 213500, global step 3417620: loss 0.5916
[2019-04-04 15:26:24,923] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 213500, global step 3417620: learning rate 0.0000
[2019-04-04 15:26:24,953] A3C_AGENT_WORKER-Thread-14 INFO:Local step 213500, global step 3417631: loss 0.5904
[2019-04-04 15:26:24,954] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 213500, global step 3417631: learning rate 0.0000
[2019-04-04 15:26:24,971] A3C_AGENT_WORKER-Thread-4 INFO:Local step 213500, global step 3417635: loss 0.5899
[2019-04-04 15:26:24,972] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 213500, global step 3417635: learning rate 0.0000
[2019-04-04 15:26:25,295] A3C_AGENT_WORKER-Thread-20 INFO:Local step 213500, global step 3417749: loss 0.5902
[2019-04-04 15:26:25,301] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 213500, global step 3417749: learning rate 0.0000
[2019-04-04 15:26:25,303] A3C_AGENT_WORKER-Thread-5 INFO:Local step 213500, global step 3417750: loss 0.5953
[2019-04-04 15:26:25,325] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 213500, global step 3417755: learning rate 0.0000
[2019-04-04 15:26:26,085] A3C_AGENT_WORKER-Thread-19 INFO:Local step 213500, global step 3418032: loss 0.5940
[2019-04-04 15:26:26,086] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 213500, global step 3418032: learning rate 0.0000
[2019-04-04 15:26:26,230] A3C_AGENT_WORKER-Thread-13 INFO:Local step 214000, global step 3418085: loss 0.2401
[2019-04-04 15:26:26,232] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 214000, global step 3418086: learning rate 0.0000
[2019-04-04 15:26:26,290] A3C_AGENT_WORKER-Thread-6 INFO:Local step 213500, global step 3418108: loss 0.5761
[2019-04-04 15:26:26,294] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 213500, global step 3418108: learning rate 0.0000
[2019-04-04 15:26:28,150] A3C_AGENT_WORKER-Thread-2 INFO:Local step 214000, global step 3419057: loss 0.2490
[2019-04-04 15:26:28,155] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 214000, global step 3419060: learning rate 0.0000
[2019-04-04 15:26:30,116] A3C_AGENT_WORKER-Thread-17 INFO:Local step 214000, global step 3419997: loss 0.2442
[2019-04-04 15:26:30,117] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 214000, global step 3419997: learning rate 0.0000
[2019-04-04 15:26:30,991] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2937552e-08 3.8171257e-09 4.2260249e-14 1.8068393e-12 1.0000000e+00
 3.4672034e-09 1.8522599e-13], sum to 1.0000
[2019-04-04 15:26:30,991] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9620
[2019-04-04 15:26:31,018] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.8, 71.5, 0.0, 0.0, 26.0, 24.0441498906682, 0.07081061009847368, 0.0, 1.0, 41523.07073874708], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 785400.0000, 
sim time next is 786000.0000, 
raw observation next is [-7.8, 72.0, 0.0, 0.0, 26.0, 23.99229963019154, 0.06199878385895064, 0.0, 1.0, 41501.10184396763], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.72, 0.0, 0.0, 0.6666666666666666, 0.4993583025159616, 0.5206662612863169, 0.0, 1.0, 0.19762429449508398], 
reward next is 0.8024, 
noisyNet noise sample is [array([-0.13992687], dtype=float32), -1.1948696]. 
=============================================
[2019-04-04 15:26:31,026] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[75.23626 ]
 [75.33368 ]
 [75.551605]
 [75.498795]
 [75.64299 ]], R is [[75.18350983]
 [75.23394775]
 [75.28379822]
 [75.33314514]
 [75.38198853]].
[2019-04-04 15:26:31,707] A3C_AGENT_WORKER-Thread-11 INFO:Local step 214000, global step 3420735: loss 0.2405
[2019-04-04 15:26:31,708] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 214000, global step 3420735: learning rate 0.0000
[2019-04-04 15:26:36,230] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0834452e-09 7.2980566e-11 3.5000076e-16 5.9694079e-15 1.0000000e+00
 3.9566694e-11 3.9047343e-16], sum to 1.0000
[2019-04-04 15:26:36,231] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3580
[2019-04-04 15:26:36,239] A3C_AGENT_WORKER-Thread-13 INFO:Local step 214500, global step 3422920: loss 0.2417
[2019-04-04 15:26:36,240] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 214500, global step 3422920: learning rate 0.0000
[2019-04-04 15:26:36,270] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 82.5, 59.0, 0.0, 26.0, 26.29754502144262, 0.4552860388030156, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 829800.0000, 
sim time next is 830400.0000, 
raw observation next is [-3.899999999999999, 83.66666666666666, 57.33333333333334, 0.0, 26.0, 26.28186492108257, 0.4503126496060097, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.35457063711911363, 0.8366666666666666, 0.19111111111111115, 0.0, 0.6666666666666666, 0.6901554100902141, 0.6501042165353366, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3093944], dtype=float32), -0.25433093]. 
=============================================
[2019-04-04 15:26:38,146] A3C_AGENT_WORKER-Thread-2 INFO:Local step 214500, global step 3423775: loss 0.2363
[2019-04-04 15:26:38,147] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 214500, global step 3423776: learning rate 0.0000
[2019-04-04 15:26:39,604] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.0697838e-09 2.9308929e-09 2.6540180e-14 5.2674946e-14 1.0000000e+00
 2.0549347e-10 3.9230429e-14], sum to 1.0000
[2019-04-04 15:26:39,608] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7492
[2019-04-04 15:26:39,613] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.0, 65.0, 0.0, 0.0, 26.0, 24.79336267688942, 0.4237380302512479, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1189800.0000, 
sim time next is 1190400.0000, 
raw observation next is [17.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.77780888046087, 0.4180460693653203, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.9584487534626038, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5648174067050725, 0.6393486897884401, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02413575], dtype=float32), -0.37865222]. 
=============================================
[2019-04-04 15:26:40,143] A3C_AGENT_WORKER-Thread-16 INFO:Local step 214000, global step 3424923: loss 0.2917
[2019-04-04 15:26:40,146] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 214000, global step 3424924: learning rate 0.0000
[2019-04-04 15:26:40,486] A3C_AGENT_WORKER-Thread-17 INFO:Local step 214500, global step 3425119: loss 0.2390
[2019-04-04 15:26:40,487] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 214500, global step 3425119: learning rate 0.0000
[2019-04-04 15:26:41,753] A3C_AGENT_WORKER-Thread-11 INFO:Local step 214500, global step 3425702: loss 0.2364
[2019-04-04 15:26:41,755] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 214500, global step 3425703: learning rate 0.0000
[2019-04-04 15:26:42,522] A3C_AGENT_WORKER-Thread-15 INFO:Local step 214000, global step 3426101: loss 0.2876
[2019-04-04 15:26:42,523] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 214000, global step 3426102: learning rate 0.0000
[2019-04-04 15:26:42,906] A3C_AGENT_WORKER-Thread-18 INFO:Local step 214000, global step 3426286: loss 0.2973
[2019-04-04 15:26:42,907] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 214000, global step 3426286: learning rate 0.0000
[2019-04-04 15:26:43,168] A3C_AGENT_WORKER-Thread-3 INFO:Local step 214000, global step 3426427: loss 0.2807
[2019-04-04 15:26:43,169] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 214000, global step 3426427: learning rate 0.0000
[2019-04-04 15:26:43,302] A3C_AGENT_WORKER-Thread-12 INFO:Local step 214000, global step 3426499: loss 0.2735
[2019-04-04 15:26:43,303] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 214000, global step 3426499: learning rate 0.0000
[2019-04-04 15:26:43,668] A3C_AGENT_WORKER-Thread-14 INFO:Local step 214000, global step 3426707: loss 0.2780
[2019-04-04 15:26:43,672] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 214000, global step 3426708: learning rate 0.0000
[2019-04-04 15:26:43,838] A3C_AGENT_WORKER-Thread-10 INFO:Local step 214000, global step 3426805: loss 0.2731
[2019-04-04 15:26:43,841] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 214000, global step 3426806: learning rate 0.0000
[2019-04-04 15:26:44,004] A3C_AGENT_WORKER-Thread-4 INFO:Local step 214000, global step 3426912: loss 0.2771
[2019-04-04 15:26:44,006] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 214000, global step 3426913: learning rate 0.0000
[2019-04-04 15:26:44,102] A3C_AGENT_WORKER-Thread-20 INFO:Local step 214000, global step 3426968: loss 0.2589
[2019-04-04 15:26:44,103] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 214000, global step 3426969: learning rate 0.0000
[2019-04-04 15:26:44,639] A3C_AGENT_WORKER-Thread-5 INFO:Local step 214000, global step 3427298: loss 0.2685
[2019-04-04 15:26:44,640] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 214000, global step 3427298: learning rate 0.0000
[2019-04-04 15:26:45,010] A3C_AGENT_WORKER-Thread-19 INFO:Local step 214000, global step 3427510: loss 0.2591
[2019-04-04 15:26:45,015] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 214000, global step 3427514: learning rate 0.0000
[2019-04-04 15:26:45,295] A3C_AGENT_WORKER-Thread-6 INFO:Local step 214000, global step 3427678: loss 0.2634
[2019-04-04 15:26:45,296] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 214000, global step 3427678: learning rate 0.0000
[2019-04-04 15:26:51,096] A3C_AGENT_WORKER-Thread-16 INFO:Local step 214500, global step 3431489: loss 0.2424
[2019-04-04 15:26:51,099] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 214500, global step 3431490: learning rate 0.0000
[2019-04-04 15:26:51,666] A3C_AGENT_WORKER-Thread-13 INFO:Local step 215000, global step 3431869: loss 0.0227
[2019-04-04 15:26:51,669] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 215000, global step 3431869: learning rate 0.0000
[2019-04-04 15:26:53,279] A3C_AGENT_WORKER-Thread-2 INFO:Local step 215000, global step 3432954: loss 0.0283
[2019-04-04 15:26:53,280] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 215000, global step 3432954: learning rate 0.0000
[2019-04-04 15:26:53,536] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8068592e-09 8.1960091e-11 9.6166072e-17 8.4420123e-15 1.0000000e+00
 1.3941828e-10 3.2751593e-16], sum to 1.0000
[2019-04-04 15:26:53,540] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5558
[2019-04-04 15:26:53,553] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.5, 84.83333333333334, 0.0, 0.0, 26.0, 25.70169867018478, 0.5338795399279557, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1565400.0000, 
sim time next is 1566000.0000, 
raw observation next is [4.4, 86.0, 0.0, 0.0, 26.0, 25.60156706634271, 0.5257190503291143, 0.0, 1.0, 18736.71445794268], 
processed observation next is [1.0, 0.13043478260869565, 0.5844875346260389, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6334639221952258, 0.6752396834430381, 0.0, 1.0, 0.08922244979972704], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.98497254], dtype=float32), 0.8820906]. 
=============================================
[2019-04-04 15:26:53,560] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[89.90487 ]
 [90.1553  ]
 [90.295555]
 [90.37801 ]
 [90.49615 ]], R is [[89.89529419]
 [89.99634552]
 [90.09638214]
 [90.15077972]
 [90.14600372]].
[2019-04-04 15:26:53,808] A3C_AGENT_WORKER-Thread-15 INFO:Local step 214500, global step 3433309: loss 0.2354
[2019-04-04 15:26:53,810] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 214500, global step 3433309: learning rate 0.0000
[2019-04-04 15:26:54,107] A3C_AGENT_WORKER-Thread-18 INFO:Local step 214500, global step 3433514: loss 0.2346
[2019-04-04 15:26:54,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 214500, global step 3433515: learning rate 0.0000
[2019-04-04 15:26:54,429] A3C_AGENT_WORKER-Thread-3 INFO:Local step 214500, global step 3433725: loss 0.2345
[2019-04-04 15:26:54,431] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 214500, global step 3433727: learning rate 0.0000
[2019-04-04 15:26:54,507] A3C_AGENT_WORKER-Thread-12 INFO:Local step 214500, global step 3433778: loss 0.2329
[2019-04-04 15:26:54,508] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 214500, global step 3433778: learning rate 0.0000
[2019-04-04 15:26:55,173] A3C_AGENT_WORKER-Thread-14 INFO:Local step 214500, global step 3434186: loss 0.2347
[2019-04-04 15:26:55,175] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 214500, global step 3434186: learning rate 0.0000
[2019-04-04 15:26:55,285] A3C_AGENT_WORKER-Thread-10 INFO:Local step 214500, global step 3434259: loss 0.2362
[2019-04-04 15:26:55,287] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 214500, global step 3434259: learning rate 0.0000
[2019-04-04 15:26:55,582] A3C_AGENT_WORKER-Thread-4 INFO:Local step 214500, global step 3434455: loss 0.2312
[2019-04-04 15:26:55,582] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 214500, global step 3434455: learning rate 0.0000
[2019-04-04 15:26:55,608] A3C_AGENT_WORKER-Thread-20 INFO:Local step 214500, global step 3434473: loss 0.2312
[2019-04-04 15:26:55,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 214500, global step 3434473: learning rate 0.0000
[2019-04-04 15:26:55,726] A3C_AGENT_WORKER-Thread-17 INFO:Local step 215000, global step 3434546: loss 0.0344
[2019-04-04 15:26:55,727] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 215000, global step 3434547: learning rate 0.0000
[2019-04-04 15:26:56,144] A3C_AGENT_WORKER-Thread-5 INFO:Local step 214500, global step 3434786: loss 0.2306
[2019-04-04 15:26:56,147] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 214500, global step 3434787: learning rate 0.0000
[2019-04-04 15:26:56,609] A3C_AGENT_WORKER-Thread-6 INFO:Local step 214500, global step 3435052: loss 0.2316
[2019-04-04 15:26:56,611] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 214500, global step 3435053: learning rate 0.0000
[2019-04-04 15:26:56,784] A3C_AGENT_WORKER-Thread-19 INFO:Local step 214500, global step 3435148: loss 0.2377
[2019-04-04 15:26:56,786] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 214500, global step 3435148: learning rate 0.0000
[2019-04-04 15:26:57,033] A3C_AGENT_WORKER-Thread-11 INFO:Local step 215000, global step 3435297: loss 0.0281
[2019-04-04 15:26:57,033] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 215000, global step 3435297: learning rate 0.0000
[2019-04-04 15:27:02,811] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6547216e-09 5.4469776e-11 6.7586215e-17 1.9118708e-16 1.0000000e+00
 7.1556434e-11 1.4999806e-16], sum to 1.0000
[2019-04-04 15:27:02,812] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6566
[2019-04-04 15:27:02,857] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 92.0, 84.0, 0.0, 26.0, 26.16851316412595, 0.5894432040883885, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1432200.0000, 
sim time next is 1432800.0000, 
raw observation next is [1.1, 92.0, 81.0, 0.0, 26.0, 26.24436953935147, 0.479959429536253, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.27, 0.0, 0.6666666666666666, 0.6870307949459559, 0.6599864765120843, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8786083], dtype=float32), -0.60531056]. 
=============================================
[2019-04-04 15:27:03,361] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.40852157e-10 1.05119476e-10 1.64417949e-16 1.15259280e-14
 1.00000000e+00 4.87680563e-10 7.09384852e-16], sum to 1.0000
[2019-04-04 15:27:03,370] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6221
[2019-04-04 15:27:03,395] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.2, 93.66666666666667, 0.0, 0.0, 26.0, 25.43935503664302, 0.4597107759704133, 0.0, 1.0, 38942.79651451706], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1479000.0000, 
sim time next is 1479600.0000, 
raw observation next is [2.2, 94.0, 0.0, 0.0, 26.0, 25.38766942035376, 0.4531116519825007, 0.0, 1.0, 63667.67689627213], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.94, 0.0, 0.0, 0.6666666666666666, 0.6156391183628133, 0.6510372173275002, 0.0, 1.0, 0.30317941379177205], 
reward next is 0.6968, 
noisyNet noise sample is [array([-1.3012303], dtype=float32), 1.1345276]. 
=============================================
[2019-04-04 15:27:06,015] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2233705e-09 6.0760903e-11 7.0675672e-16 3.0322081e-15 1.0000000e+00
 3.6108987e-11 3.0269366e-16], sum to 1.0000
[2019-04-04 15:27:06,016] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6174
[2019-04-04 15:27:06,040] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.35, 90.5, 0.0, 0.0, 26.0, 24.98537246796062, 0.496165353738079, 0.0, 1.0, 128545.1489519223], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1456200.0000, 
sim time next is 1456800.0000, 
raw observation next is [1.433333333333333, 90.0, 0.0, 0.0, 26.0, 25.10493093278794, 0.5231707867439129, 0.0, 1.0, 71121.04133216532], 
processed observation next is [1.0, 0.8695652173913043, 0.502308402585411, 0.9, 0.0, 0.0, 0.6666666666666666, 0.5920775777323284, 0.674390262247971, 0.0, 1.0, 0.33867162539126344], 
reward next is 0.6613, 
noisyNet noise sample is [array([0.4876953], dtype=float32), -0.8660584]. 
=============================================
[2019-04-04 15:27:07,019] A3C_AGENT_WORKER-Thread-16 INFO:Local step 215000, global step 3440045: loss 0.0082
[2019-04-04 15:27:07,021] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 215000, global step 3440045: learning rate 0.0000
[2019-04-04 15:27:08,940] A3C_AGENT_WORKER-Thread-15 INFO:Local step 215000, global step 3441069: loss 0.0064
[2019-04-04 15:27:08,945] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 215000, global step 3441069: learning rate 0.0000
[2019-04-04 15:27:09,294] A3C_AGENT_WORKER-Thread-13 INFO:Local step 215500, global step 3441269: loss 1.8905
[2019-04-04 15:27:09,295] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 215500, global step 3441270: learning rate 0.0000
[2019-04-04 15:27:09,482] A3C_AGENT_WORKER-Thread-18 INFO:Local step 215000, global step 3441373: loss 0.0061
[2019-04-04 15:27:09,485] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 215000, global step 3441373: learning rate 0.0000
[2019-04-04 15:27:09,563] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7652160e-10 1.5077212e-10 4.0916873e-17 1.9454790e-15 1.0000000e+00
 3.7095809e-11 3.5393047e-17], sum to 1.0000
[2019-04-04 15:27:09,564] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7140
[2019-04-04 15:27:09,576] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 79.0, 0.0, 0.0, 26.0, 25.43431565053836, 0.5186322643801795, 0.0, 1.0, 127073.1866407993], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1562400.0000, 
sim time next is 1563000.0000, 
raw observation next is [4.9, 80.16666666666667, 0.0, 0.0, 26.0, 25.424915302128, 0.5362061785335087, 0.0, 1.0, 82056.19092323219], 
processed observation next is [1.0, 0.08695652173913043, 0.5983379501385043, 0.8016666666666667, 0.0, 0.0, 0.6666666666666666, 0.6187429418439999, 0.6787353928445029, 0.0, 1.0, 0.3907437663011057], 
reward next is 0.6093, 
noisyNet noise sample is [array([-2.518654], dtype=float32), 0.32037032]. 
=============================================
[2019-04-04 15:27:09,583] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[88.25753]
 [87.91595]
 [87.6414 ]
 [87.94895]
 [88.24763]], R is [[88.22620392]
 [87.73883057]
 [87.51837158]
 [87.59860992]
 [87.72262573]].
[2019-04-04 15:27:09,854] A3C_AGENT_WORKER-Thread-3 INFO:Local step 215000, global step 3441597: loss 0.0055
[2019-04-04 15:27:09,854] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 215000, global step 3441597: learning rate 0.0000
[2019-04-04 15:27:09,940] A3C_AGENT_WORKER-Thread-12 INFO:Local step 215000, global step 3441647: loss 0.0054
[2019-04-04 15:27:09,941] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 215000, global step 3441647: learning rate 0.0000
[2019-04-04 15:27:10,065] A3C_AGENT_WORKER-Thread-10 INFO:Local step 215000, global step 3441724: loss 0.0055
[2019-04-04 15:27:10,068] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 215000, global step 3441725: learning rate 0.0000
[2019-04-04 15:27:10,389] A3C_AGENT_WORKER-Thread-2 INFO:Local step 215500, global step 3441931: loss 1.8501
[2019-04-04 15:27:10,390] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 215500, global step 3441931: learning rate 0.0000
[2019-04-04 15:27:10,508] A3C_AGENT_WORKER-Thread-14 INFO:Local step 215000, global step 3442000: loss 0.0048
[2019-04-04 15:27:10,509] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 215000, global step 3442000: learning rate 0.0000
[2019-04-04 15:27:10,783] A3C_AGENT_WORKER-Thread-4 INFO:Local step 215000, global step 3442172: loss 0.0050
[2019-04-04 15:27:10,786] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 215000, global step 3442174: learning rate 0.0000
[2019-04-04 15:27:11,055] A3C_AGENT_WORKER-Thread-20 INFO:Local step 215000, global step 3442341: loss 0.0047
[2019-04-04 15:27:11,059] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 215000, global step 3442344: learning rate 0.0000
[2019-04-04 15:27:11,452] A3C_AGENT_WORKER-Thread-5 INFO:Local step 215000, global step 3442555: loss 0.0047
[2019-04-04 15:27:11,455] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 215000, global step 3442555: learning rate 0.0000
[2019-04-04 15:27:11,482] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4503419e-10 2.9414223e-12 5.6609272e-18 2.9101326e-16 1.0000000e+00
 5.6927124e-12 1.6219640e-17], sum to 1.0000
[2019-04-04 15:27:11,485] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4750
[2019-04-04 15:27:11,511] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.833333333333334, 63.33333333333333, 202.6666666666667, 114.8333333333333, 26.0, 26.78505112875403, 0.739586141722235, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1593600.0000, 
sim time next is 1594200.0000, 
raw observation next is [9.116666666666667, 62.16666666666666, 205.3333333333333, 141.6666666666667, 26.0, 26.83624278007356, 0.7474969011430944, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7151431209602955, 0.6216666666666666, 0.6844444444444443, 0.15653775322283614, 0.6666666666666666, 0.7363535650061301, 0.7491656337143647, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20177807], dtype=float32), 0.3373299]. 
=============================================
[2019-04-04 15:27:11,847] A3C_AGENT_WORKER-Thread-19 INFO:Local step 215000, global step 3442773: loss 0.0045
[2019-04-04 15:27:11,848] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 215000, global step 3442773: learning rate 0.0000
[2019-04-04 15:27:11,939] A3C_AGENT_WORKER-Thread-6 INFO:Local step 215000, global step 3442823: loss 0.0046
[2019-04-04 15:27:11,940] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 215000, global step 3442823: learning rate 0.0000
[2019-04-04 15:27:12,797] A3C_AGENT_WORKER-Thread-17 INFO:Local step 215500, global step 3443326: loss 1.8572
[2019-04-04 15:27:12,797] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 215500, global step 3443326: learning rate 0.0000
[2019-04-04 15:27:14,016] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.9118523e-10 1.4683324e-10 6.7941429e-16 1.4916599e-14 1.0000000e+00
 3.2760111e-11 3.2983687e-15], sum to 1.0000
[2019-04-04 15:27:14,021] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9044
[2019-04-04 15:27:14,075] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.3, 73.0, 184.0, 81.0, 26.0, 24.94890293009907, 0.2594565637685023, 0.0, 1.0, 53024.40488317595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1855800.0000, 
sim time next is 1856400.0000, 
raw observation next is [-5.199999999999999, 72.33333333333333, 173.3333333333333, 67.5, 26.0, 24.95339120619924, 0.2616909527290723, 0.0, 1.0, 46737.89643300232], 
processed observation next is [0.0, 0.4782608695652174, 0.31855955678670367, 0.7233333333333333, 0.5777777777777776, 0.07458563535911603, 0.6666666666666666, 0.57944926718327, 0.5872303175763575, 0.0, 1.0, 0.22256141158572534], 
reward next is 0.7774, 
noisyNet noise sample is [array([2.6849074], dtype=float32), 0.56332266]. 
=============================================
[2019-04-04 15:27:14,391] A3C_AGENT_WORKER-Thread-11 INFO:Local step 215500, global step 3444196: loss 1.8533
[2019-04-04 15:27:14,393] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 215500, global step 3444196: learning rate 0.0000
[2019-04-04 15:27:21,079] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3978022e-10 8.6086971e-11 1.5816323e-16 1.4743799e-14 1.0000000e+00
 2.3268401e-11 2.4286330e-16], sum to 1.0000
[2019-04-04 15:27:21,080] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9498
[2019-04-04 15:27:21,111] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 86.33333333333333, 0.0, 0.0, 26.0, 25.07678731633122, 0.3891389999760537, 0.0, 1.0, 43279.59381557771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1743000.0000, 
sim time next is 1743600.0000, 
raw observation next is [-0.6, 85.66666666666667, 0.0, 0.0, 26.0, 25.05453702888274, 0.3846517031207497, 0.0, 1.0, 43325.28732294629], 
processed observation next is [0.0, 0.17391304347826086, 0.44598337950138506, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5878780857402285, 0.6282172343735832, 0.0, 1.0, 0.20631089201402997], 
reward next is 0.7937, 
noisyNet noise sample is [array([-1.9131573], dtype=float32), 0.7409322]. 
=============================================
[2019-04-04 15:27:21,985] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4798366e-10 6.4413475e-11 7.8513358e-16 3.6498197e-15 1.0000000e+00
 2.4405082e-11 4.2274072e-16], sum to 1.0000
[2019-04-04 15:27:21,985] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5870
[2019-04-04 15:27:22,038] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 86.33333333333333, 111.6666666666667, 0.0, 26.0, 24.89496022366657, 0.3414907312035306, 0.0, 1.0, 55560.83401214974], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1768200.0000, 
sim time next is 1768800.0000, 
raw observation next is [-2.3, 85.66666666666667, 115.3333333333333, 0.0, 26.0, 24.90857814900926, 0.3460754346192513, 0.0, 1.0, 42096.29336590804], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.8566666666666667, 0.3844444444444443, 0.0, 0.6666666666666666, 0.5757148457507716, 0.6153584782064171, 0.0, 1.0, 0.20045853983765732], 
reward next is 0.7995, 
noisyNet noise sample is [array([0.83884627], dtype=float32), 0.44024715]. 
=============================================
[2019-04-04 15:27:24,565] A3C_AGENT_WORKER-Thread-16 INFO:Local step 215500, global step 3448127: loss 1.7749
[2019-04-04 15:27:24,567] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 215500, global step 3448127: learning rate 0.0000
[2019-04-04 15:27:26,329] A3C_AGENT_WORKER-Thread-15 INFO:Local step 215500, global step 3448880: loss 1.7601
[2019-04-04 15:27:26,331] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 215500, global step 3448880: learning rate 0.0000
[2019-04-04 15:27:26,744] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0617789e-08 1.1904585e-09 1.4611132e-14 1.5587541e-13 1.0000000e+00
 6.9715661e-10 3.9222775e-15], sum to 1.0000
[2019-04-04 15:27:26,744] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4660
[2019-04-04 15:27:26,765] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 85.66666666666667, 0.0, 0.0, 26.0, 23.91674208160165, 0.0651271927060629, 0.0, 1.0, 46825.69917471828], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1826400.0000, 
sim time next is 1827000.0000, 
raw observation next is [-6.2, 85.0, 0.0, 0.0, 26.0, 23.95955959692031, 0.0611680790502343, 0.0, 1.0, 46830.46814311864], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.85, 0.0, 0.0, 0.6666666666666666, 0.49662996641002594, 0.5203893596834114, 0.0, 1.0, 0.2230022292529459], 
reward next is 0.7770, 
noisyNet noise sample is [array([0.14173654], dtype=float32), -1.0240154]. 
=============================================
[2019-04-04 15:27:26,772] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[77.163574]
 [77.23942 ]
 [77.23691 ]
 [77.24642 ]
 [77.38806 ]], R is [[77.10916138]
 [77.11509705]
 [77.1211319 ]
 [77.12734985]
 [77.1337738 ]].
[2019-04-04 15:27:27,134] A3C_AGENT_WORKER-Thread-18 INFO:Local step 215500, global step 3449194: loss 1.7636
[2019-04-04 15:27:27,134] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 215500, global step 3449194: learning rate 0.0000
[2019-04-04 15:27:27,533] A3C_AGENT_WORKER-Thread-3 INFO:Local step 215500, global step 3449329: loss 1.7468
[2019-04-04 15:27:27,534] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 215500, global step 3449329: learning rate 0.0000
[2019-04-04 15:27:27,855] A3C_AGENT_WORKER-Thread-10 INFO:Local step 215500, global step 3449428: loss 1.7288
[2019-04-04 15:27:27,856] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 215500, global step 3449428: learning rate 0.0000
[2019-04-04 15:27:28,231] A3C_AGENT_WORKER-Thread-12 INFO:Local step 215500, global step 3449522: loss 1.7633
[2019-04-04 15:27:28,231] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 215500, global step 3449522: learning rate 0.0000
[2019-04-04 15:27:28,666] A3C_AGENT_WORKER-Thread-14 INFO:Local step 215500, global step 3449633: loss 1.7458
[2019-04-04 15:27:28,667] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 215500, global step 3449633: learning rate 0.0000
[2019-04-04 15:27:28,795] A3C_AGENT_WORKER-Thread-4 INFO:Local step 215500, global step 3449669: loss 1.7401
[2019-04-04 15:27:28,796] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 215500, global step 3449669: learning rate 0.0000
[2019-04-04 15:27:29,167] A3C_AGENT_WORKER-Thread-20 INFO:Local step 215500, global step 3449761: loss 1.7459
[2019-04-04 15:27:29,167] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 215500, global step 3449761: learning rate 0.0000
[2019-04-04 15:27:29,204] A3C_AGENT_WORKER-Thread-5 INFO:Local step 215500, global step 3449775: loss 1.7599
[2019-04-04 15:27:29,204] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 215500, global step 3449775: learning rate 0.0000
[2019-04-04 15:27:29,668] A3C_AGENT_WORKER-Thread-6 INFO:Local step 215500, global step 3449909: loss 1.7621
[2019-04-04 15:27:29,669] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 215500, global step 3449909: learning rate 0.0000
[2019-04-04 15:27:29,887] A3C_AGENT_WORKER-Thread-19 INFO:Local step 215500, global step 3449976: loss 1.7346
[2019-04-04 15:27:29,888] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 215500, global step 3449976: learning rate 0.0000
[2019-04-04 15:27:31,895] A3C_AGENT_WORKER-Thread-13 INFO:Local step 216000, global step 3450752: loss 0.5571
[2019-04-04 15:27:31,896] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 216000, global step 3450752: learning rate 0.0000
[2019-04-04 15:27:33,361] A3C_AGENT_WORKER-Thread-2 INFO:Local step 216000, global step 3451364: loss 0.5469
[2019-04-04 15:27:33,365] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 216000, global step 3451366: learning rate 0.0000
[2019-04-04 15:27:34,929] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.2666144e-08 9.7364836e-09 1.1404502e-13 6.5107219e-13 1.0000000e+00
 1.1886480e-09 7.0611472e-14], sum to 1.0000
[2019-04-04 15:27:34,929] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3975
[2019-04-04 15:27:34,957] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.22938643934479, 0.07941157610157505, 0.0, 1.0, 41074.15279936989], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2006400.0000, 
sim time next is 2007000.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.24099295770268, 0.07411455525727823, 0.0, 1.0, 41065.97828037154], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5200827464752233, 0.5247048517524261, 0.0, 1.0, 0.19555227752557877], 
reward next is 0.8044, 
noisyNet noise sample is [array([-1.0376384], dtype=float32), 0.33947945]. 
=============================================
[2019-04-04 15:27:34,978] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[76.148705]
 [76.16751 ]
 [76.17832 ]
 [76.17398 ]
 [76.20066 ]], R is [[76.18809509]
 [76.23062134]
 [76.27262878]
 [76.31406403]
 [76.35506439]].
[2019-04-04 15:27:36,283] A3C_AGENT_WORKER-Thread-17 INFO:Local step 216000, global step 3452290: loss 0.5632
[2019-04-04 15:27:36,284] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 216000, global step 3452290: learning rate 0.0000
[2019-04-04 15:27:37,303] A3C_AGENT_WORKER-Thread-11 INFO:Local step 216000, global step 3452675: loss 0.5759
[2019-04-04 15:27:37,304] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 216000, global step 3452675: learning rate 0.0000
[2019-04-04 15:27:39,335] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.7209352e-09 2.0136441e-10 4.0370086e-15 2.0897448e-14 1.0000000e+00
 3.9359646e-10 5.4234855e-15], sum to 1.0000
[2019-04-04 15:27:39,336] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2376
[2019-04-04 15:27:39,382] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 26.31075564129307, 0.5353325273954085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2053200.0000, 
sim time next is 2053800.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 26.25482587338769, 0.5320916253941026, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.687902156115641, 0.6773638751313675, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0342969], dtype=float32), 0.14916745]. 
=============================================
[2019-04-04 15:27:47,561] A3C_AGENT_WORKER-Thread-16 INFO:Local step 216000, global step 3456325: loss 0.5888
[2019-04-04 15:27:47,562] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 216000, global step 3456325: learning rate 0.0000
[2019-04-04 15:27:49,123] A3C_AGENT_WORKER-Thread-15 INFO:Local step 216000, global step 3456809: loss 0.5928
[2019-04-04 15:27:49,123] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 216000, global step 3456809: learning rate 0.0000
[2019-04-04 15:27:50,407] A3C_AGENT_WORKER-Thread-18 INFO:Local step 216000, global step 3457251: loss 0.6065
[2019-04-04 15:27:50,409] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 216000, global step 3457251: learning rate 0.0000
[2019-04-04 15:27:50,449] A3C_AGENT_WORKER-Thread-3 INFO:Local step 216000, global step 3457264: loss 0.5996
[2019-04-04 15:27:50,464] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 216000, global step 3457266: learning rate 0.0000
[2019-04-04 15:27:51,093] A3C_AGENT_WORKER-Thread-10 INFO:Local step 216000, global step 3457511: loss 0.6077
[2019-04-04 15:27:51,095] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 216000, global step 3457513: learning rate 0.0000
[2019-04-04 15:27:51,366] A3C_AGENT_WORKER-Thread-12 INFO:Local step 216000, global step 3457636: loss 0.5994
[2019-04-04 15:27:51,376] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 216000, global step 3457637: learning rate 0.0000
[2019-04-04 15:27:51,474] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9927444e-09 2.6709951e-10 7.3793663e-15 6.6399285e-14 1.0000000e+00
 1.1193411e-10 1.0598920e-14], sum to 1.0000
[2019-04-04 15:27:51,476] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8722
[2019-04-04 15:27:51,482] A3C_AGENT_WORKER-Thread-5 INFO:Local step 216000, global step 3457685: loss 0.5928
[2019-04-04 15:27:51,483] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 216000, global step 3457685: learning rate 0.0000
[2019-04-04 15:27:51,488] A3C_AGENT_WORKER-Thread-4 INFO:Local step 216000, global step 3457686: loss 0.5897
[2019-04-04 15:27:51,488] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 216000, global step 3457686: learning rate 0.0000
[2019-04-04 15:27:51,521] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.383333333333333, 62.5, 148.3333333333333, 402.0, 26.0, 24.92252457983686, 0.3121841811942541, 0.0, 1.0, 32935.99882574663], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2371800.0000, 
sim time next is 2372400.0000, 
raw observation next is [-2.3, 62.0, 153.0, 378.0, 26.0, 24.97183695156756, 0.3160591988934365, 0.0, 1.0, 18740.33749278937], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.62, 0.51, 0.4176795580110497, 0.6666666666666666, 0.58098641263063, 0.6053530662978122, 0.0, 1.0, 0.08923970234661606], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.42021984], dtype=float32), -0.44350964]. 
=============================================
[2019-04-04 15:27:51,787] A3C_AGENT_WORKER-Thread-14 INFO:Local step 216000, global step 3457817: loss 0.6097
[2019-04-04 15:27:51,788] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 216000, global step 3457817: learning rate 0.0000
[2019-04-04 15:27:51,925] A3C_AGENT_WORKER-Thread-20 INFO:Local step 216000, global step 3457879: loss 0.5942
[2019-04-04 15:27:51,926] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 216000, global step 3457879: learning rate 0.0000
[2019-04-04 15:27:53,133] A3C_AGENT_WORKER-Thread-19 INFO:Local step 216000, global step 3458320: loss 0.5896
[2019-04-04 15:27:53,133] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 216000, global step 3458320: learning rate 0.0000
[2019-04-04 15:27:53,251] A3C_AGENT_WORKER-Thread-6 INFO:Local step 216000, global step 3458358: loss 0.5974
[2019-04-04 15:27:53,252] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 216000, global step 3458358: learning rate 0.0000
[2019-04-04 15:27:53,350] A3C_AGENT_WORKER-Thread-13 INFO:Local step 216500, global step 3458390: loss 0.3842
[2019-04-04 15:27:53,351] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 216500, global step 3458390: learning rate 0.0000
[2019-04-04 15:27:55,602] A3C_AGENT_WORKER-Thread-2 INFO:Local step 216500, global step 3459131: loss 0.3858
[2019-04-04 15:27:55,602] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 216500, global step 3459131: learning rate 0.0000
[2019-04-04 15:27:57,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4139209e-08 3.3268743e-09 7.8553844e-15 2.8047863e-13 1.0000000e+00
 7.5759105e-10 2.2380298e-14], sum to 1.0000
[2019-04-04 15:27:57,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4663
[2019-04-04 15:27:57,630] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.65, 89.0, 0.0, 0.0, 26.0, 23.97351515271772, 0.04354507662751069, 0.0, 1.0, 43538.50915625851], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2262600.0000, 
sim time next is 2263200.0000, 
raw observation next is [-8.733333333333334, 89.66666666666667, 0.0, 0.0, 26.0, 23.93692052592184, 0.03040749458420572, 0.0, 1.0, 43500.83864450233], 
processed observation next is [1.0, 0.17391304347826086, 0.22068328716528163, 0.8966666666666667, 0.0, 0.0, 0.6666666666666666, 0.4947433771601532, 0.5101358315280685, 0.0, 1.0, 0.20714685068810632], 
reward next is 0.7929, 
noisyNet noise sample is [array([1.2582251], dtype=float32), -0.99845296]. 
=============================================
[2019-04-04 15:27:57,749] A3C_AGENT_WORKER-Thread-17 INFO:Local step 216500, global step 3459922: loss 0.3878
[2019-04-04 15:27:57,749] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 216500, global step 3459922: learning rate 0.0000
[2019-04-04 15:27:58,606] A3C_AGENT_WORKER-Thread-11 INFO:Local step 216500, global step 3460302: loss 0.3936
[2019-04-04 15:27:58,608] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 216500, global step 3460303: learning rate 0.0000
[2019-04-04 15:28:06,310] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.4874345e-08 1.6830837e-08 2.0414925e-13 6.0612447e-13 9.9999988e-01
 1.1901184e-08 6.8891767e-13], sum to 1.0000
[2019-04-04 15:28:06,310] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0639
[2019-04-04 15:28:06,337] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.6964143628705, 0.2380238101315368, 0.0, 1.0, 38831.09888728323], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2339400.0000, 
sim time next is 2340000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.70190822272131, 0.2351848978657849, 0.0, 1.0, 38909.75849395445], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5584923518934426, 0.5783949659552616, 0.0, 1.0, 0.18528456425692594], 
reward next is 0.8147, 
noisyNet noise sample is [array([0.5355383], dtype=float32), 0.2236889]. 
=============================================
[2019-04-04 15:28:06,341] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[77.14766 ]
 [77.10772 ]
 [76.773674]
 [76.708916]
 [76.37297 ]], R is [[77.29872894]
 [77.34082794]
 [77.38303375]
 [77.42534637]
 [77.46741486]].
[2019-04-04 15:28:08,747] A3C_AGENT_WORKER-Thread-16 INFO:Local step 216500, global step 3464355: loss 0.3690
[2019-04-04 15:28:08,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 216500, global step 3464355: learning rate 0.0000
[2019-04-04 15:28:10,226] A3C_AGENT_WORKER-Thread-15 INFO:Local step 216500, global step 3465012: loss 0.3761
[2019-04-04 15:28:10,226] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 216500, global step 3465012: learning rate 0.0000
[2019-04-04 15:28:10,912] A3C_AGENT_WORKER-Thread-3 INFO:Local step 216500, global step 3465303: loss 0.3745
[2019-04-04 15:28:10,914] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 216500, global step 3465303: learning rate 0.0000
[2019-04-04 15:28:11,256] A3C_AGENT_WORKER-Thread-18 INFO:Local step 216500, global step 3465445: loss 0.3781
[2019-04-04 15:28:11,258] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 216500, global step 3465445: learning rate 0.0000
[2019-04-04 15:28:11,596] A3C_AGENT_WORKER-Thread-10 INFO:Local step 216500, global step 3465592: loss 0.3655
[2019-04-04 15:28:11,597] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 216500, global step 3465592: learning rate 0.0000
[2019-04-04 15:28:12,240] A3C_AGENT_WORKER-Thread-5 INFO:Local step 216500, global step 3465842: loss 0.3733
[2019-04-04 15:28:12,242] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 216500, global step 3465843: learning rate 0.0000
[2019-04-04 15:28:12,272] A3C_AGENT_WORKER-Thread-12 INFO:Local step 216500, global step 3465857: loss 0.3732
[2019-04-04 15:28:12,274] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 216500, global step 3465857: learning rate 0.0000
[2019-04-04 15:28:12,311] A3C_AGENT_WORKER-Thread-4 INFO:Local step 216500, global step 3465873: loss 0.3683
[2019-04-04 15:28:12,312] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 216500, global step 3465873: learning rate 0.0000
[2019-04-04 15:28:12,564] A3C_AGENT_WORKER-Thread-13 INFO:Local step 217000, global step 3465985: loss 0.0382
[2019-04-04 15:28:12,565] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 217000, global step 3465985: learning rate 0.0000
[2019-04-04 15:28:12,770] A3C_AGENT_WORKER-Thread-14 INFO:Local step 216500, global step 3466067: loss 0.3692
[2019-04-04 15:28:12,770] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 216500, global step 3466067: learning rate 0.0000
[2019-04-04 15:28:13,011] A3C_AGENT_WORKER-Thread-20 INFO:Local step 216500, global step 3466158: loss 0.3795
[2019-04-04 15:28:13,011] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 216500, global step 3466158: learning rate 0.0000
[2019-04-04 15:28:14,194] A3C_AGENT_WORKER-Thread-19 INFO:Local step 216500, global step 3466697: loss 0.3761
[2019-04-04 15:28:14,196] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 216500, global step 3466698: learning rate 0.0000
[2019-04-04 15:28:14,265] A3C_AGENT_WORKER-Thread-2 INFO:Local step 217000, global step 3466721: loss 0.0423
[2019-04-04 15:28:14,266] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 217000, global step 3466721: learning rate 0.0000
[2019-04-04 15:28:14,665] A3C_AGENT_WORKER-Thread-6 INFO:Local step 216500, global step 3466908: loss 0.3734
[2019-04-04 15:28:14,674] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 216500, global step 3466912: learning rate 0.0000
[2019-04-04 15:28:16,014] A3C_AGENT_WORKER-Thread-17 INFO:Local step 217000, global step 3467589: loss 0.0478
[2019-04-04 15:28:16,016] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 217000, global step 3467589: learning rate 0.0000
[2019-04-04 15:28:16,957] A3C_AGENT_WORKER-Thread-11 INFO:Local step 217000, global step 3468037: loss 0.0500
[2019-04-04 15:28:16,968] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 217000, global step 3468037: learning rate 0.0000
[2019-04-04 15:28:19,647] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0804989e-08 1.0294898e-09 2.2167581e-14 1.5608188e-13 1.0000000e+00
 2.6516063e-09 8.5329794e-14], sum to 1.0000
[2019-04-04 15:28:19,648] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3183
[2019-04-04 15:28:19,659] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.88616492047424, 0.2471875592370374, 0.0, 1.0, 41686.42731971607], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2597400.0000, 
sim time next is 2598000.0000, 
raw observation next is [-5.0, 72.0, 0.0, 0.0, 26.0, 24.88408758334603, 0.2457724636409285, 0.0, 1.0, 41671.28803202201], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5736739652788359, 0.5819241545469761, 0.0, 1.0, 0.1984347049143905], 
reward next is 0.8016, 
noisyNet noise sample is [array([-0.15513991], dtype=float32), 0.16246875]. 
=============================================
[2019-04-04 15:28:19,665] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.81148 ]
 [77.820404]
 [77.5909  ]
 [77.55623 ]
 [77.54829 ]], R is [[77.78242493]
 [77.80609131]
 [77.82933807]
 [77.85212708]
 [77.87451172]].
[2019-04-04 15:28:20,809] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.5348964e-11 1.6046968e-11 1.3807291e-17 3.2896467e-16 1.0000000e+00
 1.0922417e-11 2.9420657e-17], sum to 1.0000
[2019-04-04 15:28:20,809] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4191
[2019-04-04 15:28:20,854] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 100.0, 78.0, 27.0, 26.0, 25.9269910719712, 0.3547892456861725, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2908800.0000, 
sim time next is 2909400.0000, 
raw observation next is [2.0, 98.83333333333334, 75.66666666666666, 36.00000000000001, 26.0, 25.44418999476771, 0.3770537477383313, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.9883333333333334, 0.2522222222222222, 0.03977900552486189, 0.6666666666666666, 0.6203491662306426, 0.6256845825794438, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6579148], dtype=float32), -0.9436156]. 
=============================================
[2019-04-04 15:28:22,773] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.0679918e-10 3.7532012e-11 1.5032809e-15 2.4311557e-15 1.0000000e+00
 9.5619311e-11 1.3080825e-15], sum to 1.0000
[2019-04-04 15:28:22,773] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0183
[2019-04-04 15:28:22,787] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 46.33333333333334, 195.6666666666667, 155.3333333333333, 26.0, 25.26126112212711, 0.406638695169429, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2645400.0000, 
sim time next is 2646000.0000, 
raw observation next is [0.5, 47.0, 185.5, 168.0, 26.0, 25.58253824014778, 0.4385059889822889, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4764542936288089, 0.47, 0.6183333333333333, 0.1856353591160221, 0.6666666666666666, 0.6318781866789817, 0.6461686629940963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10568712], dtype=float32), 0.5052788]. 
=============================================
[2019-04-04 15:28:22,791] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[82.63928 ]
 [82.82438 ]
 [82.93635 ]
 [82.949615]
 [83.0033  ]], R is [[82.71272278]
 [82.88559723]
 [83.05673981]
 [83.2261734 ]
 [83.39391327]].
[2019-04-04 15:28:26,678] A3C_AGENT_WORKER-Thread-16 INFO:Local step 217000, global step 3472342: loss 0.0579
[2019-04-04 15:28:26,680] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 217000, global step 3472342: learning rate 0.0000
[2019-04-04 15:28:27,101] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.2767961e-09 9.7256214e-10 1.3096235e-14 1.6606592e-13 1.0000000e+00
 8.8948560e-10 1.3295723e-14], sum to 1.0000
[2019-04-04 15:28:27,101] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1051
[2019-04-04 15:28:27,117] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.683333333333334, 69.0, 0.0, 0.0, 26.0, 25.04123623029878, 0.3326039349587325, 0.0, 1.0, 45142.65919293525], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2674200.0000, 
sim time next is 2674800.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 26.0, 24.98012076868509, 0.3210031375292484, 0.0, 1.0, 44648.11770223535], 
processed observation next is [1.0, 1.0, 0.32409972299168976, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5816767307237575, 0.6070010458430828, 0.0, 1.0, 0.2126100842963588], 
reward next is 0.7874, 
noisyNet noise sample is [array([1.2417494], dtype=float32), -0.98734844]. 
=============================================
[2019-04-04 15:28:28,633] A3C_AGENT_WORKER-Thread-15 INFO:Local step 217000, global step 3473106: loss 0.0621
[2019-04-04 15:28:28,635] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 217000, global step 3473108: learning rate 0.0000
[2019-04-04 15:28:29,196] A3C_AGENT_WORKER-Thread-18 INFO:Local step 217000, global step 3473318: loss 0.0593
[2019-04-04 15:28:29,202] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 217000, global step 3473318: learning rate 0.0000
[2019-04-04 15:28:29,314] A3C_AGENT_WORKER-Thread-3 INFO:Local step 217000, global step 3473366: loss 0.0628
[2019-04-04 15:28:29,331] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 217000, global step 3473372: learning rate 0.0000
[2019-04-04 15:28:29,465] A3C_AGENT_WORKER-Thread-10 INFO:Local step 217000, global step 3473434: loss 0.0603
[2019-04-04 15:28:29,466] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 217000, global step 3473434: learning rate 0.0000
[2019-04-04 15:28:29,942] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.6000448e-08 5.6933502e-09 3.9437206e-13 2.7528418e-12 9.9999988e-01
 3.1564726e-09 2.3886137e-13], sum to 1.0000
[2019-04-04 15:28:29,945] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6805
[2019-04-04 15:28:29,958] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.64000573089351, 0.1954171651362139, 0.0, 1.0, 41150.30591389208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2775000.0000, 
sim time next is 2775600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.5892105258057, 0.1877015475127087, 0.0, 1.0, 41051.50370584571], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5491008771504751, 0.5625671825042362, 0.0, 1.0, 0.19548335098021766], 
reward next is 0.8045, 
noisyNet noise sample is [array([-0.01277328], dtype=float32), -1.1429985]. 
=============================================
[2019-04-04 15:28:30,127] A3C_AGENT_WORKER-Thread-5 INFO:Local step 217000, global step 3473715: loss 0.0576
[2019-04-04 15:28:30,130] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 217000, global step 3473716: learning rate 0.0000
[2019-04-04 15:28:30,438] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4936039e-09 6.7305281e-09 1.8123763e-15 1.6730000e-14 1.0000000e+00
 4.2544532e-10 7.2313930e-15], sum to 1.0000
[2019-04-04 15:28:30,439] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0879
[2019-04-04 15:28:30,468] A3C_AGENT_WORKER-Thread-12 INFO:Local step 217000, global step 3473854: loss 0.0576
[2019-04-04 15:28:30,469] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 217000, global step 3473854: learning rate 0.0000
[2019-04-04 15:28:30,499] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666666, 55.66666666666667, 100.1666666666667, 655.6666666666667, 26.0, 25.31960585309372, 0.3236922641467277, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3058800.0000, 
sim time next is 3059400.0000, 
raw observation next is [-4.333333333333333, 54.83333333333333, 101.3333333333333, 676.3333333333333, 26.0, 25.2835434939427, 0.3206013499304718, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.342566943674977, 0.5483333333333333, 0.3377777777777777, 0.747329650092081, 0.6666666666666666, 0.6069619578285584, 0.6068671166434906, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5247158], dtype=float32), 1.1411904]. 
=============================================
[2019-04-04 15:28:30,571] A3C_AGENT_WORKER-Thread-13 INFO:Local step 217500, global step 3473896: loss 0.0503
[2019-04-04 15:28:30,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 217500, global step 3473896: learning rate 0.0000
[2019-04-04 15:28:30,686] A3C_AGENT_WORKER-Thread-4 INFO:Local step 217000, global step 3473944: loss 0.0586
[2019-04-04 15:28:30,687] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 217000, global step 3473944: learning rate 0.0000
[2019-04-04 15:28:30,889] A3C_AGENT_WORKER-Thread-14 INFO:Local step 217000, global step 3474023: loss 0.0566
[2019-04-04 15:28:30,890] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 217000, global step 3474023: learning rate 0.0000
[2019-04-04 15:28:31,537] A3C_AGENT_WORKER-Thread-20 INFO:Local step 217000, global step 3474316: loss 0.0588
[2019-04-04 15:28:31,538] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 217000, global step 3474316: learning rate 0.0000
[2019-04-04 15:28:32,308] A3C_AGENT_WORKER-Thread-2 INFO:Local step 217500, global step 3474637: loss 0.0517
[2019-04-04 15:28:32,309] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 217500, global step 3474637: learning rate 0.0000
[2019-04-04 15:28:32,742] A3C_AGENT_WORKER-Thread-19 INFO:Local step 217000, global step 3474834: loss 0.0583
[2019-04-04 15:28:32,743] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 217000, global step 3474834: learning rate 0.0000
[2019-04-04 15:28:32,961] A3C_AGENT_WORKER-Thread-6 INFO:Local step 217000, global step 3474944: loss 0.0564
[2019-04-04 15:28:32,962] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 217000, global step 3474944: learning rate 0.0000
[2019-04-04 15:28:34,011] A3C_AGENT_WORKER-Thread-17 INFO:Local step 217500, global step 3475440: loss 0.0505
[2019-04-04 15:28:34,012] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 217500, global step 3475440: learning rate 0.0000
[2019-04-04 15:28:35,024] A3C_AGENT_WORKER-Thread-11 INFO:Local step 217500, global step 3475891: loss 0.0486
[2019-04-04 15:28:35,025] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 217500, global step 3475891: learning rate 0.0000
[2019-04-04 15:28:44,638] A3C_AGENT_WORKER-Thread-16 INFO:Local step 217500, global step 3480393: loss 0.0508
[2019-04-04 15:28:44,638] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 217500, global step 3480393: learning rate 0.0000
[2019-04-04 15:28:44,694] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9650395e-09 8.2487395e-10 3.6531786e-14 1.9730899e-13 1.0000000e+00
 1.6968045e-09 6.6137898e-14], sum to 1.0000
[2019-04-04 15:28:44,694] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1620
[2019-04-04 15:28:44,733] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 101.0, 751.0, 26.0, 25.18899780847928, 0.4170951695548855, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2989800.0000, 
sim time next is 2990400.0000, 
raw observation next is [-2.0, 60.0, 98.0, 737.0, 26.0, 25.17803156725109, 0.4086203844533193, 0.0, 1.0, 18706.83305864686], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6, 0.32666666666666666, 0.8143646408839779, 0.6666666666666666, 0.5981692972709242, 0.6362067948177731, 0.0, 1.0, 0.0890801574221279], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.2426515], dtype=float32), 0.9138791]. 
=============================================
[2019-04-04 15:28:45,223] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.7750609e-09 5.1688254e-10 3.0173232e-15 2.9807845e-14 1.0000000e+00
 9.3919685e-11 4.2807173e-15], sum to 1.0000
[2019-04-04 15:28:45,223] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5636
[2019-04-04 15:28:45,259] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666666, 69.0, 0.0, 0.0, 26.0, 24.70775431550354, 0.2154662424468749, 0.0, 1.0, 37866.85600501832], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3026400.0000, 
sim time next is 3027000.0000, 
raw observation next is [-4.833333333333334, 70.0, 0.0, 0.0, 26.0, 24.66737735589949, 0.2066125073129986, 0.0, 1.0, 37899.45947248332], 
processed observation next is [0.0, 0.0, 0.32871652816251157, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5556147796582908, 0.5688708357709995, 0.0, 1.0, 0.18047361653563487], 
reward next is 0.8195, 
noisyNet noise sample is [array([-0.204549], dtype=float32), -0.28737646]. 
=============================================
[2019-04-04 15:28:45,271] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.00771]
 [77.89686]
 [77.84452]
 [77.76551]
 [77.86929]], R is [[78.12734222]
 [78.1657486 ]
 [78.20383453]
 [78.24149323]
 [78.27864075]].
[2019-04-04 15:28:45,752] A3C_AGENT_WORKER-Thread-13 INFO:Local step 218000, global step 3480877: loss 0.4668
[2019-04-04 15:28:45,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 218000, global step 3480877: learning rate 0.0000
[2019-04-04 15:28:46,554] A3C_AGENT_WORKER-Thread-15 INFO:Local step 217500, global step 3481289: loss 0.0528
[2019-04-04 15:28:46,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 217500, global step 3481291: learning rate 0.0000
[2019-04-04 15:28:47,007] A3C_AGENT_WORKER-Thread-18 INFO:Local step 217500, global step 3481495: loss 0.0516
[2019-04-04 15:28:47,012] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 217500, global step 3481495: learning rate 0.0000
[2019-04-04 15:28:47,104] A3C_AGENT_WORKER-Thread-3 INFO:Local step 217500, global step 3481539: loss 0.0506
[2019-04-04 15:28:47,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 217500, global step 3481540: learning rate 0.0000
[2019-04-04 15:28:47,250] A3C_AGENT_WORKER-Thread-10 INFO:Local step 217500, global step 3481603: loss 0.0525
[2019-04-04 15:28:47,251] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 217500, global step 3481603: learning rate 0.0000
[2019-04-04 15:28:47,441] A3C_AGENT_WORKER-Thread-2 INFO:Local step 218000, global step 3481677: loss 0.4572
[2019-04-04 15:28:47,442] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 218000, global step 3481678: learning rate 0.0000
[2019-04-04 15:28:48,256] A3C_AGENT_WORKER-Thread-5 INFO:Local step 217500, global step 3482021: loss 0.0518
[2019-04-04 15:28:48,259] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 217500, global step 3482024: learning rate 0.0000
[2019-04-04 15:28:48,439] A3C_AGENT_WORKER-Thread-12 INFO:Local step 217500, global step 3482114: loss 0.0535
[2019-04-04 15:28:48,441] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 217500, global step 3482115: learning rate 0.0000
[2019-04-04 15:28:48,468] A3C_AGENT_WORKER-Thread-4 INFO:Local step 217500, global step 3482130: loss 0.0550
[2019-04-04 15:28:48,470] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 217500, global step 3482130: learning rate 0.0000
[2019-04-04 15:28:48,763] A3C_AGENT_WORKER-Thread-14 INFO:Local step 217500, global step 3482272: loss 0.0536
[2019-04-04 15:28:48,764] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 217500, global step 3482272: learning rate 0.0000
[2019-04-04 15:28:49,176] A3C_AGENT_WORKER-Thread-20 INFO:Local step 217500, global step 3482450: loss 0.0550
[2019-04-04 15:28:49,181] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 217500, global step 3482452: learning rate 0.0000
[2019-04-04 15:28:49,461] A3C_AGENT_WORKER-Thread-17 INFO:Local step 218000, global step 3482579: loss 0.4840
[2019-04-04 15:28:49,462] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 218000, global step 3482579: learning rate 0.0000
[2019-04-04 15:28:50,145] A3C_AGENT_WORKER-Thread-11 INFO:Local step 218000, global step 3482962: loss 0.4846
[2019-04-04 15:28:50,148] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 218000, global step 3482963: learning rate 0.0000
[2019-04-04 15:28:50,528] A3C_AGENT_WORKER-Thread-6 INFO:Local step 217500, global step 3483161: loss 0.0550
[2019-04-04 15:28:50,528] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 217500, global step 3483161: learning rate 0.0000
[2019-04-04 15:28:50,712] A3C_AGENT_WORKER-Thread-19 INFO:Local step 217500, global step 3483256: loss 0.0542
[2019-04-04 15:28:50,715] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 217500, global step 3483257: learning rate 0.0000
[2019-04-04 15:28:52,675] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2162349e-09 4.8496646e-10 1.4704642e-15 4.6579799e-15 1.0000000e+00
 4.1252332e-10 4.9839376e-15], sum to 1.0000
[2019-04-04 15:28:52,676] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7111
[2019-04-04 15:28:52,716] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 26.0, 24.99307021090812, 0.2783594962014384, 0.0, 1.0, 37392.48279228785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3093000.0000, 
sim time next is 3093600.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.02024484446084, 0.2914873897931139, 0.0, 1.0, 199722.1106649562], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.6666666666666666, 0.58502040370507, 0.5971624632643713, 0.0, 1.0, 0.9510576698331248], 
reward next is 0.0489, 
noisyNet noise sample is [array([1.4031025], dtype=float32), -0.0669415]. 
=============================================
[2019-04-04 15:28:52,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2246648e-09 9.7680766e-11 1.3304664e-16 6.2891214e-15 1.0000000e+00
 5.2857493e-11 3.4662719e-16], sum to 1.0000
[2019-04-04 15:28:52,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8816
[2019-04-04 15:28:52,752] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 25.35530896791559, 0.3151236808116454, 0.0, 1.0, 57221.1679832942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3133800.0000, 
sim time next is 3134400.0000, 
raw observation next is [5.333333333333334, 100.0, 0.0, 0.0, 26.0, 25.36794116826162, 0.3223340259649949, 0.0, 1.0, 53446.1610994169], 
processed observation next is [1.0, 0.2608695652173913, 0.6103416435826409, 1.0, 0.0, 0.0, 0.6666666666666666, 0.613995097355135, 0.607444675321665, 0.0, 1.0, 0.2545055290448424], 
reward next is 0.7455, 
noisyNet noise sample is [array([0.07012343], dtype=float32), 0.70615005]. 
=============================================
[2019-04-04 15:28:59,049] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.9094760e-09 2.9968550e-10 3.0340707e-15 4.1285619e-14 1.0000000e+00
 6.6312095e-10 4.6802259e-15], sum to 1.0000
[2019-04-04 15:28:59,052] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0880
[2019-04-04 15:28:59,088] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 76.0, 0.0, 0.0, 26.0, 25.44307187369419, 0.5609389308792617, 0.0, 1.0, 134383.1686525218], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3270600.0000, 
sim time next is 3271200.0000, 
raw observation next is [-4.666666666666666, 77.66666666666667, 0.0, 0.0, 26.0, 25.43694424102224, 0.57649970050355, 0.0, 1.0, 81517.70764344954], 
processed observation next is [1.0, 0.8695652173913043, 0.33333333333333337, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6197453534185199, 0.6921665668345166, 0.0, 1.0, 0.3881795602069026], 
reward next is 0.6118, 
noisyNet noise sample is [array([-0.3149893], dtype=float32), -1.7699689]. 
=============================================
[2019-04-04 15:28:59,833] A3C_AGENT_WORKER-Thread-16 INFO:Local step 218000, global step 3488202: loss 0.4145
[2019-04-04 15:28:59,834] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 218000, global step 3488203: learning rate 0.0000
[2019-04-04 15:29:00,745] A3C_AGENT_WORKER-Thread-13 INFO:Local step 218500, global step 3488628: loss 5.5297
[2019-04-04 15:29:00,745] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 218500, global step 3488628: learning rate 0.0000
[2019-04-04 15:29:01,896] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2397215e-09 2.2747801e-09 3.4739267e-14 1.3814142e-13 1.0000000e+00
 2.5203737e-09 2.0742775e-14], sum to 1.0000
[2019-04-04 15:29:01,897] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9810
[2019-04-04 15:29:01,910] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 50.0, 35.5, 317.0, 26.0, 26.26841581263401, 0.6271015688447517, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3344400.0000, 
sim time next is 3345000.0000, 
raw observation next is [-2.166666666666667, 50.83333333333334, 27.33333333333333, 255.6666666666666, 26.0, 26.42800879593095, 0.478255310578146, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4025854108956602, 0.5083333333333334, 0.0911111111111111, 0.2825046040515653, 0.6666666666666666, 0.7023340663275791, 0.659418436859382, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08324662], dtype=float32), 2.3458195]. 
=============================================
[2019-04-04 15:29:01,918] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.84915]
 [79.25602]
 [79.67202]
 [79.97215]
 [80.10619]], R is [[78.60327148]
 [78.81723785]
 [79.02906799]
 [79.23877716]
 [79.44638824]].
[2019-04-04 15:29:02,150] A3C_AGENT_WORKER-Thread-15 INFO:Local step 218000, global step 3489250: loss 0.4188
[2019-04-04 15:29:02,152] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 218000, global step 3489251: learning rate 0.0000
[2019-04-04 15:29:02,289] A3C_AGENT_WORKER-Thread-2 INFO:Local step 218500, global step 3489309: loss 5.5199
[2019-04-04 15:29:02,292] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 218500, global step 3489309: learning rate 0.0000
[2019-04-04 15:29:02,815] A3C_AGENT_WORKER-Thread-18 INFO:Local step 218000, global step 3489595: loss 0.4240
[2019-04-04 15:29:02,815] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 218000, global step 3489595: learning rate 0.0000
[2019-04-04 15:29:02,902] A3C_AGENT_WORKER-Thread-3 INFO:Local step 218000, global step 3489650: loss 0.4231
[2019-04-04 15:29:02,903] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 218000, global step 3489650: learning rate 0.0000
[2019-04-04 15:29:03,127] A3C_AGENT_WORKER-Thread-10 INFO:Local step 218000, global step 3489783: loss 0.4286
[2019-04-04 15:29:03,133] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 218000, global step 3489785: learning rate 0.0000
[2019-04-04 15:29:04,059] A3C_AGENT_WORKER-Thread-5 INFO:Local step 218000, global step 3490279: loss 0.4319
[2019-04-04 15:29:04,059] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 218000, global step 3490280: learning rate 0.0000
[2019-04-04 15:29:04,173] A3C_AGENT_WORKER-Thread-17 INFO:Local step 218500, global step 3490337: loss 5.5328
[2019-04-04 15:29:04,175] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 218500, global step 3490339: learning rate 0.0000
[2019-04-04 15:29:04,229] A3C_AGENT_WORKER-Thread-12 INFO:Local step 218000, global step 3490370: loss 0.4335
[2019-04-04 15:29:04,234] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 218000, global step 3490373: learning rate 0.0000
[2019-04-04 15:29:04,427] A3C_AGENT_WORKER-Thread-4 INFO:Local step 218000, global step 3490468: loss 0.4260
[2019-04-04 15:29:04,428] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 218000, global step 3490468: learning rate 0.0000
[2019-04-04 15:29:04,705] A3C_AGENT_WORKER-Thread-14 INFO:Local step 218000, global step 3490616: loss 0.4282
[2019-04-04 15:29:04,706] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 218000, global step 3490617: learning rate 0.0000
[2019-04-04 15:29:05,134] A3C_AGENT_WORKER-Thread-20 INFO:Local step 218000, global step 3490869: loss 0.4513
[2019-04-04 15:29:05,136] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 218000, global step 3490869: learning rate 0.0000
[2019-04-04 15:29:05,406] A3C_AGENT_WORKER-Thread-11 INFO:Local step 218500, global step 3491030: loss 5.5339
[2019-04-04 15:29:05,409] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 218500, global step 3491032: learning rate 0.0000
[2019-04-04 15:29:06,269] A3C_AGENT_WORKER-Thread-6 INFO:Local step 218000, global step 3491488: loss 0.4512
[2019-04-04 15:29:06,270] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 218000, global step 3491488: learning rate 0.0000
[2019-04-04 15:29:06,440] A3C_AGENT_WORKER-Thread-19 INFO:Local step 218000, global step 3491577: loss 0.4487
[2019-04-04 15:29:06,449] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 218000, global step 3491578: learning rate 0.0000
[2019-04-04 15:29:09,204] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9912659e-09 6.7615985e-10 3.5813972e-15 5.6736937e-14 1.0000000e+00
 3.5238448e-10 2.8655302e-15], sum to 1.0000
[2019-04-04 15:29:09,212] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7804
[2019-04-04 15:29:09,239] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 73.16666666666666, 0.0, 0.0, 26.0, 25.36881116804027, 0.4224793847500642, 0.0, 1.0, 55388.16404916532], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3466200.0000, 
sim time next is 3466800.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 26.0, 25.34175245072054, 0.4290178176126845, 0.0, 1.0, 47807.80880863599], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6118127042267117, 0.6430059392042281, 0.0, 1.0, 0.22765623242207617], 
reward next is 0.7723, 
noisyNet noise sample is [array([0.2422751], dtype=float32), -0.9699002]. 
=============================================
[2019-04-04 15:29:10,224] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2858829e-08 9.5627206e-10 2.9051871e-15 6.9213109e-14 1.0000000e+00
 3.4040770e-11 3.5947151e-15], sum to 1.0000
[2019-04-04 15:29:10,230] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5969
[2019-04-04 15:29:10,258] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333334, 70.33333333333334, 0.0, 0.0, 26.0, 25.25656293182383, 0.3938995469375655, 0.0, 1.0, 43644.0217284845], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3472800.0000, 
sim time next is 3473400.0000, 
raw observation next is [0.1666666666666666, 71.16666666666666, 0.0, 0.0, 26.0, 25.24866979153211, 0.3883509656596969, 0.0, 1.0, 42259.67274439875], 
processed observation next is [1.0, 0.17391304347826086, 0.4672206832871654, 0.7116666666666666, 0.0, 0.0, 0.6666666666666666, 0.6040558159610091, 0.6294503218865656, 0.0, 1.0, 0.20123653687808926], 
reward next is 0.7988, 
noisyNet noise sample is [array([-1.1636608], dtype=float32), 0.29217193]. 
=============================================
[2019-04-04 15:29:12,037] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2253381e-09 1.6302260e-10 1.1243345e-15 6.5122925e-15 1.0000000e+00
 5.2111513e-11 4.4017144e-16], sum to 1.0000
[2019-04-04 15:29:12,039] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4398
[2019-04-04 15:29:12,045] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 49.0, 106.3333333333333, 789.3333333333334, 26.0, 26.47720195108253, 0.6855082114605161, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3507000.0000, 
sim time next is 3507600.0000, 
raw observation next is [3.0, 49.0, 104.1666666666667, 785.1666666666667, 26.0, 26.54188448233499, 0.700323974793867, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.3472222222222223, 0.8675874769797423, 0.6666666666666666, 0.7118237068612491, 0.733441324931289, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9290786], dtype=float32), 1.3203024]. 
=============================================
[2019-04-04 15:29:12,267] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4970707e-09 3.1577602e-10 4.0245695e-15 2.3274490e-14 1.0000000e+00
 3.4556016e-10 3.6967465e-15], sum to 1.0000
[2019-04-04 15:29:12,271] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3901
[2019-04-04 15:29:12,328] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.333333333333334, 70.0, 90.0, 466.8333333333333, 26.0, 25.33802948583342, 0.4853752696487649, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3573600.0000, 
sim time next is 3574200.0000, 
raw observation next is [-6.166666666666666, 70.0, 92.0, 508.6666666666666, 26.0, 25.73419971231591, 0.5156988183070189, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.29178208679593726, 0.7, 0.30666666666666664, 0.5620626151012891, 0.6666666666666666, 0.6445166426929925, 0.6718996061023397, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.95259744], dtype=float32), -0.046854228]. 
=============================================
[2019-04-04 15:29:14,539] A3C_AGENT_WORKER-Thread-16 INFO:Local step 218500, global step 3495923: loss 5.5802
[2019-04-04 15:29:14,540] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 218500, global step 3495923: learning rate 0.0000
[2019-04-04 15:29:15,428] A3C_AGENT_WORKER-Thread-13 INFO:Local step 219000, global step 3496339: loss 3.6965
[2019-04-04 15:29:15,430] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 219000, global step 3496340: learning rate 0.0000
[2019-04-04 15:29:16,790] A3C_AGENT_WORKER-Thread-15 INFO:Local step 218500, global step 3497046: loss 5.5556
[2019-04-04 15:29:16,791] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 218500, global step 3497046: learning rate 0.0000
[2019-04-04 15:29:16,917] A3C_AGENT_WORKER-Thread-2 INFO:Local step 219000, global step 3497111: loss 4.2214
[2019-04-04 15:29:16,931] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 219000, global step 3497113: learning rate 0.0000
[2019-04-04 15:29:17,375] A3C_AGENT_WORKER-Thread-3 INFO:Local step 218500, global step 3497338: loss 5.5550
[2019-04-04 15:29:17,377] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 218500, global step 3497338: learning rate 0.0000
[2019-04-04 15:29:17,605] A3C_AGENT_WORKER-Thread-18 INFO:Local step 218500, global step 3497450: loss 5.5492
[2019-04-04 15:29:17,607] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 218500, global step 3497452: learning rate 0.0000
[2019-04-04 15:29:18,309] A3C_AGENT_WORKER-Thread-10 INFO:Local step 218500, global step 3497827: loss 5.5238
[2019-04-04 15:29:18,310] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 218500, global step 3497828: learning rate 0.0000
[2019-04-04 15:29:18,770] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.4076849e-10 6.0305538e-11 2.9009333e-16 1.1928551e-15 1.0000000e+00
 3.5229087e-11 1.1197327e-16], sum to 1.0000
[2019-04-04 15:29:18,771] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9556
[2019-04-04 15:29:18,794] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.5, 77.0, 100.0, 675.0, 26.0, 26.12970602117904, 0.4860616533691502, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3749400.0000, 
sim time next is 3750000.0000, 
raw observation next is [-3.333333333333333, 77.0, 101.8333333333333, 690.6666666666666, 26.0, 26.15391764592909, 0.4914710005667089, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.37026777469990774, 0.77, 0.3394444444444443, 0.7631675874769797, 0.6666666666666666, 0.6794931371607577, 0.6638236668555696, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33584177], dtype=float32), 1.8982818]. 
=============================================
[2019-04-04 15:29:18,832] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[88.77251]
 [88.62911]
 [88.42944]
 [88.15313]
 [87.58207]], R is [[88.95458984]
 [89.06504822]
 [89.17440033]
 [89.28265381]
 [89.29886627]].
[2019-04-04 15:29:18,916] A3C_AGENT_WORKER-Thread-17 INFO:Local step 219000, global step 3498180: loss 4.2715
[2019-04-04 15:29:18,918] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 219000, global step 3498181: learning rate 0.0000
[2019-04-04 15:29:18,966] A3C_AGENT_WORKER-Thread-5 INFO:Local step 218500, global step 3498206: loss 5.5262
[2019-04-04 15:29:18,966] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 218500, global step 3498206: learning rate 0.0000
[2019-04-04 15:29:19,464] A3C_AGENT_WORKER-Thread-12 INFO:Local step 218500, global step 3498489: loss 5.5195
[2019-04-04 15:29:19,465] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 218500, global step 3498489: learning rate 0.0000
[2019-04-04 15:29:19,558] A3C_AGENT_WORKER-Thread-4 INFO:Local step 218500, global step 3498553: loss 5.4992
[2019-04-04 15:29:19,560] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 218500, global step 3498554: learning rate 0.0000
[2019-04-04 15:29:19,686] A3C_AGENT_WORKER-Thread-14 INFO:Local step 218500, global step 3498620: loss 5.4994
[2019-04-04 15:29:19,690] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 218500, global step 3498621: learning rate 0.0000
[2019-04-04 15:29:20,189] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2034976e-09 3.9780559e-10 1.0438182e-14 4.5026405e-14 1.0000000e+00
 6.7522640e-11 6.1373441e-15], sum to 1.0000
[2019-04-04 15:29:20,197] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0734
[2019-04-04 15:29:20,217] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.61679730150756, 0.396464021803745, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3630000.0000, 
sim time next is 3630600.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.62636124295758, 0.3908884888868768, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.7119113573407203, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6355301035797982, 0.6302961629622922, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0814973], dtype=float32), 1.0510172]. 
=============================================
[2019-04-04 15:29:20,353] A3C_AGENT_WORKER-Thread-11 INFO:Local step 219000, global step 3499007: loss 3.7968
[2019-04-04 15:29:20,355] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 219000, global step 3499008: learning rate 0.0000
[2019-04-04 15:29:20,387] A3C_AGENT_WORKER-Thread-20 INFO:Local step 218500, global step 3499027: loss 5.5114
[2019-04-04 15:29:20,389] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 218500, global step 3499028: learning rate 0.0000
[2019-04-04 15:29:21,514] A3C_AGENT_WORKER-Thread-6 INFO:Local step 218500, global step 3499672: loss 5.4989
[2019-04-04 15:29:21,515] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 218500, global step 3499672: learning rate 0.0000
[2019-04-04 15:29:21,695] A3C_AGENT_WORKER-Thread-19 INFO:Local step 218500, global step 3499776: loss 5.5050
[2019-04-04 15:29:21,696] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 218500, global step 3499776: learning rate 0.0000
[2019-04-04 15:29:22,093] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 15:29:22,095] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 15:29:22,096] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:29:22,096] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 15:29:22,096] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 15:29:22,096] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:29:22,097] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:29:22,622] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run36
[2019-04-04 15:29:22,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run36
[2019-04-04 15:29:22,798] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run36
[2019-04-04 15:29:56,653] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18017897], dtype=float32), 0.22681014]
[2019-04-04 15:29:56,653] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [9.5436771925, 88.22390210666667, 0.0, 0.0, 26.0, 25.36191595917827, 0.4121729587672842, 0.0, 1.0, 37442.74491374684]
[2019-04-04 15:29:56,653] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 15:29:56,654] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.8650444e-10 4.9906065e-11 5.8980565e-17 1.5831543e-15 1.0000000e+00
 9.8145554e-12 1.2071503e-16], sampled 0.17678792992391312
[2019-04-04 15:30:17,903] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18017897], dtype=float32), 0.22681014]
[2019-04-04 15:30:17,903] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.8255883665, 82.294396165, 0.0, 0.0, 26.0, 24.72070409011163, 0.2112603902765303, 0.0, 1.0, 42507.93873930316]
[2019-04-04 15:30:17,903] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 15:30:17,904] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.3593141e-09 1.7492041e-10 2.3348554e-15 2.2448359e-14 1.0000000e+00
 1.3062658e-10 3.6340373e-15], sampled 0.9181912316913405
[2019-04-04 15:30:44,000] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18017897], dtype=float32), 0.22681014]
[2019-04-04 15:30:44,000] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.105036730666666, 88.93762951666666, 13.26793025333333, 173.9137984333333, 26.0, 24.96913603151006, 0.281498858172559, 1.0, 1.0, 52253.72428742518]
[2019-04-04 15:30:44,000] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 15:30:44,001] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.11904549e-10 1.91597155e-11 5.06168427e-17 8.94269857e-16
 1.00000000e+00 1.16442515e-11 8.49200227e-17], sampled 0.8502375466625407
[2019-04-04 15:31:05,081] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 15:31:11,124] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18017897], dtype=float32), 0.22681014]
[2019-04-04 15:31:11,125] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.5324475620000001, 45.24019754833333, 0.0, 120.3925430666667, 26.0, 25.11866966756482, 0.3460227622715324, 0.0, 1.0, 18696.06532030953]
[2019-04-04 15:31:11,125] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 15:31:11,126] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.1963618e-09 1.3065728e-09 2.8657178e-14 1.6217012e-13 1.0000000e+00
 8.0893792e-10 3.6098738e-14], sampled 0.31385248665418963
[2019-04-04 15:31:19,032] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18017897], dtype=float32), 0.22681014]
[2019-04-04 15:31:19,033] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.0, 71.0, 174.0, 421.0, 26.0, 25.61901917575493, 0.4615212266918094, 0.0, 1.0, 0.0]
[2019-04-04 15:31:19,033] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 15:31:19,034] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.0134982e-09 1.1947207e-10 2.8562417e-16 5.0662870e-15 1.0000000e+00
 2.6364742e-11 8.9575153e-16], sampled 0.16035893701169868
[2019-04-04 15:31:24,149] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 15:31:26,568] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 15:31:27,592] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 3500000, evaluation results [3500000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 15:31:28,153] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.0973186e-09 1.5330220e-10 3.6762992e-15 2.3900984e-14 1.0000000e+00
 3.5948819e-10 6.4868753e-16], sum to 1.0000
[2019-04-04 15:31:28,156] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5374
[2019-04-04 15:31:28,168] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.0, 79.33333333333334, 653.6666666666667, 26.0, 26.83458705101263, 0.7063634746883555, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3772200.0000, 
sim time next is 3772800.0000, 
raw observation next is [0.0, 60.0, 75.5, 625.0, 26.0, 26.87353256067672, 0.7121993989508891, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6, 0.25166666666666665, 0.6906077348066298, 0.6666666666666666, 0.7394610467230599, 0.7373997996502965, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2772881], dtype=float32), 0.6230259]. 
=============================================
[2019-04-04 15:31:29,113] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.22143112e-09 1.49831536e-10 1.53541186e-15 1.17242845e-14
 1.00000000e+00 3.26540600e-11 1.96037877e-15], sum to 1.0000
[2019-04-04 15:31:29,115] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4880
[2019-04-04 15:31:29,135] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.3683308743645, 0.3858237095324888, 0.0, 1.0, 41531.14267666452], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3715200.0000, 
sim time next is 3715800.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.37815196331037, 0.3836411183691106, 0.0, 1.0, 37394.58859380098], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6148459969425307, 0.6278803727897035, 0.0, 1.0, 0.1780694694942904], 
reward next is 0.8219, 
noisyNet noise sample is [array([1.2426622], dtype=float32), -0.7364502]. 
=============================================
[2019-04-04 15:31:29,666] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.4363876e-09 6.6485345e-10 3.4397318e-15 2.4448095e-14 1.0000000e+00
 2.3652649e-10 1.5305407e-15], sum to 1.0000
[2019-04-04 15:31:29,669] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3756
[2019-04-04 15:31:29,685] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.34337364449395, 0.3432903160621018, 0.0, 1.0, 41025.704529839], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3724800.0000, 
sim time next is 3725400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.30907238091333, 0.3380392414813653, 0.0, 1.0, 41044.95226634765], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.609089365076111, 0.6126797471604551, 0.0, 1.0, 0.19545215364927454], 
reward next is 0.8045, 
noisyNet noise sample is [array([2.5046177], dtype=float32), -0.8687783]. 
=============================================
[2019-04-04 15:31:35,108] A3C_AGENT_WORKER-Thread-16 INFO:Local step 219000, global step 3503916: loss 4.4557
[2019-04-04 15:31:35,110] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 219000, global step 3503916: learning rate 0.0000
[2019-04-04 15:31:35,983] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.1825033e-08 1.0423605e-09 1.9453502e-14 1.7456617e-13 1.0000000e+00
 4.5868456e-09 6.5127648e-15], sum to 1.0000
[2019-04-04 15:31:35,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0651
[2019-04-04 15:31:35,992] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6666666666666667, 52.5, 0.0, 0.0, 26.0, 25.12188865810777, 0.4413226117390202, 0.0, 1.0, 67474.37871416252], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3874200.0000, 
sim time next is 3874800.0000, 
raw observation next is [0.3333333333333334, 54.00000000000001, 0.0, 0.0, 26.0, 25.09450136023749, 0.4480532874518335, 0.0, 1.0, 197214.693303327], 
processed observation next is [1.0, 0.8695652173913043, 0.4718374884579871, 0.54, 0.0, 0.0, 0.6666666666666666, 0.5912084466864576, 0.6493510958172778, 0.0, 1.0, 0.9391175871587], 
reward next is 0.0609, 
noisyNet noise sample is [array([0.61165285], dtype=float32), 0.36022916]. 
=============================================
[2019-04-04 15:31:36,718] A3C_AGENT_WORKER-Thread-13 INFO:Local step 219500, global step 3504752: loss 0.0021
[2019-04-04 15:31:36,727] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 219500, global step 3504752: learning rate 0.0000
[2019-04-04 15:31:37,724] A3C_AGENT_WORKER-Thread-15 INFO:Local step 219000, global step 3505212: loss 4.4643
[2019-04-04 15:31:37,725] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 219000, global step 3505212: learning rate 0.0000
[2019-04-04 15:31:38,009] A3C_AGENT_WORKER-Thread-2 INFO:Local step 219500, global step 3505325: loss 0.0042
[2019-04-04 15:31:38,013] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 219500, global step 3505325: learning rate 0.0000
[2019-04-04 15:31:38,237] A3C_AGENT_WORKER-Thread-3 INFO:Local step 219000, global step 3505428: loss 3.9406
[2019-04-04 15:31:38,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 219000, global step 3505428: learning rate 0.0000
[2019-04-04 15:31:38,608] A3C_AGENT_WORKER-Thread-18 INFO:Local step 219000, global step 3505599: loss 4.4781
[2019-04-04 15:31:38,610] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 219000, global step 3505600: learning rate 0.0000
[2019-04-04 15:31:39,153] A3C_AGENT_WORKER-Thread-10 INFO:Local step 219000, global step 3505882: loss 4.4718
[2019-04-04 15:31:39,155] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 219000, global step 3505882: learning rate 0.0000
[2019-04-04 15:31:39,516] A3C_AGENT_WORKER-Thread-5 INFO:Local step 219000, global step 3506052: loss 4.4785
[2019-04-04 15:31:39,517] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 219000, global step 3506052: learning rate 0.0000
[2019-04-04 15:31:39,924] A3C_AGENT_WORKER-Thread-4 INFO:Local step 219000, global step 3506255: loss 4.4845
[2019-04-04 15:31:39,926] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 219000, global step 3506256: learning rate 0.0000
[2019-04-04 15:31:39,970] A3C_AGENT_WORKER-Thread-17 INFO:Local step 219500, global step 3506276: loss 0.0025
[2019-04-04 15:31:39,972] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 219500, global step 3506276: learning rate 0.0000
[2019-04-04 15:31:40,282] A3C_AGENT_WORKER-Thread-12 INFO:Local step 219000, global step 3506431: loss 4.4389
[2019-04-04 15:31:40,283] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 219000, global step 3506431: learning rate 0.0000
[2019-04-04 15:31:40,735] A3C_AGENT_WORKER-Thread-14 INFO:Local step 219000, global step 3506619: loss 4.4456
[2019-04-04 15:31:40,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 219000, global step 3506619: learning rate 0.0000
[2019-04-04 15:31:40,897] A3C_AGENT_WORKER-Thread-20 INFO:Local step 219000, global step 3506698: loss 3.8682
[2019-04-04 15:31:40,900] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 219000, global step 3506698: learning rate 0.0000
[2019-04-04 15:31:41,414] A3C_AGENT_WORKER-Thread-11 INFO:Local step 219500, global step 3506973: loss 0.0024
[2019-04-04 15:31:41,420] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 219500, global step 3506974: learning rate 0.0000
[2019-04-04 15:31:41,945] A3C_AGENT_WORKER-Thread-6 INFO:Local step 219000, global step 3507235: loss 4.4640
[2019-04-04 15:31:41,946] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 219000, global step 3507236: learning rate 0.0000
[2019-04-04 15:31:42,596] A3C_AGENT_WORKER-Thread-19 INFO:Local step 219000, global step 3507561: loss 4.4415
[2019-04-04 15:31:42,597] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 219000, global step 3507561: learning rate 0.0000
[2019-04-04 15:31:46,458] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.7359342e-10 1.7921614e-10 7.3255676e-16 1.6944200e-14 1.0000000e+00
 9.0393720e-11 4.0716056e-15], sum to 1.0000
[2019-04-04 15:31:46,458] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5693
[2019-04-04 15:31:46,479] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 32.0, 117.5, 792.5, 26.0, 26.68419734758389, 0.6305363616158305, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4100400.0000, 
sim time next is 4101000.0000, 
raw observation next is [-0.6666666666666667, 31.33333333333334, 118.6666666666667, 800.3333333333334, 26.0, 26.71206673227083, 0.638157027315855, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44413665743305636, 0.3133333333333334, 0.39555555555555566, 0.8843462246777164, 0.6666666666666666, 0.7260055610225692, 0.712719009105285, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11190514], dtype=float32), 0.3371535]. 
=============================================
[2019-04-04 15:31:46,484] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[82.80336]
 [82.79798]
 [82.79642]
 [82.90737]
 [82.9548 ]], R is [[82.99395752]
 [83.16401672]
 [83.33237457]
 [83.49905396]
 [83.6640625 ]].
[2019-04-04 15:31:47,217] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.6343085e-09 4.7691440e-10 1.1923546e-15 1.3517934e-14 1.0000000e+00
 3.9476331e-10 1.5361266e-15], sum to 1.0000
[2019-04-04 15:31:47,217] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0874
[2019-04-04 15:31:47,240] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 28.0, 120.5, 828.5, 26.0, 26.54601884201412, 0.4077076637400346, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4104000.0000, 
sim time next is 4104600.0000, 
raw observation next is [1.333333333333333, 28.16666666666667, 120.3333333333333, 832.6666666666667, 26.0, 26.61868396526148, 0.6264745561622386, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4995383194829178, 0.28166666666666673, 0.401111111111111, 0.9200736648250462, 0.6666666666666666, 0.7182236637717899, 0.7088248520540795, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.72123057], dtype=float32), 0.047884136]. 
=============================================
[2019-04-04 15:31:50,324] A3C_AGENT_WORKER-Thread-13 INFO:Local step 220000, global step 3511817: loss 0.5782
[2019-04-04 15:31:50,327] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 220000, global step 3511820: learning rate 0.0000
[2019-04-04 15:31:50,889] A3C_AGENT_WORKER-Thread-16 INFO:Local step 219500, global step 3512111: loss 0.0020
[2019-04-04 15:31:50,891] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 219500, global step 3512113: learning rate 0.0000
[2019-04-04 15:31:51,665] A3C_AGENT_WORKER-Thread-2 INFO:Local step 220000, global step 3512494: loss 0.3064
[2019-04-04 15:31:51,666] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 220000, global step 3512495: learning rate 0.0000
[2019-04-04 15:31:53,037] A3C_AGENT_WORKER-Thread-15 INFO:Local step 219500, global step 3513245: loss 0.0012
[2019-04-04 15:31:53,041] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 219500, global step 3513246: learning rate 0.0000
[2019-04-04 15:31:53,181] A3C_AGENT_WORKER-Thread-17 INFO:Local step 220000, global step 3513325: loss 0.2930
[2019-04-04 15:31:53,181] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 220000, global step 3513325: learning rate 0.0000
[2019-04-04 15:31:53,543] A3C_AGENT_WORKER-Thread-3 INFO:Local step 219500, global step 3513510: loss 0.0006
[2019-04-04 15:31:53,544] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 219500, global step 3513510: learning rate 0.0000
[2019-04-04 15:31:53,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7563023e-09 1.6817120e-10 3.0495367e-16 2.9345478e-15 1.0000000e+00
 1.2141599e-10 7.0163932e-16], sum to 1.0000
[2019-04-04 15:31:53,665] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4754
[2019-04-04 15:31:53,702] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.333333333333333, 53.33333333333334, 170.0, 118.0, 26.0, 25.68687741824935, 0.4058713083740586, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4267200.0000, 
sim time next is 4267800.0000, 
raw observation next is [3.5, 53.5, 182.0, 131.0, 26.0, 25.7043679939699, 0.4069762184447092, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.5595567867036012, 0.535, 0.6066666666666667, 0.14475138121546963, 0.6666666666666666, 0.6420306661641583, 0.6356587394815697, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1678], dtype=float32), -0.6607697]. 
=============================================
[2019-04-04 15:31:53,916] A3C_AGENT_WORKER-Thread-18 INFO:Local step 219500, global step 3513714: loss 0.0012
[2019-04-04 15:31:53,917] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 219500, global step 3513714: learning rate 0.0000
[2019-04-04 15:31:54,591] A3C_AGENT_WORKER-Thread-10 INFO:Local step 219500, global step 3514067: loss 0.0013
[2019-04-04 15:31:54,592] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 219500, global step 3514067: learning rate 0.0000
[2019-04-04 15:31:54,945] A3C_AGENT_WORKER-Thread-11 INFO:Local step 220000, global step 3514276: loss 0.5532
[2019-04-04 15:31:54,947] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 220000, global step 3514276: learning rate 0.0000
[2019-04-04 15:31:55,104] A3C_AGENT_WORKER-Thread-4 INFO:Local step 219500, global step 3514361: loss 0.0011
[2019-04-04 15:31:55,106] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 219500, global step 3514361: learning rate 0.0000
[2019-04-04 15:31:55,129] A3C_AGENT_WORKER-Thread-5 INFO:Local step 219500, global step 3514375: loss 0.0009
[2019-04-04 15:31:55,130] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 219500, global step 3514376: learning rate 0.0000
[2019-04-04 15:31:55,376] A3C_AGENT_WORKER-Thread-12 INFO:Local step 219500, global step 3514511: loss 0.0007
[2019-04-04 15:31:55,376] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 219500, global step 3514511: learning rate 0.0000
[2019-04-04 15:31:55,956] A3C_AGENT_WORKER-Thread-20 INFO:Local step 219500, global step 3514841: loss 0.0005
[2019-04-04 15:31:55,959] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 219500, global step 3514841: learning rate 0.0000
[2019-04-04 15:31:56,060] A3C_AGENT_WORKER-Thread-14 INFO:Local step 219500, global step 3514901: loss 0.0005
[2019-04-04 15:31:56,063] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 219500, global step 3514902: learning rate 0.0000
[2019-04-04 15:31:56,765] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2339230e-10 8.6091897e-11 2.8040318e-17 1.7976138e-15 1.0000000e+00
 2.2000015e-11 4.5199568e-16], sum to 1.0000
[2019-04-04 15:31:56,768] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5285
[2019-04-04 15:31:56,776] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.0, 49.0, 129.5, 836.0, 26.0, 26.4785787075856, 0.726668175023676, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4626000.0000, 
sim time next is 4626600.0000, 
raw observation next is [4.116666666666667, 49.0, 132.6666666666667, 828.3333333333334, 26.0, 26.65302363588679, 0.741407679153354, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5766389658356418, 0.49, 0.4422222222222224, 0.9152854511970534, 0.6666666666666666, 0.7210853029905658, 0.747135893051118, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03015384], dtype=float32), 0.7902199]. 
=============================================
[2019-04-04 15:31:57,388] A3C_AGENT_WORKER-Thread-6 INFO:Local step 219500, global step 3515721: loss 0.0006
[2019-04-04 15:31:57,389] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 219500, global step 3515721: learning rate 0.0000
[2019-04-04 15:31:58,157] A3C_AGENT_WORKER-Thread-19 INFO:Local step 219500, global step 3516180: loss 0.0009
[2019-04-04 15:31:58,160] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 219500, global step 3516180: learning rate 0.0000
[2019-04-04 15:32:00,856] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.9228253e-10 9.2331774e-11 9.1955035e-17 3.4516669e-15 1.0000000e+00
 5.2202170e-10 3.1594252e-16], sum to 1.0000
[2019-04-04 15:32:00,859] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5153
[2019-04-04 15:32:00,865] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.46666666666667, 58.66666666666666, 0.0, 0.0, 26.0, 26.96362796085408, 0.8685628850196615, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4394400.0000, 
sim time next is 4395000.0000, 
raw observation next is [10.33333333333333, 58.83333333333334, 0.0, 0.0, 26.0, 26.92705155307286, 0.8566239763717615, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7488457987072946, 0.5883333333333334, 0.0, 0.0, 0.6666666666666666, 0.7439209627560718, 0.7855413254572539, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34153384], dtype=float32), 0.05831905]. 
=============================================
[2019-04-04 15:32:00,876] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[91.60723 ]
 [92.36722 ]
 [93.6224  ]
 [94.686066]
 [94.59075 ]], R is [[91.05380249]
 [91.14326477]
 [91.23183441]
 [91.31951904]
 [91.40632629]].
[2019-04-04 15:32:01,288] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6626116e-10 5.6421909e-11 5.0445502e-16 4.4491541e-15 1.0000000e+00
 8.7162069e-11 2.1973455e-16], sum to 1.0000
[2019-04-04 15:32:01,288] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0658
[2019-04-04 15:32:01,308] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.966666666666667, 70.66666666666667, 0.0, 0.0, 26.0, 25.62632950158801, 0.4134185227469283, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4332000.0000, 
sim time next is 4332600.0000, 
raw observation next is [3.95, 70.5, 0.0, 0.0, 26.0, 25.66630557920007, 0.4025411950909819, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.57202216066482, 0.705, 0.0, 0.0, 0.6666666666666666, 0.6388587982666726, 0.6341803983636606, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42754152], dtype=float32), 0.03701218]. 
=============================================
[2019-04-04 15:32:02,621] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.06802265e-08 2.10299311e-09 2.90355843e-15 6.13028102e-14
 1.00000000e+00 2.46140580e-10 1.39763054e-14], sum to 1.0000
[2019-04-04 15:32:02,622] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8870
[2019-04-04 15:32:02,654] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 44.5, 122.0, 839.0, 26.0, 25.07470115684904, 0.4085616863526553, 0.0, 1.0, 18707.9048396576], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4793400.0000, 
sim time next is 4794000.0000, 
raw observation next is [-1.110223024625157e-16, 44.0, 130.0, 830.3333333333334, 26.0, 25.02964150469228, 0.409955091835865, 0.0, 1.0, 29842.85432878033], 
processed observation next is [0.0, 0.4782608695652174, 0.46260387811634357, 0.44, 0.43333333333333335, 0.9174953959484347, 0.6666666666666666, 0.5858034587243566, 0.6366516972786217, 0.0, 1.0, 0.1421088301370492], 
reward next is 0.8579, 
noisyNet noise sample is [array([1.197243], dtype=float32), -0.43925285]. 
=============================================
[2019-04-04 15:32:02,663] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[82.60468 ]
 [82.961845]
 [83.52082 ]
 [84.18916 ]
 [84.91446 ]], R is [[82.44199371]
 [82.52848816]
 [82.70320129]
 [82.8761673 ]
 [83.04740906]].
[2019-04-04 15:32:02,726] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.6752774e-10 2.3442489e-11 6.9203460e-18 7.5167308e-16 1.0000000e+00
 1.3308810e-11 3.6431251e-17], sum to 1.0000
[2019-04-04 15:32:02,728] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0423
[2019-04-04 15:32:02,741] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 160.1666666666667, 24.33333333333333, 26.0, 26.52677585101834, 0.6507095009820271, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4447200.0000, 
sim time next is 4447800.0000, 
raw observation next is [1.0, 86.0, 142.0, 0.0, 26.0, 26.49742627725827, 0.6415346644111017, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.47333333333333333, 0.0, 0.6666666666666666, 0.7081188564381892, 0.713844888137034, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1585321], dtype=float32), -0.05671464]. 
=============================================
[2019-04-04 15:32:03,165] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.5113454e-10 7.4891118e-12 2.4280668e-17 1.6123236e-15 1.0000000e+00
 8.8874871e-12 9.0033475e-17], sum to 1.0000
[2019-04-04 15:32:03,167] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4744
[2019-04-04 15:32:03,179] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 178.3333333333333, 48.66666666666666, 26.0, 26.51205185736346, 0.6588370988498096, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4446600.0000, 
sim time next is 4447200.0000, 
raw observation next is [1.0, 86.0, 160.1666666666667, 24.33333333333333, 26.0, 26.52679549369493, 0.6507150810776928, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.5338888888888891, 0.026887661141804783, 0.6666666666666666, 0.7105662911412441, 0.7169050270258976, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0387876], dtype=float32), 0.5277098]. 
=============================================
[2019-04-04 15:32:04,688] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1046851e-10 5.0086102e-11 1.3951255e-16 4.0934535e-16 1.0000000e+00
 3.4352934e-11 1.4090854e-16], sum to 1.0000
[2019-04-04 15:32:04,691] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0829
[2019-04-04 15:32:04,720] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 82.66666666666667, 73.33333333333334, 0.0, 26.0, 26.1593564227452, 0.5841969893587414, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4461600.0000, 
sim time next is 4462200.0000, 
raw observation next is [0.0, 81.5, 71.0, 0.0, 26.0, 26.16962614091214, 0.4739676873672589, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.815, 0.23666666666666666, 0.0, 0.6666666666666666, 0.680802178409345, 0.6579892291224196, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08005362], dtype=float32), 1.3721721]. 
=============================================
[2019-04-04 15:32:04,777] A3C_AGENT_WORKER-Thread-16 INFO:Local step 220000, global step 3519944: loss 0.4178
[2019-04-04 15:32:04,778] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 220000, global step 3519944: learning rate 0.0000
[2019-04-04 15:32:05,503] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1129231e-09 6.4245409e-10 1.2382698e-14 7.3204626e-14 1.0000000e+00
 2.7450692e-10 3.7489455e-14], sum to 1.0000
[2019-04-04 15:32:05,503] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8734
[2019-04-04 15:32:05,519] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 46.33333333333334, 0.0, 0.0, 26.0, 25.21352385495679, 0.4182326538786058, 0.0, 1.0, 56744.53178874226], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4823400.0000, 
sim time next is 4824000.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 25.32922994223994, 0.4317013226017041, 0.0, 1.0, 45650.5970813243], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6107691618533284, 0.6439004408672347, 0.0, 1.0, 0.2173837956253538], 
reward next is 0.7826, 
noisyNet noise sample is [array([-0.35474455], dtype=float32), 0.40203878]. 
=============================================
[2019-04-04 15:32:05,525] A3C_AGENT_WORKER-Thread-13 INFO:Local step 220500, global step 3520327: loss 0.0343
[2019-04-04 15:32:05,529] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 220500, global step 3520327: learning rate 0.0000
[2019-04-04 15:32:05,534] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[79.13166 ]
 [78.68317 ]
 [78.063324]
 [76.945496]
 [75.75942 ]], R is [[79.50498199]
 [79.43972015]
 [79.21704102]
 [78.63851166]
 [77.90539551]].
[2019-04-04 15:32:06,933] A3C_AGENT_WORKER-Thread-15 INFO:Local step 220000, global step 3521046: loss 0.3396
[2019-04-04 15:32:06,935] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 220000, global step 3521047: learning rate 0.0000
[2019-04-04 15:32:07,112] A3C_AGENT_WORKER-Thread-2 INFO:Local step 220500, global step 3521119: loss 0.0313
[2019-04-04 15:32:07,117] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 220500, global step 3521119: learning rate 0.0000
[2019-04-04 15:32:07,889] A3C_AGENT_WORKER-Thread-3 INFO:Local step 220000, global step 3521505: loss 0.6265
[2019-04-04 15:32:07,891] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 220000, global step 3521506: learning rate 0.0000
[2019-04-04 15:32:08,339] A3C_AGENT_WORKER-Thread-18 INFO:Local step 220000, global step 3521726: loss 0.4399
[2019-04-04 15:32:08,340] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 220000, global step 3521726: learning rate 0.0000
[2019-04-04 15:32:08,510] A3C_AGENT_WORKER-Thread-17 INFO:Local step 220500, global step 3521816: loss 0.0305
[2019-04-04 15:32:08,512] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 220500, global step 3521816: learning rate 0.0000
[2019-04-04 15:32:08,602] A3C_AGENT_WORKER-Thread-10 INFO:Local step 220000, global step 3521861: loss 0.3403
[2019-04-04 15:32:08,603] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 220000, global step 3521861: learning rate 0.0000
[2019-04-04 15:32:08,941] A3C_AGENT_WORKER-Thread-5 INFO:Local step 220000, global step 3522055: loss 0.3383
[2019-04-04 15:32:08,942] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 220000, global step 3522056: learning rate 0.0000
[2019-04-04 15:32:09,211] A3C_AGENT_WORKER-Thread-12 INFO:Local step 220000, global step 3522208: loss 0.3443
[2019-04-04 15:32:09,216] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 220000, global step 3522211: learning rate 0.0000
[2019-04-04 15:32:09,349] A3C_AGENT_WORKER-Thread-4 INFO:Local step 220000, global step 3522283: loss 0.4446
[2019-04-04 15:32:09,351] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 220000, global step 3522283: learning rate 0.0000
[2019-04-04 15:32:10,034] A3C_AGENT_WORKER-Thread-20 INFO:Local step 220000, global step 3522654: loss 0.6374
[2019-04-04 15:32:10,034] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 220000, global step 3522654: learning rate 0.0000
[2019-04-04 15:32:10,133] A3C_AGENT_WORKER-Thread-14 INFO:Local step 220000, global step 3522707: loss 0.3370
[2019-04-04 15:32:10,134] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 220000, global step 3522707: learning rate 0.0000
[2019-04-04 15:32:10,440] A3C_AGENT_WORKER-Thread-11 INFO:Local step 220500, global step 3522866: loss 0.0313
[2019-04-04 15:32:10,446] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 220500, global step 3522868: learning rate 0.0000
[2019-04-04 15:32:11,482] A3C_AGENT_WORKER-Thread-6 INFO:Local step 220000, global step 3523392: loss 0.3344
[2019-04-04 15:32:11,483] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 220000, global step 3523392: learning rate 0.0000
[2019-04-04 15:32:12,209] A3C_AGENT_WORKER-Thread-19 INFO:Local step 220000, global step 3523768: loss 0.3399
[2019-04-04 15:32:12,210] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 220000, global step 3523768: learning rate 0.0000
[2019-04-04 15:32:18,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:18,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:18,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run27
[2019-04-04 15:32:19,624] A3C_AGENT_WORKER-Thread-16 INFO:Local step 220500, global step 3527645: loss 0.0361
[2019-04-04 15:32:19,625] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 220500, global step 3527645: learning rate 0.0000
[2019-04-04 15:32:19,822] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:19,822] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:19,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run27
[2019-04-04 15:32:21,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:21,494] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:21,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run27
[2019-04-04 15:32:22,239] A3C_AGENT_WORKER-Thread-15 INFO:Local step 220500, global step 3528769: loss 0.0394
[2019-04-04 15:32:22,241] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 220500, global step 3528771: learning rate 0.0000
[2019-04-04 15:32:22,907] A3C_AGENT_WORKER-Thread-3 INFO:Local step 220500, global step 3529086: loss 0.0399
[2019-04-04 15:32:22,907] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 220500, global step 3529086: learning rate 0.0000
[2019-04-04 15:32:23,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:23,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:23,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run27
[2019-04-04 15:32:23,609] A3C_AGENT_WORKER-Thread-10 INFO:Local step 220500, global step 3529350: loss 0.0361
[2019-04-04 15:32:23,609] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 220500, global step 3529350: learning rate 0.0000
[2019-04-04 15:32:23,653] A3C_AGENT_WORKER-Thread-18 INFO:Local step 220500, global step 3529366: loss 0.0380
[2019-04-04 15:32:23,653] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 220500, global step 3529366: learning rate 0.0000
[2019-04-04 15:32:23,977] A3C_AGENT_WORKER-Thread-5 INFO:Local step 220500, global step 3529504: loss 0.0377
[2019-04-04 15:32:23,978] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 220500, global step 3529504: learning rate 0.0000
[2019-04-04 15:32:24,291] A3C_AGENT_WORKER-Thread-12 INFO:Local step 220500, global step 3529614: loss 0.0361
[2019-04-04 15:32:24,294] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 220500, global step 3529615: learning rate 0.0000
[2019-04-04 15:32:24,407] A3C_AGENT_WORKER-Thread-4 INFO:Local step 220500, global step 3529655: loss 0.0365
[2019-04-04 15:32:24,410] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 220500, global step 3529656: learning rate 0.0000
[2019-04-04 15:32:25,049] A3C_AGENT_WORKER-Thread-20 INFO:Local step 220500, global step 3529895: loss 0.0379
[2019-04-04 15:32:25,050] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 220500, global step 3529895: learning rate 0.0000
[2019-04-04 15:32:25,185] A3C_AGENT_WORKER-Thread-14 INFO:Local step 220500, global step 3529948: loss 0.0403
[2019-04-04 15:32:25,186] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 220500, global step 3529948: learning rate 0.0000
[2019-04-04 15:32:26,420] A3C_AGENT_WORKER-Thread-6 INFO:Local step 220500, global step 3530495: loss 0.0373
[2019-04-04 15:32:26,421] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 220500, global step 3530495: learning rate 0.0000
[2019-04-04 15:32:27,388] A3C_AGENT_WORKER-Thread-19 INFO:Local step 220500, global step 3530958: loss 0.0361
[2019-04-04 15:32:27,389] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 220500, global step 3530958: learning rate 0.0000
[2019-04-04 15:32:31,634] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:31,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:31,639] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run27
[2019-04-04 15:32:34,060] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:34,060] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:34,118] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run27
[2019-04-04 15:32:34,871] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:34,871] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:34,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run27
[2019-04-04 15:32:35,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:35,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:35,386] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run27
[2019-04-04 15:32:35,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:35,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:35,601] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run27
[2019-04-04 15:32:35,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:35,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:35,698] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run27
[2019-04-04 15:32:35,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:35,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:35,858] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run27
[2019-04-04 15:32:36,140] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:36,140] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:36,146] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run27
[2019-04-04 15:32:36,243] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:36,244] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:36,247] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run27
[2019-04-04 15:32:36,649] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:36,649] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:36,652] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run27
[2019-04-04 15:32:37,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:37,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:37,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run27
[2019-04-04 15:32:38,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:32:38,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:32:38,704] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run27
[2019-04-04 15:32:45,857] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5526920e-09 2.0911048e-10 3.0921531e-16 1.6924721e-14 1.0000000e+00
 4.0479658e-11 3.3632677e-16], sum to 1.0000
[2019-04-04 15:32:45,858] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9625
[2019-04-04 15:32:45,943] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 45.33333333333333, 0.0, 26.0, 23.36917674984371, -0.1038928572804503, 0.0, 1.0, 58186.25197035163], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 35400.0000, 
sim time next is 36000.0000, 
raw observation next is [7.7, 93.0, 49.0, 0.0, 26.0, 23.46912071042372, -0.0814023263393119, 0.0, 1.0, 57972.76007073244], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.16333333333333333, 0.0, 0.6666666666666666, 0.45576005920197665, 0.4728658912202293, 0.0, 1.0, 0.27606076224158305], 
reward next is 0.7239, 
noisyNet noise sample is [array([-1.1751505], dtype=float32), 1.9238955]. 
=============================================
[2019-04-04 15:32:45,964] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[87.22436 ]
 [86.864746]
 [86.47349 ]
 [86.095146]
 [85.696815]], R is [[87.46561432]
 [87.31388092]
 [87.1626358 ]
 [87.01183319]
 [86.86127472]].
[2019-04-04 15:32:54,218] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.3628486e-08 5.2766786e-09 8.3545882e-14 1.0333245e-12 1.0000000e+00
 1.5690782e-09 7.8689334e-13], sum to 1.0000
[2019-04-04 15:32:54,219] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2385
[2019-04-04 15:32:54,325] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.48293540943914, -0.2224237724192055, 1.0, 1.0, 202343.6184192479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 199200.0000, 
sim time next is 199800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.83818157401183, -0.1278753187373585, 0.0, 1.0, 203489.5591723921], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.40318179783431923, 0.45737489375421386, 0.0, 1.0, 0.9689979008209147], 
reward next is 0.0310, 
noisyNet noise sample is [array([0.07656849], dtype=float32), -0.85271347]. 
=============================================
[2019-04-04 15:33:04,372] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.7788665e-09 1.6115345e-10 4.5245823e-15 3.6301250e-14 1.0000000e+00
 2.5018751e-10 9.3347291e-15], sum to 1.0000
[2019-04-04 15:33:04,372] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8183
[2019-04-04 15:33:04,399] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 73.0, 0.0, 0.0, 26.0, 24.09642427508429, 0.08484422143561297, 0.0, 1.0, 44487.3136698486], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 261000.0000, 
sim time next is 261600.0000, 
raw observation next is [-5.966666666666667, 71.0, 0.0, 0.0, 26.0, 24.12406419762808, 0.08542457450676604, 0.0, 1.0, 44565.36676217606], 
processed observation next is [1.0, 0.0, 0.2973222530009234, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5103386831356733, 0.528474858168922, 0.0, 1.0, 0.2122160322008384], 
reward next is 0.7878, 
noisyNet noise sample is [array([0.5198632], dtype=float32), 0.004146599]. 
=============================================
[2019-04-04 15:33:12,900] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2132900e-09 1.2393704e-11 7.3426337e-16 8.6070380e-15 1.0000000e+00
 5.1617984e-11 9.5947001e-16], sum to 1.0000
[2019-04-04 15:33:12,900] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5663
[2019-04-04 15:33:12,913] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 26.0, 25.06894424634163, 0.2861628148784081, 0.0, 1.0, 42981.72279059548], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 594000.0000, 
sim time next is 594600.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 26.0, 25.03708831171677, 0.280359634190406, 0.0, 1.0, 43005.3974864143], 
processed observation next is [0.0, 0.9130434782608695, 0.38504155124653744, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5864240259763974, 0.593453211396802, 0.0, 1.0, 0.20478760707816335], 
reward next is 0.7952, 
noisyNet noise sample is [array([0.3321984], dtype=float32), -0.3006074]. 
=============================================
[2019-04-04 15:33:25,254] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.3275615e-08 5.5540443e-09 1.7577389e-14 8.7914105e-14 1.0000000e+00
 1.1640711e-09 3.0631751e-14], sum to 1.0000
[2019-04-04 15:33:25,254] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1476
[2019-04-04 15:33:25,278] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.1, 75.0, 0.0, 0.0, 26.0, 24.36373022598906, 0.05828493686467379, 0.0, 1.0, 41572.65697519847], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 703800.0000, 
sim time next is 704400.0000, 
raw observation next is [-3.0, 75.0, 0.0, 0.0, 26.0, 24.32545368468866, 0.05325961073746097, 0.0, 1.0, 41610.5220766743], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5271211403907218, 0.5177532035791537, 0.0, 1.0, 0.19814534322225857], 
reward next is 0.8019, 
noisyNet noise sample is [array([-0.06334489], dtype=float32), 0.027479673]. 
=============================================
[2019-04-04 15:33:32,604] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.3366540e-09 3.0042966e-09 1.0809744e-13 8.6239684e-14 1.0000000e+00
 1.8415644e-10 5.2382489e-14], sum to 1.0000
[2019-04-04 15:33:32,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0930
[2019-04-04 15:33:32,635] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 73.5, 0.0, 0.0, 26.0, 23.53172473310844, -0.06406408414925435, 0.0, 1.0, 43922.25067568154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 631800.0000, 
sim time next is 632400.0000, 
raw observation next is [-4.5, 75.33333333333333, 0.0, 0.0, 26.0, 23.50689521527381, -0.07007750690115096, 0.0, 1.0, 43947.63201019813], 
processed observation next is [0.0, 0.30434782608695654, 0.3379501385041552, 0.7533333333333333, 0.0, 0.0, 0.6666666666666666, 0.4589079346061509, 0.4766408310329497, 0.0, 1.0, 0.20927443814380065], 
reward next is 0.7907, 
noisyNet noise sample is [array([-1.447187], dtype=float32), -0.6790211]. 
=============================================
[2019-04-04 15:33:43,959] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5364918e-09 1.0086096e-09 7.4794077e-15 8.5620150e-14 1.0000000e+00
 3.0466698e-09 3.0949040e-15], sum to 1.0000
[2019-04-04 15:33:43,960] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0461
[2019-04-04 15:33:43,997] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.783333333333333, 71.66666666666667, 94.66666666666666, 0.0, 26.0, 25.57178881058552, 0.2966677089115439, 1.0, 1.0, 18714.79128041844], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 816600.0000, 
sim time next is 817200.0000, 
raw observation next is [-4.5, 71.0, 98.5, 0.0, 26.0, 25.59599933236032, 0.299128002212281, 1.0, 1.0, 18713.59845057247], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.3283333333333333, 0.0, 0.6666666666666666, 0.6329999443633602, 0.5997093340707603, 1.0, 1.0, 0.08911237357415462], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.0901631], dtype=float32), 0.997631]. 
=============================================
[2019-04-04 15:33:51,826] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.11761927e-10 6.80462631e-11 1.22146360e-16 2.24142917e-15
 1.00000000e+00 5.91933750e-12 1.05042395e-16], sum to 1.0000
[2019-04-04 15:33:51,834] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8802
[2019-04-04 15:33:51,850] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.500000000000001, 100.0, 0.0, 0.0, 26.0, 25.45073172805722, 0.5948524245845362, 0.0, 1.0, 28918.16341840384], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1289400.0000, 
sim time next is 1290000.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.45865742742717, 0.6000880036234665, 0.0, 1.0, 27896.39498242686], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6215547856189308, 0.7000293345411555, 0.0, 1.0, 0.13283997610679457], 
reward next is 0.8672, 
noisyNet noise sample is [array([1.041003], dtype=float32), -1.4413959]. 
=============================================
[2019-04-04 15:33:51,860] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[94.26383]
 [94.30354]
 [94.35151]
 [94.34595]
 [94.27315]], R is [[94.1634903 ]
 [94.08415222]
 [93.97887421]
 [93.83052063]
 [93.63832092]].
[2019-04-04 15:33:57,097] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.3239657e-11 6.9145883e-12 2.9824291e-17 8.1897689e-16 1.0000000e+00
 2.7196914e-12 3.2895319e-17], sum to 1.0000
[2019-04-04 15:33:57,097] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6185
[2019-04-04 15:33:57,118] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 41.33333333333334, 0.0, 26.0, 25.91991875428627, 0.4057185545253616, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1438800.0000, 
sim time next is 1439400.0000, 
raw observation next is [1.1, 92.0, 36.66666666666667, 0.0, 26.0, 25.39024309344087, 0.4281410665955603, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.12222222222222223, 0.0, 0.6666666666666666, 0.6158535911200724, 0.6427136888651868, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9550193], dtype=float32), 0.2315733]. 
=============================================
[2019-04-04 15:34:02,276] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8814110e-09 7.0851179e-11 5.5274323e-17 3.8283774e-16 1.0000000e+00
 8.5629386e-12 3.0113755e-17], sum to 1.0000
[2019-04-04 15:34:02,277] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2568
[2019-04-04 15:34:02,287] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.7, 83.33333333333333, 11.0, 0.6666666666666667, 26.0, 25.70976955076824, 0.6106740097627318, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1151400.0000, 
sim time next is 1152000.0000, 
raw observation next is [12.7, 84.0, 16.0, 0.5, 26.0, 25.69418405812822, 0.6066677634139906, 0.0, 1.0, 22230.45570289795], 
processed observation next is [0.0, 0.34782608695652173, 0.8144044321329641, 0.84, 0.05333333333333334, 0.0005524861878453039, 0.6666666666666666, 0.6411820048440182, 0.7022225878046635, 0.0, 1.0, 0.10585931287094262], 
reward next is 0.8941, 
noisyNet noise sample is [array([1.2053719], dtype=float32), 1.9541265]. 
=============================================
[2019-04-04 15:34:02,299] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[95.16342]
 [95.11318]
 [95.14885]
 [95.09148]
 [95.04905]], R is [[95.33385468]
 [95.38051605]
 [95.42671204]
 [95.38327789]
 [95.34027863]].
[2019-04-04 15:34:05,529] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.2435076e-09 2.3795017e-09 3.2874670e-14 5.4743959e-13 1.0000000e+00
 2.3159800e-10 1.9824139e-14], sum to 1.0000
[2019-04-04 15:34:05,530] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8362
[2019-04-04 15:34:05,538] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.50285567266144, 0.148620532142258, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1234200.0000, 
sim time next is 1234800.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.48179658762778, 0.1449881841441188, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.456816382302315, 0.5483293947147062, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43797192], dtype=float32), 0.84966034]. 
=============================================
[2019-04-04 15:34:07,801] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2149017e-09 5.3231329e-11 5.7584615e-17 2.2043736e-15 1.0000000e+00
 2.0452558e-11 1.0912269e-16], sum to 1.0000
[2019-04-04 15:34:07,802] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1915
[2019-04-04 15:34:07,816] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.6, 85.0, 0.0, 0.0, 26.0, 25.54210683809375, 0.5305491844727853, 0.0, 1.0, 18745.11826514277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1569600.0000, 
sim time next is 1570200.0000, 
raw observation next is [4.616666666666666, 84.83333333333334, 0.0, 0.0, 26.0, 25.6028634613063, 0.5228764481060925, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5904893813481072, 0.8483333333333334, 0.0, 0.0, 0.6666666666666666, 0.6335719551088582, 0.6742921493686974, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1332364], dtype=float32), -0.82165176]. 
=============================================
[2019-04-04 15:34:09,609] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.0770820e-11 7.4684998e-12 1.2125171e-17 1.3608282e-16 1.0000000e+00
 3.8731583e-12 4.1073256e-17], sum to 1.0000
[2019-04-04 15:34:09,611] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1572
[2019-04-04 15:34:09,623] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 73.5, 0.0, 26.0, 26.0694013417044, 0.5858391276369385, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1332000.0000, 
sim time next is 1332600.0000, 
raw observation next is [0.6000000000000001, 92.0, 83.00000000000001, 0.0, 26.0, 26.08383681923505, 0.5895221438416652, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.479224376731302, 0.92, 0.2766666666666667, 0.0, 0.6666666666666666, 0.6736530682695875, 0.6965073812805551, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0677143], dtype=float32), -0.24783921]. 
=============================================
[2019-04-04 15:34:13,352] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7129986e-09 5.3103858e-11 1.9135855e-16 1.0646777e-14 1.0000000e+00
 1.3377123e-10 2.9198067e-16], sum to 1.0000
[2019-04-04 15:34:13,359] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6627
[2019-04-04 15:34:13,375] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.50469361877814, 0.5008264903699586, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1385400.0000, 
sim time next is 1386000.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.4113129835939, 0.4829914832307636, 0.0, 1.0, 59416.58328821383], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6176094152994915, 0.6609971610769212, 0.0, 1.0, 0.2829361108962563], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.21079022], dtype=float32), 0.5853005]. 
=============================================
[2019-04-04 15:34:13,381] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[87.58127]
 [87.74458]
 [87.86303]
 [87.88068]
 [87.78487]], R is [[87.50284576]
 [87.62781525]
 [87.75154114]
 [87.74982452]
 [87.67837524]].
[2019-04-04 15:34:15,707] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.9742656e-10 1.1853684e-11 3.9644834e-17 4.1538099e-16 1.0000000e+00
 4.1777823e-12 2.0440601e-17], sum to 1.0000
[2019-04-04 15:34:15,708] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0125
[2019-04-04 15:34:15,751] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 32.0, 0.0, 26.0, 24.68832476926903, 0.4062036376556888, 1.0, 1.0, 196601.4990475629], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1440000.0000, 
sim time next is 1440600.0000, 
raw observation next is [1.1, 92.0, 27.33333333333333, 0.0, 26.0, 25.16116251655002, 0.4639600025781108, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.0911111111111111, 0.0, 0.6666666666666666, 0.5967635430458351, 0.6546533341927036, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6193076], dtype=float32), -0.62151855]. 
=============================================
[2019-04-04 15:34:17,206] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5220175e-10 4.0458970e-11 3.0328891e-16 5.9575105e-15 1.0000000e+00
 9.1756991e-11 1.9706648e-16], sum to 1.0000
[2019-04-04 15:34:17,210] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5716
[2019-04-04 15:34:17,243] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 90.0, 0.0, 0.0, 26.0, 25.67850993764301, 0.5270412535757601, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1449000.0000, 
sim time next is 1449600.0000, 
raw observation next is [1.1, 90.66666666666666, 0.0, 0.0, 26.0, 25.61233386641755, 0.5217296425415009, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.9066666666666666, 0.0, 0.0, 0.6666666666666666, 0.6343611555347959, 0.673909880847167, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4116758], dtype=float32), -0.4602469]. 
=============================================
[2019-04-04 15:34:35,081] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.3598743e-10 2.5828298e-10 4.4698572e-15 3.6461336e-14 1.0000000e+00
 9.6844852e-11 9.0995479e-16], sum to 1.0000
[2019-04-04 15:34:35,081] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5403
[2019-04-04 15:34:35,146] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.50347868542762, 0.5177836485007309, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2052000.0000, 
sim time next is 2052600.0000, 
raw observation next is [-3.9, 82.00000000000001, 0.0, 0.0, 26.0, 26.14878518868368, 0.550076255004574, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.8200000000000002, 0.0, 0.0, 0.6666666666666666, 0.6790654323903066, 0.6833587516681914, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24510936], dtype=float32), 0.8463464]. 
=============================================
[2019-04-04 15:34:40,576] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.7440209e-09 1.2550985e-09 3.2232545e-14 1.7234856e-13 1.0000000e+00
 8.0946888e-10 1.9749112e-14], sum to 1.0000
[2019-04-04 15:34:40,576] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6334
[2019-04-04 15:34:40,601] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.300000000000001, 80.0, 0.0, 0.0, 26.0, 24.33114289453402, 0.09093148515902176, 0.0, 1.0, 44918.02198627093], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1898400.0000, 
sim time next is 1899000.0000, 
raw observation next is [-7.3, 80.5, 0.0, 0.0, 26.0, 24.29085584415692, 0.08285143275370438, 0.0, 1.0, 44932.73832919998], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5242379870130766, 0.5276171442512348, 0.0, 1.0, 0.213965420615238], 
reward next is 0.7860, 
noisyNet noise sample is [array([0.8994763], dtype=float32), -1.0910381]. 
=============================================
[2019-04-04 15:34:40,607] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[76.24995 ]
 [76.464485]
 [76.67984 ]
 [76.8834  ]
 [77.10654 ]], R is [[76.08109283]
 [76.10638428]
 [76.13146973]
 [76.15631866]
 [76.18091583]].
[2019-04-04 15:34:44,745] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.0200991e-08 1.1452458e-09 1.1004606e-13 7.2685282e-13 9.9999988e-01
 6.8637460e-09 7.6878181e-14], sum to 1.0000
[2019-04-04 15:34:44,745] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4392
[2019-04-04 15:34:44,759] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.366666666666667, 76.0, 0.0, 0.0, 26.0, 24.0649240530966, 0.0649092266023231, 0.0, 1.0, 42038.93308073103], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2176800.0000, 
sim time next is 2177400.0000, 
raw observation next is [-6.283333333333333, 75.5, 0.0, 0.0, 26.0, 24.02155225524143, 0.06010347219755421, 0.0, 1.0, 42018.6835225923], 
processed observation next is [1.0, 0.17391304347826086, 0.288550323176362, 0.755, 0.0, 0.0, 0.6666666666666666, 0.5017960212701192, 0.5200344907325181, 0.0, 1.0, 0.2000889691552014], 
reward next is 0.7999, 
noisyNet noise sample is [array([-1.8375154], dtype=float32), 0.8862491]. 
=============================================
[2019-04-04 15:34:49,461] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.85377641e-09 2.34894193e-09 6.86032313e-15 1.75364429e-13
 1.00000000e+00 1.22695054e-09 1.10754166e-14], sum to 1.0000
[2019-04-04 15:34:49,463] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7326
[2019-04-04 15:34:49,502] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666667, 71.0, 121.3333333333333, 0.0, 26.0, 25.93190664160938, 0.3783519087808544, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2198400.0000, 
sim time next is 2199000.0000, 
raw observation next is [-4.583333333333333, 71.0, 125.6666666666667, 0.0, 26.0, 25.89299549139781, 0.3506213797350728, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3356417359187443, 0.71, 0.418888888888889, 0.0, 0.6666666666666666, 0.6577496242831508, 0.6168737932450242, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20938246], dtype=float32), -0.111703366]. 
=============================================
[2019-04-04 15:34:49,509] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.50463 ]
 [81.86411 ]
 [82.45966 ]
 [83.037926]
 [83.57155 ]], R is [[81.43510437]
 [81.62075043]
 [81.80454254]
 [81.98649597]
 [82.16663361]].
[2019-04-04 15:35:11,655] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8392635e-08 1.7112286e-09 1.7019994e-14 3.6498929e-13 1.0000000e+00
 4.3115303e-10 5.9448005e-14], sum to 1.0000
[2019-04-04 15:35:11,655] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1639
[2019-04-04 15:35:11,665] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.1, 60.0, 0.0, 0.0, 26.0, 24.96418304866645, 0.2882028180187571, 0.0, 1.0, 41952.53973116964], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2589600.0000, 
sim time next is 2590200.0000, 
raw observation next is [-4.2, 60.5, 0.0, 0.0, 26.0, 24.91212441849401, 0.2826067811883479, 0.0, 1.0, 41907.53943300842], 
processed observation next is [1.0, 1.0, 0.34626038781163443, 0.605, 0.0, 0.0, 0.6666666666666666, 0.5760103682078341, 0.594202260396116, 0.0, 1.0, 0.19955971158575436], 
reward next is 0.8004, 
noisyNet noise sample is [array([-0.35583535], dtype=float32), -0.27612525]. 
=============================================
[2019-04-04 15:35:15,206] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1333509e-08 7.0801498e-10 4.7202117e-14 8.8701365e-14 1.0000000e+00
 1.4042181e-09 4.4929806e-14], sum to 1.0000
[2019-04-04 15:35:15,207] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9069
[2019-04-04 15:35:15,235] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 54.66666666666667, 0.0, 0.0, 26.0, 25.45055753928966, 0.4287674455916695, 0.0, 1.0, 18759.99925482402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2323200.0000, 
sim time next is 2323800.0000, 
raw observation next is [-1.7, 55.0, 0.0, 0.0, 26.0, 25.43301926057366, 0.4261490673601802, 0.0, 1.0, 32336.74252528141], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6194182717144715, 0.6420496891200601, 0.0, 1.0, 0.15398448821562577], 
reward next is 0.8460, 
noisyNet noise sample is [array([-1.2024735], dtype=float32), 0.116650485]. 
=============================================
[2019-04-04 15:35:18,376] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-04 15:35:18,378] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 15:35:18,378] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 15:35:18,379] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:35:18,379] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 15:35:18,380] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:35:18,380] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:35:18,385] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run37
[2019-04-04 15:35:18,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run37
[2019-04-04 15:35:18,426] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run37
[2019-04-04 15:36:28,223] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18061243], dtype=float32), 0.22741756]
[2019-04-04 15:36:28,224] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.7833333333333333, 35.5, 0.0, 0.0, 26.0, 25.13965045487438, 0.2398398535041896, 0.0, 1.0, 39944.98434834379]
[2019-04-04 15:36:28,224] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 15:36:28,224] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.2037248e-08 1.4860891e-09 5.6838970e-14 2.5405722e-13 1.0000000e+00
 9.8528274e-10 8.4347094e-14], sampled 0.2901754058912105
[2019-04-04 15:37:00,624] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 15:37:19,659] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 15:37:21,548] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 15:37:22,572] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 3600000, evaluation results [3600000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 15:37:31,025] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3965685e-09 1.8376661e-10 4.7719524e-15 3.4603022e-15 1.0000000e+00
 9.0268293e-11 4.3273787e-15], sum to 1.0000
[2019-04-04 15:37:31,027] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2752
[2019-04-04 15:37:31,034] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.3, 29.0, 121.5, 338.3333333333333, 26.0, 25.81912827390418, 0.38971857952385, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2560800.0000, 
sim time next is 2561400.0000, 
raw observation next is [3.3, 29.0, 114.0, 351.0, 26.0, 25.7934749402484, 0.3964607524172286, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.554016620498615, 0.29, 0.38, 0.3878453038674033, 0.6666666666666666, 0.6494562450207001, 0.6321535841390762, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3287967], dtype=float32), 0.23606208]. 
=============================================
[2019-04-04 15:37:34,295] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4741440e-09 4.9465678e-11 2.2373616e-16 8.3449965e-15 1.0000000e+00
 7.9143539e-11 3.9333484e-16], sum to 1.0000
[2019-04-04 15:37:34,297] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1815
[2019-04-04 15:37:34,323] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.166666666666667, 98.83333333333334, 0.0, 0.0, 26.0, 24.82343801853745, 0.2270150480086178, 0.0, 1.0, 55439.05687490878], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2873400.0000, 
sim time next is 2874000.0000, 
raw observation next is [1.333333333333333, 97.66666666666667, 0.0, 0.0, 26.0, 24.77493697744571, 0.2199079602462973, 0.0, 1.0, 55431.75990371163], 
processed observation next is [1.0, 0.2608695652173913, 0.4995383194829178, 0.9766666666666667, 0.0, 0.0, 0.6666666666666666, 0.564578081453809, 0.5733026534154324, 0.0, 1.0, 0.26396076144624586], 
reward next is 0.7360, 
noisyNet noise sample is [array([-1.7922888], dtype=float32), -1.7789078]. 
=============================================
[2019-04-04 15:37:34,329] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[87.75196]
 [87.66082]
 [87.52647]
 [87.41071]
 [87.28294]], R is [[87.66599274]
 [87.52534485]
 [87.38622284]
 [87.24707794]
 [87.10829926]].
[2019-04-04 15:37:46,946] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2050168e-10 4.7809635e-11 3.0211802e-16 3.3387769e-15 1.0000000e+00
 9.0901169e-11 1.1583267e-15], sum to 1.0000
[2019-04-04 15:37:46,946] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2535
[2019-04-04 15:37:46,976] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.0, 35.0, 213.5, 429.0, 26.0, 25.62191559233806, 0.4553389357968971, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2811600.0000, 
sim time next is 2812200.0000, 
raw observation next is [4.333333333333334, 34.16666666666667, 225.3333333333333, 343.6666666666666, 26.0, 25.76431756488224, 0.4685487656589203, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.58264081255771, 0.34166666666666673, 0.751111111111111, 0.37974217311233877, 0.6666666666666666, 0.6470264637401867, 0.6561829218863068, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0501386], dtype=float32), 0.73330194]. 
=============================================
[2019-04-04 15:37:47,248] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3659239e-08 1.3552852e-08 2.0732062e-13 7.0781532e-13 9.9999988e-01
 3.9506101e-09 1.1561822e-13], sum to 1.0000
[2019-04-04 15:37:47,250] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3414
[2019-04-04 15:37:47,336] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.5, 64.0, 0.0, 0.0, 26.0, 23.68003789869175, 0.1013308320247164, 1.0, 1.0, 202976.3296492895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2791800.0000, 
sim time next is 2792400.0000, 
raw observation next is [-6.333333333333334, 64.0, 18.0, 34.49999999999999, 26.0, 24.36178959119077, 0.214762080859727, 1.0, 1.0, 192367.2218858998], 
processed observation next is [1.0, 0.30434782608695654, 0.28716528162511545, 0.64, 0.06, 0.038121546961325956, 0.6666666666666666, 0.5301491325992309, 0.5715873602865756, 1.0, 1.0, 0.9160343899328562], 
reward next is 0.0840, 
noisyNet noise sample is [array([-1.0873218], dtype=float32), -0.23488796]. 
=============================================
[2019-04-04 15:37:56,335] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2469020e-09 2.4830826e-10 9.2994318e-16 1.6744334e-14 1.0000000e+00
 7.8147072e-11 2.8664707e-15], sum to 1.0000
[2019-04-04 15:37:56,336] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5274
[2019-04-04 15:37:56,397] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 166.0, 78.0, 26.0, 24.97768633558874, 0.2861874547851889, 0.0, 1.0, 66231.45190621629], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2973600.0000, 
sim time next is 2974200.0000, 
raw observation next is [-3.833333333333333, 70.0, 170.0, 59.99999999999999, 26.0, 24.88829556260841, 0.2887273547324194, 0.0, 1.0, 104611.8469062672], 
processed observation next is [0.0, 0.43478260869565216, 0.3564173591874424, 0.7, 0.5666666666666667, 0.06629834254143646, 0.6666666666666666, 0.5740246302173674, 0.5962424515774731, 0.0, 1.0, 0.4981516519346057], 
reward next is 0.5018, 
noisyNet noise sample is [array([-0.1688536], dtype=float32), 1.2488115]. 
=============================================
[2019-04-04 15:37:58,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8712142e-08 3.8026724e-09 4.4241164e-14 7.8350716e-13 1.0000000e+00
 1.0038390e-08 5.6167775e-14], sum to 1.0000
[2019-04-04 15:37:58,296] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0578
[2019-04-04 15:37:58,331] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.5, 76.0, 0.0, 0.0, 26.0, 24.39886148542573, 0.1883777948123616, 0.0, 1.0, 43627.97321766606], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3303000.0000, 
sim time next is 3303600.0000, 
raw observation next is [-10.66666666666667, 76.0, 0.0, 0.0, 26.0, 24.33695550127962, 0.1781320863261594, 0.0, 1.0, 43656.11252143205], 
processed observation next is [1.0, 0.21739130434782608, 0.16712834718374878, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5280796251066349, 0.5593773621087198, 0.0, 1.0, 0.20788625010205738], 
reward next is 0.7921, 
noisyNet noise sample is [array([-1.0643295], dtype=float32), 2.564628]. 
=============================================
[2019-04-04 15:38:02,747] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4551742e-08 1.1839416e-09 1.8397000e-14 2.6897484e-13 1.0000000e+00
 1.2298160e-09 7.4989243e-14], sum to 1.0000
[2019-04-04 15:38:02,748] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3522
[2019-04-04 15:38:02,760] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83022827786092, 0.2940486682903393, 0.0, 1.0, 41180.0989925426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3377400.0000, 
sim time next is 3378000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.81289804778207, 0.2943538687102767, 0.0, 1.0, 41165.25105529713], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5677415039818392, 0.5981179562367589, 0.0, 1.0, 0.19602500502522444], 
reward next is 0.8040, 
noisyNet noise sample is [array([-1.5994548], dtype=float32), -0.24397236]. 
=============================================
[2019-04-04 15:38:02,763] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.633194]
 [78.692696]
 [78.87655 ]
 [78.812004]
 [78.945786]], R is [[78.59438324]
 [78.61234283]
 [78.63014984]
 [78.64776611]
 [78.66510773]].
[2019-04-04 15:38:05,272] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.1586783e-09 3.1081588e-09 4.0715125e-15 1.9795396e-13 1.0000000e+00
 1.6924014e-10 1.8350669e-14], sum to 1.0000
[2019-04-04 15:38:05,273] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6363
[2019-04-04 15:38:05,289] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3333333333333334, 40.0, 96.5, 758.0, 26.0, 25.116298001778, 0.3629848705609076, 0.0, 1.0, 18703.96265396476], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3076800.0000, 
sim time next is 3077400.0000, 
raw observation next is [-0.1666666666666666, 39.5, 94.0, 741.0, 26.0, 25.11724201495203, 0.3637912109758055, 0.0, 1.0, 18702.78260232135], 
processed observation next is [0.0, 0.6086956521739131, 0.4579870729455217, 0.395, 0.31333333333333335, 0.8187845303867404, 0.6666666666666666, 0.5931035012460025, 0.6212637369919352, 0.0, 1.0, 0.08906086953486357], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.5692837], dtype=float32), -1.4977369]. 
=============================================
[2019-04-04 15:38:12,754] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9650298e-08 2.5255420e-09 2.9956902e-13 2.0043908e-13 1.0000000e+00
 4.4851660e-09 1.5811914e-13], sum to 1.0000
[2019-04-04 15:38:12,756] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1708
[2019-04-04 15:38:12,798] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.10617480241596, 0.3471103289475787, 0.0, 1.0, 27230.04405924511], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3609000.0000, 
sim time next is 3609600.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.08197212142837, 0.3424267189433172, 0.0, 1.0, 38580.7866600815], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5901643434523643, 0.6141422396477724, 0.0, 1.0, 0.18371803171467382], 
reward next is 0.8163, 
noisyNet noise sample is [array([0.46809998], dtype=float32), -0.8742447]. 
=============================================
[2019-04-04 15:38:13,734] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.7332399e-09 2.8685890e-10 1.1223297e-14 2.4677011e-13 1.0000000e+00
 4.1384909e-09 2.4586231e-14], sum to 1.0000
[2019-04-04 15:38:13,735] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7894
[2019-04-04 15:38:13,751] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.43372729413926, 0.4669768913819838, 0.0, 1.0, 18763.23054729054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3544200.0000, 
sim time next is 3544800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.42332000466836, 0.4558003444788273, 0.0, 1.0, 28013.63339986143], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6186100003890299, 0.6519334481596091, 0.0, 1.0, 0.13339825428505442], 
reward next is 0.8666, 
noisyNet noise sample is [array([-0.19122483], dtype=float32), -1.5301863]. 
=============================================
[2019-04-04 15:38:14,308] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1827414e-08 6.2441596e-10 3.9259469e-14 1.9345579e-13 1.0000000e+00
 1.4262544e-10 2.4418083e-14], sum to 1.0000
[2019-04-04 15:38:14,309] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8036
[2019-04-04 15:38:14,334] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 41.66666666666667, 66.83333333333333, 545.6666666666666, 26.0, 25.30607634324449, 0.4472335846879134, 0.0, 1.0, 18684.31005308043], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3601200.0000, 
sim time next is 3601800.0000, 
raw observation next is [0.0, 41.0, 63.0, 515.0, 26.0, 25.28115524828947, 0.4399829074810913, 0.0, 1.0, 9342.122701325115], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.41, 0.21, 0.569060773480663, 0.6666666666666666, 0.6067629373574558, 0.6466609691603638, 0.0, 1.0, 0.04448629857773864], 
reward next is 0.9555, 
noisyNet noise sample is [array([-1.943179], dtype=float32), 0.036607046]. 
=============================================
[2019-04-04 15:38:14,974] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5192176e-08 1.8996316e-09 1.2579233e-14 1.3726104e-13 1.0000000e+00
 4.1605625e-10 9.3835343e-15], sum to 1.0000
[2019-04-04 15:38:14,975] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7848
[2019-04-04 15:38:15,088] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.666666666666667, 70.0, 73.83333333333334, 374.3333333333334, 26.0, 24.19498466147514, 0.2920640948446717, 0.0, 1.0, 202438.3946901009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3572400.0000, 
sim time next is 3573000.0000, 
raw observation next is [-6.5, 70.0, 88.0, 425.0, 26.0, 24.58190874133776, 0.4087603479271034, 0.0, 1.0, 202114.1663368627], 
processed observation next is [0.0, 0.34782608695652173, 0.28254847645429365, 0.7, 0.29333333333333333, 0.4696132596685083, 0.6666666666666666, 0.54849239511148, 0.6362534493090345, 0.0, 1.0, 0.9624484111279176], 
reward next is 0.0376, 
noisyNet noise sample is [array([0.24929297], dtype=float32), -0.46888468]. 
=============================================
[2019-04-04 15:38:15,095] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[79.87255 ]
 [78.397224]
 [77.89753 ]
 [77.458855]
 [77.08315 ]], R is [[80.7166214 ]
 [79.94546509]
 [79.94934082]
 [79.9526062 ]
 [79.95542908]].
[2019-04-04 15:38:50,640] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1833924e-10 3.9247455e-12 6.7506342e-17 1.3689975e-16 1.0000000e+00
 2.2196839e-12 4.3492525e-17], sum to 1.0000
[2019-04-04 15:38:50,643] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0887
[2019-04-04 15:38:50,664] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.35, 75.5, 0.0, 0.0, 26.0, 25.53340598579315, 0.403251406668364, 0.0, 1.0, 18745.13104393443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4321800.0000, 
sim time next is 4322400.0000, 
raw observation next is [4.3, 75.33333333333334, 0.0, 0.0, 26.0, 25.50772856899009, 0.4021014169000985, 0.0, 1.0, 37512.7629353864], 
processed observation next is [1.0, 0.0, 0.5817174515235458, 0.7533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6256440474158408, 0.6340338056333662, 0.0, 1.0, 0.17863220445422096], 
reward next is 0.8214, 
noisyNet noise sample is [array([0.46005487], dtype=float32), 0.38924652]. 
=============================================
[2019-04-04 15:38:58,545] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.8896872e-09 8.6667934e-10 3.5645867e-14 6.4961932e-14 1.0000000e+00
 6.5169564e-10 6.3401317e-14], sum to 1.0000
[2019-04-04 15:38:58,545] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0404
[2019-04-04 15:38:58,560] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 41.5, 0.0, 0.0, 26.0, 25.36681908594764, 0.4380639488454661, 0.0, 1.0, 60196.73073693195], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4147800.0000, 
sim time next is 4148400.0000, 
raw observation next is [-1.0, 41.0, 0.0, 0.0, 26.0, 25.35089755079338, 0.4366695523213549, 0.0, 1.0, 48372.32710127719], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.41, 0.0, 0.0, 0.6666666666666666, 0.6125747958994484, 0.6455565174404516, 0.0, 1.0, 0.23034441476798662], 
reward next is 0.7697, 
noisyNet noise sample is [array([-0.8571257], dtype=float32), 0.41088596]. 
=============================================
[2019-04-04 15:38:59,466] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3280523e-09 2.5686123e-10 5.2463627e-16 2.6342471e-15 1.0000000e+00
 1.3685318e-10 2.9806449e-16], sum to 1.0000
[2019-04-04 15:38:59,472] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2453
[2019-04-04 15:38:59,521] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.0, 35.0, 100.0, 744.5, 26.0, 27.09213083618538, 0.7674467675655778, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4114800.0000, 
sim time next is 4115400.0000, 
raw observation next is [4.0, 35.0, 98.0, 728.0, 26.0, 27.15451705245064, 0.7815467920649599, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.32666666666666666, 0.8044198895027624, 0.6666666666666666, 0.7628764210375533, 0.7605155973549866, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.74278593], dtype=float32), 0.37919912]. 
=============================================
[2019-04-04 15:39:00,949] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.0174815e-10 1.3965408e-10 4.9800201e-16 1.0333308e-15 1.0000000e+00
 1.0260040e-10 8.9394576e-16], sum to 1.0000
[2019-04-04 15:39:00,950] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2313
[2019-04-04 15:39:00,972] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 47.66666666666667, 261.1666666666666, 102.1666666666667, 26.0, 26.25926326193117, 0.4930044176514095, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4544400.0000, 
sim time next is 4545000.0000, 
raw observation next is [3.0, 47.0, 264.0, 113.0, 26.0, 25.85814968332861, 0.5500367451053122, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.47, 0.88, 0.12486187845303867, 0.6666666666666666, 0.6548458069440507, 0.6833455817017707, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.62552905], dtype=float32), -0.27511567]. 
=============================================
[2019-04-04 15:39:00,980] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[89.57373 ]
 [89.56906 ]
 [89.34994 ]
 [89.065926]
 [88.61975 ]], R is [[89.57453156]
 [89.67878723]
 [89.78199768]
 [89.88417816]
 [89.9853363 ]].
[2019-04-04 15:39:04,085] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.3306777e-10 1.3077765e-10 6.1652848e-17 1.1418790e-15 1.0000000e+00
 4.8572067e-11 5.8581434e-17], sum to 1.0000
[2019-04-04 15:39:04,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4882
[2019-04-04 15:39:04,099] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.666666666666667, 58.33333333333334, 203.8333333333333, 15.0, 26.0, 26.22638812295108, 0.5492931885967155, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4531200.0000, 
sim time next is 4531800.0000, 
raw observation next is [1.833333333333333, 57.66666666666666, 186.6666666666667, 12.0, 26.0, 26.22657217552036, 0.5498095788382993, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5133887349953832, 0.5766666666666665, 0.6222222222222223, 0.013259668508287293, 0.6666666666666666, 0.6855476812933633, 0.6832698596127664, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8505215], dtype=float32), 0.053081315]. 
=============================================
[2019-04-04 15:39:08,743] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0538864e-10 2.3559033e-11 2.0948418e-17 8.8050954e-16 1.0000000e+00
 4.2221456e-12 9.8512281e-18], sum to 1.0000
[2019-04-04 15:39:08,745] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0359
[2019-04-04 15:39:08,757] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.0, 37.0, 34.0, 0.0, 26.0, 28.48160046991902, 1.101731800763726, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4380000.0000, 
sim time next is 4380600.0000, 
raw observation next is [13.0, 37.5, 29.0, 0.0, 26.0, 28.60413684596418, 1.103704571478539, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.375, 0.09666666666666666, 0.0, 0.6666666666666666, 0.8836780704970151, 0.8679015238261796, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3710024], dtype=float32), -0.29474753]. 
=============================================
[2019-04-04 15:39:08,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9360109e-10 1.8807563e-11 1.5661901e-16 1.3663739e-15 1.0000000e+00
 2.6753174e-11 3.6428907e-16], sum to 1.0000
[2019-04-04 15:39:08,923] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4903
[2019-04-04 15:39:08,934] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 92.0, 209.6666666666667, 6.0, 26.0, 26.47835166792327, 0.597564631707998, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4704600.0000, 
sim time next is 4705200.0000, 
raw observation next is [0.0, 92.0, 210.5, 6.0, 26.0, 26.47532109893728, 0.5922298287576077, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.92, 0.7016666666666667, 0.0066298342541436465, 0.6666666666666666, 0.7062767582447732, 0.6974099429192026, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40192392], dtype=float32), 0.17086023]. 
=============================================
[2019-04-04 15:39:13,167] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4362549e-10 4.2285873e-11 1.2193268e-16 5.5083044e-15 1.0000000e+00
 3.6628822e-11 4.3759462e-17], sum to 1.0000
[2019-04-04 15:39:13,168] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2607
[2019-04-04 15:39:13,181] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.466666666666667, 63.33333333333333, 0.0, 0.0, 26.0, 25.77771750138209, 0.6409476996949971, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4407000.0000, 
sim time next is 4407600.0000, 
raw observation next is [7.333333333333334, 63.66666666666667, 0.0, 0.0, 26.0, 25.811454408573, 0.6357725269536028, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.6657433056325024, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6509545340477499, 0.7119241756512009, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.220836], dtype=float32), -0.18710531]. 
=============================================
[2019-04-04 15:39:19,800] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8874891e-09 1.6519772e-10 1.6507846e-16 7.2258780e-15 1.0000000e+00
 9.4217530e-11 1.3191619e-15], sum to 1.0000
[2019-04-04 15:39:19,801] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0484
[2019-04-04 15:39:19,841] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3, 49.5, 283.0, 308.0, 26.0, 25.02237498254443, 0.3505667590217771, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4879800.0000, 
sim time next is 4880400.0000, 
raw observation next is [0.5333333333333332, 48.66666666666667, 282.6666666666667, 321.6666666666667, 26.0, 25.04619155244687, 0.3522300547247559, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4773776546629733, 0.4866666666666667, 0.9422222222222223, 0.3554327808471455, 0.6666666666666666, 0.5871826293705725, 0.6174100182415853, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.601045], dtype=float32), 1.1222245]. 
=============================================
[2019-04-04 15:39:20,119] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7210030e-09 5.9665974e-11 7.2576675e-17 2.5713311e-15 1.0000000e+00
 4.0238635e-11 8.5108117e-17], sum to 1.0000
[2019-04-04 15:39:20,123] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9061
[2019-04-04 15:39:20,134] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.666666666666667, 50.0, 249.8333333333333, 58.83333333333333, 26.0, 26.07540055364835, 0.5724281011904493, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4542000.0000, 
sim time next is 4542600.0000, 
raw observation next is [2.833333333333333, 49.5, 252.6666666666667, 69.66666666666666, 26.0, 26.19039539013148, 0.5860411949199732, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.541089566020314, 0.495, 0.8422222222222224, 0.07697974217311233, 0.6666666666666666, 0.6825329491776234, 0.6953470649733244, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03913167], dtype=float32), -2.0618956]. 
=============================================
[2019-04-04 15:39:21,586] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6353498e-10 4.3120132e-11 7.9666109e-17 6.1366652e-15 1.0000000e+00
 3.2478027e-11 6.5863118e-17], sum to 1.0000
[2019-04-04 15:39:21,590] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6918
[2019-04-04 15:39:21,605] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7728512e-08 1.7505567e-09 1.3507985e-14 5.2364737e-13 1.0000000e+00
 1.2458349e-09 8.5204074e-14], sum to 1.0000
[2019-04-04 15:39:21,605] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7039
[2019-04-04 15:39:21,606] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.333333333333333, 59.66666666666667, 204.6666666666667, 15.0, 26.0, 26.20499216725819, 0.544122071180402, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4530000.0000, 
sim time next is 4530600.0000, 
raw observation next is [1.5, 59.0, 221.0, 18.0, 26.0, 26.23328918098251, 0.5521153711838116, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5041551246537397, 0.59, 0.7366666666666667, 0.019889502762430938, 0.6666666666666666, 0.6861074317485425, 0.6840384570612704, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3587835], dtype=float32), 1.1356044]. 
=============================================
[2019-04-04 15:39:21,617] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 50.0, 0.0, 0.0, 26.0, 25.21315946380812, 0.2880748799539728, 0.0, 1.0, 38510.72981795999], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4938000.0000, 
sim time next is 4938600.0000, 
raw observation next is [-1.833333333333333, 50.0, 0.0, 0.0, 26.0, 25.25603465481832, 0.2832969238504465, 0.0, 1.0, 38398.08544104564], 
processed observation next is [1.0, 0.13043478260869565, 0.41181902123730385, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6046695545681932, 0.5944323079501489, 0.0, 1.0, 0.18284802590974114], 
reward next is 0.8172, 
noisyNet noise sample is [array([-0.18806173], dtype=float32), -0.28625968]. 
=============================================
[2019-04-04 15:39:22,046] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.2319554e-09 2.2184544e-10 5.4365947e-16 9.3751614e-15 1.0000000e+00
 5.6817623e-10 1.5151175e-15], sum to 1.0000
[2019-04-04 15:39:22,047] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5781
[2019-04-04 15:39:22,055] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.0, 26.0, 106.1666666666667, 811.5, 26.0, 27.634666734034, 0.8713692954286429, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4977600.0000, 
sim time next is 4978200.0000, 
raw observation next is [8.0, 26.0, 103.3333333333333, 804.0, 26.0, 27.70022328603158, 0.8858373440458788, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.34444444444444433, 0.8883977900552487, 0.6666666666666666, 0.8083519405026317, 0.7952791146819597, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35965684], dtype=float32), -1.0170591]. 
=============================================
[2019-04-04 15:39:23,115] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.0850998e-09 1.9615367e-09 3.5446337e-14 2.3383875e-13 1.0000000e+00
 1.7151138e-09 3.7227308e-14], sum to 1.0000
[2019-04-04 15:39:23,117] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6659
[2019-04-04 15:39:23,130] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.5, 33.0, 0.0, 0.0, 26.0, 25.69425422214168, 0.5793180784118398, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5002200.0000, 
sim time next is 5002800.0000, 
raw observation next is [3.333333333333333, 34.33333333333333, 0.0, 0.0, 26.0, 25.75107345772134, 0.578261760828095, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5549399815327793, 0.34333333333333327, 0.0, 0.0, 0.6666666666666666, 0.645922788143445, 0.6927539202760317, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1892267], dtype=float32), 0.30784264]. 
=============================================
[2019-04-04 15:39:27,307] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6474267e-09 5.5483000e-11 1.5239166e-15 5.0012044e-14 1.0000000e+00
 1.3723639e-10 2.5958008e-16], sum to 1.0000
[2019-04-04 15:39:27,312] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2074
[2019-04-04 15:39:27,343] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8333333333333334, 73.0, 52.66666666666666, 22.33333333333334, 26.0, 25.80354902783638, 0.5219868787725264, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4727400.0000, 
sim time next is 4728000.0000, 
raw observation next is [0.6666666666666667, 74.0, 40.83333333333333, 25.16666666666667, 26.0, 26.02897296267603, 0.4994351703914304, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4810710987996307, 0.74, 0.1361111111111111, 0.0278084714548803, 0.6666666666666666, 0.6690810802230024, 0.6664783901304768, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.70964515], dtype=float32), -0.12056472]. 
=============================================
[2019-04-04 15:39:27,351] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[87.024536]
 [87.4149  ]
 [87.77088 ]
 [87.231186]
 [87.30908 ]], R is [[86.81089783]
 [86.94278717]
 [87.07335663]
 [86.3901062 ]
 [86.22535706]].
[2019-04-04 15:39:28,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:28,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:28,527] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run28
[2019-04-04 15:39:30,048] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:30,048] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:30,062] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run28
[2019-04-04 15:39:30,559] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:30,560] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:30,564] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run28
[2019-04-04 15:39:33,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:33,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:33,594] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run28
[2019-04-04 15:39:37,418] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9515070e-10 1.6124468e-11 7.2732469e-16 3.1526607e-15 1.0000000e+00
 6.9370225e-11 8.2259636e-16], sum to 1.0000
[2019-04-04 15:39:37,420] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3221
[2019-04-04 15:39:37,444] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 24.66666666666667, 0.0, 0.0, 26.0, 26.45034619267885, 0.700055566451771, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4990200.0000, 
sim time next is 4990800.0000, 
raw observation next is [6.0, 24.33333333333334, 0.0, 0.0, 26.0, 25.59333890328649, 0.6031959019577721, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6327782419405409, 0.7010653006525907, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.62996477], dtype=float32), -0.021659134]. 
=============================================
[2019-04-04 15:39:38,666] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.8674528e-09 1.6762677e-10 1.1066294e-14 6.4168865e-14 1.0000000e+00
 1.8584202e-10 6.1724218e-15], sum to 1.0000
[2019-04-04 15:39:38,670] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0267
[2019-04-04 15:39:38,678] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 45.0, 152.8333333333333, 404.5, 26.0, 25.11527982039378, 0.375279513956243, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4894800.0000, 
sim time next is 4895400.0000, 
raw observation next is [3.0, 45.0, 142.6666666666667, 387.0, 26.0, 25.12398559802701, 0.3731463152991172, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.47555555555555573, 0.4276243093922652, 0.6666666666666666, 0.5936654665022507, 0.6243821050997057, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5716964], dtype=float32), -0.73522586]. 
=============================================
[2019-04-04 15:39:41,074] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7615039e-08 2.9936231e-09 6.3666063e-15 1.5999001e-13 1.0000000e+00
 2.7393696e-09 6.7191593e-14], sum to 1.0000
[2019-04-04 15:39:41,075] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6292
[2019-04-04 15:39:41,089] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.833333333333333, 49.33333333333334, 0.0, 0.0, 26.0, 25.03745137651805, 0.2320528959865091, 0.0, 1.0, 38646.89735431648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4945800.0000, 
sim time next is 4946400.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.01348269307109, 0.2339237049121905, 0.0, 1.0, 38658.70368384671], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5844568910892575, 0.5779745683040635, 0.0, 1.0, 0.18408906516117482], 
reward next is 0.8159, 
noisyNet noise sample is [array([0.14288607], dtype=float32), 2.5170858]. 
=============================================
[2019-04-04 15:39:42,940] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:42,942] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:42,952] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run28
[2019-04-04 15:39:43,723] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:43,723] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:43,728] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run28
[2019-04-04 15:39:44,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:44,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:44,882] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run28
[2019-04-04 15:39:45,091] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:45,092] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:45,098] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run28
[2019-04-04 15:39:45,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:45,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:45,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run28
[2019-04-04 15:39:46,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:46,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:46,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run28
[2019-04-04 15:39:46,990] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:46,990] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:47,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run28
[2019-04-04 15:39:47,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:47,810] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:47,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run28
[2019-04-04 15:39:48,389] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:48,389] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:48,394] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run28
[2019-04-04 15:39:48,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:48,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:48,468] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run28
[2019-04-04 15:39:48,992] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:48,992] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:48,995] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run28
[2019-04-04 15:39:49,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:39:49,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:39:49,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run28
[2019-04-04 15:40:16,489] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9358914e-08 6.8802064e-10 2.1618043e-14 6.0370849e-13 1.0000000e+00
 5.2116835e-09 5.1469554e-14], sum to 1.0000
[2019-04-04 15:40:16,491] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8189
[2019-04-04 15:40:16,519] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.483333333333333, 66.66666666666667, 0.0, 0.0, 26.0, 24.73150569486299, 0.2137064023148951, 0.0, 1.0, 44754.42487174357], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 249000.0000, 
sim time next is 249600.0000, 
raw observation next is [-3.566666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 24.67938183132196, 0.2038205835825805, 0.0, 1.0, 44713.83457178399], 
processed observation next is [1.0, 0.9130434782608695, 0.3638042474607572, 0.6833333333333335, 0.0, 0.0, 0.6666666666666666, 0.5566151526101635, 0.5679401945275269, 0.0, 1.0, 0.21292302177039996], 
reward next is 0.7871, 
noisyNet noise sample is [array([0.52081954], dtype=float32), 0.87047535]. 
=============================================
[2019-04-04 15:40:38,942] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.5724372e-11 3.9093177e-12 1.0173443e-17 8.8406708e-16 1.0000000e+00
 1.1159058e-12 5.1622850e-17], sum to 1.0000
[2019-04-04 15:40:38,942] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9440
[2019-04-04 15:40:38,992] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 80.0, 136.1666666666667, 573.6666666666667, 26.0, 24.98610578876296, 0.3498936000809588, 0.0, 1.0, 18727.72066235705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 567600.0000, 
sim time next is 568200.0000, 
raw observation next is [-1.2, 80.0, 134.3333333333333, 552.3333333333333, 26.0, 24.99726839148161, 0.3501328964923981, 0.0, 1.0, 18725.75373743529], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.4477777777777776, 0.6103130755064455, 0.6666666666666666, 0.5831056992901343, 0.616710965497466, 0.0, 1.0, 0.089170255892549], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.78577787], dtype=float32), 0.027635982]. 
=============================================
[2019-04-04 15:40:45,839] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4350266e-09 2.4589378e-10 5.2734730e-15 1.4801879e-13 1.0000000e+00
 4.2675544e-10 3.7188487e-14], sum to 1.0000
[2019-04-04 15:40:45,839] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5526
[2019-04-04 15:40:45,896] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.733333333333333, 71.33333333333334, 0.0, 0.0, 26.0, 24.4236352722201, 0.09745405913226435, 0.0, 1.0, 40991.49900836598], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 692400.0000, 
sim time next is 693000.0000, 
raw observation next is [-3.65, 71.5, 0.0, 0.0, 26.0, 24.39741417185212, 0.09159121289637044, 0.0, 1.0, 40964.45745153773], 
processed observation next is [1.0, 0.0, 0.3614958448753463, 0.715, 0.0, 0.0, 0.6666666666666666, 0.5331178476543433, 0.5305304042987902, 0.0, 1.0, 0.19506884500732252], 
reward next is 0.8049, 
noisyNet noise sample is [array([-0.90604067], dtype=float32), 1.7037385]. 
=============================================
[2019-04-04 15:40:45,911] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[79.14968 ]
 [78.844   ]
 [78.63482 ]
 [78.509254]
 [78.591896]], R is [[79.21050262]
 [79.22319794]
 [79.23548889]
 [79.24724579]
 [79.25846863]].
[2019-04-04 15:40:50,660] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.04795617e-09 1.05685405e-10 1.72865501e-16 7.13322613e-15
 1.00000000e+00 6.36143638e-11 3.27690895e-16], sum to 1.0000
[2019-04-04 15:40:50,661] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7798
[2019-04-04 15:40:50,686] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.9, 92.66666666666667, 0.0, 0.0, 26.0, 25.38722490926137, 0.3861835175639843, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 925800.0000, 
sim time next is 926400.0000, 
raw observation next is [4.800000000000001, 93.33333333333334, 0.0, 0.0, 26.0, 25.62064268121268, 0.372038787056941, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5955678670360112, 0.9333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6350535567677232, 0.6240129290189803, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8062279], dtype=float32), -1.0065229]. 
=============================================
[2019-04-04 15:40:52,930] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.5154423e-12 1.3654512e-12 3.8635398e-18 6.7713184e-17 1.0000000e+00
 7.3225893e-13 2.1993714e-19], sum to 1.0000
[2019-04-04 15:40:52,930] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0259
[2019-04-04 15:40:52,946] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.3, 80.0, 107.0, 117.0, 26.0, 26.95312563801329, 0.8352418899979117, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1072800.0000, 
sim time next is 1073400.0000, 
raw observation next is [13.66666666666667, 78.33333333333334, 109.3333333333333, 77.99999999999999, 26.0, 27.04837380246572, 0.8494201794219874, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8411819021237306, 0.7833333333333334, 0.36444444444444435, 0.08618784530386739, 0.6666666666666666, 0.7540311502054765, 0.7831400598073291, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11266097], dtype=float32), 0.8834839]. 
=============================================
[2019-04-04 15:40:58,527] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8279416e-09 8.4429352e-10 3.4913150e-15 4.6440987e-14 1.0000000e+00
 2.0629776e-10 5.2580061e-15], sum to 1.0000
[2019-04-04 15:40:58,528] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1729
[2019-04-04 15:40:58,593] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.57621626320504, 0.237173682257329, 1.0, 1.0, 95137.9096368824], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 890400.0000, 
sim time next is 891000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.99289755714832, 0.2482116820843802, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5827414630956934, 0.58273722736146, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.61906475], dtype=float32), 1.2649809]. 
=============================================
[2019-04-04 15:40:58,599] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[82.90454]
 [81.03737]
 [81.04434]
 [81.03873]
 [81.05455]], R is [[84.31828308]
 [84.02205658]
 [83.99887085]
 [83.97583008]
 [83.95277405]].
[2019-04-04 15:41:04,966] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.8738547e-11 2.1609737e-12 2.9433553e-18 1.0197227e-16 1.0000000e+00
 3.1767134e-12 7.0454292e-19], sum to 1.0000
[2019-04-04 15:41:04,967] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3980
[2019-04-04 15:41:05,012] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.5, 92.83333333333333, 18.0, 0.0, 26.0, 25.40585924682344, 0.4252915335922909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 979800.0000, 
sim time next is 980400.0000, 
raw observation next is [9.600000000000001, 92.66666666666667, 22.5, 0.0, 26.0, 25.33550761313693, 0.487207385585406, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7285318559556788, 0.9266666666666667, 0.075, 0.0, 0.6666666666666666, 0.6112923010947441, 0.662402461861802, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3157213], dtype=float32), -1.0693914]. 
=============================================
[2019-04-04 15:41:05,743] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.7562941e-11 1.4104333e-11 1.2628393e-18 3.2269790e-16 1.0000000e+00
 7.4693541e-12 1.3657889e-17], sum to 1.0000
[2019-04-04 15:41:05,744] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6612
[2019-04-04 15:41:05,763] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.93333333333334, 81.0, 0.0, 0.0, 26.0, 25.68337408786368, 0.6211876087412521, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1063200.0000, 
sim time next is 1063800.0000, 
raw observation next is [12.75, 81.5, 0.0, 0.0, 26.0, 25.84529898318804, 0.6232454069619081, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.8157894736842106, 0.815, 0.0, 0.0, 0.6666666666666666, 0.6537749152656701, 0.7077484689873027, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5285317], dtype=float32), 0.212288]. 
=============================================
[2019-04-04 15:41:07,400] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-04 15:41:07,401] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 15:41:07,402] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:41:07,402] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 15:41:07,402] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 15:41:07,402] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:41:07,402] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:41:07,408] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run38
[2019-04-04 15:41:07,430] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run38
[2019-04-04 15:41:07,453] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run38
[2019-04-04 15:41:15,478] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17805588], dtype=float32), 0.23101546]
[2019-04-04 15:41:15,478] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.8, 93.0, 54.33333333333334, 25.0, 26.0, 25.41880588885229, 0.3305319434367576, 1.0, 1.0, 0.0]
[2019-04-04 15:41:15,479] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 15:41:15,479] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.3158984e-10 1.2515688e-11 2.1616195e-17 4.2644980e-16 1.0000000e+00
 7.4493502e-12 3.4416709e-17], sampled 0.37907220342625636
[2019-04-04 15:41:38,465] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.17805588], dtype=float32), 0.23101546]
[2019-04-04 15:41:38,466] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.9, 86.0, 14.5, 0.0, 26.0, 25.19677397627024, 0.3550528541206872, 1.0, 1.0, 0.0]
[2019-04-04 15:41:38,466] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 15:41:38,468] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.9156923e-10 4.4015829e-11 3.3165734e-16 3.2097437e-15 1.0000000e+00
 3.9291893e-11 4.6596659e-16], sampled 0.9490294779895585
[2019-04-04 15:41:54,014] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17805588], dtype=float32), 0.23101546]
[2019-04-04 15:41:54,015] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [16.1, 56.0, 75.0, 319.5, 26.0, 26.09527033149194, 0.7450392386752874, 0.0, 0.0, 0.0]
[2019-04-04 15:41:54,015] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 15:41:54,015] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.9103605e-10 6.4397751e-11 3.2582988e-16 2.7989581e-15 1.0000000e+00
 1.7753061e-11 7.2393937e-16], sampled 0.477167103955798
[2019-04-04 15:42:49,643] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4172 239942393.0563 1605.0250
[2019-04-04 15:43:07,999] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.4099 263463930.4480 1556.9858
[2019-04-04 15:43:11,977] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7881 275774498.6820 1233.0964
[2019-04-04 15:43:13,002] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 3700000, evaluation results [3700000.0, 7241.409855009758, 263463930.4479512, 1556.9858411143289, 7353.417175922201, 239942393.05634084, 1605.024971488376, 7182.788101514333, 275774498.68198794, 1233.096368464637]
[2019-04-04 15:43:13,860] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7270216e-11 6.5522685e-13 1.3514522e-19 3.7363670e-18 1.0000000e+00
 7.0638008e-14 3.6032334e-19], sum to 1.0000
[2019-04-04 15:43:13,861] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3128
[2019-04-04 15:43:13,881] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.86666666666667, 68.33333333333334, 230.6666666666667, 179.1666666666667, 26.0, 27.3641311709984, 0.9629047030957963, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1077600.0000, 
sim time next is 1078200.0000, 
raw observation next is [16.05, 67.5, 254.0, 215.0, 26.0, 27.48533988829138, 0.6458381962412766, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.9072022160664821, 0.675, 0.8466666666666667, 0.23756906077348067, 0.6666666666666666, 0.7904449906909484, 0.7152793987470921, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.044077], dtype=float32), 0.7017908]. 
=============================================
[2019-04-04 15:43:14,881] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4923003e-10 6.8446754e-12 2.7461802e-17 5.2269831e-17 1.0000000e+00
 4.5445167e-12 4.5222117e-18], sum to 1.0000
[2019-04-04 15:43:14,883] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8976
[2019-04-04 15:43:14,896] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.67955764624934, 0.5683818872314095, 0.0, 1.0, 60769.07598258115], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1035000.0000, 
sim time next is 1035600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.62378113556641, 0.5722525735491898, 0.0, 1.0, 71192.9313270984], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6353150946305343, 0.6907508578497299, 0.0, 1.0, 0.3390139587004686], 
reward next is 0.6610, 
noisyNet noise sample is [array([0.1906225], dtype=float32), 0.97213644]. 
=============================================
[2019-04-04 15:43:15,329] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4869267e-09 7.7731280e-11 3.9446761e-17 1.4234950e-15 1.0000000e+00
 7.9480429e-11 9.4686359e-17], sum to 1.0000
[2019-04-04 15:43:15,337] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4825
[2019-04-04 15:43:15,348] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.9, 77.83333333333333, 0.0, 0.0, 26.0, 25.73596621510431, 0.5781318738680014, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1044600.0000, 
sim time next is 1045200.0000, 
raw observation next is [14.0, 77.66666666666667, 0.0, 0.0, 26.0, 25.75494659422255, 0.5748856856963113, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.8504155124653741, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6462455495185457, 0.6916285618987704, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.53192645], dtype=float32), -1.1207619]. 
=============================================
[2019-04-04 15:43:17,487] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6408040e-10 7.0660616e-11 4.2512282e-16 6.0070310e-15 1.0000000e+00
 1.0985896e-10 7.7993052e-16], sum to 1.0000
[2019-04-04 15:43:17,494] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4998
[2019-04-04 15:43:17,512] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 59.0, 0.0, 26.0, 26.02054884255859, 0.5400607305210811, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1436400.0000, 
sim time next is 1437000.0000, 
raw observation next is [1.1, 92.0, 54.66666666666666, 0.0, 26.0, 26.05263595619633, 0.5168376971483674, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.1822222222222222, 0.0, 0.6666666666666666, 0.6710529963496942, 0.6722792323827891, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4056158], dtype=float32), 2.3586783]. 
=============================================
[2019-04-04 15:43:17,517] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[91.365074]
 [91.59103 ]
 [91.84988 ]
 [92.12907 ]
 [92.471535]], R is [[91.27809143]
 [91.36531067]
 [91.45166016]
 [91.53714752]
 [91.6217804 ]].
[2019-04-04 15:43:18,144] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7459925e-08 2.0692208e-09 1.5539236e-14 5.1819562e-13 1.0000000e+00
 2.3973629e-10 9.0090302e-14], sum to 1.0000
[2019-04-04 15:43:18,148] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6954
[2019-04-04 15:43:18,153] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.8, 63.0, 54.5, 0.0, 26.0, 25.05111479926486, 0.4855068206359608, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1180800.0000, 
sim time next is 1181400.0000, 
raw observation next is [18.71666666666667, 63.33333333333333, 46.0, 0.0, 26.0, 25.03586123379289, 0.4808262134945529, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.981071098799631, 0.6333333333333333, 0.15333333333333332, 0.0, 0.6666666666666666, 0.5863217694827408, 0.6602754044981843, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7327888], dtype=float32), -1.0424098]. 
=============================================
[2019-04-04 15:43:18,451] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6488378e-11 6.3810068e-13 2.7833980e-18 2.9876900e-17 1.0000000e+00
 5.1825888e-13 1.9458249e-18], sum to 1.0000
[2019-04-04 15:43:18,454] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8658
[2019-04-04 15:43:18,462] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 100.0, 40.33333333333334, 0.0, 26.0, 24.63914303925208, 0.4228633829409597, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1266600.0000, 
sim time next is 1267200.0000, 
raw observation next is [13.8, 100.0, 35.0, 0.0, 26.0, 24.62575282063731, 0.4222125507607001, 0.0, 1.0, 18680.41167023054], 
processed observation next is [0.0, 0.6956521739130435, 0.844875346260388, 1.0, 0.11666666666666667, 0.0, 0.6666666666666666, 0.5521460683864424, 0.6407375169202334, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([2.029666], dtype=float32), -0.59412354]. 
=============================================
[2019-04-04 15:43:19,135] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.7766683e-10 1.6728941e-11 8.8647782e-17 2.8906503e-15 1.0000000e+00
 9.6631182e-11 1.2252811e-16], sum to 1.0000
[2019-04-04 15:43:19,139] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6366
[2019-04-04 15:43:19,154] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.2, 73.0, 0.0, 0.0, 26.0, 25.51259355436496, 0.5503059161059055, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1540800.0000, 
sim time next is 1541400.0000, 
raw observation next is [7.283333333333333, 73.16666666666667, 0.0, 0.0, 26.0, 25.44778573386414, 0.5365649492223842, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6643582640812559, 0.7316666666666667, 0.0, 0.0, 0.6666666666666666, 0.6206488111553451, 0.6788549830741281, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11502688], dtype=float32), 1.0775684]. 
=============================================
[2019-04-04 15:43:23,292] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.1467452e-11 3.1263971e-11 1.1499955e-17 7.0820869e-16 1.0000000e+00
 3.3227852e-12 3.6482986e-17], sum to 1.0000
[2019-04-04 15:43:23,295] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9100
[2019-04-04 15:43:23,307] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.26666666666667, 100.0, 24.33333333333333, 0.0, 26.0, 24.60753234425115, 0.4210896593107105, 0.0, 1.0, 24169.20280818559], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1268400.0000, 
sim time next is 1269000.0000, 
raw observation next is [13.0, 100.0, 19.0, 0.0, 26.0, 24.60079649918382, 0.4214553394132321, 0.0, 1.0, 26533.60516128196], 
processed observation next is [0.0, 0.6956521739130435, 0.8227146814404434, 1.0, 0.06333333333333334, 0.0, 0.6666666666666666, 0.5500663749319848, 0.640485113137744, 0.0, 1.0, 0.12635050076800933], 
reward next is 0.8736, 
noisyNet noise sample is [array([0.6887113], dtype=float32), 0.04767887]. 
=============================================
[2019-04-04 15:43:23,321] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[95.9674  ]
 [95.994194]
 [96.025566]
 [96.03849 ]
 [96.071465]], R is [[95.87268066]
 [95.79886627]
 [95.74713898]
 [95.70071411]
 [95.65475464]].
[2019-04-04 15:43:26,161] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1928013e-10 3.1536397e-12 3.4540343e-17 5.6262072e-16 1.0000000e+00
 1.5615589e-11 2.4949089e-17], sum to 1.0000
[2019-04-04 15:43:26,164] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0460
[2019-04-04 15:43:26,170] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 85.33333333333334, 103.0, 0.0, 26.0, 25.85107104848004, 0.4987414151800367, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1686000.0000, 
sim time next is 1686600.0000, 
raw observation next is [1.1, 86.0, 107.0, 0.0, 26.0, 25.7285084255062, 0.4836082306361782, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.86, 0.3566666666666667, 0.0, 0.6666666666666666, 0.6440423687921832, 0.6612027435453928, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2612232], dtype=float32), -0.62432534]. 
=============================================
[2019-04-04 15:43:32,058] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.15953713e-09 4.49853557e-11 1.25532387e-16 1.25961772e-15
 1.00000000e+00 1.34966482e-11 1.03595484e-16], sum to 1.0000
[2019-04-04 15:43:32,058] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5908
[2019-04-04 15:43:32,097] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 99.16666666666667, 36.66666666666667, 0.0, 26.0, 25.96053950577738, 0.5070920453760568, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1415400.0000, 
sim time next is 1416000.0000, 
raw observation next is [-0.4, 98.33333333333334, 41.33333333333334, 0.0, 26.0, 25.91877907585159, 0.5087338201462407, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.45152354570637127, 0.9833333333333334, 0.1377777777777778, 0.0, 0.6666666666666666, 0.6598982563209658, 0.6695779400487468, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00107566], dtype=float32), -0.0064057685]. 
=============================================
[2019-04-04 15:43:32,110] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[92.29191 ]
 [92.364655]
 [92.39223 ]
 [92.39238 ]
 [92.284515]], R is [[92.21990967]
 [92.29771423]
 [92.3747406 ]
 [92.4509964 ]
 [92.52648926]].
[2019-04-04 15:43:41,861] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5784244e-10 1.2153293e-11 5.3811920e-17 7.0532116e-16 1.0000000e+00
 3.0065408e-11 7.0065467e-17], sum to 1.0000
[2019-04-04 15:43:41,866] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0837
[2019-04-04 15:43:41,923] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 96.66666666666667, 0.0, 26.0, 26.02091027914779, 0.6067889129800951, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1690800.0000, 
sim time next is 1691400.0000, 
raw observation next is [1.1, 88.0, 93.33333333333333, 0.0, 26.0, 26.19253713161497, 0.6223947678742777, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3111111111111111, 0.0, 0.6666666666666666, 0.6827114276345808, 0.7074649226247592, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9358623], dtype=float32), 0.21221419]. 
=============================================
[2019-04-04 15:43:44,984] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5173082e-07 8.1001428e-09 2.4214208e-13 8.3340908e-13 9.9999976e-01
 8.2428597e-09 3.8019241e-13], sum to 1.0000
[2019-04-04 15:43:44,989] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0993
[2019-04-04 15:43:45,006] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.816666666666666, 81.33333333333334, 0.0, 0.0, 26.0, 23.62382348113504, -0.08296896717519961, 0.0, 1.0, 45285.88860412144], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1918200.0000, 
sim time next is 1918800.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 26.0, 23.65563901709358, -0.08975070568775599, 0.0, 1.0, 45235.65005702887], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.82, 0.0, 0.0, 0.6666666666666666, 0.47130325142446505, 0.47008309810408133, 0.0, 1.0, 0.2154078574144232], 
reward next is 0.7846, 
noisyNet noise sample is [array([-0.92318183], dtype=float32), -0.25499526]. 
=============================================
[2019-04-04 15:44:08,162] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5660510e-09 2.0154654e-10 1.0921145e-15 2.7899680e-14 1.0000000e+00
 1.2496230e-10 5.7614779e-15], sum to 1.0000
[2019-04-04 15:44:08,162] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8278
[2019-04-04 15:44:08,224] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.833333333333334, 84.0, 91.66666666666667, 35.16666666666666, 26.0, 25.60224602895041, 0.300337264534932, 1.0, 1.0, 18741.36186802325], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2280000.0000, 
sim time next is 2280600.0000, 
raw observation next is [-7.550000000000001, 82.5, 101.0, 39.0, 26.0, 25.59322600214106, 0.3054496590856162, 1.0, 1.0, 18737.60918249515], 
processed observation next is [1.0, 0.391304347826087, 0.25346260387811637, 0.825, 0.33666666666666667, 0.0430939226519337, 0.6666666666666666, 0.6327688335117548, 0.6018165530285388, 1.0, 1.0, 0.08922671039283406], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.6277148], dtype=float32), 1.7112366]. 
=============================================
[2019-04-04 15:44:10,907] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3885627e-09 4.8195570e-10 1.2693742e-14 2.6041792e-14 1.0000000e+00
 2.9333913e-10 1.4291951e-14], sum to 1.0000
[2019-04-04 15:44:10,913] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3698
[2019-04-04 15:44:10,968] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3166666666666667, 44.66666666666666, 154.3333333333333, 63.0, 26.0, 26.05202319657091, 0.4833436670673492, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2297400.0000, 
sim time next is 2298000.0000, 
raw observation next is [-0.03333333333333333, 44.33333333333334, 137.6666666666667, 61.5, 26.0, 26.24207935206509, 0.4944745245145388, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46168051708217916, 0.4433333333333334, 0.45888888888888907, 0.06795580110497237, 0.6666666666666666, 0.6868399460054242, 0.6648248415048462, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2659461], dtype=float32), 0.60664535]. 
=============================================
[2019-04-04 15:44:10,972] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.33307 ]
 [80.99591 ]
 [81.62512 ]
 [82.217575]
 [81.719894]], R is [[79.87577057]
 [80.07701111]
 [80.27624512]
 [80.47348022]
 [79.72335815]].
[2019-04-04 15:44:11,002] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1062564e-09 8.6339269e-10 1.0060426e-13 5.2613296e-14 1.0000000e+00
 3.7914467e-09 2.1734012e-14], sum to 1.0000
[2019-04-04 15:44:11,002] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2783
[2019-04-04 15:44:11,053] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.3, 68.0, 125.0, 0.0, 26.0, 26.01928769649694, 0.4890202717176324, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2125800.0000, 
sim time next is 2126400.0000, 
raw observation next is [-5.199999999999999, 68.0, 118.5, 0.0, 26.0, 26.19532420287194, 0.5058897563403728, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.31855955678670367, 0.68, 0.395, 0.0, 0.6666666666666666, 0.6829436835726618, 0.6686299187801242, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7219079], dtype=float32), -0.5347466]. 
=============================================
[2019-04-04 15:44:27,400] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8389936e-09 3.5111578e-10 9.0116333e-16 9.6758606e-15 1.0000000e+00
 3.9153736e-10 1.6547386e-15], sum to 1.0000
[2019-04-04 15:44:27,421] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9457
[2019-04-04 15:44:27,435] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5333333333333334, 41.66666666666667, 229.6666666666667, 62.83333333333334, 26.0, 25.70799034950956, 0.3262546683184154, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2547600.0000, 
sim time next is 2548200.0000, 
raw observation next is [0.8166666666666668, 40.33333333333333, 227.3333333333333, 54.66666666666667, 26.0, 25.74900420973831, 0.3266048234345073, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4852262234533703, 0.40333333333333327, 0.7577777777777777, 0.060405156537753225, 0.6666666666666666, 0.6457503508115258, 0.6088682744781692, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6489426], dtype=float32), -1.9140874]. 
=============================================
[2019-04-04 15:44:28,308] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1588346e-08 4.6303369e-10 1.1704935e-14 3.4178921e-13 1.0000000e+00
 8.5199048e-10 9.2587647e-15], sum to 1.0000
[2019-04-04 15:44:28,308] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2652
[2019-04-04 15:44:28,380] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 91.0, 9.999999999999998, 19.33333333333334, 26.0, 24.40465840510296, 0.1884000092337333, 1.0, 1.0, 152803.0860640397], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2274000.0000, 
sim time next is 2274600.0000, 
raw observation next is [-9.5, 91.0, 17.0, 18.66666666666667, 26.0, 24.85349740843252, 0.2271013429910803, 1.0, 1.0, 34256.40173030072], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.056666666666666664, 0.02062615101289135, 0.6666666666666666, 0.5711247840360434, 0.5757004476636934, 1.0, 1.0, 0.16312572252524152], 
reward next is 0.8369, 
noisyNet noise sample is [array([0.78063786], dtype=float32), -0.16275391]. 
=============================================
[2019-04-04 15:44:36,484] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5717688e-08 4.0199173e-09 2.3245487e-13 6.9270989e-13 1.0000000e+00
 5.3939457e-09 2.7327259e-13], sum to 1.0000
[2019-04-04 15:44:36,490] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6145
[2019-04-04 15:44:36,535] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.3, 25.66666666666667, 50.33333333333333, 345.8333333333333, 26.0, 24.96937694249137, 0.2574796721873716, 0.0, 1.0, 18698.2204916016], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2478000.0000, 
sim time next is 2478600.0000, 
raw observation next is [3.3, 25.5, 48.0, 279.0, 26.0, 24.96395024788359, 0.2669856909015133, 0.0, 1.0, 18698.21395861336], 
processed observation next is [0.0, 0.6956521739130435, 0.554016620498615, 0.255, 0.16, 0.3082872928176796, 0.6666666666666666, 0.5803291873236326, 0.5889952303005045, 0.0, 1.0, 0.08903911408863505], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.41525432], dtype=float32), -0.3185274]. 
=============================================
[2019-04-04 15:44:50,088] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.4695478e-10 2.7874018e-12 8.8278220e-18 1.7017843e-15 1.0000000e+00
 4.7843817e-12 8.5274463e-18], sum to 1.0000
[2019-04-04 15:44:50,089] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3961
[2019-04-04 15:44:50,117] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8333333333333334, 94.16666666666666, 67.66666666666666, 51.99999999999999, 26.0, 25.39052173501837, 0.3089446441385026, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2884200.0000, 
sim time next is 2884800.0000, 
raw observation next is [0.6666666666666667, 95.33333333333334, 58.33333333333333, 25.99999999999999, 26.0, 25.37099036144123, 0.3073468082795464, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4810710987996307, 0.9533333333333335, 0.19444444444444442, 0.028729281767955788, 0.6666666666666666, 0.6142491967867691, 0.6024489360931821, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.547335], dtype=float32), 0.7179599]. 
=============================================
[2019-04-04 15:44:52,491] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3801932e-09 1.2856510e-10 4.4754189e-15 1.3877649e-14 1.0000000e+00
 7.8631955e-11 5.1054187e-15], sum to 1.0000
[2019-04-04 15:44:52,491] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0376
[2019-04-04 15:44:52,542] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.166666666666667, 66.0, 204.0, 110.6666666666666, 26.0, 24.97633688472051, 0.331389476273013, 0.0, 1.0, 18747.26841677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2976600.0000, 
sim time next is 2977200.0000, 
raw observation next is [-3.0, 65.0, 217.0, 154.0, 26.0, 24.97850470787592, 0.3390372724516872, 0.0, 1.0, 18743.28119188454], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.7233333333333334, 0.17016574585635358, 0.6666666666666666, 0.58154205898966, 0.6130124241505625, 0.0, 1.0, 0.08925371996135495], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.88137096], dtype=float32), 1.7587152]. 
=============================================
[2019-04-04 15:44:54,015] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.4059636e-09 7.2892820e-10 3.3245806e-15 1.8414096e-14 1.0000000e+00
 5.6326022e-10 1.8747069e-15], sum to 1.0000
[2019-04-04 15:44:54,019] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5110
[2019-04-04 15:44:54,037] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.333333333333334, 64.0, 112.1666666666667, 784.0, 26.0, 25.99632005545477, 0.4760695881557143, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2720400.0000, 
sim time next is 2721000.0000, 
raw observation next is [-8.166666666666666, 64.0, 112.3333333333333, 787.0, 26.0, 25.98686026843815, 0.4714517048107085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.23638042474607576, 0.64, 0.37444444444444436, 0.8696132596685083, 0.6666666666666666, 0.6655716890365125, 0.6571505682702362, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9746782], dtype=float32), -1.9178145]. 
=============================================
[2019-04-04 15:44:54,041] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[82.329094]
 [82.39016 ]
 [82.43891 ]
 [82.50518 ]
 [82.56312 ]], R is [[82.43443298]
 [82.61009216]
 [82.78398895]
 [82.95614624]
 [83.12658691]].
[2019-04-04 15:45:00,134] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1326786e-09 1.4756489e-10 4.0485600e-15 2.9631550e-14 1.0000000e+00
 4.1842860e-10 6.7151972e-15], sum to 1.0000
[2019-04-04 15:45:00,139] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4258
[2019-04-04 15:45:00,161] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 53.0, 0.0, 0.0, 26.0, 25.33248884828018, 0.3597224417864493, 0.0, 1.0, 87889.3319272514], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2845800.0000, 
sim time next is 2846400.0000, 
raw observation next is [2.0, 56.0, 0.0, 0.0, 26.0, 25.25836283364206, 0.3574670428721178, 0.0, 1.0, 64435.42045466311], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.56, 0.0, 0.0, 0.6666666666666666, 0.6048635694701717, 0.6191556809573726, 0.0, 1.0, 0.30683533549839576], 
reward next is 0.6932, 
noisyNet noise sample is [array([-1.1544484], dtype=float32), 0.86324537]. 
=============================================
[2019-04-04 15:45:01,776] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.31385219e-09 1.21414434e-09 4.08085973e-14 1.03372606e-13
 1.00000000e+00 6.39259268e-09 7.43321962e-14], sum to 1.0000
[2019-04-04 15:45:01,780] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8700
[2019-04-04 15:45:01,802] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.49151246997674, 0.4322263511537432, 0.0, 1.0, 18753.25437179721], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2843400.0000, 
sim time next is 2844000.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.4921789834992, 0.4279612051169308, 0.0, 1.0, 24450.23853140712], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.6243482486249334, 0.6426537350389769, 0.0, 1.0, 0.11642970729241486], 
reward next is 0.8836, 
noisyNet noise sample is [array([0.84567416], dtype=float32), -1.1730269]. 
=============================================
[2019-04-04 15:45:01,805] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.48462 ]
 [78.286606]
 [78.31653 ]
 [78.34413 ]
 [78.26442 ]], R is [[78.65299988]
 [78.77716827]
 [78.90007782]
 [79.02173615]
 [79.02877808]].
[2019-04-04 15:45:03,519] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0381938e-09 3.3435010e-10 4.0802661e-15 5.2517344e-14 1.0000000e+00
 3.4053774e-10 1.0437545e-14], sum to 1.0000
[2019-04-04 15:45:03,519] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9825
[2019-04-04 15:45:03,535] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 76.66666666666667, 0.0, 0.0, 26.0, 25.13505961989677, 0.3072225391194268, 0.0, 1.0, 54032.67155667538], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2857200.0000, 
sim time next is 2857800.0000, 
raw observation next is [1.0, 77.83333333333334, 0.0, 0.0, 26.0, 25.15086371746079, 0.310367288334381, 0.0, 1.0, 55493.04935958863], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7783333333333334, 0.0, 0.0, 0.6666666666666666, 0.5959053097883992, 0.6034557627781271, 0.0, 1.0, 0.2642526159980411], 
reward next is 0.7357, 
noisyNet noise sample is [array([-0.32432693], dtype=float32), 0.05082326]. 
=============================================
[2019-04-04 15:45:03,605] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.0757489e-10 6.9801040e-11 3.8850625e-16 5.0291048e-15 1.0000000e+00
 5.6335558e-11 5.9809388e-16], sum to 1.0000
[2019-04-04 15:45:03,608] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7610
[2019-04-04 15:45:03,635] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 98.66666666666667, 0.0, 0.0, 26.0, 25.38732239132503, 0.3438174238675167, 0.0, 1.0, 66909.44382439647], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3099000.0000, 
sim time next is 3099600.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.36049571673581, 0.3436663399354513, 0.0, 1.0, 59981.68281745168], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6133746430613174, 0.6145554466451505, 0.0, 1.0, 0.2856270610354842], 
reward next is 0.7144, 
noisyNet noise sample is [array([0.18428235], dtype=float32), 1.2369754]. 
=============================================
[2019-04-04 15:45:05,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5367388e-08 3.3204953e-10 1.9501305e-15 2.3372939e-14 1.0000000e+00
 8.0119841e-11 8.2643364e-16], sum to 1.0000
[2019-04-04 15:45:05,276] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9643
[2019-04-04 15:45:05,290] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.30027865718768, 0.506841977966908, 0.0, 1.0, 42609.47676954906], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3213600.0000, 
sim time next is 3214200.0000, 
raw observation next is [-1.833333333333333, 100.0, 0.0, 0.0, 26.0, 25.30983595712523, 0.5023871371912098, 0.0, 1.0, 41276.86057133339], 
processed observation next is [1.0, 0.17391304347826086, 0.41181902123730385, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6091529964271025, 0.6674623790637365, 0.0, 1.0, 0.1965564789111114], 
reward next is 0.8034, 
noisyNet noise sample is [array([0.39169902], dtype=float32), -1.4520292]. 
=============================================
[2019-04-04 15:45:07,401] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9931611e-10 3.4026813e-11 6.3000405e-17 4.0965933e-16 1.0000000e+00
 7.1718725e-12 9.0862932e-17], sum to 1.0000
[2019-04-04 15:45:07,401] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6154
[2019-04-04 15:45:07,429] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333334, 97.66666666666666, 53.83333333333333, 0.0, 26.0, 25.42408459446498, 0.3124504727787051, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2886000.0000, 
sim time next is 2886600.0000, 
raw observation next is [0.1666666666666666, 98.83333333333334, 58.66666666666666, 0.0, 26.0, 25.45939621854227, 0.3073891143854181, 1.0, 1.0, 18682.09354915611], 
processed observation next is [1.0, 0.391304347826087, 0.4672206832871654, 0.9883333333333334, 0.1955555555555555, 0.0, 0.6666666666666666, 0.6216163515451892, 0.6024630381284727, 1.0, 1.0, 0.08896235023407671], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.0382775], dtype=float32), -3.5978901]. 
=============================================
[2019-04-04 15:45:07,436] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1162007e-10 4.0610661e-11 3.0484537e-17 4.9821102e-16 1.0000000e+00
 6.6846688e-11 3.0458513e-16], sum to 1.0000
[2019-04-04 15:45:07,437] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3027
[2019-04-04 15:45:07,480] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3333333333333333, 92.33333333333333, 0.0, 0.0, 26.0, 24.91900329407454, 0.3910486897722302, 1.0, 1.0, 121877.1629080823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2918400.0000, 
sim time next is 2919000.0000, 
raw observation next is [-0.6666666666666667, 92.16666666666667, 0.0, 0.0, 26.0, 25.00380131112324, 0.4033841315444391, 1.0, 1.0, 36206.56696090028], 
processed observation next is [1.0, 0.782608695652174, 0.44413665743305636, 0.9216666666666667, 0.0, 0.0, 0.6666666666666666, 0.5836501092602701, 0.6344613771814797, 1.0, 1.0, 0.17241222362333467], 
reward next is 0.8276, 
noisyNet noise sample is [array([1.9304185], dtype=float32), 0.5474915]. 
=============================================
[2019-04-04 15:45:07,493] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[90.88098]
 [90.41722]
 [90.19473]
 [90.51796]
 [90.86068]], R is [[90.77120209]
 [90.2831192 ]
 [89.97475433]
 [90.07500458]
 [90.17425537]].
[2019-04-04 15:45:08,400] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6668934e-09 8.6591054e-11 9.8742633e-16 1.2345061e-14 1.0000000e+00
 3.3696668e-10 3.8650643e-15], sum to 1.0000
[2019-04-04 15:45:08,400] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4446
[2019-04-04 15:45:08,441] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.65605556096883, 0.6190375233076216, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3265200.0000, 
sim time next is 3265800.0000, 
raw observation next is [-4.0, 66.0, 0.0, 0.0, 26.0, 25.80504325472349, 0.6355565428277469, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.66, 0.0, 0.0, 0.6666666666666666, 0.6504202712269574, 0.7118521809425823, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2637751], dtype=float32), 0.23463093]. 
=============================================
[2019-04-04 15:45:16,131] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.0469744e-09 1.4512624e-09 8.6399012e-15 1.6722574e-13 1.0000000e+00
 2.7738645e-10 6.7574330e-14], sum to 1.0000
[2019-04-04 15:45:16,133] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1890
[2019-04-04 15:45:16,155] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.24909244646398, 0.107993788982587, 0.0, 1.0, 39340.94656820243], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3034800.0000, 
sim time next is 3035400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.21409516659471, 0.1046462672008083, 0.0, 1.0, 39491.5262837889], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5178412638828925, 0.5348820890669361, 0.0, 1.0, 0.18805488706566145], 
reward next is 0.8119, 
noisyNet noise sample is [array([-1.6526685], dtype=float32), 0.6902572]. 
=============================================
[2019-04-04 15:45:17,651] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5478286e-08 9.3928332e-10 1.9642882e-13 2.1256770e-13 1.0000000e+00
 1.5673433e-09 2.4796663e-13], sum to 1.0000
[2019-04-04 15:45:17,652] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0924
[2019-04-04 15:45:17,667] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 39.5, 84.0, 673.0, 26.0, 25.12352469557506, 0.368398371386837, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3079800.0000, 
sim time next is 3080400.0000, 
raw observation next is [0.6666666666666666, 39.66666666666666, 79.5, 641.8333333333334, 26.0, 25.14823281189685, 0.3740964751937536, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.4810710987996307, 0.39666666666666656, 0.265, 0.7092081031307551, 0.6666666666666666, 0.5956860676580709, 0.6246988250645845, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9430382], dtype=float32), 1.2212353]. 
=============================================
[2019-04-04 15:45:23,886] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.4642551e-09 6.0369787e-10 2.7674049e-15 1.7504053e-14 1.0000000e+00
 2.6578634e-10 1.9771914e-14], sum to 1.0000
[2019-04-04 15:45:23,886] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6209
[2019-04-04 15:45:23,902] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.166666666666667, 66.0, 0.0, 0.0, 26.0, 25.20658377191911, 0.392271798507756, 0.0, 1.0, 41023.17981131822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3553800.0000, 
sim time next is 3554400.0000, 
raw observation next is [-3.333333333333333, 67.0, 0.0, 0.0, 26.0, 25.17659730353362, 0.382607976813934, 0.0, 1.0, 41110.67988006001], 
processed observation next is [0.0, 0.13043478260869565, 0.37026777469990774, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5980497752944682, 0.6275359922713113, 0.0, 1.0, 0.19576514228600003], 
reward next is 0.8042, 
noisyNet noise sample is [array([1.9118618], dtype=float32), -1.5161657]. 
=============================================
[2019-04-04 15:45:25,047] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2845549e-09 6.3544149e-11 4.3962463e-15 1.4718006e-14 1.0000000e+00
 1.3208647e-10 5.0735008e-15], sum to 1.0000
[2019-04-04 15:45:25,048] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6089
[2019-04-04 15:45:25,061] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6666666666666667, 42.33333333333334, 88.66666666666667, 713.8333333333333, 26.0, 25.22149237277024, 0.4654928981664184, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3597600.0000, 
sim time next is 3598200.0000, 
raw observation next is [-0.5, 42.5, 86.0, 699.0, 26.0, 25.28047349603212, 0.4749877331430604, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.44875346260387816, 0.425, 0.2866666666666667, 0.7723756906077348, 0.6666666666666666, 0.6067061246693433, 0.6583292443810201, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4487193], dtype=float32), -0.38163248]. 
=============================================
[2019-04-04 15:45:27,362] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4561406e-09 2.7775016e-10 1.3883307e-15 7.4081637e-15 1.0000000e+00
 1.7930024e-10 4.6986310e-16], sum to 1.0000
[2019-04-04 15:45:27,362] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9019
[2019-04-04 15:45:27,387] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 50.0, 102.8333333333333, 739.0, 26.0, 26.36658249966575, 0.6727097672803732, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3336000.0000, 
sim time next is 3336600.0000, 
raw observation next is [-3.166666666666667, 50.0, 99.66666666666666, 726.0, 26.0, 26.51633012132351, 0.6920638479124014, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3748845798707295, 0.5, 0.3322222222222222, 0.8022099447513812, 0.6666666666666666, 0.7096941767769591, 0.7306879493041337, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.80951846], dtype=float32), 1.8780283]. 
=============================================
[2019-04-04 15:45:32,896] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9338774e-10 2.3319985e-11 4.5564851e-16 9.4231712e-15 1.0000000e+00
 2.6688702e-11 3.9254789e-16], sum to 1.0000
[2019-04-04 15:45:32,896] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8409
[2019-04-04 15:45:32,906] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 80.16666666666667, 0.0, 0.0, 26.0, 25.45119199550987, 0.4509446627406057, 0.0, 1.0, 41227.31387837045], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3459000.0000, 
sim time next is 3459600.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.43270626839787, 0.4489321576355005, 0.0, 1.0, 50079.02398787718], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6193921890331557, 0.6496440525451669, 0.0, 1.0, 0.23847154279941515], 
reward next is 0.7615, 
noisyNet noise sample is [array([-1.2184289], dtype=float32), 0.8664383]. 
=============================================
[2019-04-04 15:45:42,049] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3477337e-08 3.5737588e-09 4.2612581e-14 7.7116731e-13 1.0000000e+00
 8.9410923e-09 4.3884259e-13], sum to 1.0000
[2019-04-04 15:45:42,054] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3370
[2019-04-04 15:45:42,113] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.605323376597, 0.544946043248996, 1.0, 1.0, 155419.1280174859], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3954000.0000, 
sim time next is 3954600.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.62479704676812, 0.5655485904901334, 1.0, 1.0, 83885.95766714128], 
processed observation next is [1.0, 0.782608695652174, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.6353997538973433, 0.6885161968300445, 1.0, 1.0, 0.39945694127210135], 
reward next is 0.6005, 
noisyNet noise sample is [array([0.33646196], dtype=float32), 0.1682634]. 
=============================================
[2019-04-04 15:45:43,077] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7184095e-09 7.5639556e-10 5.2275070e-15 5.4577535e-14 1.0000000e+00
 2.9675898e-10 2.1977374e-14], sum to 1.0000
[2019-04-04 15:45:43,080] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2619
[2019-04-04 15:45:43,106] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 39.0, 38.5, 328.5, 26.0, 25.2034637168668, 0.403929426316335, 0.0, 1.0, 19321.37665248633], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3603600.0000, 
sim time next is 3604200.0000, 
raw observation next is [-0.1666666666666667, 39.5, 30.33333333333333, 266.3333333333333, 26.0, 25.16568931663544, 0.3927748459825636, 0.0, 1.0, 39948.18363735586], 
processed observation next is [0.0, 0.7391304347826086, 0.4579870729455217, 0.395, 0.1011111111111111, 0.29429097605893184, 0.6666666666666666, 0.5971407763862867, 0.6309249486608546, 0.0, 1.0, 0.19022944589217078], 
reward next is 0.8098, 
noisyNet noise sample is [array([0.32506573], dtype=float32), -0.14764243]. 
=============================================
[2019-04-04 15:45:44,978] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5066533e-08 1.1703090e-09 6.4083701e-15 1.5035721e-13 1.0000000e+00
 1.7699243e-10 3.6317662e-14], sum to 1.0000
[2019-04-04 15:45:44,980] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1504
[2019-04-04 15:45:45,000] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.5, 46.0, 114.0, 812.0, 26.0, 25.2371751727518, 0.4512479542332379, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3591000.0000, 
sim time next is 3591600.0000, 
raw observation next is [-1.333333333333333, 44.66666666666667, 112.0, 808.0, 26.0, 25.22170253276548, 0.4490522157295547, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.42566943674976926, 0.4466666666666667, 0.37333333333333335, 0.8928176795580111, 0.6666666666666666, 0.6018085443971234, 0.6496840719098516, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.907416], dtype=float32), -0.41625002]. 
=============================================
[2019-04-04 15:45:45,566] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7660048e-09 1.0597209e-09 8.9155259e-14 2.8772205e-13 1.0000000e+00
 2.4436082e-09 5.9953358e-14], sum to 1.0000
[2019-04-04 15:45:45,569] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8858
[2019-04-04 15:45:45,603] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8333333333333334, 41.5, 0.0, 0.0, 26.0, 25.10164136991879, 0.3647948938637637, 0.0, 1.0, 33510.58797841027], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3606600.0000, 
sim time next is 3607200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.09586133945616, 0.3675262634068896, 0.0, 1.0, 34470.88581350799], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5913217782880134, 0.6225087544689633, 0.0, 1.0, 0.164147075302419], 
reward next is 0.8359, 
noisyNet noise sample is [array([0.82033896], dtype=float32), -0.041709345]. 
=============================================
[2019-04-04 15:45:48,725] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.5155022e-10 1.3645543e-10 8.8862142e-16 2.0135247e-14 1.0000000e+00
 3.0132116e-11 1.8616937e-15], sum to 1.0000
[2019-04-04 15:45:48,726] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5973
[2019-04-04 15:45:48,750] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.033333333333333, 28.66666666666667, 0.0, 0.0, 26.0, 25.58419475653936, 0.3519460918342233, 0.0, 1.0, 18737.41503256207], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3642600.0000, 
sim time next is 3643200.0000, 
raw observation next is [8.0, 29.0, 0.0, 0.0, 26.0, 25.51857288306847, 0.3449456243649797, 0.0, 1.0, 54974.29965776751], 
processed observation next is [0.0, 0.17391304347826086, 0.6842105263157896, 0.29, 0.0, 0.0, 0.6666666666666666, 0.6265477402557057, 0.6149818747883266, 0.0, 1.0, 0.26178237932270243], 
reward next is 0.7382, 
noisyNet noise sample is [array([-0.426735], dtype=float32), 0.086790785]. 
=============================================
[2019-04-04 15:45:49,916] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0994962e-10 4.5923387e-11 1.4314281e-16 1.0864099e-14 1.0000000e+00
 9.4451794e-11 2.8493326e-15], sum to 1.0000
[2019-04-04 15:45:49,918] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7643
[2019-04-04 15:45:49,944] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 70.83333333333333, 0.0, 0.0, 26.0, 25.4244486039148, 0.413240672948575, 0.0, 1.0, 39837.13643324989], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3708600.0000, 
sim time next is 3709200.0000, 
raw observation next is [-1.0, 69.66666666666667, 0.0, 0.0, 26.0, 25.43213136401057, 0.4097778433212509, 0.0, 1.0, 33758.52328124882], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.6193442803342141, 0.6365926144404169, 0.0, 1.0, 0.16075487276785153], 
reward next is 0.8392, 
noisyNet noise sample is [array([0.4660287], dtype=float32), 1.0817517]. 
=============================================
[2019-04-04 15:45:58,978] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8136157e-09 4.1397044e-10 7.3511016e-15 7.2491838e-14 1.0000000e+00
 2.1823503e-09 1.6290105e-14], sum to 1.0000
[2019-04-04 15:45:58,979] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1995
[2019-04-04 15:45:58,993] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.833333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 25.38679052137896, 0.4470514642016919, 0.0, 1.0, 55318.00903982891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3887400.0000, 
sim time next is 3888000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.35253850587892, 0.4435506184995093, 0.0, 1.0, 55932.7347462754], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6127115421565765, 0.6478502061665031, 0.0, 1.0, 0.26634635593464473], 
reward next is 0.7337, 
noisyNet noise sample is [array([-1.0536613], dtype=float32), 0.17095362]. 
=============================================
[2019-04-04 15:45:59,014] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.00286]
 [79.94273]
 [80.05021]
 [80.20017]
 [80.33977]], R is [[79.89677429]
 [79.83438873]
 [79.89827728]
 [80.00994873]
 [80.10256195]].
[2019-04-04 15:45:59,699] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.25466095e-09 5.21648447e-10 5.57215034e-15 1.16524984e-14
 1.00000000e+00 1.66484571e-09 2.17075274e-15], sum to 1.0000
[2019-04-04 15:45:59,700] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4175
[2019-04-04 15:45:59,716] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.833333333333334, 49.0, 108.6666666666667, 747.3333333333334, 26.0, 26.31310518176711, 0.5777571459698381, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3924600.0000, 
sim time next is 3925200.0000, 
raw observation next is [-6.666666666666667, 49.0, 110.8333333333333, 761.1666666666667, 26.0, 26.3373231408349, 0.5852446452128477, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.27793167128347185, 0.49, 0.36944444444444435, 0.8410681399631676, 0.6666666666666666, 0.6947769284029084, 0.6950815484042826, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3412851], dtype=float32), -0.2244577]. 
=============================================
[2019-04-04 15:46:08,271] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.4471754e-08 1.9378678e-08 3.1276281e-13 2.7365542e-13 9.9999988e-01
 5.7852581e-09 2.2938850e-13], sum to 1.0000
[2019-04-04 15:46:08,271] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7331
[2019-04-04 15:46:08,301] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 30.0, 0.0, 0.0, 26.0, 25.44476501122643, 0.5102788438930119, 0.0, 1.0, 129278.3860222225], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4048200.0000, 
sim time next is 4048800.0000, 
raw observation next is [-4.0, 29.66666666666666, 0.0, 0.0, 26.0, 25.47823706627354, 0.521852799170247, 0.0, 1.0, 51730.45138291249], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.29666666666666663, 0.0, 0.0, 0.6666666666666666, 0.6231864221894616, 0.6739509330567489, 0.0, 1.0, 0.24633548277577377], 
reward next is 0.7537, 
noisyNet noise sample is [array([-1.40216], dtype=float32), 0.58713603]. 
=============================================
[2019-04-04 15:46:09,112] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1489237e-09 5.5139004e-10 7.6789741e-16 1.7902650e-14 1.0000000e+00
 9.1485469e-10 5.8312575e-16], sum to 1.0000
[2019-04-04 15:46:09,114] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7072
[2019-04-04 15:46:09,127] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 40.66666666666667, 0.0, 0.0, 26.0, 25.27357187326758, 0.4595180680933695, 0.0, 1.0, 196437.5136303064], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4134000.0000, 
sim time next is 4134600.0000, 
raw observation next is [1.0, 39.5, 0.0, 0.0, 26.0, 25.22365366529753, 0.4795293839256156, 0.0, 1.0, 197916.7503977896], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.395, 0.0, 0.0, 0.6666666666666666, 0.6019711387747941, 0.6598431279752052, 0.0, 1.0, 0.9424607161799505], 
reward next is 0.0575, 
noisyNet noise sample is [array([1.0241082], dtype=float32), -0.38355282]. 
=============================================
[2019-04-04 15:46:09,892] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.9813363e-09 1.4498045e-09 1.6803458e-14 2.1885692e-13 1.0000000e+00
 2.6384828e-10 1.0321755e-13], sum to 1.0000
[2019-04-04 15:46:09,892] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5076
[2019-04-04 15:46:09,906] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 51.5, 0.0, 0.0, 26.0, 24.69487281820997, 0.2098145453249042, 0.0, 1.0, 40243.73240846171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4174200.0000, 
sim time next is 4174800.0000, 
raw observation next is [-5.0, 52.33333333333334, 15.33333333333333, 81.33333333333331, 26.0, 24.64942613646074, 0.2154504997965918, 0.0, 1.0, 40288.6637722446], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.5233333333333334, 0.0511111111111111, 0.0898710865561694, 0.6666666666666666, 0.5541188447050617, 0.5718168332655306, 0.0, 1.0, 0.19185077986783144], 
reward next is 0.8081, 
noisyNet noise sample is [array([-0.71601266], dtype=float32), 0.82302326]. 
=============================================
[2019-04-04 15:46:15,295] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.4748872e-09 5.9178196e-10 2.9728112e-15 5.2917710e-15 1.0000000e+00
 4.4986062e-10 6.8219868e-15], sum to 1.0000
[2019-04-04 15:46:15,300] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0469
[2019-04-04 15:46:15,324] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 53.66666666666667, 0.0, 0.0, 26.0, 25.08835576830734, 0.4463976755175772, 0.0, 1.0, 198402.2911002453], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4566000.0000, 
sim time next is 4566600.0000, 
raw observation next is [2.0, 54.5, 0.0, 0.0, 26.0, 25.09001162693638, 0.4728802762754299, 0.0, 1.0, 195338.9187195869], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.545, 0.0, 0.0, 0.6666666666666666, 0.5908343022446984, 0.6576267587584766, 0.0, 1.0, 0.930185327236128], 
reward next is 0.0698, 
noisyNet noise sample is [array([1.0931249], dtype=float32), 1.4696319]. 
=============================================
[2019-04-04 15:46:15,736] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3897496e-09 5.8125393e-11 1.1892470e-16 5.6384353e-15 1.0000000e+00
 2.7636969e-11 1.3109649e-15], sum to 1.0000
[2019-04-04 15:46:15,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3383
[2019-04-04 15:46:15,768] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.5, 76.0, 0.0, 0.0, 26.0, 25.45327572439852, 0.4091464145640601, 0.0, 1.0, 59604.60207014019], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4320000.0000, 
sim time next is 4320600.0000, 
raw observation next is [4.45, 75.83333333333334, 0.0, 0.0, 26.0, 25.49833872138366, 0.414680359434894, 0.0, 1.0, 21812.8145124973], 
processed observation next is [1.0, 0.0, 0.5858725761772854, 0.7583333333333334, 0.0, 0.0, 0.6666666666666666, 0.624861560115305, 0.638226786478298, 0.0, 1.0, 0.1038705452976062], 
reward next is 0.8961, 
noisyNet noise sample is [array([0.28242028], dtype=float32), 0.7131666]. 
=============================================
[2019-04-04 15:46:32,030] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7109945e-08 1.4062443e-09 4.0052583e-14 7.7612348e-14 1.0000000e+00
 5.3831717e-10 3.6370487e-14], sum to 1.0000
[2019-04-04 15:46:32,030] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7222
[2019-04-04 15:46:32,041] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8333333333333334, 40.5, 0.0, 0.0, 26.0, 25.38988816387315, 0.3417385555975074, 0.0, 1.0, 38545.67553263682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4925400.0000, 
sim time next is 4926000.0000, 
raw observation next is [0.6666666666666667, 41.0, 0.0, 0.0, 26.0, 25.37062772014415, 0.3382371363210934, 0.0, 1.0, 48175.60118729259], 
processed observation next is [1.0, 0.0, 0.4810710987996307, 0.41, 0.0, 0.0, 0.6666666666666666, 0.6142189766786791, 0.6127457121070311, 0.0, 1.0, 0.22940762470139328], 
reward next is 0.7706, 
noisyNet noise sample is [array([0.56262577], dtype=float32), -0.24353975]. 
=============================================
[2019-04-04 15:46:32,066] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.07907 ]
 [77.82485 ]
 [77.67813 ]
 [77.69683 ]
 [77.751274]], R is [[78.46628571]
 [78.49807739]
 [78.56719208]
 [78.60094452]
 [78.67237854]].
[2019-04-04 15:46:36,923] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.7131172e-09 3.6449539e-09 6.3707610e-15 3.0818108e-14 1.0000000e+00
 1.1272712e-09 1.2371508e-14], sum to 1.0000
[2019-04-04 15:46:36,933] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4056
[2019-04-04 15:46:36,943] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 50.83333333333334, 0.0, 0.0, 26.0, 25.40576046550209, 0.3879413481717256, 0.0, 1.0, 33805.45743529902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5028600.0000, 
sim time next is 5029200.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.38558196431818, 0.3938177509568068, 0.0, 1.0, 45968.34540372325], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6154651636931817, 0.6312725836522689, 0.0, 1.0, 0.21889688287487263], 
reward next is 0.7811, 
noisyNet noise sample is [array([0.28678575], dtype=float32), -0.32431838]. 
=============================================
[2019-04-04 15:46:38,547] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.0220831e-10 6.2987379e-11 1.1376653e-16 1.6556865e-14 1.0000000e+00
 3.4606523e-11 1.7726024e-15], sum to 1.0000
[2019-04-04 15:46:38,548] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6706
[2019-04-04 15:46:38,559] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.67583451659301, 0.2766512762858221, 0.0, 1.0, 40588.72060253459], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4763400.0000, 
sim time next is 4764000.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.61516604627982, 0.2676755087886742, 0.0, 1.0, 40634.84955510809], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5512638371899851, 0.5892251695962247, 0.0, 1.0, 0.19349928359575283], 
reward next is 0.8065, 
noisyNet noise sample is [array([2.1797566], dtype=float32), 0.5751688]. 
=============================================
[2019-04-04 15:46:38,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.82769 ]
 [81.54749 ]
 [81.436005]
 [81.28787 ]
 [81.29129 ]], R is [[81.7698288 ]
 [81.7588501 ]
 [81.74817657]
 [81.73770905]
 [81.72731781]].
[2019-04-04 15:46:39,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:46:39,647] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:46:39,665] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run29
[2019-04-04 15:46:40,235] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7345865e-10 2.3605902e-11 1.5233230e-16 1.0064830e-15 1.0000000e+00
 8.5513055e-11 1.3595155e-16], sum to 1.0000
[2019-04-04 15:46:40,235] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9192
[2019-04-04 15:46:40,295] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.01826485269318, 0.4458152988725658, 1.0, 1.0, 95487.87525759239], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4734000.0000, 
sim time next is 4734600.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.07691612303338, 0.4731474510730284, 1.0, 1.0, 25706.1077613349], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5897430102527818, 0.6577158170243428, 1.0, 1.0, 0.12241003695873762], 
reward next is 0.8776, 
noisyNet noise sample is [array([1.5148565], dtype=float32), 1.0323749]. 
=============================================
[2019-04-04 15:46:41,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:46:41,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:46:41,187] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run29
[2019-04-04 15:46:41,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:46:41,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:46:41,470] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run29
[2019-04-04 15:46:41,905] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-04 15:46:41,907] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 15:46:41,907] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 15:46:41,907] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:46:41,908] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:46:41,908] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 15:46:41,908] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:46:41,913] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run39
[2019-04-04 15:46:41,932] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run39
[2019-04-04 15:46:41,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run39
[2019-04-04 15:46:42,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:46:42,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:46:42,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run29
[2019-04-04 15:48:11,720] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.1777933], dtype=float32), 0.2319004]
[2019-04-04 15:48:11,720] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83074054190701, 0.2871864921064027, 0.0, 1.0, 41104.55487610382]
[2019-04-04 15:48:11,720] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 15:48:11,721] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.9282560e-09 7.8972884e-10 7.3952312e-15 1.1722546e-13 1.0000000e+00
 3.8568115e-10 1.4258434e-14], sampled 0.3835239928430919
[2019-04-04 15:48:22,871] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1777933], dtype=float32), 0.2319004]
[2019-04-04 15:48:22,871] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.75, 38.5, 0.0, 0.0, 26.0, 25.37370665835566, 0.375953695091062, 0.0, 1.0, 41638.84802416177]
[2019-04-04 15:48:22,871] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 15:48:22,872] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.0039795e-09 1.1515124e-09 1.0344671e-14 8.8217465e-14 1.0000000e+00
 3.3273168e-10 1.7185511e-14], sampled 0.6001697452171814
[2019-04-04 15:48:24,710] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 15:48:42,438] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 15:48:46,315] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7881 275774498.6820 1233.0964
[2019-04-04 15:48:47,339] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 3800000, evaluation results [3800000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.788101514333, 275774498.68198794, 1233.096368464637]
[2019-04-04 15:48:51,033] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.1732926e-09 1.5762291e-09 2.3846525e-14 2.1064052e-13 1.0000000e+00
 6.3059180e-10 7.0339534e-14], sum to 1.0000
[2019-04-04 15:48:51,034] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0936
[2019-04-04 15:48:51,056] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.44294967077453, 0.3696935310809055, 0.0, 1.0, 18760.72606251976], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4839000.0000, 
sim time next is 4839600.0000, 
raw observation next is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.37696557602089, 0.3583675580628506, 0.0, 1.0, 59737.57759300021], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6000000000000001, 0.0, 0.0, 0.6666666666666666, 0.6147471313350742, 0.6194558526876168, 0.0, 1.0, 0.28446465520476294], 
reward next is 0.7155, 
noisyNet noise sample is [array([0.24477112], dtype=float32), -0.19109885]. 
=============================================
[2019-04-04 15:48:52,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.7747194e-09 1.0955853e-09 1.6194069e-14 2.2152840e-13 1.0000000e+00
 3.7087128e-10 4.9989442e-14], sum to 1.0000
[2019-04-04 15:48:52,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2073
[2019-04-04 15:48:52,504] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.30702256213702, 0.298429718990426, 0.0, 1.0, 75984.14160278325], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4935000.0000, 
sim time next is 4935600.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.29345449288046, 0.2949968820420242, 0.0, 1.0, 54685.57491511018], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.607787874406705, 0.598332294014008, 0.0, 1.0, 0.26040749959576276], 
reward next is 0.7396, 
noisyNet noise sample is [array([1.6694957], dtype=float32), 3.2740831]. 
=============================================
[2019-04-04 15:48:53,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.4847327e-09 4.4754647e-10 3.2722995e-16 1.5442854e-14 1.0000000e+00
 2.0469115e-10 5.2288207e-16], sum to 1.0000
[2019-04-04 15:48:53,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1562
[2019-04-04 15:48:53,575] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.5, 24.5, 123.0, 865.0, 26.0, 27.08883159375713, 0.7273320717520959, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4969800.0000, 
sim time next is 4970400.0000, 
raw observation next is [6.666666666666666, 24.33333333333334, 122.0, 864.1666666666667, 26.0, 27.08224397490171, 0.7299344693302153, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6472760849492153, 0.2433333333333334, 0.4066666666666667, 0.9548802946593002, 0.6666666666666666, 0.7568536645751426, 0.7433114897767384, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10424113], dtype=float32), -0.4020522]. 
=============================================
[2019-04-04 15:48:54,476] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.1434735e-09 5.0254545e-10 2.4073089e-15 8.5586801e-15 1.0000000e+00
 7.5978009e-11 4.2635082e-15], sum to 1.0000
[2019-04-04 15:48:54,476] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7286
[2019-04-04 15:48:54,498] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.066666666666667, 46.66666666666667, 281.6666666666666, 362.6666666666667, 26.0, 25.0530032667596, 0.3570284974588975, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4882200.0000, 
sim time next is 4882800.0000, 
raw observation next is [1.133333333333333, 46.33333333333334, 281.3333333333334, 376.3333333333333, 26.0, 25.09101525188339, 0.3629240998646095, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.49399815327793173, 0.46333333333333343, 0.937777777777778, 0.4158379373848987, 0.6666666666666666, 0.5909179376569492, 0.6209746999548699, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3580112], dtype=float32), 0.74439377]. 
=============================================
[2019-04-04 15:48:56,795] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3802142e-08 4.0724860e-10 1.7479331e-14 9.7240805e-14 1.0000000e+00
 2.4999100e-10 2.8224622e-14], sum to 1.0000
[2019-04-04 15:48:56,795] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8175
[2019-04-04 15:48:56,820] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6666666666666667, 42.5, 0.0, 0.0, 26.0, 25.42889838656133, 0.4364461506239108, 0.0, 1.0, 40243.20921118955], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5019000.0000, 
sim time next is 5019600.0000, 
raw observation next is [0.3333333333333334, 45.0, 0.0, 0.0, 26.0, 25.46852320406896, 0.445084016967152, 0.0, 1.0, 18756.26270799537], 
processed observation next is [1.0, 0.08695652173913043, 0.4718374884579871, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6223769336724132, 0.6483613389890507, 0.0, 1.0, 0.08931553670473986], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.54355925], dtype=float32), -0.28154337]. 
=============================================
[2019-04-04 15:48:58,680] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.8950943e-09 1.8855414e-10 4.3652635e-15 2.5995492e-14 1.0000000e+00
 9.3407193e-10 2.9655694e-14], sum to 1.0000
[2019-04-04 15:48:58,682] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9267
[2019-04-04 15:48:58,697] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4616948e-08 2.5329614e-09 5.6151921e-14 4.1100925e-13 1.0000000e+00
 8.4706349e-09 4.2358701e-14], sum to 1.0000
[2019-04-04 15:48:58,698] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4889
[2019-04-04 15:48:58,709] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.666666666666667, 24.0, 0.0, 0.0, 26.0, 25.98718362980066, 0.5711924483816301, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4997400.0000, 
sim time next is 4998000.0000, 
raw observation next is [5.333333333333333, 25.0, 0.0, 0.0, 26.0, 25.91691967009894, 0.5553931147604206, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6103416435826409, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6597433058415785, 0.6851310382534735, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2689964], dtype=float32), -0.69237596]. 
=============================================
[2019-04-04 15:48:58,728] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[77.56867 ]
 [78.18095 ]
 [79.323715]
 [80.42987 ]
 [80.01733 ]], R is [[77.37703705]
 [77.60326385]
 [77.82723236]
 [78.04895782]
 [78.26847076]].
[2019-04-04 15:48:58,742] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.2, 66.66666666666666, 0.0, 0.0, 26.0, 25.03464077039532, 0.2968240949860606, 1.0, 1.0, 97352.64524469146], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 157200.0000, 
sim time next is 157800.0000, 
raw observation next is [-8.3, 67.33333333333334, 0.0, 0.0, 26.0, 24.98629882654301, 0.3043687129411883, 1.0, 1.0, 110398.8680526219], 
processed observation next is [1.0, 0.8260869565217391, 0.23268698060941828, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.5821915688785841, 0.6014562376470628, 1.0, 1.0, 0.5257088954886757], 
reward next is 0.4743, 
noisyNet noise sample is [array([-0.1398498], dtype=float32), 1.9544125]. 
=============================================
[2019-04-04 15:48:59,881] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:48:59,881] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:48:59,915] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run29
[2019-04-04 15:49:00,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:49:00,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:49:00,066] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run29
[2019-04-04 15:49:00,600] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:49:00,600] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:49:00,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run29
[2019-04-04 15:49:01,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:49:01,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:49:01,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run29
[2019-04-04 15:49:01,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:49:01,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:49:01,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run29
[2019-04-04 15:49:02,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:49:02,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:49:02,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run29
[2019-04-04 15:49:03,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:49:03,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:49:03,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run29
[2019-04-04 15:49:04,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:49:04,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:49:04,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run29
[2019-04-04 15:49:04,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:49:04,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:49:04,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run29
[2019-04-04 15:49:04,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:49:04,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:49:04,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run29
[2019-04-04 15:49:04,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:49:04,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:49:04,987] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run29
[2019-04-04 15:49:05,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:49:05,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:49:05,442] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run29
[2019-04-04 15:49:13,928] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9855133e-10 3.2508333e-11 6.7922474e-17 2.0288749e-15 1.0000000e+00
 1.9157414e-11 1.5341472e-15], sum to 1.0000
[2019-04-04 15:49:13,929] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8976
[2019-04-04 15:49:13,998] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 60.0, 0.0, 26.0, 23.78553752603829, -0.002750682232440021, 0.0, 1.0, 56865.43220627758], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 37800.0000, 
sim time next is 38400.0000, 
raw observation next is [7.699999999999999, 93.0, 62.5, 0.0, 26.0, 23.91365207261305, 0.016836052997192, 0.0, 1.0, 56686.95043770633], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.20833333333333334, 0.0, 0.6666666666666666, 0.49280433938442086, 0.5056120176657307, 0.0, 1.0, 0.269937859227173], 
reward next is 0.7301, 
noisyNet noise sample is [array([0.010138], dtype=float32), -1.24892]. 
=============================================
[2019-04-04 15:49:19,466] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3784885e-10 8.4833023e-11 8.5001736e-16 6.1440437e-15 1.0000000e+00
 1.3822307e-10 5.0378772e-16], sum to 1.0000
[2019-04-04 15:49:19,466] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7827
[2019-04-04 15:49:19,509] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 91.0, 0.0, 0.0, 26.0, 24.28794645905896, 0.1270787386328958, 0.0, 1.0, 41630.59243988179], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 92400.0000, 
sim time next is 93000.0000, 
raw observation next is [-1.516666666666667, 91.0, 0.0, 0.0, 26.0, 24.24914125700089, 0.1243163986578136, 0.0, 1.0, 41820.298856722], 
processed observation next is [1.0, 0.043478260869565216, 0.4205909510618652, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5207617714167408, 0.5414387995526045, 0.0, 1.0, 0.19914428027010478], 
reward next is 0.8009, 
noisyNet noise sample is [array([0.44900697], dtype=float32), -0.9816849]. 
=============================================
[2019-04-04 15:49:19,534] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[85.4106  ]
 [86.128426]
 [86.45483 ]
 [86.564064]
 [86.93958 ]], R is [[85.34021759]
 [85.28857422]
 [85.23844147]
 [85.18977356]
 [85.14264679]].
[2019-04-04 15:49:21,027] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4429196e-08 2.9384661e-09 1.2808884e-13 7.8955137e-13 1.0000000e+00
 5.9251413e-09 7.2336451e-14], sum to 1.0000
[2019-04-04 15:49:21,027] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6343
[2019-04-04 15:49:21,058] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 70.5, 0.0, 0.0, 26.0, 24.34155138611569, 0.1532674099610606, 0.0, 1.0, 45709.03313846522], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 165000.0000, 
sim time next is 165600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.29119221619356, 0.1435314270934556, 0.0, 1.0, 45292.54869804063], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5242660180161302, 0.5478438090311518, 0.0, 1.0, 0.215678803324003], 
reward next is 0.7843, 
noisyNet noise sample is [array([1.376342], dtype=float32), -0.7026964]. 
=============================================
[2019-04-04 15:50:23,951] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4744476e-10 4.7518873e-12 1.3935391e-18 8.3133101e-17 1.0000000e+00
 3.6039937e-13 3.7787087e-18], sum to 1.0000
[2019-04-04 15:50:23,955] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5158
[2019-04-04 15:50:23,971] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.2, 83.0, 35.0, 96.5, 26.0, 26.32381137467227, 0.7129623698242654, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1068000.0000, 
sim time next is 1068600.0000, 
raw observation next is [12.2, 83.0, 48.0, 124.0, 26.0, 26.49874356104388, 0.7238223024156833, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.16, 0.13701657458563535, 0.6666666666666666, 0.7082286300869901, 0.7412741008052278, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.143719], dtype=float32), -0.6076689]. 
=============================================
[2019-04-04 15:50:26,825] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.3480221e-10 2.0946415e-10 1.6579222e-16 1.6089728e-14 1.0000000e+00
 1.9333498e-11 3.9133235e-16], sum to 1.0000
[2019-04-04 15:50:26,826] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7864
[2019-04-04 15:50:26,840] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.083333333333334, 81.50000000000001, 0.0, 0.0, 26.0, 25.69082664978889, 0.5051466383205526, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1577400.0000, 
sim time next is 1578000.0000, 
raw observation next is [5.166666666666667, 81.0, 0.0, 0.0, 26.0, 25.63322489345904, 0.4874029280722438, 0.0, 1.0, 22163.70638999898], 
processed observation next is [1.0, 0.2608695652173913, 0.6057248384118191, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6361020744549201, 0.662467642690748, 0.0, 1.0, 0.10554145899999515], 
reward next is 0.8945, 
noisyNet noise sample is [array([0.09074619], dtype=float32), -1.4612598]. 
=============================================
[2019-04-04 15:50:26,855] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[88.38837 ]
 [88.59981 ]
 [88.786415]
 [88.972824]
 [89.14952 ]], R is [[88.29520416]
 [88.41225433]
 [88.52812958]
 [88.64285278]
 [88.75642395]].
[2019-04-04 15:50:27,714] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3725949e-09 1.5422251e-10 7.7114409e-16 7.0614532e-15 1.0000000e+00
 1.2744591e-10 7.0430790e-15], sum to 1.0000
[2019-04-04 15:50:27,719] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2377
[2019-04-04 15:50:27,730] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [19.4, 49.0, 108.0, 0.0, 26.0, 27.7723280968949, 0.996548857180167, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1091400.0000, 
sim time next is 1092000.0000, 
raw observation next is [19.4, 49.0, 100.0, 0.0, 26.0, 27.72210473918631, 0.8905334312526968, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.3333333333333333, 0.0, 0.6666666666666666, 0.8101753949321925, 0.7968444770842322, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09437224], dtype=float32), 1.1812054]. 
=============================================
[2019-04-04 15:50:27,751] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[83.03579 ]
 [83.02276 ]
 [82.989716]
 [82.8424  ]
 [83.73627 ]], R is [[83.22657776]
 [83.39431   ]
 [83.5603714 ]
 [83.72476959]
 [83.88751984]].
[2019-04-04 15:50:41,641] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.01578901e-10 1.10952504e-10 1.85099992e-16 4.66171274e-15
 1.00000000e+00 2.11849191e-10 1.15592045e-16], sum to 1.0000
[2019-04-04 15:50:41,642] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4372
[2019-04-04 15:50:41,654] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.41128669687094, 0.4829851139718651, 0.0, 1.0, 59419.24199474146], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1386000.0000, 
sim time next is 1386600.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.30777632838214, 0.4852049472493903, 0.0, 1.0, 72306.19840756057], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6089813606985116, 0.6617349824164634, 0.0, 1.0, 0.3443152305121932], 
reward next is 0.6557, 
noisyNet noise sample is [array([0.36126795], dtype=float32), 0.25040585]. 
=============================================
[2019-04-04 15:50:46,995] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.5302785e-10 3.6088604e-11 3.7277716e-16 2.5638773e-15 1.0000000e+00
 4.5721319e-11 2.5865488e-16], sum to 1.0000
[2019-04-04 15:50:46,995] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8154
[2019-04-04 15:50:47,003] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.16666666666667, 59.33333333333334, 0.0, 0.0, 26.0, 26.53284822198952, 0.70295643602853, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1532400.0000, 
sim time next is 1533000.0000, 
raw observation next is [10.08333333333333, 59.66666666666666, 0.0, 0.0, 26.0, 26.43657216134831, 0.699008151533376, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7419205909510619, 0.5966666666666666, 0.0, 0.0, 0.6666666666666666, 0.7030476801123591, 0.7330027171777921, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24952991], dtype=float32), 0.49630326]. 
=============================================
[2019-04-04 15:50:47,010] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[89.22417]
 [89.3372 ]
 [89.58045]
 [90.03738]
 [90.32005]], R is [[89.28266907]
 [89.3898468 ]
 [89.49594879]
 [89.6009903 ]
 [89.70497894]].
[2019-04-04 15:51:03,578] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7927274e-09 3.7003398e-10 3.3336989e-15 3.2164501e-14 1.0000000e+00
 2.1371364e-10 4.1521805e-15], sum to 1.0000
[2019-04-04 15:51:03,579] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9267
[2019-04-04 15:51:03,595] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 81.50000000000001, 0.0, 0.0, 26.0, 24.68498930614181, 0.2254818370118452, 0.0, 1.0, 45534.38348996757], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1811400.0000, 
sim time next is 1812000.0000, 
raw observation next is [-5.0, 81.0, 0.0, 0.0, 26.0, 24.65318455613781, 0.2188015343205, 0.0, 1.0, 45499.1794824856], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.81, 0.0, 0.0, 0.6666666666666666, 0.5544320463448175, 0.5729338447735001, 0.0, 1.0, 0.2166627594404076], 
reward next is 0.7833, 
noisyNet noise sample is [array([-0.22279373], dtype=float32), 0.22302894]. 
=============================================
[2019-04-04 15:51:03,606] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[79.83027]
 [79.94832]
 [80.04888]
 [80.15487]
 [80.25907]], R is [[79.6786499 ]
 [79.66503906]
 [79.65132141]
 [79.63746643]
 [79.62348175]].
[2019-04-04 15:51:04,513] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6996272e-09 6.5773714e-10 2.7007225e-14 3.6862031e-13 1.0000000e+00
 2.1557158e-09 5.4976433e-14], sum to 1.0000
[2019-04-04 15:51:04,514] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3421
[2019-04-04 15:51:04,550] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.916666666666667, 67.5, 99.0, 0.0, 26.0, 26.30314995717199, 0.502360010045875, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2128200.0000, 
sim time next is 2128800.0000, 
raw observation next is [-4.833333333333334, 67.0, 92.5, 0.0, 26.0, 26.28267467743422, 0.4917456713990327, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.32871652816251157, 0.67, 0.30833333333333335, 0.0, 0.6666666666666666, 0.690222889786185, 0.6639152237996776, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.79997396], dtype=float32), 0.97910184]. 
=============================================
[2019-04-04 15:51:05,228] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3105597e-08 8.4833873e-10 1.1832794e-14 8.0217429e-14 1.0000000e+00
 3.5713399e-09 3.7839815e-14], sum to 1.0000
[2019-04-04 15:51:05,229] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0065
[2019-04-04 15:51:05,251] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.783333333333333, 83.0, 0.0, 0.0, 26.0, 25.26802809212501, 0.4175395957621428, 0.0, 1.0, 43317.65533994371], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2149800.0000, 
sim time next is 2150400.0000, 
raw observation next is [-5.966666666666667, 83.0, 0.0, 0.0, 26.0, 25.27757842857199, 0.4136724920858625, 0.0, 1.0, 43086.33290552856], 
processed observation next is [1.0, 0.9130434782608695, 0.2973222530009234, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6064648690476657, 0.6378908306952875, 0.0, 1.0, 0.20517301383585027], 
reward next is 0.7948, 
noisyNet noise sample is [array([-0.59927106], dtype=float32), 0.96145797]. 
=============================================
[2019-04-04 15:51:08,915] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.13904655e-08 5.66058311e-10 6.57660224e-15 9.75983075e-14
 1.00000000e+00 5.64570279e-10 6.72342482e-15], sum to 1.0000
[2019-04-04 15:51:08,915] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2661
[2019-04-04 15:51:08,954] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.449999999999999, 77.0, 171.0, 236.0, 26.0, 25.83542378510067, 0.3337884113693181, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1938600.0000, 
sim time next is 1939200.0000, 
raw observation next is [-6.166666666666666, 76.33333333333334, 181.1666666666667, 198.3333333333333, 26.0, 25.80757242495277, 0.3218550528642867, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.29178208679593726, 0.7633333333333334, 0.603888888888889, 0.21915285451197047, 0.6666666666666666, 0.6506310354127308, 0.607285017621429, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22445159], dtype=float32), 0.27926928]. 
=============================================
[2019-04-04 15:51:10,329] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.8142700e-09 1.8336126e-09 2.6402645e-14 1.1526792e-13 1.0000000e+00
 4.7384968e-10 5.5652812e-14], sum to 1.0000
[2019-04-04 15:51:10,331] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2207
[2019-04-04 15:51:10,350] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.933333333333334, 77.66666666666667, 0.0, 0.0, 26.0, 24.49183735239777, 0.1241383748052975, 0.0, 1.0, 44897.38838181293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1896000.0000, 
sim time next is 1896600.0000, 
raw observation next is [-7.116666666666666, 78.33333333333334, 0.0, 0.0, 26.0, 24.45067985141205, 0.1163085635620002, 0.0, 1.0, 44905.18816883703], 
processed observation next is [0.0, 0.9565217391304348, 0.265466297322253, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5375566542843376, 0.5387695211873335, 0.0, 1.0, 0.2138342293754144], 
reward next is 0.7862, 
noisyNet noise sample is [array([0.8899983], dtype=float32), 0.44605625]. 
=============================================
[2019-04-04 15:51:12,558] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1321317e-08 5.7495925e-10 2.3757451e-14 1.6671110e-13 1.0000000e+00
 1.6659089e-10 2.9730453e-14], sum to 1.0000
[2019-04-04 15:51:12,559] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2162
[2019-04-04 15:51:12,580] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.64377923457404, 0.2224067973328989, 0.0, 1.0, 42988.66106629596], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1984200.0000, 
sim time next is 1984800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.59951405724538, 0.2137603271376856, 0.0, 1.0, 42957.58993308824], 
processed observation next is [1.0, 1.0, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5499595047704483, 0.5712534423792285, 0.0, 1.0, 0.20455995206232494], 
reward next is 0.7954, 
noisyNet noise sample is [array([-0.37973952], dtype=float32), -0.8906501]. 
=============================================
[2019-04-04 15:51:14,147] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2607082e-09 1.0138423e-09 1.9714336e-14 1.4504348e-13 1.0000000e+00
 1.1877912e-09 9.2337222e-15], sum to 1.0000
[2019-04-04 15:51:14,147] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2049
[2019-04-04 15:51:14,190] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.65, 63.5, 137.0, 0.0, 26.0, 25.54633656131591, 0.3306396502598146, 1.0, 1.0, 23294.03547501322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1949400.0000, 
sim time next is 1950000.0000, 
raw observation next is [-3.566666666666666, 63.0, 132.8333333333333, 0.0, 26.0, 25.66003501195884, 0.3377456549579564, 1.0, 1.0, 23106.17931630252], 
processed observation next is [1.0, 0.5652173913043478, 0.3638042474607572, 0.63, 0.4427777777777776, 0.0, 0.6666666666666666, 0.63833625099657, 0.6125818849859854, 1.0, 1.0, 0.11002942531572629], 
reward next is 0.8900, 
noisyNet noise sample is [array([-0.6038275], dtype=float32), -0.0320495]. 
=============================================
[2019-04-04 15:51:14,203] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[79.10525 ]
 [79.60519 ]
 [80.0909  ]
 [80.56578 ]
 [80.946014]], R is [[78.79750061]
 [78.89860535]
 [78.99994659]
 [79.10389709]
 [79.21157837]].
[2019-04-04 15:51:21,709] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5721018e-09 3.8413994e-11 2.6246578e-15 7.6028161e-15 1.0000000e+00
 4.8979640e-11 1.6127726e-15], sum to 1.0000
[2019-04-04 15:51:21,709] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6736
[2019-04-04 15:51:21,779] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.8, 82.0, 157.0, 104.5, 26.0, 25.57576451596207, 0.3607683411922025, 1.0, 1.0, 27254.7791225034], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2107200.0000, 
sim time next is 2107800.0000, 
raw observation next is [-7.8, 82.0, 174.0, 118.0, 26.0, 25.66157323408878, 0.3846013506150874, 1.0, 1.0, 32848.6373929791], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.82, 0.58, 0.13038674033149172, 0.6666666666666666, 0.6384644361740651, 0.6282004502050291, 1.0, 1.0, 0.15642208282371], 
reward next is 0.8436, 
noisyNet noise sample is [array([0.02543396], dtype=float32), -0.28705087]. 
=============================================
[2019-04-04 15:51:40,081] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2348372e-08 1.4782611e-09 2.8755898e-14 8.6865024e-14 1.0000000e+00
 2.6061386e-09 1.4326122e-14], sum to 1.0000
[2019-04-04 15:51:40,081] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3029
[2019-04-04 15:51:40,141] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.45, 55.0, 0.0, 0.0, 26.0, 24.9377500540395, 0.3296113558886202, 0.0, 1.0, 117124.5897400343], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2316600.0000, 
sim time next is 2317200.0000, 
raw observation next is [-1.533333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 24.92057153109905, 0.3444587517886149, 0.0, 1.0, 83069.90382967867], 
processed observation next is [1.0, 0.8260869565217391, 0.42012927054478305, 0.5533333333333332, 0.0, 0.0, 0.6666666666666666, 0.5767142942582542, 0.6148195839295383, 0.0, 1.0, 0.3955709706175175], 
reward next is 0.6044, 
noisyNet noise sample is [array([1.8467243], dtype=float32), 0.09843421]. 
=============================================
[2019-04-04 15:51:40,911] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1390677e-09 6.1202460e-10 7.7712147e-15 4.6870267e-14 1.0000000e+00
 9.3698582e-10 1.8698788e-15], sum to 1.0000
[2019-04-04 15:51:40,913] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4465
[2019-04-04 15:51:40,935] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.333333333333334, 69.5, 0.0, 0.0, 26.0, 24.92198663924905, 0.3093040001751755, 0.0, 1.0, 44495.67907806685], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2675400.0000, 
sim time next is 2676000.0000, 
raw observation next is [-5.666666666666667, 70.0, 0.0, 0.0, 26.0, 24.86467627460696, 0.2975792525325999, 0.0, 1.0, 44448.41055514945], 
processed observation next is [1.0, 1.0, 0.30563250230840255, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5720563562172467, 0.5991930841775334, 0.0, 1.0, 0.21165909788166404], 
reward next is 0.7883, 
noisyNet noise sample is [array([1.7048261], dtype=float32), 1.1056453]. 
=============================================
[2019-04-04 15:51:40,974] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.774796]
 [78.951584]
 [79.01331 ]
 [79.18537 ]
 [79.28272 ]], R is [[78.55973053]
 [78.56224823]
 [78.56401825]
 [78.56341553]
 [78.55539703]].
[2019-04-04 15:51:46,144] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.8959237e-09 5.8297850e-10 6.6984649e-15 4.5188408e-14 1.0000000e+00
 1.5840653e-09 4.7073886e-15], sum to 1.0000
[2019-04-04 15:51:46,144] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9569
[2019-04-04 15:51:46,192] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666666, 57.33333333333333, 0.0, 0.0, 26.0, 25.2557123756342, 0.4008476246030813, 1.0, 1.0, 52972.08200060007], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2745600.0000, 
sim time next is 2746200.0000, 
raw observation next is [-4.833333333333334, 58.16666666666667, 0.0, 0.0, 26.0, 25.17708126520302, 0.4038828361040478, 1.0, 1.0, 86083.96083519052], 
processed observation next is [1.0, 0.782608695652174, 0.32871652816251157, 0.5816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5980901054335851, 0.6346276120346827, 1.0, 1.0, 0.40992362302471674], 
reward next is 0.5901, 
noisyNet noise sample is [array([-0.4529509], dtype=float32), 0.52442896]. 
=============================================
[2019-04-04 15:51:48,282] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.2204305e-09 2.1306457e-09 1.7233402e-14 2.5022484e-13 1.0000000e+00
 7.1460859e-10 5.0499078e-14], sum to 1.0000
[2019-04-04 15:51:48,284] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0817
[2019-04-04 15:51:48,326] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.4, 43.0, 0.0, 0.0, 26.0, 24.95973928871506, 0.256278156813525, 0.0, 1.0, 46633.13675762564], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2401200.0000, 
sim time next is 2401800.0000, 
raw observation next is [-2.566666666666666, 42.83333333333334, 0.0, 0.0, 26.0, 24.95671212739982, 0.2556937155801287, 0.0, 1.0, 48895.22770542897], 
processed observation next is [0.0, 0.8260869565217391, 0.39150507848568794, 0.42833333333333345, 0.0, 0.0, 0.6666666666666666, 0.5797260106166515, 0.5852312385267096, 0.0, 1.0, 0.23283441764489987], 
reward next is 0.7672, 
noisyNet noise sample is [array([-0.40813422], dtype=float32), 1.1956393]. 
=============================================
[2019-04-04 15:51:49,538] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1712655e-08 7.7133500e-09 1.3592381e-12 1.1917839e-11 9.9999988e-01
 9.8562909e-09 1.0644757e-12], sum to 1.0000
[2019-04-04 15:51:49,539] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8783
[2019-04-04 15:51:49,565] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.466666666666667, 53.66666666666667, 0.0, 0.0, 26.0, 24.02108867827897, 0.02710068003253727, 0.0, 1.0, 43605.94554291955], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2427600.0000, 
sim time next is 2428200.0000, 
raw observation next is [-7.55, 54.0, 0.0, 0.0, 26.0, 23.99200690418481, 0.01791844134277221, 0.0, 1.0, 43644.39458843476], 
processed observation next is [0.0, 0.08695652173913043, 0.25346260387811637, 0.54, 0.0, 0.0, 0.6666666666666666, 0.4993339086820674, 0.5059728137809241, 0.0, 1.0, 0.20783045042111792], 
reward next is 0.7922, 
noisyNet noise sample is [array([-1.100414], dtype=float32), -0.94772166]. 
=============================================
[2019-04-04 15:52:00,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.0362222e-10 8.9403498e-11 1.2294012e-16 1.0808542e-14 1.0000000e+00
 3.2444221e-11 4.1715174e-15], sum to 1.0000
[2019-04-04 15:52:00,884] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4139
[2019-04-04 15:52:00,938] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.283333333333333, 66.66666666666667, 112.6666666666667, 152.3333333333333, 26.0, 25.86916515468575, 0.3641132432542695, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2627400.0000, 
sim time next is 2628000.0000, 
raw observation next is [-5.0, 65.0, 122.5, 183.0, 26.0, 25.84888873727002, 0.3632664000250728, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.32409972299168976, 0.65, 0.4083333333333333, 0.2022099447513812, 0.6666666666666666, 0.6540740614391684, 0.6210888000083576, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8207192], dtype=float32), 0.018736092]. 
=============================================
[2019-04-04 15:52:00,941] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.190186]
 [84.1834  ]
 [84.19095 ]
 [84.42374 ]
 [84.639915]], R is [[84.31643677]
 [84.47327423]
 [84.62854004]
 [84.78225708]
 [84.93443298]].
[2019-04-04 15:52:03,911] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.3624468e-09 3.2112768e-10 7.8456511e-15 1.7482465e-14 1.0000000e+00
 2.6171412e-10 1.6916105e-14], sum to 1.0000
[2019-04-04 15:52:03,920] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4255
[2019-04-04 15:52:03,936] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 69.0, 0.0, 0.0, 26.0, 24.98012077027178, 0.3210031381878685, 0.0, 1.0, 44648.1177099901], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2674800.0000, 
sim time next is 2675400.0000, 
raw observation next is [-5.333333333333334, 69.5, 0.0, 0.0, 26.0, 24.92198664113243, 0.309304000879939, 0.0, 1.0, 44495.67908228382], 
processed observation next is [1.0, 1.0, 0.31486611265004616, 0.695, 0.0, 0.0, 0.6666666666666666, 0.5768322200943693, 0.6031013336266463, 0.0, 1.0, 0.21188418610611345], 
reward next is 0.7881, 
noisyNet noise sample is [array([1.6697304], dtype=float32), -0.49942294]. 
=============================================
[2019-04-04 15:52:06,500] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.9134516e-09 9.0054753e-10 2.0886091e-14 6.4879336e-14 1.0000000e+00
 6.5142469e-10 1.9598701e-15], sum to 1.0000
[2019-04-04 15:52:06,501] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9237
[2019-04-04 15:52:06,544] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 50.0, 45.5, 387.1666666666667, 26.0, 25.98176265823048, 0.3524239787806667, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2738400.0000, 
sim time next is 2739000.0000, 
raw observation next is [-3.0, 50.0, 37.00000000000001, 320.3333333333334, 26.0, 25.90765318205798, 0.458679618375827, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3795013850415513, 0.5, 0.12333333333333335, 0.35395948434622476, 0.6666666666666666, 0.6589710985048317, 0.6528932061252757, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03321017], dtype=float32), 0.4320108]. 
=============================================
[2019-04-04 15:52:06,548] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.50146]
 [80.95225]
 [81.19453]
 [81.38492]
 [81.56034]], R is [[80.23084259]
 [80.42853546]
 [80.62425232]
 [80.81800842]
 [81.00982666]].
[2019-04-04 15:52:07,233] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.86983545e-09 1.42969983e-10 5.65439597e-15 1.35420375e-14
 1.00000000e+00 2.20711494e-09 5.69344207e-15], sum to 1.0000
[2019-04-04 15:52:07,233] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7446
[2019-04-04 15:52:07,287] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 111.0, 793.5, 26.0, 25.36193817959384, 0.4880610590784626, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2725200.0000, 
sim time next is 2725800.0000, 
raw observation next is [-5.8, 58.5, 110.3333333333333, 791.6666666666666, 26.0, 25.80143069086825, 0.5242403938711852, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.30193905817174516, 0.585, 0.36777777777777765, 0.8747697974217311, 0.6666666666666666, 0.6501192242390209, 0.6747467979570617, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2795792], dtype=float32), -0.6070961]. 
=============================================
[2019-04-04 15:52:09,568] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0065580e-11 3.2847063e-12 4.2114358e-18 6.4037451e-17 1.0000000e+00
 3.2231086e-13 5.0145927e-19], sum to 1.0000
[2019-04-04 15:52:09,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2592
[2019-04-04 15:52:09,586] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 100.0, 109.0, 755.8333333333334, 26.0, 26.95941278526431, 0.7165725179040797, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3148800.0000, 
sim time next is 3149400.0000, 
raw observation next is [7.0, 100.0, 110.0, 765.6666666666666, 26.0, 27.0094714031854, 0.7295866054708681, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.36666666666666664, 0.8460405156537752, 0.6666666666666666, 0.7507892835987834, 0.743195535156956, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0194077], dtype=float32), 0.022999007]. 
=============================================
[2019-04-04 15:52:14,430] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9293549e-09 1.5775066e-10 2.2777254e-15 7.0830089e-15 1.0000000e+00
 8.1432205e-10 6.0755911e-15], sum to 1.0000
[2019-04-04 15:52:14,432] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1604
[2019-04-04 15:52:14,444] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.38606919123463, 0.480813867011401, 0.0, 1.0, 65253.33220046727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3277800.0000, 
sim time next is 3278400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.30924503634062, 0.4738241063874835, 0.0, 1.0, 61593.24341529336], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.609103753028385, 0.6579413687958279, 0.0, 1.0, 0.2933011591204446], 
reward next is 0.7067, 
noisyNet noise sample is [array([0.24765691], dtype=float32), 1.3666137]. 
=============================================
[2019-04-04 15:52:16,407] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4039181e-09 1.1700249e-10 5.6202441e-16 2.5665108e-14 1.0000000e+00
 1.9141388e-10 1.0146285e-15], sum to 1.0000
[2019-04-04 15:52:16,408] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0708
[2019-04-04 15:52:16,474] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.333333333333332, 78.16666666666667, 92.0, 469.0, 26.0, 25.92329362085936, 0.4898977509574289, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3315000.0000, 
sim time next is 3315600.0000, 
raw observation next is [-9.0, 77.0, 95.0, 505.5, 26.0, 26.02510287490513, 0.5086039640174768, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.21329639889196678, 0.77, 0.31666666666666665, 0.5585635359116022, 0.6666666666666666, 0.6687585729087608, 0.6695346546724923, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46865162], dtype=float32), 0.89463526]. 
=============================================
[2019-04-04 15:52:17,415] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0192216e-08 4.8055337e-09 1.1614406e-13 9.3695039e-13 1.0000000e+00
 1.3239880e-09 5.6784465e-14], sum to 1.0000
[2019-04-04 15:52:17,435] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4878
[2019-04-04 15:52:17,463] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.66666666666667, 76.0, 0.0, 0.0, 26.0, 24.33749295212526, 0.1782532668965448, 0.0, 1.0, 43655.79349703693], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3303600.0000, 
sim time next is 3304200.0000, 
raw observation next is [-10.83333333333333, 76.0, 0.0, 0.0, 26.0, 24.28928102890347, 0.160247660794143, 0.0, 1.0, 43689.07372746626], 
processed observation next is [1.0, 0.21739130434782608, 0.16251154201292714, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5241067524086226, 0.553415886931381, 0.0, 1.0, 0.2080432082260298], 
reward next is 0.7920, 
noisyNet noise sample is [array([0.23356248], dtype=float32), -0.8119761]. 
=============================================
[2019-04-04 15:52:22,395] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.3778548e-09 2.0030105e-10 2.4683578e-15 2.3768375e-14 1.0000000e+00
 5.0445245e-11 4.0028736e-15], sum to 1.0000
[2019-04-04 15:52:22,395] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4913
[2019-04-04 15:52:22,443] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 218.5, 487.5, 26.0, 25.01530866122972, 0.3796030940559951, 0.0, 1.0, 20252.17320258997], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2980800.0000, 
sim time next is 2981400.0000, 
raw observation next is [-3.0, 65.0, 206.0, 555.3333333333334, 26.0, 25.03694188342874, 0.383526950967225, 0.0, 1.0, 18911.59584853701], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.6866666666666666, 0.6136279926335175, 0.6666666666666666, 0.5864118236190615, 0.627842316989075, 0.0, 1.0, 0.09005521832636672], 
reward next is 0.9099, 
noisyNet noise sample is [array([-0.17181253], dtype=float32), -0.07544492]. 
=============================================
[2019-04-04 15:52:24,934] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.7231615e-09 8.8572337e-11 1.9253531e-15 1.0711836e-14 1.0000000e+00
 7.1207616e-11 6.6095493e-15], sum to 1.0000
[2019-04-04 15:52:24,935] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9416
[2019-04-04 15:52:24,973] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.833333333333333, 65.0, 0.0, 0.0, 26.0, 25.19161710240627, 0.3310810330604742, 0.0, 1.0, 39179.88505517791], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3015600.0000, 
sim time next is 3016200.0000, 
raw observation next is [-3.916666666666667, 65.0, 0.0, 0.0, 26.0, 25.15990485286268, 0.3261655613780657, 0.0, 1.0, 39034.20271788361], 
processed observation next is [0.0, 0.9130434782608695, 0.3541089566020314, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5966587377385567, 0.6087218537926885, 0.0, 1.0, 0.18587715579944575], 
reward next is 0.8141, 
noisyNet noise sample is [array([-0.27945155], dtype=float32), 2.6948469]. 
=============================================
[2019-04-04 15:52:26,385] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.10505671e-09 6.24784113e-10 1.64179701e-17 1.50280501e-15
 1.00000000e+00 1.00236285e-10 1.94038625e-16], sum to 1.0000
[2019-04-04 15:52:26,387] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2094
[2019-04-04 15:52:26,411] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 100.0, 0.0, 0.0, 26.0, 25.44783624541237, 0.3255046827590696, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3136800.0000, 
sim time next is 3137400.0000, 
raw observation next is [6.0, 100.0, 1.0, 82.0, 26.0, 25.43268671489021, 0.3279879996486627, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.6288088642659281, 1.0, 0.0033333333333333335, 0.09060773480662983, 0.6666666666666666, 0.6193905595741841, 0.609329333216221, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2982675], dtype=float32), 0.10368541]. 
=============================================
[2019-04-04 15:52:28,267] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.3688757e-10 1.1029777e-10 1.4159238e-15 5.5806380e-15 1.0000000e+00
 1.8643422e-10 1.3040916e-15], sum to 1.0000
[2019-04-04 15:52:28,269] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5899
[2019-04-04 15:52:28,305] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.166666666666667, 51.5, 115.3333333333333, 811.6666666666666, 26.0, 26.24533130218042, 0.654914585271503, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3503400.0000, 
sim time next is 3504000.0000, 
raw observation next is [2.333333333333333, 51.0, 115.1666666666667, 808.8333333333334, 26.0, 26.35110653098186, 0.562369496005198, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5272391505078486, 0.51, 0.383888888888889, 0.8937384898710866, 0.6666666666666666, 0.6959255442484883, 0.6874564986683994, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8008666], dtype=float32), 0.22008345]. 
=============================================
[2019-04-04 15:52:28,313] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[87.880135]
 [87.95234 ]
 [87.996605]
 [88.0498  ]
 [88.13549 ]], R is [[87.87817383]
 [87.99938965]
 [88.11940002]
 [88.23820496]
 [88.35582733]].
[2019-04-04 15:52:29,700] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0225513e-10 1.5399441e-10 1.3825805e-15 3.9323454e-15 1.0000000e+00
 1.8840235e-11 1.4526448e-16], sum to 1.0000
[2019-04-04 15:52:29,703] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4125
[2019-04-04 15:52:29,733] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 93.33333333333334, 0.0, 0.0, 26.0, 25.52153142516223, 0.3645888095188204, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3096600.0000, 
sim time next is 3097200.0000, 
raw observation next is [-1.0, 94.66666666666667, 0.0, 0.0, 26.0, 25.50467926997363, 0.3557194503525108, 0.0, 1.0, 18751.87322578303], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9466666666666668, 0.0, 0.0, 0.6666666666666666, 0.625389939164469, 0.6185731501175036, 0.0, 1.0, 0.08929463440849063], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.8054413], dtype=float32), 0.30283907]. 
=============================================
[2019-04-04 15:52:30,150] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6439047e-10 2.1413952e-11 5.3388265e-17 8.5662861e-16 1.0000000e+00
 1.3538055e-11 1.3253805e-16], sum to 1.0000
[2019-04-04 15:52:30,151] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8235
[2019-04-04 15:52:30,162] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 26.0, 25.59860306413687, 0.6658341691568163, 0.0, 1.0, 196443.7691617402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3184200.0000, 
sim time next is 3184800.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 26.0, 25.56557186941834, 0.6910978688142017, 0.0, 1.0, 161187.6734097901], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6304643224515282, 0.7303659562714006, 0.0, 1.0, 0.767560349570429], 
reward next is 0.2324, 
noisyNet noise sample is [array([0.5154224], dtype=float32), -0.29624555]. 
=============================================
[2019-04-04 15:52:31,010] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.7960871e-10 3.6448181e-11 1.5232765e-16 1.4639550e-15 1.0000000e+00
 3.9153698e-12 4.0797506e-16], sum to 1.0000
[2019-04-04 15:52:31,011] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4633
[2019-04-04 15:52:31,022] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.4490357534984, 0.3040782511513347, 0.0, 1.0, 54092.1978302533], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3130800.0000, 
sim time next is 3131400.0000, 
raw observation next is [3.833333333333333, 100.0, 0.0, 0.0, 26.0, 25.38944096592018, 0.3078523009753407, 0.0, 1.0, 85065.83354989595], 
processed observation next is [1.0, 0.21739130434782608, 0.5687903970452447, 1.0, 0.0, 0.0, 0.6666666666666666, 0.615786747160015, 0.6026174336584469, 0.0, 1.0, 0.40507539785664737], 
reward next is 0.5949, 
noisyNet noise sample is [array([0.7349761], dtype=float32), 0.6534519]. 
=============================================
[2019-04-04 15:52:31,679] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2804264e-10 2.9104050e-12 2.8817608e-18 2.6635187e-16 1.0000000e+00
 1.3552109e-11 7.1170430e-18], sum to 1.0000
[2019-04-04 15:52:31,679] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7473
[2019-04-04 15:52:31,700] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 100.0, 8.0, 116.0, 26.0, 27.24153708891369, 0.8477939859413324, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3173400.0000, 
sim time next is 3174000.0000, 
raw observation next is [6.0, 100.0, 0.0, 0.0, 26.0, 26.57025950595279, 0.860109869587166, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.0, 0.0, 0.6666666666666666, 0.7141882921627326, 0.7867032898623886, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6346502], dtype=float32), -0.658983]. 
=============================================
[2019-04-04 15:52:31,710] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[97.0889 ]
 [97.69014]
 [98.36949]
 [98.90134]
 [99.31189]], R is [[96.4393692 ]
 [96.47497559]
 [96.51022339]
 [96.54512024]
 [96.57967377]].
[2019-04-04 15:52:31,891] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7496423e-11 2.5161108e-11 1.0585160e-18 1.6872514e-17 1.0000000e+00
 1.1970232e-12 2.0611285e-18], sum to 1.0000
[2019-04-04 15:52:31,894] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5467
[2019-04-04 15:52:31,911] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 100.0, 102.0, 680.0, 26.0, 26.6100663913923, 0.6089255457011042, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3145800.0000, 
sim time next is 3146400.0000, 
raw observation next is [7.0, 100.0, 103.5, 696.5, 26.0, 26.64464233686327, 0.6339443979195666, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.345, 0.7696132596685082, 0.6666666666666666, 0.7203868614052725, 0.7113147993065222, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7726311], dtype=float32), 0.5067144]. 
=============================================
[2019-04-04 15:52:37,221] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7036381e-08 4.0432069e-10 3.6527462e-14 7.0032440e-14 1.0000000e+00
 1.2897140e-09 2.0195325e-14], sum to 1.0000
[2019-04-04 15:52:37,223] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0598
[2019-04-04 15:52:37,271] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.19398400787602, 0.4397831196081598, 1.0, 1.0, 47874.26886152965], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3355200.0000, 
sim time next is 3355800.0000, 
raw observation next is [-3.166666666666667, 56.66666666666667, 0.0, 0.0, 26.0, 25.14064865540981, 0.4414105383031666, 0.0, 1.0, 70680.62659444446], 
processed observation next is [1.0, 0.8695652173913043, 0.3748845798707295, 0.5666666666666668, 0.0, 0.0, 0.6666666666666666, 0.5950540546174841, 0.6471368461010555, 0.0, 1.0, 0.3365744123544974], 
reward next is 0.6634, 
noisyNet noise sample is [array([0.02082962], dtype=float32), -0.5822344]. 
=============================================
[2019-04-04 15:52:38,020] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6548956e-09 2.6304381e-10 1.1392734e-14 1.4833808e-14 1.0000000e+00
 1.3137172e-09 1.2715769e-14], sum to 1.0000
[2019-04-04 15:52:38,021] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0245
[2019-04-04 15:52:38,044] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666666, 77.66666666666667, 0.0, 0.0, 26.0, 25.43694424102224, 0.57649970050355, 0.0, 1.0, 81517.70764344954], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3271200.0000, 
sim time next is 3271800.0000, 
raw observation next is [-4.833333333333334, 79.33333333333333, 0.0, 0.0, 26.0, 25.5155076689512, 0.594410836544495, 0.0, 1.0, 18749.83764524996], 
processed observation next is [1.0, 0.8695652173913043, 0.32871652816251157, 0.7933333333333333, 0.0, 0.0, 0.6666666666666666, 0.6262923057459334, 0.6981369455148316, 0.0, 1.0, 0.08928494116785696], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.7658873], dtype=float32), 0.51946557]. 
=============================================
[2019-04-04 15:52:43,957] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-04 15:52:43,957] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 15:52:43,960] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 15:52:43,961] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:52:43,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:52:43,961] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 15:52:43,962] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:52:43,967] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run40
[2019-04-04 15:52:43,990] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run40
[2019-04-04 15:52:43,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run40
[2019-04-04 15:53:15,669] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17891642], dtype=float32), 0.23167863]
[2019-04-04 15:53:15,669] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.5, 71.0, 0.0, 0.0, 26.0, 25.22653095545854, 0.4654421727969347, 0.0, 1.0, 39068.18050883782]
[2019-04-04 15:53:15,669] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 15:53:15,670] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.5490595e-09 1.9504590e-10 8.0028001e-16 1.1041163e-14 1.0000000e+00
 6.1080675e-11 1.4549970e-15], sampled 0.195491838565517
[2019-04-04 15:54:25,340] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 15:54:45,670] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 15:54:50,332] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6682 275799682.2396 1231.4887
[2019-04-04 15:54:51,356] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 3900000, evaluation results [3900000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.668179811327, 275799682.2396222, 1231.4886876164037]
[2019-04-04 15:54:56,332] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4223377e-09 8.9483858e-11 1.9603402e-16 6.7487871e-15 1.0000000e+00
 1.8038203e-11 2.0548824e-15], sum to 1.0000
[2019-04-04 15:54:56,332] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0570
[2019-04-04 15:54:56,352] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1666666666666666, 71.16666666666666, 0.0, 0.0, 26.0, 25.24967894764472, 0.3885077954651557, 0.0, 1.0, 42275.45729647216], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3473400.0000, 
sim time next is 3474000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.22227008058188, 0.3830110376495086, 0.0, 1.0, 41941.58722307802], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6018558400484899, 0.6276703458831695, 0.0, 1.0, 0.19972184391941916], 
reward next is 0.8003, 
noisyNet noise sample is [array([1.4556603], dtype=float32), 0.34665924]. 
=============================================
[2019-04-04 15:54:56,363] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.99035 ]
 [83.10749 ]
 [83.22147 ]
 [83.2937  ]
 [83.306496]], R is [[82.84607697]
 [82.81630707]
 [82.78006744]
 [82.72162628]
 [82.60089111]].
[2019-04-04 15:55:01,687] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.4310390e-10 1.6682428e-10 1.2979864e-16 6.7688171e-16 1.0000000e+00
 9.0627766e-12 3.5280390e-15], sum to 1.0000
[2019-04-04 15:55:01,695] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7247
[2019-04-04 15:55:01,746] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.166666666666667, 65.83333333333334, 103.6666666666667, 703.3333333333334, 26.0, 25.81619406882663, 0.5144090744836186, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3577800.0000, 
sim time next is 3578400.0000, 
raw observation next is [-5.0, 65.0, 105.5, 717.0, 26.0, 25.76153513195192, 0.507373816237126, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.32409972299168976, 0.65, 0.3516666666666667, 0.7922651933701658, 0.6666666666666666, 0.6467945943293266, 0.6691246054123754, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.88480055], dtype=float32), -1.4114227]. 
=============================================
[2019-04-04 15:55:04,539] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2033474e-08 4.7647253e-10 1.6687232e-14 1.7644949e-13 1.0000000e+00
 1.3801398e-09 6.0424037e-14], sum to 1.0000
[2019-04-04 15:55:04,539] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4399
[2019-04-04 15:55:04,561] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.1090030671362, 0.3379630142542716, 0.0, 1.0, 18707.36617033886], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3611400.0000, 
sim time next is 3612000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.0866344742168, 0.3438723731695686, 0.0, 1.0, 198834.4758787108], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5905528728514001, 0.6146241243898561, 0.0, 1.0, 0.9468308375176705], 
reward next is 0.0532, 
noisyNet noise sample is [array([-1.3295391], dtype=float32), 1.298473]. 
=============================================
[2019-04-04 15:55:04,570] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.30046 ]
 [74.12371 ]
 [73.954025]
 [73.64264 ]
 [73.388   ]], R is [[74.74837494]
 [74.91181183]
 [75.0736084 ]
 [75.12294006]
 [75.18733215]].
[2019-04-04 15:55:06,779] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.6805949e-09 8.2969764e-10 2.1066000e-15 7.6087533e-14 1.0000000e+00
 6.5007927e-10 6.6800156e-15], sum to 1.0000
[2019-04-04 15:55:06,782] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1765
[2019-04-04 15:55:06,795] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.666666666666667, 53.0, 67.83333333333333, 552.5, 26.0, 25.50114277811348, 0.4736768808382569, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3687600.0000, 
sim time next is 3688200.0000, 
raw observation next is [4.5, 54.5, 64.0, 522.0, 26.0, 25.49433936147977, 0.4675428400757573, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5872576177285319, 0.545, 0.21333333333333335, 0.5767955801104973, 0.6666666666666666, 0.6245282801233142, 0.6558476133585858, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02975088], dtype=float32), -0.51028603]. 
=============================================
[2019-04-04 15:55:07,163] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6913406e-09 6.3381494e-10 9.1621098e-15 4.9718059e-14 1.0000000e+00
 6.7271622e-10 2.6985496e-14], sum to 1.0000
[2019-04-04 15:55:07,170] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3083
[2019-04-04 15:55:07,184] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 69.0, 0.0, 0.0, 26.0, 25.0967240590406, 0.284856366480653, 0.0, 1.0, 41965.4500325559], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3738000.0000, 
sim time next is 3738600.0000, 
raw observation next is [-3.5, 71.0, 0.0, 0.0, 26.0, 25.17048719683362, 0.2713464447522007, 0.0, 1.0, 42380.85366097815], 
processed observation next is [1.0, 0.2608695652173913, 0.36565096952908593, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5975405997361349, 0.5904488149174002, 0.0, 1.0, 0.20181358886180073], 
reward next is 0.7982, 
noisyNet noise sample is [array([-0.5038467], dtype=float32), 0.88748723]. 
=============================================
[2019-04-04 15:55:10,915] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.70490833e-09 2.57207455e-09 2.08565126e-14 6.81169259e-14
 1.00000000e+00 1.75456372e-10 1.32683374e-14], sum to 1.0000
[2019-04-04 15:55:10,916] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1543
[2019-04-04 15:55:10,943] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.38827017221515, 0.3396789945630165, 0.0, 1.0, 48584.28647223032], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4242600.0000, 
sim time next is 4243200.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.399380291248, 0.3405564443632004, 0.0, 1.0, 37943.53958209865], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6166150242706667, 0.6135188147877334, 0.0, 1.0, 0.18068352181951738], 
reward next is 0.8193, 
noisyNet noise sample is [array([-2.7267337], dtype=float32), -1.1455715]. 
=============================================
[2019-04-04 15:55:14,225] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.40272971e-09 7.98781874e-10 5.63606723e-16 4.52800097e-15
 1.00000000e+00 1.04120754e-10 1.65969489e-15], sum to 1.0000
[2019-04-04 15:55:14,230] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7543
[2019-04-04 15:55:14,267] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 43.66666666666667, 67.83333333333333, 578.6666666666666, 26.0, 26.92615575070677, 0.6416576933693239, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3860400.0000, 
sim time next is 3861000.0000, 
raw observation next is [3.0, 43.0, 64.0, 551.0, 26.0, 26.40145178322973, 0.6786942340358814, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.43, 0.21333333333333335, 0.6088397790055249, 0.6666666666666666, 0.700120981935811, 0.7262314113452938, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11813306], dtype=float32), -0.5122853]. 
=============================================
[2019-04-04 15:55:14,276] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[85.47463 ]
 [85.71134 ]
 [85.86168 ]
 [85.95843 ]
 [86.098076]], R is [[85.32614899]
 [85.47288513]
 [85.61815643]
 [85.76197815]
 [85.90435791]].
[2019-04-04 15:55:15,921] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.7213061e-09 4.8044041e-10 1.7433442e-15 1.2406954e-14 1.0000000e+00
 4.5308290e-10 2.4968441e-15], sum to 1.0000
[2019-04-04 15:55:15,922] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8282
[2019-04-04 15:55:15,952] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.833333333333334, 49.0, 108.6666666666667, 747.3333333333334, 26.0, 26.31455525900564, 0.5769094090833565, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3924600.0000, 
sim time next is 3925200.0000, 
raw observation next is [-6.666666666666667, 49.0, 110.8333333333333, 761.1666666666667, 26.0, 26.34016524656331, 0.5861658492813148, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.27793167128347185, 0.49, 0.36944444444444435, 0.8410681399631676, 0.6666666666666666, 0.6950137705469425, 0.6953886164271049, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1709713], dtype=float32), -0.05033497]. 
=============================================
[2019-04-04 15:55:16,813] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.4499901e-08 5.6328302e-09 2.3351782e-13 7.7387988e-13 9.9999988e-01
 5.4758056e-09 2.1412212e-13], sum to 1.0000
[2019-04-04 15:55:16,820] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9266
[2019-04-04 15:55:16,833] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.40898836576733, -0.05821822565539881, 0.0, 1.0, 43100.53661572932], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3999000.0000, 
sim time next is 3999600.0000, 
raw observation next is [-14.0, 69.0, 0.0, 0.0, 26.0, 23.38111150403071, -0.06589034144726517, 0.0, 1.0, 42986.44179462276], 
processed observation next is [1.0, 0.30434782608695654, 0.07479224376731301, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4484259586692258, 0.4780365528509116, 0.0, 1.0, 0.204697341879156], 
reward next is 0.7953, 
noisyNet noise sample is [array([-0.92125577], dtype=float32), -0.1005553]. 
=============================================
[2019-04-04 15:55:18,721] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3759238e-09 1.9951635e-10 1.5553727e-15 7.0303019e-15 1.0000000e+00
 5.6140655e-11 2.5096974e-15], sum to 1.0000
[2019-04-04 15:55:18,722] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2647
[2019-04-04 15:55:18,774] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.833333333333333, 43.83333333333334, 117.0, 827.6666666666666, 26.0, 24.98641452462878, 0.5612866809265894, 1.0, 1.0, 197607.0417151776], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3935400.0000, 
sim time next is 3936000.0000, 
raw observation next is [-5.666666666666666, 42.66666666666667, 116.5, 825.8333333333334, 26.0, 25.72538801491692, 0.6518016156106955, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3056325023084026, 0.4266666666666667, 0.3883333333333333, 0.912523020257827, 0.6666666666666666, 0.64378233457641, 0.7172672052035652, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.57449913], dtype=float32), 1.3249688]. 
=============================================
[2019-04-04 15:55:18,781] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[83.387924]
 [82.45408 ]
 [81.49426 ]
 [81.60189 ]
 [81.6369  ]], R is [[83.40356445]
 [82.62854004]
 [81.868927  ]
 [82.05023956]
 [82.22973633]].
[2019-04-04 15:55:30,987] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.0494544e-10 2.3596008e-10 9.5019288e-16 8.8835409e-15 1.0000000e+00
 4.5196753e-11 4.8662568e-15], sum to 1.0000
[2019-04-04 15:55:30,987] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1162
[2019-04-04 15:55:30,993] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.966666666666667, 56.66666666666667, 100.6666666666667, 622.0, 26.0, 25.48967700975895, 0.4710996821151426, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4290600.0000, 
sim time next is 4291200.0000, 
raw observation next is [7.0, 56.0, 93.0, 605.5, 26.0, 25.52816825895559, 0.4729049014733318, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6565096952908588, 0.56, 0.31, 0.669060773480663, 0.6666666666666666, 0.6273473549129657, 0.6576349671577773, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6191531], dtype=float32), -1.4375625]. 
=============================================
[2019-04-04 15:55:33,953] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5794882e-09 9.2943125e-11 4.6887322e-15 1.4767665e-14 1.0000000e+00
 9.0159887e-11 4.9620647e-15], sum to 1.0000
[2019-04-04 15:55:33,954] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0151
[2019-04-04 15:55:33,962] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.85, 40.5, 24.0, 163.0, 26.0, 25.18321481555864, 0.337642489267477, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4210200.0000, 
sim time next is 4210800.0000, 
raw observation next is [1.8, 40.66666666666667, 20.0, 135.8333333333333, 26.0, 25.09720162069807, 0.3256544111341745, 0.0, 1.0, 57569.90501296682], 
processed observation next is [0.0, 0.7391304347826086, 0.5124653739612189, 0.40666666666666673, 0.06666666666666667, 0.1500920810313075, 0.6666666666666666, 0.5914334683915058, 0.6085514703780581, 0.0, 1.0, 0.2741424048236515], 
reward next is 0.7259, 
noisyNet noise sample is [array([0.32976398], dtype=float32), -0.33185387]. 
=============================================
[2019-04-04 15:55:34,383] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.6065838e-10 3.3048869e-11 9.7330835e-17 2.6785083e-15 1.0000000e+00
 5.2131296e-11 3.7824170e-17], sum to 1.0000
[2019-04-04 15:55:34,387] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8957
[2019-04-04 15:55:34,396] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 160.5, 3.0, 26.0, 26.30362775329605, 0.5517013319958007, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4708800.0000, 
sim time next is 4709400.0000, 
raw observation next is [1.0, 86.0, 143.0, 2.0, 26.0, 26.27787934298708, 0.5451404294497492, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.4766666666666667, 0.0022099447513812156, 0.6666666666666666, 0.6898232785822568, 0.6817134764832496, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.154768], dtype=float32), 1.8784871]. 
=============================================
[2019-04-04 15:55:37,799] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3997890e-10 5.5922077e-12 1.8286388e-17 9.3220200e-16 1.0000000e+00
 2.1573166e-12 8.7394312e-18], sum to 1.0000
[2019-04-04 15:55:37,801] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3319
[2019-04-04 15:55:37,848] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 92.0, 140.5, 3.0, 26.0, 25.55653112505581, 0.58163055371721, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4456800.0000, 
sim time next is 4457400.0000, 
raw observation next is [0.0, 90.83333333333334, 122.0, 2.0, 26.0, 25.96011903403402, 0.6076128394872086, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.9083333333333334, 0.4066666666666667, 0.0022099447513812156, 0.6666666666666666, 0.6633432528361682, 0.7025376131624029, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36167982], dtype=float32), 0.6400183]. 
=============================================
[2019-04-04 15:55:52,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:55:52,514] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:55:52,536] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run30
[2019-04-04 15:55:54,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:55:54,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:55:54,311] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run30
[2019-04-04 15:55:54,889] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:55:54,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:55:54,918] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run30
[2019-04-04 15:55:55,272] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:55:55,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:55:55,280] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run30
[2019-04-04 15:55:55,378] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6866276e-09 3.0773883e-10 5.1459295e-16 5.8614574e-15 1.0000000e+00
 9.6375331e-11 8.6305743e-16], sum to 1.0000
[2019-04-04 15:55:55,379] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4854
[2019-04-04 15:55:55,391] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.466666666666667, 45.0, 104.1666666666667, 146.6666666666667, 26.0, 27.08872328330055, 0.824105111997112, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4639200.0000, 
sim time next is 4639800.0000, 
raw observation next is [5.333333333333334, 45.5, 91.33333333333334, 146.3333333333333, 26.0, 27.32431249771876, 0.8472860430723551, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6103416435826409, 0.455, 0.30444444444444446, 0.1616942909760589, 0.6666666666666666, 0.7770260414765634, 0.7824286810241183, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.84269404], dtype=float32), 0.21111323]. 
=============================================
[2019-04-04 15:56:05,596] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.0208613e-10 2.8472319e-10 1.0290587e-15 8.3827131e-14 1.0000000e+00
 7.5789708e-11 7.8389799e-15], sum to 1.0000
[2019-04-04 15:56:05,596] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2707
[2019-04-04 15:56:05,625] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.35640208156383, 0.3567713754029698, 0.0, 1.0, 61935.65522029768], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4839600.0000, 
sim time next is 4840200.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.29485773235561, 0.35123287126939, 0.0, 1.0, 50682.42266036142], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6079048110296341, 0.6170776237564634, 0.0, 1.0, 0.24134486981124487], 
reward next is 0.7587, 
noisyNet noise sample is [array([1.3040898], dtype=float32), -0.43471652]. 
=============================================
[2019-04-04 15:56:06,166] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8025272e-09 1.0205689e-09 1.2248290e-14 6.6152663e-14 1.0000000e+00
 2.0901807e-09 6.1756278e-14], sum to 1.0000
[2019-04-04 15:56:06,167] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6192
[2019-04-04 15:56:06,185] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.666666666666667, 44.66666666666667, 44.5, 208.6666666666667, 26.0, 25.06693309305746, 0.3247907785473129, 0.0, 1.0, 41813.47865874399], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4900800.0000, 
sim time next is 4901400.0000, 
raw observation next is [2.5, 44.5, 33.0, 187.0, 26.0, 25.03250746213482, 0.3204815964217457, 0.0, 1.0, 48584.60499123881], 
processed observation next is [0.0, 0.7391304347826086, 0.5318559556786704, 0.445, 0.11, 0.20662983425414364, 0.6666666666666666, 0.586042288511235, 0.6068271988072486, 0.0, 1.0, 0.23135526186304195], 
reward next is 0.7686, 
noisyNet noise sample is [array([-0.43581897], dtype=float32), -0.6908904]. 
=============================================
[2019-04-04 15:56:07,504] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.1903463e-10 3.2580834e-11 6.5477362e-16 1.2630821e-15 1.0000000e+00
 4.1653980e-10 1.5914376e-15], sum to 1.0000
[2019-04-04 15:56:07,504] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2209
[2019-04-04 15:56:07,511] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.166666666666666, 24.83333333333334, 122.6666666666667, 858.3333333333334, 26.0, 27.05254997449015, 0.7240104156376329, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4968600.0000, 
sim time next is 4969200.0000, 
raw observation next is [6.333333333333333, 24.66666666666667, 122.8333333333333, 861.6666666666667, 26.0, 27.12856165655557, 0.7266486556808721, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6380424746075716, 0.2466666666666667, 0.40944444444444433, 0.9521178637200738, 0.6666666666666666, 0.7607134713796307, 0.7422162185602907, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7843433], dtype=float32), -0.34638947]. 
=============================================
[2019-04-04 15:56:10,270] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4996474e-10 6.7137254e-11 1.0296629e-16 1.4656817e-15 1.0000000e+00
 2.4859389e-11 3.6941651e-16], sum to 1.0000
[2019-04-04 15:56:10,270] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8670
[2019-04-04 15:56:10,281] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.333333333333334, 25.66666666666667, 94.83333333333334, 781.5, 26.0, 26.24934143355232, 0.7429722611791032, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4980000.0000, 
sim time next is 4980600.0000, 
raw observation next is [8.5, 25.5, 92.0, 774.0, 26.0, 26.79393048728001, 0.811827658382259, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.698060941828255, 0.255, 0.30666666666666664, 0.8552486187845304, 0.6666666666666666, 0.7328275406066677, 0.770609219460753, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.57276946], dtype=float32), 1.7147356]. 
=============================================
[2019-04-04 15:56:10,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:56:10,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:56:10,524] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run30
[2019-04-04 15:56:12,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:56:12,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:56:12,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run30
[2019-04-04 15:56:13,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:56:13,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:56:13,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run30
[2019-04-04 15:56:13,687] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:56:13,687] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:56:13,711] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run30
[2019-04-04 15:56:14,048] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:56:14,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:56:14,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run30
[2019-04-04 15:56:14,810] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:56:14,811] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:56:14,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run30
[2019-04-04 15:56:14,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:56:14,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:56:14,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run30
[2019-04-04 15:56:15,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:56:15,642] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:56:15,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run30
[2019-04-04 15:56:15,917] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:56:15,917] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:56:15,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run30
[2019-04-04 15:56:16,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:56:16,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:56:16,140] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run30
[2019-04-04 15:56:16,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:56:16,718] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:56:16,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run30
[2019-04-04 15:56:18,111] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 15:56:18,111] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:56:18,114] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run30
[2019-04-04 15:56:22,652] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.35811143e-10 6.64257704e-10 2.21142324e-15 7.59603629e-14
 1.00000000e+00 1.07942454e-10 2.82843190e-15], sum to 1.0000
[2019-04-04 15:56:22,652] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1466
[2019-04-04 15:56:22,670] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 10.5, 0.0, 26.0, 21.43065937668549, -0.5063447317097076, 0.0, 1.0, 40257.02696294181], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 28800.0000, 
sim time next is 29400.0000, 
raw observation next is [7.7, 93.0, 14.0, 0.0, 26.0, 21.51326628850029, -0.4975644264762343, 0.0, 1.0, 40219.3532568429], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.04666666666666667, 0.0, 0.6666666666666666, 0.2927721907083575, 0.3341451911745886, 0.0, 1.0, 0.19152072979449], 
reward next is 0.8085, 
noisyNet noise sample is [array([-0.22051059], dtype=float32), 0.8707126]. 
=============================================
[2019-04-04 15:56:26,601] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3428089e-09 7.9272600e-10 3.6186959e-15 8.8950921e-14 1.0000000e+00
 1.8014351e-10 1.0171053e-14], sum to 1.0000
[2019-04-04 15:56:26,602] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0112
[2019-04-04 15:56:26,626] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.18126504728737, -0.5845499848920434, 0.0, 1.0, 40374.04381263767], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 22200.0000, 
sim time next is 22800.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.19925789201175, -0.5770661901155092, 0.0, 1.0, 40350.60735328061], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.2666048243343126, 0.3076446032948303, 0.0, 1.0, 0.19214574930133624], 
reward next is 0.8079, 
noisyNet noise sample is [array([-0.567332], dtype=float32), 0.16696319]. 
=============================================
[2019-04-04 15:56:29,189] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3235165e-08 1.6247883e-09 1.9314314e-13 3.6775110e-12 1.0000000e+00
 3.1117120e-09 5.6446389e-13], sum to 1.0000
[2019-04-04 15:56:29,189] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3160
[2019-04-04 15:56:29,213] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.1, 79.5, 0.0, 0.0, 26.0, 24.43398432703777, 0.1529599120571196, 0.0, 1.0, 47549.85133668809], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 336600.0000, 
sim time next is 337200.0000, 
raw observation next is [-13.2, 80.33333333333333, 0.0, 0.0, 26.0, 24.34954793102826, 0.1391706597204653, 0.0, 1.0, 47560.13240088879], 
processed observation next is [1.0, 0.9130434782608695, 0.09695290858725764, 0.8033333333333332, 0.0, 0.0, 0.6666666666666666, 0.5291289942523548, 0.5463902199068218, 0.0, 1.0, 0.22647682095661328], 
reward next is 0.7735, 
noisyNet noise sample is [array([1.1050969], dtype=float32), -0.06596416]. 
=============================================
[2019-04-04 15:56:36,986] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.3079863e-09 1.2471990e-10 2.1711669e-15 3.0337020e-14 1.0000000e+00
 7.1796263e-10 3.4891316e-15], sum to 1.0000
[2019-04-04 15:56:36,988] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6245
[2019-04-04 15:56:37,033] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.033333333333333, 77.0, 71.50000000000001, 49.33333333333332, 26.0, 25.35924442139778, 0.2269481945412968, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 206400.0000, 
sim time next is 207000.0000, 
raw observation next is [-7.85, 76.5, 79.0, 0.0, 26.0, 25.40561044885512, 0.2208469208240678, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24515235457063714, 0.765, 0.2633333333333333, 0.0, 0.6666666666666666, 0.6171342040712601, 0.5736156402746893, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1925743], dtype=float32), -0.7796462]. 
=============================================
[2019-04-04 15:56:37,050] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.239555]
 [81.75763 ]
 [82.191574]
 [82.456985]
 [82.567154]], R is [[80.96087646]
 [81.15126801]
 [81.33975983]
 [81.52635956]
 [81.71109772]].
[2019-04-04 15:56:46,216] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.72058696e-07 1.40202914e-07 5.60757300e-12 8.51018145e-11
 9.99999166e-01 1.04443046e-07 1.00684847e-12], sum to 1.0000
[2019-04-04 15:56:46,216] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4557
[2019-04-04 15:56:46,241] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.2, 70.33333333333334, 0.0, 0.0, 26.0, 22.6376909608548, -0.2684946061424244, 0.0, 1.0, 49318.89717682296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 357600.0000, 
sim time next is 358200.0000, 
raw observation next is [-15.3, 71.0, 0.0, 0.0, 26.0, 22.58888909672341, -0.2735285213462109, 0.0, 1.0, 49324.75371863984], 
processed observation next is [1.0, 0.13043478260869565, 0.03878116343490302, 0.71, 0.0, 0.0, 0.6666666666666666, 0.38240742472695083, 0.40882382621792973, 0.0, 1.0, 0.23487977961257067], 
reward next is 0.7651, 
noisyNet noise sample is [array([-1.676856], dtype=float32), -2.4883745]. 
=============================================
[2019-04-04 15:56:47,777] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1129789e-08 2.2038076e-10 1.1488354e-14 4.9181677e-14 1.0000000e+00
 1.4942607e-09 4.3932811e-14], sum to 1.0000
[2019-04-04 15:56:47,778] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4996
[2019-04-04 15:56:47,856] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.2, 62.0, 0.0, 0.0, 26.0, 25.22340059142506, 0.3386289547966256, 1.0, 1.0, 76785.97914466236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 327000.0000, 
sim time next is 327600.0000, 
raw observation next is [-12.3, 63.0, 0.0, 0.0, 26.0, 25.21510038075135, 0.3430936516802909, 1.0, 1.0, 72578.91216996865], 
processed observation next is [1.0, 0.8260869565217391, 0.12188365650969527, 0.63, 0.0, 0.0, 0.6666666666666666, 0.6012583650626123, 0.614364550560097, 1.0, 1.0, 0.34561386747604117], 
reward next is 0.6544, 
noisyNet noise sample is [array([1.0413377], dtype=float32), 1.4791719]. 
=============================================
[2019-04-04 15:56:57,255] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5805130e-08 4.1682853e-09 1.1569147e-13 2.3502324e-13 1.0000000e+00
 1.5280659e-09 1.4379759e-13], sum to 1.0000
[2019-04-04 15:56:57,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9258
[2019-04-04 15:56:57,292] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.0, 36.66666666666666, 0.0, 0.0, 26.0, 25.86782381641483, 0.3854921802980939, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 407400.0000, 
sim time next is 408000.0000, 
raw observation next is [-9.100000000000001, 37.33333333333334, 0.0, 0.0, 26.0, 25.85811952447198, 0.2129631684409733, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.21052631578947364, 0.3733333333333334, 0.0, 0.0, 0.6666666666666666, 0.6548432937059984, 0.5709877228136578, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.93493015], dtype=float32), 0.325114]. 
=============================================
[2019-04-04 15:56:57,302] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[71.816925]
 [72.86877 ]
 [73.42497 ]
 [73.90962 ]
 [74.379456]], R is [[71.22767639]
 [71.51540375]
 [71.80024719]
 [72.08224487]
 [72.36141968]].
[2019-04-04 15:57:00,763] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2924408e-08 4.5963566e-09 8.1914490e-14 1.4370957e-13 1.0000000e+00
 1.1305468e-08 4.7855908e-14], sum to 1.0000
[2019-04-04 15:57:00,764] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4800
[2019-04-04 15:57:00,802] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 37.0, 75.0, 0.0, 26.0, 25.59504763784712, 0.2857197871753176, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 486000.0000, 
sim time next is 486600.0000, 
raw observation next is [0.1833333333333333, 36.5, 68.66666666666666, 0.0, 26.0, 25.67639453373625, 0.2910535843651808, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46768236380424755, 0.365, 0.22888888888888886, 0.0, 0.6666666666666666, 0.6396995444780208, 0.5970178614550603, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4337354], dtype=float32), -0.52806276]. 
=============================================
[2019-04-04 15:57:02,627] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3648677e-09 6.8923162e-10 4.3836886e-14 2.6227691e-13 1.0000000e+00
 1.3879167e-09 2.3800721e-14], sum to 1.0000
[2019-04-04 15:57:02,627] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0040
[2019-04-04 15:57:02,695] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.283333333333333, 27.5, 125.6666666666667, 0.0, 26.0, 25.0116562379266, 0.1487558390120679, 1.0, 1.0, 90539.09158590702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 478200.0000, 
sim time next is 478800.0000, 
raw observation next is [-1.2, 28.0, 124.0, 0.0, 26.0, 24.98717643856152, 0.1569680169613915, 1.0, 1.0, 73046.70398085014], 
processed observation next is [1.0, 0.5652173913043478, 0.42936288088642666, 0.28, 0.41333333333333333, 0.0, 0.6666666666666666, 0.5822647032134599, 0.5523226723204638, 1.0, 1.0, 0.34784144752785784], 
reward next is 0.6522, 
noisyNet noise sample is [array([0.92148364], dtype=float32), 0.2618357]. 
=============================================
[2019-04-04 15:57:07,530] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.7062252e-10 1.5996340e-10 5.8696440e-16 2.2877833e-15 1.0000000e+00
 1.9180995e-11 4.3457722e-16], sum to 1.0000
[2019-04-04 15:57:07,531] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1545
[2019-04-04 15:57:07,564] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.15, 83.5, 0.0, 0.0, 26.0, 24.63705609063565, 0.1965082454869978, 0.0, 1.0, 40301.92992875182], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 534600.0000, 
sim time next is 535200.0000, 
raw observation next is [1.966666666666667, 84.0, 0.0, 0.0, 26.0, 24.61235870966858, 0.1916643380998029, 0.0, 1.0, 40381.52118663226], 
processed observation next is [0.0, 0.17391304347826086, 0.5170821791320407, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5510298924723817, 0.5638881126999343, 0.0, 1.0, 0.19229295803158217], 
reward next is 0.8077, 
noisyNet noise sample is [array([1.5346391], dtype=float32), 2.322248]. 
=============================================
[2019-04-04 15:57:12,235] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.95409241e-09 8.69059658e-10 1.17907986e-14 2.87264190e-13
 1.00000000e+00 2.44329601e-09 6.22113241e-14], sum to 1.0000
[2019-04-04 15:57:12,235] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1860
[2019-04-04 15:57:12,316] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.2, 75.0, 19.0, 0.0, 26.0, 23.74291597577906, 0.089077775979575, 0.0, 1.0, 203163.5345786391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 635400.0000, 
sim time next is 636000.0000, 
raw observation next is [-4.1, 73.66666666666666, 38.33333333333333, 8.499999999999998, 26.0, 24.40412218314254, 0.1910318033902373, 0.0, 1.0, 154797.5837856291], 
processed observation next is [0.0, 0.34782608695652173, 0.3490304709141275, 0.7366666666666666, 0.12777777777777777, 0.009392265193370164, 0.6666666666666666, 0.5336768485952117, 0.5636772677967458, 0.0, 1.0, 0.7371313513601386], 
reward next is 0.2629, 
noisyNet noise sample is [array([0.24853715], dtype=float32), 1.2455236]. 
=============================================
[2019-04-04 15:57:12,323] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.305405]
 [77.14966 ]
 [76.1026  ]
 [75.99115 ]
 [75.86183 ]], R is [[78.87026978]
 [78.11412048]
 [77.36929321]
 [77.38615417]
 [77.4029007 ]].
[2019-04-04 15:57:23,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.9330914e-09 1.3321325e-08 1.7330013e-14 2.3510406e-12 1.0000000e+00
 5.0332921e-10 2.3988490e-14], sum to 1.0000
[2019-04-04 15:57:23,166] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2276
[2019-04-04 15:57:23,172] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.08333333333333, 95.5, 0.0, 0.0, 26.0, 23.69239481485265, 0.1869648008425449, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1227000.0000, 
sim time next is 1227600.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.6694380135571, 0.1823527124382886, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.47245316779642516, 0.5607842374794295, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.59048176], dtype=float32), -0.28046024]. 
=============================================
[2019-04-04 15:57:25,822] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5640012e-10 8.5640817e-12 2.2713360e-18 4.3628642e-16 1.0000000e+00
 5.0148596e-12 1.5743230e-18], sum to 1.0000
[2019-04-04 15:57:25,822] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7664
[2019-04-04 15:57:25,843] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.9000000000000001, 92.0, 106.1666666666667, 0.0, 26.0, 26.09995478100097, 0.5899821423180988, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1334400.0000, 
sim time next is 1335000.0000, 
raw observation next is [1.0, 92.0, 110.3333333333333, 0.0, 26.0, 26.10696861605725, 0.5911839350635207, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.92, 0.36777777777777765, 0.0, 0.6666666666666666, 0.6755807180047709, 0.6970613116878402, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28560665], dtype=float32), -1.1099792]. 
=============================================
[2019-04-04 15:57:25,849] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[93.922935]
 [93.71941 ]
 [93.60774 ]
 [93.5189  ]
 [93.450745]], R is [[94.10784912]
 [94.16677094]
 [94.22510529]
 [94.28285217]
 [94.34002686]].
[2019-04-04 15:57:29,006] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2470933e-09 7.2811390e-10 8.7811169e-15 2.9427439e-14 1.0000000e+00
 9.2849267e-10 1.1024791e-14], sum to 1.0000
[2019-04-04 15:57:29,006] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6877
[2019-04-04 15:57:29,022] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.3, 82.33333333333334, 0.0, 0.0, 26.0, 24.83370908585168, 0.2567675518681377, 0.0, 1.0, 41560.48334186422], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 857400.0000, 
sim time next is 858000.0000, 
raw observation next is [-3.2, 81.66666666666667, 0.0, 0.0, 26.0, 24.81111378945771, 0.251325396764369, 0.0, 1.0, 41484.55889650544], 
processed observation next is [1.0, 0.9565217391304348, 0.37396121883656513, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.5675928157881426, 0.5837751322547896, 0.0, 1.0, 0.1975455185547878], 
reward next is 0.8025, 
noisyNet noise sample is [array([0.54951304], dtype=float32), 1.5528044]. 
=============================================
[2019-04-04 15:57:29,035] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[80.94822 ]
 [80.80991 ]
 [80.657814]
 [80.48939 ]
 [80.43105 ]], R is [[80.94650269]
 [80.93913269]
 [80.93154907]
 [80.9240036 ]
 [80.91548157]].
[2019-04-04 15:57:29,487] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.976288e-10 2.190924e-10 7.026089e-16 4.873688e-15 1.000000e+00
 5.771454e-11 8.097625e-17], sum to 1.0000
[2019-04-04 15:57:29,488] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8804
[2019-04-04 15:57:29,554] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.45272369469817, 0.4729381533723028, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1409400.0000, 
sim time next is 1410000.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.56988399704093, 0.4634611704376015, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6308236664200774, 0.6544870568125338, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41816586], dtype=float32), -1.2130444]. 
=============================================
[2019-04-04 15:57:29,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[88.94341 ]
 [89.300896]
 [87.906006]
 [87.94218 ]
 [87.98487 ]], R is [[90.11242676]
 [90.21130371]
 [90.30918884]
 [90.22212219]
 [90.13594055]].
[2019-04-04 15:57:34,768] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4316133e-09 1.4408773e-10 6.0606544e-17 1.3317677e-14 1.0000000e+00
 6.5738567e-11 2.0823740e-16], sum to 1.0000
[2019-04-04 15:57:34,769] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9988
[2019-04-04 15:57:34,777] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.0, 63.0, 0.0, 0.0, 26.0, 25.85834131782396, 0.6974888628006704, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1114200.0000, 
sim time next is 1114800.0000, 
raw observation next is [12.9, 63.33333333333333, 0.0, 0.0, 26.0, 25.86903651974072, 0.692406211023188, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8199445983379503, 0.6333333333333333, 0.0, 0.0, 0.6666666666666666, 0.6557530433117268, 0.7308020703410626, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40705213], dtype=float32), -0.46148372]. 
=============================================
[2019-04-04 15:57:36,339] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.7654966e-11 1.6597544e-12 1.1405103e-18 1.2155644e-17 1.0000000e+00
 1.0818871e-11 1.0311280e-18], sum to 1.0000
[2019-04-04 15:57:36,341] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4388
[2019-04-04 15:57:36,351] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.55, 83.5, 119.0, 0.0, 26.0, 26.28732920525865, 0.6472029538507013, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 999000.0000, 
sim time next is 999600.0000, 
raw observation next is [13.83333333333333, 82.66666666666667, 114.8333333333333, 0.0, 26.0, 26.42200492643954, 0.6687111653216687, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8457987072945522, 0.8266666666666667, 0.38277777777777766, 0.0, 0.6666666666666666, 0.7018337438699618, 0.7229037217738896, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.82159466], dtype=float32), -0.629857]. 
=============================================
[2019-04-04 15:57:37,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7767295e-09 4.5314513e-10 6.4646706e-15 1.7063809e-14 1.0000000e+00
 2.4400986e-11 5.7457629e-15], sum to 1.0000
[2019-04-04 15:57:37,232] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9819
[2019-04-04 15:57:37,246] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.6, 58.00000000000001, 0.0, 0.0, 26.0, 26.15194938505705, 0.7121613283182157, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1106400.0000, 
sim time next is 1107000.0000, 
raw observation next is [14.4, 58.5, 0.0, 0.0, 26.0, 26.01673206486544, 0.6957325988332679, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.585, 0.0, 0.0, 0.6666666666666666, 0.6680610054054533, 0.7319108662777559, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08099359], dtype=float32), -0.049355872]. 
=============================================
[2019-04-04 15:57:37,264] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.644775]
 [81.29049 ]
 [80.127075]
 [79.960815]
 [81.63951 ]], R is [[80.16460419]
 [80.36296082]
 [80.5593338 ]
 [80.7537384 ]
 [80.94620514]].
[2019-04-04 15:57:38,661] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.97161129e-10 1.01387274e-10 1.48325131e-17 3.77567393e-15
 1.00000000e+00 2.57292087e-11 1.25057771e-16], sum to 1.0000
[2019-04-04 15:57:38,669] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0249
[2019-04-04 15:57:38,682] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.083333333333334, 82.0, 0.0, 0.0, 26.0, 25.50739211132708, 0.5538633692975163, 0.0, 1.0, 54057.90749161537], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1554600.0000, 
sim time next is 1555200.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.47615539470172, 0.5532872035109152, 0.0, 1.0, 58752.76207859685], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6230129495584767, 0.6844290678369718, 0.0, 1.0, 0.27977505751712783], 
reward next is 0.7202, 
noisyNet noise sample is [array([-0.12888241], dtype=float32), 0.53297096]. 
=============================================
[2019-04-04 15:57:40,103] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1014848e-09 3.8822892e-10 1.8130816e-15 9.2435594e-14 1.0000000e+00
 1.9928928e-10 4.5124156e-14], sum to 1.0000
[2019-04-04 15:57:40,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5346
[2019-04-04 15:57:40,112] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.38333333333333, 64.66666666666667, 19.33333333333334, 0.0, 26.0, 24.95630396905947, 0.4613817278594322, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1183800.0000, 
sim time next is 1184400.0000, 
raw observation next is [18.3, 65.0, 14.5, 0.0, 26.0, 24.93528120249126, 0.4565269821222449, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.65, 0.04833333333333333, 0.0, 0.6666666666666666, 0.5779401002076051, 0.652175660707415, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.66727597], dtype=float32), 0.8903526]. 
=============================================
[2019-04-04 15:57:40,949] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3933800e-09 1.1835571e-10 1.6852204e-15 2.1450536e-15 1.0000000e+00
 2.2853380e-10 8.8863158e-16], sum to 1.0000
[2019-04-04 15:57:40,950] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8671
[2019-04-04 15:57:40,959] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.53333333333333, 64.66666666666667, 0.0, 0.0, 26.0, 25.75688258905783, 0.6538773999553766, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1117200.0000, 
sim time next is 1117800.0000, 
raw observation next is [12.45, 65.0, 0.0, 0.0, 26.0, 25.70857329790896, 0.6426826768894213, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8074792243767314, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6423811081590799, 0.7142275589631404, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17433354], dtype=float32), -0.92661834]. 
=============================================
[2019-04-04 15:57:41,564] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6370199e-09 8.0468449e-10 4.5301092e-15 9.4643884e-14 1.0000000e+00
 4.7654498e-11 1.0277686e-14], sum to 1.0000
[2019-04-04 15:57:41,568] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8449
[2019-04-04 15:57:41,574] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 65.0, 120.0, 0.0, 26.0, 25.06659119900056, 0.4975187137436901, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1176000.0000, 
sim time next is 1176600.0000, 
raw observation next is [18.3, 65.0, 112.0, 0.0, 26.0, 25.05496969162755, 0.4945159504815321, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.37333333333333335, 0.0, 0.6666666666666666, 0.5879141409689627, 0.6648386501605107, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1015023], dtype=float32), 1.5230637]. 
=============================================
[2019-04-04 15:57:44,924] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1852993e-09 2.2734170e-10 1.2056006e-14 2.8894514e-13 1.0000000e+00
 1.8491106e-10 1.7584465e-14], sum to 1.0000
[2019-04-04 15:57:44,925] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9982
[2019-04-04 15:57:44,931] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.1, 79.33333333333333, 0.0, 0.0, 26.0, 24.12386493166898, 0.2835439597778939, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1212000.0000, 
sim time next is 1212600.0000, 
raw observation next is [16.1, 79.66666666666667, 0.0, 0.0, 26.0, 24.14419765435936, 0.2794004776141606, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5120164711966133, 0.5931334925380535, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12675764], dtype=float32), 1.1757822]. 
=============================================
[2019-04-04 15:57:51,088] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.6727298e-10 4.2738653e-11 2.0167283e-16 3.2486059e-16 1.0000000e+00
 1.0593161e-10 1.3636812e-16], sum to 1.0000
[2019-04-04 15:57:51,089] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1970
[2019-04-04 15:57:51,106] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 89.0, 0.0, 0.0, 26.0, 25.4708141889144, 0.5593187180845671, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1458000.0000, 
sim time next is 1458600.0000, 
raw observation next is [1.516666666666667, 89.5, 0.0, 0.0, 26.0, 25.54640891455367, 0.559402699966962, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5046168051708219, 0.895, 0.0, 0.0, 0.6666666666666666, 0.6288674095461392, 0.686467566655654, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45243564], dtype=float32), 1.4892828]. 
=============================================
[2019-04-04 15:57:57,781] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9365026e-10 2.5839815e-11 3.2565204e-17 1.0953599e-16 1.0000000e+00
 2.6797094e-11 8.4638338e-17], sum to 1.0000
[2019-04-04 15:57:57,782] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4942
[2019-04-04 15:57:57,796] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.800000000000001, 83.0, 100.0, 700.0, 26.0, 25.86404210075208, 0.5672829693584897, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1513800.0000, 
sim time next is 1514400.0000, 
raw observation next is [6.266666666666667, 79.66666666666667, 97.5, 700.1666666666667, 26.0, 25.04896042811094, 0.4998282251462601, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6361957525392429, 0.7966666666666667, 0.325, 0.7736648250460406, 0.6666666666666666, 0.5874133690092451, 0.6666094083820867, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.606643], dtype=float32), 0.31767756]. 
=============================================
[2019-04-04 15:58:05,056] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5800408e-11 4.3092835e-11 3.5165876e-17 2.3920650e-17 1.0000000e+00
 1.3370433e-11 1.1073879e-17], sum to 1.0000
[2019-04-04 15:58:05,062] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1615
[2019-04-04 15:58:05,098] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.75, 92.0, 30.0, 0.0, 26.0, 25.61121932650918, 0.5147705210505017, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1672200.0000, 
sim time next is 1672800.0000, 
raw observation next is [2.566666666666667, 92.0, 33.83333333333333, 0.0, 26.0, 25.72070642675004, 0.5391370567125381, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5337026777469991, 0.92, 0.11277777777777777, 0.0, 0.6666666666666666, 0.6433922022291702, 0.6797123522375127, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.425681], dtype=float32), 0.38349187]. 
=============================================
[2019-04-04 15:58:06,334] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2970209e-09 4.8759663e-11 5.6702232e-17 2.2667087e-15 1.0000000e+00
 5.9634117e-11 4.5391882e-16], sum to 1.0000
[2019-04-04 15:58:06,335] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2379
[2019-04-04 15:58:06,354] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.34741150621355, 0.4688759645000855, 0.0, 1.0, 43155.96083755461], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1729200.0000, 
sim time next is 1729800.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.34379746092989, 0.466308905971616, 0.0, 1.0, 43102.71766610291], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6119831217441574, 0.6554363019905387, 0.0, 1.0, 0.20525103650525195], 
reward next is 0.7947, 
noisyNet noise sample is [array([0.92016757], dtype=float32), 0.049503516]. 
=============================================
[2019-04-04 15:58:10,413] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5771073e-09 6.7915723e-10 4.6665167e-15 2.3988136e-13 1.0000000e+00
 2.0826991e-09 1.3318896e-14], sum to 1.0000
[2019-04-04 15:58:10,413] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0770
[2019-04-04 15:58:10,429] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.583333333333333, 86.0, 0.0, 0.0, 26.0, 24.59211190686628, 0.213487994398448, 0.0, 1.0, 42736.2127536391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2081400.0000, 
sim time next is 2082000.0000, 
raw observation next is [-4.666666666666667, 86.0, 0.0, 0.0, 26.0, 24.63242083288477, 0.2098084902511472, 0.0, 1.0, 42742.73320311969], 
processed observation next is [1.0, 0.08695652173913043, 0.3333333333333333, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5527017360737307, 0.5699361634170491, 0.0, 1.0, 0.20353682477676044], 
reward next is 0.7965, 
noisyNet noise sample is [array([1.7100059], dtype=float32), -0.24358714]. 
=============================================
[2019-04-04 15:58:10,434] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[79.32936 ]
 [79.374344]
 [79.55067 ]
 [79.501114]
 [79.64008 ]], R is [[79.27626801]
 [79.27999878]
 [79.28366852]
 [79.28736877]
 [79.29110718]].
[2019-04-04 15:58:14,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4323570e-08 2.3063493e-09 2.9893246e-14 2.2488912e-13 1.0000000e+00
 3.6998174e-10 3.7342799e-14], sum to 1.0000
[2019-04-04 15:58:14,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1227
[2019-04-04 15:58:14,295] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.199999999999999, 80.33333333333334, 0.0, 0.0, 26.0, 23.7484162462781, 0.01056195952734516, 0.0, 1.0, 46988.89903472168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1831200.0000, 
sim time next is 1831800.0000, 
raw observation next is [-6.2, 79.66666666666667, 0.0, 0.0, 26.0, 23.71202925239799, 0.003305331476134062, 0.0, 1.0, 47022.70051528562], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.47600243769983247, 0.5011017771587113, 0.0, 1.0, 0.2239176215013601], 
reward next is 0.7761, 
noisyNet noise sample is [array([0.7816806], dtype=float32), -0.030030925]. 
=============================================
[2019-04-04 15:58:17,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3786847e-09 5.8124727e-11 7.1501953e-16 1.9744431e-15 1.0000000e+00
 1.4143887e-10 4.7151862e-16], sum to 1.0000
[2019-04-04 15:58:17,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3229
[2019-04-04 15:58:17,270] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.833333333333334, 81.33333333333334, 99.33333333333334, 496.8333333333333, 26.0, 25.64870273224192, 0.3023858389620486, 1.0, 1.0, 19330.05588518688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1935600.0000, 
sim time next is 1936200.0000, 
raw observation next is [-7.566666666666666, 80.16666666666667, 113.6666666666667, 444.6666666666667, 26.0, 25.64999854351018, 0.3051916386090234, 1.0, 1.0, 21621.63947403311], 
processed observation next is [1.0, 0.391304347826087, 0.2530009233610342, 0.8016666666666667, 0.378888888888889, 0.4913443830570903, 0.6666666666666666, 0.6374998786258482, 0.6017305462030078, 1.0, 1.0, 0.10296018797158624], 
reward next is 0.8970, 
noisyNet noise sample is [array([0.77579933], dtype=float32), 0.47288075]. 
=============================================
[2019-04-04 15:58:32,317] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4776546e-09 5.2006954e-11 2.1741506e-15 4.5697797e-14 1.0000000e+00
 2.1779870e-10 3.8802211e-15], sum to 1.0000
[2019-04-04 15:58:32,317] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0965
[2019-04-04 15:58:32,359] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.80720439953568, 0.4543548338294279, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2056200.0000, 
sim time next is 2056800.0000, 
raw observation next is [-3.9, 83.33333333333334, 0.0, 0.0, 26.0, 25.70976880488472, 0.41794603542989, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6424807337403934, 0.6393153451432967, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9485196], dtype=float32), 0.53301656]. 
=============================================
[2019-04-04 15:58:32,692] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3742563e-09 1.6094734e-10 7.8050537e-15 2.1471831e-14 1.0000000e+00
 3.3367101e-10 7.2107571e-16], sum to 1.0000
[2019-04-04 15:58:32,693] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5247
[2019-04-04 15:58:32,736] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 82.0, 51.5, 0.0, 26.0, 25.6591874945291, 0.4100955332805907, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2044800.0000, 
sim time next is 2045400.0000, 
raw observation next is [-3.9, 82.00000000000001, 45.0, 0.0, 26.0, 25.86807795872235, 0.4128267448173003, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.8200000000000002, 0.15, 0.0, 0.6666666666666666, 0.6556731632268624, 0.6376089149391001, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1379433], dtype=float32), -0.32048213]. 
=============================================
[2019-04-04 15:58:34,796] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-04 15:58:34,799] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 15:58:34,800] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:58:34,800] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 15:58:34,801] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 15:58:34,802] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:58:34,802] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 15:58:35,426] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run41
[2019-04-04 15:58:35,450] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run41
[2019-04-04 15:58:35,451] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run41
[2019-04-04 15:58:52,172] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17858922], dtype=float32), 0.23274545]
[2019-04-04 15:58:52,173] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-14.94826257333333, 56.13899682333334, 102.07691639, 726.1590166, 26.0, 25.6183775136458, 0.3171145511056825, 1.0, 1.0, 97141.85728440378]
[2019-04-04 15:58:52,173] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 15:58:52,173] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.3864921e-09 9.1626573e-10 9.1733702e-15 6.6652555e-14 1.0000000e+00
 4.3511100e-10 1.7280899e-14], sampled 0.2640558895969679
[2019-04-04 15:59:08,229] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17858922], dtype=float32), 0.23274545]
[2019-04-04 15:59:08,229] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.8311060005, 94.69005735666667, 22.52719959, 0.0, 26.0, 25.08054158247037, 0.280686246915172, 1.0, 1.0, 0.0]
[2019-04-04 15:59:08,229] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 15:59:08,230] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.5200223e-10 5.0779062e-11 1.5168285e-16 2.4340044e-15 1.0000000e+00
 3.2207154e-11 1.9022098e-16], sampled 0.4746443561324343
[2019-04-04 15:59:16,555] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17858922], dtype=float32), 0.23274545]
[2019-04-04 15:59:16,556] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [9.8, 71.0, 0.0, 0.0, 26.0, 25.6074573919634, 0.5768808764508668, 0.0, 1.0, 35880.99444360162]
[2019-04-04 15:59:16,556] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 15:59:16,556] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.8111437e-10 3.5285979e-11 6.2073380e-17 9.7100215e-16 1.0000000e+00
 1.2290854e-11 9.8443962e-17], sampled 0.6978051615696726
[2019-04-04 16:00:16,981] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 16:00:36,424] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 16:00:38,719] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 16:00:39,742] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 4000000, evaluation results [4000000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 16:00:40,363] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0585459e-09 3.2638259e-10 1.9238049e-14 1.0001035e-14 1.0000000e+00
 1.0837635e-09 1.6970389e-15], sum to 1.0000
[2019-04-04 16:00:40,364] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7112
[2019-04-04 16:00:40,411] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.21337421468839, 0.4197677922716005, 0.0, 1.0, 77955.55148427679], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2061000.0000, 
sim time next is 2061600.0000, 
raw observation next is [-3.899999999999999, 86.0, 0.0, 0.0, 26.0, 25.26755232067439, 0.4270578525363126, 0.0, 1.0, 55099.86548829035], 
processed observation next is [1.0, 0.8695652173913043, 0.35457063711911363, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6056293600561992, 0.6423526175121043, 0.0, 1.0, 0.2623803118490017], 
reward next is 0.7376, 
noisyNet noise sample is [array([-0.33048877], dtype=float32), -1.218472]. 
=============================================
[2019-04-04 16:00:41,117] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2015117e-08 2.5150384e-09 2.9211690e-14 1.6304972e-13 1.0000000e+00
 6.7160338e-10 1.3101656e-14], sum to 1.0000
[2019-04-04 16:00:41,119] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6739
[2019-04-04 16:00:41,146] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.283333333333333, 86.33333333333333, 0.0, 0.0, 26.0, 24.11543424104069, 0.1041199616978891, 0.0, 1.0, 43822.83469472344], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2092200.0000, 
sim time next is 2092800.0000, 
raw observation next is [-6.366666666666667, 85.66666666666667, 0.0, 0.0, 26.0, 24.17871140862254, 0.1069521174714709, 0.0, 1.0, 43565.18079941847], 
processed observation next is [1.0, 0.21739130434782608, 0.28624192059095105, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5148926173852116, 0.5356507058238237, 0.0, 1.0, 0.2074532419019927], 
reward next is 0.7925, 
noisyNet noise sample is [array([-0.17902927], dtype=float32), 0.5386742]. 
=============================================
[2019-04-04 16:00:47,025] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.2830663e-09 1.5594828e-09 3.9731192e-14 5.6562760e-14 1.0000000e+00
 4.9197446e-10 9.2385538e-14], sum to 1.0000
[2019-04-04 16:00:47,028] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6715
[2019-04-04 16:00:47,041] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.9666666666666667, 36.0, 0.0, 0.0, 26.0, 25.13442353306431, 0.2362241604483756, 0.0, 1.0, 39826.3370697416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2503200.0000, 
sim time next is 2503800.0000, 
raw observation next is [-1.15, 36.5, 0.0, 0.0, 26.0, 25.1185014532935, 0.231527154231455, 0.0, 1.0, 39723.49990331775], 
processed observation next is [0.0, 1.0, 0.4307479224376732, 0.365, 0.0, 0.0, 0.6666666666666666, 0.593208454441125, 0.5771757180771516, 0.0, 1.0, 0.18915952334913216], 
reward next is 0.8108, 
noisyNet noise sample is [array([-0.8091557], dtype=float32), 0.0017364299]. 
=============================================
[2019-04-04 16:00:51,261] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.09013534e-09 2.76268786e-09 1.16684675e-14 5.73576898e-14
 1.00000000e+00 6.58413712e-10 1.37026333e-14], sum to 1.0000
[2019-04-04 16:00:51,262] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2169
[2019-04-04 16:00:51,334] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 69.5, 0.0, 0.0, 26.0, 24.62713541657186, 0.2786356089839603, 1.0, 1.0, 198788.7899079951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2223000.0000, 
sim time next is 2223600.0000, 
raw observation next is [-4.5, 69.0, 0.0, 0.0, 26.0, 24.81157685581888, 0.4004334896178374, 1.0, 1.0, 191048.2190190194], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5676314046515735, 0.6334778298726125, 1.0, 1.0, 0.9097534239000924], 
reward next is 0.0902, 
noisyNet noise sample is [array([1.4372634], dtype=float32), 1.492903]. 
=============================================
[2019-04-04 16:00:53,485] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.9108514e-09 1.6928575e-09 3.2634255e-15 6.9834492e-14 1.0000000e+00
 2.6402194e-10 6.4101061e-15], sum to 1.0000
[2019-04-04 16:00:53,485] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0836
[2019-04-04 16:00:53,542] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.116666666666667, 85.5, 82.33333333333334, 31.33333333333334, 26.0, 25.64342517018971, 0.3027720383717242, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2279400.0000, 
sim time next is 2280000.0000, 
raw observation next is [-7.833333333333334, 84.0, 91.66666666666667, 35.16666666666666, 26.0, 25.60224602911239, 0.300337264670916, 1.0, 1.0, 18741.36186803853], 
processed observation next is [1.0, 0.391304347826087, 0.2456140350877193, 0.84, 0.3055555555555556, 0.038858195211786364, 0.6666666666666666, 0.6335205024260325, 0.600112421556972, 1.0, 1.0, 0.089244580323993], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.5314857], dtype=float32), -1.3461602]. 
=============================================
[2019-04-04 16:00:53,546] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.72453 ]
 [82.88076 ]
 [83.015366]
 [83.15979 ]
 [83.21298 ]], R is [[82.7294693 ]
 [82.9021759 ]
 [83.07315826]
 [83.24242401]
 [83.41000366]].
[2019-04-04 16:00:53,925] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5013230e-08 4.0031685e-09 7.4389496e-14 8.8856179e-13 1.0000000e+00
 3.0399250e-09 3.0725493e-14], sum to 1.0000
[2019-04-04 16:00:53,926] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8351
[2019-04-04 16:00:53,950] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.900000000000002, 91.0, 0.0, 0.0, 26.0, 23.69355740786933, -0.008804508397183941, 0.0, 1.0, 43320.51480679933], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2265600.0000, 
sim time next is 2266200.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.68407757114236, -0.01663578595711585, 0.0, 1.0, 43271.2803997323], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.47367313092853003, 0.49445473801429474, 0.0, 1.0, 0.20605371618920143], 
reward next is 0.7939, 
noisyNet noise sample is [array([-0.0941002], dtype=float32), -0.05828227]. 
=============================================
[2019-04-04 16:00:54,216] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0246130e-07 1.2335164e-08 1.1400933e-13 5.7113065e-13 9.9999988e-01
 2.3181432e-08 1.1518725e-13], sum to 1.0000
[2019-04-04 16:00:54,217] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4445
[2019-04-04 16:00:54,230] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.2, 91.0, 0.0, 0.0, 26.0, 23.52940267793915, -0.05221051923191525, 0.0, 1.0, 43142.95608769925], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2269800.0000, 
sim time next is 2270400.0000, 
raw observation next is [-9.3, 91.0, 0.0, 0.0, 26.0, 23.5329613668323, -0.06176326675082239, 0.0, 1.0, 43108.37511797318], 
processed observation next is [1.0, 0.2608695652173913, 0.20498614958448752, 0.91, 0.0, 0.0, 0.6666666666666666, 0.46108011390269166, 0.4794122444163926, 0.0, 1.0, 0.20527797675225323], 
reward next is 0.7947, 
noisyNet noise sample is [array([0.04747353], dtype=float32), 0.5566725]. 
=============================================
[2019-04-04 16:00:59,076] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.57769477e-08 1.36599509e-09 6.37878896e-14 2.10888151e-13
 1.00000000e+00 1.02019575e-08 5.49143928e-14], sum to 1.0000
[2019-04-04 16:00:59,077] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6303
[2019-04-04 16:00:59,087] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 64.0, 0.0, 0.0, 26.0, 24.93851977188574, 0.2885731181041361, 0.0, 1.0, 38405.31497745497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2334000.0000, 
sim time next is 2334600.0000, 
raw observation next is [-2.3, 63.5, 0.0, 0.0, 26.0, 24.88476976725409, 0.2796836047760086, 0.0, 1.0, 38405.67156784458], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.635, 0.0, 0.0, 0.6666666666666666, 0.573730813937841, 0.5932278682586696, 0.0, 1.0, 0.18288415032306946], 
reward next is 0.8171, 
noisyNet noise sample is [array([0.5656928], dtype=float32), 0.617898]. 
=============================================
[2019-04-04 16:01:00,466] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1075482e-09 8.0558132e-10 2.7404379e-15 1.7913170e-14 1.0000000e+00
 1.2248135e-09 2.4318988e-14], sum to 1.0000
[2019-04-04 16:01:00,468] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5077
[2019-04-04 16:01:00,526] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.933333333333333, 57.0, 162.3333333333333, 330.0, 26.0, 25.03557437893017, 0.3150167121052453, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2373600.0000, 
sim time next is 2374200.0000, 
raw observation next is [-1.75, 54.5, 167.0, 306.0, 26.0, 25.02089385311508, 0.3041878930014714, 0.0, 1.0, 18727.72755412558], 
processed observation next is [0.0, 0.4782608695652174, 0.4141274238227147, 0.545, 0.5566666666666666, 0.33812154696132596, 0.6666666666666666, 0.58507448775959, 0.6013959643338238, 0.0, 1.0, 0.08917965501964562], 
reward next is 0.9108, 
noisyNet noise sample is [array([1.8567954], dtype=float32), 0.24873863]. 
=============================================
[2019-04-04 16:01:07,207] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.6470314e-08 6.0803389e-09 2.3019044e-14 1.2144219e-13 9.9999988e-01
 8.3282886e-10 2.8647772e-14], sum to 1.0000
[2019-04-04 16:01:07,207] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2787
[2019-04-04 16:01:07,234] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.8, 27.66666666666667, 87.5, 834.1666666666667, 26.0, 24.96967212508838, 0.2724280723877689, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2467200.0000, 
sim time next is 2467800.0000, 
raw observation next is [1.9, 27.5, 87.0, 832.0, 26.0, 24.96753885545584, 0.2717845547127604, 0.0, 1.0, 18709.64947918148], 
processed observation next is [0.0, 0.5652173913043478, 0.515235457063712, 0.275, 0.29, 0.9193370165745857, 0.6666666666666666, 0.5806282379546532, 0.5905948515709202, 0.0, 1.0, 0.08909356894848323], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.7480701], dtype=float32), -0.387603]. 
=============================================
[2019-04-04 16:01:10,615] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.5259204e-10 2.5924887e-10 1.1028403e-15 2.1093150e-14 1.0000000e+00
 1.6294550e-10 2.3141063e-15], sum to 1.0000
[2019-04-04 16:01:10,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0384
[2019-04-04 16:01:10,625] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.266666666666667, 27.33333333333334, 169.0, 447.5, 26.0, 25.63608393242848, 0.3692025396751665, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2554800.0000, 
sim time next is 2555400.0000, 
raw observation next is [3.533333333333333, 26.66666666666667, 167.0, 413.0, 26.0, 25.78976682835471, 0.3643415257364966, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5604801477377656, 0.2666666666666667, 0.5566666666666666, 0.456353591160221, 0.6666666666666666, 0.6491472356962259, 0.6214471752454989, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7955577], dtype=float32), 0.52044207]. 
=============================================
[2019-04-04 16:01:12,344] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0378943e-09 7.2941736e-11 1.8971598e-15 6.8416892e-15 1.0000000e+00
 3.3337947e-11 2.2868320e-15], sum to 1.0000
[2019-04-04 16:01:12,344] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6831
[2019-04-04 16:01:12,351] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8166666666666668, 40.33333333333333, 227.3333333333333, 54.66666666666667, 26.0, 25.74900420966059, 0.3266048234068262, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2548200.0000, 
sim time next is 2548800.0000, 
raw observation next is [1.1, 39.0, 225.0, 46.5, 26.0, 25.74823556740443, 0.3315675070646803, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.39, 0.75, 0.05138121546961326, 0.6666666666666666, 0.6456862972837024, 0.6105225023548934, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.81255], dtype=float32), 0.45961407]. 
=============================================
[2019-04-04 16:01:18,402] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7597702e-09 2.4063052e-10 7.5449417e-16 1.7042603e-14 1.0000000e+00
 1.3789102e-10 1.3094854e-15], sum to 1.0000
[2019-04-04 16:01:18,407] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8996
[2019-04-04 16:01:18,437] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.083333333333333, 62.5, 176.0, 240.3333333333333, 26.0, 25.73674000120192, 0.3659970881998338, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2631000.0000, 
sim time next is 2631600.0000, 
raw observation next is [-3.9, 62.0, 188.0, 223.0, 26.0, 25.72983903135655, 0.3624869343925809, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3545706371191136, 0.62, 0.6266666666666667, 0.24640883977900552, 0.6666666666666666, 0.6441532526130459, 0.6208289781308604, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3174824], dtype=float32), -0.17901357]. 
=============================================
[2019-04-04 16:01:18,837] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.96304267e-09 4.40906556e-09 2.74656159e-14 8.72626190e-13
 1.00000000e+00 1.03255759e-09 1.12567894e-13], sum to 1.0000
[2019-04-04 16:01:18,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6724
[2019-04-04 16:01:18,854] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 71.16666666666667, 0.0, 0.0, 26.0, 23.76003163223161, -0.009147425199142195, 0.0, 1.0, 40215.70086086091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3046200.0000, 
sim time next is 3046800.0000, 
raw observation next is [-6.0, 72.33333333333334, 0.0, 0.0, 26.0, 23.7330721969261, -0.01542434045060288, 0.0, 1.0, 40257.18401207107], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.4777560164105082, 0.4948585531831324, 0.0, 1.0, 0.19170087624795748], 
reward next is 0.8083, 
noisyNet noise sample is [array([-0.00035863], dtype=float32), 1.7127907]. 
=============================================
[2019-04-04 16:01:20,649] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7796334e-09 3.6177786e-11 1.3053502e-16 1.1902466e-14 1.0000000e+00
 4.8033390e-11 1.1108203e-15], sum to 1.0000
[2019-04-04 16:01:20,650] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3155
[2019-04-04 16:01:20,668] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 26.0, 25.32411882882785, 0.3255890385534634, 0.0, 1.0, 51076.45258798439], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3108000.0000, 
sim time next is 3108600.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 26.0, 25.31655395984503, 0.325450011551193, 0.0, 1.0, 43995.63845416694], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6097128299870859, 0.6084833371837309, 0.0, 1.0, 0.2095030402579378], 
reward next is 0.7905, 
noisyNet noise sample is [array([-0.21838981], dtype=float32), -0.36900184]. 
=============================================
[2019-04-04 16:01:22,324] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4635257e-08 8.0961291e-09 2.6689846e-13 3.1087470e-12 1.0000000e+00
 1.0006752e-08 1.9032361e-13], sum to 1.0000
[2019-04-04 16:01:22,324] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6473
[2019-04-04 16:01:22,346] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.83333333333333, 83.0, 0.0, 0.0, 26.0, 23.08776589071056, -0.1098836645589719, 0.0, 1.0, 43425.87876517962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2699400.0000, 
sim time next is 2700000.0000, 
raw observation next is [-16.0, 83.0, 0.0, 0.0, 26.0, 23.08831823219908, -0.1238760929130464, 0.0, 1.0, 43343.64555769015], 
processed observation next is [1.0, 0.2608695652173913, 0.01939058171745151, 0.83, 0.0, 0.0, 0.6666666666666666, 0.42402651934992325, 0.45870796902898453, 0.0, 1.0, 0.2063983121794769], 
reward next is 0.7936, 
noisyNet noise sample is [array([0.10805525], dtype=float32), 0.5955971]. 
=============================================
[2019-04-04 16:01:22,350] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[69.545456]
 [69.82942 ]
 [70.10595 ]
 [70.371346]
 [70.63734 ]], R is [[69.37926483]
 [69.47868347]
 [69.57680511]
 [69.67353058]
 [69.76873779]].
[2019-04-04 16:01:22,361] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7610373e-09 3.2875552e-10 4.6678165e-15 1.0051828e-14 1.0000000e+00
 1.4938389e-09 5.4003221e-15], sum to 1.0000
[2019-04-04 16:01:22,361] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3088
[2019-04-04 16:01:22,376] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.666666666666666, 24.83333333333334, 87.0, 25.33333333333333, 26.0, 25.79043815289645, 0.4135690880039108, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2821800.0000, 
sim time next is 2822400.0000, 
raw observation next is [6.6, 25.0, 83.0, 38.0, 26.0, 25.91058962810022, 0.4272785654203761, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6454293628808865, 0.25, 0.27666666666666667, 0.041988950276243095, 0.6666666666666666, 0.6592158023416849, 0.6424261884734587, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1738199], dtype=float32), 0.090710916]. 
=============================================
[2019-04-04 16:01:22,782] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7216430e-08 1.7325398e-09 4.0166265e-14 1.3773914e-13 1.0000000e+00
 4.3221706e-09 3.9165086e-14], sum to 1.0000
[2019-04-04 16:01:22,782] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0163
[2019-04-04 16:01:22,800] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.76099326033708, 0.2224683840272108, 0.0, 1.0, 41709.81366177485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2772000.0000, 
sim time next is 2772600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.68585088684743, 0.2206022040159997, 0.0, 1.0, 41617.36115635196], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5571542405706191, 0.5735340680053332, 0.0, 1.0, 0.19817791026834267], 
reward next is 0.8018, 
noisyNet noise sample is [array([0.03656125], dtype=float32), 1.9658055]. 
=============================================
[2019-04-04 16:01:23,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4889150e-09 5.7173294e-11 1.0552957e-16 3.4577554e-15 1.0000000e+00
 9.5018084e-11 1.7356601e-15], sum to 1.0000
[2019-04-04 16:01:23,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2512
[2019-04-04 16:01:23,150] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 100.0, 0.0, 0.0, 26.0, 25.48563932737209, 0.2997386159822519, 0.0, 1.0, 23191.08641308888], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3121200.0000, 
sim time next is 3121800.0000, 
raw observation next is [2.1, 100.0, 0.0, 0.0, 26.0, 25.36069076651513, 0.2977900190480895, 0.0, 1.0, 99007.53519417213], 
processed observation next is [1.0, 0.13043478260869565, 0.5207756232686982, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6133908972095942, 0.5992633396826965, 0.0, 1.0, 0.47146445330558157], 
reward next is 0.5285, 
noisyNet noise sample is [array([-0.32013527], dtype=float32), -0.32706758]. 
=============================================
[2019-04-04 16:01:25,546] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0220430e-09 2.7727801e-10 1.2749318e-16 8.9401109e-15 1.0000000e+00
 9.2611863e-11 6.0202411e-16], sum to 1.0000
[2019-04-04 16:01:25,546] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7207
[2019-04-04 16:01:25,594] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 64.0, 122.6666666666667, 215.6666666666667, 26.0, 25.97073989789176, 0.3958755484246468, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2796600.0000, 
sim time next is 2797200.0000, 
raw observation next is [-6.0, 64.0, 130.0, 220.0, 26.0, 25.94047087890155, 0.3824461688610287, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.296398891966759, 0.64, 0.43333333333333335, 0.2430939226519337, 0.6666666666666666, 0.6617059065751292, 0.6274820562870096, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5801771], dtype=float32), -1.3903942]. 
=============================================
[2019-04-04 16:01:26,244] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6996634e-08 8.2303080e-09 4.6554796e-13 5.6794017e-13 9.9999988e-01
 1.4947915e-08 1.8711683e-13], sum to 1.0000
[2019-04-04 16:01:26,244] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3013
[2019-04-04 16:01:26,255] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.5, 61.5, 0.0, 0.0, 26.0, 24.47663804810006, 0.1493318662793169, 0.0, 1.0, 40796.00714950899], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2781000.0000, 
sim time next is 2781600.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 26.0, 24.48126200918618, 0.140989198123086, 0.0, 1.0, 40847.32819530793], 
processed observation next is [1.0, 0.17391304347826086, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5401051674321818, 0.546996399374362, 0.0, 1.0, 0.19451108664432346], 
reward next is 0.8055, 
noisyNet noise sample is [array([1.1366684], dtype=float32), -0.7294248]. 
=============================================
[2019-04-04 16:01:26,810] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.8763824e-10 4.0047528e-11 2.5729180e-17 2.9780990e-16 1.0000000e+00
 2.1760052e-11 1.7545842e-17], sum to 1.0000
[2019-04-04 16:01:26,810] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4542
[2019-04-04 16:01:26,831] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 90.33333333333334, 102.6666666666667, 776.1666666666667, 26.0, 26.82647024531245, 0.823871619616234, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3248400.0000, 
sim time next is 3249000.0000, 
raw observation next is [-3.0, 85.5, 101.0, 769.0, 26.0, 26.89286401289647, 0.8343189298899149, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.855, 0.33666666666666667, 0.8497237569060774, 0.6666666666666666, 0.7410720010747059, 0.778106309963305, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.87636155], dtype=float32), 0.5797323]. 
=============================================
[2019-04-04 16:01:26,840] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[91.91741 ]
 [92.02483 ]
 [92.04387 ]
 [91.971   ]
 [91.939095]], R is [[91.90370941]
 [91.98467255]
 [92.06482697]
 [92.1441803 ]
 [92.22274017]].
[2019-04-04 16:01:36,301] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5611726e-09 2.6065550e-10 3.5014899e-16 1.2633092e-14 1.0000000e+00
 1.3352218e-10 2.3480300e-15], sum to 1.0000
[2019-04-04 16:01:36,301] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8373
[2019-04-04 16:01:36,374] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 218.5, 487.5, 26.0, 25.0154117037099, 0.3796631308175777, 0.0, 1.0, 20236.14053968042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2980800.0000, 
sim time next is 2981400.0000, 
raw observation next is [-3.0, 65.0, 206.0, 555.3333333333334, 26.0, 25.03699551841049, 0.3835740888596506, 0.0, 1.0, 18914.12134872733], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.6866666666666666, 0.6136279926335175, 0.6666666666666666, 0.5864162932008741, 0.6278580296198836, 0.0, 1.0, 0.0900672445177492], 
reward next is 0.9099, 
noisyNet noise sample is [array([-0.873648], dtype=float32), 0.47059533]. 
=============================================
[2019-04-04 16:01:36,375] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.39142239e-09 2.46356630e-10 3.33560542e-16 1.19296036e-14
 1.00000000e+00 1.21752483e-10 2.41650051e-15], sum to 1.0000
[2019-04-04 16:01:36,376] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1305
[2019-04-04 16:01:36,442] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 206.0, 555.3333333333334, 26.0, 25.03699551841049, 0.3835740888596506, 0.0, 1.0, 18914.12134872733], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2981400.0000, 
sim time next is 2982000.0000, 
raw observation next is [-3.0, 65.0, 193.5, 623.1666666666667, 26.0, 25.04758641699102, 0.3867747562995048, 0.0, 1.0, 20972.21808509487], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.645, 0.6885819521178638, 0.6666666666666666, 0.587298868082585, 0.6289249187665016, 0.0, 1.0, 0.09986770516711843], 
reward next is 0.9001, 
noisyNet noise sample is [array([-0.873648], dtype=float32), 0.47059533]. 
=============================================
[2019-04-04 16:01:36,447] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[83.91365]
 [84.06403]
 [84.12261]
 [83.99694]
 [83.6531 ]], R is [[83.79245758]
 [83.86447144]
 [83.92946625]
 [83.94124603]
 [83.88980103]].
[2019-04-04 16:01:39,315] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0190008e-09 1.8108062e-09 8.8595470e-15 1.0899929e-13 1.0000000e+00
 1.3426689e-10 8.1857128e-15], sum to 1.0000
[2019-04-04 16:01:39,315] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1259
[2019-04-04 16:01:39,366] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 71.5, 3.0, 107.0, 26.0, 25.37672434919093, 0.4031467028837495, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3483000.0000, 
sim time next is 3483600.0000, 
raw observation next is [-0.6666666666666666, 71.33333333333333, 17.16666666666666, 155.6666666666667, 26.0, 25.49589557310507, 0.3939210791841697, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44413665743305636, 0.7133333333333333, 0.0572222222222222, 0.17200736648250467, 0.6666666666666666, 0.6246579644254225, 0.6313070263947232, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4399153], dtype=float32), 0.022175912]. 
=============================================
[2019-04-04 16:01:41,226] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0085903e-09 1.5479601e-10 1.3160908e-15 7.9980434e-15 1.0000000e+00
 3.5367646e-11 9.1336293e-16], sum to 1.0000
[2019-04-04 16:01:41,226] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3976
[2019-04-04 16:01:41,273] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.96200863393279, 0.2769924552415536, 0.0, 1.0, 38224.2023017942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3021000.0000, 
sim time next is 3021600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93082887562847, 0.275264098916418, 0.0, 1.0, 38151.03247958672], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5775690729690393, 0.591754699638806, 0.0, 1.0, 0.18167158323612723], 
reward next is 0.8183, 
noisyNet noise sample is [array([0.09991289], dtype=float32), 0.31248727]. 
=============================================
[2019-04-04 16:01:46,341] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.5603996e-10 1.2570887e-10 9.0336223e-17 3.6670970e-15 1.0000000e+00
 3.7101048e-11 4.3951517e-17], sum to 1.0000
[2019-04-04 16:01:46,344] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7175
[2019-04-04 16:01:46,353] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.84752034841645, 0.7112529270885184, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3189000.0000, 
sim time next is 3189600.0000, 
raw observation next is [2.0, 100.0, 0.0, 0.0, 26.0, 25.87782462001626, 0.7014821440637427, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6564853850013549, 0.7338273813545809, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4773405], dtype=float32), -0.8352211]. 
=============================================
[2019-04-04 16:01:46,863] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3237272e-10 9.9744570e-12 1.7810459e-17 8.7974364e-17 1.0000000e+00
 5.7632562e-12 8.1575832e-18], sum to 1.0000
[2019-04-04 16:01:46,863] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2443
[2019-04-04 16:01:46,873] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.666666666666666, 100.0, 0.0, 0.0, 26.0, 26.59308428014088, 0.8123461875782118, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3177600.0000, 
sim time next is 3178200.0000, 
raw observation next is [4.333333333333333, 100.0, 0.0, 0.0, 26.0, 26.54783908984825, 0.7978468587245514, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.58264081255771, 1.0, 0.0, 0.0, 0.6666666666666666, 0.7123199241540208, 0.7659489529081838, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1353142], dtype=float32), -0.7262067]. 
=============================================
[2019-04-04 16:01:48,020] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3142072e-11 3.1045104e-12 1.3202372e-18 4.7305695e-17 1.0000000e+00
 9.4762978e-13 8.1142525e-19], sum to 1.0000
[2019-04-04 16:01:48,021] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6006
[2019-04-04 16:01:48,039] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.666666666666666, 95.33333333333334, 113.8333333333333, 808.0, 26.0, 27.1543198533792, 0.7877491987856852, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3152400.0000, 
sim time next is 3153000.0000, 
raw observation next is [7.833333333333334, 94.16666666666666, 113.6666666666667, 811.0, 26.0, 27.1658024320778, 0.8023468797008455, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6795937211449677, 0.9416666666666665, 0.378888888888889, 0.8961325966850828, 0.6666666666666666, 0.7638168693398167, 0.7674489599002818, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03943537], dtype=float32), 1.510589]. 
=============================================
[2019-04-04 16:01:48,048] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[101.39592 ]
 [101.323074]
 [101.252754]
 [101.1943  ]
 [101.145836]], R is [[101.45168304]
 [101.43716431]
 [101.42279053]
 [101.40856171]
 [101.39447784]].
[2019-04-04 16:01:49,794] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.5844304e-11 3.3700445e-11 1.3281078e-17 2.3685940e-16 1.0000000e+00
 1.4323494e-11 7.5589961e-17], sum to 1.0000
[2019-04-04 16:01:49,795] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8464
[2019-04-04 16:01:49,801] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 73.33333333333334, 114.3333333333333, 819.0, 26.0, 26.66898455269624, 0.7655423178396576, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3240600.0000, 
sim time next is 3241200.0000, 
raw observation next is [-2.0, 75.66666666666667, 114.6666666666667, 821.0, 26.0, 26.69576276207401, 0.7607817923349128, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.7566666666666667, 0.38222222222222235, 0.907182320441989, 0.6666666666666666, 0.7246468968395009, 0.7535939307783043, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39655972], dtype=float32), -2.229511]. 
=============================================
[2019-04-04 16:01:56,743] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5495738e-09 2.8088484e-10 1.6483817e-15 3.0164832e-15 1.0000000e+00
 2.1909742e-10 1.7728999e-15], sum to 1.0000
[2019-04-04 16:01:56,746] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1649
[2019-04-04 16:01:56,764] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.833333333333334, 62.33333333333334, 114.3333333333333, 778.6666666666667, 26.0, 26.3765146874999, 0.5983824960098195, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3323400.0000, 
sim time next is 3324000.0000, 
raw observation next is [-6.666666666666667, 60.66666666666667, 115.1666666666667, 788.3333333333333, 26.0, 26.37421745066384, 0.599708571964395, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.27793167128347185, 0.6066666666666667, 0.383888888888889, 0.871086556169429, 0.6666666666666666, 0.6978514542219866, 0.6999028573214651, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5506685], dtype=float32), -0.54348606]. 
=============================================
[2019-04-04 16:01:56,771] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[83.33929 ]
 [83.47109 ]
 [83.57989 ]
 [83.671715]
 [83.72247 ]], R is [[83.35614014]
 [83.52258301]
 [83.68735504]
 [83.85047913]
 [84.01197815]].
[2019-04-04 16:01:57,438] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0694702e-09 3.0138511e-10 1.5777584e-15 1.9529641e-14 1.0000000e+00
 7.4874773e-10 4.1953275e-15], sum to 1.0000
[2019-04-04 16:01:57,438] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6669
[2019-04-04 16:01:57,479] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.833333333333333, 50.0, 108.6666666666667, 768.0, 26.0, 25.08475193583931, 0.5242490753562211, 1.0, 1.0, 196283.9503181581], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3334200.0000, 
sim time next is 3334800.0000, 
raw observation next is [-3.666666666666667, 50.0, 107.3333333333333, 760.0, 26.0, 25.63191487302622, 0.6044598366050491, 1.0, 1.0, 70268.53295287178], 
processed observation next is [1.0, 0.6086956521739131, 0.3610341643582641, 0.5, 0.3577777777777777, 0.8397790055248618, 0.6666666666666666, 0.6359929060855182, 0.7014866122016831, 1.0, 1.0, 0.33461206168034185], 
reward next is 0.6654, 
noisyNet noise sample is [array([-0.44075683], dtype=float32), 0.6814576]. 
=============================================
[2019-04-04 16:02:02,397] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.7885977e-10 7.2565405e-11 3.1923536e-16 1.5470175e-15 1.0000000e+00
 8.3227911e-11 1.8478684e-15], sum to 1.0000
[2019-04-04 16:02:02,402] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4316
[2019-04-04 16:02:02,412] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 49.0, 112.0, 784.0, 26.0, 26.62964288771311, 0.6193417494711474, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3409200.0000, 
sim time next is 3409800.0000, 
raw observation next is [3.0, 48.33333333333334, 113.0, 790.6666666666667, 26.0, 26.5925714502043, 0.6134914095450461, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.48333333333333345, 0.37666666666666665, 0.8736648250460406, 0.6666666666666666, 0.7160476208503583, 0.7044971365150153, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.07252277], dtype=float32), 0.7901081]. 
=============================================
[2019-04-04 16:02:07,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3659115e-09 5.8237948e-10 1.8942032e-14 1.8827930e-13 1.0000000e+00
 1.8678802e-10 3.1588078e-14], sum to 1.0000
[2019-04-04 16:02:07,307] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5030
[2019-04-04 16:02:07,321] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.166666666666667, 61.83333333333333, 0.0, 0.0, 26.0, 25.33783800569078, 0.4426263929020646, 0.0, 1.0, 48197.00873069876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3546600.0000, 
sim time next is 3547200.0000, 
raw observation next is [-2.333333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 25.31621735089672, 0.4378637149555928, 0.0, 1.0, 43882.10599252098], 
processed observation next is [0.0, 0.043478260869565216, 0.3979686057248385, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6096847792413934, 0.6459545716518643, 0.0, 1.0, 0.20896240948819514], 
reward next is 0.7910, 
noisyNet noise sample is [array([0.670455], dtype=float32), 1.8755203]. 
=============================================
[2019-04-04 16:02:07,808] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0744692e-09 4.0947309e-10 1.9031990e-14 2.3043647e-14 1.0000000e+00
 1.2189398e-10 7.2623826e-15], sum to 1.0000
[2019-04-04 16:02:07,808] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9788
[2019-04-04 16:02:07,831] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 24.97751382424077, 0.342961551266677, 0.0, 1.0, 40934.28562353843], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3558600.0000, 
sim time next is 3559200.0000, 
raw observation next is [-4.666666666666666, 67.0, 0.0, 0.0, 26.0, 24.97033722668381, 0.3352563306694469, 0.0, 1.0, 40884.05439789644], 
processed observation next is [0.0, 0.17391304347826086, 0.33333333333333337, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5808614355569842, 0.611752110223149, 0.0, 1.0, 0.19468597332331639], 
reward next is 0.8053, 
noisyNet noise sample is [array([-1.236359], dtype=float32), 0.33042896]. 
=============================================
[2019-04-04 16:02:09,558] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1397085e-09 2.3113993e-10 2.6671512e-14 4.4939834e-14 1.0000000e+00
 2.4376065e-10 6.4517614e-15], sum to 1.0000
[2019-04-04 16:02:09,558] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7284
[2019-04-04 16:02:09,570] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.666666666666667, 73.0, 0.0, 0.0, 26.0, 25.09311153071049, 0.2654983568218994, 0.0, 1.0, 42294.13679531665], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3739200.0000, 
sim time next is 3739800.0000, 
raw observation next is [-3.833333333333333, 75.0, 0.0, 0.0, 26.0, 25.05456889135645, 0.2548830330956566, 0.0, 1.0, 42198.15862915746], 
processed observation next is [1.0, 0.2608695652173913, 0.3564173591874424, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5878807409463708, 0.5849610110318856, 0.0, 1.0, 0.20094361251979742], 
reward next is 0.7991, 
noisyNet noise sample is [array([0.41603088], dtype=float32), 0.9085125]. 
=============================================
[2019-04-04 16:02:15,751] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7930513e-09 2.2476639e-10 6.9313701e-16 1.1299635e-14 1.0000000e+00
 5.4234373e-10 4.0285917e-16], sum to 1.0000
[2019-04-04 16:02:15,751] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9740
[2019-04-04 16:02:15,784] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 69.16666666666667, 0.0, 0.0, 26.0, 25.55640243386429, 0.4889559031761019, 1.0, 1.0, 187265.7954302656], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3779400.0000, 
sim time next is 3780000.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.2058476220222, 0.5175081352670056, 1.0, 1.0, 133091.7173681742], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6004873018351834, 0.6725027117556684, 1.0, 1.0, 0.6337700827055914], 
reward next is 0.3662, 
noisyNet noise sample is [array([-0.0440934], dtype=float32), -0.3790324]. 
=============================================
[2019-04-04 16:02:15,800] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[83.106926]
 [82.6802  ]
 [83.18874 ]
 [83.76729 ]
 [84.341286]], R is [[82.94008636]
 [82.21894836]
 [82.39675903]
 [82.57279205]
 [82.74706268]].
[2019-04-04 16:02:16,791] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9161973e-08 8.7748442e-10 1.8472843e-14 9.8354769e-14 1.0000000e+00
 6.4777089e-10 4.3868168e-14], sum to 1.0000
[2019-04-04 16:02:16,796] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8303
[2019-04-04 16:02:16,809] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.13698601003539, 0.3567738301742678, 0.0, 1.0, 39465.56702069094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4159200.0000, 
sim time next is 4159800.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.16227750858355, 0.3551370357502175, 0.0, 1.0, 39467.01730089705], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5968564590486292, 0.6183790119167392, 0.0, 1.0, 0.1879381776233193], 
reward next is 0.8121, 
noisyNet noise sample is [array([0.02317797], dtype=float32), 0.80018187]. 
=============================================
[2019-04-04 16:02:23,422] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5637544e-08 3.1425482e-09 2.7668521e-14 7.0856610e-14 1.0000000e+00
 4.5539466e-09 1.6738777e-14], sum to 1.0000
[2019-04-04 16:02:23,425] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4515
[2019-04-04 16:02:23,439] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.333333333333334, 65.0, 0.0, 0.0, 26.0, 25.06357434415938, 0.3137894475939862, 0.0, 1.0, 41413.85482362679], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3908400.0000, 
sim time next is 3909000.0000, 
raw observation next is [-5.666666666666666, 62.0, 0.0, 0.0, 26.0, 25.00639193068015, 0.2999354561944977, 0.0, 1.0, 41582.25356642237], 
processed observation next is [1.0, 0.21739130434782608, 0.3056325023084026, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5838659942233457, 0.5999784853981659, 0.0, 1.0, 0.19801073126867794], 
reward next is 0.8020, 
noisyNet noise sample is [array([1.3629313], dtype=float32), -1.1847826]. 
=============================================
[2019-04-04 16:02:23,452] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[79.66585 ]
 [79.8982  ]
 [80.03879 ]
 [80.115265]
 [80.14383 ]], R is [[79.36102295]
 [79.37020111]
 [79.38011169]
 [79.39059448]
 [79.40135956]].
[2019-04-04 16:02:27,480] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9387317e-08 8.8093435e-09 2.6799214e-13 1.5162589e-12 1.0000000e+00
 7.1509301e-09 1.7104681e-13], sum to 1.0000
[2019-04-04 16:02:27,480] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6279
[2019-04-04 16:02:27,494] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.833333333333332, 52.33333333333334, 0.0, 0.0, 26.0, 25.19483158524974, 0.396749364768706, 0.0, 1.0, 46823.93937000672], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3970200.0000, 
sim time next is 3970800.0000, 
raw observation next is [-9.0, 53.0, 0.0, 0.0, 26.0, 25.14489148113849, 0.3855041463160229, 0.0, 1.0, 44826.77681142368], 
processed observation next is [1.0, 1.0, 0.21329639889196678, 0.53, 0.0, 0.0, 0.6666666666666666, 0.5954076234282075, 0.628501382105341, 0.0, 1.0, 0.2134608419591604], 
reward next is 0.7865, 
noisyNet noise sample is [array([0.09671193], dtype=float32), 0.80672354]. 
=============================================
[2019-04-04 16:02:29,911] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.6104449e-08 8.7339718e-09 5.2682099e-13 4.5200432e-12 9.9999988e-01
 3.1993764e-08 2.3517848e-12], sum to 1.0000
[2019-04-04 16:02:29,912] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0161
[2019-04-04 16:02:29,927] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.0, 67.0, 0.0, 0.0, 26.0, 23.72295404440339, 0.01967444336171912, 0.0, 1.0, 43821.18513515228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3993600.0000, 
sim time next is 3994200.0000, 
raw observation next is [-13.0, 66.0, 0.0, 0.0, 26.0, 23.68647104575859, 0.005360513786127984, 0.0, 1.0, 43795.13356183017], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.66, 0.0, 0.0, 0.6666666666666666, 0.47387258714654923, 0.5017868379287093, 0.0, 1.0, 0.20854825505633412], 
reward next is 0.7915, 
noisyNet noise sample is [array([0.5048021], dtype=float32), 2.372393]. 
=============================================
[2019-04-04 16:02:30,400] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5416582e-09 1.3437935e-10 3.9401381e-15 2.7774429e-14 1.0000000e+00
 2.7984848e-10 1.1561503e-15], sum to 1.0000
[2019-04-04 16:02:30,400] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7766
[2019-04-04 16:02:30,415] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.0, 35.0, 93.83333333333334, 652.0, 26.0, 26.39612992105547, 0.7121850346888964, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4117200.0000, 
sim time next is 4117800.0000, 
raw observation next is [4.0, 35.0, 93.66666666666666, 609.0, 26.0, 26.76431163642936, 0.7429458689806013, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.3122222222222222, 0.6729281767955801, 0.6666666666666666, 0.7303593030357799, 0.7476486229935339, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7397597], dtype=float32), -1.6906785]. 
=============================================
[2019-04-04 16:02:30,891] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2709400e-07 2.7436480e-08 6.7697177e-13 2.9181809e-12 9.9999988e-01
 1.3202891e-08 1.2100572e-12], sum to 1.0000
[2019-04-04 16:02:30,896] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4200
[2019-04-04 16:02:30,927] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 34.0, 0.0, 0.0, 26.0, 24.75145932833067, 0.2153881591440212, 0.0, 1.0, 40344.09091042816], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4078800.0000, 
sim time next is 4079400.0000, 
raw observation next is [-4.0, 34.66666666666667, 0.0, 0.0, 26.0, 24.75015551718098, 0.21779020926106, 0.0, 1.0, 40289.94911502206], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.34666666666666673, 0.0, 0.0, 0.6666666666666666, 0.5625129597650815, 0.5725967364203534, 0.0, 1.0, 0.19185690054772409], 
reward next is 0.8081, 
noisyNet noise sample is [array([0.38964766], dtype=float32), 1.6637455]. 
=============================================
[2019-04-04 16:02:35,994] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7360656e-08 5.4949525e-09 3.0195298e-14 1.8554494e-13 1.0000000e+00
 1.9601230e-09 1.8586541e-14], sum to 1.0000
[2019-04-04 16:02:35,996] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4120
[2019-04-04 16:02:36,012] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 36.66666666666666, 0.0, 0.0, 26.0, 25.86550842223248, 0.5939934594095875, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4137000.0000, 
sim time next is 4137600.0000, 
raw observation next is [1.0, 37.33333333333334, 0.0, 0.0, 26.0, 25.92050040810451, 0.5908294607137937, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.3733333333333334, 0.0, 0.0, 0.6666666666666666, 0.6600417006753757, 0.6969431535712646, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9266273], dtype=float32), 0.55313504]. 
=============================================
[2019-04-04 16:02:42,072] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1009967e-09 7.1381850e-10 8.7903962e-16 2.1039026e-14 1.0000000e+00
 1.5870714e-09 6.7745551e-15], sum to 1.0000
[2019-04-04 16:02:42,073] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6106
[2019-04-04 16:02:42,095] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 82.83333333333333, 0.0, 0.0, 26.0, 25.38902652808165, 0.4697391456173693, 0.0, 1.0, 45402.23338457645], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4745400.0000, 
sim time next is 4746000.0000, 
raw observation next is [-3.0, 81.66666666666667, 0.0, 0.0, 26.0, 25.40581957839249, 0.4590603956058303, 0.0, 1.0, 32798.00726343801], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.6171516315327074, 0.6530201318686101, 0.0, 1.0, 0.15618098696875243], 
reward next is 0.8438, 
noisyNet noise sample is [array([0.6099025], dtype=float32), -0.87528414]. 
=============================================
[2019-04-04 16:02:42,102] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.842285]
 [83.78768 ]
 [83.467155]
 [83.158325]
 [82.92379 ]], R is [[83.8469162 ]
 [83.79224396]
 [83.61372375]
 [83.42111969]
 [83.30156708]].
[2019-04-04 16:02:45,873] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9344546e-09 2.1193972e-10 1.6407914e-14 1.4140630e-13 1.0000000e+00
 6.5504790e-10 8.9169835e-15], sum to 1.0000
[2019-04-04 16:02:45,876] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6442
[2019-04-04 16:02:45,894] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.333333333333333, 62.66666666666667, 20.0, 190.0, 26.0, 25.43072541181663, 0.3950869718801283, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4297200.0000, 
sim time next is 4297800.0000, 
raw observation next is [6.266666666666667, 63.33333333333334, 16.0, 152.0, 26.0, 25.38699427573753, 0.3793694377006334, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.6361957525392429, 0.6333333333333334, 0.05333333333333334, 0.16795580110497238, 0.6666666666666666, 0.6155828563114607, 0.6264564792335444, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06746992], dtype=float32), 0.11962638]. 
=============================================
[2019-04-04 16:02:47,949] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.8934991e-10 1.3279270e-11 1.7278301e-17 5.0119780e-17 1.0000000e+00
 8.4105882e-12 1.8826537e-16], sum to 1.0000
[2019-04-04 16:02:47,950] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8892
[2019-04-04 16:02:47,975] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.916666666666666, 54.5, 102.0, 615.0, 26.0, 26.43610748911877, 0.5946642443541466, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4353000.0000, 
sim time next is 4353600.0000, 
raw observation next is [7.533333333333333, 52.00000000000001, 104.5, 646.0, 26.0, 26.5035778571493, 0.6311712231301311, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6712834718374886, 0.52, 0.34833333333333333, 0.7138121546961326, 0.6666666666666666, 0.7086314880957749, 0.7103904077100437, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.078963], dtype=float32), -0.24315844]. 
=============================================
[2019-04-04 16:02:53,220] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.0128180e-09 1.2616647e-10 5.3567228e-15 4.5227470e-14 1.0000000e+00
 4.3075623e-09 8.1819664e-15], sum to 1.0000
[2019-04-04 16:02:53,221] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2526
[2019-04-04 16:02:53,239] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.57342984522224, 0.5806921726010715, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4482000.0000, 
sim time next is 4482600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.65371880691808, 0.5793196436221375, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6378099005765065, 0.6931065478740459, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.83309716], dtype=float32), 1.1285472]. 
=============================================
[2019-04-04 16:02:54,527] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.34771952e-09 1.05185465e-10 5.55408821e-16 1.40909208e-14
 1.00000000e+00 7.55596349e-11 1.03733160e-15], sum to 1.0000
[2019-04-04 16:02:54,531] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5627
[2019-04-04 16:02:54,554] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8, 71.0, 0.0, 0.0, 26.0, 25.31325788294019, 0.4176807778749377, 0.0, 1.0, 41506.86053912377], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4510800.0000, 
sim time next is 4511400.0000, 
raw observation next is [-0.8333333333333334, 71.0, 0.0, 0.0, 26.0, 25.35530218076197, 0.4114322405859454, 0.0, 1.0, 41403.7783423022], 
processed observation next is [1.0, 0.21739130434782608, 0.43951985226223456, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6129418483968309, 0.6371440801953151, 0.0, 1.0, 0.19716084924905808], 
reward next is 0.8028, 
noisyNet noise sample is [array([1.0968013], dtype=float32), -1.5387758]. 
=============================================
[2019-04-04 16:02:54,877] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2761114e-09 2.2619119e-10 7.5310275e-15 8.9703818e-14 1.0000000e+00
 5.3698818e-10 2.3223926e-15], sum to 1.0000
[2019-04-04 16:02:54,878] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3538
[2019-04-04 16:02:54,894] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.09999999999999999, 72.0, 0.0, 0.0, 26.0, 25.44215356016062, 0.4859025938813107, 0.0, 1.0, 69174.3681276014], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4486800.0000, 
sim time next is 4487400.0000, 
raw observation next is [-0.15, 72.0, 0.0, 0.0, 26.0, 25.39546664835329, 0.4832920733036319, 0.0, 1.0, 76774.62109910484], 
processed observation next is [1.0, 0.9565217391304348, 0.458448753462604, 0.72, 0.0, 0.0, 0.6666666666666666, 0.616288887362774, 0.6610973577678773, 0.0, 1.0, 0.36559343380526116], 
reward next is 0.6344, 
noisyNet noise sample is [array([0.74283165], dtype=float32), -0.030813241]. 
=============================================
[2019-04-04 16:02:56,843] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0376000e-08 1.3055341e-09 1.5924646e-14 5.1670040e-13 1.0000000e+00
 3.1377447e-09 6.4957717e-14], sum to 1.0000
[2019-04-04 16:02:56,848] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4154
[2019-04-04 16:02:56,861] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.6306044072592, 0.457304126344701, 0.0, 1.0, 20867.71847577832], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4573200.0000, 
sim time next is 4573800.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.44565018086555, 0.4462786935532688, 0.0, 1.0, 123417.1092031513], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6204708484054624, 0.6487595645177563, 0.0, 1.0, 0.5877005200150062], 
reward next is 0.4123, 
noisyNet noise sample is [array([0.84194285], dtype=float32), 1.1231993]. 
=============================================
[2019-04-04 16:02:59,350] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:02:59,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:02:59,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run31
[2019-04-04 16:03:00,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:00,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:00,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run31
[2019-04-04 16:03:01,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:01,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:01,647] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run31
[2019-04-04 16:03:02,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:02,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:02,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run31
[2019-04-04 16:03:04,286] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4346748e-09 1.5304978e-10 9.5362857e-15 9.7354697e-15 1.0000000e+00
 9.5513777e-11 4.2904612e-15], sum to 1.0000
[2019-04-04 16:03:04,288] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1183
[2019-04-04 16:03:04,303] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3333333333333333, 52.33333333333334, 0.0, 0.0, 26.0, 25.40426883093802, 0.4060904260296592, 0.0, 1.0, 61956.56321075404], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4828800.0000, 
sim time next is 4829400.0000, 
raw observation next is [-0.5, 53.0, 0.0, 0.0, 26.0, 25.40817977239692, 0.4062094375856608, 0.0, 1.0, 46284.32255566843], 
processed observation next is [0.0, 0.9130434782608695, 0.44875346260387816, 0.53, 0.0, 0.0, 0.6666666666666666, 0.61734831436641, 0.635403145861887, 0.0, 1.0, 0.22040153597937345], 
reward next is 0.7796, 
noisyNet noise sample is [array([-0.59099376], dtype=float32), 1.0720416]. 
=============================================
[2019-04-04 16:03:06,135] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6089616e-09 1.3108613e-09 3.2537032e-16 1.7569747e-14 1.0000000e+00
 1.5082476e-10 1.2246229e-15], sum to 1.0000
[2019-04-04 16:03:06,136] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4970
[2019-04-04 16:03:06,196] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.666666666666667, 58.66666666666667, 156.5, 678.5, 26.0, 25.47515189107802, 0.4567333648159682, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4789200.0000, 
sim time next is 4789800.0000, 
raw observation next is [-2.5, 55.5, 153.0, 730.0, 26.0, 25.4234042153134, 0.4520164603736592, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.39335180055401664, 0.555, 0.51, 0.8066298342541437, 0.6666666666666666, 0.6186170179427833, 0.6506721534578864, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0711844], dtype=float32), 1.0977381]. 
=============================================
[2019-04-04 16:03:06,751] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.5897298e-09 5.2919052e-10 3.0172903e-14 2.1018580e-13 1.0000000e+00
 4.1952422e-10 4.2066275e-14], sum to 1.0000
[2019-04-04 16:03:06,754] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9758
[2019-04-04 16:03:06,793] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.166666666666667, 42.5, 0.0, 0.0, 26.0, 25.01037799400192, 0.3332522072303377, 0.0, 1.0, 30957.57610692652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4819800.0000, 
sim time next is 4820400.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 24.9944706456201, 0.3295429179878711, 0.0, 1.0, 40564.43101955771], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.5828725538016751, 0.6098476393292903, 0.0, 1.0, 0.19316395723598911], 
reward next is 0.8068, 
noisyNet noise sample is [array([1.1259573], dtype=float32), -0.5417357]. 
=============================================
[2019-04-04 16:03:10,907] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.9497755e-11 5.7263630e-12 1.7964672e-17 2.8419604e-16 1.0000000e+00
 9.8789744e-12 5.6938494e-17], sum to 1.0000
[2019-04-04 16:03:10,907] A3C_AGENT_WORKER-Thread-13 INFO:Local step 255000, global step 4073570: loss 0.0815
[2019-04-04 16:03:10,908] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7733
[2019-04-04 16:03:10,911] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 255000, global step 4073574: learning rate 0.0000
[2019-04-04 16:03:10,954] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 85.5, 0.0, 26.0, 24.29980903397128, 0.09298115682669444, 0.0, 1.0, 40312.28220496483], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 43200.0000, 
sim time next is 43800.0000, 
raw observation next is [7.800000000000001, 91.83333333333333, 89.0, 0.0, 26.0, 24.29831713240117, 0.09991727762204568, 0.0, 1.0, 42086.33208248488], 
processed observation next is [0.0, 0.5217391304347826, 0.6786703601108034, 0.9183333333333333, 0.2966666666666667, 0.0, 0.6666666666666666, 0.5248597610334308, 0.5333057592073486, 0.0, 1.0, 0.2004111051546899], 
reward next is 0.7996, 
noisyNet noise sample is [array([-0.3001261], dtype=float32), 0.52865237]. 
=============================================
[2019-04-04 16:03:11,966] A3C_AGENT_WORKER-Thread-2 INFO:Local step 255000, global step 4074151: loss 0.0822
[2019-04-04 16:03:11,970] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 255000, global step 4074152: learning rate 0.0000
[2019-04-04 16:03:13,439] A3C_AGENT_WORKER-Thread-17 INFO:Local step 255000, global step 4074922: loss 0.0761
[2019-04-04 16:03:13,447] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 255000, global step 4074922: learning rate 0.0000
[2019-04-04 16:03:14,251] A3C_AGENT_WORKER-Thread-11 INFO:Local step 255000, global step 4075321: loss 0.0773
[2019-04-04 16:03:14,252] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 255000, global step 4075321: learning rate 0.0000
[2019-04-04 16:03:15,446] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5825040e-09 5.2220964e-11 1.6993442e-16 1.4432226e-15 1.0000000e+00
 6.2112183e-11 1.2664781e-16], sum to 1.0000
[2019-04-04 16:03:15,449] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7767
[2019-04-04 16:03:15,479] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.0, 19.0, 98.5, 757.3333333333334, 26.0, 28.70031491238792, 1.020129015944065, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5066400.0000, 
sim time next is 5067000.0000, 
raw observation next is [12.0, 19.0, 96.0, 745.0, 26.0, 28.2759170948364, 1.077923960883614, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.19, 0.32, 0.8232044198895028, 0.6666666666666666, 0.8563264245697001, 0.8593079869612046, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2210788], dtype=float32), -0.1831563]. 
=============================================
[2019-04-04 16:03:15,486] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[92.498474]
 [92.3838  ]
 [92.18859 ]
 [91.99523 ]
 [91.84458 ]], R is [[92.65899658]
 [92.73240662]
 [92.80508423]
 [92.87703705]
 [92.94826508]].
[2019-04-04 16:03:17,087] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.7898237e-10 7.0871620e-10 2.0549451e-15 4.3425761e-15 1.0000000e+00
 3.0108691e-10 2.3274390e-15], sum to 1.0000
[2019-04-04 16:03:17,087] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8589
[2019-04-04 16:03:17,104] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 24.66666666666667, 0.0, 0.0, 26.0, 26.44951793019192, 0.6998700036166247, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4990200.0000, 
sim time next is 4990800.0000, 
raw observation next is [6.0, 24.33333333333334, 0.0, 0.0, 26.0, 25.59247627288587, 0.6030075510698097, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6327063560738226, 0.7010025170232699, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4944886], dtype=float32), 0.9284845]. 
=============================================
[2019-04-04 16:03:17,406] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:17,406] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:17,431] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run31
[2019-04-04 16:03:18,190] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6852770e-09 1.5566262e-10 2.4257073e-16 1.8721698e-15 1.0000000e+00
 5.2449853e-11 3.4907141e-16], sum to 1.0000
[2019-04-04 16:03:18,192] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8676
[2019-04-04 16:03:18,230] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.333333333333334, 25.66666666666667, 94.83333333333334, 781.5, 26.0, 26.24929314123739, 0.7429404651217113, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4980000.0000, 
sim time next is 4980600.0000, 
raw observation next is [8.5, 25.5, 92.0, 774.0, 26.0, 26.79387919217487, 0.8117948413471828, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.698060941828255, 0.255, 0.30666666666666664, 0.8552486187845304, 0.6666666666666666, 0.7328232660145725, 0.770598280449061, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22603868], dtype=float32), 0.43414998]. 
=============================================
[2019-04-04 16:03:20,116] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.9562491e-09 1.8270215e-09 1.1812093e-14 3.7623408e-14 1.0000000e+00
 4.6465116e-09 4.5301633e-14], sum to 1.0000
[2019-04-04 16:03:20,118] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8581
[2019-04-04 16:03:20,135] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 48.0, 0.0, 0.0, 26.0, 25.52493922158673, 0.3854097915885747, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5031000.0000, 
sim time next is 5031600.0000, 
raw observation next is [-1.0, 47.33333333333333, 0.0, 0.0, 26.0, 25.47873540416075, 0.3778082714766819, 0.0, 1.0, 29962.85000739314], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 0.4733333333333333, 0.0, 0.0, 0.6666666666666666, 0.6232279503467293, 0.6259360904922273, 0.0, 1.0, 0.14268023813044353], 
reward next is 0.8573, 
noisyNet noise sample is [array([-0.9413125], dtype=float32), -0.8185609]. 
=============================================
[2019-04-04 16:03:20,244] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:20,244] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:20,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run31
[2019-04-04 16:03:21,116] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:21,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:21,123] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run31
[2019-04-04 16:03:21,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:21,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:21,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run31
[2019-04-04 16:03:22,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:22,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:22,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run31
[2019-04-04 16:03:22,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:22,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:22,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run31
[2019-04-04 16:03:23,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:23,502] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:23,506] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run31
[2019-04-04 16:03:23,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:23,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:23,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run31
[2019-04-04 16:03:23,990] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:23,990] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:23,995] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run31
[2019-04-04 16:03:24,279] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:24,279] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:24,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run31
[2019-04-04 16:03:24,739] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:24,739] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:24,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run31
[2019-04-04 16:03:24,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:03:24,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:03:24,808] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run31
[2019-04-04 16:03:29,818] A3C_AGENT_WORKER-Thread-15 INFO:Local step 255000, global step 4080328: loss 0.0839
[2019-04-04 16:03:29,819] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 255000, global step 4080328: learning rate 0.0000
[2019-04-04 16:03:33,401] A3C_AGENT_WORKER-Thread-4 INFO:Local step 255000, global step 4081272: loss 0.0888
[2019-04-04 16:03:33,402] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 255000, global step 4081272: learning rate 0.0000
[2019-04-04 16:03:33,799] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7823106e-09 1.5979577e-09 2.0371187e-15 8.4179625e-14 1.0000000e+00
 2.8093788e-10 4.3785556e-15], sum to 1.0000
[2019-04-04 16:03:33,800] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5525
[2019-04-04 16:03:33,813] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 6.999999999999998, 0.0, 26.0, 21.40465774760514, -0.5231967467561093, 0.0, 1.0, 40280.18676955297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 28200.0000, 
sim time next is 28800.0000, 
raw observation next is [7.7, 93.0, 10.5, 0.0, 26.0, 21.42550372836596, -0.5076159673993136, 0.0, 1.0, 40260.21273470944], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.035, 0.0, 0.6666666666666666, 0.2854586440304967, 0.3307946775335621, 0.0, 1.0, 0.19171529873671164], 
reward next is 0.8083, 
noisyNet noise sample is [array([2.4697886], dtype=float32), -1.7449175]. 
=============================================
[2019-04-04 16:03:33,960] A3C_AGENT_WORKER-Thread-13 INFO:Local step 255500, global step 4081442: loss 4.3349
[2019-04-04 16:03:33,960] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 255500, global step 4081442: learning rate 0.0000
[2019-04-04 16:03:34,742] A3C_AGENT_WORKER-Thread-6 INFO:Local step 255000, global step 4081652: loss 0.0892
[2019-04-04 16:03:34,742] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 255000, global step 4081652: learning rate 0.0000
[2019-04-04 16:03:34,911] A3C_AGENT_WORKER-Thread-16 INFO:Local step 255000, global step 4081706: loss 0.0928
[2019-04-04 16:03:34,912] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 255000, global step 4081706: learning rate 0.0000
[2019-04-04 16:03:35,489] A3C_AGENT_WORKER-Thread-2 INFO:Local step 255500, global step 4081893: loss 4.3383
[2019-04-04 16:03:35,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 255500, global step 4081895: learning rate 0.0000
[2019-04-04 16:03:35,518] A3C_AGENT_WORKER-Thread-12 INFO:Local step 255000, global step 4081901: loss 0.0901
[2019-04-04 16:03:35,522] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 255000, global step 4081901: learning rate 0.0000
[2019-04-04 16:03:36,432] A3C_AGENT_WORKER-Thread-3 INFO:Local step 255000, global step 4082212: loss 0.0815
[2019-04-04 16:03:36,433] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 255000, global step 4082213: learning rate 0.0000
[2019-04-04 16:03:36,813] A3C_AGENT_WORKER-Thread-10 INFO:Local step 255000, global step 4082351: loss 0.0856
[2019-04-04 16:03:36,815] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 255000, global step 4082352: learning rate 0.0000
[2019-04-04 16:03:36,903] A3C_AGENT_WORKER-Thread-19 INFO:Local step 255000, global step 4082383: loss 0.0838
[2019-04-04 16:03:36,906] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 255000, global step 4082383: learning rate 0.0000
[2019-04-04 16:03:37,215] A3C_AGENT_WORKER-Thread-5 INFO:Local step 255000, global step 4082497: loss 0.0823
[2019-04-04 16:03:37,216] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 255000, global step 4082497: learning rate 0.0000
[2019-04-04 16:03:37,508] A3C_AGENT_WORKER-Thread-17 INFO:Local step 255500, global step 4082626: loss 4.3575
[2019-04-04 16:03:37,509] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 255500, global step 4082626: learning rate 0.0000
[2019-04-04 16:03:37,634] A3C_AGENT_WORKER-Thread-11 INFO:Local step 255500, global step 4082673: loss 4.3664
[2019-04-04 16:03:37,634] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 255500, global step 4082673: learning rate 0.0000
[2019-04-04 16:03:37,700] A3C_AGENT_WORKER-Thread-20 INFO:Local step 255000, global step 4082696: loss 0.0840
[2019-04-04 16:03:37,705] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 255000, global step 4082698: learning rate 0.0000
[2019-04-04 16:03:37,906] A3C_AGENT_WORKER-Thread-14 INFO:Local step 255000, global step 4082772: loss 0.0840
[2019-04-04 16:03:37,908] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 255000, global step 4082772: learning rate 0.0000
[2019-04-04 16:03:38,240] A3C_AGENT_WORKER-Thread-18 INFO:Local step 255000, global step 4082915: loss 0.0799
[2019-04-04 16:03:38,241] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 255000, global step 4082915: learning rate 0.0000
[2019-04-04 16:03:40,400] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8817094e-08 3.0350106e-10 3.3166636e-15 8.9468018e-14 1.0000000e+00
 2.4708287e-09 9.2016188e-15], sum to 1.0000
[2019-04-04 16:03:40,400] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6668
[2019-04-04 16:03:40,451] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.75, 73.5, 124.0, 0.0, 26.0, 25.30363341476753, 0.2102068848240179, 1.0, 1.0, 27229.45300691673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 210600.0000, 
sim time next is 211200.0000, 
raw observation next is [-6.566666666666666, 73.0, 128.8333333333333, 0.0, 26.0, 25.27125930951207, 0.2122594869255848, 1.0, 1.0, 27964.40259905128], 
processed observation next is [1.0, 0.43478260869565216, 0.28070175438596495, 0.73, 0.4294444444444443, 0.0, 0.6666666666666666, 0.6059382757926727, 0.5707531623085282, 1.0, 1.0, 0.13316382190024417], 
reward next is 0.8668, 
noisyNet noise sample is [array([0.18604685], dtype=float32), -1.8071079]. 
=============================================
[2019-04-04 16:03:45,018] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2160588e-08 1.5339150e-09 1.0936748e-13 3.1634339e-13 1.0000000e+00
 2.1483684e-09 1.5263493e-14], sum to 1.0000
[2019-04-04 16:03:45,019] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1646
[2019-04-04 16:03:45,053] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.616666666666667, 25.5, 126.6666666666667, 0.0, 26.0, 25.19090155349328, 0.1730926304103895, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 475800.0000, 
sim time next is 476400.0000, 
raw observation next is [-1.533333333333333, 26.0, 127.8333333333333, 0.0, 26.0, 25.22750397353451, 0.1635449509935762, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.42012927054478305, 0.26, 0.426111111111111, 0.0, 0.6666666666666666, 0.6022919977945426, 0.5545149836645255, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0492487], dtype=float32), 1.3854431]. 
=============================================
[2019-04-04 16:03:49,711] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4087813e-09 4.5341120e-11 1.9662129e-15 5.7047254e-15 1.0000000e+00
 2.6331085e-10 7.1464599e-16], sum to 1.0000
[2019-04-04 16:03:49,711] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9717
[2019-04-04 16:03:49,753] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.266666666666667, 96.0, 0.0, 0.0, 26.0, 24.84121472045991, 0.2138670807223709, 0.0, 1.0, 150002.8257844827], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 505200.0000, 
sim time next is 505800.0000, 
raw observation next is [1.35, 96.0, 0.0, 0.0, 26.0, 24.78871121552008, 0.2289868463834409, 0.0, 1.0, 92249.76041109435], 
processed observation next is [1.0, 0.8695652173913043, 0.5000000000000001, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5657259346266734, 0.5763289487944804, 0.0, 1.0, 0.43928457338616356], 
reward next is 0.5607, 
noisyNet noise sample is [array([-0.9329818], dtype=float32), -0.4555373]. 
=============================================
[2019-04-04 16:03:50,550] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.3681535e-09 1.3681481e-10 5.9710249e-15 4.1222593e-14 1.0000000e+00
 4.6638593e-10 1.8904918e-15], sum to 1.0000
[2019-04-04 16:03:50,550] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8811
[2019-04-04 16:03:50,610] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.01088119691745, 0.3038584847402263, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 241200.0000, 
sim time next is 241800.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.25497261295747, 0.3128082876679251, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6045810510797892, 0.6042694292226417, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04964501], dtype=float32), -1.3626078]. 
=============================================
[2019-04-04 16:03:53,366] A3C_AGENT_WORKER-Thread-15 INFO:Local step 255500, global step 4087808: loss 4.4037
[2019-04-04 16:03:53,367] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 255500, global step 4087808: learning rate 0.0000
[2019-04-04 16:03:57,070] A3C_AGENT_WORKER-Thread-4 INFO:Local step 255500, global step 4088917: loss 4.4157
[2019-04-04 16:03:57,070] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 255500, global step 4088917: learning rate 0.0000
[2019-04-04 16:03:58,127] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3143212e-07 5.7965903e-09 5.8047886e-13 5.1466266e-12 9.9999988e-01
 1.7044018e-08 4.2676125e-13], sum to 1.0000
[2019-04-04 16:03:58,127] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3645
[2019-04-04 16:03:58,141] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.65, 76.0, 0.0, 0.0, 26.0, 24.08729227958167, 0.07841914605768205, 0.0, 1.0, 47073.77561571465], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 340200.0000, 
sim time next is 340800.0000, 
raw observation next is [-13.73333333333333, 74.0, 0.0, 0.0, 26.0, 24.05200385276931, 0.06375852477283252, 0.0, 1.0, 47062.94473026501], 
processed observation next is [1.0, 0.9565217391304348, 0.08217913204062795, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5043336543974425, 0.5212528415909442, 0.0, 1.0, 0.22410926062030956], 
reward next is 0.7759, 
noisyNet noise sample is [array([-1.2095857], dtype=float32), 0.5669073]. 
=============================================
[2019-04-04 16:03:58,258] A3C_AGENT_WORKER-Thread-6 INFO:Local step 255500, global step 4089299: loss 4.4288
[2019-04-04 16:03:58,259] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 255500, global step 4089299: learning rate 0.0000
[2019-04-04 16:03:58,845] A3C_AGENT_WORKER-Thread-16 INFO:Local step 255500, global step 4089496: loss 4.4185
[2019-04-04 16:03:58,847] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 255500, global step 4089496: learning rate 0.0000
[2019-04-04 16:03:59,197] A3C_AGENT_WORKER-Thread-13 INFO:Local step 256000, global step 4089623: loss 0.3832
[2019-04-04 16:03:59,197] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 256000, global step 4089623: learning rate 0.0000
[2019-04-04 16:03:59,540] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5150162e-07 7.8377489e-08 4.2919227e-12 1.0128398e-11 9.9999964e-01
 7.3206778e-08 3.9856148e-12], sum to 1.0000
[2019-04-04 16:03:59,541] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8145
[2019-04-04 16:03:59,577] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 26.0, 22.74675652679871, -0.2489099481155672, 0.0, 1.0, 49272.9961415028], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 356400.0000, 
sim time next is 357000.0000, 
raw observation next is [-15.1, 69.66666666666667, 0.0, 0.0, 26.0, 22.6748325099077, -0.2577075683602514, 0.0, 1.0, 49309.35487379272], 
processed observation next is [1.0, 0.13043478260869565, 0.0443213296398892, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.38956937582564155, 0.4140974772132495, 0.0, 1.0, 0.23480645177996534], 
reward next is 0.7652, 
noisyNet noise sample is [array([-1.0307304], dtype=float32), 1.0870236]. 
=============================================
[2019-04-04 16:03:59,597] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[65.14267 ]
 [65.249886]
 [65.464195]
 [65.57887 ]
 [65.670815]], R is [[65.16244507]
 [65.27618408]
 [65.38916779]
 [65.50149536]
 [65.61320496]].
[2019-04-04 16:03:59,746] A3C_AGENT_WORKER-Thread-12 INFO:Local step 255500, global step 4089833: loss 4.4233
[2019-04-04 16:03:59,747] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 255500, global step 4089833: learning rate 0.0000
[2019-04-04 16:04:00,007] A3C_AGENT_WORKER-Thread-10 INFO:Local step 255500, global step 4089930: loss 4.4327
[2019-04-04 16:04:00,008] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 255500, global step 4089930: learning rate 0.0000
[2019-04-04 16:04:00,145] A3C_AGENT_WORKER-Thread-3 INFO:Local step 255500, global step 4089978: loss 4.4265
[2019-04-04 16:04:00,146] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 255500, global step 4089978: learning rate 0.0000
[2019-04-04 16:04:00,365] A3C_AGENT_WORKER-Thread-19 INFO:Local step 255500, global step 4090057: loss 4.4271
[2019-04-04 16:04:00,365] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 255500, global step 4090057: learning rate 0.0000
[2019-04-04 16:04:00,480] A3C_AGENT_WORKER-Thread-2 INFO:Local step 256000, global step 4090104: loss 0.3828
[2019-04-04 16:04:00,489] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 256000, global step 4090104: learning rate 0.0000
[2019-04-04 16:04:00,702] A3C_AGENT_WORKER-Thread-5 INFO:Local step 255500, global step 4090174: loss 4.4234
[2019-04-04 16:04:00,706] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 255500, global step 4090174: learning rate 0.0000
[2019-04-04 16:04:01,685] A3C_AGENT_WORKER-Thread-14 INFO:Local step 255500, global step 4090496: loss 4.4336
[2019-04-04 16:04:01,686] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 255500, global step 4090496: learning rate 0.0000
[2019-04-04 16:04:01,882] A3C_AGENT_WORKER-Thread-11 INFO:Local step 256000, global step 4090555: loss 0.3859
[2019-04-04 16:04:01,882] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 256000, global step 4090555: learning rate 0.0000
[2019-04-04 16:04:01,966] A3C_AGENT_WORKER-Thread-20 INFO:Local step 255500, global step 4090576: loss 4.4309
[2019-04-04 16:04:01,966] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 255500, global step 4090576: learning rate 0.0000
[2019-04-04 16:04:02,018] A3C_AGENT_WORKER-Thread-18 INFO:Local step 255500, global step 4090588: loss 4.4281
[2019-04-04 16:04:02,019] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 255500, global step 4090588: learning rate 0.0000
[2019-04-04 16:04:02,685] A3C_AGENT_WORKER-Thread-17 INFO:Local step 256000, global step 4090771: loss 0.3895
[2019-04-04 16:04:02,686] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 256000, global step 4090771: learning rate 0.0000
[2019-04-04 16:04:08,000] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1491144e-09 1.3187099e-10 1.9592124e-15 3.0995372e-14 1.0000000e+00
 3.8694537e-11 6.6636505e-15], sum to 1.0000
[2019-04-04 16:04:08,002] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3467
[2019-04-04 16:04:08,053] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 75.0, 69.66666666666666, 0.0, 26.0, 25.7257839119451, 0.3023056461516808, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 813000.0000, 
sim time next is 813600.0000, 
raw observation next is [-6.2, 75.0, 74.0, 0.0, 26.0, 25.72031056173442, 0.2993871238007094, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2908587257617729, 0.75, 0.24666666666666667, 0.0, 0.6666666666666666, 0.6433592134778682, 0.5997957079335698, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2558191], dtype=float32), -0.18684535]. 
=============================================
[2019-04-04 16:04:14,439] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.8423987e-09 2.6417407e-10 2.3218878e-15 2.6694414e-14 1.0000000e+00
 4.1882703e-10 5.9999774e-15], sum to 1.0000
[2019-04-04 16:04:14,440] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4390
[2019-04-04 16:04:14,454] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 76.0, 0.0, 0.0, 26.0, 24.67162888489069, 0.1996481132520652, 0.0, 1.0, 39073.39288228069], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 878400.0000, 
sim time next is 879000.0000, 
raw observation next is [-1.1, 75.33333333333334, 0.0, 0.0, 26.0, 24.74311487872179, 0.1952045187475566, 0.0, 1.0, 39017.27112761688], 
processed observation next is [1.0, 0.17391304347826086, 0.4321329639889197, 0.7533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5619262398934826, 0.5650681729158522, 0.0, 1.0, 0.18579652917912798], 
reward next is 0.8142, 
noisyNet noise sample is [array([1.0272757], dtype=float32), -0.6345187]. 
=============================================
[2019-04-04 16:04:14,457] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.91072]
 [80.91701]
 [80.93251]
 [80.94369]
 [80.95778]], R is [[80.89504242]
 [80.90002441]
 [80.9046936 ]
 [80.90911102]
 [80.91327667]].
[2019-04-04 16:04:17,071] A3C_AGENT_WORKER-Thread-15 INFO:Local step 256000, global step 4095908: loss 0.3570
[2019-04-04 16:04:17,072] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 256000, global step 4095908: learning rate 0.0000
[2019-04-04 16:04:18,491] A3C_AGENT_WORKER-Thread-13 INFO:Local step 256500, global step 4096476: loss 0.0003
[2019-04-04 16:04:18,492] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 256500, global step 4096476: learning rate 0.0000
[2019-04-04 16:04:19,733] A3C_AGENT_WORKER-Thread-2 INFO:Local step 256500, global step 4097048: loss 0.0007
[2019-04-04 16:04:19,734] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 256500, global step 4097049: learning rate 0.0000
[2019-04-04 16:04:21,472] A3C_AGENT_WORKER-Thread-4 INFO:Local step 256000, global step 4097804: loss 0.3724
[2019-04-04 16:04:21,474] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 256000, global step 4097806: learning rate 0.0000
[2019-04-04 16:04:21,496] A3C_AGENT_WORKER-Thread-11 INFO:Local step 256500, global step 4097817: loss 0.0011
[2019-04-04 16:04:21,501] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 256500, global step 4097817: learning rate 0.0000
[2019-04-04 16:04:21,816] A3C_AGENT_WORKER-Thread-6 INFO:Local step 256000, global step 4097953: loss 0.3781
[2019-04-04 16:04:21,816] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 256000, global step 4097953: learning rate 0.0000
[2019-04-04 16:04:22,032] A3C_AGENT_WORKER-Thread-17 INFO:Local step 256500, global step 4098037: loss 0.0008
[2019-04-04 16:04:22,032] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 256500, global step 4098037: learning rate 0.0000
[2019-04-04 16:04:22,744] A3C_AGENT_WORKER-Thread-16 INFO:Local step 256000, global step 4098344: loss 0.3686
[2019-04-04 16:04:22,744] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 256000, global step 4098344: learning rate 0.0000
[2019-04-04 16:04:23,445] A3C_AGENT_WORKER-Thread-12 INFO:Local step 256000, global step 4098683: loss 0.3783
[2019-04-04 16:04:23,447] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 256000, global step 4098683: learning rate 0.0000
[2019-04-04 16:04:23,801] A3C_AGENT_WORKER-Thread-10 INFO:Local step 256000, global step 4098860: loss 0.3756
[2019-04-04 16:04:23,805] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 256000, global step 4098861: learning rate 0.0000
[2019-04-04 16:04:23,810] A3C_AGENT_WORKER-Thread-3 INFO:Local step 256000, global step 4098865: loss 0.3743
[2019-04-04 16:04:23,812] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 256000, global step 4098865: learning rate 0.0000
[2019-04-04 16:04:23,946] A3C_AGENT_WORKER-Thread-19 INFO:Local step 256000, global step 4098925: loss 0.3715
[2019-04-04 16:04:23,947] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 256000, global step 4098925: learning rate 0.0000
[2019-04-04 16:04:24,352] A3C_AGENT_WORKER-Thread-5 INFO:Local step 256000, global step 4099122: loss 0.3751
[2019-04-04 16:04:24,369] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 256000, global step 4099132: learning rate 0.0000
[2019-04-04 16:04:25,342] A3C_AGENT_WORKER-Thread-20 INFO:Local step 256000, global step 4099666: loss 0.3742
[2019-04-04 16:04:25,344] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 256000, global step 4099666: learning rate 0.0000
[2019-04-04 16:04:25,590] A3C_AGENT_WORKER-Thread-14 INFO:Local step 256000, global step 4099814: loss 0.3758
[2019-04-04 16:04:25,592] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 256000, global step 4099815: learning rate 0.0000
[2019-04-04 16:04:25,923] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-04 16:04:25,926] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 16:04:25,927] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 16:04:25,927] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:04:25,927] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:04:25,927] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 16:04:25,928] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:04:25,932] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run42
[2019-04-04 16:04:25,953] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run42
[2019-04-04 16:04:25,971] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run42
[2019-04-04 16:04:35,617] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18062012], dtype=float32), 0.23630293]
[2019-04-04 16:04:35,617] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.9, 77.33333333333333, 0.0, 0.0, 26.0, 23.44703702574226, -0.08226595229892962, 0.0, 1.0, 44239.52451735205]
[2019-04-04 16:04:35,617] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 16:04:35,618] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.8393929e-08 5.8914691e-09 1.2086634e-13 1.6120124e-12 1.0000000e+00
 3.4333318e-09 1.9644231e-13], sampled 0.6012234717912526
[2019-04-04 16:06:07,963] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4172 239942393.0563 1605.0250
[2019-04-04 16:06:20,077] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18062012], dtype=float32), 0.23630293]
[2019-04-04 16:06:20,077] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.1666666666666667, 91.0, 211.3333333333333, 6.0, 26.0, 26.43009834600052, 0.5835979034360378, 1.0, 1.0, 0.0]
[2019-04-04 16:06:20,078] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 16:06:20,079] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.0349303e-11 5.3922092e-12 5.1450407e-18 1.0120728e-16 1.0000000e+00
 1.6318172e-12 1.1452412e-17], sampled 0.2578404677637539
[2019-04-04 16:06:27,048] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 16:06:29,409] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7881 275774498.6820 1233.0964
[2019-04-04 16:06:30,433] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 4100000, evaluation results [4100000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.417175922201, 239942393.05634084, 1605.024971488376, 7182.788101514333, 275774498.68198794, 1233.096368464637]
[2019-04-04 16:06:30,567] A3C_AGENT_WORKER-Thread-18 INFO:Local step 256000, global step 4100060: loss 0.3717
[2019-04-04 16:06:30,567] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 256000, global step 4100060: learning rate 0.0000
[2019-04-04 16:06:33,218] A3C_AGENT_WORKER-Thread-13 INFO:Local step 257000, global step 4101543: loss 0.1049
[2019-04-04 16:06:33,219] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 257000, global step 4101543: learning rate 0.0000
[2019-04-04 16:06:34,259] A3C_AGENT_WORKER-Thread-2 INFO:Local step 257000, global step 4102090: loss 0.1103
[2019-04-04 16:06:34,261] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 257000, global step 4102090: learning rate 0.0000
[2019-04-04 16:06:35,360] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0932360e-08 2.5508224e-09 1.4724528e-13 1.0509351e-12 1.0000000e+00
 1.4383327e-09 2.3152037e-13], sum to 1.0000
[2019-04-04 16:06:35,360] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7215
[2019-04-04 16:06:35,382] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.299999999999999, 71.66666666666667, 0.0, 0.0, 26.0, 23.83253649860911, 0.0001336569709558255, 0.0, 1.0, 41624.18366700479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 795000.0000, 
sim time next is 795600.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.80297573593259, -0.01086529722337826, 0.0, 1.0, 41711.71489312355], 
processed observation next is [1.0, 0.21739130434782608, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.4835813113277159, 0.4963782342588739, 0.0, 1.0, 0.1986272137767788], 
reward next is 0.8014, 
noisyNet noise sample is [array([0.7311846], dtype=float32), 0.7259079]. 
=============================================
[2019-04-04 16:06:35,853] A3C_AGENT_WORKER-Thread-17 INFO:Local step 257000, global step 4102862: loss 0.1190
[2019-04-04 16:06:35,854] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 257000, global step 4102862: learning rate 0.0000
[2019-04-04 16:06:36,095] A3C_AGENT_WORKER-Thread-11 INFO:Local step 257000, global step 4102955: loss 0.1135
[2019-04-04 16:06:36,096] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 257000, global step 4102956: learning rate 0.0000
[2019-04-04 16:06:36,832] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.9098605e-10 5.4074450e-11 9.4570890e-16 8.0424059e-15 1.0000000e+00
 3.4141182e-10 1.9928688e-15], sum to 1.0000
[2019-04-04 16:06:36,838] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2010
[2019-04-04 16:06:36,891] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.45, 75.0, 32.0, 0.0, 26.0, 25.59155502017575, 0.31225846181947, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 808200.0000, 
sim time next is 808800.0000, 
raw observation next is [-6.366666666666667, 75.0, 36.83333333333333, 0.0, 26.0, 25.75384630888435, 0.3308070796805104, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.28624192059095105, 0.75, 0.12277777777777776, 0.0, 0.6666666666666666, 0.6461538590736957, 0.6102690265601701, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5673162], dtype=float32), -1.2390826]. 
=============================================
[2019-04-04 16:06:40,367] A3C_AGENT_WORKER-Thread-15 INFO:Local step 256500, global step 4104576: loss 0.0003
[2019-04-04 16:06:40,380] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 256500, global step 4104577: learning rate 0.0000
[2019-04-04 16:06:44,010] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5294597e-09 1.0790931e-10 6.3316745e-17 3.2546608e-15 1.0000000e+00
 3.9980134e-11 5.2285984e-17], sum to 1.0000
[2019-04-04 16:06:44,013] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1651
[2019-04-04 16:06:44,042] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.2, 92.33333333333334, 0.0, 0.0, 26.0, 25.35996090284966, 0.4808283260737784, 0.0, 1.0, 45556.5062802248], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1476600.0000, 
sim time next is 1477200.0000, 
raw observation next is [2.2, 92.66666666666667, 0.0, 0.0, 26.0, 25.4346853636094, 0.4888824950244632, 0.0, 1.0, 18764.59470446041], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6195571136341167, 0.6629608316748211, 0.0, 1.0, 0.08935521287838291], 
reward next is 0.9106, 
noisyNet noise sample is [array([-0.22747439], dtype=float32), -0.23227222]. 
=============================================
[2019-04-04 16:06:44,731] A3C_AGENT_WORKER-Thread-4 INFO:Local step 256500, global step 4106660: loss 0.0003
[2019-04-04 16:06:44,732] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 256500, global step 4106660: learning rate 0.0000
[2019-04-04 16:06:44,851] A3C_AGENT_WORKER-Thread-6 INFO:Local step 256500, global step 4106721: loss 0.0003
[2019-04-04 16:06:44,851] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 256500, global step 4106722: learning rate 0.0000
[2019-04-04 16:06:46,233] A3C_AGENT_WORKER-Thread-12 INFO:Local step 256500, global step 4107436: loss 0.0003
[2019-04-04 16:06:46,234] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 256500, global step 4107436: learning rate 0.0000
[2019-04-04 16:06:46,244] A3C_AGENT_WORKER-Thread-16 INFO:Local step 256500, global step 4107440: loss 0.0003
[2019-04-04 16:06:46,246] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 256500, global step 4107441: learning rate 0.0000
[2019-04-04 16:06:46,520] A3C_AGENT_WORKER-Thread-10 INFO:Local step 256500, global step 4107616: loss 0.0003
[2019-04-04 16:06:46,521] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 256500, global step 4107616: learning rate 0.0000
[2019-04-04 16:06:47,016] A3C_AGENT_WORKER-Thread-19 INFO:Local step 256500, global step 4107915: loss 0.0004
[2019-04-04 16:06:47,017] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 256500, global step 4107915: learning rate 0.0000
[2019-04-04 16:06:47,104] A3C_AGENT_WORKER-Thread-3 INFO:Local step 256500, global step 4107974: loss 0.0003
[2019-04-04 16:06:47,105] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 256500, global step 4107975: learning rate 0.0000
[2019-04-04 16:06:47,384] A3C_AGENT_WORKER-Thread-5 INFO:Local step 256500, global step 4108164: loss 0.0003
[2019-04-04 16:06:47,385] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 256500, global step 4108164: learning rate 0.0000
[2019-04-04 16:06:47,974] A3C_AGENT_WORKER-Thread-13 INFO:Local step 257500, global step 4108553: loss 0.0333
[2019-04-04 16:06:47,977] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 257500, global step 4108555: learning rate 0.0000
[2019-04-04 16:06:47,992] A3C_AGENT_WORKER-Thread-20 INFO:Local step 256500, global step 4108564: loss 0.0004
[2019-04-04 16:06:47,994] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 256500, global step 4108566: learning rate 0.0000
[2019-04-04 16:06:48,334] A3C_AGENT_WORKER-Thread-14 INFO:Local step 256500, global step 4108803: loss 0.0004
[2019-04-04 16:06:48,336] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 256500, global step 4108805: learning rate 0.0000
[2019-04-04 16:06:49,135] A3C_AGENT_WORKER-Thread-18 INFO:Local step 256500, global step 4109210: loss 0.0005
[2019-04-04 16:06:49,139] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 256500, global step 4109210: learning rate 0.0000
[2019-04-04 16:06:49,461] A3C_AGENT_WORKER-Thread-2 INFO:Local step 257500, global step 4109427: loss 0.0337
[2019-04-04 16:06:49,464] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 257500, global step 4109427: learning rate 0.0000
[2019-04-04 16:06:50,685] A3C_AGENT_WORKER-Thread-15 INFO:Local step 257000, global step 4110239: loss 0.1128
[2019-04-04 16:06:50,690] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 257000, global step 4110241: learning rate 0.0000
[2019-04-04 16:06:51,330] A3C_AGENT_WORKER-Thread-17 INFO:Local step 257500, global step 4110668: loss 0.0330
[2019-04-04 16:06:51,333] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 257500, global step 4110670: learning rate 0.0000
[2019-04-04 16:06:51,390] A3C_AGENT_WORKER-Thread-11 INFO:Local step 257500, global step 4110713: loss 0.0306
[2019-04-04 16:06:51,391] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 257500, global step 4110714: learning rate 0.0000
[2019-04-04 16:06:53,751] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6263270e-08 4.8785851e-09 6.8952704e-15 6.5571691e-13 1.0000000e+00
 3.4920566e-10 7.7068905e-14], sum to 1.0000
[2019-04-04 16:06:53,755] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6917
[2019-04-04 16:06:53,759] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.1, 82.5, 0.0, 0.0, 26.0, 24.0049628987579, 0.2516670621751306, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1216200.0000, 
sim time next is 1216800.0000, 
raw observation next is [16.1, 83.0, 0.0, 0.0, 26.0, 23.98415991919266, 0.2472898991815484, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9085872576177286, 0.83, 0.0, 0.0, 0.6666666666666666, 0.498679993266055, 0.5824299663938495, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6087042], dtype=float32), -0.78609115]. 
=============================================
[2019-04-04 16:06:53,999] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5991032e-10 1.0634277e-10 3.1913065e-16 6.4274910e-15 1.0000000e+00
 6.7008808e-11 9.6806559e-16], sum to 1.0000
[2019-04-04 16:06:53,999] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0313
[2019-04-04 16:06:54,003] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [19.1, 51.5, 140.0, 0.0, 26.0, 27.60724373800159, 0.9761799510574584, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1089000.0000, 
sim time next is 1089600.0000, 
raw observation next is [19.2, 50.66666666666667, 132.0, 0.0, 26.0, 27.67123835579016, 0.9903940898624205, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.994459833795014, 0.5066666666666667, 0.44, 0.0, 0.6666666666666666, 0.80593652964918, 0.8301313632874735, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00222039], dtype=float32), -0.63136196]. 
=============================================
[2019-04-04 16:06:55,971] A3C_AGENT_WORKER-Thread-4 INFO:Local step 257000, global step 4113812: loss 0.1220
[2019-04-04 16:06:55,972] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 257000, global step 4113812: learning rate 0.0000
[2019-04-04 16:06:56,057] A3C_AGENT_WORKER-Thread-6 INFO:Local step 257000, global step 4113872: loss 0.1233
[2019-04-04 16:06:56,058] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 257000, global step 4113872: learning rate 0.0000
[2019-04-04 16:06:56,334] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4208952e-09 8.1831419e-10 3.6035018e-15 1.8027179e-13 1.0000000e+00
 4.4860428e-11 1.3124967e-14], sum to 1.0000
[2019-04-04 16:06:56,337] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3954
[2019-04-04 16:06:56,344] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.35477446268608, 0.3242944085350237, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1205400.0000, 
sim time next is 1206000.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 26.0, 24.3302397221327, 0.3183132283336421, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 1.0, 0.922437673130194, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5275199768443916, 0.6061044094445474, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.50007945], dtype=float32), 0.31322378]. 
=============================================
[2019-04-04 16:06:56,362] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[76.51462]
 [76.53289]
 [76.54685]
 [76.56066]
 [76.58083]], R is [[76.7412262 ]
 [76.97381592]
 [77.20407867]
 [77.43203735]
 [77.65771484]].
[2019-04-04 16:06:57,263] A3C_AGENT_WORKER-Thread-16 INFO:Local step 257000, global step 4114660: loss 0.1272
[2019-04-04 16:06:57,264] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 257000, global step 4114660: learning rate 0.0000
[2019-04-04 16:06:57,638] A3C_AGENT_WORKER-Thread-12 INFO:Local step 257000, global step 4114893: loss 0.1242
[2019-04-04 16:06:57,641] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 257000, global step 4114893: learning rate 0.0000
[2019-04-04 16:06:58,148] A3C_AGENT_WORKER-Thread-10 INFO:Local step 257000, global step 4115187: loss 0.1233
[2019-04-04 16:06:58,152] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 257000, global step 4115188: learning rate 0.0000
[2019-04-04 16:06:58,623] A3C_AGENT_WORKER-Thread-19 INFO:Local step 257000, global step 4115422: loss 0.1223
[2019-04-04 16:06:58,624] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 257000, global step 4115422: learning rate 0.0000
[2019-04-04 16:06:58,821] A3C_AGENT_WORKER-Thread-5 INFO:Local step 257000, global step 4115525: loss 0.1299
[2019-04-04 16:06:58,823] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 257000, global step 4115527: learning rate 0.0000
[2019-04-04 16:06:58,837] A3C_AGENT_WORKER-Thread-3 INFO:Local step 257000, global step 4115532: loss 0.1205
[2019-04-04 16:06:58,839] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 257000, global step 4115532: learning rate 0.0000
[2019-04-04 16:06:59,319] A3C_AGENT_WORKER-Thread-20 INFO:Local step 257000, global step 4115767: loss 0.1257
[2019-04-04 16:06:59,322] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 257000, global step 4115767: learning rate 0.0000
[2019-04-04 16:06:59,715] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.3005246e-10 2.5743851e-10 2.6248069e-16 1.3561856e-15 1.0000000e+00
 4.2419564e-11 6.0940881e-16], sum to 1.0000
[2019-04-04 16:06:59,716] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1487
[2019-04-04 16:06:59,767] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.38145868974833, 0.5545750491299244, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1322400.0000, 
sim time next is 1323000.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.62146487627501, 0.583940720553012, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6351220730229175, 0.694646906851004, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6712717], dtype=float32), -1.1716499]. 
=============================================
[2019-04-04 16:06:59,778] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[90.65723 ]
 [89.24592 ]
 [89.19039 ]
 [89.273384]
 [89.437584]], R is [[92.00167847]
 [92.08166504]
 [91.89186859]
 [91.83503723]
 [91.81397247]].
[2019-04-04 16:07:00,084] A3C_AGENT_WORKER-Thread-14 INFO:Local step 257000, global step 4116177: loss 0.1244
[2019-04-04 16:07:00,085] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 257000, global step 4116177: learning rate 0.0000
[2019-04-04 16:07:00,320] A3C_AGENT_WORKER-Thread-18 INFO:Local step 257000, global step 4116284: loss 0.1180
[2019-04-04 16:07:00,322] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 257000, global step 4116285: learning rate 0.0000
[2019-04-04 16:07:04,928] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.0775750e-09 6.1879030e-10 1.0595680e-15 8.1800669e-14 1.0000000e+00
 6.0511124e-10 3.6323047e-15], sum to 1.0000
[2019-04-04 16:07:04,931] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1165
[2019-04-04 16:07:04,948] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.26038349037503, 0.474824300342533, 0.0, 1.0, 39573.48332132596], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1392000.0000, 
sim time next is 1392600.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.29225732617215, 0.4808329460423746, 0.0, 1.0, 39517.65007590649], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.607688110514346, 0.6602776486807915, 0.0, 1.0, 0.1881792860757452], 
reward next is 0.8118, 
noisyNet noise sample is [array([-0.5478369], dtype=float32), -1.7805094]. 
=============================================
[2019-04-04 16:07:06,084] A3C_AGENT_WORKER-Thread-15 INFO:Local step 257500, global step 4118971: loss 0.0227
[2019-04-04 16:07:06,084] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 257500, global step 4118971: learning rate 0.0000
[2019-04-04 16:07:06,529] A3C_AGENT_WORKER-Thread-13 INFO:Local step 258000, global step 4119170: loss 2.3009
[2019-04-04 16:07:06,531] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 258000, global step 4119170: learning rate 0.0000
[2019-04-04 16:07:07,966] A3C_AGENT_WORKER-Thread-2 INFO:Local step 258000, global step 4119788: loss 2.2867
[2019-04-04 16:07:07,969] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 258000, global step 4119789: learning rate 0.0000
[2019-04-04 16:07:09,378] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1675866e-09 6.9238566e-11 1.9265056e-16 3.8007897e-15 1.0000000e+00
 3.0869161e-11 4.6500240e-16], sum to 1.0000
[2019-04-04 16:07:09,378] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7892
[2019-04-04 16:07:09,393] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.266666666666667, 92.0, 0.0, 0.0, 26.0, 25.44486725899488, 0.4830487081962787, 0.0, 1.0, 63602.48902815383], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1462800.0000, 
sim time next is 1463400.0000, 
raw observation next is [1.35, 92.0, 0.0, 0.0, 26.0, 25.36074191887129, 0.4814286824874179, 0.0, 1.0, 87271.99197556192], 
processed observation next is [1.0, 0.9565217391304348, 0.5000000000000001, 0.92, 0.0, 0.0, 0.6666666666666666, 0.613395159905941, 0.6604762274958059, 0.0, 1.0, 0.41558091416934245], 
reward next is 0.5844, 
noisyNet noise sample is [array([1.231455], dtype=float32), -0.89779824]. 
=============================================
[2019-04-04 16:07:09,857] A3C_AGENT_WORKER-Thread-11 INFO:Local step 258000, global step 4120706: loss 2.2978
[2019-04-04 16:07:09,858] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 258000, global step 4120706: learning rate 0.0000
[2019-04-04 16:07:10,382] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.82633941e-10 1.03315995e-10 4.41297892e-16 3.40752588e-15
 1.00000000e+00 4.88781966e-11 4.11790176e-16], sum to 1.0000
[2019-04-04 16:07:10,382] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9647
[2019-04-04 16:07:10,401] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.30413320814224, 0.4507406696310007, 0.0, 1.0, 36927.63048388365], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1488000.0000, 
sim time next is 1488600.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.34599851997064, 0.4491987963235839, 0.0, 1.0, 36899.92242290112], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6121665433308866, 0.6497329321078613, 0.0, 1.0, 0.17571391629952915], 
reward next is 0.8243, 
noisyNet noise sample is [array([-1.0642205], dtype=float32), -0.8716872]. 
=============================================
[2019-04-04 16:07:10,426] A3C_AGENT_WORKER-Thread-17 INFO:Local step 258000, global step 4120997: loss 2.2843
[2019-04-04 16:07:10,431] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 258000, global step 4121000: learning rate 0.0000
[2019-04-04 16:07:11,106] A3C_AGENT_WORKER-Thread-6 INFO:Local step 257500, global step 4121370: loss 0.0196
[2019-04-04 16:07:11,107] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 257500, global step 4121370: learning rate 0.0000
[2019-04-04 16:07:11,259] A3C_AGENT_WORKER-Thread-4 INFO:Local step 257500, global step 4121442: loss 0.0186
[2019-04-04 16:07:11,262] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 257500, global step 4121444: learning rate 0.0000
[2019-04-04 16:07:12,605] A3C_AGENT_WORKER-Thread-16 INFO:Local step 257500, global step 4122228: loss 0.0173
[2019-04-04 16:07:12,608] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 257500, global step 4122228: learning rate 0.0000
[2019-04-04 16:07:12,681] A3C_AGENT_WORKER-Thread-12 INFO:Local step 257500, global step 4122280: loss 0.0190
[2019-04-04 16:07:12,683] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 257500, global step 4122281: learning rate 0.0000
[2019-04-04 16:07:13,021] A3C_AGENT_WORKER-Thread-10 INFO:Local step 257500, global step 4122487: loss 0.0186
[2019-04-04 16:07:13,022] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 257500, global step 4122488: learning rate 0.0000
[2019-04-04 16:07:13,964] A3C_AGENT_WORKER-Thread-5 INFO:Local step 257500, global step 4123045: loss 0.0176
[2019-04-04 16:07:13,964] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 257500, global step 4123045: learning rate 0.0000
[2019-04-04 16:07:13,980] A3C_AGENT_WORKER-Thread-19 INFO:Local step 257500, global step 4123055: loss 0.0172
[2019-04-04 16:07:13,981] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 257500, global step 4123056: learning rate 0.0000
[2019-04-04 16:07:14,330] A3C_AGENT_WORKER-Thread-3 INFO:Local step 257500, global step 4123284: loss 0.0174
[2019-04-04 16:07:14,332] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 257500, global step 4123284: learning rate 0.0000
[2019-04-04 16:07:14,341] A3C_AGENT_WORKER-Thread-20 INFO:Local step 257500, global step 4123291: loss 0.0169
[2019-04-04 16:07:14,344] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 257500, global step 4123293: learning rate 0.0000
[2019-04-04 16:07:14,978] A3C_AGENT_WORKER-Thread-14 INFO:Local step 257500, global step 4123670: loss 0.0176
[2019-04-04 16:07:14,982] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 257500, global step 4123674: learning rate 0.0000
[2019-04-04 16:07:15,424] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.1973292e-09 4.3338798e-11 5.5782731e-16 9.3592956e-15 1.0000000e+00
 2.9547237e-10 4.4273112e-16], sum to 1.0000
[2019-04-04 16:07:15,426] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2344
[2019-04-04 16:07:15,473] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 86.66666666666667, 105.8333333333333, 0.0, 26.0, 25.5962663802712, 0.3685088842205732, 1.0, 1.0, 94340.80365552373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1687200.0000, 
sim time next is 1687800.0000, 
raw observation next is [1.1, 87.33333333333334, 104.6666666666667, 0.0, 26.0, 24.88493441504368, 0.4216428668331084, 1.0, 1.0, 196217.9094192962], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.8733333333333334, 0.348888888888889, 0.0, 0.6666666666666666, 0.5737445345869734, 0.6405476222777028, 1.0, 1.0, 0.9343709972347438], 
reward next is 0.0656, 
noisyNet noise sample is [array([1.1649716], dtype=float32), -0.12599231]. 
=============================================
[2019-04-04 16:07:15,548] A3C_AGENT_WORKER-Thread-18 INFO:Local step 257500, global step 4124024: loss 0.0160
[2019-04-04 16:07:15,557] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 257500, global step 4124027: learning rate 0.0000
[2019-04-04 16:07:23,630] A3C_AGENT_WORKER-Thread-15 INFO:Local step 258000, global step 4127571: loss 2.2064
[2019-04-04 16:07:23,633] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 258000, global step 4127571: learning rate 0.0000
[2019-04-04 16:07:27,786] A3C_AGENT_WORKER-Thread-13 INFO:Local step 258500, global step 4129092: loss 0.0339
[2019-04-04 16:07:27,787] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 258500, global step 4129092: learning rate 0.0000
[2019-04-04 16:07:28,627] A3C_AGENT_WORKER-Thread-2 INFO:Local step 258500, global step 4129435: loss 0.0361
[2019-04-04 16:07:28,643] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 258500, global step 4129436: learning rate 0.0000
[2019-04-04 16:07:28,728] A3C_AGENT_WORKER-Thread-4 INFO:Local step 258000, global step 4129473: loss 2.1354
[2019-04-04 16:07:28,731] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 258000, global step 4129473: learning rate 0.0000
[2019-04-04 16:07:28,961] A3C_AGENT_WORKER-Thread-6 INFO:Local step 258000, global step 4129564: loss 2.1234
[2019-04-04 16:07:28,962] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 258000, global step 4129564: learning rate 0.0000
[2019-04-04 16:07:30,391] A3C_AGENT_WORKER-Thread-16 INFO:Local step 258000, global step 4130006: loss 2.1028
[2019-04-04 16:07:30,393] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 258000, global step 4130006: learning rate 0.0000
[2019-04-04 16:07:30,435] A3C_AGENT_WORKER-Thread-12 INFO:Local step 258000, global step 4130018: loss 2.1241
[2019-04-04 16:07:30,435] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 258000, global step 4130018: learning rate 0.0000
[2019-04-04 16:07:30,811] A3C_AGENT_WORKER-Thread-11 INFO:Local step 258500, global step 4130155: loss 0.0377
[2019-04-04 16:07:30,812] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 258500, global step 4130155: learning rate 0.0000
[2019-04-04 16:07:31,369] A3C_AGENT_WORKER-Thread-17 INFO:Local step 258500, global step 4130347: loss 0.0354
[2019-04-04 16:07:31,370] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 258500, global step 4130347: learning rate 0.0000
[2019-04-04 16:07:31,439] A3C_AGENT_WORKER-Thread-10 INFO:Local step 258000, global step 4130372: loss 2.1224
[2019-04-04 16:07:31,440] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 258000, global step 4130372: learning rate 0.0000
[2019-04-04 16:07:31,785] A3C_AGENT_WORKER-Thread-19 INFO:Local step 258000, global step 4130484: loss 2.1262
[2019-04-04 16:07:31,786] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 258000, global step 4130484: learning rate 0.0000
[2019-04-04 16:07:32,002] A3C_AGENT_WORKER-Thread-20 INFO:Local step 258000, global step 4130560: loss 2.1149
[2019-04-04 16:07:32,004] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 258000, global step 4130561: learning rate 0.0000
[2019-04-04 16:07:32,122] A3C_AGENT_WORKER-Thread-5 INFO:Local step 258000, global step 4130601: loss 2.1275
[2019-04-04 16:07:32,122] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 258000, global step 4130601: learning rate 0.0000
[2019-04-04 16:07:32,432] A3C_AGENT_WORKER-Thread-3 INFO:Local step 258000, global step 4130699: loss 2.1284
[2019-04-04 16:07:32,433] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 258000, global step 4130699: learning rate 0.0000
[2019-04-04 16:07:33,599] A3C_AGENT_WORKER-Thread-14 INFO:Local step 258000, global step 4131111: loss 2.1256
[2019-04-04 16:07:33,600] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 258000, global step 4131111: learning rate 0.0000
[2019-04-04 16:07:33,801] A3C_AGENT_WORKER-Thread-18 INFO:Local step 258000, global step 4131189: loss 2.1201
[2019-04-04 16:07:33,803] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 258000, global step 4131189: learning rate 0.0000
[2019-04-04 16:07:34,542] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7418206e-09 1.8221172e-10 8.4314835e-16 9.1250673e-15 1.0000000e+00
 3.3195327e-10 9.0958697e-16], sum to 1.0000
[2019-04-04 16:07:34,543] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7327
[2019-04-04 16:07:34,600] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 70.5, 128.0, 0.0, 26.0, 26.07871873689234, 0.4904732108328942, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2211000.0000, 
sim time next is 2211600.0000, 
raw observation next is [-3.9, 70.0, 124.0, 0.0, 26.0, 26.26037114906872, 0.50449445929547, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.7, 0.41333333333333333, 0.0, 0.6666666666666666, 0.6883642624223935, 0.6681648197651566, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8409383], dtype=float32), -0.22814119]. 
=============================================
[2019-04-04 16:07:34,709] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2979274e-09 3.3216227e-10 7.4131671e-15 3.0179480e-13 1.0000000e+00
 4.7560450e-10 8.6695491e-15], sum to 1.0000
[2019-04-04 16:07:34,709] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6889
[2019-04-04 16:07:34,729] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.466666666666667, 76.0, 0.0, 0.0, 26.0, 24.08021359554714, 0.02575532535994588, 0.0, 1.0, 45202.72877517273], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1905600.0000, 
sim time next is 1906200.0000, 
raw observation next is [-7.55, 76.5, 0.0, 0.0, 26.0, 24.05540669040379, 0.02239121730731762, 0.0, 1.0, 45193.15126390557], 
processed observation next is [1.0, 0.043478260869565216, 0.25346260387811637, 0.765, 0.0, 0.0, 0.6666666666666666, 0.5046172242003157, 0.5074637391024391, 0.0, 1.0, 0.21520548220907415], 
reward next is 0.7848, 
noisyNet noise sample is [array([0.9493073], dtype=float32), -2.606356]. 
=============================================
[2019-04-04 16:07:39,679] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.4222045e-09 9.4604380e-10 3.8309797e-15 2.5761346e-13 1.0000000e+00
 1.0744563e-09 1.2158491e-14], sum to 1.0000
[2019-04-04 16:07:39,679] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1032
[2019-04-04 16:07:39,716] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.600000000000001, 82.16666666666667, 0.0, 0.0, 26.0, 25.16418641321962, 0.3812225876438158, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1973400.0000, 
sim time next is 1974000.0000, 
raw observation next is [-5.6, 81.33333333333334, 0.0, 0.0, 26.0, 25.26932642007238, 0.3818918051824087, 0.0, 1.0, 83868.14641908035], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.8133333333333335, 0.0, 0.0, 0.6666666666666666, 0.6057772016726984, 0.6272972683941362, 0.0, 1.0, 0.39937212580514453], 
reward next is 0.6006, 
noisyNet noise sample is [array([0.00729268], dtype=float32), -0.15018018]. 
=============================================
[2019-04-04 16:07:39,724] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[78.17174 ]
 [79.44908 ]
 [80.788246]
 [81.66826 ]
 [81.026245]], R is [[77.31604004]
 [77.5428772 ]
 [77.50498199]
 [76.88851929]
 [76.37259674]].
[2019-04-04 16:07:40,759] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.7265296e-10 2.2043035e-10 1.6823621e-15 1.8258812e-14 1.0000000e+00
 3.5888171e-11 1.9566431e-15], sum to 1.0000
[2019-04-04 16:07:40,760] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1398
[2019-04-04 16:07:40,814] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.866666666666666, 85.0, 62.33333333333333, 0.0, 26.0, 25.54746188489154, 0.2797162900974745, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2020800.0000, 
sim time next is 2021400.0000, 
raw observation next is [-5.8, 84.5, 69.0, 0.0, 26.0, 25.50788034531727, 0.284191166954882, 1.0, 1.0, 18724.46385866081], 
processed observation next is [1.0, 0.391304347826087, 0.30193905817174516, 0.845, 0.23, 0.0, 0.6666666666666666, 0.625656695443106, 0.5947303889849607, 1.0, 1.0, 0.08916411361267051], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.1305999], dtype=float32), 0.41511142]. 
=============================================
[2019-04-04 16:07:44,950] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.1137889e-08 2.5299003e-09 1.6347345e-14 7.4144190e-13 1.0000000e+00
 1.0170808e-09 2.2350906e-14], sum to 1.0000
[2019-04-04 16:07:44,950] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5570
[2019-04-04 16:07:44,967] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.100000000000001, 86.83333333333334, 0.0, 0.0, 26.0, 24.47069031061058, 0.1740438330499393, 0.0, 1.0, 43217.85449254519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2085000.0000, 
sim time next is 2085600.0000, 
raw observation next is [-5.2, 87.66666666666667, 0.0, 0.0, 26.0, 24.46721326904312, 0.1702331428976488, 0.0, 1.0, 43302.18611546865], 
processed observation next is [1.0, 0.13043478260869565, 0.31855955678670367, 0.8766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5389344390869267, 0.556744380965883, 0.0, 1.0, 0.20620088626413643], 
reward next is 0.7938, 
noisyNet noise sample is [array([0.3210354], dtype=float32), 1.7524242]. 
=============================================
[2019-04-04 16:07:45,632] A3C_AGENT_WORKER-Thread-15 INFO:Local step 258500, global step 4135414: loss 0.0266
[2019-04-04 16:07:45,632] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 258500, global step 4135414: learning rate 0.0000
[2019-04-04 16:07:49,516] A3C_AGENT_WORKER-Thread-13 INFO:Local step 259000, global step 4136811: loss 0.3701
[2019-04-04 16:07:49,524] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 259000, global step 4136812: learning rate 0.0000
[2019-04-04 16:07:50,069] A3C_AGENT_WORKER-Thread-2 INFO:Local step 259000, global step 4137034: loss 0.3639
[2019-04-04 16:07:50,070] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 259000, global step 4137035: learning rate 0.0000
[2019-04-04 16:07:50,604] A3C_AGENT_WORKER-Thread-4 INFO:Local step 258500, global step 4137236: loss 0.0251
[2019-04-04 16:07:50,608] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 258500, global step 4137237: learning rate 0.0000
[2019-04-04 16:07:51,745] A3C_AGENT_WORKER-Thread-6 INFO:Local step 258500, global step 4137636: loss 0.0266
[2019-04-04 16:07:51,746] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 258500, global step 4137636: learning rate 0.0000
[2019-04-04 16:07:52,499] A3C_AGENT_WORKER-Thread-16 INFO:Local step 258500, global step 4137941: loss 0.0255
[2019-04-04 16:07:52,500] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 258500, global step 4137942: learning rate 0.0000
[2019-04-04 16:07:52,677] A3C_AGENT_WORKER-Thread-12 INFO:Local step 258500, global step 4138020: loss 0.0257
[2019-04-04 16:07:52,678] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 258500, global step 4138020: learning rate 0.0000
[2019-04-04 16:07:52,710] A3C_AGENT_WORKER-Thread-11 INFO:Local step 259000, global step 4138032: loss 0.3577
[2019-04-04 16:07:52,713] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 259000, global step 4138034: learning rate 0.0000
[2019-04-04 16:07:53,373] A3C_AGENT_WORKER-Thread-17 INFO:Local step 259000, global step 4138257: loss 0.3595
[2019-04-04 16:07:53,374] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 259000, global step 4138257: learning rate 0.0000
[2019-04-04 16:07:53,653] A3C_AGENT_WORKER-Thread-10 INFO:Local step 258500, global step 4138353: loss 0.0231
[2019-04-04 16:07:53,654] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 258500, global step 4138354: learning rate 0.0000
[2019-04-04 16:07:54,174] A3C_AGENT_WORKER-Thread-19 INFO:Local step 258500, global step 4138586: loss 0.0233
[2019-04-04 16:07:54,175] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 258500, global step 4138586: learning rate 0.0000
[2019-04-04 16:07:54,336] A3C_AGENT_WORKER-Thread-20 INFO:Local step 258500, global step 4138669: loss 0.0245
[2019-04-04 16:07:54,337] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 258500, global step 4138669: learning rate 0.0000
[2019-04-04 16:07:54,513] A3C_AGENT_WORKER-Thread-5 INFO:Local step 258500, global step 4138746: loss 0.0231
[2019-04-04 16:07:54,515] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 258500, global step 4138746: learning rate 0.0000
[2019-04-04 16:07:54,973] A3C_AGENT_WORKER-Thread-3 INFO:Local step 258500, global step 4138940: loss 0.0237
[2019-04-04 16:07:54,976] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 258500, global step 4138941: learning rate 0.0000
[2019-04-04 16:07:55,650] A3C_AGENT_WORKER-Thread-14 INFO:Local step 258500, global step 4139188: loss 0.0223
[2019-04-04 16:07:55,652] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 258500, global step 4139189: learning rate 0.0000
[2019-04-04 16:07:56,214] A3C_AGENT_WORKER-Thread-18 INFO:Local step 258500, global step 4139406: loss 0.0237
[2019-04-04 16:07:56,215] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 258500, global step 4139406: learning rate 0.0000
[2019-04-04 16:08:00,847] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9663294e-09 2.5996747e-09 1.6717879e-14 1.4735878e-13 1.0000000e+00
 1.8678954e-09 7.1521124e-14], sum to 1.0000
[2019-04-04 16:08:00,859] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4827
[2019-04-04 16:08:00,872] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 77.0, 0.0, 0.0, 26.0, 24.39091407359459, 0.1913458234247527, 0.0, 1.0, 44341.99231880936], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2247600.0000, 
sim time next is 2248200.0000, 
raw observation next is [-6.7, 76.5, 0.0, 0.0, 26.0, 24.39784917613707, 0.1978254317769287, 0.0, 1.0, 44328.39378739957], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.765, 0.0, 0.0, 0.6666666666666666, 0.5331540980114223, 0.5659418105923096, 0.0, 1.0, 0.21108758946380746], 
reward next is 0.7889, 
noisyNet noise sample is [array([-1.9209485], dtype=float32), -1.1464282]. 
=============================================
[2019-04-04 16:08:02,559] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.1433008e-09 1.1827250e-09 1.5790658e-14 3.3383089e-13 1.0000000e+00
 8.6377440e-11 5.3306538e-14], sum to 1.0000
[2019-04-04 16:08:02,561] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1095
[2019-04-04 16:08:02,574] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.74856982035262, 0.2363332469226217, 0.0, 1.0, 39071.34737238337], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2341200.0000, 
sim time next is 2341800.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.72690201793949, 0.2265364780359926, 0.0, 1.0, 39176.75122516284], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5605751681616242, 0.5755121593453308, 0.0, 1.0, 0.18655595821506116], 
reward next is 0.8134, 
noisyNet noise sample is [array([1.2236218], dtype=float32), -0.9183628]. 
=============================================
[2019-04-04 16:08:07,217] A3C_AGENT_WORKER-Thread-15 INFO:Local step 259000, global step 4143682: loss 0.3516
[2019-04-04 16:08:07,218] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 259000, global step 4143682: learning rate 0.0000
[2019-04-04 16:08:07,988] A3C_AGENT_WORKER-Thread-2 INFO:Local step 259500, global step 4144000: loss 0.0061
[2019-04-04 16:08:07,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 259500, global step 4144002: learning rate 0.0000
[2019-04-04 16:08:08,086] A3C_AGENT_WORKER-Thread-13 INFO:Local step 259500, global step 4144035: loss 0.0060
[2019-04-04 16:08:08,088] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 259500, global step 4144035: learning rate 0.0000
[2019-04-04 16:08:11,165] A3C_AGENT_WORKER-Thread-11 INFO:Local step 259500, global step 4145203: loss 0.0058
[2019-04-04 16:08:11,168] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 259500, global step 4145203: learning rate 0.0000
[2019-04-04 16:08:11,656] A3C_AGENT_WORKER-Thread-4 INFO:Local step 259000, global step 4145442: loss 0.3469
[2019-04-04 16:08:11,659] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 259000, global step 4145442: learning rate 0.0000
[2019-04-04 16:08:12,315] A3C_AGENT_WORKER-Thread-17 INFO:Local step 259500, global step 4145737: loss 0.0059
[2019-04-04 16:08:12,315] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 259500, global step 4145737: learning rate 0.0000
[2019-04-04 16:08:13,050] A3C_AGENT_WORKER-Thread-6 INFO:Local step 259000, global step 4146058: loss 0.3463
[2019-04-04 16:08:13,052] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 259000, global step 4146058: learning rate 0.0000
[2019-04-04 16:08:13,330] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.2038590e-09 1.1468854e-09 4.5836419e-14 1.1810679e-13 1.0000000e+00
 4.7121346e-10 3.3227564e-14], sum to 1.0000
[2019-04-04 16:08:13,330] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4379
[2019-04-04 16:08:13,380] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1333333333333333, 30.0, 89.33333333333333, 842.3333333333334, 26.0, 24.9516089298658, 0.2537982378817378, 0.0, 1.0, 25270.20705756185], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2463600.0000, 
sim time next is 2464200.0000, 
raw observation next is [0.5, 29.5, 90.0, 845.0, 26.0, 24.92205102458085, 0.2570342367508476, 0.0, 1.0, 32810.6120823123], 
processed observation next is [0.0, 0.5217391304347826, 0.4764542936288089, 0.295, 0.3, 0.9337016574585635, 0.6666666666666666, 0.5768375853817375, 0.5856780789169492, 0.0, 1.0, 0.15624100991577286], 
reward next is 0.8438, 
noisyNet noise sample is [array([0.08932926], dtype=float32), 1.1576878]. 
=============================================
[2019-04-04 16:08:13,703] A3C_AGENT_WORKER-Thread-16 INFO:Local step 259000, global step 4146359: loss 0.3407
[2019-04-04 16:08:13,704] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 259000, global step 4146359: learning rate 0.0000
[2019-04-04 16:08:13,818] A3C_AGENT_WORKER-Thread-12 INFO:Local step 259000, global step 4146414: loss 0.3347
[2019-04-04 16:08:13,820] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 259000, global step 4146414: learning rate 0.0000
[2019-04-04 16:08:14,550] A3C_AGENT_WORKER-Thread-10 INFO:Local step 259000, global step 4146738: loss 0.3345
[2019-04-04 16:08:14,551] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 259000, global step 4146738: learning rate 0.0000
[2019-04-04 16:08:14,890] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.2555352e-09 6.9608697e-10 1.8449252e-14 4.6647304e-14 1.0000000e+00
 2.7706759e-10 5.2015606e-14], sum to 1.0000
[2019-04-04 16:08:14,892] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1933
[2019-04-04 16:08:14,926] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.35, 29.5, 0.0, 0.0, 26.0, 24.94139547057776, 0.2493861225494482, 0.0, 1.0, 106876.708714007], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2489400.0000, 
sim time next is 2490000.0000, 
raw observation next is [-0.4666666666666666, 29.33333333333334, 0.0, 0.0, 26.0, 25.01811646359623, 0.265045019761083, 0.0, 1.0, 62842.34780527483], 
processed observation next is [0.0, 0.8260869565217391, 0.44967682363804257, 0.2933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5848430386330191, 0.588348339920361, 0.0, 1.0, 0.2992492752632135], 
reward next is 0.7008, 
noisyNet noise sample is [array([-1.5471505], dtype=float32), -1.2625431]. 
=============================================
[2019-04-04 16:08:14,932] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[74.59421 ]
 [74.05925 ]
 [73.16556 ]
 [73.01302 ]
 [72.787544]], R is [[74.7959671 ]
 [74.53907776]
 [73.96708679]
 [74.05926514]
 [74.12287903]].
[2019-04-04 16:08:15,008] A3C_AGENT_WORKER-Thread-19 INFO:Local step 259000, global step 4146913: loss 0.3338
[2019-04-04 16:08:15,009] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 259000, global step 4146913: learning rate 0.0000
[2019-04-04 16:08:15,414] A3C_AGENT_WORKER-Thread-20 INFO:Local step 259000, global step 4147108: loss 0.3312
[2019-04-04 16:08:15,414] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 259000, global step 4147108: learning rate 0.0000
[2019-04-04 16:08:15,639] A3C_AGENT_WORKER-Thread-5 INFO:Local step 259000, global step 4147215: loss 0.3303
[2019-04-04 16:08:15,639] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 259000, global step 4147215: learning rate 0.0000
[2019-04-04 16:08:15,778] A3C_AGENT_WORKER-Thread-3 INFO:Local step 259000, global step 4147284: loss 0.3208
[2019-04-04 16:08:15,779] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 259000, global step 4147284: learning rate 0.0000
[2019-04-04 16:08:15,787] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8225778e-08 4.3072830e-09 1.0669999e-13 2.6765192e-13 1.0000000e+00
 2.7085809e-09 3.2758765e-13], sum to 1.0000
[2019-04-04 16:08:15,790] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5238
[2019-04-04 16:08:15,846] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.55, 29.0, 0.0, 0.0, 26.0, 24.91554849102632, 0.2113485216110403, 0.0, 1.0, 41154.06603762173], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2485800.0000, 
sim time next is 2486400.0000, 
raw observation next is [0.3666666666666668, 29.33333333333334, 0.0, 0.0, 26.0, 24.90555538474279, 0.2109580907247238, 0.0, 1.0, 46503.78432833819], 
processed observation next is [0.0, 0.782608695652174, 0.4727608494921515, 0.2933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5754629487285658, 0.5703193635749079, 0.0, 1.0, 0.22144659203970565], 
reward next is 0.7786, 
noisyNet noise sample is [array([-0.28214782], dtype=float32), 0.6537531]. 
=============================================
[2019-04-04 16:08:15,944] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9998205e-08 6.4544028e-09 5.1772612e-14 3.5585901e-13 1.0000000e+00
 1.5465738e-09 9.0119007e-14], sum to 1.0000
[2019-04-04 16:08:15,946] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9008
[2019-04-04 16:08:15,962] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.633333333333333, 55.0, 0.0, 0.0, 26.0, 24.63798139656422, 0.13627945957687, 0.0, 1.0, 39082.22022812819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2529600.0000, 
sim time next is 2530200.0000, 
raw observation next is [-2.716666666666667, 54.5, 0.0, 0.0, 26.0, 24.80429998977505, 0.1362474526116382, 0.0, 1.0, 38862.78827562343], 
processed observation next is [1.0, 0.2608695652173913, 0.3873499538319483, 0.545, 0.0, 0.0, 0.6666666666666666, 0.5670249991479208, 0.5454158175372127, 0.0, 1.0, 0.18506089655058777], 
reward next is 0.8149, 
noisyNet noise sample is [array([1.4261695], dtype=float32), 0.35995516]. 
=============================================
[2019-04-04 16:08:15,975] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7111213e-09 2.5342084e-09 4.8799035e-14 1.1733015e-13 1.0000000e+00
 2.6451252e-09 6.3576301e-14], sum to 1.0000
[2019-04-04 16:08:15,975] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9791
[2019-04-04 16:08:16,012] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 28.0, 0.0, 0.0, 26.0, 24.90493343808769, 0.2225962882593833, 0.0, 1.0, 25416.21777239611], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2484000.0000, 
sim time next is 2484600.0000, 
raw observation next is [0.9166666666666667, 28.33333333333334, 0.0, 0.0, 26.0, 24.92738764217152, 0.2196868325290545, 0.0, 1.0, 18719.20270990485], 
processed observation next is [0.0, 0.782608695652174, 0.48799630655586346, 0.2833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5772823035142934, 0.5732289441763515, 0.0, 1.0, 0.08913906052335643], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.519123], dtype=float32), -1.3258852]. 
=============================================
[2019-04-04 16:08:17,212] A3C_AGENT_WORKER-Thread-14 INFO:Local step 259000, global step 4147932: loss 0.3227
[2019-04-04 16:08:17,212] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 259000, global step 4147933: learning rate 0.0000
[2019-04-04 16:08:17,305] A3C_AGENT_WORKER-Thread-18 INFO:Local step 259000, global step 4147977: loss 0.3297
[2019-04-04 16:08:17,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 259000, global step 4147977: learning rate 0.0000
[2019-04-04 16:08:18,351] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.62781555e-09 2.33807002e-10 3.47621813e-15 1.29440349e-13
 1.00000000e+00 2.47536047e-10 1.01365026e-14], sum to 1.0000
[2019-04-04 16:08:18,353] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9023
[2019-04-04 16:08:18,378] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 48.33333333333334, 133.5, 43.0, 26.0, 25.8088608955518, 0.3008977615819905, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2542800.0000, 
sim time next is 2543400.0000, 
raw observation next is [-0.8999999999999999, 48.0, 133.0, 45.0, 26.0, 25.79285968796375, 0.299740736197508, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.43767313019390586, 0.48, 0.44333333333333336, 0.049723756906077346, 0.6666666666666666, 0.6494049739969793, 0.5999135787325026, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1231598], dtype=float32), 0.34758818]. 
=============================================
[2019-04-04 16:08:19,974] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8946633e-09 3.7488797e-11 3.0755189e-16 8.0023158e-15 1.0000000e+00
 6.4971695e-11 7.2978707e-16], sum to 1.0000
[2019-04-04 16:08:19,976] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1813
[2019-04-04 16:08:19,982] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8166666666666668, 40.33333333333333, 227.3333333333333, 54.66666666666667, 26.0, 25.74900418370447, 0.3266048186206319, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2548200.0000, 
sim time next is 2548800.0000, 
raw observation next is [1.1, 39.0, 225.0, 46.5, 26.0, 25.74823554188868, 0.3315675024375843, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.39, 0.75, 0.05138121546961326, 0.6666666666666666, 0.64568629515739, 0.6105225008125281, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6244829], dtype=float32), 1.5925469]. 
=============================================
[2019-04-04 16:08:20,370] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4956581e-09 9.1193486e-10 1.3954304e-14 3.6876779e-14 1.0000000e+00
 2.1267601e-09 8.6581133e-15], sum to 1.0000
[2019-04-04 16:08:20,370] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2099
[2019-04-04 16:08:20,375] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.3, 29.0, 136.5, 313.0, 26.0, 25.77883727663482, 0.3782907114441148, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2559600.0000, 
sim time next is 2560200.0000, 
raw observation next is [3.3, 29.0, 129.0, 325.6666666666667, 26.0, 25.76516896993916, 0.3884350068559966, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.554016620498615, 0.29, 0.43, 0.3598526703499079, 0.6666666666666666, 0.6470974141615967, 0.6294783356186655, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9329197], dtype=float32), 2.292665]. 
=============================================
[2019-04-04 16:08:20,452] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1995348e-08 1.3073164e-08 1.8678916e-13 2.3170955e-12 1.0000000e+00
 1.4959982e-08 2.4219849e-13], sum to 1.0000
[2019-04-04 16:08:20,452] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4744
[2019-04-04 16:08:20,507] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.95026502417611, 0.3355338535142305, 0.0, 1.0, 76772.56585334317], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2577600.0000, 
sim time next is 2578200.0000, 
raw observation next is [-1.883333333333333, 46.0, 0.0, 0.0, 26.0, 24.95971637046695, 0.3438015891527702, 0.0, 1.0, 54248.03499981008], 
processed observation next is [1.0, 0.8695652173913043, 0.4104339796860573, 0.46, 0.0, 0.0, 0.6666666666666666, 0.5799763642055792, 0.61460052971759, 0.0, 1.0, 0.2583239761895718], 
reward next is 0.7417, 
noisyNet noise sample is [array([1.8683699], dtype=float32), 0.2491989]. 
=============================================
[2019-04-04 16:08:22,439] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7650311e-08 1.5055495e-09 2.5603014e-14 1.0326639e-13 1.0000000e+00
 1.1438160e-09 1.2054840e-13], sum to 1.0000
[2019-04-04 16:08:22,439] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3980
[2019-04-04 16:08:22,502] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.01212543823027, 0.3165279943920702, 0.0, 1.0, 31281.85961883276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3006000.0000, 
sim time next is 3006600.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 0.0, 0.0, 26.0, 25.01223570045791, 0.313162568886139, 0.0, 1.0, 36502.04375997736], 
processed observation next is [0.0, 0.8260869565217391, 0.4025854108956602, 0.6083333333333333, 0.0, 0.0, 0.6666666666666666, 0.5843529750381592, 0.6043875229620463, 0.0, 1.0, 0.17381925599989217], 
reward next is 0.8262, 
noisyNet noise sample is [array([2.2428052], dtype=float32), -0.14447883]. 
=============================================
[2019-04-04 16:08:25,032] A3C_AGENT_WORKER-Thread-15 INFO:Local step 259500, global step 4151513: loss 0.0075
[2019-04-04 16:08:25,033] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 259500, global step 4151513: learning rate 0.0000
[2019-04-04 16:08:25,594] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6653916e-09 3.7604689e-10 9.3179059e-14 1.2849846e-13 1.0000000e+00
 1.0635722e-09 7.2164293e-15], sum to 1.0000
[2019-04-04 16:08:25,600] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3467
[2019-04-04 16:08:25,650] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 62.5, 0.0, 0.0, 26.0, 24.9714935240008, 0.3584046472876619, 0.0, 1.0, 81202.30901040214], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2663400.0000, 
sim time next is 2664000.0000, 
raw observation next is [-1.2, 63.0, 0.0, 0.0, 26.0, 24.95541146598098, 0.372744739425137, 0.0, 1.0, 64368.91402166285], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5796176221650816, 0.6242482464750457, 0.0, 1.0, 0.30651863819839453], 
reward next is 0.6935, 
noisyNet noise sample is [array([-0.2582202], dtype=float32), -0.061586548]. 
=============================================
[2019-04-04 16:08:25,666] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[79.12734 ]
 [79.869865]
 [78.79452 ]
 [80.15803 ]
 [81.82473 ]], R is [[78.19321442]
 [78.0246048 ]
 [77.82945251]
 [77.92887878]
 [78.14958954]].
[2019-04-04 16:08:26,019] A3C_AGENT_WORKER-Thread-2 INFO:Local step 260000, global step 4151914: loss 0.0457
[2019-04-04 16:08:26,021] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 260000, global step 4151914: learning rate 0.0000
[2019-04-04 16:08:26,187] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9893064e-08 6.7464643e-09 1.1828421e-13 1.1743632e-12 9.9999988e-01
 8.6568512e-09 4.0406946e-13], sum to 1.0000
[2019-04-04 16:08:26,187] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8176
[2019-04-04 16:08:26,198] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.95, 87.0, 0.0, 0.0, 26.0, 23.69552951623065, 0.02579448694962996, 0.0, 1.0, 44524.82658922286], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2691000.0000, 
sim time next is 2691600.0000, 
raw observation next is [-14.3, 88.33333333333334, 0.0, 0.0, 26.0, 23.60193751766129, 0.01515930510516033, 0.0, 1.0, 44530.642679713], 
processed observation next is [1.0, 0.13043478260869565, 0.06648199445983377, 0.8833333333333334, 0.0, 0.0, 0.6666666666666666, 0.46682812647177424, 0.5050531017017201, 0.0, 1.0, 0.21205067942720474], 
reward next is 0.7879, 
noisyNet noise sample is [array([2.5541997], dtype=float32), -1.3409595]. 
=============================================
[2019-04-04 16:08:26,631] A3C_AGENT_WORKER-Thread-13 INFO:Local step 260000, global step 4152182: loss 0.0461
[2019-04-04 16:08:26,633] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 260000, global step 4152183: learning rate 0.0000
[2019-04-04 16:08:28,537] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.9208249e-09 3.4354786e-10 1.7808643e-15 1.3117559e-14 1.0000000e+00
 6.1173749e-10 7.8746721e-16], sum to 1.0000
[2019-04-04 16:08:28,550] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8540
[2019-04-04 16:08:28,594] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.166666666666666, 64.0, 112.3333333333333, 787.0, 26.0, 25.98687942932154, 0.471455138247023, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2721000.0000, 
sim time next is 2721600.0000, 
raw observation next is [-8.0, 64.0, 112.5, 790.0, 26.0, 25.94382274499809, 0.4710879600432011, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.24099722991689754, 0.64, 0.375, 0.8729281767955801, 0.6666666666666666, 0.6619852287498409, 0.6570293200144004, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3531591], dtype=float32), -0.68531924]. 
=============================================
[2019-04-04 16:08:28,598] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0766213e-08 4.0874934e-08 6.9751984e-13 6.8538196e-12 9.9999988e-01
 3.6650537e-08 2.5285600e-12], sum to 1.0000
[2019-04-04 16:08:28,598] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8948
[2019-04-04 16:08:28,616] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.78028221837407, -0.1963134623918677, 0.0, 1.0, 43241.98676371115], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2703600.0000, 
sim time next is 2704200.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.69845057978952, -0.2064646891524361, 0.0, 1.0, 43299.03822097232], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.6666666666666666, 0.39153754831579324, 0.431178436949188, 0.0, 1.0, 0.20618589629034437], 
reward next is 0.7938, 
noisyNet noise sample is [array([1.3426856], dtype=float32), -1.3425266]. 
=============================================
[2019-04-04 16:08:29,089] A3C_AGENT_WORKER-Thread-4 INFO:Local step 259500, global step 4153240: loss 0.0074
[2019-04-04 16:08:29,092] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 259500, global step 4153242: learning rate 0.0000
[2019-04-04 16:08:29,407] A3C_AGENT_WORKER-Thread-11 INFO:Local step 260000, global step 4153366: loss 0.0471
[2019-04-04 16:08:29,407] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 260000, global step 4153367: learning rate 0.0000
[2019-04-04 16:08:30,892] A3C_AGENT_WORKER-Thread-17 INFO:Local step 260000, global step 4153964: loss 0.0462
[2019-04-04 16:08:30,892] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 260000, global step 4153964: learning rate 0.0000
[2019-04-04 16:08:31,056] A3C_AGENT_WORKER-Thread-6 INFO:Local step 259500, global step 4154038: loss 0.0077
[2019-04-04 16:08:31,058] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 259500, global step 4154038: learning rate 0.0000
[2019-04-04 16:08:31,630] A3C_AGENT_WORKER-Thread-12 INFO:Local step 259500, global step 4154325: loss 0.0080
[2019-04-04 16:08:31,631] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 259500, global step 4154325: learning rate 0.0000
[2019-04-04 16:08:31,895] A3C_AGENT_WORKER-Thread-16 INFO:Local step 259500, global step 4154453: loss 0.0073
[2019-04-04 16:08:31,896] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 259500, global step 4154453: learning rate 0.0000
[2019-04-04 16:08:32,322] A3C_AGENT_WORKER-Thread-10 INFO:Local step 259500, global step 4154666: loss 0.0067
[2019-04-04 16:08:32,324] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 259500, global step 4154671: learning rate 0.0000
[2019-04-04 16:08:32,458] A3C_AGENT_WORKER-Thread-19 INFO:Local step 259500, global step 4154744: loss 0.0075
[2019-04-04 16:08:32,459] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 259500, global step 4154744: learning rate 0.0000
[2019-04-04 16:08:32,902] A3C_AGENT_WORKER-Thread-20 INFO:Local step 259500, global step 4154958: loss 0.0074
[2019-04-04 16:08:32,905] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 259500, global step 4154958: learning rate 0.0000
[2019-04-04 16:08:33,485] A3C_AGENT_WORKER-Thread-3 INFO:Local step 259500, global step 4155223: loss 0.0069
[2019-04-04 16:08:33,486] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 259500, global step 4155223: learning rate 0.0000
[2019-04-04 16:08:33,844] A3C_AGENT_WORKER-Thread-5 INFO:Local step 259500, global step 4155391: loss 0.0071
[2019-04-04 16:08:33,845] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 259500, global step 4155391: learning rate 0.0000
[2019-04-04 16:08:34,844] A3C_AGENT_WORKER-Thread-14 INFO:Local step 259500, global step 4155867: loss 0.0066
[2019-04-04 16:08:34,846] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 259500, global step 4155869: learning rate 0.0000
[2019-04-04 16:08:34,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.0836775e-10 3.6311208e-11 3.2511724e-16 1.6252792e-15 1.0000000e+00
 4.3384123e-11 2.8940249e-16], sum to 1.0000
[2019-04-04 16:08:34,854] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4334
[2019-04-04 16:08:34,867] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.166666666666667, 72.0, 67.66666666666666, 569.3333333333333, 26.0, 26.96690688026437, 0.8385472700211835, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3255000.0000, 
sim time next is 3255600.0000, 
raw observation next is [-3.333333333333333, 73.0, 63.33333333333334, 540.1666666666667, 26.0, 27.08710808171668, 0.5242205858470527, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.37026777469990774, 0.73, 0.21111111111111114, 0.5968692449355434, 0.6666666666666666, 0.7572590068097232, 0.674740195282351, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3867984], dtype=float32), 0.346088]. 
=============================================
[2019-04-04 16:08:35,197] A3C_AGENT_WORKER-Thread-18 INFO:Local step 259500, global step 4156060: loss 0.0065
[2019-04-04 16:08:35,200] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 259500, global step 4156062: learning rate 0.0000
[2019-04-04 16:08:36,411] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.3783288e-08 7.9382030e-09 3.4869114e-14 4.4866200e-13 1.0000000e+00
 2.6294260e-09 3.9330372e-14], sum to 1.0000
[2019-04-04 16:08:36,414] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4329
[2019-04-04 16:08:36,425] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.44010117749938, 0.4359041879690144, 0.0, 1.0, 18761.67463247466], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2842200.0000, 
sim time next is 2842800.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.47783603258707, 0.4338033655427676, 0.0, 1.0, 18757.30193030979], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.6231530027155893, 0.6446011218475892, 0.0, 1.0, 0.08932048538242757], 
reward next is 0.9107, 
noisyNet noise sample is [array([1.8320527], dtype=float32), 0.8426517]. 
=============================================
[2019-04-04 16:08:41,231] A3C_AGENT_WORKER-Thread-2 INFO:Local step 260500, global step 4158914: loss 0.0253
[2019-04-04 16:08:41,233] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 260500, global step 4158915: learning rate 0.0000
[2019-04-04 16:08:41,469] A3C_AGENT_WORKER-Thread-13 INFO:Local step 260500, global step 4159025: loss 0.0278
[2019-04-04 16:08:41,470] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 260500, global step 4159026: learning rate 0.0000
[2019-04-04 16:08:42,930] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.36660647e-09 1.74076109e-09 8.06083394e-15 3.39672985e-14
 1.00000000e+00 1.09919324e-10 6.95086206e-15], sum to 1.0000
[2019-04-04 16:08:42,931] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0009
[2019-04-04 16:08:42,953] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.833333333333333, 84.16666666666666, 0.0, 0.0, 26.0, 24.66953346760217, 0.2749367843015045, 0.0, 1.0, 42836.4864539621], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2947800.0000, 
sim time next is 2948400.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.64372066448442, 0.2656114594595926, 0.0, 1.0, 42759.22353337883], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.6666666666666666, 0.553643388707035, 0.5885371531531975, 0.0, 1.0, 0.2036153501589468], 
reward next is 0.7964, 
noisyNet noise sample is [array([0.36951017], dtype=float32), -0.27339023]. 
=============================================
[2019-04-04 16:08:43,800] A3C_AGENT_WORKER-Thread-15 INFO:Local step 260000, global step 4160049: loss 0.0498
[2019-04-04 16:08:43,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 260000, global step 4160049: learning rate 0.0000
[2019-04-04 16:08:44,233] A3C_AGENT_WORKER-Thread-11 INFO:Local step 260500, global step 4160252: loss 0.0289
[2019-04-04 16:08:44,234] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 260500, global step 4160253: learning rate 0.0000
[2019-04-04 16:08:45,768] A3C_AGENT_WORKER-Thread-17 INFO:Local step 260500, global step 4160920: loss 0.0316
[2019-04-04 16:08:45,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 260500, global step 4160920: learning rate 0.0000
[2019-04-04 16:08:47,403] A3C_AGENT_WORKER-Thread-4 INFO:Local step 260000, global step 4161639: loss 0.0537
[2019-04-04 16:08:47,404] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 260000, global step 4161639: learning rate 0.0000
[2019-04-04 16:08:47,753] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.6630499e-10 8.0654955e-11 4.9133904e-16 2.9561875e-15 1.0000000e+00
 5.8236811e-11 3.8620409e-16], sum to 1.0000
[2019-04-04 16:08:47,754] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5075
[2019-04-04 16:08:47,764] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.166666666666667, 60.33333333333334, 113.0, 796.6666666666667, 26.0, 26.16044079276675, 0.6013058261387765, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3496200.0000, 
sim time next is 3496800.0000, 
raw observation next is [1.333333333333333, 59.66666666666667, 114.0, 803.3333333333333, 26.0, 26.23691739661443, 0.6117170861154113, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4995383194829178, 0.5966666666666667, 0.38, 0.8876611418047882, 0.6666666666666666, 0.6864097830512025, 0.7039056953718038, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1907904], dtype=float32), 0.25112134]. 
=============================================
[2019-04-04 16:08:48,371] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7908706e-09 7.1133641e-11 2.2853746e-16 9.7061014e-15 1.0000000e+00
 1.4386201e-10 3.3791001e-15], sum to 1.0000
[2019-04-04 16:08:48,371] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4302
[2019-04-04 16:08:48,389] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.32017394606766, 0.4705100514240397, 0.0, 1.0, 57577.32838748945], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3451800.0000, 
sim time next is 3452400.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.35414748331831, 0.4752441141724841, 0.0, 1.0, 47020.69152333237], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6128456236098593, 0.6584147047241614, 0.0, 1.0, 0.22390805487301127], 
reward next is 0.7761, 
noisyNet noise sample is [array([-2.3387291], dtype=float32), -0.09115577]. 
=============================================
[2019-04-04 16:08:48,406] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7859670e-08 2.9045975e-09 1.3007139e-14 5.2394910e-13 1.0000000e+00
 4.6402832e-10 6.6960922e-14], sum to 1.0000
[2019-04-04 16:08:48,407] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6030
[2019-04-04 16:08:48,417] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.21406333171262, 0.1046082741597717, 0.0, 1.0, 39491.52241072408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3035400.0000, 
sim time next is 3036000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.2113661826549, 0.09871443371297679, 0.0, 1.0, 39621.6907900564], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5176138485545749, 0.532904811237659, 0.0, 1.0, 0.1886747180478876], 
reward next is 0.8113, 
noisyNet noise sample is [array([1.6557542], dtype=float32), -2.1870012]. 
=============================================
[2019-04-04 16:08:48,428] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.647934]
 [77.64384 ]
 [77.644005]
 [77.74643 ]
 [77.758156]], R is [[77.6211319 ]
 [77.65686798]
 [77.69296265]
 [77.72941589]
 [77.76634216]].
[2019-04-04 16:08:48,674] A3C_AGENT_WORKER-Thread-6 INFO:Local step 260000, global step 4162299: loss 0.0510
[2019-04-04 16:08:48,677] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 260000, global step 4162300: learning rate 0.0000
[2019-04-04 16:08:49,811] A3C_AGENT_WORKER-Thread-12 INFO:Local step 260000, global step 4162888: loss 0.0512
[2019-04-04 16:08:49,814] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 260000, global step 4162888: learning rate 0.0000
[2019-04-04 16:08:50,081] A3C_AGENT_WORKER-Thread-16 INFO:Local step 260000, global step 4163004: loss 0.0514
[2019-04-04 16:08:50,081] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 260000, global step 4163004: learning rate 0.0000
[2019-04-04 16:08:50,276] A3C_AGENT_WORKER-Thread-10 INFO:Local step 260000, global step 4163107: loss 0.0516
[2019-04-04 16:08:50,277] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 260000, global step 4163107: learning rate 0.0000
[2019-04-04 16:08:51,004] A3C_AGENT_WORKER-Thread-19 INFO:Local step 260000, global step 4163454: loss 0.0523
[2019-04-04 16:08:51,006] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 260000, global step 4163454: learning rate 0.0000
[2019-04-04 16:08:51,603] A3C_AGENT_WORKER-Thread-20 INFO:Local step 260000, global step 4163720: loss 0.0488
[2019-04-04 16:08:51,605] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 260000, global step 4163721: learning rate 0.0000
[2019-04-04 16:08:51,605] A3C_AGENT_WORKER-Thread-3 INFO:Local step 260000, global step 4163722: loss 0.0521
[2019-04-04 16:08:51,611] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 260000, global step 4163724: learning rate 0.0000
[2019-04-04 16:08:52,127] A3C_AGENT_WORKER-Thread-5 INFO:Local step 260000, global step 4163978: loss 0.0532
[2019-04-04 16:08:52,128] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 260000, global step 4163979: learning rate 0.0000
[2019-04-04 16:08:53,056] A3C_AGENT_WORKER-Thread-14 INFO:Local step 260000, global step 4164485: loss 0.0499
[2019-04-04 16:08:53,057] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 260000, global step 4164486: learning rate 0.0000
[2019-04-04 16:08:53,542] A3C_AGENT_WORKER-Thread-18 INFO:Local step 260000, global step 4164757: loss 0.0526
[2019-04-04 16:08:53,542] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 260000, global step 4164757: learning rate 0.0000
[2019-04-04 16:08:55,599] A3C_AGENT_WORKER-Thread-2 INFO:Local step 261000, global step 4166000: loss 0.0021
[2019-04-04 16:08:55,600] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 261000, global step 4166000: learning rate 0.0000
[2019-04-04 16:08:55,625] A3C_AGENT_WORKER-Thread-13 INFO:Local step 261000, global step 4166013: loss 0.0022
[2019-04-04 16:08:55,626] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 261000, global step 4166013: learning rate 0.0000
[2019-04-04 16:08:58,517] A3C_AGENT_WORKER-Thread-15 INFO:Local step 260500, global step 4167727: loss 0.0362
[2019-04-04 16:08:58,518] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 260500, global step 4167727: learning rate 0.0000
[2019-04-04 16:08:58,890] A3C_AGENT_WORKER-Thread-11 INFO:Local step 261000, global step 4167904: loss 0.0033
[2019-04-04 16:08:58,891] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 261000, global step 4167905: learning rate 0.0000
[2019-04-04 16:09:00,327] A3C_AGENT_WORKER-Thread-17 INFO:Local step 261000, global step 4168665: loss 0.0035
[2019-04-04 16:09:00,328] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 261000, global step 4168665: learning rate 0.0000
[2019-04-04 16:09:02,028] A3C_AGENT_WORKER-Thread-4 INFO:Local step 260500, global step 4169516: loss 0.0416
[2019-04-04 16:09:02,030] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 260500, global step 4169517: learning rate 0.0000
[2019-04-04 16:09:02,911] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3807039e-08 2.0623991e-10 9.1029902e-15 8.5133574e-14 1.0000000e+00
 5.4110383e-10 1.0900797e-14], sum to 1.0000
[2019-04-04 16:09:02,911] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1802
[2019-04-04 16:09:02,933] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.10998559131959, 0.2868346757047724, 0.0, 1.0, 41559.91926817975], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3735000.0000, 
sim time next is 3735600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.12308212306839, 0.2812522794213915, 0.0, 1.0, 41589.25816835256], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5935901769223658, 0.5937507598071305, 0.0, 1.0, 0.19804408651596456], 
reward next is 0.8020, 
noisyNet noise sample is [array([-1.5536356], dtype=float32), -0.22248812]. 
=============================================
[2019-04-04 16:09:03,854] A3C_AGENT_WORKER-Thread-6 INFO:Local step 260500, global step 4170301: loss 0.0437
[2019-04-04 16:09:03,856] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 260500, global step 4170302: learning rate 0.0000
[2019-04-04 16:09:04,793] A3C_AGENT_WORKER-Thread-12 INFO:Local step 260500, global step 4170779: loss 0.0421
[2019-04-04 16:09:04,795] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 260500, global step 4170780: learning rate 0.0000
[2019-04-04 16:09:05,514] A3C_AGENT_WORKER-Thread-16 INFO:Local step 260500, global step 4171145: loss 0.0410
[2019-04-04 16:09:05,516] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 260500, global step 4171146: learning rate 0.0000
[2019-04-04 16:09:05,800] A3C_AGENT_WORKER-Thread-10 INFO:Local step 260500, global step 4171282: loss 0.0421
[2019-04-04 16:09:05,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 260500, global step 4171286: learning rate 0.0000
[2019-04-04 16:09:06,350] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5933321e-08 2.7868849e-09 3.1361873e-14 1.4598994e-13 1.0000000e+00
 8.7644564e-10 1.4506168e-14], sum to 1.0000
[2019-04-04 16:09:06,351] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2452
[2019-04-04 16:09:06,378] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.666666666666666, 75.0, 0.0, 0.0, 26.0, 24.81757119343762, 0.2608578902756526, 0.0, 1.0, 41678.88882150688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3385200.0000, 
sim time next is 3385800.0000, 
raw observation next is [-5.5, 74.0, 0.0, 0.0, 26.0, 24.83559818590935, 0.2561221339111996, 0.0, 1.0, 41826.83753521829], 
processed observation next is [1.0, 0.17391304347826086, 0.3102493074792244, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5696331821591126, 0.5853740446370665, 0.0, 1.0, 0.19917541683437281], 
reward next is 0.8008, 
noisyNet noise sample is [array([-0.8483759], dtype=float32), 0.98023546]. 
=============================================
[2019-04-04 16:09:06,518] A3C_AGENT_WORKER-Thread-3 INFO:Local step 260500, global step 4171645: loss 0.0394
[2019-04-04 16:09:06,520] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 260500, global step 4171645: learning rate 0.0000
[2019-04-04 16:09:06,663] A3C_AGENT_WORKER-Thread-19 INFO:Local step 260500, global step 4171717: loss 0.0396
[2019-04-04 16:09:06,664] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 260500, global step 4171717: learning rate 0.0000
[2019-04-04 16:09:06,804] A3C_AGENT_WORKER-Thread-20 INFO:Local step 260500, global step 4171790: loss 0.0391
[2019-04-04 16:09:06,806] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 260500, global step 4171791: learning rate 0.0000
[2019-04-04 16:09:07,360] A3C_AGENT_WORKER-Thread-5 INFO:Local step 260500, global step 4172067: loss 0.0373
[2019-04-04 16:09:07,364] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 260500, global step 4172069: learning rate 0.0000
[2019-04-04 16:09:08,450] A3C_AGENT_WORKER-Thread-14 INFO:Local step 260500, global step 4172590: loss 0.0368
[2019-04-04 16:09:08,451] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 260500, global step 4172590: learning rate 0.0000
[2019-04-04 16:09:09,010] A3C_AGENT_WORKER-Thread-18 INFO:Local step 260500, global step 4172902: loss 0.0353
[2019-04-04 16:09:09,017] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 260500, global step 4172906: learning rate 0.0000
[2019-04-04 16:09:09,591] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.6634041e-09 2.1111839e-09 1.7439004e-14 2.0942068e-13 1.0000000e+00
 4.4494877e-10 2.1124714e-14], sum to 1.0000
[2019-04-04 16:09:09,593] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6146
[2019-04-04 16:09:09,619] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.333333333333333, 73.0, 0.0, 0.0, 26.0, 24.81343564491192, 0.2443408097062853, 0.0, 1.0, 41986.64931718499], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3386400.0000, 
sim time next is 3387000.0000, 
raw observation next is [-5.166666666666667, 72.0, 0.0, 0.0, 26.0, 24.75290932201118, 0.2399870660142025, 0.0, 1.0, 42152.54706159546], 
processed observation next is [1.0, 0.17391304347826086, 0.31948291782086796, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5627424435009317, 0.5799956886714008, 0.0, 1.0, 0.200726414579026], 
reward next is 0.7993, 
noisyNet noise sample is [array([-1.7351592], dtype=float32), 1.105764]. 
=============================================
[2019-04-04 16:09:09,626] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.955696]
 [77.01863 ]
 [77.078354]
 [77.14749 ]
 [77.20029 ]], R is [[76.91415405]
 [76.94507599]
 [76.97645569]
 [77.00822449]
 [77.04039001]].
[2019-04-04 16:09:10,632] A3C_AGENT_WORKER-Thread-2 INFO:Local step 261500, global step 4173770: loss 0.0122
[2019-04-04 16:09:10,635] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 261500, global step 4173771: learning rate 0.0000
[2019-04-04 16:09:10,655] A3C_AGENT_WORKER-Thread-13 INFO:Local step 261500, global step 4173781: loss 0.0123
[2019-04-04 16:09:10,656] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 261500, global step 4173781: learning rate 0.0000
[2019-04-04 16:09:13,743] A3C_AGENT_WORKER-Thread-15 INFO:Local step 261000, global step 4175379: loss 0.0032
[2019-04-04 16:09:13,744] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 261000, global step 4175379: learning rate 0.0000
[2019-04-04 16:09:14,085] A3C_AGENT_WORKER-Thread-11 INFO:Local step 261500, global step 4175587: loss 0.0104
[2019-04-04 16:09:14,085] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 261500, global step 4175587: learning rate 0.0000
[2019-04-04 16:09:14,448] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8264560e-09 6.2364947e-10 4.6591352e-15 4.7281050e-14 1.0000000e+00
 1.0595041e-10 5.0813257e-15], sum to 1.0000
[2019-04-04 16:09:14,448] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9859
[2019-04-04 16:09:14,461] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.333333333333333, 58.66666666666667, 0.0, 0.0, 26.0, 25.32664909587156, 0.4761261504333592, 0.0, 1.0, 30000.68699572867], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3525600.0000, 
sim time next is 3526200.0000, 
raw observation next is [1.0, 62.0, 0.0, 0.0, 26.0, 25.2822383910371, 0.4604585426575825, 0.0, 1.0, 29648.74710968238], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6068531992530918, 0.6534861808858609, 0.0, 1.0, 0.14118451004610658], 
reward next is 0.8588, 
noisyNet noise sample is [array([2.1193256], dtype=float32), 0.60147476]. 
=============================================
[2019-04-04 16:09:15,379] A3C_AGENT_WORKER-Thread-17 INFO:Local step 261500, global step 4176291: loss 0.0104
[2019-04-04 16:09:15,380] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 261500, global step 4176291: learning rate 0.0000
[2019-04-04 16:09:16,552] A3C_AGENT_WORKER-Thread-4 INFO:Local step 261000, global step 4176946: loss 0.0038
[2019-04-04 16:09:16,574] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 261000, global step 4176946: learning rate 0.0000
[2019-04-04 16:09:19,142] A3C_AGENT_WORKER-Thread-6 INFO:Local step 261000, global step 4178191: loss 0.0017
[2019-04-04 16:09:19,142] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 261000, global step 4178191: learning rate 0.0000
[2019-04-04 16:09:19,438] A3C_AGENT_WORKER-Thread-12 INFO:Local step 261000, global step 4178372: loss 0.0026
[2019-04-04 16:09:19,439] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 261000, global step 4178373: learning rate 0.0000
[2019-04-04 16:09:20,253] A3C_AGENT_WORKER-Thread-10 INFO:Local step 261000, global step 4178843: loss 0.0020
[2019-04-04 16:09:20,255] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 261000, global step 4178843: learning rate 0.0000
[2019-04-04 16:09:20,610] A3C_AGENT_WORKER-Thread-16 INFO:Local step 261000, global step 4179065: loss 0.0014
[2019-04-04 16:09:20,614] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 261000, global step 4179067: learning rate 0.0000
[2019-04-04 16:09:21,552] A3C_AGENT_WORKER-Thread-19 INFO:Local step 261000, global step 4179607: loss 0.0013
[2019-04-04 16:09:21,553] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 261000, global step 4179607: learning rate 0.0000
[2019-04-04 16:09:21,720] A3C_AGENT_WORKER-Thread-3 INFO:Local step 261000, global step 4179699: loss 0.0015
[2019-04-04 16:09:21,721] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 261000, global step 4179699: learning rate 0.0000
[2019-04-04 16:09:21,725] A3C_AGENT_WORKER-Thread-20 INFO:Local step 261000, global step 4179704: loss 0.0014
[2019-04-04 16:09:21,731] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 261000, global step 4179704: learning rate 0.0000
[2019-04-04 16:09:22,088] A3C_AGENT_WORKER-Thread-5 INFO:Local step 261000, global step 4179888: loss 0.0015
[2019-04-04 16:09:22,088] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 261000, global step 4179888: learning rate 0.0000
[2019-04-04 16:09:23,387] A3C_AGENT_WORKER-Thread-14 INFO:Local step 261000, global step 4180644: loss 0.0011
[2019-04-04 16:09:23,387] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 261000, global step 4180644: learning rate 0.0000
[2019-04-04 16:09:23,901] A3C_AGENT_WORKER-Thread-18 INFO:Local step 261000, global step 4180954: loss 0.0012
[2019-04-04 16:09:23,901] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 261000, global step 4180954: learning rate 0.0000
[2019-04-04 16:09:26,435] A3C_AGENT_WORKER-Thread-2 INFO:Local step 262000, global step 4182332: loss 2.1800
[2019-04-04 16:09:26,438] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 262000, global step 4182332: learning rate 0.0000
[2019-04-04 16:09:26,485] A3C_AGENT_WORKER-Thread-13 INFO:Local step 262000, global step 4182354: loss 2.1683
[2019-04-04 16:09:26,488] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 262000, global step 4182356: learning rate 0.0000
[2019-04-04 16:09:27,649] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7120945e-08 1.4133323e-09 1.7846542e-13 5.5244714e-13 1.0000000e+00
 6.3329808e-09 2.5614946e-13], sum to 1.0000
[2019-04-04 16:09:27,651] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2596
[2019-04-04 16:09:27,691] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.78718439280524, 0.5751314969369984, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3952800.0000, 
sim time next is 3953400.0000, 
raw observation next is [-6.0, 41.00000000000001, 0.0, 0.0, 26.0, 25.80547195727349, 0.5799614583563187, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.296398891966759, 0.4100000000000001, 0.0, 0.0, 0.6666666666666666, 0.6504559964394575, 0.6933204861187728, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0516038], dtype=float32), -0.6610689]. 
=============================================
[2019-04-04 16:09:28,342] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3205489e-08 2.1634226e-09 5.8103912e-14 1.1992078e-12 1.0000000e+00
 8.4856533e-10 2.5222394e-13], sum to 1.0000
[2019-04-04 16:09:28,346] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8385
[2019-04-04 16:09:28,395] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.59845030919902, 0.5348337616249458, 0.0, 1.0, 40290.96860002138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3960600.0000, 
sim time next is 3961200.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.58385122735627, 0.5287850970543447, 0.0, 1.0, 51395.37053861781], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6319876022796892, 0.6762616990181148, 0.0, 1.0, 0.24473985970770387], 
reward next is 0.7553, 
noisyNet noise sample is [array([0.24637643], dtype=float32), -1.1293182]. 
=============================================
[2019-04-04 16:09:28,972] A3C_AGENT_WORKER-Thread-15 INFO:Local step 261500, global step 4183686: loss 0.0138
[2019-04-04 16:09:28,973] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 261500, global step 4183686: learning rate 0.0000
[2019-04-04 16:09:30,098] A3C_AGENT_WORKER-Thread-11 INFO:Local step 262000, global step 4184221: loss 2.2007
[2019-04-04 16:09:30,099] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 262000, global step 4184221: learning rate 0.0000
[2019-04-04 16:09:31,062] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.2885980e-10 8.8784438e-11 1.6906936e-15 1.2239136e-14 1.0000000e+00
 6.2895425e-11 2.7645243e-15], sum to 1.0000
[2019-04-04 16:09:31,063] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7850
[2019-04-04 16:09:31,082] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 69.0, 0.0, 0.0, 26.0, 24.97062439121923, 0.2886724327862144, 0.0, 1.0, 56787.51264856807], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4302000.0000, 
sim time next is 4302600.0000, 
raw observation next is [5.9, 69.66666666666667, 0.0, 0.0, 26.0, 24.92816442303795, 0.287298939287108, 0.0, 1.0, 56698.43934784835], 
processed observation next is [0.0, 0.8260869565217391, 0.626038781163435, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.5773470352531623, 0.5957663130957026, 0.0, 1.0, 0.2699925683230874], 
reward next is 0.7300, 
noisyNet noise sample is [array([-0.45607516], dtype=float32), 1.1402915]. 
=============================================
[2019-04-04 16:09:31,605] A3C_AGENT_WORKER-Thread-17 INFO:Local step 262000, global step 4185012: loss 2.1865
[2019-04-04 16:09:31,606] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 262000, global step 4185013: learning rate 0.0000
[2019-04-04 16:09:32,296] A3C_AGENT_WORKER-Thread-4 INFO:Local step 261500, global step 4185384: loss 0.0141
[2019-04-04 16:09:32,299] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 261500, global step 4185386: learning rate 0.0000
[2019-04-04 16:09:34,653] A3C_AGENT_WORKER-Thread-6 INFO:Local step 261500, global step 4186582: loss 0.0136
[2019-04-04 16:09:34,654] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 261500, global step 4186583: learning rate 0.0000
[2019-04-04 16:09:34,935] A3C_AGENT_WORKER-Thread-12 INFO:Local step 261500, global step 4186711: loss 0.0130
[2019-04-04 16:09:34,937] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 261500, global step 4186711: learning rate 0.0000
[2019-04-04 16:09:35,012] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2034809e-09 2.0879404e-10 2.6553749e-14 2.4852618e-14 1.0000000e+00
 6.6774847e-10 1.8735486e-15], sum to 1.0000
[2019-04-04 16:09:35,012] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3523
[2019-04-04 16:09:35,063] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.833333333333333, 43.83333333333334, 117.0, 827.6666666666666, 26.0, 24.98641452462878, 0.5612866809265894, 1.0, 1.0, 197607.0417151776], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3935400.0000, 
sim time next is 3936000.0000, 
raw observation next is [-5.666666666666666, 42.66666666666667, 116.5, 825.8333333333334, 26.0, 25.72538801491692, 0.6518016156106955, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3056325023084026, 0.4266666666666667, 0.3883333333333333, 0.912523020257827, 0.6666666666666666, 0.64378233457641, 0.7172672052035652, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42183065], dtype=float32), 0.6742515]. 
=============================================
[2019-04-04 16:09:35,071] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.19054 ]
 [82.26545 ]
 [81.30238 ]
 [81.411095]
 [81.446014]], R is [[83.1984787 ]
 [82.42550659]
 [81.66792297]
 [81.85124207]
 [82.0327301 ]].
[2019-04-04 16:09:35,491] A3C_AGENT_WORKER-Thread-10 INFO:Local step 261500, global step 4186989: loss 0.0133
[2019-04-04 16:09:35,492] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 261500, global step 4186990: learning rate 0.0000
[2019-04-04 16:09:35,815] A3C_AGENT_WORKER-Thread-16 INFO:Local step 261500, global step 4187156: loss 0.0133
[2019-04-04 16:09:35,815] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 261500, global step 4187156: learning rate 0.0000
[2019-04-04 16:09:36,837] A3C_AGENT_WORKER-Thread-20 INFO:Local step 261500, global step 4187684: loss 0.0114
[2019-04-04 16:09:36,838] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 261500, global step 4187685: learning rate 0.0000
[2019-04-04 16:09:36,937] A3C_AGENT_WORKER-Thread-3 INFO:Local step 261500, global step 4187736: loss 0.0125
[2019-04-04 16:09:36,938] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 261500, global step 4187736: learning rate 0.0000
[2019-04-04 16:09:37,183] A3C_AGENT_WORKER-Thread-19 INFO:Local step 261500, global step 4187866: loss 0.0121
[2019-04-04 16:09:37,185] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 261500, global step 4187866: learning rate 0.0000
[2019-04-04 16:09:37,643] A3C_AGENT_WORKER-Thread-5 INFO:Local step 261500, global step 4188091: loss 0.0128
[2019-04-04 16:09:37,648] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 261500, global step 4188091: learning rate 0.0000
[2019-04-04 16:09:38,857] A3C_AGENT_WORKER-Thread-14 INFO:Local step 261500, global step 4188681: loss 0.0132
[2019-04-04 16:09:38,858] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 261500, global step 4188682: learning rate 0.0000
[2019-04-04 16:09:38,999] A3C_AGENT_WORKER-Thread-18 INFO:Local step 261500, global step 4188745: loss 0.0132
[2019-04-04 16:09:39,001] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 261500, global step 4188745: learning rate 0.0000
[2019-04-04 16:09:39,983] A3C_AGENT_WORKER-Thread-2 INFO:Local step 262500, global step 4189222: loss 0.1129
[2019-04-04 16:09:39,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 262500, global step 4189224: learning rate 0.0000
[2019-04-04 16:09:39,996] A3C_AGENT_WORKER-Thread-13 INFO:Local step 262500, global step 4189234: loss 0.1219
[2019-04-04 16:09:39,998] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 262500, global step 4189234: learning rate 0.0000
[2019-04-04 16:09:43,318] A3C_AGENT_WORKER-Thread-11 INFO:Local step 262500, global step 4190996: loss 0.1972
[2019-04-04 16:09:43,320] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 262500, global step 4190996: learning rate 0.0000
[2019-04-04 16:09:44,483] A3C_AGENT_WORKER-Thread-17 INFO:Local step 262500, global step 4191673: loss 0.1963
[2019-04-04 16:09:44,487] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 262500, global step 4191673: learning rate 0.0000
[2019-04-04 16:09:44,519] A3C_AGENT_WORKER-Thread-15 INFO:Local step 262000, global step 4191695: loss 2.2029
[2019-04-04 16:09:44,520] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 262000, global step 4191695: learning rate 0.0000
[2019-04-04 16:09:47,917] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.1439689e-09 3.4579111e-09 4.4867013e-15 1.4282041e-13 1.0000000e+00
 8.8086333e-10 7.3907191e-15], sum to 1.0000
[2019-04-04 16:09:47,918] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2145
[2019-04-04 16:09:47,930] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 26.0, 25.63348347897109, 0.4876803952457934, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4675200.0000, 
sim time next is 4675800.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 26.0, 25.61653559418387, 0.485542799813776, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6347112995153225, 0.6618475999379253, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6119171], dtype=float32), -0.8410567]. 
=============================================
[2019-04-04 16:09:48,173] A3C_AGENT_WORKER-Thread-4 INFO:Local step 262000, global step 4193637: loss 2.2030
[2019-04-04 16:09:48,175] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 262000, global step 4193638: learning rate 0.0000
[2019-04-04 16:09:48,816] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.6137498e-11 4.1432235e-12 4.8411606e-18 4.2091734e-17 1.0000000e+00
 2.8744355e-12 1.6233506e-17], sum to 1.0000
[2019-04-04 16:09:48,820] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7757
[2019-04-04 16:09:48,841] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.6, 33.0, 118.3333333333333, 104.8333333333333, 26.0, 28.67900076714331, 1.156723238507671, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4375200.0000, 
sim time next is 4375800.0000, 
raw observation next is [13.45, 33.5, 103.0, 0.0, 26.0, 28.81713935996287, 0.9759806240102084, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8351800554016622, 0.335, 0.3433333333333333, 0.0, 0.6666666666666666, 0.9014282799969058, 0.8253268746700694, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35020736], dtype=float32), -1.4017421]. 
=============================================
[2019-04-04 16:09:50,120] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.35362939e-08 3.32474132e-10 3.39475919e-15 5.01037031e-14
 1.00000000e+00 3.09531928e-10 1.23338585e-14], sum to 1.0000
[2019-04-04 16:09:50,126] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8926
[2019-04-04 16:09:50,158] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.066666666666666, 92.33333333333333, 20.66666666666666, 69.83333333333331, 26.0, 23.82462752765663, 0.09269742144019343, 0.0, 1.0, 41895.28683810084], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4779600.0000, 
sim time next is 4780200.0000, 
raw observation next is [-6.033333333333333, 92.16666666666667, 41.33333333333332, 139.6666666666666, 26.0, 23.80453718530156, 0.09875852160040827, 0.0, 1.0, 41792.89059023307], 
processed observation next is [0.0, 0.30434782608695654, 0.29547553093259465, 0.9216666666666667, 0.13777777777777775, 0.1543278084714548, 0.6666666666666666, 0.48371143210846324, 0.5329195072001361, 0.0, 1.0, 0.19901376471539559], 
reward next is 0.8010, 
noisyNet noise sample is [array([0.3572801], dtype=float32), 1.2646415]. 
=============================================
[2019-04-04 16:09:50,563] A3C_AGENT_WORKER-Thread-6 INFO:Local step 262000, global step 4194921: loss 2.1894
[2019-04-04 16:09:50,563] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 262000, global step 4194921: learning rate 0.0000
[2019-04-04 16:09:50,621] A3C_AGENT_WORKER-Thread-12 INFO:Local step 262000, global step 4194944: loss 2.1789
[2019-04-04 16:09:50,622] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 262000, global step 4194945: learning rate 0.0000
[2019-04-04 16:09:50,789] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2675803e-09 3.3303409e-09 3.5454719e-14 4.0692943e-14 1.0000000e+00
 2.9489078e-10 1.4708016e-14], sum to 1.0000
[2019-04-04 16:09:50,789] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3951
[2019-04-04 16:09:50,799] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 26.0, 25.44761343293107, 0.3679362901582939, 0.0, 1.0, 43844.53831833181], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4227600.0000, 
sim time next is 4228200.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 25.425181894845, 0.3692935434976026, 0.0, 1.0, 51830.97270445188], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.47, 0.0, 0.0, 0.6666666666666666, 0.61876515790375, 0.6230978478325342, 0.0, 1.0, 0.24681415573548515], 
reward next is 0.7532, 
noisyNet noise sample is [array([-0.85099924], dtype=float32), 0.43813372]. 
=============================================
[2019-04-04 16:09:51,073] A3C_AGENT_WORKER-Thread-10 INFO:Local step 262000, global step 4195185: loss 2.1800
[2019-04-04 16:09:51,073] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 262000, global step 4195185: learning rate 0.0000
[2019-04-04 16:09:51,669] A3C_AGENT_WORKER-Thread-16 INFO:Local step 262000, global step 4195507: loss 2.1697
[2019-04-04 16:09:51,672] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 262000, global step 4195507: learning rate 0.0000
[2019-04-04 16:09:52,066] A3C_AGENT_WORKER-Thread-3 INFO:Local step 262000, global step 4195742: loss 2.1714
[2019-04-04 16:09:52,070] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 262000, global step 4195744: learning rate 0.0000
[2019-04-04 16:09:52,424] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.9247266e-10 2.6614549e-10 2.7757784e-15 6.8971381e-15 1.0000000e+00
 3.6197802e-11 7.5279217e-16], sum to 1.0000
[2019-04-04 16:09:52,427] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1804
[2019-04-04 16:09:52,441] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.3278374281473, 0.3182025651051663, 0.0, 1.0, 39166.55344262797], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4260000.0000, 
sim time next is 4260600.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.32033578336089, 0.3182279726903386, 0.0, 1.0, 39151.97129737681], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6100279819467408, 0.6060759908967795, 0.0, 1.0, 0.1864379585589372], 
reward next is 0.8136, 
noisyNet noise sample is [array([-0.12102982], dtype=float32), 0.09054117]. 
=============================================
[2019-04-04 16:09:52,684] A3C_AGENT_WORKER-Thread-20 INFO:Local step 262000, global step 4196104: loss 2.1562
[2019-04-04 16:09:52,687] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 262000, global step 4196105: learning rate 0.0000
[2019-04-04 16:09:53,296] A3C_AGENT_WORKER-Thread-19 INFO:Local step 262000, global step 4196441: loss 2.1567
[2019-04-04 16:09:53,297] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 262000, global step 4196442: learning rate 0.0000
[2019-04-04 16:09:53,327] A3C_AGENT_WORKER-Thread-5 INFO:Local step 262000, global step 4196462: loss 2.1692
[2019-04-04 16:09:53,328] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 262000, global step 4196463: learning rate 0.0000
[2019-04-04 16:09:54,269] A3C_AGENT_WORKER-Thread-14 INFO:Local step 262000, global step 4196997: loss 2.1694
[2019-04-04 16:09:54,272] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 262000, global step 4196998: learning rate 0.0000
[2019-04-04 16:09:54,782] A3C_AGENT_WORKER-Thread-18 INFO:Local step 262000, global step 4197296: loss 2.1499
[2019-04-04 16:09:54,783] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 262000, global step 4197296: learning rate 0.0000
[2019-04-04 16:09:55,079] A3C_AGENT_WORKER-Thread-13 INFO:Local step 263000, global step 4197458: loss 1.2185
[2019-04-04 16:09:55,080] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 263000, global step 4197458: learning rate 0.0000
[2019-04-04 16:09:55,313] A3C_AGENT_WORKER-Thread-2 INFO:Local step 263000, global step 4197589: loss 1.2096
[2019-04-04 16:09:55,315] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 263000, global step 4197589: learning rate 0.0000
[2019-04-04 16:09:56,340] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.9839867e-09 1.0439324e-09 3.3864442e-14 6.1527010e-14 1.0000000e+00
 5.8184763e-10 6.0487693e-14], sum to 1.0000
[2019-04-04 16:09:56,342] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5182
[2019-04-04 16:09:56,354] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333334, 49.66666666666667, 0.0, 0.0, 26.0, 25.52879045003419, 0.4202535413601708, 0.0, 1.0, 18749.89870522999], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4826400.0000, 
sim time next is 4827000.0000, 
raw observation next is [0.1666666666666666, 50.33333333333333, 0.0, 0.0, 26.0, 25.50359238061223, 0.4116091926501249, 0.0, 1.0, 28322.98360395114], 
processed observation next is [0.0, 0.8695652173913043, 0.4672206832871654, 0.5033333333333333, 0.0, 0.0, 0.6666666666666666, 0.6252993650510191, 0.6372030642167082, 0.0, 1.0, 0.13487135049500543], 
reward next is 0.8651, 
noisyNet noise sample is [array([-1.2830596], dtype=float32), -1.3807446]. 
=============================================
[2019-04-04 16:09:56,359] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[80.16167]
 [80.15724]
 [80.26289]
 [80.25455]
 [80.086  ]], R is [[80.24901581]
 [80.35723877]
 [80.55366516]
 [80.7481308 ]
 [80.85129547]].
[2019-04-04 16:09:57,680] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.1285251e-10 3.3505937e-10 2.3488618e-16 1.0341160e-14 1.0000000e+00
 8.1442415e-11 5.6920281e-16], sum to 1.0000
[2019-04-04 16:09:57,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9217
[2019-04-04 16:09:57,701] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 67.0, 0.0, 0.0, 26.0, 25.71695211339989, 0.5603448967645739, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4417200.0000, 
sim time next is 4417800.0000, 
raw observation next is [4.916666666666667, 67.0, 0.0, 0.0, 26.0, 25.66995787406922, 0.5492686605644578, 0.0, 1.0, 45597.96056158301], 
processed observation next is [1.0, 0.13043478260869565, 0.5987996306555864, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6391631561724349, 0.6830895535214859, 0.0, 1.0, 0.21713314553134766], 
reward next is 0.7829, 
noisyNet noise sample is [array([-0.80406433], dtype=float32), -1.9820389]. 
=============================================
[2019-04-04 16:09:58,588] A3C_AGENT_WORKER-Thread-15 INFO:Local step 262500, global step 4199543: loss 0.0962
[2019-04-04 16:09:58,588] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 262500, global step 4199543: learning rate 0.0000
[2019-04-04 16:09:58,684] A3C_AGENT_WORKER-Thread-11 INFO:Local step 263000, global step 4199598: loss 1.2395
[2019-04-04 16:09:58,686] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 263000, global step 4199599: learning rate 0.0000
[2019-04-04 16:09:58,780] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9386240e-10 5.6547930e-12 9.4931204e-17 1.4774927e-15 1.0000000e+00
 1.3236011e-10 9.4362554e-17], sum to 1.0000
[2019-04-04 16:09:58,782] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3163
[2019-04-04 16:09:58,805] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 77.0, 33.0, 36.66666666666667, 26.0, 25.81146491351729, 0.5492524162230287, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4468200.0000, 
sim time next is 4468800.0000, 
raw observation next is [0.0, 76.0, 29.0, 45.83333333333334, 26.0, 26.0092372485427, 0.5334514802934902, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.76, 0.09666666666666666, 0.050644567219152864, 0.6666666666666666, 0.6674364373785583, 0.6778171600978301, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9066877], dtype=float32), -0.21000561]. 
=============================================
[2019-04-04 16:09:59,414] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-04 16:09:59,417] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 16:09:59,417] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 16:09:59,418] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:09:59,418] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:09:59,418] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 16:09:59,419] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:09:59,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run43
[2019-04-04 16:09:59,459] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run43
[2019-04-04 16:09:59,487] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run43
[2019-04-04 16:10:47,386] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.18021035], dtype=float32), 0.23791586]
[2019-04-04 16:10:47,387] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.3, 91.0, 0.0, 0.0, 26.0, 25.24575960440761, 0.4961564481554479, 0.0, 1.0, 55515.22310768355]
[2019-04-04 16:10:47,387] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 16:10:47,388] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.73775946e-10 2.41680963e-11 7.06314305e-17 9.92672216e-16
 1.00000000e+00 1.22948405e-11 1.08526560e-16], sampled 0.7359628005854474
[2019-04-04 16:11:26,932] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18021035], dtype=float32), 0.23791586]
[2019-04-04 16:11:26,933] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.533391224333333, 98.12141515333334, 0.0, 0.0, 26.0, 25.03803646766261, 0.2571010895572035, 0.0, 1.0, 54095.02577466056]
[2019-04-04 16:11:26,933] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 16:11:26,933] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.5550537e-10 8.8391926e-11 1.5205014e-16 3.5731276e-15 1.0000000e+00
 2.2076769e-11 2.7575610e-16], sampled 0.06704268552292258
[2019-04-04 16:11:40,753] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 16:11:47,481] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18021035], dtype=float32), 0.23791586]
[2019-04-04 16:11:47,482] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.360819853333333, 33.88817915666667, 0.0, 0.0, 26.0, 25.44810110419541, 0.5513064101158855, 0.0, 1.0, 72675.26504955487]
[2019-04-04 16:11:47,482] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 16:11:47,483] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.5211993e-09 5.3045174e-10 1.6360977e-14 5.4151362e-14 1.0000000e+00
 6.5173539e-10 2.1791252e-14], sampled 0.5375863714008617
[2019-04-04 16:12:00,717] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 16:12:04,001] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 16:12:05,023] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 4200000, evaluation results [4200000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 16:12:05,735] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1412342e-10 1.6323605e-10 4.8035281e-16 2.3016457e-14 1.0000000e+00
 9.9039395e-11 2.7989156e-15], sum to 1.0000
[2019-04-04 16:12:05,737] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1433
[2019-04-04 16:12:05,762] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.58525910557882, 0.4842134121350963, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4496400.0000, 
sim time next is 4497000.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.57266697065237, 0.4723028009517899, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6310555808876975, 0.6574342669839299, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2075654], dtype=float32), 0.7133414]. 
=============================================
[2019-04-04 16:12:05,778] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.57224]
 [82.85497]
 [83.06993]
 [83.23983]
 [83.17413]], R is [[82.48603821]
 [82.66117859]
 [82.83456421]
 [83.00621796]
 [82.95246124]].
[2019-04-04 16:12:05,867] A3C_AGENT_WORKER-Thread-17 INFO:Local step 263000, global step 4200493: loss 1.2223
[2019-04-04 16:12:05,870] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 263000, global step 4200493: learning rate 0.0000
[2019-04-04 16:12:07,077] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2002661e-09 2.8918598e-10 1.5179176e-15 5.9055171e-14 1.0000000e+00
 2.9534167e-10 1.8838768e-14], sum to 1.0000
[2019-04-04 16:12:07,080] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6105
[2019-04-04 16:12:07,094] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 45.0, 187.3333333333333, 406.0, 26.0, 25.08109033064682, 0.3743315182478941, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4893000.0000, 
sim time next is 4893600.0000, 
raw observation next is [3.0, 45.0, 175.1666666666667, 414.0, 26.0, 25.09219895970005, 0.3758999285088311, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.583888888888889, 0.4574585635359116, 0.6666666666666666, 0.5910165799750041, 0.6252999761696104, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.93499017], dtype=float32), 0.12659368]. 
=============================================
[2019-04-04 16:12:07,356] A3C_AGENT_WORKER-Thread-4 INFO:Local step 262500, global step 4201348: loss 0.2102
[2019-04-04 16:12:07,356] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 262500, global step 4201348: learning rate 0.0000
[2019-04-04 16:12:08,856] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.2872855e-09 3.6267483e-10 8.7127816e-15 4.0409164e-14 1.0000000e+00
 1.6155547e-09 1.8195088e-14], sum to 1.0000
[2019-04-04 16:12:08,858] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8117
[2019-04-04 16:12:08,900] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.04366571245032, 0.4639515522434013, 0.0, 1.0, 18709.42571430249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4479000.0000, 
sim time next is 4479600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.05547894147045, 0.4736609112378769, 0.0, 1.0, 198953.6940626408], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5879565784558709, 0.6578869704126257, 0.0, 1.0, 0.9473985431554324], 
reward next is 0.0526, 
noisyNet noise sample is [array([0.69925034], dtype=float32), -0.48637542]. 
=============================================
[2019-04-04 16:12:09,268] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1223004e-10 1.4104340e-10 4.6957978e-17 4.9547044e-16 1.0000000e+00
 2.6740981e-11 6.8052672e-17], sum to 1.0000
[2019-04-04 16:12:09,268] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1736
[2019-04-04 16:12:09,295] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.666666666666667, 42.0, 113.3333333333333, 735.0, 26.0, 26.9169401937611, 0.6924961518559917, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5046600.0000, 
sim time next is 5047200.0000, 
raw observation next is [3.0, 41.0, 114.0, 753.5, 26.0, 26.96871884357482, 0.7190067492381874, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5457063711911359, 0.41, 0.38, 0.8325966850828729, 0.6666666666666666, 0.7473932369645683, 0.7396689164127291, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20082444], dtype=float32), 0.51823354]. 
=============================================
[2019-04-04 16:12:09,445] A3C_AGENT_WORKER-Thread-6 INFO:Local step 262500, global step 4202394: loss 0.2214
[2019-04-04 16:12:09,448] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 262500, global step 4202394: learning rate 0.0000
[2019-04-04 16:12:09,675] A3C_AGENT_WORKER-Thread-12 INFO:Local step 262500, global step 4202518: loss 0.2140
[2019-04-04 16:12:09,676] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 262500, global step 4202518: learning rate 0.0000
[2019-04-04 16:12:10,168] A3C_AGENT_WORKER-Thread-10 INFO:Local step 262500, global step 4202772: loss 0.1136
[2019-04-04 16:12:10,169] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 262500, global step 4202772: learning rate 0.0000
[2019-04-04 16:12:10,243] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4245749e-09 1.7683385e-10 2.9790308e-16 2.4400565e-15 1.0000000e+00
 1.2913527e-10 9.1546982e-16], sum to 1.0000
[2019-04-04 16:12:10,247] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3381
[2019-04-04 16:12:10,258] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 51.0, 119.5, 0.0, 26.0, 26.2437799942858, 0.5315757237672294, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4534800.0000, 
sim time next is 4535400.0000, 
raw observation next is [2.0, 49.5, 121.0, 0.0, 26.0, 26.18880363947555, 0.5317382853579254, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.518005540166205, 0.495, 0.4033333333333333, 0.0, 0.6666666666666666, 0.6824003032896293, 0.6772460951193086, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.77118], dtype=float32), 0.22877714]. 
=============================================
[2019-04-04 16:12:10,641] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5417553e-09 3.7889869e-10 9.8338164e-16 1.2077502e-14 1.0000000e+00
 7.5470331e-11 9.3491929e-16], sum to 1.0000
[2019-04-04 16:12:10,653] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0864
[2019-04-04 16:12:10,687] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 48.0, 122.5, 0.0, 26.0, 26.19201331313189, 0.5356320453702064, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4536000.0000, 
sim time next is 4536600.0000, 
raw observation next is [2.0, 48.66666666666666, 124.0, 0.0, 26.0, 26.20560012144517, 0.4272645924947922, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.4866666666666666, 0.41333333333333333, 0.0, 0.6666666666666666, 0.6838000101204308, 0.6424215308315974, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2047176], dtype=float32), -0.02898397]. 
=============================================
[2019-04-04 16:12:10,905] A3C_AGENT_WORKER-Thread-3 INFO:Local step 262500, global step 4203184: loss 0.2134
[2019-04-04 16:12:10,906] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 262500, global step 4203184: learning rate 0.0000
[2019-04-04 16:12:11,508] A3C_AGENT_WORKER-Thread-16 INFO:Local step 262500, global step 4203506: loss 0.1088
[2019-04-04 16:12:11,511] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 262500, global step 4203506: learning rate 0.0000
[2019-04-04 16:12:12,120] A3C_AGENT_WORKER-Thread-20 INFO:Local step 262500, global step 4203833: loss 0.1158
[2019-04-04 16:12:12,123] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 262500, global step 4203833: learning rate 0.0000
[2019-04-04 16:12:12,260] A3C_AGENT_WORKER-Thread-19 INFO:Local step 262500, global step 4203908: loss 0.2144
[2019-04-04 16:12:12,261] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 262500, global step 4203908: learning rate 0.0000
[2019-04-04 16:12:12,352] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:12,352] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:12,396] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run32
[2019-04-04 16:12:12,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:12,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:12,468] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run32
[2019-04-04 16:12:12,583] A3C_AGENT_WORKER-Thread-5 INFO:Local step 262500, global step 4204034: loss 0.1076
[2019-04-04 16:12:12,585] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 262500, global step 4204034: learning rate 0.0000
[2019-04-04 16:12:13,476] A3C_AGENT_WORKER-Thread-14 INFO:Local step 262500, global step 4204478: loss 0.1137
[2019-04-04 16:12:13,476] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 262500, global step 4204478: learning rate 0.0000
[2019-04-04 16:12:13,839] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1538408e-09 9.5192951e-11 2.9344567e-16 2.1869408e-14 1.0000000e+00
 4.5386458e-11 1.6681304e-15], sum to 1.0000
[2019-04-04 16:12:13,840] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1937
[2019-04-04 16:12:13,854] A3C_AGENT_WORKER-Thread-18 INFO:Local step 262500, global step 4204673: loss 0.1125
[2019-04-04 16:12:13,855] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 262500, global step 4204673: learning rate 0.0000
[2019-04-04 16:12:13,891] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 71.0, 61.5, 85.5, 26.0, 25.49129194221538, 0.4107406309821665, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4608000.0000, 
sim time next is 4608600.0000, 
raw observation next is [-2.0, 71.0, 82.00000000000001, 114.0, 26.0, 25.49882561944834, 0.4087828718492701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.2733333333333334, 0.12596685082872927, 0.6666666666666666, 0.6249021349540284, 0.6362609572830901, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6943897], dtype=float32), -0.9105106]. 
=============================================
[2019-04-04 16:12:15,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:15,822] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:15,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run32
[2019-04-04 16:12:16,918] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.09960485e-09 1.12352704e-10 1.03997435e-15 9.22171885e-15
 1.00000000e+00 1.04544673e-09 1.12119930e-15], sum to 1.0000
[2019-04-04 16:12:16,918] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0552
[2019-04-04 16:12:16,930] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 78.0, 0.0, 0.0, 26.0, 25.29691304876611, 0.4383573089699069, 1.0, 1.0, 25752.05802064527], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4732200.0000, 
sim time next is 4732800.0000, 
raw observation next is [-0.6666666666666666, 78.0, 0.0, 0.0, 26.0, 25.17633014363118, 0.4193736174476752, 1.0, 1.0, 58459.89135443052], 
processed observation next is [1.0, 0.782608695652174, 0.44413665743305636, 0.78, 0.0, 0.0, 0.6666666666666666, 0.598027511969265, 0.6397912058158918, 1.0, 1.0, 0.2783804350210977], 
reward next is 0.7216, 
noisyNet noise sample is [array([2.22513], dtype=float32), 1.0378802]. 
=============================================
[2019-04-04 16:12:17,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:17,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:17,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run32
[2019-04-04 16:12:17,628] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3074505e-09 1.8005282e-10 2.4500832e-15 3.2306387e-15 1.0000000e+00
 7.6176954e-11 3.1798002e-15], sum to 1.0000
[2019-04-04 16:12:17,633] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2505
[2019-04-04 16:12:17,652] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 53.0, 0.0, 0.0, 26.0, 25.40868720406428, 0.4062103222818248, 0.0, 1.0, 46370.21712659623], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4829400.0000, 
sim time next is 4830000.0000, 
raw observation next is [-0.6666666666666666, 53.66666666666667, 0.0, 0.0, 26.0, 25.40621991298435, 0.4037542966081791, 0.0, 1.0, 41953.61412243982], 
processed observation next is [0.0, 0.9130434782608695, 0.44413665743305636, 0.5366666666666667, 0.0, 0.0, 0.6666666666666666, 0.617184992748696, 0.6345847655360597, 0.0, 1.0, 0.19977911486876107], 
reward next is 0.8002, 
noisyNet noise sample is [array([-0.10880104], dtype=float32), -0.29628533]. 
=============================================
[2019-04-04 16:12:17,665] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.58465 ]
 [80.53592 ]
 [80.395035]
 [80.24396 ]
 [80.08326 ]], R is [[80.61672211]
 [80.58974457]
 [80.48820496]
 [80.38565826]
 [80.337677  ]].
[2019-04-04 16:12:18,690] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3356427e-09 1.0526920e-09 1.9297548e-14 9.0126908e-14 1.0000000e+00
 2.4392763e-10 5.6841760e-15], sum to 1.0000
[2019-04-04 16:12:18,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4275
[2019-04-04 16:12:18,719] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.86994482783865, 0.2487244369522439, 0.0, 1.0, 39195.5902503394], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4852800.0000, 
sim time next is 4853400.0000, 
raw observation next is [-3.166666666666667, 61.83333333333333, 0.0, 0.0, 26.0, 24.84376861246443, 0.2425194134853127, 0.0, 1.0, 39191.54217420246], 
processed observation next is [0.0, 0.17391304347826086, 0.3748845798707295, 0.6183333333333333, 0.0, 0.0, 0.6666666666666666, 0.5703140510387025, 0.5808398044951043, 0.0, 1.0, 0.186626391305726], 
reward next is 0.8134, 
noisyNet noise sample is [array([2.1831658], dtype=float32), -0.2130473]. 
=============================================
[2019-04-04 16:12:19,922] A3C_AGENT_WORKER-Thread-15 INFO:Local step 263000, global step 4207410: loss 1.2087
[2019-04-04 16:12:19,925] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 263000, global step 4207412: learning rate 0.0000
[2019-04-04 16:12:22,712] A3C_AGENT_WORKER-Thread-4 INFO:Local step 263000, global step 4208578: loss 1.1896
[2019-04-04 16:12:22,713] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 263000, global step 4208578: learning rate 0.0000
[2019-04-04 16:12:24,294] A3C_AGENT_WORKER-Thread-6 INFO:Local step 263000, global step 4209323: loss 1.2103
[2019-04-04 16:12:24,298] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 263000, global step 4209323: learning rate 0.0000
[2019-04-04 16:12:24,634] A3C_AGENT_WORKER-Thread-12 INFO:Local step 263000, global step 4209494: loss 1.2025
[2019-04-04 16:12:24,635] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 263000, global step 4209495: learning rate 0.0000
[2019-04-04 16:12:24,897] A3C_AGENT_WORKER-Thread-10 INFO:Local step 263000, global step 4209653: loss 1.1890
[2019-04-04 16:12:24,899] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 263000, global step 4209654: learning rate 0.0000
[2019-04-04 16:12:25,570] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.6067595e-10 2.7348263e-10 1.6493935e-16 1.2792933e-14 1.0000000e+00
 4.1898981e-11 5.1561670e-16], sum to 1.0000
[2019-04-04 16:12:25,573] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4713
[2019-04-04 16:12:25,616] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3, 49.5, 283.0, 308.0, 26.0, 25.02252860489354, 0.350427896761375, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4879800.0000, 
sim time next is 4880400.0000, 
raw observation next is [0.5333333333333332, 48.66666666666667, 282.6666666666667, 321.6666666666667, 26.0, 25.045310802392, 0.3520030303747122, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4773776546629733, 0.4866666666666667, 0.9422222222222223, 0.3554327808471455, 0.6666666666666666, 0.5871092335326665, 0.6173343434582373, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32359192], dtype=float32), 0.52978057]. 
=============================================
[2019-04-04 16:12:25,625] A3C_AGENT_WORKER-Thread-3 INFO:Local step 263000, global step 4210048: loss 1.2032
[2019-04-04 16:12:25,632] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 263000, global step 4210050: learning rate 0.0000
[2019-04-04 16:12:25,752] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2091883e-10 7.3360311e-11 2.5037484e-16 2.1299740e-14 1.0000000e+00
 2.2046599e-11 1.0544592e-15], sum to 1.0000
[2019-04-04 16:12:25,753] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2759
[2019-04-04 16:12:25,767] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8333333333333334, 54.33333333333334, 0.0, 0.0, 26.0, 25.39971396882441, 0.400528060401578, 0.0, 1.0, 42981.08300898232], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4830600.0000, 
sim time next is 4831200.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.38944873487608, 0.4004623108233962, 0.0, 1.0, 45997.61775416045], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6157873945730067, 0.6334874369411321, 0.0, 1.0, 0.21903627501981165], 
reward next is 0.7810, 
noisyNet noise sample is [array([0.9820236], dtype=float32), 0.75201285]. 
=============================================
[2019-04-04 16:12:25,846] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0966579e-09 1.7803875e-09 2.5530246e-15 4.9759804e-14 1.0000000e+00
 5.1753241e-11 1.3946055e-14], sum to 1.0000
[2019-04-04 16:12:25,846] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7200
[2019-04-04 16:12:25,862] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.5, 62.5, 0.0, 0.0, 26.0, 24.57684250125638, 0.1767932969518152, 0.0, 1.0, 39480.93899097785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4861800.0000, 
sim time next is 4862400.0000, 
raw observation next is [-3.666666666666667, 63.33333333333333, 0.0, 0.0, 26.0, 24.561418362849, 0.1711930859669913, 0.0, 1.0, 39478.78124148501], 
processed observation next is [0.0, 0.2608695652173913, 0.3610341643582641, 0.6333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5467848635707501, 0.5570643619889971, 0.0, 1.0, 0.18799419638802387], 
reward next is 0.8120, 
noisyNet noise sample is [array([0.75981456], dtype=float32), 0.4564254]. 
=============================================
[2019-04-04 16:12:26,599] A3C_AGENT_WORKER-Thread-16 INFO:Local step 263000, global step 4210537: loss 1.2016
[2019-04-04 16:12:26,605] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 263000, global step 4210537: learning rate 0.0000
[2019-04-04 16:12:26,657] A3C_AGENT_WORKER-Thread-19 INFO:Local step 263000, global step 4210561: loss 1.2054
[2019-04-04 16:12:26,658] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 263000, global step 4210561: learning rate 0.0000
[2019-04-04 16:12:26,887] A3C_AGENT_WORKER-Thread-20 INFO:Local step 263000, global step 4210673: loss 1.2035
[2019-04-04 16:12:26,887] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 263000, global step 4210673: learning rate 0.0000
[2019-04-04 16:12:27,618] A3C_AGENT_WORKER-Thread-5 INFO:Local step 263000, global step 4211041: loss 1.2059
[2019-04-04 16:12:27,620] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 263000, global step 4211041: learning rate 0.0000
[2019-04-04 16:12:28,030] A3C_AGENT_WORKER-Thread-14 INFO:Local step 263000, global step 4211278: loss 1.1999
[2019-04-04 16:12:28,031] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 263000, global step 4211278: learning rate 0.0000
[2019-04-04 16:12:28,642] A3C_AGENT_WORKER-Thread-18 INFO:Local step 263000, global step 4211554: loss 1.2085
[2019-04-04 16:12:28,644] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 263000, global step 4211554: learning rate 0.0000
[2019-04-04 16:12:31,078] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:31,078] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:31,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run32
[2019-04-04 16:12:31,991] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.5339672e-09 1.4964219e-10 2.1593307e-15 6.3253562e-15 1.0000000e+00
 2.9984790e-10 8.4609689e-15], sum to 1.0000
[2019-04-04 16:12:31,992] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5583
[2019-04-04 16:12:32,012] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 29.0, 119.5, 824.0, 26.0, 26.67919196133209, 0.6126211494010018, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4964400.0000, 
sim time next is 4965000.0000, 
raw observation next is [3.5, 28.33333333333334, 120.3333333333333, 831.0, 26.0, 26.6679092600734, 0.6278172608510285, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5595567867036012, 0.2833333333333334, 0.401111111111111, 0.918232044198895, 0.6666666666666666, 0.7223257716727834, 0.7092724202836762, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42087057], dtype=float32), -0.3954638]. 
=============================================
[2019-04-04 16:12:32,033] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.63441 ]
 [84.689514]
 [84.76524 ]
 [84.784904]
 [84.90987 ]], R is [[84.7696228 ]
 [84.92192841]
 [85.07270813]
 [85.22198486]
 [85.36976624]].
[2019-04-04 16:12:33,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:33,802] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:33,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run32
[2019-04-04 16:12:35,488] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:35,488] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:35,506] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run32
[2019-04-04 16:12:35,668] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:35,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:35,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run32
[2019-04-04 16:12:35,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:35,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:35,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run32
[2019-04-04 16:12:36,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:36,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:36,295] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run32
[2019-04-04 16:12:37,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:37,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:37,551] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run32
[2019-04-04 16:12:37,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:37,578] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:37,604] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run32
[2019-04-04 16:12:38,034] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:38,034] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:38,038] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run32
[2019-04-04 16:12:38,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:38,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:38,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run32
[2019-04-04 16:12:38,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:38,695] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:38,698] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run32
[2019-04-04 16:12:39,132] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:12:39,132] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:12:39,135] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run32
[2019-04-04 16:12:57,197] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.0803423e-08 2.2300595e-08 1.2080135e-13 2.9282002e-12 9.9999988e-01
 7.1488020e-09 3.3869107e-13], sum to 1.0000
[2019-04-04 16:12:57,198] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0368
[2019-04-04 16:12:57,227] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.40460480661837, -0.0921791583340894, 0.0, 1.0, 44036.39763170204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 183600.0000, 
sim time next is 184200.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.36077660502278, -0.1025641881882867, 0.0, 1.0, 44089.18847806747], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.44673138375189847, 0.46581193727057113, 0.0, 1.0, 0.20994851656222607], 
reward next is 0.7901, 
noisyNet noise sample is [array([0.9247904], dtype=float32), -2.5546944]. 
=============================================
[2019-04-04 16:12:58,166] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.18826365e-09 9.50411527e-10 1.43554161e-14 1.36164029e-13
 1.00000000e+00 5.48295187e-10 1.20527876e-14], sum to 1.0000
[2019-04-04 16:12:58,166] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9016
[2019-04-04 16:12:58,181] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.1, 81.0, 0.0, 0.0, 26.0, 24.31689746827882, 0.1236142042310517, 0.0, 1.0, 44300.34911821029], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 256800.0000, 
sim time next is 257400.0000, 
raw observation next is [-4.2, 80.5, 0.0, 0.0, 26.0, 24.27589445138345, 0.1156554013669549, 0.0, 1.0, 44331.01748190192], 
processed observation next is [1.0, 1.0, 0.34626038781163443, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5229912042819542, 0.5385518004556517, 0.0, 1.0, 0.21110008324715201], 
reward next is 0.7889, 
noisyNet noise sample is [array([0.9786139], dtype=float32), -1.5730708]. 
=============================================
[2019-04-04 16:13:05,262] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.7749070e-10 1.6651529e-10 2.3065522e-16 4.2276833e-15 1.0000000e+00
 4.5906921e-11 3.9091756e-16], sum to 1.0000
[2019-04-04 16:13:05,263] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5334
[2019-04-04 16:13:05,282] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 24.35674429421071, 0.1374871290145538, 0.0, 1.0, 41095.33576160912], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 546000.0000, 
sim time next is 546600.0000, 
raw observation next is [0.5, 92.0, 12.0, 37.99999999999999, 26.0, 24.31886237994276, 0.1465090635922772, 0.0, 1.0, 41086.71925139962], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.04, 0.04198895027624309, 0.6666666666666666, 0.5265718649952301, 0.5488363545307591, 0.0, 1.0, 0.1956510440542839], 
reward next is 0.8043, 
noisyNet noise sample is [array([2.5818813], dtype=float32), 0.46291068]. 
=============================================
[2019-04-04 16:13:32,614] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8700453e-09 1.8890305e-09 2.2642597e-14 6.3066419e-13 1.0000000e+00
 5.3393778e-09 5.1528097e-14], sum to 1.0000
[2019-04-04 16:13:32,615] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9651
[2019-04-04 16:13:32,629] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.2945352898435, 0.0645722782368196, 0.0, 1.0, 41219.09105351521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 700200.0000, 
sim time next is 700800.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.31331202549129, 0.05789884752056908, 0.0, 1.0, 41266.88143577805], 
processed observation next is [1.0, 0.08695652173913043, 0.368421052631579, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5261093354576074, 0.5192996158401897, 0.0, 1.0, 0.19650895921799072], 
reward next is 0.8035, 
noisyNet noise sample is [array([-0.1157875], dtype=float32), 0.071679235]. 
=============================================
[2019-04-04 16:13:32,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7486662e-08 2.6669951e-09 2.5207025e-15 4.2009370e-13 1.0000000e+00
 7.9329512e-09 3.1079197e-14], sum to 1.0000
[2019-04-04 16:13:32,739] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1604
[2019-04-04 16:13:32,847] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.4, 77.66666666666667, 12.66666666666667, 0.0, 26.0, 23.4318208916124, -0.08399500494480457, 0.0, 1.0, 43984.58088701643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 634200.0000, 
sim time next is 634800.0000, 
raw observation next is [-4.300000000000001, 76.33333333333334, 15.83333333333333, 0.0, 26.0, 23.4103136204088, -0.01117987522583241, 0.0, 1.0, 202373.3420255714], 
processed observation next is [0.0, 0.34782608695652173, 0.34349030470914127, 0.7633333333333334, 0.05277777777777777, 0.0, 0.6666666666666666, 0.45085946836740004, 0.49627337492472257, 0.0, 1.0, 0.9636825810741495], 
reward next is 0.0363, 
noisyNet noise sample is [array([-0.868663], dtype=float32), 1.399725]. 
=============================================
[2019-04-04 16:13:37,622] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4036984e-11 5.9635907e-12 2.8115385e-18 1.1430900e-16 1.0000000e+00
 2.4991064e-12 2.9272760e-18], sum to 1.0000
[2019-04-04 16:13:37,623] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2297
[2019-04-04 16:13:37,635] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.15, 81.5, 0.0, 0.0, 26.0, 25.62833738050518, 0.612753278804334, 0.0, 1.0, 25007.81568162289], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1146600.0000, 
sim time next is 1147200.0000, 
raw observation next is [12.33333333333333, 81.0, 0.0, 0.0, 26.0, 25.64422794781785, 0.6138420827345835, 0.0, 1.0, 18727.69435256536], 
processed observation next is [0.0, 0.2608695652173913, 0.8042474607571561, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6370189956514875, 0.7046140275781946, 0.0, 1.0, 0.0891794969169779], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.57519615], dtype=float32), -0.7968215]. 
=============================================
[2019-04-04 16:13:38,817] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7343285e-09 2.7551076e-11 3.1328046e-17 1.2699281e-15 1.0000000e+00
 8.7794598e-11 2.6744521e-17], sum to 1.0000
[2019-04-04 16:13:38,818] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2351
[2019-04-04 16:13:38,830] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 26.10142924071922, 0.6387332914151563, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1039200.0000, 
sim time next is 1039800.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 26.06520965562358, 0.6319593920389058, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6721008046352983, 0.7106531306796353, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00232821], dtype=float32), -1.7528781]. 
=============================================
[2019-04-04 16:13:40,345] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5170848e-09 3.3845593e-10 1.0319796e-15 5.2438471e-14 1.0000000e+00
 6.1824490e-11 7.8598203e-15], sum to 1.0000
[2019-04-04 16:13:40,346] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7320
[2019-04-04 16:13:40,368] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 100.0, 76.0, 0.0, 26.0, 23.30472534653298, 0.1230740888890166, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1245600.0000, 
sim time next is 1246200.0000, 
raw observation next is [14.9, 100.0, 76.66666666666667, 0.0, 26.0, 23.30085875325023, 0.1224842962833061, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.8753462603878118, 1.0, 0.2555555555555556, 0.0, 0.6666666666666666, 0.441738229437519, 0.540828098761102, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4216445], dtype=float32), -0.7594407]. 
=============================================
[2019-04-04 16:13:45,838] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0648622e-11 8.3991737e-12 8.3419242e-19 2.3973633e-17 1.0000000e+00
 8.1174970e-13 3.2165292e-18], sum to 1.0000
[2019-04-04 16:13:45,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6565
[2019-04-04 16:13:45,853] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 100.0, 92.0, 0.0, 26.0, 24.807981013721, 0.4533110297562424, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1258800.0000, 
sim time next is 1259400.0000, 
raw observation next is [13.8, 100.0, 89.0, 0.0, 26.0, 24.79224152592196, 0.4509893656227959, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.2966666666666667, 0.0, 0.6666666666666666, 0.5660201271601633, 0.6503297885409319, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16778529], dtype=float32), 1.1825472]. 
=============================================
[2019-04-04 16:13:53,146] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7832040e-10 7.4867662e-13 4.2724722e-18 5.7426895e-17 1.0000000e+00
 6.4821857e-13 5.0579179e-18], sum to 1.0000
[2019-04-04 16:13:53,150] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3359
[2019-04-04 16:13:53,187] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.4, 100.0, 0.0, 0.0, 26.0, 24.86154588581087, 0.3362083880392246, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 932400.0000, 
sim time next is 933000.0000, 
raw observation next is [4.5, 100.0, 0.0, 0.0, 26.0, 25.01040611665172, 0.343445083538449, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5872576177285319, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5842005097209766, 0.6144816945128163, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00380516], dtype=float32), -1.3697436]. 
=============================================
[2019-04-04 16:13:53,203] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[94.36271 ]
 [94.386826]
 [94.239944]
 [93.590675]
 [92.53458 ]], R is [[94.3766098 ]
 [94.43284607]
 [94.29601288]
 [93.73822021]
 [92.95678711]].
[2019-04-04 16:13:53,427] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7863674e-10 6.9952147e-12 5.7920808e-17 4.7390069e-16 1.0000000e+00
 7.5636589e-12 2.2301613e-16], sum to 1.0000
[2019-04-04 16:13:53,427] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8826
[2019-04-04 16:13:53,470] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 80.0, 38.5, 0.0, 26.0, 25.43876838303224, 0.2926772511049878, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 896400.0000, 
sim time next is 897000.0000, 
raw observation next is [1.1, 80.66666666666667, 41.66666666666666, 0.0, 26.0, 25.45656068546499, 0.2978875419778842, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8066666666666668, 0.13888888888888887, 0.0, 0.6666666666666666, 0.6213800571220824, 0.5992958473259614, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.64862573], dtype=float32), 1.2917477]. 
=============================================
[2019-04-04 16:13:53,480] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[87.42907]
 [87.48198]
 [87.55466]
 [87.53154]
 [87.45204]], R is [[87.47016144]
 [87.59545898]
 [87.71950531]
 [87.84230804]
 [87.96388245]].
[2019-04-04 16:13:57,923] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.0235414e-11 2.2698358e-12 2.9328518e-19 2.2041371e-16 1.0000000e+00
 1.2177534e-12 5.4205609e-18], sum to 1.0000
[2019-04-04 16:13:57,923] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0981
[2019-04-04 16:13:57,949] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.08333333333333, 92.16666666666667, 49.00000000000001, 0.0, 26.0, 26.24191036606288, 0.5790427284364678, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 983400.0000, 
sim time next is 984000.0000, 
raw observation next is [10.16666666666667, 92.33333333333334, 54.5, 0.0, 26.0, 26.2638841170213, 0.5918602792300621, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.7442289935364729, 0.9233333333333335, 0.18166666666666667, 0.0, 0.6666666666666666, 0.6886570097517749, 0.6972867597433541, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9859107], dtype=float32), 0.4354142]. 
=============================================
[2019-04-04 16:13:57,955] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[99.26029]
 [99.23347]
 [99.22734]
 [99.21595]
 [99.14306]], R is [[99.28461456]
 [99.29177094]
 [99.29885101]
 [99.30586243]
 [99.31280518]].
[2019-04-04 16:14:00,828] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2621365e-10 5.6258969e-12 2.5566949e-18 6.4690308e-17 1.0000000e+00
 1.0424385e-12 5.7541800e-17], sum to 1.0000
[2019-04-04 16:14:00,831] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8291
[2019-04-04 16:14:00,841] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.36666666666667, 78.33333333333334, 0.0, 0.0, 26.0, 25.63120367914834, 0.6301708280965469, 0.0, 1.0, 31093.25052916748], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1131600.0000, 
sim time next is 1132200.0000, 
raw observation next is [10.55, 78.0, 0.0, 0.0, 26.0, 25.63013051171822, 0.6302560912871304, 0.0, 1.0, 27857.28152798081], 
processed observation next is [0.0, 0.08695652173913043, 0.754847645429363, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6358442093098517, 0.7100853637623769, 0.0, 1.0, 0.13265372156181338], 
reward next is 0.8673, 
noisyNet noise sample is [array([-0.14116691], dtype=float32), 0.62522244]. 
=============================================
[2019-04-04 16:14:02,795] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6838557e-10 2.3817390e-10 1.6960026e-16 2.9551616e-15 1.0000000e+00
 2.0152184e-11 1.6849569e-15], sum to 1.0000
[2019-04-04 16:14:02,796] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5551
[2019-04-04 16:14:02,801] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.9, 53.16666666666666, 150.3333333333333, 0.0, 26.0, 27.08528108495512, 0.9200332921271582, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1087800.0000, 
sim time next is 1088400.0000, 
raw observation next is [19.0, 52.33333333333334, 145.1666666666667, 0.0, 26.0, 27.40784368325561, 0.9539301194011202, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.9889196675900279, 0.5233333333333334, 0.48388888888888903, 0.0, 0.6666666666666666, 0.7839869736046342, 0.8179767064670401, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5776122], dtype=float32), 1.4954273]. 
=============================================
[2019-04-04 16:14:04,143] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.16628607e-11 1.28271595e-11 7.32458175e-18 7.58701837e-17
 1.00000000e+00 1.11414057e-12 2.11745705e-18], sum to 1.0000
[2019-04-04 16:14:04,144] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7951
[2019-04-04 16:14:04,169] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.6, 77.0, 0.0, 0.0, 26.0, 25.64744354767834, 0.6187837538314601, 0.0, 1.0, 21317.82871265694], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1141200.0000, 
sim time next is 1141800.0000, 
raw observation next is [11.6, 78.0, 0.0, 0.0, 26.0, 25.66381537278685, 0.6221985357629286, 0.0, 1.0, 18725.29157426547], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6386512810655708, 0.7073995119209763, 0.0, 1.0, 0.08916805511554986], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.04418117], dtype=float32), 0.8953944]. 
=============================================
[2019-04-04 16:14:06,167] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3969367e-10 8.3164552e-12 3.1011881e-17 7.7071766e-16 1.0000000e+00
 6.3337231e-11 1.9675100e-16], sum to 1.0000
[2019-04-04 16:14:06,168] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2912
[2019-04-04 16:14:06,189] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 66.5, 0.0, 26.0, 25.47373677555073, 0.5259551656334516, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1695600.0000, 
sim time next is 1696200.0000, 
raw observation next is [1.183333333333333, 86.83333333333334, 62.0, 0.0, 26.0, 25.78764675601477, 0.550179951911525, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49538319482917825, 0.8683333333333334, 0.20666666666666667, 0.0, 0.6666666666666666, 0.6489705630012308, 0.6833933173038417, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4306083], dtype=float32), -0.94874066]. 
=============================================
[2019-04-04 16:14:07,114] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5908240e-09 5.1982885e-10 1.2578795e-15 2.9401293e-14 1.0000000e+00
 7.6848250e-11 7.1358899e-16], sum to 1.0000
[2019-04-04 16:14:07,114] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1901
[2019-04-04 16:14:07,163] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 83.66666666666667, 105.6666666666667, 0.0, 26.0, 25.04354683405544, 0.3414250648240061, 0.0, 1.0, 27091.3267856393], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1779000.0000, 
sim time next is 1779600.0000, 
raw observation next is [-2.8, 84.33333333333334, 102.3333333333333, 0.0, 26.0, 25.00035820575731, 0.3359901142471367, 0.0, 1.0, 58093.70840388358], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8433333333333334, 0.341111111111111, 0.0, 0.6666666666666666, 0.583363183813109, 0.6119967047490456, 0.0, 1.0, 0.27663670668515994], 
reward next is 0.7234, 
noisyNet noise sample is [array([0.69585294], dtype=float32), -0.77400756]. 
=============================================
[2019-04-04 16:14:07,171] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0764628e-10 1.7090936e-11 5.0631133e-17 1.5225453e-15 1.0000000e+00
 7.0041728e-12 1.3880652e-16], sum to 1.0000
[2019-04-04 16:14:07,178] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5369
[2019-04-04 16:14:07,194] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.133333333333334, 98.66666666666667, 0.0, 0.0, 26.0, 25.47016644932731, 0.5871705928361077, 0.0, 1.0, 34510.80564566636], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1293600.0000, 
sim time next is 1294200.0000, 
raw observation next is [4.95, 98.0, 0.0, 0.0, 26.0, 25.45623195497535, 0.5844756418495208, 0.0, 1.0, 41039.44138190585], 
processed observation next is [0.0, 1.0, 0.5997229916897507, 0.98, 0.0, 0.0, 0.6666666666666666, 0.6213526629146126, 0.6948252139498403, 0.0, 1.0, 0.19542591134240883], 
reward next is 0.8046, 
noisyNet noise sample is [array([0.31992593], dtype=float32), -0.42280123]. 
=============================================
[2019-04-04 16:14:07,789] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.2414699e-11 4.0872538e-11 5.1947611e-17 2.1427381e-16 1.0000000e+00
 1.4795035e-11 9.3397013e-18], sum to 1.0000
[2019-04-04 16:14:07,791] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5962
[2019-04-04 16:14:07,824] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.18891464649863, 0.5024572851166687, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1360200.0000, 
sim time next is 1360800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.29054711430716, 0.5044652025209218, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.60754559285893, 0.6681550675069738, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60402644], dtype=float32), -0.58256]. 
=============================================
[2019-04-04 16:14:16,685] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3599274e-08 1.9041815e-08 7.5769930e-14 6.3645248e-13 1.0000000e+00
 2.0915765e-09 7.3719741e-13], sum to 1.0000
[2019-04-04 16:14:16,685] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0197
[2019-04-04 16:14:16,714] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.2, 78.0, 0.0, 0.0, 26.0, 23.94058100398517, 0.00342531778932476, 0.0, 1.0, 44742.46258679492], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1910400.0000, 
sim time next is 1911000.0000, 
raw observation next is [-8.3, 78.0, 0.0, 0.0, 26.0, 24.03163609780182, 0.01282613638708691, 0.0, 1.0, 44658.31806991061], 
processed observation next is [1.0, 0.08695652173913043, 0.23268698060941828, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5026363414834849, 0.5042753787956956, 0.0, 1.0, 0.21265865747576482], 
reward next is 0.7873, 
noisyNet noise sample is [array([0.38782293], dtype=float32), 1.0965216]. 
=============================================
[2019-04-04 16:14:16,726] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[74.21757 ]
 [74.26979 ]
 [74.62441 ]
 [74.8439  ]
 [74.974106]], R is [[73.92552185]
 [73.97320557]
 [74.02006531]
 [74.06620789]
 [74.11161041]].
[2019-04-04 16:14:17,193] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2232025e-09 1.2062379e-10 5.5556957e-17 1.1028992e-15 1.0000000e+00
 2.2731036e-11 1.6182752e-16], sum to 1.0000
[2019-04-04 16:14:17,197] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2452
[2019-04-04 16:14:17,245] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.266666666666667, 100.0, 15.0, 0.0, 26.0, 25.39767921473589, 0.4609235474253701, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1498800.0000, 
sim time next is 1499400.0000, 
raw observation next is [1.35, 100.0, 18.0, 0.0, 26.0, 25.56808937687418, 0.476879240252498, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5000000000000001, 1.0, 0.06, 0.0, 0.6666666666666666, 0.6306741147395151, 0.6589597467508327, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86608255], dtype=float32), 0.7617596]. 
=============================================
[2019-04-04 16:14:18,428] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8180391e-09 1.4600268e-10 1.3563098e-15 3.0576574e-15 1.0000000e+00
 1.6181628e-10 8.4373397e-16], sum to 1.0000
[2019-04-04 16:14:18,428] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7412
[2019-04-04 16:14:18,452] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.9, 84.0, 0.0, 0.0, 26.0, 25.82975265513583, 0.6095683316204346, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1636200.0000, 
sim time next is 1636800.0000, 
raw observation next is [7.0, 83.33333333333334, 0.0, 0.0, 26.0, 25.71671323169789, 0.5972398067995621, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6565096952908588, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6430594359748243, 0.6990799355998542, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08068863], dtype=float32), 0.98154193]. 
=============================================
[2019-04-04 16:14:21,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.4495158e-10 3.7240877e-10 3.1818128e-16 9.3032322e-15 1.0000000e+00
 1.1316653e-10 3.6082870e-16], sum to 1.0000
[2019-04-04 16:14:21,663] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8734
[2019-04-04 16:14:21,686] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.95732534493053, 0.5073731241078655, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1704000.0000, 
sim time next is 1704600.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.82405169351288, 0.4750351593069679, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6520043077927401, 0.6583450531023226, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.1465201], dtype=float32), -0.46836403]. 
=============================================
[2019-04-04 16:14:24,750] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.63018276e-11 1.55365928e-11 1.33291942e-17 1.24278315e-16
 1.00000000e+00 7.80238634e-12 1.04088214e-17], sum to 1.0000
[2019-04-04 16:14:24,753] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2340
[2019-04-04 16:14:24,761] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.583333333333333, 92.0, 10.66666666666666, 0.0, 26.0, 25.53719615756183, 0.5072758764730888, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1669800.0000, 
sim time next is 1670400.0000, 
raw observation next is [3.3, 92.0, 15.5, 0.0, 26.0, 25.49241399248445, 0.4995711220421897, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.554016620498615, 0.92, 0.051666666666666666, 0.0, 0.6666666666666666, 0.6243678327070376, 0.6665237073473965, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05453942], dtype=float32), 0.41144407]. 
=============================================
[2019-04-04 16:14:29,188] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.7309005e-09 5.4487570e-10 4.9161840e-16 4.9998691e-14 1.0000000e+00
 1.4220548e-10 2.2713394e-15], sum to 1.0000
[2019-04-04 16:14:29,190] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0582
[2019-04-04 16:14:29,202] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.4, 91.66666666666667, 0.0, 0.0, 26.0, 25.34224321382181, 0.455689660509276, 0.0, 1.0, 43483.40204423034], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1732800.0000, 
sim time next is 1733400.0000, 
raw observation next is [0.35, 91.5, 0.0, 0.0, 26.0, 25.32528733029473, 0.4566209919602346, 0.0, 1.0, 43096.09327019761], 
processed observation next is [0.0, 0.043478260869565216, 0.47229916897506935, 0.915, 0.0, 0.0, 0.6666666666666666, 0.6104406108578943, 0.6522069973200783, 0.0, 1.0, 0.20521949176284576], 
reward next is 0.7948, 
noisyNet noise sample is [array([-0.05076753], dtype=float32), 1.2584347]. 
=============================================
[2019-04-04 16:14:36,988] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6773515e-07 8.6571479e-09 1.5032017e-13 1.7233776e-12 9.9999976e-01
 4.4050386e-09 5.2983599e-13], sum to 1.0000
[2019-04-04 16:14:36,994] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6914
[2019-04-04 16:14:37,108] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.08224720889179, -0.09719169503739607, 1.0, 1.0, 202359.7642663681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1927200.0000, 
sim time next is 1927800.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.62482685556823, 0.00984117161578458, 1.0, 1.0, 203280.437627387], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4687355712973525, 0.5032803905385949, 1.0, 1.0, 0.9680020839399381], 
reward next is 0.0320, 
noisyNet noise sample is [array([-0.15056737], dtype=float32), -1.9443873]. 
=============================================
[2019-04-04 16:14:52,207] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2376743e-07 1.2659799e-09 6.5887705e-14 1.1433777e-12 9.9999976e-01
 1.2192362e-08 1.0728749e-13], sum to 1.0000
[2019-04-04 16:14:52,208] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0514
[2019-04-04 16:14:52,233] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.24099296882595, 0.07411455812404721, 0.0, 1.0, 41065.97827217368], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2007000.0000, 
sim time next is 2007600.0000, 
raw observation next is [-6.199999999999999, 87.0, 0.0, 0.0, 26.0, 24.21321926624926, 0.0638995299894536, 0.0, 1.0, 41079.6130000487], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5177682721874385, 0.5212998433298178, 0.0, 1.0, 0.19561720476213668], 
reward next is 0.8044, 
noisyNet noise sample is [array([-0.8328525], dtype=float32), 0.27882597]. 
=============================================
[2019-04-04 16:14:54,739] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8995309e-09 7.2523099e-10 1.5589911e-14 3.3189499e-14 1.0000000e+00
 1.1300541e-10 9.9179460e-15], sum to 1.0000
[2019-04-04 16:14:54,739] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4246
[2019-04-04 16:14:54,764] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 66.33333333333333, 0.0, 0.0, 26.0, 24.17204015059864, 0.1080277332314757, 0.0, 1.0, 41138.57092740813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2353200.0000, 
sim time next is 2353800.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.14957927692748, 0.1030538797508116, 0.0, 1.0, 41174.56090611978], 
processed observation next is [0.0, 0.21739130434782608, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5124649397439566, 0.5343512932502705, 0.0, 1.0, 0.19606933764818943], 
reward next is 0.8039, 
noisyNet noise sample is [array([-0.20596807], dtype=float32), 1.3446157]. 
=============================================
[2019-04-04 16:15:01,070] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.2560386e-09 1.9808040e-10 4.1308858e-14 4.6365493e-14 1.0000000e+00
 1.4788111e-09 2.4620957e-14], sum to 1.0000
[2019-04-04 16:15:01,071] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6338
[2019-04-04 16:15:01,095] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.800000000000001, 82.83333333333334, 0.0, 0.0, 26.0, 25.19147858430776, 0.3490897065371416, 0.0, 1.0, 42519.48520470044], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2153400.0000, 
sim time next is 2154000.0000, 
raw observation next is [-6.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.18136214464397, 0.3295934755158327, 0.0, 1.0, 42670.16858881278], 
processed observation next is [1.0, 0.9565217391304348, 0.27146814404432135, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5984468453869974, 0.6098644918386109, 0.0, 1.0, 0.20319127899434658], 
reward next is 0.7968, 
noisyNet noise sample is [array([2.3185077], dtype=float32), 0.1661787]. 
=============================================
[2019-04-04 16:15:01,098] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[77.99184]
 [77.9315 ]
 [77.80173]
 [77.74724]
 [77.66751]], R is [[77.98362732]
 [78.00131226]
 [78.02031708]
 [78.03885651]
 [78.05602264]].
[2019-04-04 16:15:07,413] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.5854750e-08 2.3781721e-09 4.2833081e-14 8.8737101e-13 9.9999988e-01
 6.2294827e-09 7.9921299e-14], sum to 1.0000
[2019-04-04 16:15:07,414] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8583
[2019-04-04 16:15:07,441] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.64195206909533, -0.02871228459203798, 0.0, 1.0, 43241.56279205647], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2266800.0000, 
sim time next is 2267400.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.58457687107246, -0.03019939283480087, 0.0, 1.0, 43230.28271583483], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.46538140592270505, 0.4899335357217331, 0.0, 1.0, 0.205858489123023], 
reward next is 0.7941, 
noisyNet noise sample is [array([1.2171979], dtype=float32), -0.048721284]. 
=============================================
[2019-04-04 16:15:07,649] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8301798e-09 3.2889164e-10 2.2513265e-15 4.8020947e-14 1.0000000e+00
 2.1490396e-10 7.6590845e-15], sum to 1.0000
[2019-04-04 16:15:07,649] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2768
[2019-04-04 16:15:07,692] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.300000000000001, 70.0, 138.6666666666667, 0.0, 26.0, 25.53259224164248, 0.3382447850936686, 1.0, 1.0, 52090.45364378035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2200800.0000, 
sim time next is 2201400.0000, 
raw observation next is [-4.2, 69.5, 143.0, 0.0, 26.0, 25.54572033190659, 0.3521147731734492, 1.0, 1.0, 34939.25661649966], 
processed observation next is [1.0, 0.4782608695652174, 0.34626038781163443, 0.695, 0.4766666666666667, 0.0, 0.6666666666666666, 0.6288100276588825, 0.6173715910578164, 1.0, 1.0, 0.1663774124595222], 
reward next is 0.8336, 
noisyNet noise sample is [array([-1.2464204], dtype=float32), -1.1795669]. 
=============================================
[2019-04-04 16:15:13,570] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.1879771e-09 1.0189312e-09 1.9027633e-14 2.1135556e-14 1.0000000e+00
 3.0798855e-09 2.1212087e-14], sum to 1.0000
[2019-04-04 16:15:13,571] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9361
[2019-04-04 16:15:13,615] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.166666666666667, 54.83333333333333, 0.0, 0.0, 26.0, 25.56935076759242, 0.4792574334540262, 1.0, 1.0, 47580.82979253989], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2743800.0000, 
sim time next is 2744400.0000, 
raw observation next is [-4.333333333333334, 55.66666666666667, 0.0, 0.0, 26.0, 25.66744556684868, 0.4395421483357246, 1.0, 1.0, 49875.6726982142], 
processed observation next is [1.0, 0.782608695652174, 0.3425669436749769, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.6389537972373901, 0.6465140494452415, 1.0, 1.0, 0.23750320332482952], 
reward next is 0.7625, 
noisyNet noise sample is [array([-1.5490351], dtype=float32), 1.5011888]. 
=============================================
[2019-04-04 16:15:14,984] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.31173204e-08 1.01930295e-08 6.59436630e-14 5.39872400e-12
 1.00000000e+00 3.66172648e-09 1.25018377e-13], sum to 1.0000
[2019-04-04 16:15:14,984] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6490
[2019-04-04 16:15:14,999] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.14957927692748, 0.1030538797508116, 0.0, 1.0, 41174.56090611978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2353800.0000, 
sim time next is 2354400.0000, 
raw observation next is [-2.8, 65.0, 0.0, 0.0, 26.0, 24.12885982520534, 0.1025527686571428, 0.0, 1.0, 41193.98300876558], 
processed observation next is [0.0, 0.2608695652173913, 0.38504155124653744, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5107383187671116, 0.5341842562190476, 0.0, 1.0, 0.19616182385126466], 
reward next is 0.8038, 
noisyNet noise sample is [array([-2.9197009], dtype=float32), -0.6281251]. 
=============================================
[2019-04-04 16:15:19,507] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.8515852e-09 1.8272516e-09 7.3419406e-14 3.1758878e-13 1.0000000e+00
 1.8590665e-09 2.8660020e-14], sum to 1.0000
[2019-04-04 16:15:19,509] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5607
[2019-04-04 16:15:19,524] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.95, 43.0, 0.0, 0.0, 26.0, 25.08302442322354, 0.2547449836014367, 0.0, 1.0, 43029.45516421142], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2410200.0000, 
sim time next is 2410800.0000, 
raw observation next is [-4.133333333333334, 43.33333333333334, 0.0, 0.0, 26.0, 25.03559631370035, 0.2459451347132758, 0.0, 1.0, 43025.26883430746], 
processed observation next is [0.0, 0.9130434782608695, 0.34810710987996313, 0.4333333333333334, 0.0, 0.0, 0.6666666666666666, 0.5862996928083625, 0.5819817115710919, 0.0, 1.0, 0.20488223254432125], 
reward next is 0.7951, 
noisyNet noise sample is [array([0.04130276], dtype=float32), 0.033910774]. 
=============================================
[2019-04-04 16:15:21,192] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5161657e-09 4.4912846e-10 3.3467549e-14 6.8372703e-14 1.0000000e+00
 2.9009031e-10 2.4920306e-14], sum to 1.0000
[2019-04-04 16:15:21,193] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7364
[2019-04-04 16:15:21,207] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.95, 43.0, 0.0, 0.0, 26.0, 25.08302442326526, 0.2547449836129242, 0.0, 1.0, 43029.45516421563], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2410200.0000, 
sim time next is 2410800.0000, 
raw observation next is [-4.133333333333334, 43.33333333333334, 0.0, 0.0, 26.0, 25.03559631373835, 0.2459451347242951, 0.0, 1.0, 43025.26883430732], 
processed observation next is [0.0, 0.9130434782608695, 0.34810710987996313, 0.4333333333333334, 0.0, 0.0, 0.6666666666666666, 0.586299692811529, 0.5819817115747651, 0.0, 1.0, 0.20488223254432056], 
reward next is 0.7951, 
noisyNet noise sample is [array([-0.46405372], dtype=float32), 1.4953085]. 
=============================================
[2019-04-04 16:15:24,363] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.7729905e-09 3.1637781e-09 1.1510462e-14 3.8357928e-14 1.0000000e+00
 4.9046228e-10 7.7112677e-15], sum to 1.0000
[2019-04-04 16:15:24,363] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9888
[2019-04-04 16:15:24,403] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1333333333333333, 30.0, 89.33333333333333, 842.3333333333334, 26.0, 24.95160892992472, 0.2537982378969185, 0.0, 1.0, 25270.20701227181], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2463600.0000, 
sim time next is 2464200.0000, 
raw observation next is [0.5, 29.5, 90.0, 845.0, 26.0, 24.92205102465552, 0.2570342367615934, 0.0, 1.0, 32810.6120489648], 
processed observation next is [0.0, 0.5217391304347826, 0.4764542936288089, 0.295, 0.3, 0.9337016574585635, 0.6666666666666666, 0.57683758538796, 0.5856780789205311, 0.0, 1.0, 0.15624100975697525], 
reward next is 0.8438, 
noisyNet noise sample is [array([-1.2460499], dtype=float32), -0.3602318]. 
=============================================
[2019-04-04 16:15:30,161] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1204357e-09 3.6706682e-10 1.5159218e-14 5.7122194e-14 1.0000000e+00
 6.3998851e-10 8.3195051e-15], sum to 1.0000
[2019-04-04 16:15:30,161] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6505
[2019-04-04 16:15:30,187] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 55.0, 60.5, 506.1666666666666, 26.0, 25.15504157409245, 0.3867889432776109, 0.0, 1.0, 18700.26861021547], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2996400.0000, 
sim time next is 2997000.0000, 
raw observation next is [-1.0, 55.0, 56.0, 474.0, 26.0, 25.10925670332085, 0.3801365180209062, 0.0, 1.0, 41341.61273422657], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.55, 0.18666666666666668, 0.523756906077348, 0.6666666666666666, 0.5924380586100707, 0.6267121726736354, 0.0, 1.0, 0.19686482254393603], 
reward next is 0.8031, 
noisyNet noise sample is [array([1.0229859], dtype=float32), 2.3586996]. 
=============================================
[2019-04-04 16:15:30,191] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[78.39014]
 [78.61894]
 [78.93899]
 [79.21451]
 [79.34967]], R is [[78.30661011]
 [78.43449402]
 [78.65014648]
 [78.86364746]
 [78.9859314 ]].
[2019-04-04 16:15:34,291] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4946301e-09 1.0675354e-09 1.3406419e-14 6.9882461e-14 1.0000000e+00
 6.5649131e-10 2.8789427e-14], sum to 1.0000
[2019-04-04 16:15:34,297] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9595
[2019-04-04 16:15:34,326] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.166666666666667, 55.83333333333334, 74.0, 602.6666666666667, 26.0, 25.13784345841927, 0.409603896676402, 0.0, 1.0, 18706.99856053889], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2994600.0000, 
sim time next is 2995200.0000, 
raw observation next is [-1.0, 55.0, 69.5, 570.5, 26.0, 25.14849950862115, 0.4078059009112123, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.55, 0.23166666666666666, 0.6303867403314917, 0.6666666666666666, 0.5957082923850958, 0.6359353003037375, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.4560559], dtype=float32), -0.42459425]. 
=============================================
[2019-04-04 16:15:34,604] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.5931325e-08 5.1584075e-09 2.3989003e-13 1.2686008e-12 9.9999988e-01
 8.6026324e-09 2.0420299e-13], sum to 1.0000
[2019-04-04 16:15:34,606] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9227
[2019-04-04 16:15:34,621] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.33333333333334, 83.0, 0.0, 0.0, 26.0, 23.23859888825698, -0.07442576054004162, 0.0, 1.0, 43693.10650337339], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2697600.0000, 
sim time next is 2698200.0000, 
raw observation next is [-15.5, 83.0, 0.0, 0.0, 26.0, 23.23170491496091, -0.08618049970495417, 0.0, 1.0, 43575.22849479772], 
processed observation next is [1.0, 0.21739130434782608, 0.033240997229916885, 0.83, 0.0, 0.0, 0.6666666666666666, 0.4359754095800759, 0.47127316676501524, 0.0, 1.0, 0.20750108807046533], 
reward next is 0.7925, 
noisyNet noise sample is [array([0.767061], dtype=float32), -0.828926]. 
=============================================
[2019-04-04 16:15:35,845] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7050663e-10 4.3777149e-10 1.1451812e-15 5.3158097e-14 1.0000000e+00
 2.0210858e-10 3.9248072e-15], sum to 1.0000
[2019-04-04 16:15:35,845] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5534
[2019-04-04 16:15:35,874] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.733333333333333, 51.66666666666667, 241.5, 151.0, 26.0, 25.77486686494326, 0.3909109622739632, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2636400.0000, 
sim time next is 2637000.0000, 
raw observation next is [-1.45, 50.5, 245.0, 147.0, 26.0, 25.71924457395604, 0.2802656686946031, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.422437673130194, 0.505, 0.8166666666666667, 0.16243093922651933, 0.6666666666666666, 0.6432703811630033, 0.5934218895648677, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.41707286], dtype=float32), 0.16975737]. 
=============================================
[2019-04-04 16:15:35,882] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[84.65167 ]
 [84.929   ]
 [85.148476]
 [85.30532 ]
 [85.43353 ]], R is [[84.50339508]
 [84.65836334]
 [84.81178284]
 [84.96366882]
 [85.11403656]].
[2019-04-04 16:15:40,781] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0173956e-08 1.9540458e-09 1.2195374e-14 1.3309557e-13 1.0000000e+00
 3.6981523e-10 5.4884967e-14], sum to 1.0000
[2019-04-04 16:15:40,784] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4042
[2019-04-04 16:15:40,793] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 39.5, 84.0, 673.0, 26.0, 25.12393866727817, 0.3661078196000065, 0.0, 1.0, 18696.50628669648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3079800.0000, 
sim time next is 3080400.0000, 
raw observation next is [0.6666666666666666, 39.66666666666666, 79.5, 641.8333333333334, 26.0, 25.14453710619336, 0.3744564724023651, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.4810710987996307, 0.39666666666666656, 0.265, 0.7092081031307551, 0.6666666666666666, 0.5953780921827801, 0.6248188241341217, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21707965], dtype=float32), 0.3549712]. 
=============================================
[2019-04-04 16:15:43,810] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0769668e-09 3.0849284e-10 5.8046278e-15 9.7588580e-15 1.0000000e+00
 3.4857348e-10 1.3011747e-15], sum to 1.0000
[2019-04-04 16:15:43,810] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6596
[2019-04-04 16:15:43,826] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.666666666666666, 86.66666666666667, 0.0, 0.0, 26.0, 25.21507537586745, 0.4375903904648803, 0.0, 1.0, 43287.81792890625], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3282000.0000, 
sim time next is 3282600.0000, 
raw observation next is [-6.833333333333334, 85.33333333333333, 0.0, 0.0, 26.0, 25.17846248563679, 0.4275299010193425, 0.0, 1.0, 43338.49519533444], 
processed observation next is [1.0, 1.0, 0.27331486611265005, 0.8533333333333333, 0.0, 0.0, 0.6666666666666666, 0.5982052071363991, 0.6425099670064475, 0.0, 1.0, 0.20637378664444972], 
reward next is 0.7936, 
noisyNet noise sample is [array([0.86232436], dtype=float32), 1.2384272]. 
=============================================
[2019-04-04 16:15:46,402] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.03894510e-09 4.19096341e-10 1.79872596e-14 1.24964905e-14
 1.00000000e+00 8.61270943e-10 1.41009225e-14], sum to 1.0000
[2019-04-04 16:15:46,403] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7931
[2019-04-04 16:15:46,448] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.666666666666667, 39.33333333333334, 0.0, 0.0, 26.0, 25.29262542137564, 0.4006014867821659, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2834400.0000, 
sim time next is 2835000.0000, 
raw observation next is [2.5, 40.5, 0.0, 0.0, 26.0, 25.41454799667424, 0.3943384816616369, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5318559556786704, 0.405, 0.0, 0.0, 0.6666666666666666, 0.6178789997228534, 0.6314461605538789, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7909447], dtype=float32), 0.42910716]. 
=============================================
[2019-04-04 16:15:46,465] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.641975]
 [83.495346]
 [83.60278 ]
 [83.207695]
 [82.16252 ]], R is [[81.84087372]
 [82.02246857]
 [82.20224762]
 [81.96289825]
 [81.19226074]].
[2019-04-04 16:15:54,095] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.0956560e-09 1.1295003e-10 4.9270005e-15 1.9673355e-14 1.0000000e+00
 5.7688487e-10 2.2390619e-15], sum to 1.0000
[2019-04-04 16:15:54,096] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9179
[2019-04-04 16:15:54,119] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.96170169489411, 0.3695505450696226, 0.0, 1.0, 43424.40573680677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2936400.0000, 
sim time next is 2937000.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 25.01016376543799, 0.3650717017741769, 0.0, 1.0, 43370.72802736415], 
processed observation next is [1.0, 1.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5841803137864993, 0.6216905672580589, 0.0, 1.0, 0.2065272763207817], 
reward next is 0.7935, 
noisyNet noise sample is [array([0.37691012], dtype=float32), -0.008666893]. 
=============================================
[2019-04-04 16:15:54,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.682724]
 [82.7607  ]
 [82.684975]
 [82.73448 ]
 [82.65383 ]], R is [[82.59144592]
 [82.55875397]
 [82.52619171]
 [82.49370575]
 [82.46121979]].
[2019-04-04 16:15:54,766] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2148022e-09 2.8124350e-10 2.6940535e-15 1.7515842e-14 1.0000000e+00
 3.1118172e-10 1.7358520e-15], sum to 1.0000
[2019-04-04 16:15:54,769] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7302
[2019-04-04 16:15:54,786] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.21945494825468, 0.3898761112925737, 0.0, 1.0, 41924.89455352424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3475800.0000, 
sim time next is 3476400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.26774394784553, 0.3860617360472606, 0.0, 1.0, 41906.98576001008], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6056453289871276, 0.6286872453490869, 0.0, 1.0, 0.19955707504766704], 
reward next is 0.8004, 
noisyNet noise sample is [array([0.36873233], dtype=float32), 0.6654169]. 
=============================================
[2019-04-04 16:15:54,842] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5568676e-09 5.4768690e-10 4.6963217e-15 2.9563246e-14 1.0000000e+00
 2.1871580e-10 1.1492912e-14], sum to 1.0000
[2019-04-04 16:15:54,845] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6233
[2019-04-04 16:15:54,860] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.91119044940651, 0.2616550216187267, 0.0, 1.0, 38016.96600052259], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3022800.0000, 
sim time next is 3023400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.87637108815484, 0.253889389323029, 0.0, 1.0, 37972.96968153277], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5730309240129033, 0.5846297964410097, 0.0, 1.0, 0.18082366515015605], 
reward next is 0.8192, 
noisyNet noise sample is [array([0.6226866], dtype=float32), 2.005468]. 
=============================================
[2019-04-04 16:15:56,032] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6224973e-08 7.6358680e-10 1.5914687e-14 6.0711459e-14 1.0000000e+00
 7.2654944e-09 2.0003872e-14], sum to 1.0000
[2019-04-04 16:15:56,033] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6741
[2019-04-04 16:15:56,046] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 65.5, 0.0, 0.0, 26.0, 24.65540965378807, 0.2429058055935412, 0.0, 1.0, 42892.47305937942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3389400.0000, 
sim time next is 3390000.0000, 
raw observation next is [-3.666666666666667, 63.66666666666667, 0.0, 0.0, 26.0, 24.79544601669897, 0.2434178373407299, 0.0, 1.0, 42747.84890546789], 
processed observation next is [1.0, 0.21739130434782608, 0.3610341643582641, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5662871680582476, 0.5811392791135767, 0.0, 1.0, 0.2035611852641328], 
reward next is 0.7964, 
noisyNet noise sample is [array([1.4520427], dtype=float32), 0.24465781]. 
=============================================
[2019-04-04 16:15:56,054] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.172714]
 [77.246155]
 [77.33691 ]
 [77.40026 ]
 [77.4543  ]], R is [[77.13792419]
 [77.16230011]
 [77.18751526]
 [77.21216583]
 [77.23868561]].
[2019-04-04 16:15:56,519] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3309103e-09 2.1880216e-10 1.8821089e-15 9.0383540e-15 1.0000000e+00
 6.4796085e-11 9.7483233e-16], sum to 1.0000
[2019-04-04 16:15:56,520] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7989
[2019-04-04 16:15:56,579] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.5, 62.5, 2.0, 107.0, 26.0, 24.92115233006141, 0.3300567186641471, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3396600.0000, 
sim time next is 3397200.0000, 
raw observation next is [-2.333333333333333, 61.66666666666667, 16.16666666666666, 159.5, 26.0, 25.41409869212297, 0.3732327566930405, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3979686057248385, 0.6166666666666667, 0.05388888888888887, 0.17624309392265194, 0.6666666666666666, 0.6178415576769142, 0.6244109188976802, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9150126], dtype=float32), 0.12753206]. 
=============================================
[2019-04-04 16:16:01,313] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 16:16:01,313] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 16:16:01,314] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 16:16:01,314] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:16:01,314] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:16:01,314] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 16:16:01,315] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:16:01,319] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run44
[2019-04-04 16:16:01,344] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run44
[2019-04-04 16:16:01,365] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run44
[2019-04-04 16:16:19,459] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18365844], dtype=float32), 0.23874658]
[2019-04-04 16:16:19,459] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-13.41764931, 51.83485546, 0.0, 0.0, 26.0, 24.10500084467616, 0.04031287076175491, 0.0, 1.0, 45329.12958440668]
[2019-04-04 16:16:19,459] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 16:16:19,460] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.8632555e-08 6.4698829e-09 7.1430622e-13 2.6179720e-12 9.9999988e-01
 7.2730084e-09 8.6920743e-13], sampled 0.26637349389144793
[2019-04-04 16:17:26,287] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.18365844], dtype=float32), 0.23874658]
[2019-04-04 16:17:26,288] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.066666666666667, 49.83333333333334, 0.0, 0.0, 26.0, 25.26326696787693, 0.5247899745674492, 0.0, 1.0, 62168.32927261296]
[2019-04-04 16:17:26,288] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 16:17:26,289] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.1979358e-09 3.3116909e-10 5.7205247e-15 3.8639902e-14 1.0000000e+00
 3.8346010e-10 7.8751863e-15], sampled 0.0372646439700961
[2019-04-04 16:17:42,000] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 16:18:02,269] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 16:18:03,857] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 16:18:04,881] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 4300000, evaluation results [4300000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 16:18:14,274] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.8504064e-09 2.2427702e-09 3.2278135e-14 5.1425325e-13 1.0000000e+00
 2.4152453e-09 1.6286097e-14], sum to 1.0000
[2019-04-04 16:18:14,276] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1734
[2019-04-04 16:18:14,292] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.89583828553497, 0.3187500087092634, 0.0, 1.0, 43869.76460736858], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3293400.0000, 
sim time next is 3294000.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.83977598718884, 0.3093039836941421, 0.0, 1.0, 43908.25010039363], 
processed observation next is [1.0, 0.13043478260869565, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5699813322657367, 0.6031013278980474, 0.0, 1.0, 0.20908690523996964], 
reward next is 0.7909, 
noisyNet noise sample is [array([0.5280239], dtype=float32), -1.2568626]. 
=============================================
[2019-04-04 16:18:14,300] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[76.62849 ]
 [76.849724]
 [76.98973 ]
 [77.11823 ]
 [77.3159  ]], R is [[76.53598785]
 [76.56172943]
 [76.58738708]
 [76.61278534]
 [76.63793182]].
[2019-04-04 16:18:17,727] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1342381e-10 3.3610701e-11 2.1487542e-16 1.7199799e-15 1.0000000e+00
 6.3660244e-11 1.9216467e-16], sum to 1.0000
[2019-04-04 16:18:17,728] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4399
[2019-04-04 16:18:17,753] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 71.0, 113.0, 795.5, 26.0, 26.42042521017812, 0.5675738566548758, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3754800.0000, 
sim time next is 3755400.0000, 
raw observation next is [-2.833333333333333, 70.0, 113.6666666666667, 804.3333333333334, 26.0, 26.40492278255343, 0.5734627790939955, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3841181902123731, 0.7, 0.378888888888889, 0.8887661141804789, 0.6666666666666666, 0.7004102318794526, 0.6911542596979986, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6942565], dtype=float32), 0.525491]. 
=============================================
[2019-04-04 16:18:22,219] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6312572e-09 1.1992549e-10 1.4574356e-15 8.4854050e-15 1.0000000e+00
 2.0513800e-09 3.0038520e-15], sum to 1.0000
[2019-04-04 16:18:22,221] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4678
[2019-04-04 16:18:22,232] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 75.0, 0.0, 0.0, 26.0, 25.91929329511269, 0.6035285081036788, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3533400.0000, 
sim time next is 3534000.0000, 
raw observation next is [-0.6666666666666666, 76.0, 0.0, 0.0, 26.0, 25.87519201918083, 0.5954230398723149, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.44413665743305636, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6562660015984024, 0.698474346624105, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.94286406], dtype=float32), -1.602992]. 
=============================================
[2019-04-04 16:18:22,243] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[81.60603 ]
 [81.676254]
 [81.66629 ]
 [81.59984 ]
 [81.41621 ]], R is [[81.68510437]
 [81.86825562]
 [82.04957581]
 [82.2290802 ]
 [82.40679169]].
[2019-04-04 16:18:28,299] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3601570e-09 5.6710220e-10 2.2685004e-14 1.4807216e-13 1.0000000e+00
 1.8474934e-09 3.9050956e-14], sum to 1.0000
[2019-04-04 16:18:28,302] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7215
[2019-04-04 16:18:28,316] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.36522951713013, 0.4470787477215231, 0.0, 1.0, 56671.10350364526], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3546000.0000, 
sim time next is 3546600.0000, 
raw observation next is [-2.166666666666667, 61.83333333333333, 0.0, 0.0, 26.0, 25.33771420791134, 0.4425421275368251, 0.0, 1.0, 48188.34168120848], 
processed observation next is [0.0, 0.043478260869565216, 0.4025854108956602, 0.6183333333333333, 0.0, 0.0, 0.6666666666666666, 0.6114761839926116, 0.6475140425122751, 0.0, 1.0, 0.2294682937200404], 
reward next is 0.7705, 
noisyNet noise sample is [array([-1.2955289], dtype=float32), 1.1083411]. 
=============================================
[2019-04-04 16:18:29,254] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.97534944e-09 1.78594750e-10 6.34084206e-15 6.15110583e-14
 1.00000000e+00 3.47708917e-10 1.05487425e-14], sum to 1.0000
[2019-04-04 16:18:29,256] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0989
[2019-04-04 16:18:29,265] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.166666666666667, 49.5, 79.33333333333334, 644.0, 26.0, 25.50086354021177, 0.4811387465583408, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3685800.0000, 
sim time next is 3686400.0000, 
raw observation next is [5.0, 50.0, 75.5, 613.5, 26.0, 25.49181742354188, 0.4776236168054471, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6011080332409973, 0.5, 0.25166666666666665, 0.6779005524861879, 0.6666666666666666, 0.6243181186284902, 0.6592078722684823, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3172379], dtype=float32), 1.5376558]. 
=============================================
[2019-04-04 16:18:30,228] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0956928e-09 5.2741911e-10 2.1600145e-15 6.1573264e-14 1.0000000e+00
 2.4524457e-10 9.5960964e-15], sum to 1.0000
[2019-04-04 16:18:30,231] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5418
[2019-04-04 16:18:30,244] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.42331593790172, 0.4558442200287259, 0.0, 1.0, 27994.75473199581], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3544800.0000, 
sim time next is 3545400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.386791325735, 0.4503159184502948, 0.0, 1.0, 54565.45376048211], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6155659438112501, 0.6501053061500982, 0.0, 1.0, 0.25983549409753387], 
reward next is 0.7402, 
noisyNet noise sample is [array([0.45832503], dtype=float32), 0.09403414]. 
=============================================
[2019-04-04 16:18:37,757] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0710827e-08 2.7531142e-09 3.1873541e-15 6.3404584e-14 1.0000000e+00
 4.3130788e-09 1.9857290e-14], sum to 1.0000
[2019-04-04 16:18:37,758] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5416
[2019-04-04 16:18:37,767] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.333333333333333, 45.66666666666667, 15.0, 149.1666666666667, 26.0, 26.44956132257613, 0.649785861337591, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3865200.0000, 
sim time next is 3865800.0000, 
raw observation next is [2.166666666666667, 46.83333333333333, 0.0, 0.0, 26.0, 26.25076543567939, 0.623012666396578, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5226223453370269, 0.46833333333333327, 0.0, 0.0, 0.6666666666666666, 0.6875637863066159, 0.7076708887988593, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1121644], dtype=float32), -0.59538543]. 
=============================================
[2019-04-04 16:18:38,488] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0031302e-09 1.5553531e-10 1.6939271e-16 1.3319551e-15 1.0000000e+00
 1.2173879e-10 3.0312120e-16], sum to 1.0000
[2019-04-04 16:18:38,489] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7805
[2019-04-04 16:18:38,499] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 60.0, 117.0, 823.5, 26.0, 26.27323082813619, 0.5754859920270948, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3762000.0000, 
sim time next is 3762600.0000, 
raw observation next is [-0.8333333333333334, 60.0, 116.3333333333333, 821.6666666666666, 26.0, 26.18595233752092, 0.5621817119204694, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.43951985226223456, 0.6, 0.38777777777777767, 0.9079189686924494, 0.6666666666666666, 0.68216269479341, 0.6873939039734899, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8566902], dtype=float32), -0.16446644]. 
=============================================
[2019-04-04 16:18:40,108] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4162880e-08 1.3770738e-09 2.0139176e-13 1.4100822e-13 1.0000000e+00
 1.0294859e-09 1.0225640e-13], sum to 1.0000
[2019-04-04 16:18:40,110] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1552
[2019-04-04 16:18:40,122] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.95, 40.16666666666667, 38.66666666666666, 292.0, 26.0, 25.37276819825335, 0.3876040710761961, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4209000.0000, 
sim time next is 4209600.0000, 
raw observation next is [1.9, 40.33333333333334, 31.33333333333333, 227.5, 26.0, 25.28837358209701, 0.3638631904154543, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.515235457063712, 0.40333333333333343, 0.10444444444444442, 0.2513812154696133, 0.6666666666666666, 0.6073644651747507, 0.6212877301384848, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8622222], dtype=float32), -0.33838806]. 
=============================================
[2019-04-04 16:18:40,363] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0126654e-09 1.8393212e-10 8.1573730e-16 9.3329131e-15 1.0000000e+00
 1.5937685e-10 4.5209918e-16], sum to 1.0000
[2019-04-04 16:18:40,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4446
[2019-04-04 16:18:40,382] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.5, 62.5, 119.0, 829.0, 26.0, 26.52165887833812, 0.5952352113584488, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3760200.0000, 
sim time next is 3760800.0000, 
raw observation next is [-1.333333333333333, 61.66666666666667, 118.3333333333333, 827.1666666666667, 26.0, 26.41330772666328, 0.5852217052839573, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.42566943674976926, 0.6166666666666667, 0.3944444444444443, 0.9139963167587478, 0.6666666666666666, 0.70110897722194, 0.6950739017613191, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38488793], dtype=float32), -0.16900642]. 
=============================================
[2019-04-04 16:18:53,753] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.99260713e-09 9.26861254e-10 2.17367776e-15 1.02645214e-14
 1.00000000e+00 5.81198811e-10 2.51419157e-15], sum to 1.0000
[2019-04-04 16:18:53,757] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5853
[2019-04-04 16:18:53,795] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.666666666666667, 28.0, 114.6666666666667, 831.8333333333334, 26.0, 25.72185177863083, 0.6144264541652384, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4022400.0000, 
sim time next is 4023000.0000, 
raw observation next is [-3.5, 27.5, 114.0, 830.0, 26.0, 26.30503091981505, 0.662308395504141, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.36565096952908593, 0.275, 0.38, 0.9171270718232044, 0.6666666666666666, 0.6920859099845874, 0.720769465168047, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40679505], dtype=float32), 1.5482643]. 
=============================================
[2019-04-04 16:18:53,800] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.44423 ]
 [80.56113 ]
 [79.761795]
 [79.29727 ]
 [79.39482 ]], R is [[80.58687592]
 [80.78100586]
 [80.14561462]
 [79.83782196]
 [80.03944397]].
[2019-04-04 16:18:55,305] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.4331906e-10 9.3746441e-11 1.4673264e-15 3.9176718e-15 1.0000000e+00
 1.0676428e-10 3.6211384e-16], sum to 1.0000
[2019-04-04 16:18:55,309] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8559
[2019-04-04 16:18:55,324] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 30.0, 116.0, 830.0, 26.0, 26.0182354974869, 0.601183805232368, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4109400.0000, 
sim time next is 4110000.0000, 
raw observation next is [3.0, 30.33333333333333, 114.3333333333333, 824.0, 26.0, 25.62503902805559, 0.5887775489073009, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.3033333333333333, 0.381111111111111, 0.9104972375690608, 0.6666666666666666, 0.6354199190046325, 0.6962591829691003, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29928777], dtype=float32), -2.0671332]. 
=============================================
[2019-04-04 16:18:55,346] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[83.47244]
 [83.42094]
 [83.38654]
 [83.33442]
 [83.24082]], R is [[83.57991791]
 [83.74411774]
 [83.90667725]
 [84.06761169]
 [84.22693634]].
[2019-04-04 16:18:55,435] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0383771e-09 1.4409406e-10 1.0701335e-15 2.6846575e-14 1.0000000e+00
 2.4889671e-10 1.2629905e-15], sum to 1.0000
[2019-04-04 16:18:55,436] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2299
[2019-04-04 16:18:55,444] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 50.0, 127.0, 0.0, 26.0, 26.08128910576426, 0.4948255847780966, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4537800.0000, 
sim time next is 4538400.0000, 
raw observation next is [2.0, 50.66666666666666, 147.0, 7.999999999999998, 26.0, 25.95027183673363, 0.4788837386873139, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.5066666666666666, 0.49, 0.008839779005524859, 0.6666666666666666, 0.6625226530611359, 0.6596279128957713, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39244887], dtype=float32), -1.246022]. 
=============================================
[2019-04-04 16:19:03,626] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.76749779e-09 2.61364430e-10 1.21061015e-14 4.21707110e-14
 1.00000000e+00 2.13836782e-10 4.48086867e-15], sum to 1.0000
[2019-04-04 16:19:03,628] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8527
[2019-04-04 16:19:03,642] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 43.66666666666667, 0.0, 0.0, 26.0, 25.4605922537011, 0.3754897795999305, 0.0, 1.0, 63657.87193689325], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4223400.0000, 
sim time next is 4224000.0000, 
raw observation next is [1.0, 44.33333333333334, 0.0, 0.0, 26.0, 25.4327301756918, 0.3769240224889601, 0.0, 1.0, 62938.51109537486], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.4433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6193941813076499, 0.6256413408296534, 0.0, 1.0, 0.29970719569226123], 
reward next is 0.7003, 
noisyNet noise sample is [array([-0.4427465], dtype=float32), 1.1127621]. 
=============================================
[2019-04-04 16:19:03,648] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.98637]
 [79.8615 ]
 [79.79105]
 [79.88284]
 [80.04319]], R is [[79.93516541]
 [79.83267975]
 [79.8033905 ]
 [79.91610718]
 [80.11694336]].
[2019-04-04 16:19:03,743] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.7726716e-09 6.6627653e-10 1.2721664e-14 1.0435380e-13 1.0000000e+00
 3.5570891e-10 8.7554577e-16], sum to 1.0000
[2019-04-04 16:19:03,744] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2604
[2019-04-04 16:19:03,760] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 49.0, 55.0, 26.5, 26.0, 25.31731681964389, 0.3284558695620488, 0.0, 1.0, 39003.59232193655], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4262400.0000, 
sim time next is 4263000.0000, 
raw observation next is [3.0, 49.66666666666667, 73.33333333333334, 35.33333333333334, 26.0, 25.3200537156521, 0.3338228228280108, 0.0, 1.0, 38912.68561880826], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.4966666666666667, 0.24444444444444446, 0.03904235727440148, 0.6666666666666666, 0.6100044763043417, 0.6112742742760036, 0.0, 1.0, 0.185298502946706], 
reward next is 0.8147, 
noisyNet noise sample is [array([0.8165303], dtype=float32), 1.3940979]. 
=============================================
[2019-04-04 16:19:03,767] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[83.46786]
 [83.01611]
 [82.67376]
 [82.48452]
 [82.48667]], R is [[83.96401978]
 [83.93865204]
 [83.91319275]
 [83.88774872]
 [83.86243439]].
[2019-04-04 16:19:05,268] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0426235e-08 2.2975242e-09 1.5567656e-14 1.9147375e-14 1.0000000e+00
 2.2897897e-09 1.2443878e-14], sum to 1.0000
[2019-04-04 16:19:05,273] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0849
[2019-04-04 16:19:05,287] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.166666666666667, 42.83333333333334, 169.0, 388.3333333333334, 26.0, 25.0931981429602, 0.4073776579417943, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4201800.0000, 
sim time next is 4202400.0000, 
raw observation next is [2.333333333333333, 41.66666666666667, 164.5, 463.1666666666667, 26.0, 25.20113628922448, 0.4295205542339202, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5272391505078486, 0.41666666666666674, 0.5483333333333333, 0.5117863720073665, 0.6666666666666666, 0.6000946907687066, 0.6431735180779734, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5376933], dtype=float32), 1.5228013]. 
=============================================
[2019-04-04 16:19:07,029] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7673644e-10 2.4276574e-11 1.6232329e-17 3.0829426e-16 1.0000000e+00
 1.4845627e-12 1.6349221e-17], sum to 1.0000
[2019-04-04 16:19:07,031] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4547
[2019-04-04 16:19:07,046] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 31.0, 120.0, 828.0, 26.0, 27.85434041813352, 0.9436665739078233, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4361400.0000, 
sim time next is 4362000.0000, 
raw observation next is [14.2, 30.0, 119.6666666666667, 832.1666666666666, 26.0, 27.95418968979403, 0.9700577438978802, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8559556786703602, 0.3, 0.398888888888889, 0.9195211786372007, 0.6666666666666666, 0.8295158074828359, 0.8233525812992935, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04037806], dtype=float32), -0.18458332]. 
=============================================
[2019-04-04 16:19:07,058] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[96.058716]
 [95.84062 ]
 [95.70789 ]
 [95.62714 ]
 [95.58062 ]], R is [[96.35083008]
 [96.38732147]
 [96.42344666]
 [96.45921326]
 [96.49462128]].
[2019-04-04 16:19:15,983] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1000060e-08 3.3390546e-09 1.5562906e-14 5.4062841e-13 1.0000000e+00
 1.5708419e-09 2.8167615e-14], sum to 1.0000
[2019-04-04 16:19:15,983] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6589
[2019-04-04 16:19:15,999] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 47.33333333333333, 0.0, 0.0, 26.0, 25.21328554463949, 0.2737075932498869, 0.0, 1.0, 38354.71879419986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4941600.0000, 
sim time next is 4942200.0000, 
raw observation next is [-2.0, 46.66666666666667, 0.0, 0.0, 26.0, 25.19327901870772, 0.2702674551648435, 0.0, 1.0, 38402.94039040452], 
processed observation next is [1.0, 0.17391304347826086, 0.40720221606648205, 0.46666666666666673, 0.0, 0.0, 0.6666666666666666, 0.5994399182256434, 0.5900891517216146, 0.0, 1.0, 0.182871144716212], 
reward next is 0.8171, 
noisyNet noise sample is [array([1.2841613], dtype=float32), 1.2768141]. 
=============================================
[2019-04-04 16:19:18,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:18,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:18,354] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run33
[2019-04-04 16:19:19,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:19,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:19,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run33
[2019-04-04 16:19:24,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:24,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:24,161] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run33
[2019-04-04 16:19:25,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:25,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:25,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run33
[2019-04-04 16:19:28,874] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.4987973e-09 1.9851952e-10 7.2300382e-16 1.5501137e-14 1.0000000e+00
 4.0434746e-11 7.3249563e-15], sum to 1.0000
[2019-04-04 16:19:28,875] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2934
[2019-04-04 16:19:28,889] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 93.0, 0.0, 0.0, 26.0, 23.94451579214939, 0.1098643347689528, 0.0, 1.0, 41838.28672744806], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4777200.0000, 
sim time next is 4777800.0000, 
raw observation next is [-6.166666666666667, 92.83333333333333, 0.0, 0.0, 26.0, 23.91174705682173, 0.1027463790175114, 0.0, 1.0, 41873.49629247133], 
processed observation next is [0.0, 0.30434782608695654, 0.2917820867959372, 0.9283333333333332, 0.0, 0.0, 0.6666666666666666, 0.49264558806847764, 0.5342487930058372, 0.0, 1.0, 0.19939760139272064], 
reward next is 0.8006, 
noisyNet noise sample is [array([-0.22743957], dtype=float32), 0.83839965]. 
=============================================
[2019-04-04 16:19:29,779] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0049522e-10 1.0154709e-11 4.3336849e-17 4.5414053e-16 1.0000000e+00
 1.5545336e-11 6.3226238e-17], sum to 1.0000
[2019-04-04 16:19:29,780] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0462
[2019-04-04 16:19:29,812] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.15, 87.0, 0.0, 0.0, 26.0, 24.72741507064501, 0.2322295403405835, 0.0, 1.0, 41220.00777357756], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 73800.0000, 
sim time next is 74400.0000, 
raw observation next is [1.966666666666667, 86.33333333333334, 0.0, 0.0, 26.0, 24.70785078188865, 0.2289149600841955, 0.0, 1.0, 40836.20769726911], 
processed observation next is [0.0, 0.8695652173913043, 0.5170821791320407, 0.8633333333333334, 0.0, 0.0, 0.6666666666666666, 0.5589875651573873, 0.5763049866947318, 0.0, 1.0, 0.19445813189175765], 
reward next is 0.8055, 
noisyNet noise sample is [array([0.16000783], dtype=float32), 0.854612]. 
=============================================
[2019-04-04 16:19:37,561] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.25055415e-08 1.38429468e-09 8.32255187e-15 1.40165698e-13
 1.00000000e+00 4.18635349e-10 7.81595840e-15], sum to 1.0000
[2019-04-04 16:19:37,568] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0849
[2019-04-04 16:19:37,581] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 50.0, 0.0, 0.0, 26.0, 25.26628704323304, 0.292921750733034, 0.0, 1.0, 39996.01001760258], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4936800.0000, 
sim time next is 4937400.0000, 
raw observation next is [-1.5, 50.0, 0.0, 0.0, 26.0, 25.23796087682423, 0.2845205912529093, 0.0, 1.0, 38825.56857207812], 
processed observation next is [1.0, 0.13043478260869565, 0.4210526315789474, 0.5, 0.0, 0.0, 0.6666666666666666, 0.603163406402019, 0.5948401970843031, 0.0, 1.0, 0.18488365986703867], 
reward next is 0.8151, 
noisyNet noise sample is [array([1.4587011], dtype=float32), -0.25868073]. 
=============================================
[2019-04-04 16:19:37,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:37,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:37,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run33
[2019-04-04 16:19:40,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:40,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:40,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run33
[2019-04-04 16:19:41,028] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3895546e-09 7.0748585e-10 8.8363805e-14 1.6406139e-13 1.0000000e+00
 6.9889539e-10 2.2568739e-14], sum to 1.0000
[2019-04-04 16:19:41,029] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8041
[2019-04-04 16:19:41,083] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.333333333333333, 47.33333333333333, 15.5, 93.33333333333331, 26.0, 25.34959820029953, 0.2671667975618244, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4952400.0000, 
sim time next is 4953000.0000, 
raw observation next is [-2.166666666666667, 46.66666666666667, 30.99999999999999, 186.6666666666666, 26.0, 25.2853267655452, 0.2732875079297988, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4025854108956602, 0.46666666666666673, 0.10333333333333329, 0.20626151012891336, 0.6666666666666666, 0.6071105637954334, 0.5910958359765995, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.94101083], dtype=float32), -0.2759123]. 
=============================================
[2019-04-04 16:19:41,094] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[79.48345 ]
 [78.179504]
 [76.863335]
 [75.35944 ]
 [75.45846 ]], R is [[81.1209259 ]
 [81.30971527]
 [81.49662018]
 [81.64195251]
 [81.64100647]].
[2019-04-04 16:19:42,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:42,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:42,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run33
[2019-04-04 16:19:43,695] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.85823329e-09 6.62493893e-10 1.05553434e-14 8.89086439e-15
 1.00000000e+00 3.86959575e-10 2.36348662e-15], sum to 1.0000
[2019-04-04 16:19:43,696] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6057
[2019-04-04 16:19:43,707] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.85, 19.0, 0.0, 0.0, 26.0, 26.93989040781812, 0.8161162524804394, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5088600.0000, 
sim time next is 5089200.0000, 
raw observation next is [8.799999999999999, 19.0, 0.0, 0.0, 26.0, 26.88264725414273, 0.8037245520315454, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7063711911357342, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7402206045118941, 0.7679081840105151, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5352368], dtype=float32), -0.5405794]. 
=============================================
[2019-04-04 16:19:44,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:44,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:44,096] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run33
[2019-04-04 16:19:44,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:44,378] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:44,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run33
[2019-04-04 16:19:44,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:44,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:44,964] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run33
[2019-04-04 16:19:45,410] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:45,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:45,414] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run33
[2019-04-04 16:19:45,626] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:45,627] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:45,632] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run33
[2019-04-04 16:19:45,872] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:45,872] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:45,876] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run33
[2019-04-04 16:19:46,092] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:46,092] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:46,095] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run33
[2019-04-04 16:19:47,443] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:47,443] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:47,446] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run33
[2019-04-04 16:19:48,322] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:19:48,322] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:19:48,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run33
[2019-04-04 16:20:07,285] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5907065e-07 8.6014182e-09 2.7577173e-13 2.3927097e-12 9.9999988e-01
 8.9132186e-09 1.6418754e-13], sum to 1.0000
[2019-04-04 16:20:07,287] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5811
[2019-04-04 16:20:07,332] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.87643701519136, -0.2140357019886231, 0.0, 1.0, 44859.09113292233], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 192600.0000, 
sim time next is 193200.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.85587122432072, -0.2268265116231744, 0.0, 1.0, 44904.32313048874], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4046559353600599, 0.4243911627922752, 0.0, 1.0, 0.21383011014518447], 
reward next is 0.7862, 
noisyNet noise sample is [array([1.0484148], dtype=float32), 1.3366351]. 
=============================================
[2019-04-04 16:20:09,759] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.00764241e-09 8.10814332e-11 4.91485256e-16 1.24820545e-14
 1.00000000e+00 4.99344621e-10 3.58396665e-15], sum to 1.0000
[2019-04-04 16:20:09,760] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4470
[2019-04-04 16:20:09,812] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.1, 60.5, 56.0, 0.0, 26.0, 25.68863868328954, 0.368193010126407, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 228600.0000, 
sim time next is 229200.0000, 
raw observation next is [-3.2, 61.0, 49.66666666666667, 0.0, 26.0, 25.90958840810787, 0.3794003033747428, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.37396121883656513, 0.61, 0.16555555555555557, 0.0, 0.6666666666666666, 0.6591323673423224, 0.626466767791581, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.98460454], dtype=float32), 0.11836622]. 
=============================================
[2019-04-04 16:20:13,696] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8391184e-09 2.1869034e-10 2.6977879e-14 7.3573329e-14 1.0000000e+00
 1.5072418e-09 6.8273761e-15], sum to 1.0000
[2019-04-04 16:20:13,696] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3942
[2019-04-04 16:20:13,710] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 80.83333333333334, 0.0, 0.0, 26.0, 24.37152588411204, 0.1466592689292625, 0.0, 1.0, 44204.07276081126], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 255000.0000, 
sim time next is 255600.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 24.38566997383884, 0.1410159234194928, 0.0, 1.0, 44220.48794776208], 
processed observation next is [1.0, 1.0, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5321391644865701, 0.5470053078064976, 0.0, 1.0, 0.21057375213220036], 
reward next is 0.7894, 
noisyNet noise sample is [array([0.6922225], dtype=float32), 0.9404661]. 
=============================================
[2019-04-04 16:20:20,929] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9918374e-09 1.0534930e-10 1.9032116e-16 4.0466914e-15 1.0000000e+00
 9.2877914e-11 1.1261460e-15], sum to 1.0000
[2019-04-04 16:20:20,930] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9770
[2019-04-04 16:20:20,984] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 76.0, 24.16666666666667, 0.0, 26.0, 25.5957000379225, 0.2795684418655094, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 721200.0000, 
sim time next is 721800.0000, 
raw observation next is [-2.3, 76.0, 29.0, 0.0, 26.0, 25.62488788658147, 0.2866839647982914, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.09666666666666666, 0.0, 0.6666666666666666, 0.6354073238817891, 0.5955613215994305, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33651194], dtype=float32), -1.6461854]. 
=============================================
[2019-04-04 16:20:29,325] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3186006e-08 2.4643714e-09 6.6870046e-14 2.1088212e-13 1.0000000e+00
 5.6250010e-10 3.0077005e-14], sum to 1.0000
[2019-04-04 16:20:29,328] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8154
[2019-04-04 16:20:29,389] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 35.0, 106.5, 0.0, 26.0, 25.57522878924646, 0.2512371426267092, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 482400.0000, 
sim time next is 483000.0000, 
raw observation next is [-0.5, 35.33333333333334, 102.3333333333333, 0.0, 26.0, 25.53809487908172, 0.1569319384306704, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.44875346260387816, 0.35333333333333344, 0.341111111111111, 0.0, 0.6666666666666666, 0.6281745732568099, 0.5523106461435568, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6403462], dtype=float32), -0.8910203]. 
=============================================
[2019-04-04 16:20:29,391] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.2133690e-09 5.0390653e-10 1.5343558e-14 1.3768819e-13 1.0000000e+00
 3.5348846e-10 1.1771564e-14], sum to 1.0000
[2019-04-04 16:20:29,392] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7575
[2019-04-04 16:20:29,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[76.51067 ]
 [76.61514 ]
 [76.728714]
 [76.880806]
 [76.78705 ]], R is [[76.57312775]
 [76.80739594]
 [77.0393219 ]
 [77.26892853]
 [77.27484894]].
[2019-04-04 16:20:29,450] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.2, 59.0, 0.0, 0.0, 26.0, 25.01062894572211, 0.323574584611505, 0.0, 1.0, 77117.9779619405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 764400.0000, 
sim time next is 765000.0000, 
raw observation next is [-5.3, 59.5, 0.0, 0.0, 26.0, 25.02814590646781, 0.3206801064475471, 0.0, 1.0, 57270.85033824471], 
processed observation next is [1.0, 0.8695652173913043, 0.31578947368421056, 0.595, 0.0, 0.0, 0.6666666666666666, 0.5856788255389841, 0.606893368815849, 0.0, 1.0, 0.2727183349440224], 
reward next is 0.7273, 
noisyNet noise sample is [array([-0.6729514], dtype=float32), -0.59816617]. 
=============================================
[2019-04-04 16:20:29,456] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[79.69298]
 [81.38216]
 [81.33005]
 [80.90082]
 [80.5708 ]], R is [[77.95262146]
 [77.80587006]
 [77.72298431]
 [77.41529846]
 [77.17982483]].
[2019-04-04 16:20:29,814] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9986778e-07 1.8828439e-08 2.2886677e-12 7.4648378e-12 9.9999976e-01
 6.2549255e-09 3.6707947e-13], sum to 1.0000
[2019-04-04 16:20:29,826] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9357
[2019-04-04 16:20:29,890] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 36.5, 23.0, 0.0, 26.0, 25.15166619543329, 0.1718082270720356, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 462600.0000, 
sim time next is 463200.0000, 
raw observation next is [-6.733333333333333, 35.33333333333334, 29.5, 0.0, 26.0, 25.36660695181106, 0.1877402394140734, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2760849492151431, 0.35333333333333344, 0.09833333333333333, 0.0, 0.6666666666666666, 0.6138839126509218, 0.5625800798046912, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5234468], dtype=float32), 0.263143]. 
=============================================
[2019-04-04 16:20:37,806] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.4797282e-09 2.3888547e-10 4.9610423e-15 1.8998985e-14 1.0000000e+00
 6.0706461e-11 1.6505714e-15], sum to 1.0000
[2019-04-04 16:20:37,806] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8493
[2019-04-04 16:20:37,864] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 68.0, 135.0, 51.0, 26.0, 25.18753304071021, 0.2427406720562342, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 639000.0000, 
sim time next is 639600.0000, 
raw observation next is [-3.899999999999999, 67.0, 129.1666666666667, 42.5, 26.0, 25.12818477152884, 0.2261491716400879, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.35457063711911363, 0.67, 0.4305555555555557, 0.04696132596685083, 0.6666666666666666, 0.5940153976274033, 0.5753830572133626, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13395588], dtype=float32), -1.3183562]. 
=============================================
[2019-04-04 16:20:38,071] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.2093969e-10 1.4143833e-10 6.3111811e-16 1.6032136e-14 1.0000000e+00
 1.2621991e-10 6.8922690e-16], sum to 1.0000
[2019-04-04 16:20:38,072] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4664
[2019-04-04 16:20:38,115] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 87.0, 0.0, 0.0, 26.0, 24.94347769943689, 0.2897663857754063, 0.0, 1.0, 56447.18051434342], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 581400.0000, 
sim time next is 582000.0000, 
raw observation next is [-2.1, 87.0, 0.0, 0.0, 26.0, 24.95239493852945, 0.2902188317208621, 0.0, 1.0, 45190.40318764975], 
processed observation next is [0.0, 0.7391304347826086, 0.404432132963989, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5793662448774543, 0.5967396105736207, 0.0, 1.0, 0.21519239613166546], 
reward next is 0.7848, 
noisyNet noise sample is [array([-0.22971971], dtype=float32), 0.4919185]. 
=============================================
[2019-04-04 16:20:38,118] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.59438 ]
 [81.37528 ]
 [81.34094 ]
 [81.517204]
 [81.55473 ]], R is [[81.77198792]
 [81.68547058]
 [81.64058685]
 [81.66913605]
 [81.7266922 ]].
[2019-04-04 16:20:48,773] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.4956734e-10 5.3366943e-11 6.4200386e-16 4.7400938e-15 1.0000000e+00
 1.0925959e-11 3.7101234e-16], sum to 1.0000
[2019-04-04 16:20:48,773] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6671
[2019-04-04 16:20:48,816] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.1, 73.33333333333334, 89.0, 40.83333333333334, 26.0, 25.85638004857996, 0.3285341627199098, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 724800.0000, 
sim time next is 725400.0000, 
raw observation next is [-2.0, 72.0, 101.0, 49.0, 26.0, 25.91605775664026, 0.3368523696921388, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.40720221606648205, 0.72, 0.33666666666666667, 0.05414364640883978, 0.6666666666666666, 0.6596714797200217, 0.6122841232307129, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2810055], dtype=float32), -1.7319783]. 
=============================================
[2019-04-04 16:20:59,703] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5734486e-09 3.8471573e-10 1.2909610e-14 3.1965489e-14 1.0000000e+00
 1.6153698e-09 1.5799636e-14], sum to 1.0000
[2019-04-04 16:20:59,703] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6489
[2019-04-04 16:20:59,726] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.09999999999999998, 72.0, 0.0, 0.0, 26.0, 24.55555474292931, 0.1551288372275779, 0.0, 1.0, 38869.95170859948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 885000.0000, 
sim time next is 885600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.49590564542908, 0.154421358325213, 0.0, 1.0, 38818.53853722116], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5413254704524233, 0.5514737861084044, 0.0, 1.0, 0.18485018351057694], 
reward next is 0.8151, 
noisyNet noise sample is [array([0.809732], dtype=float32), 0.3409756]. 
=============================================
[2019-04-04 16:21:04,353] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8119874e-10 5.3154096e-12 2.6172979e-16 9.6009978e-16 1.0000000e+00
 4.1609730e-11 5.5754620e-17], sum to 1.0000
[2019-04-04 16:21:04,354] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7846
[2019-04-04 16:21:04,392] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 24.64845458987838, 0.2953339551923961, 0.0, 1.0, 47947.87873243982], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 936600.0000, 
sim time next is 937200.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 24.67225078103023, 0.3103696567001642, 0.0, 1.0, 168195.256806504], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5560208984191858, 0.603456552233388, 0.0, 1.0, 0.8009297943166858], 
reward next is 0.1991, 
noisyNet noise sample is [array([0.080981], dtype=float32), -0.51655686]. 
=============================================
[2019-04-04 16:21:05,271] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.97107020e-09 1.23271102e-10 1.95245184e-16 2.39438765e-14
 1.00000000e+00 1.04197435e-10 4.22837259e-17], sum to 1.0000
[2019-04-04 16:21:05,273] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4233
[2019-04-04 16:21:05,303] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 24.70158379200804, 0.3368219642401173, 0.0, 1.0, 103057.1943746025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 937800.0000, 
sim time next is 938400.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 24.7814017232316, 0.3537146797952328, 0.0, 1.0, 61892.60461609864], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5651168102692999, 0.6179048932650776, 0.0, 1.0, 0.29472668864808876], 
reward next is 0.7053, 
noisyNet noise sample is [array([0.29188994], dtype=float32), -1.8106217]. 
=============================================
[2019-04-04 16:21:05,748] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.5248405e-11 3.3980634e-11 3.2234342e-17 3.1922927e-16 1.0000000e+00
 3.5234596e-11 6.6111563e-17], sum to 1.0000
[2019-04-04 16:21:05,751] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9847
[2019-04-04 16:21:05,766] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 95.0, 0.0, 0.0, 26.0, 25.69233237195553, 0.5990000713505521, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1647600.0000, 
sim time next is 1648200.0000, 
raw observation next is [7.1, 95.5, 0.0, 0.0, 26.0, 25.7737198326699, 0.5893497643287899, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6592797783933518, 0.955, 0.0, 0.0, 0.6666666666666666, 0.6478099860558251, 0.6964499214429299, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45598987], dtype=float32), -0.46715215]. 
=============================================
[2019-04-04 16:21:09,409] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6781200e-10 1.1609036e-11 1.2917570e-17 2.2222635e-16 1.0000000e+00
 1.0489030e-11 4.2251317e-17], sum to 1.0000
[2019-04-04 16:21:09,414] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6587
[2019-04-04 16:21:09,424] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.41666666666667, 72.0, 0.0, 0.0, 26.0, 25.73457136783875, 0.6408098685650018, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1123800.0000, 
sim time next is 1124400.0000, 
raw observation next is [11.23333333333333, 73.0, 0.0, 0.0, 26.0, 25.69369235832968, 0.6387205114542581, 0.0, 1.0, 59485.98662951997], 
processed observation next is [0.0, 0.0, 0.7737765466297323, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6411410298608068, 0.7129068371514194, 0.0, 1.0, 0.2832666029977141], 
reward next is 0.7167, 
noisyNet noise sample is [array([0.78730553], dtype=float32), -0.2797229]. 
=============================================
[2019-04-04 16:21:19,185] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.6488668e-11 3.2782535e-12 4.4100125e-18 4.6659805e-16 1.0000000e+00
 1.0453679e-11 3.2608942e-18], sum to 1.0000
[2019-04-04 16:21:19,185] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4680
[2019-04-04 16:21:19,220] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 73.5, 0.0, 26.0, 26.06933093980359, 0.5858146544161991, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1332000.0000, 
sim time next is 1332600.0000, 
raw observation next is [0.6000000000000001, 92.0, 83.00000000000001, 0.0, 26.0, 26.08377060801283, 0.5894985798686035, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.479224376731302, 0.92, 0.2766666666666667, 0.0, 0.6666666666666666, 0.6736475506677359, 0.6964995266228678, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15837145], dtype=float32), 0.82873225]. 
=============================================
[2019-04-04 16:21:19,696] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1367166e-10 1.6260732e-11 1.1710282e-16 7.9284454e-16 1.0000000e+00
 1.2078241e-10 1.7475838e-17], sum to 1.0000
[2019-04-04 16:21:19,696] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5461
[2019-04-04 16:21:19,747] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.82650199266327, 0.4745547060503221, 1.0, 1.0, 173384.5018540667], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1363800.0000, 
sim time next is 1364400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.95386693081405, 0.4906541637273035, 1.0, 1.0, 25897.31252789599], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5794889109011709, 0.6635513879091012, 1.0, 1.0, 0.12332053584712376], 
reward next is 0.8767, 
noisyNet noise sample is [array([-1.646676], dtype=float32), -0.26385844]. 
=============================================
[2019-04-04 16:21:28,413] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.3305297e-10 5.2554100e-11 2.6717513e-16 2.8839867e-15 1.0000000e+00
 1.8228081e-11 4.1761167e-16], sum to 1.0000
[2019-04-04 16:21:28,423] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9064
[2019-04-04 16:21:28,430] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.566666666666666, 71.0, 0.0, 0.0, 26.0, 25.60369169868568, 0.5626294930699033, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1540200.0000, 
sim time next is 1540800.0000, 
raw observation next is [7.2, 73.0, 0.0, 0.0, 26.0, 25.51259354773389, 0.5503059142422683, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.662049861495845, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6260494623111574, 0.6834353047474228, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7052638], dtype=float32), -0.25367922]. 
=============================================
[2019-04-04 16:21:28,535] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9826047e-10 3.8733079e-11 5.3596605e-17 2.8570493e-15 1.0000000e+00
 6.5658082e-12 5.7266541e-17], sum to 1.0000
[2019-04-04 16:21:28,537] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2175
[2019-04-04 16:21:28,566] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.383333333333333, 99.33333333333334, 64.33333333333333, 0.0, 26.0, 26.07092028861077, 0.5225919103896128, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1505400.0000, 
sim time next is 1506000.0000, 
raw observation next is [2.566666666666667, 98.66666666666667, 68.66666666666667, 0.0, 26.0, 26.07536368925812, 0.5281367598658216, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5337026777469991, 0.9866666666666667, 0.2288888888888889, 0.0, 0.6666666666666666, 0.6729469741048432, 0.6760455866219406, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08822163], dtype=float32), 1.476738]. 
=============================================
[2019-04-04 16:21:28,581] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[92.89441]
 [92.94083]
 [92.93567]
 [93.04994]
 [93.09094]], R is [[92.94728851]
 [93.01781464]
 [93.08763885]
 [93.15676117]
 [93.22519684]].
[2019-04-04 16:21:29,296] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.9785421e-09 1.7027603e-09 2.2264308e-15 1.1658285e-13 1.0000000e+00
 1.3948105e-10 1.1250945e-14], sum to 1.0000
[2019-04-04 16:21:29,297] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1594
[2019-04-04 16:21:29,332] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.733333333333333, 84.0, 74.5, 0.0, 26.0, 25.53170063397243, 0.291092646269304, 1.0, 1.0, 18721.93193307729], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2022000.0000, 
sim time next is 2022600.0000, 
raw observation next is [-5.666666666666667, 83.5, 80.0, 0.0, 26.0, 25.56227599053554, 0.2913263285630277, 1.0, 1.0, 23670.91528807987], 
processed observation next is [1.0, 0.391304347826087, 0.30563250230840255, 0.835, 0.26666666666666666, 0.0, 0.6666666666666666, 0.6301896658779617, 0.5971087761876759, 1.0, 1.0, 0.11271864422895175], 
reward next is 0.8873, 
noisyNet noise sample is [array([-0.07801556], dtype=float32), 0.79485583]. 
=============================================
[2019-04-04 16:21:30,639] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7294363e-09 1.2215093e-10 4.4777881e-17 4.4832088e-16 1.0000000e+00
 1.3440594e-11 1.1138876e-16], sum to 1.0000
[2019-04-04 16:21:30,640] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6135
[2019-04-04 16:21:30,655] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.666666666666667, 95.0, 85.5, 590.0, 26.0, 26.19654312208585, 0.6130452562119274, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1509600.0000, 
sim time next is 1510200.0000, 
raw observation next is [3.85, 94.5, 88.0, 708.0, 26.0, 26.27395995342153, 0.6351616017938336, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.569252077562327, 0.945, 0.29333333333333333, 0.7823204419889502, 0.6666666666666666, 0.6894966627851277, 0.7117205339312779, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16758479], dtype=float32), 2.0960743]. 
=============================================
[2019-04-04 16:21:30,706] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2797017e-09 8.3178402e-11 2.5567157e-17 2.6451309e-16 1.0000000e+00
 8.8406209e-12 6.5132301e-17], sum to 1.0000
[2019-04-04 16:21:30,707] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1205
[2019-04-04 16:21:30,735] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.033333333333333, 94.0, 90.0, 706.6666666666666, 26.0, 26.34012007023141, 0.4875533163716628, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1510800.0000, 
sim time next is 1511400.0000, 
raw observation next is [4.216666666666667, 93.5, 92.0, 705.3333333333334, 26.0, 26.33059330236475, 0.6457535270026376, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5794090489381348, 0.935, 0.30666666666666664, 0.7793738489871087, 0.6666666666666666, 0.6942161085303958, 0.7152511756675458, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16758479], dtype=float32), 2.0960743]. 
=============================================
[2019-04-04 16:21:32,514] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.1440358e-10 2.1377875e-11 8.1282638e-17 5.4611216e-16 1.0000000e+00
 7.8450198e-12 6.4533058e-17], sum to 1.0000
[2019-04-04 16:21:32,516] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6123
[2019-04-04 16:21:32,532] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.966666666666667, 92.0, 49.16666666666667, 0.0, 26.0, 25.80962680398824, 0.5479880991806702, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1675200.0000, 
sim time next is 1675800.0000, 
raw observation next is [1.85, 92.0, 53.0, 0.0, 26.0, 25.88313747963912, 0.5534494344879923, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5138504155124655, 0.92, 0.17666666666666667, 0.0, 0.6666666666666666, 0.6569281233032601, 0.6844831448293308, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6249217], dtype=float32), 0.9718331]. 
=============================================
[2019-04-04 16:21:33,568] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2618860e-10 1.9671022e-11 1.3077469e-17 2.0266924e-16 1.0000000e+00
 5.9167782e-11 8.2097486e-18], sum to 1.0000
[2019-04-04 16:21:33,568] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4380
[2019-04-04 16:21:33,581] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.54808700039857, 0.5774333991883701, 0.0, 1.0, 18746.80632975739], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1654200.0000, 
sim time next is 1654800.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.62177001203582, 0.5734418037391309, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6351475010029851, 0.6911472679130437, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7494282], dtype=float32), 0.75700206]. 
=============================================
[2019-04-04 16:21:35,540] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.4763671e-10 2.9869029e-10 4.2189012e-16 1.6805317e-14 1.0000000e+00
 5.6739620e-11 9.2370288e-16], sum to 1.0000
[2019-04-04 16:21:35,541] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5718
[2019-04-04 16:21:35,561] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8999999999999999, 85.0, 0.0, 0.0, 26.0, 24.98323077973184, 0.3633686343987776, 0.0, 1.0, 43639.16557818901], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1747800.0000, 
sim time next is 1748400.0000, 
raw observation next is [-1.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.97014485707519, 0.3563168636361345, 0.0, 1.0, 43686.58880193325], 
processed observation next is [0.0, 0.21739130434782608, 0.4349030470914128, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5808454047562659, 0.6187722878787115, 0.0, 1.0, 0.2080313752473012], 
reward next is 0.7920, 
noisyNet noise sample is [array([1.2257531], dtype=float32), 0.101794384]. 
=============================================
[2019-04-04 16:21:38,043] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.74765885e-10 4.74874584e-11 1.06457516e-16 3.32770192e-15
 1.00000000e+00 1.61440722e-11 1.64887761e-16], sum to 1.0000
[2019-04-04 16:21:38,043] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1823
[2019-04-04 16:21:38,056] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.03333333333333333, 91.0, 0.0, 0.0, 26.0, 25.25811779220126, 0.4286123428248299, 0.0, 1.0, 42983.26192506426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1738200.0000, 
sim time next is 1738800.0000, 
raw observation next is [0.0, 91.0, 0.0, 0.0, 26.0, 25.23818175507393, 0.4239944045019424, 0.0, 1.0, 43014.64197649722], 
processed observation next is [0.0, 0.13043478260869565, 0.46260387811634357, 0.91, 0.0, 0.0, 0.6666666666666666, 0.6031818129228276, 0.6413314681673141, 0.0, 1.0, 0.20483162845951058], 
reward next is 0.7952, 
noisyNet noise sample is [array([1.094217], dtype=float32), -0.06842343]. 
=============================================
[2019-04-04 16:21:40,742] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6717975e-09 2.5293503e-09 1.9801465e-14 9.9769578e-14 1.0000000e+00
 3.2813532e-10 1.7511967e-14], sum to 1.0000
[2019-04-04 16:21:40,743] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3782
[2019-04-04 16:21:40,760] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 79.0, 0.0, 0.0, 26.0, 24.53150546021012, 0.1924360573144774, 0.0, 1.0, 45542.41804920807], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1814400.0000, 
sim time next is 1815000.0000, 
raw observation next is [-5.100000000000001, 78.83333333333334, 0.0, 0.0, 26.0, 24.50227598250461, 0.1857034321543864, 0.0, 1.0, 45590.42883935147], 
processed observation next is [0.0, 0.0, 0.32132963988919666, 0.7883333333333334, 0.0, 0.0, 0.6666666666666666, 0.5418563318753842, 0.5619011440514622, 0.0, 1.0, 0.21709728018738797], 
reward next is 0.7829, 
noisyNet noise sample is [array([-1.1925917], dtype=float32), -1.7274777]. 
=============================================
[2019-04-04 16:21:40,767] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.26005]
 [79.45463]
 [79.60169]
 [79.74483]
 [79.88505]], R is [[79.13413239]
 [79.12592316]
 [79.1179657 ]
 [79.11019135]
 [79.10250092]].
[2019-04-04 16:21:41,105] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 16:21:41,108] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 16:21:41,108] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:21:41,108] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 16:21:41,109] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 16:21:41,109] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:21:41,109] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:21:41,114] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run45
[2019-04-04 16:21:41,143] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run45
[2019-04-04 16:21:41,161] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run45
[2019-04-04 16:21:49,138] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.17893884], dtype=float32), 0.23927796]
[2019-04-04 16:21:49,139] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.578542271666667, 59.08783457666667, 33.38897386333333, 61.57225904333333, 26.0, 25.39509394821037, 0.3464480965162242, 1.0, 1.0, 31868.71620467773]
[2019-04-04 16:21:49,139] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 16:21:49,140] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.9841891e-09 1.2733130e-09 2.3623251e-14 1.2881134e-13 1.0000000e+00
 1.0183231e-09 3.4140591e-14], sampled 0.17566068748393604
[2019-04-04 16:22:27,468] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17893884], dtype=float32), 0.23927796]
[2019-04-04 16:22:27,468] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [14.9, 63.0, 0.0, 0.0, 26.0, 25.88613162118968, 0.650756172430066, 0.0, 1.0, 0.0]
[2019-04-04 16:22:27,468] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 16:22:27,470] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.4029842e-09 1.7839568e-09 1.6727000e-14 3.5282714e-13 1.0000000e+00
 3.0478831e-10 4.5517901e-14], sampled 0.2507688362240873
[2019-04-04 16:23:22,083] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 16:23:41,003] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 16:23:44,263] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 16:23:45,287] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 4400000, evaluation results [4400000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 16:23:51,692] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9220016e-09 3.9289588e-10 5.9173898e-14 1.3158235e-13 1.0000000e+00
 9.4466435e-10 4.6196537e-14], sum to 1.0000
[2019-04-04 16:23:51,693] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0243
[2019-04-04 16:23:51,716] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 78.5, 0.0, 0.0, 26.0, 24.13183606154047, 0.05253270698211352, 0.0, 1.0, 45088.71409883568], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1902600.0000, 
sim time next is 1903200.0000, 
raw observation next is [-7.3, 77.33333333333334, 0.0, 0.0, 26.0, 24.13928440994148, 0.04695705221837337, 0.0, 1.0, 45104.12566466458], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5116070341617901, 0.5156523507394578, 0.0, 1.0, 0.21478155078411704], 
reward next is 0.7852, 
noisyNet noise sample is [array([0.63009083], dtype=float32), -0.06754249]. 
=============================================
[2019-04-04 16:24:10,105] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8666541e-09 2.8131752e-10 9.7757311e-16 1.0134879e-14 1.0000000e+00
 4.3229514e-10 2.2388984e-16], sum to 1.0000
[2019-04-04 16:24:10,105] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7101
[2019-04-04 16:24:10,143] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.466666666666667, 77.33333333333334, 222.1666666666667, 66.83333333333333, 26.0, 25.7989863798544, 0.3892462476684395, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2112000.0000, 
sim time next is 2112600.0000, 
raw observation next is [-7.383333333333333, 76.16666666666666, 236.3333333333333, 73.66666666666666, 26.0, 25.77627067562454, 0.38804024351933, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.25807940904893817, 0.7616666666666666, 0.7877777777777776, 0.08139963167587476, 0.6666666666666666, 0.6480225563020451, 0.6293467478397766, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0423644], dtype=float32), 0.3566677]. 
=============================================
[2019-04-04 16:24:18,957] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5684701e-09 4.7084603e-10 1.1521752e-14 2.1072812e-13 1.0000000e+00
 1.9015565e-09 4.4697485e-14], sum to 1.0000
[2019-04-04 16:24:18,958] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9838
[2019-04-04 16:24:19,013] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 71.0, 19.0, 0.0, 26.0, 25.64419987231317, 0.400212119657408, 1.0, 1.0, 44421.56753954262], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2221200.0000, 
sim time next is 2221800.0000, 
raw observation next is [-4.5, 70.5, 13.66666666666666, 0.0, 26.0, 25.68560474353259, 0.3129328095002157, 1.0, 1.0, 36275.1717695824], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.705, 0.04555555555555554, 0.0, 0.6666666666666666, 0.6404670619610492, 0.604310936500072, 1.0, 1.0, 0.1727389131884876], 
reward next is 0.8273, 
noisyNet noise sample is [array([2.732829], dtype=float32), -0.81706977]. 
=============================================
[2019-04-04 16:24:23,144] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0472702e-08 3.5002203e-09 1.8405503e-13 9.0036751e-13 1.0000000e+00
 6.5205157e-09 1.9474339e-13], sum to 1.0000
[2019-04-04 16:24:23,144] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4267
[2019-04-04 16:24:23,162] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.46256172473535, 0.2001795386215073, 0.0, 1.0, 44246.44014636074], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2246400.0000, 
sim time next is 2247000.0000, 
raw observation next is [-6.700000000000001, 77.5, 0.0, 0.0, 26.0, 24.417795168362, 0.1932522347808426, 0.0, 1.0, 44309.1788440236], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.775, 0.0, 0.0, 0.6666666666666666, 0.5348162640301668, 0.5644174115936141, 0.0, 1.0, 0.2109960897334457], 
reward next is 0.7890, 
noisyNet noise sample is [array([-1.1870908], dtype=float32), -1.3350714]. 
=============================================
[2019-04-04 16:24:23,184] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[75.70572]
 [75.8695 ]
 [75.99625]
 [76.13194]
 [76.2478 ]], R is [[75.65238953]
 [75.68516541]
 [75.71792603]
 [75.75056458]
 [75.78296661]].
[2019-04-04 16:24:25,486] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7716083e-08 2.5579454e-09 5.6747328e-14 1.8252006e-12 1.0000000e+00
 7.3984813e-10 2.9966088e-14], sum to 1.0000
[2019-04-04 16:24:25,488] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2680
[2019-04-04 16:24:25,501] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.68653063945685, 0.2182912949650862, 0.0, 1.0, 39302.48632719841], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2342400.0000, 
sim time next is 2343000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.64512216029369, 0.2108473974206465, 0.0, 1.0, 39421.29277041178], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.553760180024474, 0.5702824658068821, 0.0, 1.0, 0.18772044176386563], 
reward next is 0.8123, 
noisyNet noise sample is [array([1.1551356], dtype=float32), -0.44948876]. 
=============================================
[2019-04-04 16:24:25,508] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[78.175255]
 [78.192444]
 [77.846   ]
 [77.706795]
 [77.63303 ]], R is [[78.30828094]
 [78.33804321]
 [78.36811066]
 [78.39837646]
 [78.42871094]].
[2019-04-04 16:24:26,490] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2033700e-08 7.9143616e-09 2.4183102e-13 3.7163354e-12 1.0000000e+00
 5.3692801e-09 6.0143286e-13], sum to 1.0000
[2019-04-04 16:24:26,492] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4946
[2019-04-04 16:24:26,540] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.33333333333334, 83.0, 0.0, 0.0, 26.0, 23.23840890989992, -0.07448604657206552, 0.0, 1.0, 43693.1887575923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2697600.0000, 
sim time next is 2698200.0000, 
raw observation next is [-15.5, 83.0, 0.0, 0.0, 26.0, 23.23151621992661, -0.08624045724646244, 0.0, 1.0, 43575.31322947524], 
processed observation next is [1.0, 0.21739130434782608, 0.033240997229916885, 0.83, 0.0, 0.0, 0.6666666666666666, 0.43595968499388427, 0.4712531809178459, 0.0, 1.0, 0.20750149156892972], 
reward next is 0.7925, 
noisyNet noise sample is [array([0.3254183], dtype=float32), -0.41117302]. 
=============================================
[2019-04-04 16:24:32,985] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7078156e-08 2.4600977e-09 1.4034708e-13 4.8752175e-13 1.0000000e+00
 1.4135318e-09 1.2631653e-13], sum to 1.0000
[2019-04-04 16:24:32,986] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6637
[2019-04-04 16:24:33,035] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 28.0, 0.0, 0.0, 26.0, 24.9049334044124, 0.2225962910582121, 0.0, 1.0, 25416.22797062771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2484000.0000, 
sim time next is 2484600.0000, 
raw observation next is [0.9166666666666667, 28.33333333333334, 0.0, 0.0, 26.0, 24.92738763032844, 0.2196868391772389, 0.0, 1.0, 18719.20271372673], 
processed observation next is [0.0, 0.782608695652174, 0.48799630655586346, 0.2833333333333334, 0.0, 0.0, 0.6666666666666666, 0.57728230252737, 0.573228946392413, 0.0, 1.0, 0.08913906054155585], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.981614], dtype=float32), 0.73877114]. 
=============================================
[2019-04-04 16:24:33,434] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.5788905e-09 1.6687115e-09 1.3051231e-13 3.3467630e-13 1.0000000e+00
 4.0480392e-09 5.3440406e-14], sum to 1.0000
[2019-04-04 16:24:33,434] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0624
[2019-04-04 16:24:33,455] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 32.0, 0.0, 0.0, 26.0, 25.66008080435066, 0.174769518270306, 1.0, 1.0, 9340.205835115268], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2568600.0000, 
sim time next is 2569200.0000, 
raw observation next is [1.233333333333333, 33.0, 0.0, 0.0, 26.0, 25.22821627600324, 0.3076222972266268, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.7391304347826086, 0.49676823638042483, 0.33, 0.0, 0.0, 0.6666666666666666, 0.6023513563336035, 0.6025407657422089, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.35309198], dtype=float32), -0.78163654]. 
=============================================
[2019-04-04 16:24:37,401] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0495170e-08 1.3803241e-09 3.7211386e-15 1.8163985e-14 1.0000000e+00
 2.7544364e-10 3.3348818e-15], sum to 1.0000
[2019-04-04 16:24:37,402] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9305
[2019-04-04 16:24:37,428] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 62.0, 188.0, 223.0, 26.0, 25.72983946886018, 0.3624306617998936, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2631600.0000, 
sim time next is 2632200.0000, 
raw observation next is [-3.633333333333333, 60.66666666666667, 200.0, 205.6666666666667, 26.0, 25.68689787472823, 0.3663131294030944, 1.0, 1.0, 81163.7873413776], 
processed observation next is [1.0, 0.4782608695652174, 0.3619575253924285, 0.6066666666666667, 0.6666666666666666, 0.22725598526703505, 0.6666666666666666, 0.6405748228940192, 0.6221043764676981, 1.0, 1.0, 0.3864942254351314], 
reward next is 0.6135, 
noisyNet noise sample is [array([2.040772], dtype=float32), 1.5507959]. 
=============================================
[2019-04-04 16:24:40,858] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6595646e-08 2.1641697e-09 1.8053183e-14 1.9051756e-13 1.0000000e+00
 7.8810070e-10 2.7161687e-14], sum to 1.0000
[2019-04-04 16:24:40,859] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5477
[2019-04-04 16:24:40,871] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.4, 76.66666666666667, 0.0, 0.0, 26.0, 24.5597244414221, 0.1633768088989821, 0.0, 1.0, 42220.72690495487], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2605200.0000, 
sim time next is 2605800.0000, 
raw observation next is [-5.5, 77.33333333333333, 0.0, 0.0, 26.0, 24.52460153602203, 0.1586903278828207, 0.0, 1.0, 42318.73160743754], 
processed observation next is [1.0, 0.13043478260869565, 0.3102493074792244, 0.7733333333333333, 0.0, 0.0, 0.6666666666666666, 0.5437167946685024, 0.5528967759609402, 0.0, 1.0, 0.2015177695592264], 
reward next is 0.7985, 
noisyNet noise sample is [array([-0.09539691], dtype=float32), 1.0174196]. 
=============================================
[2019-04-04 16:24:42,614] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.30712879e-09 4.38901027e-10 5.35104224e-16 1.02261415e-14
 1.00000000e+00 4.31899343e-11 5.56940934e-15], sum to 1.0000
[2019-04-04 16:24:42,614] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7305
[2019-04-04 16:24:42,657] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666666, 55.66666666666667, 100.1666666666667, 655.6666666666667, 26.0, 25.31961299982201, 0.32367323728887, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3058800.0000, 
sim time next is 3059400.0000, 
raw observation next is [-4.333333333333333, 54.83333333333333, 101.3333333333333, 676.3333333333333, 26.0, 25.28355076342375, 0.3205824730220652, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.342566943674977, 0.5483333333333333, 0.3377777777777777, 0.747329650092081, 0.6666666666666666, 0.6069625636186459, 0.6068608243406884, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.56199604], dtype=float32), -0.20689306]. 
=============================================
[2019-04-04 16:24:48,387] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8252026e-11 2.0066793e-12 7.3553221e-19 2.2229999e-17 1.0000000e+00
 3.0879376e-13 2.4050553e-19], sum to 1.0000
[2019-04-04 16:24:48,390] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8257
[2019-04-04 16:24:48,419] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 100.0, 96.33333333333333, 604.5, 26.0, 26.37750165131248, 0.557150436802962, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3144000.0000, 
sim time next is 3144600.0000, 
raw observation next is [7.0, 100.0, 99.0, 647.0, 26.0, 26.425757168267, 0.5733669161989429, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.33, 0.7149171270718232, 0.6666666666666666, 0.7021464306889168, 0.6911223053996477, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48625124], dtype=float32), -1.5547667]. 
=============================================
[2019-04-04 16:24:56,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8461030e-09 2.5996649e-09 5.4972002e-15 5.5796405e-14 1.0000000e+00
 6.5931727e-10 2.7926232e-15], sum to 1.0000
[2019-04-04 16:24:56,891] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8605
[2019-04-04 16:24:56,922] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.5, 27.0, 118.0, 0.0, 26.0, 25.92091289639496, 0.4121785671108053, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2817000.0000, 
sim time next is 2817600.0000, 
raw observation next is [6.666666666666666, 26.0, 114.1666666666667, 0.0, 26.0, 25.85769130177954, 0.3072778393010481, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6472760849492153, 0.26, 0.38055555555555565, 0.0, 0.6666666666666666, 0.6548076084816282, 0.6024259464336826, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.595622], dtype=float32), -1.4993223]. 
=============================================
[2019-04-04 16:24:57,756] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3729196e-09 3.3722178e-11 1.4808550e-16 1.4258427e-15 1.0000000e+00
 2.0914149e-11 1.6626280e-16], sum to 1.0000
[2019-04-04 16:24:57,757] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4147
[2019-04-04 16:24:57,813] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 93.0, 34.99999999999999, 69.99999999999999, 26.0, 25.0028618658468, 0.265262834512584, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2879400.0000, 
sim time next is 2880000.0000, 
raw observation next is [2.0, 93.0, 52.5, 91.5, 26.0, 24.98835636064229, 0.2538481095845812, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.518005540166205, 0.93, 0.175, 0.1011049723756906, 0.6666666666666666, 0.5823630300535241, 0.5846160365281937, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.434563], dtype=float32), 0.28924248]. 
=============================================
[2019-04-04 16:24:57,819] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[92.26897 ]
 [91.062675]
 [89.87275 ]
 [88.571434]
 [87.09604 ]], R is [[93.41810608]
 [93.48392487]
 [93.54908752]
 [93.61359406]
 [93.67745972]].
[2019-04-04 16:25:05,554] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9115203e-09 1.1813069e-09 7.8879045e-15 4.0725398e-14 1.0000000e+00
 1.4900364e-10 6.2183886e-15], sum to 1.0000
[2019-04-04 16:25:05,554] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7559
[2019-04-04 16:25:05,578] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.07714131888929, 0.2991396163851277, 0.0, 1.0, 38453.80505484386], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3019200.0000, 
sim time next is 3019800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.03708636856776, 0.2914686970838749, 0.0, 1.0, 38372.66270633808], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5864238640473133, 0.5971562323612917, 0.0, 1.0, 0.18272696526827656], 
reward next is 0.8173, 
noisyNet noise sample is [array([-2.0170019], dtype=float32), 0.47566503]. 
=============================================
[2019-04-04 16:25:20,143] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1962841e-10 4.1112572e-11 6.5935046e-16 4.0261204e-15 1.0000000e+00
 6.6881119e-11 2.2927183e-16], sum to 1.0000
[2019-04-04 16:25:20,145] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0605
[2019-04-04 16:25:20,192] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666667, 75.0, 76.66666666666667, 397.3333333333333, 26.0, 25.36013395063827, 0.4162427444799803, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3831600.0000, 
sim time next is 3832200.0000, 
raw observation next is [-4.5, 74.0, 91.0, 447.0, 26.0, 25.55966662384668, 0.4589948921311067, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3379501385041552, 0.74, 0.30333333333333334, 0.49392265193370166, 0.6666666666666666, 0.6299722186538901, 0.6529982973770355, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07973479], dtype=float32), 0.13810848]. 
=============================================
[2019-04-04 16:25:34,454] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.61472684e-08 8.96373031e-10 1.01287725e-14 4.54865705e-14
 1.00000000e+00 5.49453039e-10 1.03570700e-14], sum to 1.0000
[2019-04-04 16:25:34,455] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5859
[2019-04-04 16:25:34,471] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.19808889259354, 0.3840323683396914, 0.0, 1.0, 41925.13190279879], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3475200.0000, 
sim time next is 3475800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.21945427633769, 0.3898760185386463, 0.0, 1.0, 41924.89315751389], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6016211896948075, 0.6299586728462154, 0.0, 1.0, 0.19964234836911376], 
reward next is 0.8004, 
noisyNet noise sample is [array([1.5640607], dtype=float32), -1.6250117]. 
=============================================
[2019-04-04 16:25:36,102] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.26660224e-10 1.65404856e-10 1.08147175e-16 1.05918410e-15
 1.00000000e+00 9.40408654e-11 2.20561741e-16], sum to 1.0000
[2019-04-04 16:25:36,104] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5660
[2019-04-04 16:25:36,127] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.166666666666667, 51.5, 115.3333333333333, 811.6666666666666, 26.0, 26.24545046086575, 0.6549440250324003, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3503400.0000, 
sim time next is 3504000.0000, 
raw observation next is [2.333333333333333, 51.0, 115.1666666666667, 808.8333333333334, 26.0, 26.35121527248686, 0.5623962004338697, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5272391505078486, 0.51, 0.383888888888889, 0.8937384898710866, 0.6666666666666666, 0.6959346060405718, 0.6874654001446232, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1042982], dtype=float32), 0.60726774]. 
=============================================
[2019-04-04 16:25:36,146] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[88.048515]
 [88.118095]
 [88.15711 ]
 [88.203415]
 [88.28262 ]], R is [[88.04537201]
 [88.16491699]
 [88.28327179]
 [88.4004364 ]
 [88.51643372]].
[2019-04-04 16:25:42,368] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2650515e-08 1.6519811e-09 5.5291461e-15 1.8813786e-13 1.0000000e+00
 4.4083487e-10 1.5960315e-14], sum to 1.0000
[2019-04-04 16:25:42,368] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6855
[2019-04-04 16:25:42,378] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.00049181090968, 0.3142414897220134, 0.0, 1.0, 43747.46604876549], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3816000.0000, 
sim time next is 3816600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.97928260234481, 0.309256991751385, 0.0, 1.0, 43818.65602690108], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5816068835287341, 0.6030856639171284, 0.0, 1.0, 0.20866026679476704], 
reward next is 0.7913, 
noisyNet noise sample is [array([0.18777718], dtype=float32), 0.13965191]. 
=============================================
[2019-04-04 16:25:54,562] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.0296906e-11 2.6493020e-12 1.3418883e-17 2.7801538e-16 1.0000000e+00
 3.3309965e-12 1.7799117e-17], sum to 1.0000
[2019-04-04 16:25:54,565] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9339
[2019-04-04 16:25:54,600] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1666666666666667, 70.16666666666667, 119.0, 22.0, 26.0, 26.14609533435241, 0.5261775150980134, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4525800.0000, 
sim time next is 4526400.0000, 
raw observation next is [0.3333333333333333, 68.33333333333334, 121.0, 11.0, 26.0, 26.17637667250271, 0.5235705215053835, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4718374884579871, 0.6833333333333335, 0.4033333333333333, 0.012154696132596685, 0.6666666666666666, 0.6813647227085591, 0.6745235071684612, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23882088], dtype=float32), 0.5686665]. 
=============================================
[2019-04-04 16:26:08,071] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9006210e-09 5.5626076e-10 2.5590310e-15 1.1545334e-14 1.0000000e+00
 3.0762204e-10 2.3534103e-15], sum to 1.0000
[2019-04-04 16:26:08,073] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1957
[2019-04-04 16:26:08,104] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 36.0, 34.66666666666666, 120.3333333333333, 26.0, 26.94615128396702, 0.5227945436096707, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4123200.0000, 
sim time next is 4123800.0000, 
raw observation next is [3.0, 35.5, 23.0, 57.0, 26.0, 26.75923698954552, 0.621544183768778, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.355, 0.07666666666666666, 0.06298342541436464, 0.6666666666666666, 0.7299364157954601, 0.7071813945895927, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2506659], dtype=float32), -0.25617665]. 
=============================================
[2019-04-04 16:26:15,160] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.2030240e-09 1.1589307e-10 9.4796995e-16 6.6622814e-14 1.0000000e+00
 1.7910712e-10 1.0241245e-15], sum to 1.0000
[2019-04-04 16:26:15,160] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2919
[2019-04-04 16:26:15,175] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.5, 80.5, 0.0, 0.0, 26.0, 25.13363831025312, 0.4103597405903348, 0.0, 1.0, 41771.59933595829], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4750200.0000, 
sim time next is 4750800.0000, 
raw observation next is [-3.666666666666667, 81.66666666666667, 0.0, 0.0, 26.0, 25.10098590304364, 0.4027348290658545, 0.0, 1.0, 41688.47630803732], 
processed observation next is [1.0, 1.0, 0.3610341643582641, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.5917488252536366, 0.6342449430219516, 0.0, 1.0, 0.1985165538477968], 
reward next is 0.8015, 
noisyNet noise sample is [array([-0.58021796], dtype=float32), 0.36989117]. 
=============================================
[2019-04-04 16:26:16,619] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.19106608e-10 1.03574805e-11 1.81719876e-18 1.61501284e-16
 1.00000000e+00 3.06052857e-11 2.03723319e-17], sum to 1.0000
[2019-04-04 16:26:16,619] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9648
[2019-04-04 16:26:16,639] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 26.0, 27.65435093386403, 0.9486084692514133, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4366200.0000, 
sim time next is 4366800.0000, 
raw observation next is [14.6, 29.0, 116.5, 847.5, 26.0, 26.80021988443928, 0.8912619638857318, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8670360110803325, 0.29, 0.3883333333333333, 0.93646408839779, 0.6666666666666666, 0.7333516570366067, 0.797087321295244, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04214642], dtype=float32), -0.3257456]. 
=============================================
[2019-04-04 16:26:21,943] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6957057e-10 1.6100414e-10 3.1680197e-15 6.2001973e-15 1.0000000e+00
 7.3565584e-11 2.5875456e-16], sum to 1.0000
[2019-04-04 16:26:21,945] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2542
[2019-04-04 16:26:22,004] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6666666666666667, 72.83333333333334, 74.00000000000001, 44.00000000000001, 26.0, 25.41036355312939, 0.4008030355904367, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4522200.0000, 
sim time next is 4522800.0000, 
raw observation next is [-0.5333333333333334, 72.66666666666667, 92.5, 55.0, 26.0, 25.31262803029769, 0.4536621607290998, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44783010156971376, 0.7266666666666667, 0.30833333333333335, 0.06077348066298342, 0.6666666666666666, 0.6093856691914743, 0.6512207202430332, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01091812], dtype=float32), -1.9420633]. 
=============================================
[2019-04-04 16:26:24,038] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:24,038] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:24,074] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run34
[2019-04-04 16:26:24,273] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6016540e-09 3.5252698e-10 2.3200461e-15 5.2828962e-15 1.0000000e+00
 4.7040255e-10 2.9897680e-15], sum to 1.0000
[2019-04-04 16:26:24,274] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3450
[2019-04-04 16:26:24,300] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.5, 51.0, 0.0, 0.0, 26.0, 26.4034021026214, 0.703426409010727, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4645800.0000, 
sim time next is 4646400.0000, 
raw observation next is [3.333333333333333, 51.66666666666666, 0.0, 0.0, 26.0, 26.36935321635586, 0.6963164191219818, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5549399815327793, 0.5166666666666666, 0.0, 0.0, 0.6666666666666666, 0.6974461013629885, 0.7321054730406606, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2931622], dtype=float32), 0.26859692]. 
=============================================
[2019-04-04 16:26:25,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:25,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:25,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run34
[2019-04-04 16:26:29,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.06799104e-10 9.70223415e-11 7.37091843e-17 3.56475298e-16
 1.00000000e+00 1.90372457e-11 4.35903801e-16], sum to 1.0000
[2019-04-04 16:26:29,411] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9541
[2019-04-04 16:26:29,428] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6666666666666666, 57.33333333333334, 134.8333333333333, 724.0, 26.0, 26.55444265845037, 0.6626655534905667, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4616400.0000, 
sim time next is 4617000.0000, 
raw observation next is [1.0, 56.0, 129.0, 767.0, 26.0, 26.64821513833766, 0.6714485132878815, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.56, 0.43, 0.8475138121546961, 0.6666666666666666, 0.7206845948614719, 0.7238161710959604, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18369676], dtype=float32), 1.9502783]. 
=============================================
[2019-04-04 16:26:29,433] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[91.29243 ]
 [91.50055 ]
 [91.6754  ]
 [91.802444]
 [91.85435 ]], R is [[91.17655182]
 [91.26478577]
 [91.35214233]
 [91.43862152]
 [91.52423859]].
[2019-04-04 16:26:30,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:30,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:30,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run34
[2019-04-04 16:26:32,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:32,735] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:32,768] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run34
[2019-04-04 16:26:39,839] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3032303e-09 2.3268679e-10 5.8892490e-15 1.0799228e-14 1.0000000e+00
 2.2858163e-11 2.2216967e-15], sum to 1.0000
[2019-04-04 16:26:39,842] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3561
[2019-04-04 16:26:39,887] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 56.0, 300.0, 164.0, 26.0, 25.13741411070457, 0.3281255832882717, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4876200.0000, 
sim time next is 4876800.0000, 
raw observation next is [-0.9333333333333333, 54.66666666666667, 297.1666666666666, 188.0, 26.0, 25.07791305109466, 0.3215223932088137, 0.0, 1.0, 18713.40456387374], 
processed observation next is [0.0, 0.43478260869565216, 0.4367497691597415, 0.5466666666666667, 0.9905555555555552, 0.20773480662983426, 0.6666666666666666, 0.5898260875912218, 0.6071741310696046, 0.0, 1.0, 0.08911145030416066], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.5851441], dtype=float32), -0.2498738]. 
=============================================
[2019-04-04 16:26:45,456] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:45,456] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:45,462] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run34
[2019-04-04 16:26:46,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:46,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:46,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run34
[2019-04-04 16:26:46,999] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.0558703e-10 2.9492887e-11 4.4003714e-16 2.8052008e-15 1.0000000e+00
 1.5730290e-11 1.0952638e-16], sum to 1.0000
[2019-04-04 16:26:46,999] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9967
[2019-04-04 16:26:47,064] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 47.0, 104.5, 615.5, 26.0, 26.4648692260104, 0.5930842015635752, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5043600.0000, 
sim time next is 5044200.0000, 
raw observation next is [1.333333333333333, 46.00000000000001, 107.0, 643.0, 26.0, 26.48413188075181, 0.6055358069028267, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4995383194829178, 0.4600000000000001, 0.3566666666666667, 0.7104972375690608, 0.6666666666666666, 0.7070109900626509, 0.701845268967609, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5098599], dtype=float32), -0.64742064]. 
=============================================
[2019-04-04 16:26:47,638] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:47,638] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:47,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run34
[2019-04-04 16:26:48,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:48,626] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:48,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run34
[2019-04-04 16:26:50,216] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:50,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:50,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run34
[2019-04-04 16:26:50,526] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:50,527] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:50,531] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run34
[2019-04-04 16:26:50,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:50,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:50,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run34
[2019-04-04 16:26:51,707] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:51,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:51,712] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run34
[2019-04-04 16:26:52,568] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:52,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:52,572] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run34
[2019-04-04 16:26:53,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:53,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:53,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run34
[2019-04-04 16:26:53,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:53,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:53,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run34
[2019-04-04 16:26:54,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:26:54,328] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:26:54,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run34
[2019-04-04 16:27:00,550] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6577736e-11 2.5320702e-11 2.3646296e-17 2.1380758e-16 1.0000000e+00
 3.1128945e-12 3.4666008e-17], sum to 1.0000
[2019-04-04 16:27:00,550] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2302
[2019-04-04 16:27:00,610] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 72.5, 0.0, 26.0, 24.2292305681491, 0.08142033091543659, 0.0, 1.0, 18781.81187885616], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 40800.0000, 
sim time next is 41400.0000, 
raw observation next is [7.7, 93.0, 75.0, 0.0, 26.0, 24.27588328884637, 0.08595440321509079, 0.0, 1.0, 18776.34583035036], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.25, 0.0, 0.6666666666666666, 0.5229902740705308, 0.5286514677383636, 0.0, 1.0, 0.089411170620716], 
reward next is 0.9106, 
noisyNet noise sample is [array([-0.47181872], dtype=float32), 0.24205963]. 
=============================================
[2019-04-04 16:27:00,830] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4470970e-09 8.6979549e-11 2.3012527e-16 1.6011074e-15 1.0000000e+00
 3.0460044e-11 5.5326429e-17], sum to 1.0000
[2019-04-04 16:27:00,831] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3299
[2019-04-04 16:27:00,880] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.9, 90.66666666666667, 92.5, 0.0, 26.0, 24.31302030504014, 0.1092863644306539, 0.0, 1.0, 35410.2030073198], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 44400.0000, 
sim time next is 45000.0000, 
raw observation next is [8.0, 89.5, 96.0, 0.0, 26.0, 24.33680959862573, 0.1155099110610531, 0.0, 1.0, 24220.52529228139], 
processed observation next is [0.0, 0.5217391304347826, 0.6842105263157896, 0.895, 0.32, 0.0, 0.6666666666666666, 0.5280674665521442, 0.5385033036870177, 0.0, 1.0, 0.11533583472514947], 
reward next is 0.8847, 
noisyNet noise sample is [array([0.5156756], dtype=float32), 0.48438963]. 
=============================================
[2019-04-04 16:27:00,884] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[90.68508 ]
 [90.56624 ]
 [90.40571 ]
 [90.234764]
 [90.12998 ]], R is [[90.71369934]
 [90.63794708]
 [90.54511261]
 [90.46961212]
 [90.45413208]].
[2019-04-04 16:27:12,527] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3232503e-09 1.6050924e-10 2.7288064e-15 2.0990327e-14 1.0000000e+00
 5.7685740e-10 4.4889951e-15], sum to 1.0000
[2019-04-04 16:27:12,528] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7740
[2019-04-04 16:27:12,584] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.299999999999999, 62.66666666666666, 0.0, 0.0, 26.0, 25.08833349017365, 0.3534617899370764, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 150600.0000, 
sim time next is 151200.0000, 
raw observation next is [-7.3, 61.0, 0.0, 0.0, 26.0, 25.2670052795049, 0.3632497698211629, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.26038781163434904, 0.61, 0.0, 0.0, 0.6666666666666666, 0.605583773292075, 0.6210832566070543, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39835224], dtype=float32), 0.21653907]. 
=============================================
[2019-04-04 16:27:14,202] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9133732e-09 1.1016994e-10 7.4093752e-16 5.0272058e-15 1.0000000e+00
 6.9981541e-11 9.9035694e-17], sum to 1.0000
[2019-04-04 16:27:14,203] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5221
[2019-04-04 16:27:14,223] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.783333333333333, 84.5, 0.0, 0.0, 26.0, 24.58400008269679, 0.1866279134998324, 0.0, 1.0, 40465.11093470645], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 535800.0000, 
sim time next is 536400.0000, 
raw observation next is [1.6, 85.0, 0.0, 0.0, 26.0, 24.55543412441036, 0.1816735689282826, 0.0, 1.0, 40547.69598100542], 
processed observation next is [0.0, 0.21739130434782608, 0.5069252077562327, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5462861770341968, 0.5605578563094276, 0.0, 1.0, 0.19308426657621627], 
reward next is 0.8069, 
noisyNet noise sample is [array([0.967249], dtype=float32), -0.26351613]. 
=============================================
[2019-04-04 16:27:24,996] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6219832e-09 3.2021816e-10 1.1149557e-14 3.7781972e-14 1.0000000e+00
 3.3273359e-10 9.7548753e-15], sum to 1.0000
[2019-04-04 16:27:24,997] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8551
[2019-04-04 16:27:25,064] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.683333333333334, 44.83333333333334, 87.0, 715.6666666666666, 26.0, 25.06819890024882, 0.4037670495192358, 1.0, 1.0, 102076.4181119362], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 305400.0000, 
sim time next is 306000.0000, 
raw observation next is [-9.5, 44.0, 89.0, 694.5, 26.0, 25.4794413497271, 0.4482817415489773, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.2966666666666667, 0.7674033149171271, 0.6666666666666666, 0.623286779143925, 0.6494272471829924, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4518137], dtype=float32), 1.2574989]. 
=============================================
[2019-04-04 16:27:25,067] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[81.10152 ]
 [80.97653 ]
 [80.362305]
 [79.92913 ]
 [80.27095 ]], R is [[80.86112976]
 [80.5664444 ]
 [79.80548859]
 [79.32764435]
 [79.53437042]].
[2019-04-04 16:27:30,615] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3931245e-09 1.6059171e-09 7.0853870e-15 1.5716740e-14 1.0000000e+00
 1.5576390e-10 6.1791121e-15], sum to 1.0000
[2019-04-04 16:27:30,615] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5122
[2019-04-04 16:27:30,647] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.89336096568624, 0.1915395110221566, 0.0, 1.0, 42266.39494510648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 681600.0000, 
sim time next is 682200.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.85352561417448, 0.1847764468168616, 0.0, 1.0, 42207.89583058372], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.57112713451454, 0.5615921489389538, 0.0, 1.0, 0.20098998014563677], 
reward next is 0.7990, 
noisyNet noise sample is [array([-0.5107617], dtype=float32), 0.13989675]. 
=============================================
[2019-04-04 16:27:34,265] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 16:27:34,271] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 16:27:34,271] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 16:27:34,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:27:34,272] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 16:27:34,273] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:27:34,273] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:27:34,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run46
[2019-04-04 16:27:34,950] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run46
[2019-04-04 16:27:34,974] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run46
[2019-04-04 16:27:53,544] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.18056396], dtype=float32), 0.24282533]
[2019-04-04 16:27:53,544] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-12.43333333333333, 68.66666666666666, 0.0, 0.0, 26.0, 22.59624406979377, -0.2874007626413372, 0.0, 1.0, 43973.51900842328]
[2019-04-04 16:27:53,544] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 16:27:53,545] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.1343271e-07 2.5230730e-08 1.0107476e-12 8.7257493e-12 9.9999988e-01
 1.1721121e-08 1.4458199e-12], sampled 0.48686648291905044
[2019-04-04 16:28:28,326] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.18056396], dtype=float32), 0.24282533]
[2019-04-04 16:28:28,326] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.6, 70.0, 0.0, 0.0, 26.0, 25.0398303417263, 0.402355526039484, 0.0, 1.0, 85938.23950478146]
[2019-04-04 16:28:28,327] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 16:28:28,328] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.2375828e-09 5.3459376e-10 7.6034251e-15 7.1943889e-14 1.0000000e+00
 6.4893829e-10 9.2033035e-15], sampled 0.6291187141400721
[2019-04-04 16:29:06,300] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18056396], dtype=float32), 0.24282533]
[2019-04-04 16:29:06,301] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.0, 79.0, 0.0, 0.0, 26.0, 25.15808459510772, 0.512016733080575, 0.0, 1.0, 101792.0516016514]
[2019-04-04 16:29:06,302] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 16:29:06,303] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.5083328e-10 4.0664916e-11 2.2617410e-16 3.3901393e-15 1.0000000e+00
 3.5556627e-11 4.9588072e-16], sampled 0.481704936878674
[2019-04-04 16:29:15,246] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 16:29:36,080] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 16:29:37,612] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 16:29:38,636] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 4500000, evaluation results [4500000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 16:29:44,136] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5482817e-09 4.1925845e-11 2.2696678e-15 5.6136289e-14 1.0000000e+00
 3.8454753e-11 1.1570681e-15], sum to 1.0000
[2019-04-04 16:29:44,137] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8269
[2019-04-04 16:29:44,170] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.9, 83.0, 0.0, 0.0, 26.0, 24.88429899742902, 0.2481074856600332, 0.0, 1.0, 42942.98670597298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 598200.0000, 
sim time next is 598800.0000, 
raw observation next is [-3.0, 83.0, 0.0, 0.0, 26.0, 24.85454374331913, 0.2462078533496335, 0.0, 1.0, 42900.8397148517], 
processed observation next is [0.0, 0.9565217391304348, 0.3795013850415513, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5712119786099276, 0.5820692844498778, 0.0, 1.0, 0.20428971292786524], 
reward next is 0.7957, 
noisyNet noise sample is [array([-1.3254095], dtype=float32), 0.10642186]. 
=============================================
[2019-04-04 16:29:45,066] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5844522e-09 6.2739813e-10 8.5532577e-16 4.5830388e-14 1.0000000e+00
 6.3067235e-10 1.6897780e-15], sum to 1.0000
[2019-04-04 16:29:45,080] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3614
[2019-04-04 16:29:45,126] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 24.9223400871344, 0.1970835229507505, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 501000.0000, 
sim time next is 501600.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 24.83436643191689, 0.1811760085326208, 0.0, 1.0, 45226.26876912604], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5695305359930742, 0.560392002844207, 0.0, 1.0, 0.21536318461488593], 
reward next is 0.7846, 
noisyNet noise sample is [array([0.23722655], dtype=float32), -0.32896268]. 
=============================================
[2019-04-04 16:29:45,611] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6131745e-09 5.1398996e-11 3.4887704e-16 1.2624751e-15 1.0000000e+00
 5.1345328e-10 1.9810965e-16], sum to 1.0000
[2019-04-04 16:29:45,611] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9679
[2019-04-04 16:29:45,632] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.3, 96.0, 0.0, 0.0, 26.0, 24.83589931883902, 0.2335312388634035, 0.0, 1.0, 40390.96628448059], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 514800.0000, 
sim time next is 515400.0000, 
raw observation next is [3.383333333333333, 96.16666666666666, 0.0, 0.0, 26.0, 24.83846333596739, 0.2335783803970969, 0.0, 1.0, 40247.04249967411], 
processed observation next is [1.0, 1.0, 0.5563250230840259, 0.9616666666666666, 0.0, 0.0, 0.6666666666666666, 0.5698719446639492, 0.5778594601323657, 0.0, 1.0, 0.19165258333178148], 
reward next is 0.8083, 
noisyNet noise sample is [array([-2.531442], dtype=float32), -0.10746274]. 
=============================================
[2019-04-04 16:29:49,436] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.0014052e-10 8.1399248e-11 4.0332969e-16 1.1157423e-15 1.0000000e+00
 1.8569807e-11 6.8180514e-17], sum to 1.0000
[2019-04-04 16:29:49,436] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3986
[2019-04-04 16:29:49,476] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 80.0, 135.3333333333333, 528.6666666666666, 26.0, 24.93676420521925, 0.3278864464451518, 0.0, 1.0, 47463.65194559583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 565800.0000, 
sim time next is 566400.0000, 
raw observation next is [-1.2, 80.0, 136.6666666666667, 561.8333333333334, 26.0, 24.9289562597189, 0.3402804795784267, 0.0, 1.0, 45473.46253993316], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.4555555555555557, 0.6208103130755065, 0.6666666666666666, 0.5774130216432415, 0.6134268265261422, 0.0, 1.0, 0.21654029780920553], 
reward next is 0.7835, 
noisyNet noise sample is [array([0.19134758], dtype=float32), -0.56346947]. 
=============================================
[2019-04-04 16:29:49,643] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5939152e-09 3.6887812e-11 4.5753829e-16 6.6105077e-15 1.0000000e+00
 1.6106365e-11 1.1314719e-16], sum to 1.0000
[2019-04-04 16:29:49,644] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5702
[2019-04-04 16:29:49,695] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 80.0, 38.5, 0.0, 26.0, 25.43877239049447, 0.2926793150721171, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 896400.0000, 
sim time next is 897000.0000, 
raw observation next is [1.1, 80.66666666666667, 41.66666666666666, 0.0, 26.0, 25.45656451645117, 0.2978895785936328, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8066666666666668, 0.13888888888888887, 0.0, 0.6666666666666666, 0.6213803763709308, 0.5992965261978777, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3423975], dtype=float32), -0.28198445]. 
=============================================
[2019-04-04 16:29:49,708] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[87.584175]
 [87.64968 ]
 [87.7379  ]
 [87.73571 ]
 [87.68107 ]], R is [[87.61956024]
 [87.74336243]
 [87.86592865]
 [87.98726654]
 [88.10739136]].
[2019-04-04 16:29:49,804] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3083049e-09 5.7030208e-10 8.4664577e-15 2.1392119e-14 1.0000000e+00
 6.6864064e-10 5.7927495e-15], sum to 1.0000
[2019-04-04 16:29:49,806] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5197
[2019-04-04 16:29:49,871] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.116666666666666, 61.16666666666667, 0.0, 0.0, 26.0, 24.90989488804176, 0.2084445278075242, 0.0, 1.0, 40587.31029618593], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 672600.0000, 
sim time next is 673200.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.91640972892575, 0.2060606162388115, 0.0, 1.0, 42814.41656011585], 
processed observation next is [0.0, 0.8260869565217391, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5763674774104791, 0.5686868720796038, 0.0, 1.0, 0.2038781740957898], 
reward next is 0.7961, 
noisyNet noise sample is [array([0.33385843], dtype=float32), 1.3968366]. 
=============================================
[2019-04-04 16:29:51,219] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.5138954e-09 1.1918694e-09 2.2448518e-15 4.9656958e-13 1.0000000e+00
 4.7716553e-10 1.0830665e-14], sum to 1.0000
[2019-04-04 16:29:51,222] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5657
[2019-04-04 16:29:51,276] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.933333333333334, 62.33333333333333, 92.83333333333334, 48.33333333333333, 26.0, 24.8460837467065, 0.2131081668827622, 0.0, 1.0, 52101.4133552081], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 646800.0000, 
sim time next is 647400.0000, 
raw observation next is [-2.816666666666667, 61.66666666666666, 96.66666666666667, 58.66666666666666, 26.0, 24.85476660110087, 0.2199771139468637, 0.0, 1.0, 42602.63827804882], 
processed observation next is [0.0, 0.4782608695652174, 0.38457987072945526, 0.6166666666666666, 0.32222222222222224, 0.06482504604051564, 0.6666666666666666, 0.5712305500917392, 0.5733257046489545, 0.0, 1.0, 0.20286970608594676], 
reward next is 0.7971, 
noisyNet noise sample is [array([0.7437183], dtype=float32), -0.6152807]. 
=============================================
[2019-04-04 16:30:00,329] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.4084473e-10 2.9554720e-11 3.0685696e-16 4.6558657e-15 1.0000000e+00
 3.8993111e-11 5.4801751e-16], sum to 1.0000
[2019-04-04 16:30:00,332] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0419
[2019-04-04 16:30:00,342] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 46.66666666666667, 87.5, 763.1666666666667, 26.0, 25.55760650785383, 0.4070371205251902, 1.0, 1.0, 18680.44174680189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 740400.0000, 
sim time next is 741000.0000, 
raw observation next is [0.5, 45.83333333333333, 86.0, 753.3333333333333, 26.0, 25.70936473707101, 0.4214204690064184, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.45833333333333326, 0.2866666666666667, 0.8324125230202577, 0.6666666666666666, 0.6424470614225841, 0.6404734896688061, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.965486], dtype=float32), -0.58601487]. 
=============================================
[2019-04-04 16:30:00,370] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[86.66758]
 [86.73959]
 [86.76586]
 [86.80759]
 [86.80402]], R is [[86.58055115]
 [86.62579346]
 [86.670578  ]
 [86.71492004]
 [86.75881958]].
[2019-04-04 16:30:06,481] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4988810e-10 1.9933148e-10 1.4719957e-16 9.9547132e-15 1.0000000e+00
 1.9208747e-11 7.9496446e-16], sum to 1.0000
[2019-04-04 16:30:06,481] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3827
[2019-04-04 16:30:06,518] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 80.0, 38.5, 0.0, 26.0, 25.43885467930304, 0.2926945786535601, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 896400.0000, 
sim time next is 897000.0000, 
raw observation next is [1.1, 80.66666666666667, 41.66666666666666, 0.0, 26.0, 25.45664181560224, 0.2979043319373087, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8066666666666668, 0.13888888888888887, 0.0, 0.6666666666666666, 0.6213868179668532, 0.5993014439791029, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.376713], dtype=float32), -0.14526726]. 
=============================================
[2019-04-04 16:30:06,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[87.653625]
 [87.717285]
 [87.80558 ]
 [87.80135 ]
 [87.746124]], R is [[87.68630219]
 [87.80944061]
 [87.93135071]
 [88.0520401 ]
 [88.17152405]].
[2019-04-04 16:30:11,668] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6290436e-10 1.0180501e-11 1.9865659e-18 8.9417774e-16 1.0000000e+00
 2.8158619e-12 4.9232917e-18], sum to 1.0000
[2019-04-04 16:30:11,670] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6628
[2019-04-04 16:30:11,680] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.93333333333334, 81.0, 0.0, 0.0, 26.0, 25.68458646035426, 0.6220057747451385, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1063200.0000, 
sim time next is 1063800.0000, 
raw observation next is [12.75, 81.5, 0.0, 0.0, 26.0, 25.84707204777667, 0.6241410718658272, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.8157894736842106, 0.815, 0.0, 0.0, 0.6666666666666666, 0.6539226706480559, 0.7080470239552757, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8772197], dtype=float32), 0.75950944]. 
=============================================
[2019-04-04 16:30:14,807] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0379245e-09 1.4626602e-11 1.5567127e-17 4.6149812e-16 1.0000000e+00
 4.6010973e-11 2.3092628e-17], sum to 1.0000
[2019-04-04 16:30:14,808] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1002
[2019-04-04 16:30:14,863] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.4, 98.0, 0.0, 0.0, 26.0, 24.66443929490443, 0.2630755873959294, 1.0, 1.0, 177619.9640930173], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 930600.0000, 
sim time next is 931200.0000, 
raw observation next is [4.4, 98.66666666666666, 0.0, 0.0, 26.0, 24.61628977288606, 0.2952000933824206, 1.0, 1.0, 128126.0992089877], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.9866666666666666, 0.0, 0.0, 0.6666666666666666, 0.5513574810738383, 0.5984000311274735, 1.0, 1.0, 0.6101242819475604], 
reward next is 0.3899, 
noisyNet noise sample is [array([0.03525111], dtype=float32), -1.4781761]. 
=============================================
[2019-04-04 16:30:15,170] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.8544062e-10 6.3971481e-11 5.6304371e-16 1.7825431e-15 1.0000000e+00
 8.1712533e-11 1.8808447e-16], sum to 1.0000
[2019-04-04 16:30:15,172] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3580
[2019-04-04 16:30:15,187] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 26.0, 25.42849152435607, 0.4161167539307236, 0.0, 1.0, 65653.75781405295], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1494000.0000, 
sim time next is 1494600.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 26.0, 25.25421895178648, 0.4131166771204006, 0.0, 1.0, 82273.49406361136], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6045182459822067, 0.6377055590401336, 0.0, 1.0, 0.3917785431600541], 
reward next is 0.6082, 
noisyNet noise sample is [array([-1.5860903], dtype=float32), -0.931689]. 
=============================================
[2019-04-04 16:30:16,581] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0185328e-09 1.3019154e-10 3.1648775e-16 1.0528079e-14 1.0000000e+00
 1.7006464e-10 8.4264675e-16], sum to 1.0000
[2019-04-04 16:30:16,586] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6260
[2019-04-04 16:30:16,617] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.433333333333333, 90.0, 0.0, 0.0, 26.0, 25.10493229176598, 0.5231709872932774, 0.0, 1.0, 71120.44818159031], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1456800.0000, 
sim time next is 1457400.0000, 
raw observation next is [1.516666666666667, 89.5, 0.0, 0.0, 26.0, 25.31118574703884, 0.5472547384332428, 0.0, 1.0, 50075.17392464632], 
processed observation next is [1.0, 0.8695652173913043, 0.5046168051708219, 0.895, 0.0, 0.0, 0.6666666666666666, 0.6092654789199035, 0.6824182461444143, 0.0, 1.0, 0.23845320916498247], 
reward next is 0.7615, 
noisyNet noise sample is [array([-1.4765376], dtype=float32), -0.9272032]. 
=============================================
[2019-04-04 16:30:19,659] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6144090e-10 1.4990292e-11 1.7772928e-17 8.5542037e-16 1.0000000e+00
 2.8154538e-12 1.5767940e-17], sum to 1.0000
[2019-04-04 16:30:19,661] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4940
[2019-04-04 16:30:19,671] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 25.587497513894, 0.518297422670269, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1018800.0000, 
sim time next is 1019400.0000, 
raw observation next is [14.4, 80.33333333333333, 0.0, 0.0, 26.0, 25.60961249199437, 0.4980832658711078, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.8033333333333332, 0.0, 0.0, 0.6666666666666666, 0.6341343743328641, 0.6660277552903693, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.70236045], dtype=float32), 1.5009209]. 
=============================================
[2019-04-04 16:30:20,675] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.21943289e-09 8.60154559e-10 4.30247262e-16 1.02541106e-14
 1.00000000e+00 1.32833550e-10 1.00165691e-15], sum to 1.0000
[2019-04-04 16:30:20,679] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2466
[2019-04-04 16:30:20,690] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.607781757961, 0.547714816692149, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1315200.0000, 
sim time next is 1315800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.55343471851834, 0.5325912243353471, 0.0, 1.0, 18744.20025108876], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6294528932098616, 0.6775304081117824, 0.0, 1.0, 0.08925809643375601], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.38862255], dtype=float32), 0.99171185]. 
=============================================
[2019-04-04 16:30:22,381] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4025767e-10 2.2665923e-11 6.7191616e-17 7.8586473e-16 1.0000000e+00
 1.9824850e-11 1.2250894e-16], sum to 1.0000
[2019-04-04 16:30:22,386] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7712
[2019-04-04 16:30:22,395] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 62.0, 0.0, 26.0, 25.81545256955999, 0.5207890719827629, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1349400.0000, 
sim time next is 1350000.0000, 
raw observation next is [1.1, 92.0, 57.5, 0.0, 26.0, 25.7401672190306, 0.5141834533222419, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.19166666666666668, 0.0, 0.6666666666666666, 0.6450139349192167, 0.6713944844407473, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17350541], dtype=float32), -0.36092544]. 
=============================================
[2019-04-04 16:30:22,407] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[91.09589 ]
 [91.294426]
 [91.50828 ]
 [91.72975 ]
 [91.9931  ]], R is [[90.99970245]
 [91.08970642]
 [91.17881012]
 [91.26702118]
 [91.35435486]].
[2019-04-04 16:30:23,593] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.6244345e-09 1.3686579e-09 1.4369819e-15 1.2156040e-13 1.0000000e+00
 3.2526318e-11 2.3871087e-15], sum to 1.0000
[2019-04-04 16:30:23,598] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4939
[2019-04-04 16:30:23,602] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.53735677037202, 0.1600743026768607, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1232400.0000, 
sim time next is 1233000.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.54150564836344, 0.1569367935697974, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.46179213736362, 0.5523122645232658, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7619852], dtype=float32), -0.8635912]. 
=============================================
[2019-04-04 16:30:23,612] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[78.30034 ]
 [78.36406 ]
 [78.409256]
 [78.43706 ]
 [78.46675 ]], R is [[78.46735382]
 [78.68267822]
 [78.89585114]
 [79.10689545]
 [79.31582642]].
[2019-04-04 16:30:25,601] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6610600e-09 2.7685694e-09 2.2250977e-15 5.8559093e-13 1.0000000e+00
 1.8240436e-10 1.0446109e-14], sum to 1.0000
[2019-04-04 16:30:25,601] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4387
[2019-04-04 16:30:25,608] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.1, 83.0, 0.0, 0.0, 26.0, 23.98415975236648, 0.2472898561248677, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1216800.0000, 
sim time next is 1217400.0000, 
raw observation next is [16.0, 84.66666666666667, 0.0, 0.0, 26.0, 23.96296553033594, 0.254279542018727, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9058171745152357, 0.8466666666666667, 0.0, 0.0, 0.6666666666666666, 0.4969137941946616, 0.5847598473395756, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43751526], dtype=float32), -0.16183135]. 
=============================================
[2019-04-04 16:30:29,918] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7257561e-09 8.6570950e-10 1.0557519e-14 5.3632683e-14 1.0000000e+00
 2.0789950e-10 1.6378148e-14], sum to 1.0000
[2019-04-04 16:30:29,918] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9282
[2019-04-04 16:30:29,974] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 78.0, 134.0, 72.5, 26.0, 25.01960916925928, 0.2180462250683564, 0.0, 1.0, 18757.33677803666], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1850400.0000, 
sim time next is 1851000.0000, 
raw observation next is [-5.600000000000001, 77.5, 129.3333333333333, 65.33333333333333, 26.0, 24.93607076169066, 0.2105018772562035, 0.0, 1.0, 76125.10333373927], 
processed observation next is [0.0, 0.43478260869565216, 0.3074792243767313, 0.775, 0.43111111111111095, 0.0721915285451197, 0.6666666666666666, 0.5780058968075551, 0.5701672924187345, 0.0, 1.0, 0.3625004920654251], 
reward next is 0.6375, 
noisyNet noise sample is [array([0.48048285], dtype=float32), 3.696971]. 
=============================================
[2019-04-04 16:30:29,976] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[80.974495]
 [81.11384 ]
 [81.236435]
 [81.092445]
 [80.69482 ]], R is [[80.81918335]
 [80.92167664]
 [81.11245728]
 [81.30133057]
 [81.4883194 ]].
[2019-04-04 16:30:31,705] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0498370e-10 5.3621923e-11 3.2218132e-16 6.4511924e-16 1.0000000e+00
 3.9749898e-11 1.5148683e-16], sum to 1.0000
[2019-04-04 16:30:31,705] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9361
[2019-04-04 16:30:31,754] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.4166666666666667, 92.5, 94.0, 0.0, 26.0, 25.31756435367811, 0.3170491854762916, 1.0, 1.0, 16605.69518828773], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1428600.0000, 
sim time next is 1429200.0000, 
raw observation next is [0.5, 92.0, 93.0, 0.0, 26.0, 24.93872131436758, 0.4130379131899932, 1.0, 1.0, 169883.7333201864], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.92, 0.31, 0.0, 0.6666666666666666, 0.5782267761972983, 0.6376793043966644, 1.0, 1.0, 0.8089701586675543], 
reward next is 0.1910, 
noisyNet noise sample is [array([-0.8365584], dtype=float32), 0.11286552]. 
=============================================
[2019-04-04 16:30:34,628] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0011595e-09 2.9631655e-11 2.8472123e-16 8.5879180e-15 1.0000000e+00
 2.3760344e-11 5.2400026e-16], sum to 1.0000
[2019-04-04 16:30:34,630] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8437
[2019-04-04 16:30:34,642] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.31807984437363, 0.4423852724225016, 0.0, 1.0, 36945.80790106035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1487400.0000, 
sim time next is 1488000.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.30414035222843, 0.4507419289768504, 0.0, 1.0, 36927.63525064533], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6086783626857025, 0.6502473096589502, 0.0, 1.0, 0.17584588214593017], 
reward next is 0.8242, 
noisyNet noise sample is [array([1.152339], dtype=float32), 0.6405097]. 
=============================================
[2019-04-04 16:30:34,673] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[88.080894]
 [88.082306]
 [88.069534]
 [88.077896]
 [88.069046]], R is [[88.029953  ]
 [87.97371674]
 [87.91814423]
 [87.86266327]
 [87.80654144]].
[2019-04-04 16:30:35,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3129350e-10 6.3514582e-11 3.4492416e-17 6.9366874e-16 1.0000000e+00
 2.1828308e-11 7.7659316e-17], sum to 1.0000
[2019-04-04 16:30:35,037] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5475
[2019-04-04 16:30:35,100] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 26.0, 25.49721544008539, 0.4669732082634064, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1495800.0000, 
sim time next is 1496400.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 26.0, 25.59916432138828, 0.4606874464664826, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6332636934490233, 0.6535624821554942, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0607986], dtype=float32), -1.0204289]. 
=============================================
[2019-04-04 16:30:36,374] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3923342e-10 3.4375613e-11 3.7318125e-16 1.5336038e-14 1.0000000e+00
 3.7464636e-11 6.0244684e-16], sum to 1.0000
[2019-04-04 16:30:36,378] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2591
[2019-04-04 16:30:36,395] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.35270304741604, 0.5109843133841991, 0.0, 1.0, 37502.10939047627], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1470600.0000, 
sim time next is 1471200.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.54763398624428, 0.5095550577307714, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6289694988536899, 0.6698516859102571, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.455298], dtype=float32), 1.1496279]. 
=============================================
[2019-04-04 16:30:42,732] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2348606e-10 1.2528442e-11 3.3939806e-16 1.4715753e-15 1.0000000e+00
 2.5776216e-11 2.4821800e-16], sum to 1.0000
[2019-04-04 16:30:42,732] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6896
[2019-04-04 16:30:42,737] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.983333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.65166767937008, 0.591841409681305, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1626600.0000, 
sim time next is 1627200.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 26.0, 25.57808109625306, 0.5760091901266039, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6759002770083103, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6315067580210885, 0.6920030633755346, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1972083], dtype=float32), -0.35179198]. 
=============================================
[2019-04-04 16:30:44,532] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.2731922e-10 1.3339306e-11 7.4597106e-17 1.3881030e-15 1.0000000e+00
 3.4066101e-11 1.0805310e-17], sum to 1.0000
[2019-04-04 16:30:44,535] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3267
[2019-04-04 16:30:44,592] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.00000000000001, 0.0, 0.0, 26.0, 24.64938874448017, 0.5124098373369325, 1.0, 1.0, 77890.3812191822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1710600.0000, 
sim time next is 1711200.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.28155576358002, 0.5501635474682665, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6067963136316683, 0.6833878491560889, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.92098176], dtype=float32), 0.4340843]. 
=============================================
[2019-04-04 16:30:47,790] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3521863e-09 7.9127691e-11 1.3414393e-15 1.4834488e-14 1.0000000e+00
 4.2726669e-11 4.4537194e-16], sum to 1.0000
[2019-04-04 16:30:47,791] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1554
[2019-04-04 16:30:47,806] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333333, 93.0, 0.0, 0.0, 26.0, 25.35972591412169, 0.4775027568051808, 0.0, 1.0, 44091.09230824796], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1726800.0000, 
sim time next is 1727400.0000, 
raw observation next is [0.4166666666666667, 92.5, 0.0, 0.0, 26.0, 25.35711319156054, 0.4754876439045391, 0.0, 1.0, 43508.28725438931], 
processed observation next is [1.0, 1.0, 0.47414589104339805, 0.925, 0.0, 0.0, 0.6666666666666666, 0.6130927659633784, 0.658495881301513, 0.0, 1.0, 0.20718232025899672], 
reward next is 0.7928, 
noisyNet noise sample is [array([-0.69848394], dtype=float32), 0.060360875]. 
=============================================
[2019-04-04 16:31:05,375] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0477109e-08 1.8028797e-09 3.7079836e-14 1.2767394e-13 1.0000000e+00
 1.9741555e-09 1.8428264e-13], sum to 1.0000
[2019-04-04 16:31:05,378] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6141
[2019-04-04 16:31:05,392] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 63.0, 0.0, 0.0, 26.0, 24.92131215792089, 0.2976721071974973, 0.0, 1.0, 38520.7256513087], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2331600.0000, 
sim time next is 2332200.0000, 
raw observation next is [-2.3, 64.0, 0.0, 0.0, 26.0, 24.88333670793936, 0.2946870969455991, 0.0, 1.0, 38538.31011026005], 
processed observation next is [1.0, 1.0, 0.3988919667590028, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5736113923282801, 0.5982290323151996, 0.0, 1.0, 0.18351576242980974], 
reward next is 0.8165, 
noisyNet noise sample is [array([1.8922399], dtype=float32), 0.7807963]. 
=============================================
[2019-04-04 16:31:08,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.7358335e-09 2.6524244e-10 8.1287990e-15 3.8718249e-14 1.0000000e+00
 1.5912360e-09 1.6182985e-14], sum to 1.0000
[2019-04-04 16:31:08,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2443
[2019-04-04 16:31:08,966] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.69257264824572, 0.2316150998966837, 0.0, 1.0, 43000.31830027781], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1983600.0000, 
sim time next is 1984200.0000, 
raw observation next is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.64378953849547, 0.2224093403647981, 0.0, 1.0, 42988.65764516341], 
processed observation next is [1.0, 1.0, 0.3074792243767313, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5536491282079558, 0.574136446788266, 0.0, 1.0, 0.2047078935483972], 
reward next is 0.7953, 
noisyNet noise sample is [array([0.10195678], dtype=float32), 0.7728591]. 
=============================================
[2019-04-04 16:31:19,188] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.53656060e-07 1.39817855e-08 1.87573508e-13 9.05917648e-13
 9.99999881e-01 4.59523442e-09 2.42988301e-13], sum to 1.0000
[2019-04-04 16:31:19,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7452
[2019-04-04 16:31:19,217] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 75.66666666666667, 0.0, 0.0, 26.0, 23.97494269428077, 0.04286471687196584, 0.0, 1.0, 41960.29144185136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2178600.0000, 
sim time next is 2179200.0000, 
raw observation next is [-6.2, 76.33333333333334, 0.0, 0.0, 26.0, 23.91386966632721, 0.03290730749565168, 0.0, 1.0, 41953.82701675346], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7633333333333334, 0.0, 0.0, 0.6666666666666666, 0.49282247219393405, 0.5109691024985505, 0.0, 1.0, 0.19978012865120695], 
reward next is 0.8002, 
noisyNet noise sample is [array([2.1496022], dtype=float32), 1.6608856]. 
=============================================
[2019-04-04 16:31:19,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.0635897e-08 9.5156372e-09 2.0924618e-13 1.9411155e-12 9.9999988e-01
 3.3204268e-09 2.4386442e-13], sum to 1.0000
[2019-04-04 16:31:19,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5098
[2019-04-04 16:31:19,527] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 75.0, 0.0, 0.0, 26.0, 23.64670639709287, -0.02123984752434952, 0.0, 1.0, 41927.15062469022], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2185200.0000, 
sim time next is 2185800.0000, 
raw observation next is [-5.600000000000001, 75.0, 0.0, 0.0, 26.0, 23.65141906218541, -0.02152885008387157, 0.0, 1.0, 41938.48588479517], 
processed observation next is [1.0, 0.30434782608695654, 0.3074792243767313, 0.75, 0.0, 0.0, 0.6666666666666666, 0.47095158851545094, 0.4928237166387095, 0.0, 1.0, 0.19970707564188175], 
reward next is 0.8003, 
noisyNet noise sample is [array([0.3520268], dtype=float32), -0.9405505]. 
=============================================
[2019-04-04 16:31:20,965] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9816064e-08 2.0407300e-09 7.7559663e-14 1.0059812e-13 1.0000000e+00
 3.6431120e-09 2.0547503e-14], sum to 1.0000
[2019-04-04 16:31:20,965] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6938
[2019-04-04 16:31:20,980] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 73.66666666666667, 0.0, 0.0, 26.0, 24.9300129304801, 0.292686209445028, 0.0, 1.0, 44254.91272699591], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2241600.0000, 
sim time next is 2242200.0000, 
raw observation next is [-6.1, 74.33333333333333, 0.0, 0.0, 26.0, 24.85463039955346, 0.2794598799159582, 0.0, 1.0, 44247.95534781723], 
processed observation next is [1.0, 0.9565217391304348, 0.29362880886426596, 0.7433333333333333, 0.0, 0.0, 0.6666666666666666, 0.5712191999627884, 0.5931532933053194, 0.0, 1.0, 0.21070454927532015], 
reward next is 0.7893, 
noisyNet noise sample is [array([-0.05476711], dtype=float32), -0.08085729]. 
=============================================
[2019-04-04 16:31:22,483] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1298592e-08 7.1601289e-09 8.1221230e-14 8.6127123e-13 1.0000000e+00
 2.8304386e-08 2.2506891e-13], sum to 1.0000
[2019-04-04 16:31:22,484] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0611
[2019-04-04 16:31:22,523] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.49007990360808, 0.169993000209038, 0.0, 1.0, 42515.78503846233], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2168400.0000, 
sim time next is 2169000.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.50037241068129, 0.1581035272567075, 0.0, 1.0, 42501.328628356], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5416977008901075, 0.5527011757522359, 0.0, 1.0, 0.20238727918264762], 
reward next is 0.7976, 
noisyNet noise sample is [array([0.37383413], dtype=float32), 0.8823554]. 
=============================================
[2019-04-04 16:31:22,540] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.072685]
 [75.08628 ]
 [75.09993 ]
 [75.254036]
 [75.240074]], R is [[75.15244293]
 [75.19846344]
 [75.24376678]
 [75.28843689]
 [75.3326416 ]].
[2019-04-04 16:31:32,047] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7853056e-08 1.1472797e-08 1.0171083e-12 2.4949914e-12 1.0000000e+00
 7.6686097e-09 7.2670320e-13], sum to 1.0000
[2019-04-04 16:31:32,053] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6714
[2019-04-04 16:31:32,091] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.50507020761061, -0.08546550047607909, 0.0, 1.0, 44402.19221778094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2434800.0000, 
sim time next is 2435400.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.46585376875863, -0.09378784246084727, 0.0, 1.0, 44418.1969805519], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.45548781406321925, 0.4687373858463842, 0.0, 1.0, 0.21151522371691378], 
reward next is 0.7885, 
noisyNet noise sample is [array([0.50594574], dtype=float32), 0.40086547]. 
=============================================
[2019-04-04 16:31:32,370] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5402630e-08 1.9936848e-09 5.4560882e-14 1.1804351e-13 1.0000000e+00
 1.1499914e-09 8.1340452e-14], sum to 1.0000
[2019-04-04 16:31:32,371] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6020
[2019-04-04 16:31:32,391] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.1175673685575, 0.2891689939408613, 0.0, 1.0, 43929.79475021863], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2405400.0000, 
sim time next is 2406000.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15746208591813, 0.2868182535797676, 0.0, 1.0, 43326.67374303031], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5964551738265108, 0.5956060845265893, 0.0, 1.0, 0.20631749401443006], 
reward next is 0.7937, 
noisyNet noise sample is [array([-1.5067321], dtype=float32), -0.09154812]. 
=============================================
[2019-04-04 16:31:32,403] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[75.67235]
 [75.69403]
 [75.68895]
 [75.63096]
 [75.50497]], R is [[75.65434265]
 [75.68860626]
 [75.71835327]
 [75.73304749]
 [75.69898987]].
[2019-04-04 16:31:37,226] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.9183040e-08 2.6162301e-08 1.8176710e-13 8.6018269e-13 9.9999988e-01
 1.2596916e-08 1.0570136e-13], sum to 1.0000
[2019-04-04 16:31:37,226] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5019
[2019-04-04 16:31:37,246] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 44.83333333333334, 0.0, 0.0, 26.0, 24.92735402915525, 0.1840188681975645, 0.0, 1.0, 38577.21825501302], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2517000.0000, 
sim time next is 2517600.0000, 
raw observation next is [-1.7, 45.66666666666667, 0.0, 0.0, 26.0, 24.9841075803252, 0.1888183636094426, 0.0, 1.0, 38511.0899909212], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5820089650271001, 0.5629394545364809, 0.0, 1.0, 0.1833861428139105], 
reward next is 0.8166, 
noisyNet noise sample is [array([-0.3467308], dtype=float32), 0.580315]. 
=============================================
[2019-04-04 16:31:40,556] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8300593e-09 5.5640081e-10 1.0100682e-14 1.4179791e-13 1.0000000e+00
 5.3521099e-10 2.0490559e-14], sum to 1.0000
[2019-04-04 16:31:40,556] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2332
[2019-04-04 16:31:40,597] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.55, 29.0, 0.0, 0.0, 26.0, 24.91554848828708, 0.2113485210179908, 0.0, 1.0, 41154.06809486618], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2485800.0000, 
sim time next is 2486400.0000, 
raw observation next is [0.3666666666666668, 29.33333333333334, 0.0, 0.0, 26.0, 24.90555538207729, 0.2109580905632913, 0.0, 1.0, 46503.78528962587], 
processed observation next is [0.0, 0.782608695652174, 0.4727608494921515, 0.2933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5754629485064408, 0.5703193635210971, 0.0, 1.0, 0.22144659661726604], 
reward next is 0.7786, 
noisyNet noise sample is [array([0.35466066], dtype=float32), 0.115946464]. 
=============================================
[2019-04-04 16:31:43,351] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5439155e-09 7.6628742e-10 2.8019188e-14 2.1881727e-13 1.0000000e+00
 7.3032957e-10 4.5504273e-14], sum to 1.0000
[2019-04-04 16:31:43,352] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1200
[2019-04-04 16:31:43,401] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.8666666666666667, 29.0, 89.5, 842.8333333333334, 26.0, 24.9122262737366, 0.2626160083225564, 0.0, 1.0, 26143.37100430447], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2464800.0000, 
sim time next is 2465400.0000, 
raw observation next is [1.233333333333333, 28.5, 89.0, 840.6666666666666, 26.0, 24.91859079848704, 0.2696375241517449, 0.0, 1.0, 18719.87687657799], 
processed observation next is [0.0, 0.5217391304347826, 0.49676823638042483, 0.285, 0.2966666666666667, 0.9289134438305708, 0.6666666666666666, 0.5765492332072532, 0.5898791747172484, 0.0, 1.0, 0.08914227084084757], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.6266574], dtype=float32), -0.012258299]. 
=============================================
[2019-04-04 16:31:52,240] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6921449e-08 1.7896788e-09 2.9644554e-14 1.3328491e-12 1.0000000e+00
 3.7097103e-10 4.4035495e-14], sum to 1.0000
[2019-04-04 16:31:52,240] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3016
[2019-04-04 16:31:52,257] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.26666666666667, 80.66666666666666, 0.0, 0.0, 26.0, 24.06676582559286, 0.1011715966724734, 0.0, 1.0, 44448.17262870198], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2688000.0000, 
sim time next is 2688600.0000, 
raw observation next is [-12.58333333333333, 81.83333333333334, 0.0, 0.0, 26.0, 23.96966267641738, 0.08782126950975816, 0.0, 1.0, 44478.09682929819], 
processed observation next is [1.0, 0.08695652173913043, 0.11403508771929832, 0.8183333333333335, 0.0, 0.0, 0.6666666666666666, 0.49747188970144823, 0.5292737565032527, 0.0, 1.0, 0.21180046109189615], 
reward next is 0.7882, 
noisyNet noise sample is [array([-0.07335746], dtype=float32), 1.1840008]. 
=============================================
[2019-04-04 16:31:54,594] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.1362958e-10 3.5257120e-11 3.2301443e-16 8.1282103e-15 1.0000000e+00
 2.2269025e-11 7.1890598e-16], sum to 1.0000
[2019-04-04 16:31:54,594] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5775
[2019-04-04 16:31:54,646] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 58.0, 155.6666666666667, 278.8333333333333, 26.0, 25.89895900641, 0.4056434687414204, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2799600.0000, 
sim time next is 2800200.0000, 
raw observation next is [-3.5, 56.5, 159.3333333333333, 324.6666666666666, 26.0, 25.92652492156721, 0.4110572763108416, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.36565096952908593, 0.565, 0.531111111111111, 0.3587476979742172, 0.6666666666666666, 0.6605437434639342, 0.6370190921036138, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2540895], dtype=float32), -0.151996]. 
=============================================
[2019-04-04 16:31:58,170] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.2517778e-09 4.1215839e-10 9.5074038e-16 2.0470598e-14 1.0000000e+00
 2.6436209e-10 1.5883809e-15], sum to 1.0000
[2019-04-04 16:31:58,171] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4839
[2019-04-04 16:31:58,186] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.0, 64.0, 112.5, 790.0, 26.0, 25.94380731560665, 0.4710848062256616, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2721600.0000, 
sim time next is 2722200.0000, 
raw observation next is [-7.666666666666667, 63.16666666666667, 112.6666666666667, 793.0, 26.0, 25.9332442247323, 0.4757844382074192, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2502308402585411, 0.6316666666666667, 0.37555555555555564, 0.876243093922652, 0.6666666666666666, 0.6611036853943583, 0.6585948127358064, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8789792], dtype=float32), -0.13581882]. 
=============================================
[2019-04-04 16:32:03,484] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2860539e-09 2.8301120e-10 1.7428920e-15 2.0884539e-14 1.0000000e+00
 3.4159012e-11 8.6586418e-15], sum to 1.0000
[2019-04-04 16:32:03,485] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2024
[2019-04-04 16:32:03,522] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 81.66666666666667, 0.0, 0.0, 26.0, 24.26601986517974, 0.1819866375840216, 0.0, 1.0, 42757.22139099934], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2956800.0000, 
sim time next is 2957400.0000, 
raw observation next is [-3.5, 80.5, 0.0, 0.0, 26.0, 24.23845302805552, 0.1783680590068732, 0.0, 1.0, 42709.77259739765], 
processed observation next is [0.0, 0.21739130434782608, 0.36565096952908593, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5198710856712934, 0.5594560196689577, 0.0, 1.0, 0.20337986951141737], 
reward next is 0.7966, 
noisyNet noise sample is [array([0.17279136], dtype=float32), 0.573192]. 
=============================================
[2019-04-04 16:32:03,708] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.64773981e-09 1.84536178e-10 4.36539473e-16 1.73056692e-14
 1.00000000e+00 1.12368566e-10 5.92643119e-16], sum to 1.0000
[2019-04-04 16:32:03,709] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4264
[2019-04-04 16:32:03,715] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666667, 52.66666666666667, 114.6666666666667, 801.8333333333334, 26.0, 25.94154108597878, 0.5677092541714563, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3331200.0000, 
sim time next is 3331800.0000, 
raw observation next is [-4.5, 52.0, 114.0, 800.0, 26.0, 26.01127376304867, 0.5806633954306307, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.52, 0.38, 0.8839779005524862, 0.6666666666666666, 0.6676061469207225, 0.6935544651435436, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1930581], dtype=float32), 2.399881]. 
=============================================
[2019-04-04 16:32:08,049] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2176275e-08 5.3599480e-09 1.0503975e-13 2.4763294e-13 1.0000000e+00
 1.0039993e-09 1.0943211e-14], sum to 1.0000
[2019-04-04 16:32:08,049] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8416
[2019-04-04 16:32:08,063] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 62.5, 0.0, 0.0, 26.0, 24.68831369794104, 0.2071734665632334, 0.0, 1.0, 42846.83216407803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3393000.0000, 
sim time next is 3393600.0000, 
raw observation next is [-3.0, 63.33333333333333, 0.0, 0.0, 26.0, 24.62811026913117, 0.1969820042321454, 0.0, 1.0, 42831.4019685987], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5523425224275975, 0.5656606680773818, 0.0, 1.0, 0.20395905699332714], 
reward next is 0.7960, 
noisyNet noise sample is [array([0.6685069], dtype=float32), -0.16081315]. 
=============================================
[2019-04-04 16:32:08,683] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6438594e-09 4.5016019e-10 9.5800347e-16 2.8871033e-14 1.0000000e+00
 3.4960151e-10 2.1152321e-15], sum to 1.0000
[2019-04-04 16:32:08,686] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1853
[2019-04-04 16:32:08,714] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.92064855721406, 0.2598244955948753, 0.0, 1.0, 55605.71011821129], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2865600.0000, 
sim time next is 2866200.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.92127583618235, 0.2581053805920003, 0.0, 1.0, 55708.26414967434], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5767729863485291, 0.586035126864, 0.0, 1.0, 0.2652774483317826], 
reward next is 0.7347, 
noisyNet noise sample is [array([-0.7333271], dtype=float32), -1.2370545]. 
=============================================
[2019-04-04 16:32:09,000] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.99048758e-09 1.02586950e-09 2.20466921e-14 1.08495264e-13
 1.00000000e+00 1.35649247e-09 1.20562367e-14], sum to 1.0000
[2019-04-04 16:32:09,000] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8034
[2019-04-04 16:32:09,032] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.44038713668873, 0.4883955209231823, 0.0, 1.0, 36045.67701928398], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3277200.0000, 
sim time next is 3277800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.38606946151899, 0.4808139264237869, 0.0, 1.0, 65253.22067029626], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6155057884599158, 0.660271308807929, 0.0, 1.0, 0.310729622239506], 
reward next is 0.6893, 
noisyNet noise sample is [array([0.8808468], dtype=float32), -0.7284913]. 
=============================================
[2019-04-04 16:32:14,983] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2194390e-09 1.4679011e-10 1.4893855e-14 8.7796808e-14 1.0000000e+00
 3.4526898e-10 6.9725588e-15], sum to 1.0000
[2019-04-04 16:32:14,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1647
[2019-04-04 16:32:15,001] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.49150222375233, 0.2263271719078879, 0.0, 1.0, 41029.51704678917], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3566400.0000, 
sim time next is 3567000.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.44715650073534, 0.217205481214319, 0.0, 1.0, 41119.29973461839], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5372630417279449, 0.5724018270714396, 0.0, 1.0, 0.19580618921246853], 
reward next is 0.8042, 
noisyNet noise sample is [array([0.18513395], dtype=float32), -2.7717464]. 
=============================================
[2019-04-04 16:32:15,015] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.08639 ]
 [78.262405]
 [78.438225]
 [78.61489 ]
 [78.78538 ]], R is [[77.93592072]
 [77.96118164]
 [77.98654175]
 [78.01191711]
 [78.03717041]].
[2019-04-04 16:32:15,489] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5025916e-09 1.2495950e-09 9.5299219e-15 7.2912976e-14 1.0000000e+00
 6.2804317e-11 3.2188787e-15], sum to 1.0000
[2019-04-04 16:32:15,489] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8212
[2019-04-04 16:32:15,551] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8666666666666667, 88.66666666666666, 0.0, 0.0, 26.0, 25.00418651525825, 0.2799923742474308, 0.0, 1.0, 31470.29840689731], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3091200.0000, 
sim time next is 3091800.0000, 
raw observation next is [-0.9333333333333333, 90.33333333333334, 0.0, 0.0, 26.0, 25.00188602415534, 0.2774153425278744, 0.0, 1.0, 34676.37830138261], 
processed observation next is [0.0, 0.782608695652174, 0.4367497691597415, 0.9033333333333334, 0.0, 0.0, 0.6666666666666666, 0.583490502012945, 0.5924717808426249, 0.0, 1.0, 0.1651256109589648], 
reward next is 0.8349, 
noisyNet noise sample is [array([0.70041096], dtype=float32), -1.3634938]. 
=============================================
[2019-04-04 16:32:17,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.39777553e-09 2.02514250e-09 1.02319945e-14 4.26316263e-13
 1.00000000e+00 3.88907795e-10 1.07554285e-13], sum to 1.0000
[2019-04-04 16:32:17,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5542
[2019-04-04 16:32:17,500] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 71.16666666666667, 0.0, 0.0, 26.0, 23.76050077892432, -0.009071059238600748, 0.0, 1.0, 40215.42394641919], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3046200.0000, 
sim time next is 3046800.0000, 
raw observation next is [-6.0, 72.33333333333334, 0.0, 0.0, 26.0, 23.73353739858523, -0.01534848547495667, 0.0, 1.0, 40256.90878842475], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.4777947832154359, 0.49488383817501447, 0.0, 1.0, 0.19169956565916546], 
reward next is 0.8083, 
noisyNet noise sample is [array([-3.0423143], dtype=float32), 1.6825935]. 
=============================================
[2019-04-04 16:32:19,527] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.9487245e-09 9.7125707e-10 1.2526225e-14 1.4003470e-14 1.0000000e+00
 4.6300014e-10 1.7124192e-14], sum to 1.0000
[2019-04-04 16:32:19,529] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8304
[2019-04-04 16:32:19,540] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 44.66666666666667, 0.0, 0.0, 26.0, 25.44716478977913, 0.3926915560734632, 0.0, 1.0, 57170.96803896352], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3619200.0000, 
sim time next is 3619800.0000, 
raw observation next is [-1.5, 46.0, 0.0, 0.0, 26.0, 25.41822427316493, 0.3913601890567307, 0.0, 1.0, 59799.85087491576], 
processed observation next is [0.0, 0.9130434782608695, 0.4210526315789474, 0.46, 0.0, 0.0, 0.6666666666666666, 0.6181853560970776, 0.6304533963522435, 0.0, 1.0, 0.28476119464245603], 
reward next is 0.7152, 
noisyNet noise sample is [array([0.31322828], dtype=float32), 0.9997753]. 
=============================================
[2019-04-04 16:32:25,979] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4063166e-09 2.9907679e-10 5.2927775e-16 1.8136358e-14 1.0000000e+00
 6.4590672e-10 7.3655815e-16], sum to 1.0000
[2019-04-04 16:32:25,980] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0111
[2019-04-04 16:32:26,000] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 75.0, 25.66666666666666, 239.6666666666666, 26.0, 26.30203421567724, 0.689042488085561, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3258600.0000, 
sim time next is 3259200.0000, 
raw observation next is [-4.0, 73.0, 17.33333333333333, 171.8333333333333, 26.0, 26.45359562160778, 0.6222055795329622, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.73, 0.05777777777777776, 0.18987108655616938, 0.6666666666666666, 0.7044663018006482, 0.7074018598443207, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11144447], dtype=float32), 0.11430205]. 
=============================================
[2019-04-04 16:32:27,249] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0915765e-09 8.2751778e-11 1.1319647e-15 6.2651525e-15 1.0000000e+00
 9.0125844e-11 2.4286158e-15], sum to 1.0000
[2019-04-04 16:32:27,250] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9052
[2019-04-04 16:32:27,264] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 66.0, 0.0, 0.0, 26.0, 25.39374073044985, 0.3923412097665551, 0.0, 1.0, 42512.23070796502], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3712200.0000, 
sim time next is 3712800.0000, 
raw observation next is [-3.0, 67.0, 0.0, 0.0, 26.0, 25.38717103586502, 0.3898575067910418, 0.0, 1.0, 43638.87020305778], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6155975863220849, 0.6299525022636806, 0.0, 1.0, 0.20780414382408466], 
reward next is 0.7922, 
noisyNet noise sample is [array([-0.16572915], dtype=float32), 1.1361201]. 
=============================================
[2019-04-04 16:32:29,961] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1403073e-09 7.6141982e-10 6.3069755e-14 3.4423721e-14 1.0000000e+00
 1.0753153e-09 1.2294024e-14], sum to 1.0000
[2019-04-04 16:32:29,962] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0441
[2019-04-04 16:32:29,978] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 59.16666666666666, 0.0, 0.0, 26.0, 25.25054319504078, 0.4553723000403219, 0.0, 1.0, 58579.33719536583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3883800.0000, 
sim time next is 3884400.0000, 
raw observation next is [-1.0, 60.0, 0.0, 0.0, 26.0, 25.29114024272175, 0.4664981747624774, 0.0, 1.0, 46428.91918482632], 
processed observation next is [1.0, 1.0, 0.4349030470914128, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6075950202268124, 0.6554993915874925, 0.0, 1.0, 0.22109009135631583], 
reward next is 0.7789, 
noisyNet noise sample is [array([0.27395466], dtype=float32), -0.5902895]. 
=============================================
[2019-04-04 16:32:36,727] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3214645e-10 1.3364040e-10 1.4029555e-15 1.9039288e-14 1.0000000e+00
 1.1434468e-10 2.7566577e-15], sum to 1.0000
[2019-04-04 16:32:36,728] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7103
[2019-04-04 16:32:36,755] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.5, 73.0, 0.0, 0.0, 26.0, 25.18650612557476, 0.4291710848602819, 1.0, 1.0, 59785.90768403962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3436200.0000, 
sim time next is 3436800.0000, 
raw observation next is [1.333333333333333, 75.0, 0.0, 0.0, 26.0, 25.09432860842811, 0.4514649525827606, 1.0, 1.0, 92167.60119952037], 
processed observation next is [1.0, 0.782608695652174, 0.4995383194829178, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5911940507023425, 0.6504883175275868, 1.0, 1.0, 0.4388933390453351], 
reward next is 0.5611, 
noisyNet noise sample is [array([1.1332071], dtype=float32), -0.117366776]. 
=============================================
[2019-04-04 16:32:38,777] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.6845509e-08 1.4259611e-09 5.6084803e-14 1.1644526e-13 9.9999988e-01
 2.5262354e-09 5.7889301e-14], sum to 1.0000
[2019-04-04 16:32:38,781] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3184
[2019-04-04 16:32:38,802] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.43966584641768, 0.4319221035363641, 0.0, 1.0, 18764.2363597959], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3615000.0000, 
sim time next is 3615600.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.51458716784886, 0.4348061621342995, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6262155973207383, 0.6449353873780999, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40628028], dtype=float32), 1.2375803]. 
=============================================
[2019-04-04 16:32:47,472] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0064713e-08 1.4885402e-09 2.1418892e-15 2.3522986e-14 1.0000000e+00
 1.9367020e-10 1.4603080e-14], sum to 1.0000
[2019-04-04 16:32:47,474] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9365
[2019-04-04 16:32:47,506] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.666666666666667, 54.33333333333334, 113.5, 806.3333333333333, 26.0, 25.24346596021376, 0.4357946954860744, 0.0, 1.0, 18709.55386432581], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3583200.0000, 
sim time next is 3583800.0000, 
raw observation next is [-3.5, 54.5, 114.0, 816.0, 26.0, 25.18608528852008, 0.4333736219871623, 0.0, 1.0, 33975.53752175967], 
processed observation next is [0.0, 0.4782608695652174, 0.36565096952908593, 0.545, 0.38, 0.901657458563536, 0.6666666666666666, 0.5988404407100066, 0.6444578739957207, 0.0, 1.0, 0.16178827391314127], 
reward next is 0.8382, 
noisyNet noise sample is [array([0.12415651], dtype=float32), -1.883497]. 
=============================================
[2019-04-04 16:32:49,386] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0865855e-10 2.2351836e-11 4.5229754e-16 6.1773890e-16 1.0000000e+00
 9.6327918e-11 1.9181972e-16], sum to 1.0000
[2019-04-04 16:32:49,391] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2803
[2019-04-04 16:32:49,402] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.666666666666667, 49.0, 111.8333333333333, 817.0, 26.0, 25.55411720354804, 0.584784904598113, 1.0, 1.0, 20808.86853415503], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3850800.0000, 
sim time next is 3851400.0000, 
raw observation next is [1.833333333333333, 48.5, 110.6666666666667, 810.0, 26.0, 25.99444775266101, 0.6242105957092621, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5133887349953832, 0.485, 0.368888888888889, 0.8950276243093923, 0.6666666666666666, 0.6662039793884175, 0.7080701985697541, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9840078], dtype=float32), 0.5376696]. 
=============================================
[2019-04-04 16:32:50,105] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6583628e-08 4.4218669e-09 3.0623633e-14 2.3424586e-13 1.0000000e+00
 1.4909830e-10 1.3591591e-14], sum to 1.0000
[2019-04-04 16:32:50,105] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4910
[2019-04-04 16:32:50,130] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 46.0, 0.0, 0.0, 26.0, 25.346458877539, 0.4001946656421596, 0.0, 1.0, 39398.92685802391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4154400.0000, 
sim time next is 4155000.0000, 
raw observation next is [-2.166666666666667, 46.66666666666667, 0.0, 0.0, 26.0, 25.33418411348596, 0.3950901428398056, 0.0, 1.0, 39287.32134497493], 
processed observation next is [0.0, 0.08695652173913043, 0.4025854108956602, 0.46666666666666673, 0.0, 0.0, 0.6666666666666666, 0.6111820094571634, 0.6316967142799351, 0.0, 1.0, 0.18708248259511873], 
reward next is 0.8129, 
noisyNet noise sample is [array([1.3001193], dtype=float32), -1.7655421]. 
=============================================
[2019-04-04 16:32:50,140] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[77.722565]
 [77.585   ]
 [77.56933 ]
 [77.23465 ]
 [77.18302 ]], R is [[77.78832245]
 [77.82282257]
 [77.85440826]
 [77.87697601]
 [77.87477112]].
[2019-04-04 16:32:50,705] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0616630e-09 7.8714334e-11 2.2323659e-16 2.7183960e-15 1.0000000e+00
 1.3032148e-10 6.3316539e-16], sum to 1.0000
[2019-04-04 16:32:50,708] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1450
[2019-04-04 16:32:50,726] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 61.66666666666667, 118.3333333333333, 827.1666666666667, 26.0, 26.41338906913596, 0.5852328429500971, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3760800.0000, 
sim time next is 3761400.0000, 
raw observation next is [-1.166666666666667, 60.83333333333333, 117.6666666666667, 825.3333333333334, 26.0, 26.29409121968606, 0.5847115690352311, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.43028624192059095, 0.6083333333333333, 0.3922222222222223, 0.9119705340699816, 0.6666666666666666, 0.6911742683071717, 0.694903856345077, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04721711], dtype=float32), -0.9390231]. 
=============================================
[2019-04-04 16:32:58,614] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2594218e-08 1.7922853e-09 2.2105905e-13 6.4569384e-13 1.0000000e+00
 2.8509721e-09 2.8916016e-13], sum to 1.0000
[2019-04-04 16:32:58,619] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4876
[2019-04-04 16:32:58,642] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.166666666666667, 45.66666666666666, 0.0, 0.0, 26.0, 25.50091855842233, 0.5139939196344698, 0.0, 1.0, 35579.95031615561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3964200.0000, 
sim time next is 3964800.0000, 
raw observation next is [-7.333333333333334, 46.33333333333334, 0.0, 0.0, 26.0, 25.50418321105462, 0.509539746353438, 0.0, 1.0, 33949.33953691371], 
processed observation next is [1.0, 0.9130434782608695, 0.2594644506001847, 0.46333333333333343, 0.0, 0.0, 0.6666666666666666, 0.6253486009212184, 0.6698465821178127, 0.0, 1.0, 0.161663521604351], 
reward next is 0.8383, 
noisyNet noise sample is [array([-0.02872944], dtype=float32), -1.6309265]. 
=============================================
[2019-04-04 16:33:00,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8396064e-09 2.2932413e-10 7.7241003e-16 5.7174948e-14 1.0000000e+00
 4.1858425e-10 3.3213735e-15], sum to 1.0000
[2019-04-04 16:33:00,296] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9734
[2019-04-04 16:33:00,327] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.4, 72.5, 0.0, 0.0, 26.0, 25.3550323021227, 0.4812392703157455, 0.0, 1.0, 43665.2809005191], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4491000.0000, 
sim time next is 4491600.0000, 
raw observation next is [-0.4333333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.35169516544937, 0.4781297209670565, 0.0, 1.0, 43548.96672254112], 
processed observation next is [1.0, 1.0, 0.45060018467220686, 0.7266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6126412637874475, 0.6593765736556855, 0.0, 1.0, 0.20737603201210056], 
reward next is 0.7926, 
noisyNet noise sample is [array([0.04156426], dtype=float32), 0.07459951]. 
=============================================
[2019-04-04 16:33:06,098] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7510076e-09 5.3058530e-10 1.0491796e-14 1.0170509e-14 1.0000000e+00
 4.2639256e-10 1.3160111e-14], sum to 1.0000
[2019-04-04 16:33:06,099] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2834
[2019-04-04 16:33:06,111] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.833333333333333, 20.33333333333334, 94.0, 739.3333333333334, 26.0, 26.70867530201923, 0.685247873804098, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4029000.0000, 
sim time next is 4029600.0000, 
raw observation next is [-1.666666666666667, 20.66666666666667, 91.5, 725.6666666666667, 26.0, 26.78124872627836, 0.6944232978558449, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4164358264081256, 0.20666666666666672, 0.305, 0.801841620626151, 0.6666666666666666, 0.7317707271898634, 0.731474432618615, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.41398236], dtype=float32), 0.22309175]. 
=============================================
[2019-04-04 16:33:08,519] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2508551e-09 2.1444710e-09 6.6602455e-15 4.6293031e-14 1.0000000e+00
 1.8696053e-10 1.9231951e-15], sum to 1.0000
[2019-04-04 16:33:08,520] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5618
[2019-04-04 16:33:08,531] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 41.33333333333334, 191.5, 185.6666666666666, 26.0, 25.15364223446223, 0.3727043831919213, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4198800.0000, 
sim time next is 4199400.0000, 
raw observation next is [2.0, 42.0, 187.0, 89.0, 26.0, 25.08606931384682, 0.3565203415129723, 0.0, 1.0, 36758.33166552232], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.42, 0.6233333333333333, 0.09834254143646409, 0.6666666666666666, 0.5905057761539018, 0.6188401138376575, 0.0, 1.0, 0.17503967459772532], 
reward next is 0.8250, 
noisyNet noise sample is [array([0.19172484], dtype=float32), -0.5471975]. 
=============================================
[2019-04-04 16:33:08,623] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0615178e-08 7.1078645e-09 5.2625839e-14 3.8535115e-13 1.0000000e+00
 1.4640209e-09 8.6428064e-14], sum to 1.0000
[2019-04-04 16:33:08,624] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0442
[2019-04-04 16:33:08,695] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.33333333333333, 65.0, 15.5, 73.99999999999999, 26.0, 24.12756819371886, 0.1932347707114159, 1.0, 1.0, 152491.108436137], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4002000.0000, 
sim time next is 4002600.0000, 
raw observation next is [-13.16666666666667, 64.0, 30.99999999999999, 148.0, 26.0, 24.70749413096282, 0.2836979714722832, 1.0, 1.0, 104626.9240744983], 
processed observation next is [1.0, 0.30434782608695654, 0.09787626962142189, 0.64, 0.10333333333333329, 0.16353591160220995, 0.6666666666666666, 0.5589578442469018, 0.5945659904907611, 1.0, 1.0, 0.4982234479738014], 
reward next is 0.5018, 
noisyNet noise sample is [array([-0.9524943], dtype=float32), 0.4124979]. 
=============================================
[2019-04-04 16:33:10,811] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0344092e-08 8.3515939e-10 2.1875373e-14 8.0046707e-14 1.0000000e+00
 2.7414485e-10 5.0243328e-14], sum to 1.0000
[2019-04-04 16:33:10,813] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9032
[2019-04-04 16:33:10,830] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 47.0, 0.0, 0.0, 26.0, 25.38566361800856, 0.327652652311764, 0.0, 1.0, 40794.61271488109], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4253400.0000, 
sim time next is 4254000.0000, 
raw observation next is [3.0, 47.66666666666666, 0.0, 0.0, 26.0, 25.37248014022236, 0.3287813556459256, 0.0, 1.0, 47224.17807059728], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.47666666666666657, 0.0, 0.0, 0.6666666666666666, 0.6143733450185301, 0.6095937852153085, 0.0, 1.0, 0.2248770384314156], 
reward next is 0.7751, 
noisyNet noise sample is [array([0.3147767], dtype=float32), 0.61322576]. 
=============================================
[2019-04-04 16:33:10,838] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[81.79091 ]
 [81.77779 ]
 [81.82301 ]
 [81.860565]
 [81.84248 ]], R is [[81.79695129]
 [81.78472137]
 [81.81304932]
 [81.84908295]
 [81.83375549]].
[2019-04-04 16:33:12,125] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-04 16:33:12,126] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 16:33:12,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:33:12,127] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 16:33:12,128] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 16:33:12,128] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:33:12,129] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:33:12,144] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run47
[2019-04-04 16:33:12,167] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run47
[2019-04-04 16:33:12,191] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run47
[2019-04-04 16:34:49,882] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18186556], dtype=float32), 0.24415806]
[2019-04-04 16:34:49,882] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.762373582, 41.49474035, 113.06760355, 808.3937071, 26.0, 25.64625477828405, 0.495518786448806, 0.0, 1.0, 0.0]
[2019-04-04 16:34:49,882] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 16:34:49,883] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [9.4312325e-10 1.4321644e-10 3.4264246e-16 2.6265310e-15 1.0000000e+00
 3.6305667e-11 5.8315466e-16], sampled 0.8595603649253755
[2019-04-04 16:34:54,064] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.3624 239953894.2748 1605.2424
[2019-04-04 16:35:10,294] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18186556], dtype=float32), 0.24415806]
[2019-04-04 16:35:10,294] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.60808832, 85.91709489333334, 0.0, 0.0, 26.0, 25.60151085022918, 0.5126447377549491, 0.0, 1.0, 0.0]
[2019-04-04 16:35:10,294] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 16:35:10,295] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.4718139e-09 2.3015934e-10 1.4366202e-15 1.7833364e-14 1.0000000e+00
 1.8794410e-10 2.4818972e-15], sampled 0.9484785404840945
[2019-04-04 16:35:13,148] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 16:35:16,677] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 16:35:17,701] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 4600000, evaluation results [4600000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.362408215044, 239953894.27484185, 1605.2424218380552, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 16:35:23,608] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.6320495e-10 1.6758065e-11 1.0273991e-16 5.6778450e-16 1.0000000e+00
 6.3087747e-12 7.2382578e-17], sum to 1.0000
[2019-04-04 16:35:23,608] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9012
[2019-04-04 16:35:23,650] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.55, 71.16666666666667, 61.33333333333334, 327.3333333333334, 26.0, 25.43369636489835, 0.4005715969078036, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4349400.0000, 
sim time next is 4350000.0000, 
raw observation next is [4.1, 68.33333333333334, 76.66666666666667, 409.1666666666667, 26.0, 25.4997993757092, 0.4622861639351675, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5761772853185596, 0.6833333333333335, 0.2555555555555556, 0.4521178637200737, 0.6666666666666666, 0.6249832813090999, 0.6540953879783892, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7060705], dtype=float32), -1.9396809]. 
=============================================
[2019-04-04 16:35:23,658] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[93.36947 ]
 [91.85272 ]
 [90.192986]
 [88.5753  ]
 [86.82413 ]], R is [[94.75724792]
 [94.80967712]
 [94.8615799 ]
 [94.91296387]
 [94.96383667]].
[2019-04-04 16:35:28,976] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1102687e-09 1.8802505e-11 3.4369495e-16 2.2255126e-16 1.0000000e+00
 3.2435991e-11 7.2434506e-17], sum to 1.0000
[2019-04-04 16:35:28,977] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4617
[2019-04-04 16:35:28,989] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.6, 58.5, 0.0, 0.0, 26.0, 26.881646586482, 0.846078015276379, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4393800.0000, 
sim time next is 4394400.0000, 
raw observation next is [10.46666666666667, 58.66666666666666, 0.0, 0.0, 26.0, 26.838783392074, 0.8398746089571408, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7525392428439522, 0.5866666666666666, 0.0, 0.0, 0.6666666666666666, 0.7365652826728333, 0.7799582029857136, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7843543], dtype=float32), 0.9985774]. 
=============================================
[2019-04-04 16:35:30,251] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4367431e-08 8.3818347e-10 1.3587800e-15 1.0346526e-14 1.0000000e+00
 6.3632599e-10 3.4303759e-15], sum to 1.0000
[2019-04-04 16:35:30,254] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4541
[2019-04-04 16:35:30,295] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.333333333333333, 29.83333333333333, 114.0, 774.3333333333333, 26.0, 26.50985697759241, 0.5562265478449404, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4961400.0000, 
sim time next is 4962000.0000, 
raw observation next is [1.666666666666667, 29.66666666666667, 115.5, 788.6666666666667, 26.0, 26.57533981722935, 0.5669867453639352, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5087719298245615, 0.2966666666666667, 0.385, 0.8714548802946593, 0.6666666666666666, 0.7146116514357791, 0.6889955817879784, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0499724], dtype=float32), -1.6983448]. 
=============================================
[2019-04-04 16:35:30,323] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[85.67857 ]
 [85.7966  ]
 [85.94351 ]
 [86.160255]
 [86.31415 ]], R is [[85.7116394 ]
 [85.85452271]
 [85.99597931]
 [86.13601685]
 [86.2746582 ]].
[2019-04-04 16:35:37,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:35:37,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:35:37,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run35
[2019-04-04 16:35:37,922] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.4815479e-09 3.8291925e-10 5.6020527e-15 5.4313558e-14 1.0000000e+00
 1.1117769e-10 1.1087253e-14], sum to 1.0000
[2019-04-04 16:35:37,926] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2938
[2019-04-04 16:35:38,005] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.9666666666666667, 71.33333333333334, 0.0, 0.0, 26.0, 25.22255245833653, 0.3829510078717007, 0.0, 1.0, 40436.78336678357], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4518600.0000, 
sim time next is 4519200.0000, 
raw observation next is [-0.9333333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.23569937657802, 0.4293044229446605, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4367497691597415, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6029749480481682, 0.6431014743148868, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4679501], dtype=float32), 0.9593533]. 
=============================================
[2019-04-04 16:35:39,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:35:39,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:35:39,294] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run35
[2019-04-04 16:35:42,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:35:42,428] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:35:42,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run35
[2019-04-04 16:35:43,548] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0264004e-08 5.7969862e-10 1.4277730e-14 6.3691728e-14 1.0000000e+00
 3.2191447e-10 3.7706529e-14], sum to 1.0000
[2019-04-04 16:35:43,554] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6560
[2019-04-04 16:35:43,565] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 35.0, 73.83333333333334, 488.3333333333333, 26.0, 25.21209725570218, 0.4208267278909406, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4812000.0000, 
sim time next is 4812600.0000, 
raw observation next is [3.0, 34.5, 65.66666666666667, 427.6666666666667, 26.0, 25.2047885382624, 0.4080857255923309, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.345, 0.2188888888888889, 0.47255985267034994, 0.6666666666666666, 0.6003990448551999, 0.6360285751974436, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.76954705], dtype=float32), 1.7677765]. 
=============================================
[2019-04-04 16:35:44,908] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4284585e-09 8.7070889e-10 4.0038510e-15 2.0879759e-14 1.0000000e+00
 5.2755894e-10 7.3178065e-15], sum to 1.0000
[2019-04-04 16:35:44,910] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9317
[2019-04-04 16:35:44,938] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.67546716344021, 0.2765720123654719, 0.0, 1.0, 40588.60536785293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4763400.0000, 
sim time next is 4764000.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.61479886716265, 0.2675959926212834, 0.0, 1.0, 40634.7634157454], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5512332389302209, 0.5891986642070944, 0.0, 1.0, 0.19349887340831143], 
reward next is 0.8065, 
noisyNet noise sample is [array([-0.851075], dtype=float32), -0.7170688]. 
=============================================
[2019-04-04 16:35:44,953] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.51264 ]
 [81.342735]
 [81.325066]
 [81.22949 ]
 [81.29079 ]], R is [[81.45545197]
 [81.44762421]
 [81.44006348]
 [81.43268585]
 [81.425354  ]].
[2019-04-04 16:35:45,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:35:45,990] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:35:45,999] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run35
[2019-04-04 16:35:49,108] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0569050e-09 1.5549800e-09 6.6182788e-15 3.9580217e-14 1.0000000e+00
 3.3058722e-10 1.3784473e-14], sum to 1.0000
[2019-04-04 16:35:49,108] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9182
[2019-04-04 16:35:49,121] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.35754877895628, 0.2002646779981427, 0.0, 1.0, 41246.05906660381], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4770000.0000, 
sim time next is 4770600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.31044005695305, 0.1964546842205911, 0.0, 1.0, 41290.3157754593], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5258700047460874, 0.565484894740197, 0.0, 1.0, 0.19662055131171094], 
reward next is 0.8034, 
noisyNet noise sample is [array([1.4047822], dtype=float32), 0.22034717]. 
=============================================
[2019-04-04 16:35:57,180] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7417674e-09 6.9996531e-10 1.1651832e-14 1.3140529e-13 1.0000000e+00
 2.9791877e-10 1.8836864e-14], sum to 1.0000
[2019-04-04 16:35:57,181] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6416
[2019-04-04 16:35:57,194] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 26.0, 25.51932013184473, 0.4298736029722867, 0.0, 1.0, 54014.33037596649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5016600.0000, 
sim time next is 5017200.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 26.0, 25.45462104546192, 0.4276552528690216, 0.0, 1.0, 76352.17091921164], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.4, 0.0, 0.0, 0.6666666666666666, 0.62121842045516, 0.6425517509563405, 0.0, 1.0, 0.3635817662819602], 
reward next is 0.6364, 
noisyNet noise sample is [array([-3.0729907], dtype=float32), 0.619385]. 
=============================================
[2019-04-04 16:35:57,624] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:35:57,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:35:57,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run35
[2019-04-04 16:35:58,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:35:58,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:35:58,254] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run35
[2019-04-04 16:36:00,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:36:00,623] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:36:00,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run35
[2019-04-04 16:36:01,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:36:01,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:36:01,572] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run35
[2019-04-04 16:36:01,623] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:36:01,623] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:36:01,628] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run35
[2019-04-04 16:36:03,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:36:03,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:36:03,232] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run35
[2019-04-04 16:36:03,755] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:36:03,755] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:36:03,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run35
[2019-04-04 16:36:05,114] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:36:05,114] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:36:05,119] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run35
[2019-04-04 16:36:05,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:36:05,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:36:05,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run35
[2019-04-04 16:36:06,649] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:36:06,650] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:36:06,655] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run35
[2019-04-04 16:36:07,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:36:07,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:36:07,180] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run35
[2019-04-04 16:36:07,819] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:36:07,819] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:36:07,823] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run35
[2019-04-04 16:36:09,270] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.8774593e-09 1.6810384e-10 1.0921348e-16 4.8423342e-14 1.0000000e+00
 2.7661770e-10 2.2061318e-15], sum to 1.0000
[2019-04-04 16:36:09,270] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5969
[2019-04-04 16:36:09,350] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 32.33333333333334, 0.0, 26.0, 22.94383014226748, -0.1925630636117169, 0.0, 1.0, 59349.60713792682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 33000.0000, 
sim time next is 33600.0000, 
raw observation next is [7.7, 93.0, 35.16666666666666, 0.0, 26.0, 23.09922377279745, -0.167245102879706, 0.0, 1.0, 58869.7088830855], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.11722222222222219, 0.0, 0.6666666666666666, 0.4249353143997876, 0.44425163237343135, 0.0, 1.0, 0.2803319470623119], 
reward next is 0.7197, 
noisyNet noise sample is [array([-0.7604487], dtype=float32), 0.2160209]. 
=============================================
[2019-04-04 16:36:17,151] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2281094e-10 6.9894167e-11 5.3523052e-17 4.4361699e-16 1.0000000e+00
 1.3398617e-11 2.8671909e-16], sum to 1.0000
[2019-04-04 16:36:17,154] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4926
[2019-04-04 16:36:17,234] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 86.0, 83.0, 0.0, 26.0, 24.41718533840986, 0.1510286983186968, 0.0, 1.0, 29232.89322347052], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 50400.0000, 
sim time next is 51000.0000, 
raw observation next is [7.616666666666667, 86.0, 81.66666666666666, 0.0, 26.0, 24.42810398181012, 0.1566890980686547, 0.0, 1.0, 30201.51050098607], 
processed observation next is [0.0, 0.6086956521739131, 0.6735918744228995, 0.86, 0.2722222222222222, 0.0, 0.6666666666666666, 0.5356753318175101, 0.5522296993562182, 0.0, 1.0, 0.14381671667136225], 
reward next is 0.8562, 
noisyNet noise sample is [array([-0.18649405], dtype=float32), 0.16988716]. 
=============================================
[2019-04-04 16:36:17,243] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[90.11776 ]
 [90.21156 ]
 [90.29484 ]
 [90.331535]
 [90.33754 ]], R is [[89.97225189]
 [89.93332672]
 [89.87858582]
 [89.78755188]
 [89.67433167]].
[2019-04-04 16:36:18,823] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6273021e-09 5.0082688e-10 3.3096491e-15 5.1850285e-14 1.0000000e+00
 2.3665420e-10 6.2568137e-16], sum to 1.0000
[2019-04-04 16:36:18,823] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9893
[2019-04-04 16:36:18,873] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.8, 61.0, 41.0, 4.5, 26.0, 25.39044886956636, 0.2590533967950376, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 118800.0000, 
sim time next is 119400.0000, 
raw observation next is [-7.8, 63.16666666666667, 42.33333333333334, 2.999999999999999, 26.0, 25.32839872079835, 0.2555946615142058, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.6316666666666667, 0.14111111111111113, 0.0033149171270718224, 0.6666666666666666, 0.6106998933998625, 0.5851982205047352, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37472472], dtype=float32), -1.1830332]. 
=============================================
[2019-04-04 16:36:20,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2516775e-09 6.3932842e-10 1.3603462e-15 1.3915711e-14 1.0000000e+00
 6.5968958e-10 8.7180680e-15], sum to 1.0000
[2019-04-04 16:36:20,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9820
[2019-04-04 16:36:20,602] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 61.0, 145.0, 231.9999999999999, 26.0, 25.92492206423267, 0.4448538507722042, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 137400.0000, 
sim time next is 138000.0000, 
raw observation next is [-6.700000000000001, 61.0, 146.5, 169.0, 26.0, 25.90161525428586, 0.4339716532546312, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.48833333333333334, 0.1867403314917127, 0.6666666666666666, 0.6584679378571551, 0.6446572177515437, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30173528], dtype=float32), -0.2586263]. 
=============================================
[2019-04-04 16:36:20,609] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[81.18284 ]
 [81.770515]
 [82.34851 ]
 [82.930016]
 [83.47792 ]], R is [[80.69233704]
 [80.88541412]
 [81.07656097]
 [81.26579285]
 [81.45313263]].
[2019-04-04 16:36:30,033] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.6173025e-10 2.3176372e-10 7.2796034e-16 3.2601671e-14 1.0000000e+00
 2.7486777e-11 4.5323711e-16], sum to 1.0000
[2019-04-04 16:36:30,035] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6069
[2019-04-04 16:36:30,049] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.766666666666667, 88.66666666666667, 0.0, 0.0, 26.0, 25.01204825982068, 0.2613228568739197, 0.0, 1.0, 39463.7431824008], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 523200.0000, 
sim time next is 523800.0000, 
raw observation next is [4.65, 88.5, 0.0, 0.0, 26.0, 25.0285444592274, 0.2584000868868433, 0.0, 1.0, 39520.93334243803], 
processed observation next is [0.0, 0.043478260869565216, 0.5914127423822716, 0.885, 0.0, 0.0, 0.6666666666666666, 0.5857120382689501, 0.5861333622956144, 0.0, 1.0, 0.18819492067827634], 
reward next is 0.8118, 
noisyNet noise sample is [array([-1.0984216], dtype=float32), 0.08640824]. 
=============================================
[2019-04-04 16:36:36,234] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6436394e-07 2.2755962e-08 4.9477618e-12 1.5645664e-11 9.9999964e-01
 7.5062850e-08 6.0897728e-13], sum to 1.0000
[2019-04-04 16:36:36,234] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2119
[2019-04-04 16:36:36,284] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.26581323464423, -0.1446673006140628, 0.0, 1.0, 48391.01285250529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 352800.0000, 
sim time next is 353400.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.20465457082566, -0.1682197647735297, 0.0, 1.0, 48611.32968001892], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.6666666666666666, 0.43372121423547166, 0.44392674507549007, 0.0, 1.0, 0.23148252228580438], 
reward next is 0.7685, 
noisyNet noise sample is [array([0.3924273], dtype=float32), -0.68136615]. 
=============================================
[2019-04-04 16:36:38,253] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.55585791e-09 2.42889070e-10 2.53122477e-15 9.24192024e-14
 1.00000000e+00 1.14676664e-10 1.00923628e-14], sum to 1.0000
[2019-04-04 16:36:38,253] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9013
[2019-04-04 16:36:38,308] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.23333333333333, 82.0, 36.66666666666666, 694.5, 26.0, 25.35711787261956, 0.220502922722089, 1.0, 1.0, 38015.83886369481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 379200.0000, 
sim time next is 379800.0000, 
raw observation next is [-15.05, 78.0, 39.0, 738.0, 26.0, 25.38449185070468, 0.2460616842839414, 1.0, 1.0, 53266.67675694892], 
processed observation next is [1.0, 0.391304347826087, 0.0457063711911357, 0.78, 0.13, 0.8154696132596685, 0.6666666666666666, 0.6153743208920567, 0.5820205614279804, 1.0, 1.0, 0.25365084169975677], 
reward next is 0.7463, 
noisyNet noise sample is [array([0.4257503], dtype=float32), -0.4119604]. 
=============================================
[2019-04-04 16:36:41,732] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8184581e-07 9.4835265e-08 3.7723869e-12 3.0063084e-11 9.9999905e-01
 8.0704702e-08 7.1832557e-12], sum to 1.0000
[2019-04-04 16:36:41,732] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2512
[2019-04-04 16:36:41,749] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.1, 69.66666666666667, 0.0, 0.0, 26.0, 22.6748325099077, -0.2577075683602514, 0.0, 1.0, 49309.35487379272], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 357000.0000, 
sim time next is 357600.0000, 
raw observation next is [-15.2, 70.33333333333334, 0.0, 0.0, 26.0, 22.63755588034809, -0.2684941272091597, 0.0, 1.0, 49318.87435764341], 
processed observation next is [1.0, 0.13043478260869565, 0.04155124653739613, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.38646299002900736, 0.4105019575969468, 0.0, 1.0, 0.23485178265544482], 
reward next is 0.7651, 
noisyNet noise sample is [array([0.7734145], dtype=float32), -0.6009853]. 
=============================================
[2019-04-04 16:36:51,334] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7102458e-06 1.2667424e-07 1.5195137e-11 4.2309430e-11 9.9999809e-01
 1.4986593e-07 1.8796766e-11], sum to 1.0000
[2019-04-04 16:36:51,334] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4881
[2019-04-04 16:36:51,427] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.3, 42.5, 0.0, 0.0, 26.0, 22.4965068315464, -0.3338943571010537, 0.0, 1.0, 46024.90226123131], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 457800.0000, 
sim time next is 458400.0000, 
raw observation next is [-8.2, 42.0, 0.0, 0.0, 26.0, 22.53838770817008, -0.2641602647133505, 1.0, 1.0, 202324.9093070951], 
processed observation next is [1.0, 0.30434782608695654, 0.23545706371191139, 0.42, 0.0, 0.0, 0.6666666666666666, 0.3781989756808401, 0.41194657842888316, 1.0, 1.0, 0.9634519490814053], 
reward next is 0.0365, 
noisyNet noise sample is [array([0.10222998], dtype=float32), 0.20588744]. 
=============================================
[2019-04-04 16:36:54,893] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.8173816e-09 3.2211100e-10 1.8917255e-15 4.9122991e-14 1.0000000e+00
 5.0255317e-10 3.2342272e-15], sum to 1.0000
[2019-04-04 16:36:54,893] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5020
[2019-04-04 16:36:54,925] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.633333333333333, 79.33333333333334, 0.0, 0.0, 26.0, 24.73016002560378, 0.2302153155016655, 0.0, 1.0, 40793.1002559886], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 861600.0000, 
sim time next is 862200.0000, 
raw observation next is [-2.55, 79.5, 0.0, 0.0, 26.0, 24.6989905086281, 0.2250003971687735, 0.0, 1.0, 40658.50822053703], 
processed observation next is [1.0, 1.0, 0.3919667590027701, 0.795, 0.0, 0.0, 0.6666666666666666, 0.5582492090523417, 0.5750001323895911, 0.0, 1.0, 0.19361194390731917], 
reward next is 0.8064, 
noisyNet noise sample is [array([-1.6577975], dtype=float32), 0.029747698]. 
=============================================
[2019-04-04 16:36:57,167] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0024857e-09 8.3741876e-12 5.9236063e-16 1.8194706e-14 1.0000000e+00
 2.7995405e-11 2.3593417e-16], sum to 1.0000
[2019-04-04 16:36:57,167] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5404
[2019-04-04 16:36:57,202] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 86.33333333333333, 0.0, 0.0, 26.0, 25.15082842321876, 0.3093559024933675, 0.0, 1.0, 43777.46517673419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 591000.0000, 
sim time next is 591600.0000, 
raw observation next is [-2.8, 85.66666666666667, 0.0, 0.0, 26.0, 25.12969152148893, 0.3040057792005885, 0.0, 1.0, 43519.88621633604], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5941409601240775, 0.6013352597335295, 0.0, 1.0, 0.207237553411124], 
reward next is 0.7928, 
noisyNet noise sample is [array([-1.3527575], dtype=float32), 0.86216086]. 
=============================================
[2019-04-04 16:37:01,750] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.4564585e-10 1.2615541e-10 3.0910562e-16 5.1781276e-15 1.0000000e+00
 4.6946745e-11 3.5525626e-16], sum to 1.0000
[2019-04-04 16:37:01,750] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1776
[2019-04-04 16:37:01,794] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 76.0, 52.99999999999999, 16.33333333333333, 26.0, 25.7085610999198, 0.3040692127652189, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 723000.0000, 
sim time next is 723600.0000, 
raw observation next is [-2.3, 76.0, 65.0, 24.5, 26.0, 25.78850472143813, 0.3082369895711872, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3988919667590028, 0.76, 0.21666666666666667, 0.02707182320441989, 0.6666666666666666, 0.6490420601198442, 0.6027456631903957, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8617604], dtype=float32), -0.4238591]. 
=============================================
[2019-04-04 16:37:11,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.4840376e-09 2.3836841e-10 9.2951755e-16 8.6220229e-15 1.0000000e+00
 3.3951109e-10 7.1565297e-15], sum to 1.0000
[2019-04-04 16:37:11,408] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1683
[2019-04-04 16:37:11,469] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 80.16666666666667, 69.66666666666666, 0.0, 26.0, 26.30753802702824, 0.4623724381611289, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 828600.0000, 
sim time next is 829200.0000, 
raw observation next is [-3.9, 81.33333333333334, 64.33333333333333, 0.0, 26.0, 26.30578184367622, 0.4596517849655453, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.8133333333333335, 0.21444444444444444, 0.0, 0.6666666666666666, 0.6921484869730182, 0.6532172616551818, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5088707], dtype=float32), -0.91441864]. 
=============================================
[2019-04-04 16:37:14,720] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0789128e-09 9.5536731e-11 1.0376878e-15 2.5623143e-14 1.0000000e+00
 2.2906184e-10 2.1462976e-15], sum to 1.0000
[2019-04-04 16:37:14,723] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0297
[2019-04-04 16:37:14,767] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 25.59399549704215, 0.3830377520661661, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 839400.0000, 
sim time next is 840000.0000, 
raw observation next is [-3.9, 84.66666666666667, 0.0, 0.0, 26.0, 25.80338652824813, 0.3683900876079432, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8466666666666667, 0.0, 0.0, 0.6666666666666666, 0.6502822106873442, 0.6227966958693144, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42866063], dtype=float32), -0.07381219]. 
=============================================
[2019-04-04 16:37:14,778] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[86.33495 ]
 [86.690025]
 [86.81324 ]
 [85.857445]
 [85.6285  ]], R is [[86.22402191]
 [86.36178589]
 [86.49816895]
 [85.688591  ]
 [85.60863495]].
[2019-04-04 16:37:16,779] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3682002e-11 3.6078538e-12 1.1406719e-17 3.7181713e-16 1.0000000e+00
 5.2750907e-12 8.0249556e-18], sum to 1.0000
[2019-04-04 16:37:16,779] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1871
[2019-04-04 16:37:16,802] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.7, 98.66666666666666, 0.0, 0.0, 26.0, 25.50047278851907, 0.6086856166769475, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1284000.0000, 
sim time next is 1284600.0000, 
raw observation next is [5.600000000000001, 99.33333333333334, 0.0, 0.0, 26.0, 25.52826701024134, 0.602905717538565, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.6177285318559558, 0.9933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6273555841867783, 0.700968572512855, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.97021544], dtype=float32), 1.2380543]. 
=============================================
[2019-04-04 16:37:18,462] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2371124e-10 5.8144991e-12 3.8882482e-17 3.2299349e-16 1.0000000e+00
 2.2862436e-11 7.1988529e-17], sum to 1.0000
[2019-04-04 16:37:18,464] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3860
[2019-04-04 16:37:18,500] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.600000000000001, 92.66666666666667, 24.0, 0.0, 26.0, 25.76920356991501, 0.3958484809577676, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 922800.0000, 
sim time next is 923400.0000, 
raw observation next is [4.7, 92.5, 18.0, 0.0, 26.0, 25.72357548480072, 0.2908680021155182, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.592797783933518, 0.925, 0.06, 0.0, 0.6666666666666666, 0.64363129040006, 0.5969560007051727, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9071101], dtype=float32), -0.20393634]. 
=============================================
[2019-04-04 16:37:21,022] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0191256e-09 4.7980853e-10 8.5738380e-16 2.8097965e-14 1.0000000e+00
 6.8118330e-11 1.1159934e-15], sum to 1.0000
[2019-04-04 16:37:21,026] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0200
[2019-04-04 16:37:21,058] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.35, 90.5, 0.0, 0.0, 26.0, 24.98561020270904, 0.4962619311861744, 0.0, 1.0, 128476.1828372091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1456200.0000, 
sim time next is 1456800.0000, 
raw observation next is [1.433333333333333, 90.0, 0.0, 0.0, 26.0, 25.10536806942533, 0.5232939731319182, 0.0, 1.0, 71090.98304658936], 
processed observation next is [1.0, 0.8695652173913043, 0.502308402585411, 0.9, 0.0, 0.0, 0.6666666666666666, 0.5921140057854443, 0.674431324377306, 0.0, 1.0, 0.3385284906980446], 
reward next is 0.6615, 
noisyNet noise sample is [array([0.5881473], dtype=float32), -1.2781744]. 
=============================================
[2019-04-04 16:37:22,429] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.6003258e-10 1.0906013e-11 4.2927650e-17 3.3447813e-15 1.0000000e+00
 2.0983992e-11 2.6490590e-16], sum to 1.0000
[2019-04-04 16:37:22,430] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0729
[2019-04-04 16:37:22,461] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 24.70086440426074, 0.3366543195328385, 0.0, 1.0, 103112.1341272378], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 937800.0000, 
sim time next is 938400.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 24.78065630819543, 0.3535527096759234, 0.0, 1.0, 61907.96474093026], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.565054692349619, 0.6178509032253078, 0.0, 1.0, 0.2947998320996679], 
reward next is 0.7052, 
noisyNet noise sample is [array([0.5027056], dtype=float32), 0.89612156]. 
=============================================
[2019-04-04 16:37:28,202] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.5021316e-10 1.3316993e-10 4.4340044e-16 6.9906159e-15 1.0000000e+00
 2.9600922e-11 1.4008584e-16], sum to 1.0000
[2019-04-04 16:37:28,205] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1758
[2019-04-04 16:37:28,219] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.09999999999999999, 90.33333333333334, 0.0, 0.0, 26.0, 25.21365417977136, 0.4197741944862929, 0.0, 1.0, 43043.18319297626], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1739400.0000, 
sim time next is 1740000.0000, 
raw observation next is [-0.2, 89.66666666666667, 0.0, 0.0, 26.0, 25.19614758077373, 0.4151683824499144, 0.0, 1.0, 43074.70186752963], 
processed observation next is [0.0, 0.13043478260869565, 0.4570637119113574, 0.8966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5996789650644775, 0.6383894608166382, 0.0, 1.0, 0.20511762794061728], 
reward next is 0.7949, 
noisyNet noise sample is [array([0.0822534], dtype=float32), -0.33077982]. 
=============================================
[2019-04-04 16:37:28,223] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[88.92073]
 [88.79445]
 [88.83607]
 [88.78062]
 [88.84935]], R is [[88.72749329]
 [88.63524628]
 [88.54407501]
 [88.45397186]
 [88.36489105]].
[2019-04-04 16:37:32,660] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7956993e-10 4.3125538e-12 8.3847750e-18 2.7651965e-17 1.0000000e+00
 2.5664365e-11 4.2304680e-18], sum to 1.0000
[2019-04-04 16:37:32,661] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4025
[2019-04-04 16:37:32,683] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.13333333333333, 59.66666666666667, 213.3333333333333, 222.1666666666667, 26.0, 26.94132035141762, 0.7923946706538979, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1596000.0000, 
sim time next is 1596600.0000, 
raw observation next is [10.5, 59.0, 216.0, 249.0, 26.0, 26.99750847200023, 0.8087422834896266, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7534626038781165, 0.59, 0.72, 0.2751381215469613, 0.6666666666666666, 0.7497923726666859, 0.7695807611632088, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1895447], dtype=float32), 1.3882688]. 
=============================================
[2019-04-04 16:37:33,083] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7542635e-09 3.8594092e-10 3.6166537e-15 8.9208839e-14 1.0000000e+00
 2.3918716e-11 8.8572160e-15], sum to 1.0000
[2019-04-04 16:37:33,083] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4994
[2019-04-04 16:37:33,087] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 26.0, 23.86640422915267, 0.219573235368984, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1222200.0000, 
sim time next is 1222800.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 26.0, 23.83822058241037, 0.2167560391762901, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.6666666666666666, 0.4865183818675307, 0.5722520130587634, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4787822], dtype=float32), 0.54213905]. 
=============================================
[2019-04-04 16:37:33,232] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.7907727e-11 2.3157171e-12 9.5017021e-18 2.6221146e-16 1.0000000e+00
 1.5761516e-12 1.9157976e-17], sum to 1.0000
[2019-04-04 16:37:33,234] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1855
[2019-04-04 16:37:33,248] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.53018345483856, 0.5951283877924668, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1285200.0000, 
sim time next is 1285800.0000, 
raw observation next is [5.500000000000001, 100.0, 0.0, 0.0, 26.0, 25.50893177496982, 0.58889453863897, 0.0, 1.0, 25460.43694372525], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6257443145808184, 0.6962981795463233, 0.0, 1.0, 0.12124017592250119], 
reward next is 0.8788, 
noisyNet noise sample is [array([1.0521514], dtype=float32), -0.07472662]. 
=============================================
[2019-04-04 16:37:33,866] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2641463e-09 2.6293030e-11 4.2741699e-17 3.1902596e-16 1.0000000e+00
 1.7560259e-11 2.6896981e-16], sum to 1.0000
[2019-04-04 16:37:33,866] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9970
[2019-04-04 16:37:33,884] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 64.0, 0.0, 26.0, 26.09554245711539, 0.5845045098020911, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1331400.0000, 
sim time next is 1332000.0000, 
raw observation next is [0.5, 92.0, 73.5, 0.0, 26.0, 26.06933093980359, 0.5858146544161991, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4764542936288089, 0.92, 0.245, 0.0, 0.6666666666666666, 0.6724442449836324, 0.6952715514720663, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11822078], dtype=float32), 0.69243497]. 
=============================================
[2019-04-04 16:37:33,894] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[93.558525]
 [93.577545]
 [93.63422 ]
 [93.83064 ]
 [94.05975 ]], R is [[93.67222595]
 [93.73550415]
 [93.79814911]
 [93.86016846]
 [93.92156982]].
[2019-04-04 16:37:35,340] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7669359e-10 2.7871799e-11 1.3142683e-17 2.2194254e-16 1.0000000e+00
 1.3061006e-11 2.9796088e-17], sum to 1.0000
[2019-04-04 16:37:35,343] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0156
[2019-04-04 16:37:35,357] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.05, 97.0, 0.0, 0.0, 26.0, 25.59157854665773, 0.5383218821252674, 0.0, 1.0, 34057.44987478071], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1661400.0000, 
sim time next is 1662000.0000, 
raw observation next is [5.866666666666667, 97.0, 0.0, 0.0, 26.0, 25.52834463412115, 0.5318575765883615, 0.0, 1.0, 65498.16514955866], 
processed observation next is [1.0, 0.21739130434782608, 0.6251154201292707, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6273620528434293, 0.6772858588627871, 0.0, 1.0, 0.3118960245217079], 
reward next is 0.6881, 
noisyNet noise sample is [array([-0.64658433], dtype=float32), -0.11704574]. 
=============================================
[2019-04-04 16:37:35,366] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[92.991295]
 [92.99325 ]
 [93.19735 ]
 [93.22228 ]
 [93.11009 ]], R is [[92.88964081]
 [92.79856873]
 [92.87058258]
 [92.78469086]
 [92.58843231]].
[2019-04-04 16:37:48,451] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2523820e-09 1.7201154e-10 1.3813575e-15 1.2377881e-14 1.0000000e+00
 1.5877517e-10 4.7624936e-16], sum to 1.0000
[2019-04-04 16:37:48,452] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5587
[2019-04-04 16:37:48,465] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.416666666666667, 77.0, 0.0, 0.0, 26.0, 26.0733046707324, 0.6296122677314592, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1548600.0000, 
sim time next is 1549200.0000, 
raw observation next is [6.233333333333333, 78.0, 0.0, 0.0, 26.0, 26.00307684138615, 0.608577350971204, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6352723915050786, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6669230701155126, 0.7028591169904014, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7653823], dtype=float32), 0.15755686]. 
=============================================
[2019-04-04 16:37:49,299] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.9008550e-11 1.8295016e-12 7.6990580e-18 5.2620512e-18 1.0000000e+00
 1.7622256e-12 4.6483696e-18], sum to 1.0000
[2019-04-04 16:37:49,302] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1976
[2019-04-04 16:37:49,308] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.4, 93.0, 94.0, 704.0, 26.0, 26.07850338486253, 0.6292648129571005, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1512000.0000, 
sim time next is 1512600.0000, 
raw observation next is [4.866666666666667, 89.66666666666667, 96.0, 702.6666666666667, 26.0, 26.25145992693346, 0.6347437841269329, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5974145891043399, 0.8966666666666667, 0.32, 0.7764272559852671, 0.6666666666666666, 0.6876216605777884, 0.7115812613756444, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1647768], dtype=float32), -0.11743572]. 
=============================================
[2019-04-04 16:38:01,099] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1491154e-07 2.4904081e-08 2.7442640e-13 2.0548422e-12 9.9999988e-01
 2.4945964e-08 1.0667235e-12], sum to 1.0000
[2019-04-04 16:38:01,100] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3298
[2019-04-04 16:38:01,127] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.483333333333334, 78.66666666666667, 0.0, 0.0, 26.0, 23.72033704680488, -0.06885211425472014, 0.0, 1.0, 45157.24580353095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1915800.0000, 
sim time next is 1916400.0000, 
raw observation next is [-8.566666666666666, 79.33333333333334, 0.0, 0.0, 26.0, 23.69219643196866, -0.07374439484303724, 0.0, 1.0, 45241.58457821688], 
processed observation next is [1.0, 0.17391304347826086, 0.22530009233610343, 0.7933333333333334, 0.0, 0.0, 0.6666666666666666, 0.4743497026640551, 0.47541853505232096, 0.0, 1.0, 0.21543611703912802], 
reward next is 0.7846, 
noisyNet noise sample is [array([1.0575225], dtype=float32), -0.5438076]. 
=============================================
[2019-04-04 16:38:01,154] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1504048e-07 3.2331339e-08 2.7447505e-13 2.9804301e-11 9.9999976e-01
 1.1851185e-07 9.5218256e-13], sum to 1.0000
[2019-04-04 16:38:01,154] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7165
[2019-04-04 16:38:01,184] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.65, 80.0, 0.0, 0.0, 26.0, 23.66567746485026, -0.07956074174271732, 0.0, 1.0, 45283.45904593596], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1917000.0000, 
sim time next is 1917600.0000, 
raw observation next is [-8.733333333333334, 80.66666666666667, 0.0, 0.0, 26.0, 23.64070896793135, -0.08477908298899073, 0.0, 1.0, 45300.64057085157], 
processed observation next is [1.0, 0.17391304347826086, 0.22068328716528163, 0.8066666666666668, 0.0, 0.0, 0.6666666666666666, 0.47005908066094576, 0.4717403056703364, 0.0, 1.0, 0.21571733605167412], 
reward next is 0.7843, 
noisyNet noise sample is [array([-0.05494229], dtype=float32), 0.9407718]. 
=============================================
[2019-04-04 16:38:03,186] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4214031e-09 4.3418250e-10 7.8483415e-16 9.5527791e-15 1.0000000e+00
 4.4874657e-10 7.8749722e-16], sum to 1.0000
[2019-04-04 16:38:03,187] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9534
[2019-04-04 16:38:03,245] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.883333333333333, 75.66666666666666, 191.3333333333333, 160.6666666666667, 26.0, 25.74678207992311, 0.3170436465611439, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1939800.0000, 
sim time next is 1940400.0000, 
raw observation next is [-5.6, 75.0, 201.5, 123.0, 26.0, 25.71021253659253, 0.3177921883230682, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.30747922437673136, 0.75, 0.6716666666666666, 0.13591160220994475, 0.6666666666666666, 0.6425177113827107, 0.6059307294410227, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5487316], dtype=float32), 0.018608423]. 
=============================================
[2019-04-04 16:38:06,526] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0161427e-08 2.9278574e-10 2.8168890e-15 7.4191684e-14 1.0000000e+00
 2.6378538e-10 7.7736754e-15], sum to 1.0000
[2019-04-04 16:38:06,534] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0959
[2019-04-04 16:38:06,606] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.1, 71.66666666666667, 162.6666666666667, 54.00000000000001, 26.0, 24.96335824706727, 0.2639562418564828, 0.0, 1.0, 40201.99451361896], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1857000.0000, 
sim time next is 1857600.0000, 
raw observation next is [-5.0, 71.0, 152.0, 40.5, 26.0, 24.98434372107353, 0.2632274245334646, 0.0, 1.0, 30976.7040338974], 
processed observation next is [0.0, 0.5217391304347826, 0.32409972299168976, 0.71, 0.5066666666666667, 0.044751381215469614, 0.6666666666666666, 0.582028643422794, 0.5877424748444882, 0.0, 1.0, 0.14750811444713047], 
reward next is 0.8525, 
noisyNet noise sample is [array([-0.4525876], dtype=float32), -0.23533219]. 
=============================================
[2019-04-04 16:38:12,949] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.05252712e-09 2.25505836e-10 6.00066092e-16 1.13912565e-14
 1.00000000e+00 3.07772731e-11 1.94048345e-15], sum to 1.0000
[2019-04-04 16:38:12,950] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1424
[2019-04-04 16:38:12,987] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.1, 71.66666666666667, 107.0, 300.6666666666667, 26.0, 26.02729742006818, 0.4225753403472534, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2195400.0000, 
sim time next is 2196000.0000, 
raw observation next is [-5.0, 71.0, 109.5, 225.5, 26.0, 26.01552172714307, 0.4176920555865027, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.32409972299168976, 0.71, 0.365, 0.24917127071823206, 0.6666666666666666, 0.6679601439285893, 0.6392306851955009, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3358164], dtype=float32), 0.33968943]. 
=============================================
[2019-04-04 16:38:12,995] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[84.40733]
 [84.79576]
 [84.87011]
 [84.7585 ]
 [84.69722]], R is [[84.01295471]
 [84.17282867]
 [84.33110046]
 [84.48779297]
 [84.64291382]].
[2019-04-04 16:38:13,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.46803729e-08 4.59704008e-09 1.28641596e-14 1.03970950e-13
 9.99999881e-01 1.20445443e-09 1.91112148e-14], sum to 1.0000
[2019-04-04 16:38:13,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7363
[2019-04-04 16:38:13,234] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.2383888176091, 0.08507686143774777, 0.0, 1.0, 41093.74538437244], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2004000.0000, 
sim time next is 2004600.0000, 
raw observation next is [-6.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.22569323388572, 0.08144313660149123, 0.0, 1.0, 41130.94272612949], 
processed observation next is [1.0, 0.17391304347826086, 0.29362880886426596, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5188077694904768, 0.527147712200497, 0.0, 1.0, 0.19586163202918805], 
reward next is 0.8041, 
noisyNet noise sample is [array([-1.5937867], dtype=float32), 0.35641015]. 
=============================================
[2019-04-04 16:38:14,450] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.0900499e-08 4.6568212e-09 1.8881250e-14 8.3974868e-13 9.9999988e-01
 1.1576807e-09 1.9426767e-14], sum to 1.0000
[2019-04-04 16:38:14,453] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9190
[2019-04-04 16:38:14,481] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 88.33333333333334, 0.0, 0.0, 26.0, 24.33172137236876, 0.1258417183561126, 0.0, 1.0, 43419.99033754042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2090400.0000, 
sim time next is 2091000.0000, 
raw observation next is [-6.1, 87.66666666666666, 0.0, 0.0, 26.0, 24.27567584507393, 0.1103144950452029, 0.0, 1.0, 43443.14289678018], 
processed observation next is [1.0, 0.17391304347826086, 0.29362880886426596, 0.8766666666666666, 0.0, 0.0, 0.6666666666666666, 0.5229729870894942, 0.536771498348401, 0.0, 1.0, 0.20687210903228656], 
reward next is 0.7931, 
noisyNet noise sample is [array([0.58932227], dtype=float32), 2.147571]. 
=============================================
[2019-04-04 16:38:14,485] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[77.5603  ]
 [77.67908 ]
 [77.792274]
 [77.90628 ]
 [77.99343 ]], R is [[77.43302917]
 [77.45193481]
 [77.47077179]
 [77.48859406]
 [77.50565338]].
[2019-04-04 16:38:14,741] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1417649e-09 7.5014536e-11 3.6789508e-15 2.0721421e-14 1.0000000e+00
 1.5320195e-10 8.4788321e-16], sum to 1.0000
[2019-04-04 16:38:14,742] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9429
[2019-04-04 16:38:14,787] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.166666666666666, 76.33333333333334, 181.1666666666667, 198.3333333333333, 26.0, 25.80757290739385, 0.3218563386201945, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1939200.0000, 
sim time next is 1939800.0000, 
raw observation next is [-5.883333333333333, 75.66666666666666, 191.3333333333333, 160.6666666666667, 26.0, 25.74678148480752, 0.317044719009646, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2996306555863343, 0.7566666666666666, 0.6377777777777777, 0.1775322283609577, 0.6666666666666666, 0.64556512373396, 0.6056815730032153, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11019953], dtype=float32), 0.9589791]. 
=============================================
[2019-04-04 16:38:24,002] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1834146e-09 7.5536183e-10 1.9323810e-14 7.6541814e-14 1.0000000e+00
 9.8544817e-10 2.2736031e-14], sum to 1.0000
[2019-04-04 16:38:24,003] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7800
[2019-04-04 16:38:24,066] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.75, 66.5, 86.0, 0.0, 26.0, 26.23126169467157, 0.4832084465382914, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2129400.0000, 
sim time next is 2130000.0000, 
raw observation next is [-4.666666666666667, 66.0, 76.0, 0.0, 26.0, 26.19650846481104, 0.3698032425726821, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3333333333333333, 0.66, 0.25333333333333335, 0.0, 0.6666666666666666, 0.6830423720675866, 0.6232677475242273, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8679133], dtype=float32), -0.990126]. 
=============================================
[2019-04-04 16:38:24,069] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[79.1983  ]
 [79.23327 ]
 [79.30804 ]
 [79.30362 ]
 [79.175705]], R is [[79.22576141]
 [79.4335022 ]
 [79.63916779]
 [79.84277344]
 [80.04434967]].
[2019-04-04 16:38:24,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.1146950e-09 2.5753883e-09 3.6649854e-15 2.5887590e-13 1.0000000e+00
 9.6188246e-10 1.8752156e-14], sum to 1.0000
[2019-04-04 16:38:24,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8367
[2019-04-04 16:38:24,671] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 66.33333333333333, 0.0, 0.0, 26.0, 24.17204015059864, 0.1080277332314757, 0.0, 1.0, 41138.57092740813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2353200.0000, 
sim time next is 2353800.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.14957927692748, 0.1030538797508116, 0.0, 1.0, 41174.56090611978], 
processed observation next is [0.0, 0.21739130434782608, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5124649397439566, 0.5343512932502705, 0.0, 1.0, 0.19606933764818943], 
reward next is 0.8039, 
noisyNet noise sample is [array([0.04509368], dtype=float32), -0.60995936]. 
=============================================
[2019-04-04 16:38:35,303] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.6499536e-09 4.0851983e-10 3.6440463e-15 1.9147275e-13 1.0000000e+00
 2.3896669e-09 7.8282226e-15], sum to 1.0000
[2019-04-04 16:38:35,303] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1540
[2019-04-04 16:38:35,320] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 26.0, 24.78970726491308, 0.2670124122433631, 0.0, 1.0, 44203.4439275136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2242800.0000, 
sim time next is 2243400.0000, 
raw observation next is [-6.283333333333333, 75.5, 0.0, 0.0, 26.0, 24.72898716201501, 0.2551157193383058, 0.0, 1.0, 44154.56163385813], 
processed observation next is [1.0, 1.0, 0.288550323176362, 0.755, 0.0, 0.0, 0.6666666666666666, 0.5607489301679175, 0.5850385731127686, 0.0, 1.0, 0.21025981730408633], 
reward next is 0.7897, 
noisyNet noise sample is [array([0.5580458], dtype=float32), 0.30666327]. 
=============================================
[2019-04-04 16:38:37,264] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2841793e-08 3.7531618e-09 9.0902195e-15 4.8921891e-13 1.0000000e+00
 1.2638860e-09 1.0408758e-14], sum to 1.0000
[2019-04-04 16:38:37,264] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9105
[2019-04-04 16:38:37,321] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666666, 70.16666666666667, 0.0, 0.0, 26.0, 25.41523387828481, 0.3803971607674866, 1.0, 1.0, 27581.76156181923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2229000.0000, 
sim time next is 2229600.0000, 
raw observation next is [-4.733333333333333, 70.33333333333334, 0.0, 0.0, 26.0, 25.24432452775536, 0.3543646827905398, 1.0, 1.0, 32414.26670119195], 
processed observation next is [1.0, 0.8260869565217391, 0.3314866112650046, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.60369371064628, 0.6181215609301799, 1.0, 1.0, 0.15435365095805692], 
reward next is 0.8456, 
noisyNet noise sample is [array([0.5341265], dtype=float32), -2.2097478]. 
=============================================
[2019-04-04 16:38:46,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.3612709e-09 3.1468850e-09 7.0413632e-14 4.2985090e-13 1.0000000e+00
 2.3530808e-09 4.0945197e-14], sum to 1.0000
[2019-04-04 16:38:46,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8115
[2019-04-04 16:38:46,620] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.1356741353611, 0.2756762548485061, 0.0, 1.0, 43156.80929670222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2407200.0000, 
sim time next is 2407800.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.1096981319699, 0.2730478633123376, 0.0, 1.0, 43155.10214263996], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.592474844330825, 0.5910159544374459, 0.0, 1.0, 0.2055004863935236], 
reward next is 0.7945, 
noisyNet noise sample is [array([-0.17477545], dtype=float32), 0.7798791]. 
=============================================
[2019-04-04 16:39:03,712] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2315060e-09 2.1791006e-10 2.4366695e-16 6.4844789e-15 1.0000000e+00
 1.6478766e-10 1.2781617e-14], sum to 1.0000
[2019-04-04 16:39:03,713] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0581
[2019-04-04 16:39:03,767] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 64.0, 115.3333333333333, 211.3333333333333, 26.0, 25.89550556735848, 0.3998757740785086, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2796000.0000, 
sim time next is 2796600.0000, 
raw observation next is [-6.0, 64.0, 122.6666666666667, 215.6666666666667, 26.0, 25.97070354683662, 0.3958676085653379, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.408888888888889, 0.23830570902394113, 0.6666666666666666, 0.6642252955697184, 0.6319558695217793, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2482538], dtype=float32), 0.051671527]. 
=============================================
[2019-04-04 16:39:08,567] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1511342e-10 2.6389078e-12 1.7911289e-18 9.6440134e-17 1.0000000e+00
 2.5333540e-12 7.4297743e-18], sum to 1.0000
[2019-04-04 16:39:08,568] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8785
[2019-04-04 16:39:08,574] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 100.0, 0.0, 0.0, 26.0, 26.718479101126, 0.8644826297224816, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3174600.0000, 
sim time next is 3175200.0000, 
raw observation next is [6.0, 100.0, 0.0, 0.0, 26.0, 26.92410994366321, 0.8679789419045033, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 1.0, 0.0, 0.0, 0.6666666666666666, 0.7436758286386009, 0.7893263139681678, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1174759], dtype=float32), -1.1750866]. 
=============================================
[2019-04-04 16:39:10,131] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-04 16:39:10,132] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 16:39:10,132] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 16:39:10,132] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:39:10,133] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 16:39:10,133] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:39:10,133] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:39:10,140] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run48
[2019-04-04 16:39:10,168] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run48
[2019-04-04 16:39:10,186] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run48
[2019-04-04 16:40:21,796] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18536985], dtype=float32), 0.24578245]
[2019-04-04 16:40:21,796] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-7.199999999999999, 78.33333333333334, 53.66666666666667, 2.666666666666666, 26.0, 25.39764990736178, 0.2959647085806856, 1.0, 1.0, 0.0]
[2019-04-04 16:40:21,797] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 16:40:21,799] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.5965014e-09 1.9192936e-10 5.6407561e-16 7.4762126e-15 1.0000000e+00
 4.3170415e-11 1.0670395e-15], sampled 0.03703917695304737
[2019-04-04 16:40:51,674] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 16:41:10,009] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.4981 263445398.1310 1557.1271
[2019-04-04 16:41:14,000] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 16:41:15,025] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 4700000, evaluation results [4700000.0, 7241.498104138273, 263445398.13096312, 1557.1271045198032, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 16:41:16,183] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.3071919e-09 1.0779124e-10 1.5575213e-16 1.3982064e-14 1.0000000e+00
 3.6962183e-11 2.1857512e-15], sum to 1.0000
[2019-04-04 16:41:16,183] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2780
[2019-04-04 16:41:16,198] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.9087736204782, 0.25053962543479, 0.0, 1.0, 55732.82498001164], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2866800.0000, 
sim time next is 2867400.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.86627579735421, 0.2409202968726812, 0.0, 1.0, 55762.96409974764], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5721896497795175, 0.580306765624227, 0.0, 1.0, 0.26553792428451256], 
reward next is 0.7345, 
noisyNet noise sample is [array([-0.8682538], dtype=float32), 0.23238806]. 
=============================================
[2019-04-04 16:41:18,317] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3155874e-10 3.5632393e-11 5.0812623e-17 2.1074840e-15 1.0000000e+00
 2.0129520e-11 7.3561430e-17], sum to 1.0000
[2019-04-04 16:41:18,318] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9601
[2019-04-04 16:41:18,355] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 39.5, 178.0, 685.0, 26.0, 24.47357525944434, 0.2947716770021302, 1.0, 1.0, 196217.9094192963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2809800.0000, 
sim time next is 2810400.0000, 
raw observation next is [3.333333333333333, 38.0, 189.8333333333333, 599.6666666666667, 26.0, 24.76463514246722, 0.3882678771836235, 1.0, 1.0, 181237.4230646731], 
processed observation next is [1.0, 0.5217391304347826, 0.5549399815327793, 0.38, 0.6327777777777777, 0.6626151012891345, 0.6666666666666666, 0.5637195952056017, 0.6294226257278744, 1.0, 1.0, 0.8630353479270148], 
reward next is 0.1370, 
noisyNet noise sample is [array([-0.21455799], dtype=float32), 0.14449212]. 
=============================================
[2019-04-04 16:41:36,508] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4291062e-09 5.1284843e-10 6.6358488e-15 1.9272573e-14 1.0000000e+00
 5.7108117e-11 3.9508395e-15], sum to 1.0000
[2019-04-04 16:41:36,511] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5842
[2019-04-04 16:41:36,522] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.333333333333332, 26.66666666666666, 0.0, 0.0, 26.0, 25.48610577752343, 0.3606439124807085, 0.0, 1.0, 37736.32092651453], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3639000.0000, 
sim time next is 3639600.0000, 
raw observation next is [8.2, 27.0, 0.0, 0.0, 26.0, 25.47932475650829, 0.3600904498552017, 0.0, 1.0, 37436.16457791593], 
processed observation next is [0.0, 0.13043478260869565, 0.6897506925207757, 0.27, 0.0, 0.0, 0.6666666666666666, 0.6232770630423575, 0.6200301499517339, 0.0, 1.0, 0.17826745037102823], 
reward next is 0.8217, 
noisyNet noise sample is [array([-1.3352522], dtype=float32), 1.6477987]. 
=============================================
[2019-04-04 16:41:37,982] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6224263e-08 3.4516643e-09 1.9077237e-14 7.9246624e-13 1.0000000e+00
 1.6882304e-09 9.4222005e-15], sum to 1.0000
[2019-04-04 16:41:37,983] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8497
[2019-04-04 16:41:38,006] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.96824970200049, 0.3323573878781877, 0.0, 1.0, 43832.84653377983], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3292800.0000, 
sim time next is 3293400.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.89582260510088, 0.318744611139806, 0.0, 1.0, 43869.78680151067], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5746518837584066, 0.6062482037132687, 0.0, 1.0, 0.20890374667386036], 
reward next is 0.7911, 
noisyNet noise sample is [array([-0.0339526], dtype=float32), 0.8953332]. 
=============================================
[2019-04-04 16:41:38,316] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.8809214e-09 6.6163014e-10 3.6731679e-14 8.5775598e-14 1.0000000e+00
 8.9404573e-10 5.5816947e-14], sum to 1.0000
[2019-04-04 16:41:38,316] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7888
[2019-04-04 16:41:38,353] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.333333333333334, 74.33333333333334, 0.0, 0.0, 26.0, 25.50820604336247, 0.5477557965852217, 0.0, 1.0, 166371.0832780506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3270000.0000, 
sim time next is 3270600.0000, 
raw observation next is [-4.5, 76.0, 0.0, 0.0, 26.0, 25.44302111312995, 0.5609343780680771, 0.0, 1.0, 134403.981265021], 
processed observation next is [1.0, 0.8695652173913043, 0.3379501385041552, 0.76, 0.0, 0.0, 0.6666666666666666, 0.620251759427496, 0.6869781260226923, 0.0, 1.0, 0.6400189584048619], 
reward next is 0.3600, 
noisyNet noise sample is [array([-1.0946172], dtype=float32), 0.3777656]. 
=============================================
[2019-04-04 16:41:43,328] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1654668e-08 2.5718685e-09 6.5413265e-14 2.0163737e-13 1.0000000e+00
 1.7819979e-09 2.2915445e-14], sum to 1.0000
[2019-04-04 16:41:43,328] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2629
[2019-04-04 16:41:43,340] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.166666666666667, 72.0, 0.0, 0.0, 26.0, 24.75325045217686, 0.2400841941741658, 0.0, 1.0, 42152.43364872318], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3387000.0000, 
sim time next is 3387600.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.74658514334127, 0.2482205678714617, 0.0, 1.0, 42285.4112472412], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5622154286117725, 0.5827401892904872, 0.0, 1.0, 0.20135910117733904], 
reward next is 0.7986, 
noisyNet noise sample is [array([-0.4669183], dtype=float32), -0.44018704]. 
=============================================
[2019-04-04 16:41:46,746] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.30904243e-09 3.33409522e-10 1.65959991e-15 1.66705539e-15
 1.00000000e+00 1.02053525e-10 3.37360030e-15], sum to 1.0000
[2019-04-04 16:41:46,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7548
[2019-04-04 16:41:46,778] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.32017368937917, 0.4705100610407931, 0.0, 1.0, 57577.31104786236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3451800.0000, 
sim time next is 3452400.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.35414721068781, 0.4752441256031778, 0.0, 1.0, 47020.68219367509], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6128456008906508, 0.6584147085343927, 0.0, 1.0, 0.22390801044607186], 
reward next is 0.7761, 
noisyNet noise sample is [array([-1.5332403], dtype=float32), -3.0426793]. 
=============================================
[2019-04-04 16:41:48,992] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.6145852e-09 8.6413030e-11 3.1258703e-16 5.7735345e-15 1.0000000e+00
 1.7864142e-10 9.2191468e-16], sum to 1.0000
[2019-04-04 16:41:48,993] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6913
[2019-04-04 16:41:49,001] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.5, 59.0, 115.0, 810.0, 26.0, 26.31439222117447, 0.6162053536616964, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3497400.0000, 
sim time next is 3498000.0000, 
raw observation next is [1.666666666666667, 58.33333333333334, 115.1666666666667, 812.1666666666666, 26.0, 26.31595385589979, 0.6223694682622131, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5087719298245615, 0.5833333333333335, 0.383888888888889, 0.8974217311233885, 0.6666666666666666, 0.6929961546583158, 0.7074564894207377, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39336923], dtype=float32), -1.3196009]. 
=============================================
[2019-04-04 16:41:49,013] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[88.50107]
 [88.52779]
 [88.60831]
 [88.7354 ]
 [88.99701]], R is [[88.61975098]
 [88.73355103]
 [88.84621429]
 [88.95775604]
 [89.06817627]].
[2019-04-04 16:41:49,395] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7641734e-07 1.4755037e-08 4.7825172e-13 9.0809088e-12 9.9999976e-01
 2.1616948e-08 2.7730515e-12], sum to 1.0000
[2019-04-04 16:41:49,398] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7448
[2019-04-04 16:41:49,415] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.0, 63.0, 0.0, 0.0, 26.0, 23.50305425402401, -0.02381925271310393, 0.0, 1.0, 43640.75058165681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3996000.0000, 
sim time next is 3996600.0000, 
raw observation next is [-13.16666666666667, 64.0, 0.0, 0.0, 26.0, 23.48277154005672, -0.02710755317832251, 0.0, 1.0, 43537.02841509998], 
processed observation next is [1.0, 0.2608695652173913, 0.09787626962142189, 0.64, 0.0, 0.0, 0.6666666666666666, 0.4568976283380601, 0.4909641489405592, 0.0, 1.0, 0.20731918292904752], 
reward next is 0.7927, 
noisyNet noise sample is [array([-0.3918349], dtype=float32), 0.20851448]. 
=============================================
[2019-04-04 16:41:55,392] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.2033739e-09 2.5841813e-09 4.2522460e-14 2.0253623e-13 1.0000000e+00
 7.4538209e-10 2.1606501e-14], sum to 1.0000
[2019-04-04 16:41:55,396] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2646
[2019-04-04 16:41:55,420] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.11465787601567, 0.2796750641870042, 0.0, 1.0, 41759.95178050848], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3736800.0000, 
sim time next is 3737400.0000, 
raw observation next is [-3.166666666666667, 67.0, 0.0, 0.0, 26.0, 25.09939071135673, 0.2765520114051477, 0.0, 1.0, 41866.44929006769], 
processed observation next is [1.0, 0.2608695652173913, 0.3748845798707295, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5916158926130608, 0.5921840038017159, 0.0, 1.0, 0.19936404423841758], 
reward next is 0.8006, 
noisyNet noise sample is [array([1.0690292], dtype=float32), -0.6225403]. 
=============================================
[2019-04-04 16:41:57,855] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.4251577e-10 4.5519818e-10 2.4803734e-15 1.7554907e-14 1.0000000e+00
 6.2417155e-11 1.7519242e-15], sum to 1.0000
[2019-04-04 16:41:57,855] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8816
[2019-04-04 16:41:57,865] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 44.66666666666667, 112.0, 808.0, 26.0, 25.22186137151628, 0.4491822774438102, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3591600.0000, 
sim time next is 3592200.0000, 
raw observation next is [-1.166666666666667, 43.33333333333333, 110.0, 804.0, 26.0, 25.20694767144075, 0.4495232465059161, 0.0, 1.0, 18694.89423474776], 
processed observation next is [0.0, 0.5652173913043478, 0.43028624192059095, 0.4333333333333333, 0.36666666666666664, 0.8883977900552487, 0.6666666666666666, 0.6005789726200627, 0.6498410821686387, 0.0, 1.0, 0.08902330587975124], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.07704262], dtype=float32), 0.03405391]. 
=============================================
[2019-04-04 16:41:59,874] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9110020e-10 3.1524144e-11 1.6163256e-16 2.1320081e-16 1.0000000e+00
 3.4461858e-12 2.6264795e-16], sum to 1.0000
[2019-04-04 16:41:59,874] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8729
[2019-04-04 16:41:59,886] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.5, 29.0, 89.0, 403.0, 26.0, 25.57816791549734, 0.4251029120049966, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3659400.0000, 
sim time next is 3660000.0000, 
raw observation next is [10.0, 28.0, 91.0, 446.3333333333334, 26.0, 25.60440144088291, 0.4311184540463154, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.739612188365651, 0.28, 0.30333333333333334, 0.49318600368324134, 0.6666666666666666, 0.6337001200735758, 0.6437061513487717, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22062732], dtype=float32), 0.31589034]. 
=============================================
[2019-04-04 16:41:59,900] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[86.30057]
 [85.73103]
 [85.16722]
 [84.6125 ]
 [83.98551]], R is [[86.88691711]
 [87.01805115]
 [87.14787292]
 [87.27639771]
 [87.31433105]].
[2019-04-04 16:42:05,983] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0180129e-08 3.9983358e-10 2.2675563e-15 7.9561452e-14 1.0000000e+00
 2.0679808e-10 2.6064701e-14], sum to 1.0000
[2019-04-04 16:42:05,983] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8247
[2019-04-04 16:42:06,004] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.56835280338623, 0.3992654858853702, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4221600.0000, 
sim time next is 4222200.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.54041199890901, 0.3889289257051717, 0.0, 1.0, 20490.55642260643], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6283676665757509, 0.6296429752350572, 0.0, 1.0, 0.09757407820288777], 
reward next is 0.9024, 
noisyNet noise sample is [array([-1.7803234], dtype=float32), -0.10265723]. 
=============================================
[2019-04-04 16:42:08,300] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4820534e-08 3.5892587e-09 4.5891093e-14 5.4926777e-13 1.0000000e+00
 9.6768638e-10 2.2529268e-13], sum to 1.0000
[2019-04-04 16:42:08,302] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5430
[2019-04-04 16:42:08,323] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.5, 58.0, 0.0, 0.0, 26.0, 24.78239309942443, 0.2949495958883099, 0.0, 1.0, 44044.73145953747], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3976200.0000, 
sim time next is 3976800.0000, 
raw observation next is [-10.66666666666667, 58.0, 0.0, 0.0, 26.0, 24.74016222437765, 0.29269584752192, 0.0, 1.0, 44003.44536940601], 
processed observation next is [1.0, 0.0, 0.16712834718374878, 0.58, 0.0, 0.0, 0.6666666666666666, 0.561680185364804, 0.5975652825073067, 0.0, 1.0, 0.2095402160447905], 
reward next is 0.7905, 
noisyNet noise sample is [array([-0.7113648], dtype=float32), -1.7274349]. 
=============================================
[2019-04-04 16:42:11,485] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2526661e-09 1.3518039e-10 1.0862105e-15 3.3544429e-14 1.0000000e+00
 1.8983809e-10 4.6535575e-15], sum to 1.0000
[2019-04-04 16:42:11,489] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2383
[2019-04-04 16:42:11,511] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 58.33333333333334, 0.0, 0.0, 26.0, 25.26587104649294, 0.4508109024158993, 0.0, 1.0, 91780.9056321479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3883200.0000, 
sim time next is 3883800.0000, 
raw observation next is [-1.0, 59.16666666666666, 0.0, 0.0, 26.0, 25.25054179735576, 0.4553710316402038, 0.0, 1.0, 58578.37354714172], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.5916666666666666, 0.0, 0.0, 0.6666666666666666, 0.6042118164463132, 0.651790343880068, 0.0, 1.0, 0.27894463593877006], 
reward next is 0.7211, 
noisyNet noise sample is [array([-1.199004], dtype=float32), 0.43863988]. 
=============================================
[2019-04-04 16:42:11,860] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0345010e-10 2.0617492e-11 6.5825690e-17 5.4794016e-16 1.0000000e+00
 5.9243103e-11 9.1627994e-17], sum to 1.0000
[2019-04-04 16:42:11,860] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9591
[2019-04-04 16:42:11,894] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 105.5, 727.5, 26.0, 26.42990437739521, 0.5906963048012277, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3837600.0000, 
sim time next is 3838200.0000, 
raw observation next is [-1.833333333333333, 60.0, 107.0, 743.3333333333334, 26.0, 26.4690924820679, 0.6026229937558375, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.41181902123730385, 0.6, 0.3566666666666667, 0.8213627992633518, 0.6666666666666666, 0.7057577068389916, 0.7008743312519458, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12744547], dtype=float32), 0.714648]. 
=============================================
[2019-04-04 16:42:16,560] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4226041e-08 1.2381596e-09 2.0372490e-13 2.2417905e-13 1.0000000e+00
 1.7112678e-09 3.3083462e-13], sum to 1.0000
[2019-04-04 16:42:16,561] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1965
[2019-04-04 16:42:16,578] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.333333333333334, 54.66666666666667, 0.0, 0.0, 26.0, 25.04758665064276, 0.3623928550474909, 0.0, 1.0, 44096.88068650051], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3972000.0000, 
sim time next is 3972600.0000, 
raw observation next is [-9.5, 55.5, 0.0, 0.0, 26.0, 24.99905421771798, 0.3508781878704945, 0.0, 1.0, 44084.77803309889], 
processed observation next is [1.0, 1.0, 0.1994459833795014, 0.555, 0.0, 0.0, 0.6666666666666666, 0.583254518143165, 0.6169593959568315, 0.0, 1.0, 0.20992751444332805], 
reward next is 0.7901, 
noisyNet noise sample is [array([-0.18532386], dtype=float32), 0.58971894]. 
=============================================
[2019-04-04 16:42:20,439] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.01004913e-10 9.39682998e-10 1.96406146e-15 1.21199175e-14
 1.00000000e+00 1.33995953e-10 9.68279784e-16], sum to 1.0000
[2019-04-04 16:42:20,441] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3770
[2019-04-04 16:42:20,454] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 56.16666666666666, 0.0, 0.0, 26.0, 25.49747224396426, 0.5453997427993461, 0.0, 1.0, 22967.33403757819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4662600.0000, 
sim time next is 4663200.0000, 
raw observation next is [2.0, 55.33333333333334, 0.0, 0.0, 26.0, 25.57414706072524, 0.5490675403996356, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.5533333333333335, 0.0, 0.0, 0.6666666666666666, 0.6311789217271034, 0.6830225134665452, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.383084], dtype=float32), -0.153976]. 
=============================================
[2019-04-04 16:42:26,093] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4007817e-10 3.9503050e-11 2.9559491e-16 1.0297111e-14 1.0000000e+00
 1.8244646e-10 9.5295166e-16], sum to 1.0000
[2019-04-04 16:42:26,098] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6964
[2019-04-04 16:42:26,156] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 49.5, 92.0, 488.0, 26.0, 24.9621856491577, 0.3966993242809136, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4177800.0000, 
sim time next is 4178400.0000, 
raw observation next is [-4.333333333333334, 48.0, 94.66666666666667, 516.6666666666666, 26.0, 25.52959482961722, 0.4358909059357602, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.3425669436749769, 0.48, 0.3155555555555556, 0.570902394106814, 0.6666666666666666, 0.627466235801435, 0.6452969686452534, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1084391], dtype=float32), 1.5419283]. 
=============================================
[2019-04-04 16:42:28,366] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.46503487e-09 4.96011843e-10 1.57592979e-15 6.19700824e-14
 1.00000000e+00 1.04041144e-10 1.24694367e-15], sum to 1.0000
[2019-04-04 16:42:28,367] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2088
[2019-04-04 16:42:28,380] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.57668580463875, 0.4094884708870941, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4221000.0000, 
sim time next is 4221600.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.56863832361972, 0.3992794108809102, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6307198603016433, 0.6330931369603033, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4114385], dtype=float32), -0.38776094]. 
=============================================
[2019-04-04 16:42:29,250] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1629647e-09 5.3240595e-10 3.7534867e-15 2.3917768e-14 1.0000000e+00
 4.8620916e-11 1.6507642e-14], sum to 1.0000
[2019-04-04 16:42:29,250] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5640
[2019-04-04 16:42:29,262] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 37.0, 114.0, 544.0, 26.0, 25.36370608791399, 0.4415293921270841, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4204800.0000, 
sim time next is 4205400.0000, 
raw observation next is [2.833333333333333, 37.5, 98.66666666666666, 546.0, 26.0, 25.39880552109168, 0.4492120064236864, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.541089566020314, 0.375, 0.32888888888888884, 0.6033149171270719, 0.6666666666666666, 0.6165671267576401, 0.6497373354745621, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10257903], dtype=float32), 0.40949124]. 
=============================================
[2019-04-04 16:42:42,755] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1264923e-09 8.5369808e-11 9.1773913e-16 1.7843537e-14 1.0000000e+00
 8.9465813e-10 1.6966441e-15], sum to 1.0000
[2019-04-04 16:42:42,756] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9217
[2019-04-04 16:42:42,769] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.57186889025147, 0.5705043419262082, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4483200.0000, 
sim time next is 4483800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.60474089722592, 0.5650338282331148, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.63372840810216, 0.6883446094110383, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8203408], dtype=float32), -0.5950291]. 
=============================================
[2019-04-04 16:42:43,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:42:43,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:42:43,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run36
[2019-04-04 16:42:45,267] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:42:45,267] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:42:45,286] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run36
[2019-04-04 16:42:47,274] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3003792e-09 4.1041531e-10 4.5599646e-15 2.9565217e-14 1.0000000e+00
 6.2448624e-10 1.1969900e-14], sum to 1.0000
[2019-04-04 16:42:47,277] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6615
[2019-04-04 16:42:47,354] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.833333333333333, 76.0, 0.0, 0.0, 26.0, 24.98957644119882, 0.3217974053656201, 0.0, 1.0, 36279.50673121424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4605000.0000, 
sim time next is 4605600.0000, 
raw observation next is [-2.666666666666667, 75.0, 0.0, 0.0, 26.0, 24.96422695534218, 0.3422100520189668, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.38873499538319484, 0.75, 0.0, 0.0, 0.6666666666666666, 0.580352246278515, 0.6140700173396556, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2155235], dtype=float32), 1.9983671]. 
=============================================
[2019-04-04 16:42:48,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:42:48,959] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:42:48,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run36
[2019-04-04 16:42:53,100] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1260806e-09 8.2724001e-11 1.4833822e-16 4.1919682e-15 1.0000000e+00
 1.2856626e-11 5.8241213e-16], sum to 1.0000
[2019-04-04 16:42:53,100] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0069
[2019-04-04 16:42:53,162] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.699999999999999, 93.0, 41.66666666666666, 0.0, 26.0, 23.28598681914837, -0.1226141324581852, 0.0, 1.0, 58381.46038731085], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 34800.0000, 
sim time next is 35400.0000, 
raw observation next is [7.7, 93.0, 45.33333333333333, 0.0, 26.0, 23.38419836032465, -0.1002904867048895, 0.0, 1.0, 58165.86569307836], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.15111111111111108, 0.0, 0.6666666666666666, 0.4486831966937208, 0.46656983776503685, 0.0, 1.0, 0.2769803128241827], 
reward next is 0.7230, 
noisyNet noise sample is [array([0.27454948], dtype=float32), 0.41718212]. 
=============================================
[2019-04-04 16:42:54,399] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:42:54,400] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:42:54,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run36
[2019-04-04 16:42:54,814] A3C_AGENT_WORKER-Thread-2 INFO:Local step 297500, global step 4751877: loss 0.3127
[2019-04-04 16:42:54,817] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 297500, global step 4751880: learning rate 0.0000
[2019-04-04 16:42:56,227] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.74263162e-09 1.12729638e-10 2.61018031e-15 1.08310366e-14
 1.00000000e+00 2.69347461e-10 3.24495379e-15], sum to 1.0000
[2019-04-04 16:42:56,228] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3177
[2019-04-04 16:42:56,268] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 84.0, 0.0, 0.0, 26.0, 25.09266322408297, 0.3978784527014491, 0.0, 1.0, 41540.77341939047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4752000.0000, 
sim time next is 4752600.0000, 
raw observation next is [-4.0, 81.83333333333333, 0.0, 0.0, 26.0, 25.06649548594388, 0.3964726447586895, 0.0, 1.0, 41479.64434455329], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.8183333333333332, 0.0, 0.0, 0.6666666666666666, 0.5888746238286565, 0.6321575482528965, 0.0, 1.0, 0.19752211592644425], 
reward next is 0.8025, 
noisyNet noise sample is [array([0.54541147], dtype=float32), -0.12166436]. 
=============================================
[2019-04-04 16:42:57,054] A3C_AGENT_WORKER-Thread-13 INFO:Local step 297500, global step 4752903: loss 0.3039
[2019-04-04 16:42:57,062] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 297500, global step 4752903: learning rate 0.0000
[2019-04-04 16:43:00,629] A3C_AGENT_WORKER-Thread-11 INFO:Local step 297500, global step 4754595: loss 0.3036
[2019-04-04 16:43:00,630] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 297500, global step 4754595: learning rate 0.0000
[2019-04-04 16:43:01,800] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1789437e-09 5.3274019e-10 5.8762335e-15 4.2597389e-14 1.0000000e+00
 3.1039807e-10 4.1066739e-14], sum to 1.0000
[2019-04-04 16:43:01,805] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6019
[2019-04-04 16:43:01,826] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.8, 44.33333333333334, 266.0, 385.6666666666667, 26.0, 25.08177547609669, 0.3706394664126781, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4887600.0000, 
sim time next is 4888200.0000, 
raw observation next is [1.9, 44.16666666666667, 260.0, 383.3333333333333, 26.0, 25.10472383474497, 0.3704792037217491, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.515235457063712, 0.4416666666666667, 0.8666666666666667, 0.42357274401473294, 0.6666666666666666, 0.5920603195620808, 0.6234930679072497, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25039712], dtype=float32), -0.108423404]. 
=============================================
[2019-04-04 16:43:03,518] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:43:03,518] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:43:03,568] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run36
[2019-04-04 16:43:04,805] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5754739e-09 1.5999247e-09 3.8053887e-15 9.9214324e-14 1.0000000e+00
 7.9444396e-10 4.1120976e-14], sum to 1.0000
[2019-04-04 16:43:04,807] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4247
[2019-04-04 16:43:04,819] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.41465357185336, 0.2967461376526185, 0.0, 1.0, 65475.10942325972], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4934400.0000, 
sim time next is 4935000.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.30702256213702, 0.298429718990426, 0.0, 1.0, 75984.14160278325], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6089185468447518, 0.5994765729968087, 0.0, 1.0, 0.3618292457275393], 
reward next is 0.6382, 
noisyNet noise sample is [array([1.8935813], dtype=float32), 2.8669856]. 
=============================================
[2019-04-04 16:43:04,825] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.200165]
 [77.90927 ]
 [78.26187 ]
 [78.464455]
 [78.56986 ]], R is [[78.11297607]
 [78.02006531]
 [78.23986816]
 [78.45747375]
 [78.67289734]].
[2019-04-04 16:43:05,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:43:05,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:43:05,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run36
[2019-04-04 16:43:06,255] A3C_AGENT_WORKER-Thread-17 INFO:Local step 297500, global step 4757406: loss 0.3105
[2019-04-04 16:43:06,256] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 297500, global step 4757408: learning rate 0.0000
[2019-04-04 16:43:07,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:43:07,446] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:43:07,453] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run36
[2019-04-04 16:43:07,774] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:43:07,776] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:43:07,790] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run36
[2019-04-04 16:43:07,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:43:07,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:43:07,840] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run36
[2019-04-04 16:43:09,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:43:09,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:43:09,155] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run36
[2019-04-04 16:43:09,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.61682197e-04 9.59496974e-05 1.17950428e-07 1.66739471e-06
 9.99687791e-01 5.26898875e-05 1.15280095e-07], sum to 1.0000
[2019-04-04 16:43:09,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2623
[2019-04-04 16:43:09,716] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 23.0, 20.02946818878658, -0.8375176159099885, 0.0, 1.0, 44966.57586995771], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 5400.0000, 
sim time next is 6000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 23.5, 20.11674216971234, -0.8201090299824442, 0.0, 1.0, 44508.21856916837], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.4583333333333333, 0.17639518080936156, 0.22663032333918529, 0.0, 1.0, 0.2119438979484208], 
reward next is 0.7881, 
noisyNet noise sample is [array([-0.23623557], dtype=float32), 1.4715593]. 
=============================================
[2019-04-04 16:43:09,730] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[38.130283]
 [34.701088]
 [31.006956]
 [27.037697]
 [24.129301]], R is [[42.13658905]
 [42.50109863]
 [42.85843658]
 [43.20396805]
 [43.52016449]].
[2019-04-04 16:43:10,130] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:43:10,130] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:43:10,135] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run36
[2019-04-04 16:43:11,581] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7465870e-10 1.8170250e-11 7.4288145e-17 2.0300195e-16 1.0000000e+00
 1.6597172e-11 8.6334349e-17], sum to 1.0000
[2019-04-04 16:43:11,584] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7430
[2019-04-04 16:43:11,594] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 26.0, 28.28859907957054, 1.050920043744063, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5078400.0000, 
sim time next is 5079000.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 26.0, 28.15221949070999, 1.03603497216244, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.0, 0.0, 0.6666666666666666, 0.846018290892499, 0.8453449907208134, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3835449], dtype=float32), -1.5573435]. 
=============================================
[2019-04-04 16:43:11,610] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[91.199196]
 [91.25622 ]
 [91.55998 ]
 [92.08352 ]
 [92.8216  ]], R is [[91.35652161]
 [91.44295502]
 [91.52852631]
 [91.6132431 ]
 [91.69711304]].
[2019-04-04 16:43:12,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:43:12,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:43:12,468] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run36
[2019-04-04 16:43:13,406] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:43:13,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:43:13,410] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run36
[2019-04-04 16:43:14,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:43:14,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:43:14,064] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run36
[2019-04-04 16:43:15,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:43:15,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:43:15,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run36
[2019-04-04 16:43:15,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:43:15,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:43:15,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run36
[2019-04-04 16:43:15,903] A3C_AGENT_WORKER-Thread-15 INFO:Local step 297500, global step 4760121: loss 0.3188
[2019-04-04 16:43:15,904] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 297500, global step 4760121: learning rate 0.0000
[2019-04-04 16:43:17,934] A3C_AGENT_WORKER-Thread-2 INFO:Local step 298000, global step 4760481: loss 0.0417
[2019-04-04 16:43:17,934] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 298000, global step 4760481: learning rate 0.0000
[2019-04-04 16:43:18,766] A3C_AGENT_WORKER-Thread-4 INFO:Local step 297500, global step 4760628: loss 0.3251
[2019-04-04 16:43:18,766] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 297500, global step 4760628: learning rate 0.0000
[2019-04-04 16:43:20,876] A3C_AGENT_WORKER-Thread-10 INFO:Local step 297500, global step 4761125: loss 0.3173
[2019-04-04 16:43:20,877] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 297500, global step 4761125: learning rate 0.0000
[2019-04-04 16:43:21,011] A3C_AGENT_WORKER-Thread-16 INFO:Local step 297500, global step 4761172: loss 0.3109
[2019-04-04 16:43:21,014] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 297500, global step 4761172: learning rate 0.0000
[2019-04-04 16:43:21,140] A3C_AGENT_WORKER-Thread-3 INFO:Local step 297500, global step 4761214: loss 0.3115
[2019-04-04 16:43:21,140] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 297500, global step 4761214: learning rate 0.0000
[2019-04-04 16:43:21,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.02126596e-09 2.45567566e-10 8.93308369e-16 9.82995881e-15
 1.00000000e+00 1.02805695e-10 2.64348686e-16], sum to 1.0000
[2019-04-04 16:43:21,405] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3660
[2019-04-04 16:43:21,438] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.516666666666667, 91.0, 0.0, 0.0, 26.0, 24.24828699002052, 0.1239873661405742, 0.0, 1.0, 41820.93564750976], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 93000.0000, 
sim time next is 93600.0000, 
raw observation next is [-1.7, 91.0, 0.0, 0.0, 26.0, 24.24097673665148, 0.1224959785669079, 0.0, 1.0, 41976.10967808698], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5200813947209566, 0.540831992855636, 0.0, 1.0, 0.19988623656231894], 
reward next is 0.8001, 
noisyNet noise sample is [array([-1.4442588], dtype=float32), 1.247822]. 
=============================================
[2019-04-04 16:43:21,520] A3C_AGENT_WORKER-Thread-13 INFO:Local step 298000, global step 4761348: loss 0.0393
[2019-04-04 16:43:21,521] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 298000, global step 4761348: learning rate 0.0000
[2019-04-04 16:43:21,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4668427e-10 1.2879287e-10 4.5702547e-16 1.3397728e-14 1.0000000e+00
 7.1455918e-11 4.0040473e-16], sum to 1.0000
[2019-04-04 16:43:21,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8485
[2019-04-04 16:43:21,620] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.616666666666667, 87.66666666666666, 0.0, 0.0, 26.0, 24.1077106364033, 0.09426996084459081, 0.0, 1.0, 42783.50436198704], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 96600.0000, 
sim time next is 97200.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.11857918722998, 0.08498302132933376, 0.0, 1.0, 42939.58580982206], 
processed observation next is [1.0, 0.13043478260869565, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5098815989358316, 0.5283276737764445, 0.0, 1.0, 0.20447421814200983], 
reward next is 0.7955, 
noisyNet noise sample is [array([2.0001512], dtype=float32), -1.2990763]. 
=============================================
[2019-04-04 16:43:22,201] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9086531e-09 2.4721897e-10 5.4251198e-15 2.7294012e-14 1.0000000e+00
 2.7656388e-10 3.0054108e-15], sum to 1.0000
[2019-04-04 16:43:22,201] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9187
[2019-04-04 16:43:22,280] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 17.5, 0.0, 26.0, 21.56089873928766, -0.4186038387525526, 0.0, 1.0, 132515.1166631253], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 30000.0000, 
sim time next is 30600.0000, 
raw observation next is [7.7, 93.0, 21.0, 0.0, 26.0, 21.9191293035185, -0.3492143463773412, 0.0, 1.0, 100090.5945139615], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.07, 0.0, 0.6666666666666666, 0.3265941086265416, 0.38359521787421963, 0.0, 1.0, 0.47662187863791194], 
reward next is 0.5234, 
noisyNet noise sample is [array([0.53384244], dtype=float32), -1.7413075]. 
=============================================
[2019-04-04 16:43:22,952] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5857037e-09 7.1616546e-10 5.3172440e-16 1.0077629e-14 1.0000000e+00
 2.8375061e-10 2.3015173e-15], sum to 1.0000
[2019-04-04 16:43:22,953] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7676
[2019-04-04 16:43:23,008] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 93.0, 49.0, 0.0, 26.0, 23.48396747548868, -0.07784156757973969, 0.0, 1.0, 57952.71270752177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 36000.0000, 
sim time next is 36600.0000, 
raw observation next is [7.7, 93.0, 52.66666666666667, 0.0, 26.0, 23.58465297994817, -0.05738324692424232, 0.0, 1.0, 57320.74950573059], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.17555555555555558, 0.0, 0.6666666666666666, 0.46538774832901425, 0.48087225102525255, 0.0, 1.0, 0.27295595002728856], 
reward next is 0.7270, 
noisyNet noise sample is [array([-0.71595716], dtype=float32), -0.8165258]. 
=============================================
[2019-04-04 16:43:23,049] A3C_AGENT_WORKER-Thread-6 INFO:Local step 297500, global step 4761819: loss 0.3174
[2019-04-04 16:43:23,049] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 297500, global step 4761819: learning rate 0.0000
[2019-04-04 16:43:23,917] A3C_AGENT_WORKER-Thread-20 INFO:Local step 297500, global step 4762142: loss 0.3179
[2019-04-04 16:43:23,919] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 297500, global step 4762142: learning rate 0.0000
[2019-04-04 16:43:25,348] A3C_AGENT_WORKER-Thread-11 INFO:Local step 298000, global step 4762557: loss 0.0351
[2019-04-04 16:43:25,350] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 298000, global step 4762557: learning rate 0.0000
[2019-04-04 16:43:25,714] A3C_AGENT_WORKER-Thread-12 INFO:Local step 297500, global step 4762655: loss 0.3028
[2019-04-04 16:43:25,717] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 297500, global step 4762655: learning rate 0.0000
[2019-04-04 16:43:26,423] A3C_AGENT_WORKER-Thread-14 INFO:Local step 297500, global step 4762873: loss 0.3047
[2019-04-04 16:43:26,424] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 297500, global step 4762873: learning rate 0.0000
[2019-04-04 16:43:27,235] A3C_AGENT_WORKER-Thread-19 INFO:Local step 297500, global step 4763145: loss 0.3000
[2019-04-04 16:43:27,236] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 297500, global step 4763145: learning rate 0.0000
[2019-04-04 16:43:28,728] A3C_AGENT_WORKER-Thread-5 INFO:Local step 297500, global step 4763663: loss 0.2985
[2019-04-04 16:43:28,738] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 297500, global step 4763663: learning rate 0.0000
[2019-04-04 16:43:29,082] A3C_AGENT_WORKER-Thread-18 INFO:Local step 297500, global step 4763804: loss 0.3090
[2019-04-04 16:43:29,082] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 297500, global step 4763804: learning rate 0.0000
[2019-04-04 16:43:30,991] A3C_AGENT_WORKER-Thread-17 INFO:Local step 298000, global step 4764457: loss 0.0369
[2019-04-04 16:43:30,992] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 298000, global step 4764457: learning rate 0.0000
[2019-04-04 16:43:40,760] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.34492062e-08 4.99823205e-09 1.03085536e-13 2.41759087e-13
 1.00000000e+00 3.21821325e-09 4.75297936e-14], sum to 1.0000
[2019-04-04 16:43:40,761] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8639
[2019-04-04 16:43:40,844] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.05, 45.5, 24.0, 240.0, 26.0, 25.76128099372234, 0.3823115372238777, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 318600.0000, 
sim time next is 319200.0000, 
raw observation next is [-10.23333333333333, 46.66666666666666, 20.0, 201.0, 26.0, 25.42981181439963, 0.3669205537689624, 1.0, 1.0, 197724.8579282573], 
processed observation next is [1.0, 0.6956521739130435, 0.1791320406278856, 0.46666666666666656, 0.06666666666666667, 0.22209944751381216, 0.6666666666666666, 0.6191509845333026, 0.6223068512563208, 1.0, 1.0, 0.9415469425155109], 
reward next is 0.0585, 
noisyNet noise sample is [array([0.09592012], dtype=float32), 0.60873896]. 
=============================================
[2019-04-04 16:43:41,018] A3C_AGENT_WORKER-Thread-15 INFO:Local step 298000, global step 4767639: loss 0.0368
[2019-04-04 16:43:41,023] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 298000, global step 4767642: learning rate 0.0000
[2019-04-04 16:43:41,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5812220e-08 3.0821656e-09 9.6278908e-14 1.7686090e-13 1.0000000e+00
 8.9467689e-10 2.9411390e-14], sum to 1.0000
[2019-04-04 16:43:41,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2227
[2019-04-04 16:43:41,694] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.8, 58.0, 0.0, 0.0, 26.0, 25.69763889700857, 0.4094052510977698, 1.0, 1.0, 85440.40477456921], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 324600.0000, 
sim time next is 325200.0000, 
raw observation next is [-11.9, 59.00000000000001, 0.0, 0.0, 26.0, 25.70373045920429, 0.3760708104200527, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.13296398891966757, 0.5900000000000001, 0.0, 0.0, 0.6666666666666666, 0.6419775382670242, 0.6253569368066842, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2788465], dtype=float32), 0.26358703]. 
=============================================
[2019-04-04 16:43:41,861] A3C_AGENT_WORKER-Thread-2 INFO:Local step 298500, global step 4767901: loss 0.0307
[2019-04-04 16:43:41,861] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 298500, global step 4767901: learning rate 0.0000
[2019-04-04 16:43:43,983] A3C_AGENT_WORKER-Thread-4 INFO:Local step 298000, global step 4768689: loss 0.0423
[2019-04-04 16:43:43,984] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 298000, global step 4768689: learning rate 0.0000
[2019-04-04 16:43:45,483] A3C_AGENT_WORKER-Thread-10 INFO:Local step 298000, global step 4769149: loss 0.0342
[2019-04-04 16:43:45,484] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 298000, global step 4769149: learning rate 0.0000
[2019-04-04 16:43:45,516] A3C_AGENT_WORKER-Thread-13 INFO:Local step 298500, global step 4769160: loss 0.0274
[2019-04-04 16:43:45,532] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 298500, global step 4769160: learning rate 0.0000
[2019-04-04 16:43:45,580] A3C_AGENT_WORKER-Thread-16 INFO:Local step 298000, global step 4769178: loss 0.0383
[2019-04-04 16:43:45,581] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 298000, global step 4769178: learning rate 0.0000
[2019-04-04 16:43:46,126] A3C_AGENT_WORKER-Thread-3 INFO:Local step 298000, global step 4769354: loss 0.0362
[2019-04-04 16:43:46,127] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 298000, global step 4769355: learning rate 0.0000
[2019-04-04 16:43:48,297] A3C_AGENT_WORKER-Thread-6 INFO:Local step 298000, global step 4770023: loss 0.0326
[2019-04-04 16:43:48,298] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 298000, global step 4770024: learning rate 0.0000
[2019-04-04 16:43:48,703] A3C_AGENT_WORKER-Thread-20 INFO:Local step 298000, global step 4770180: loss 0.0327
[2019-04-04 16:43:48,704] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 298000, global step 4770180: learning rate 0.0000
[2019-04-04 16:43:49,109] A3C_AGENT_WORKER-Thread-11 INFO:Local step 298500, global step 4770340: loss 0.0267
[2019-04-04 16:43:49,110] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 298500, global step 4770340: learning rate 0.0000
[2019-04-04 16:43:49,826] A3C_AGENT_WORKER-Thread-12 INFO:Local step 298000, global step 4770588: loss 0.0309
[2019-04-04 16:43:49,826] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 298000, global step 4770588: learning rate 0.0000
[2019-04-04 16:43:50,959] A3C_AGENT_WORKER-Thread-14 INFO:Local step 298000, global step 4770956: loss 0.0308
[2019-04-04 16:43:50,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 298000, global step 4770956: learning rate 0.0000
[2019-04-04 16:43:51,243] A3C_AGENT_WORKER-Thread-19 INFO:Local step 298000, global step 4771056: loss 0.0295
[2019-04-04 16:43:51,244] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 298000, global step 4771056: learning rate 0.0000
[2019-04-04 16:43:52,448] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.9317710e-10 1.7272687e-11 1.0169491e-16 5.2961531e-15 1.0000000e+00
 2.5919088e-11 1.6213099e-15], sum to 1.0000
[2019-04-04 16:43:52,449] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2661
[2019-04-04 16:43:52,462] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.883333333333334, 88.83333333333334, 0.0, 0.0, 26.0, 24.97097467198192, 0.259025063272781, 0.0, 1.0, 39658.65650971423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 522600.0000, 
sim time next is 523200.0000, 
raw observation next is [4.766666666666667, 88.66666666666667, 0.0, 0.0, 26.0, 25.01288307094501, 0.261500652234903, 0.0, 1.0, 39463.38394054865], 
processed observation next is [0.0, 0.043478260869565216, 0.5946445060018468, 0.8866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5844069225787507, 0.587166884078301, 0.0, 1.0, 0.18792087590737452], 
reward next is 0.8121, 
noisyNet noise sample is [array([1.2967613], dtype=float32), -2.0111768]. 
=============================================
[2019-04-04 16:43:53,539] A3C_AGENT_WORKER-Thread-5 INFO:Local step 298000, global step 4771818: loss 0.0263
[2019-04-04 16:43:53,539] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 298000, global step 4771818: learning rate 0.0000
[2019-04-04 16:43:53,908] A3C_AGENT_WORKER-Thread-18 INFO:Local step 298000, global step 4771931: loss 0.0272
[2019-04-04 16:43:53,908] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 298000, global step 4771931: learning rate 0.0000
[2019-04-04 16:43:54,274] A3C_AGENT_WORKER-Thread-17 INFO:Local step 298500, global step 4772050: loss 0.0317
[2019-04-04 16:43:54,274] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 298500, global step 4772050: learning rate 0.0000
[2019-04-04 16:44:01,511] A3C_AGENT_WORKER-Thread-2 INFO:Local step 299000, global step 4774690: loss 0.0049
[2019-04-04 16:44:01,512] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 299000, global step 4774690: learning rate 0.0000
[2019-04-04 16:44:02,670] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9300801e-09 2.1398039e-10 1.6332712e-16 3.0399098e-15 1.0000000e+00
 6.3636661e-12 2.2370545e-16], sum to 1.0000
[2019-04-04 16:44:02,673] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9107
[2019-04-04 16:44:02,694] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.133333333333333, 87.33333333333334, 0.0, 0.0, 26.0, 24.87004101045364, 0.2364615018867271, 0.0, 1.0, 39681.22722427696], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 526800.0000, 
sim time next is 527400.0000, 
raw observation next is [4.05, 87.0, 0.0, 0.0, 26.0, 24.84643455622105, 0.2332904375082095, 0.0, 1.0, 39709.94832219808], 
processed observation next is [0.0, 0.08695652173913043, 0.5747922437673131, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5705362130184207, 0.5777634791694032, 0.0, 1.0, 0.18909499201046703], 
reward next is 0.8109, 
noisyNet noise sample is [array([-0.884907], dtype=float32), 1.5022241]. 
=============================================
[2019-04-04 16:44:04,340] A3C_AGENT_WORKER-Thread-15 INFO:Local step 298500, global step 4775890: loss 0.0330
[2019-04-04 16:44:04,341] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 298500, global step 4775890: learning rate 0.0000
[2019-04-04 16:44:04,764] A3C_AGENT_WORKER-Thread-13 INFO:Local step 299000, global step 4776057: loss 0.0060
[2019-04-04 16:44:04,766] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 299000, global step 4776057: learning rate 0.0000
[2019-04-04 16:44:06,886] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8414842e-08 4.4531809e-10 5.3031103e-14 2.2566856e-13 1.0000000e+00
 4.7338539e-10 2.6927653e-14], sum to 1.0000
[2019-04-04 16:44:06,886] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5444
[2019-04-04 16:44:06,928] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 54.0, 83.0, 38.0, 26.0, 24.8930522903707, 0.2220592633000703, 0.0, 1.0, 41610.37552953602], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 660600.0000, 
sim time next is 661200.0000, 
raw observation next is [-0.6, 54.0, 73.66666666666667, 34.16666666666666, 26.0, 24.88215429611764, 0.2206695205238098, 0.0, 1.0, 46390.18375461274], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.24555555555555558, 0.03775322283609575, 0.6666666666666666, 0.5735128580098033, 0.5735565068412699, 0.0, 1.0, 0.22090563692672735], 
reward next is 0.7791, 
noisyNet noise sample is [array([0.5078092], dtype=float32), -0.539128]. 
=============================================
[2019-04-04 16:44:07,525] A3C_AGENT_WORKER-Thread-4 INFO:Local step 298500, global step 4777251: loss 0.0313
[2019-04-04 16:44:07,527] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 298500, global step 4777252: learning rate 0.0000
[2019-04-04 16:44:08,197] A3C_AGENT_WORKER-Thread-11 INFO:Local step 299000, global step 4777585: loss 0.0061
[2019-04-04 16:44:08,200] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 299000, global step 4777585: learning rate 0.0000
[2019-04-04 16:44:08,501] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.1498080e-10 1.9258640e-11 8.8486620e-17 3.0317685e-15 1.0000000e+00
 9.4999729e-12 2.1368853e-16], sum to 1.0000
[2019-04-04 16:44:08,505] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1667
[2019-04-04 16:44:08,511] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [19.11666666666667, 49.16666666666667, 54.0, 0.0, 26.0, 27.72136614412108, 0.9971248320740161, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1095000.0000, 
sim time next is 1095600.0000, 
raw observation next is [18.83333333333333, 49.33333333333334, 44.5, 0.0, 26.0, 27.80882912825913, 1.00730801300034, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.9843028624192061, 0.4933333333333334, 0.14833333333333334, 0.0, 0.6666666666666666, 0.8174024273549275, 0.83576933766678, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5183318], dtype=float32), -0.10418048]. 
=============================================
[2019-04-04 16:44:08,657] A3C_AGENT_WORKER-Thread-10 INFO:Local step 298500, global step 4777832: loss 0.0324
[2019-04-04 16:44:08,657] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 298500, global step 4777832: learning rate 0.0000
[2019-04-04 16:44:08,690] A3C_AGENT_WORKER-Thread-16 INFO:Local step 298500, global step 4777846: loss 0.0308
[2019-04-04 16:44:08,692] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 298500, global step 4777847: learning rate 0.0000
[2019-04-04 16:44:09,129] A3C_AGENT_WORKER-Thread-3 INFO:Local step 298500, global step 4778086: loss 0.0321
[2019-04-04 16:44:09,130] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 298500, global step 4778086: learning rate 0.0000
[2019-04-04 16:44:11,034] A3C_AGENT_WORKER-Thread-20 INFO:Local step 298500, global step 4778971: loss 0.0306
[2019-04-04 16:44:11,037] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 298500, global step 4778973: learning rate 0.0000
[2019-04-04 16:44:11,311] A3C_AGENT_WORKER-Thread-6 INFO:Local step 298500, global step 4779103: loss 0.0294
[2019-04-04 16:44:11,312] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 298500, global step 4779104: learning rate 0.0000
[2019-04-04 16:44:11,422] A3C_AGENT_WORKER-Thread-2 INFO:Local step 299500, global step 4779170: loss 1.3810
[2019-04-04 16:44:11,423] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 299500, global step 4779170: learning rate 0.0000
[2019-04-04 16:44:12,580] A3C_AGENT_WORKER-Thread-12 INFO:Local step 298500, global step 4779786: loss 0.0315
[2019-04-04 16:44:12,583] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 298500, global step 4779786: learning rate 0.0000
[2019-04-04 16:44:13,242] A3C_AGENT_WORKER-Thread-17 INFO:Local step 299000, global step 4780029: loss 0.0056
[2019-04-04 16:44:13,242] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 299000, global step 4780029: learning rate 0.0000
[2019-04-04 16:44:13,537] A3C_AGENT_WORKER-Thread-14 INFO:Local step 298500, global step 4780158: loss 0.0308
[2019-04-04 16:44:13,538] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 298500, global step 4780158: learning rate 0.0000
[2019-04-04 16:44:13,771] A3C_AGENT_WORKER-Thread-19 INFO:Local step 298500, global step 4780282: loss 0.0322
[2019-04-04 16:44:13,771] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 298500, global step 4780282: learning rate 0.0000
[2019-04-04 16:44:14,489] A3C_AGENT_WORKER-Thread-13 INFO:Local step 299500, global step 4780680: loss 1.3821
[2019-04-04 16:44:14,490] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 299500, global step 4780681: learning rate 0.0000
[2019-04-04 16:44:16,097] A3C_AGENT_WORKER-Thread-18 INFO:Local step 298500, global step 4781431: loss 0.0326
[2019-04-04 16:44:16,099] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 298500, global step 4781431: learning rate 0.0000
[2019-04-04 16:44:16,137] A3C_AGENT_WORKER-Thread-5 INFO:Local step 298500, global step 4781448: loss 0.0349
[2019-04-04 16:44:16,141] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 298500, global step 4781450: learning rate 0.0000
[2019-04-04 16:44:18,045] A3C_AGENT_WORKER-Thread-11 INFO:Local step 299500, global step 4782327: loss 1.4063
[2019-04-04 16:44:18,063] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 299500, global step 4782331: learning rate 0.0000
[2019-04-04 16:44:21,902] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4034867e-08 6.8803374e-10 3.6435475e-14 5.4267192e-13 1.0000000e+00
 4.0817887e-09 1.4584549e-13], sum to 1.0000
[2019-04-04 16:44:21,903] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8648
[2019-04-04 16:44:21,921] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.36194400541292, 0.1583034681269115, 0.0, 1.0, 41881.33942706395], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 777600.0000, 
sim time next is 778200.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.3251091605698, 0.1499225659407764, 0.0, 1.0, 41810.71204551426], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5270924300474832, 0.5499741886469255, 0.0, 1.0, 0.19909862878816312], 
reward next is 0.8009, 
noisyNet noise sample is [array([0.5516268], dtype=float32), -0.4720959]. 
=============================================
[2019-04-04 16:44:23,050] A3C_AGENT_WORKER-Thread-15 INFO:Local step 299000, global step 4784521: loss 0.0030
[2019-04-04 16:44:23,053] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 299000, global step 4784521: learning rate 0.0000
[2019-04-04 16:44:23,607] A3C_AGENT_WORKER-Thread-17 INFO:Local step 299500, global step 4784774: loss 1.4315
[2019-04-04 16:44:23,610] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 299500, global step 4784774: learning rate 0.0000
[2019-04-04 16:44:23,817] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.6550897e-10 1.4702597e-11 3.5497041e-16 1.3201385e-15 1.0000000e+00
 3.4709912e-11 2.7080236e-16], sum to 1.0000
[2019-04-04 16:44:23,817] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8750
[2019-04-04 16:44:23,862] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.16534337330609, 0.2882112853256677, 1.0, 1.0, 55203.53091954767], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 841800.0000, 
sim time next is 842400.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.13553171864319, 0.2844315993486783, 1.0, 1.0, 60477.78975647275], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5946276432202658, 0.5948105331162261, 1.0, 1.0, 0.2879894750308226], 
reward next is 0.7120, 
noisyNet noise sample is [array([-1.420602], dtype=float32), 0.37607712]. 
=============================================
[2019-04-04 16:44:24,305] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7519639e-08 2.0186828e-09 1.1332139e-14 1.0570278e-13 1.0000000e+00
 8.4115276e-10 7.2272841e-15], sum to 1.0000
[2019-04-04 16:44:24,305] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5198
[2019-04-04 16:44:24,322] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.9, 79.66666666666667, 0.0, 0.0, 26.0, 24.79001940996297, 0.2480076181376819, 0.0, 1.0, 41164.89214548578], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 859800.0000, 
sim time next is 860400.0000, 
raw observation next is [-2.8, 79.0, 0.0, 0.0, 26.0, 24.79137853569679, 0.2419912314705668, 0.0, 1.0, 41034.72958760418], 
processed observation next is [1.0, 1.0, 0.38504155124653744, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5659482113080658, 0.5806637438235223, 0.0, 1.0, 0.19540347422668658], 
reward next is 0.8046, 
noisyNet noise sample is [array([0.72937936], dtype=float32), 1.0207517]. 
=============================================
[2019-04-04 16:44:26,148] A3C_AGENT_WORKER-Thread-4 INFO:Local step 299000, global step 4785981: loss 0.0032
[2019-04-04 16:44:26,150] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 299000, global step 4785981: learning rate 0.0000
[2019-04-04 16:44:26,581] A3C_AGENT_WORKER-Thread-2 INFO:Local step 300000, global step 4786218: loss 19.8196
[2019-04-04 16:44:26,583] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 300000, global step 4786219: learning rate 0.0000
[2019-04-04 16:44:26,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2511977e-09 5.3330722e-11 9.2087421e-16 2.0214055e-14 1.0000000e+00
 5.8992672e-11 2.5015718e-16], sum to 1.0000
[2019-04-04 16:44:26,628] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4233
[2019-04-04 16:44:26,674] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.13529510005398, 0.2494024886136979, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 891600.0000, 
sim time next is 892200.0000, 
raw observation next is [0.0, 72.0, 9.666666666666664, 0.0, 26.0, 25.06802907566821, 0.239358943512335, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.032222222222222215, 0.0, 0.6666666666666666, 0.5890024229723508, 0.5797863145041117, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.7063987], dtype=float32), 1.8505067]. 
=============================================
[2019-04-04 16:44:27,205] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5361439e-11 2.8559095e-12 7.1328854e-19 4.2068619e-17 1.0000000e+00
 3.7259028e-12 2.6824715e-18], sum to 1.0000
[2019-04-04 16:44:27,205] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1876
[2019-04-04 16:44:27,213] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.116666666666667, 62.16666666666666, 205.3333333333333, 141.6666666666667, 26.0, 26.83624278007356, 0.7474969011430944, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1594200.0000, 
sim time next is 1594800.0000, 
raw observation next is [9.4, 61.0, 208.0, 168.5, 26.0, 26.82862144157125, 0.7603841826108401, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7229916897506927, 0.61, 0.6933333333333334, 0.1861878453038674, 0.6666666666666666, 0.7357184534642709, 0.7534613942036134, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29028144], dtype=float32), 0.3094485]. 
=============================================
[2019-04-04 16:44:27,305] A3C_AGENT_WORKER-Thread-10 INFO:Local step 299000, global step 4786574: loss 0.0035
[2019-04-04 16:44:27,305] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 299000, global step 4786574: learning rate 0.0000
[2019-04-04 16:44:27,353] A3C_AGENT_WORKER-Thread-16 INFO:Local step 299000, global step 4786601: loss 0.0033
[2019-04-04 16:44:27,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 299000, global step 4786601: learning rate 0.0000
[2019-04-04 16:44:27,390] A3C_AGENT_WORKER-Thread-3 INFO:Local step 299000, global step 4786620: loss 0.0038
[2019-04-04 16:44:27,391] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 299000, global step 4786620: learning rate 0.0000
[2019-04-04 16:44:28,740] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.07301806e-10 2.43232821e-12 3.72606195e-18 3.84953644e-17
 1.00000000e+00 1.45928287e-11 1.43026476e-17], sum to 1.0000
[2019-04-04 16:44:28,748] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9293
[2019-04-04 16:44:28,762] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 24.89884660281793, 0.4069819499647864, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1023600.0000, 
sim time next is 1024200.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 24.84999102411097, 0.4279181735159576, 0.0, 1.0, 196653.9623809474], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5708325853425809, 0.6426393911719859, 0.0, 1.0, 0.9364474399092734], 
reward next is 0.0636, 
noisyNet noise sample is [array([-0.2047721], dtype=float32), 0.34355366]. 
=============================================
[2019-04-04 16:44:28,848] A3C_AGENT_WORKER-Thread-20 INFO:Local step 299000, global step 4787531: loss 0.0030
[2019-04-04 16:44:28,848] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 299000, global step 4787531: learning rate 0.0000
[2019-04-04 16:44:29,628] A3C_AGENT_WORKER-Thread-13 INFO:Local step 300000, global step 4788028: loss 19.8430
[2019-04-04 16:44:29,629] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 300000, global step 4788028: learning rate 0.0000
[2019-04-04 16:44:29,794] A3C_AGENT_WORKER-Thread-6 INFO:Local step 299000, global step 4788120: loss 0.0033
[2019-04-04 16:44:29,795] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 299000, global step 4788120: learning rate 0.0000
[2019-04-04 16:44:30,817] A3C_AGENT_WORKER-Thread-12 INFO:Local step 299000, global step 4788761: loss 0.0037
[2019-04-04 16:44:30,820] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 299000, global step 4788762: learning rate 0.0000
[2019-04-04 16:44:31,623] A3C_AGENT_WORKER-Thread-14 INFO:Local step 299000, global step 4789264: loss 0.0033
[2019-04-04 16:44:31,625] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 299000, global step 4789264: learning rate 0.0000
[2019-04-04 16:44:32,386] A3C_AGENT_WORKER-Thread-19 INFO:Local step 299000, global step 4789714: loss 0.0031
[2019-04-04 16:44:32,388] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 299000, global step 4789714: learning rate 0.0000
[2019-04-04 16:44:32,931] A3C_AGENT_WORKER-Thread-11 INFO:Local step 300000, global step 4790042: loss 19.8521
[2019-04-04 16:44:32,935] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 300000, global step 4790043: learning rate 0.0000
[2019-04-04 16:44:33,643] A3C_AGENT_WORKER-Thread-15 INFO:Local step 299500, global step 4790499: loss 1.3747
[2019-04-04 16:44:33,645] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 299500, global step 4790501: learning rate 0.0000
[2019-04-04 16:44:34,555] A3C_AGENT_WORKER-Thread-5 INFO:Local step 299000, global step 4791132: loss 0.0031
[2019-04-04 16:44:34,556] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 299000, global step 4791133: learning rate 0.0000
[2019-04-04 16:44:34,928] A3C_AGENT_WORKER-Thread-18 INFO:Local step 299000, global step 4791396: loss 0.0028
[2019-04-04 16:44:34,928] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 299000, global step 4791396: learning rate 0.0000
[2019-04-04 16:44:37,031] A3C_AGENT_WORKER-Thread-4 INFO:Local step 299500, global step 4792736: loss 1.4010
[2019-04-04 16:44:37,043] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 299500, global step 4792738: learning rate 0.0000
[2019-04-04 16:44:38,228] A3C_AGENT_WORKER-Thread-3 INFO:Local step 299500, global step 4793484: loss 1.4000
[2019-04-04 16:44:38,233] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 299500, global step 4793484: learning rate 0.0000
[2019-04-04 16:44:38,286] A3C_AGENT_WORKER-Thread-10 INFO:Local step 299500, global step 4793522: loss 1.3896
[2019-04-04 16:44:38,286] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 299500, global step 4793522: learning rate 0.0000
[2019-04-04 16:44:38,566] A3C_AGENT_WORKER-Thread-16 INFO:Local step 299500, global step 4793702: loss 1.3822
[2019-04-04 16:44:38,568] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 299500, global step 4793702: learning rate 0.0000
[2019-04-04 16:44:38,840] A3C_AGENT_WORKER-Thread-17 INFO:Local step 300000, global step 4793878: loss 19.8343
[2019-04-04 16:44:38,842] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 300000, global step 4793879: learning rate 0.0000
[2019-04-04 16:44:39,124] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0872798e-08 2.1155371e-09 1.5762341e-14 2.0233043e-13 1.0000000e+00
 5.0857407e-10 3.3064646e-14], sum to 1.0000
[2019-04-04 16:44:39,125] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4903
[2019-04-04 16:44:39,133] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 64.66666666666667, 9.666666666666664, 0.0, 26.0, 24.91410311822124, 0.4517360514103541, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1185000.0000, 
sim time next is 1185600.0000, 
raw observation next is [18.3, 64.33333333333334, 0.0, 0.0, 26.0, 24.89327663485841, 0.446278429122271, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.6433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5744397195715342, 0.6487594763740904, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.67001516], dtype=float32), 0.8260937]. 
=============================================
[2019-04-04 16:44:39,855] A3C_AGENT_WORKER-Thread-20 INFO:Local step 299500, global step 4794497: loss 1.3693
[2019-04-04 16:44:39,858] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 299500, global step 4794497: learning rate 0.0000
[2019-04-04 16:44:40,875] A3C_AGENT_WORKER-Thread-6 INFO:Local step 299500, global step 4795062: loss 1.3900
[2019-04-04 16:44:40,877] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 299500, global step 4795062: learning rate 0.0000
[2019-04-04 16:44:42,071] A3C_AGENT_WORKER-Thread-12 INFO:Local step 299500, global step 4795724: loss 1.3773
[2019-04-04 16:44:42,076] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 299500, global step 4795724: learning rate 0.0000
[2019-04-04 16:44:42,744] A3C_AGENT_WORKER-Thread-14 INFO:Local step 299500, global step 4796083: loss 1.3606
[2019-04-04 16:44:42,745] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 299500, global step 4796083: learning rate 0.0000
[2019-04-04 16:44:43,623] A3C_AGENT_WORKER-Thread-19 INFO:Local step 299500, global step 4796544: loss 1.3545
[2019-04-04 16:44:43,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 299500, global step 4796545: learning rate 0.0000
[2019-04-04 16:44:45,293] A3C_AGENT_WORKER-Thread-2 INFO:Local step 300500, global step 4797347: loss 0.2891
[2019-04-04 16:44:45,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 300500, global step 4797347: learning rate 0.0000
[2019-04-04 16:44:45,341] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.25248170e-10 1.09525826e-11 1.31676235e-17 3.68545226e-16
 1.00000000e+00 3.51274175e-12 8.79206385e-18], sum to 1.0000
[2019-04-04 16:44:45,341] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9160
[2019-04-04 16:44:45,347] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 124.6666666666667, 0.0, 26.0, 26.11833518701989, 0.5828239579084681, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1338000.0000, 
sim time next is 1338600.0000, 
raw observation next is [1.1, 92.0, 122.3333333333333, 0.0, 26.0, 26.05895303016255, 0.5791894115800746, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.4077777777777777, 0.0, 0.6666666666666666, 0.6715794191802124, 0.6930631371933581, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43126792], dtype=float32), 1.4103853]. 
=============================================
[2019-04-04 16:44:45,355] A3C_AGENT_WORKER-Thread-5 INFO:Local step 299500, global step 4797374: loss 1.3477
[2019-04-04 16:44:45,357] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 299500, global step 4797375: learning rate 0.0000
[2019-04-04 16:44:45,538] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5805855e-09 9.1360225e-11 1.2358719e-15 1.5745777e-15 1.0000000e+00
 8.2541807e-11 5.9607810e-16], sum to 1.0000
[2019-04-04 16:44:45,541] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9275
[2019-04-04 16:44:45,559] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.4, 98.33333333333333, 0.0, 0.0, 26.0, 25.35515219223926, 0.4634920563357586, 0.0, 1.0, 73008.8662973276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1395600.0000, 
sim time next is 1396200.0000, 
raw observation next is [-0.5, 99.16666666666667, 0.0, 0.0, 26.0, 25.29121214708896, 0.4543449070520557, 0.0, 1.0, 57177.13757754653], 
processed observation next is [1.0, 0.13043478260869565, 0.44875346260387816, 0.9916666666666667, 0.0, 0.0, 0.6666666666666666, 0.6076010122574133, 0.6514483023506853, 0.0, 1.0, 0.27227208370260253], 
reward next is 0.7277, 
noisyNet noise sample is [array([-0.28805378], dtype=float32), -0.27521995]. 
=============================================
[2019-04-04 16:44:46,021] A3C_AGENT_WORKER-Thread-18 INFO:Local step 299500, global step 4797710: loss 1.3527
[2019-04-04 16:44:46,021] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 299500, global step 4797710: learning rate 0.0000
[2019-04-04 16:44:48,433] A3C_AGENT_WORKER-Thread-13 INFO:Local step 300500, global step 4798874: loss 0.3015
[2019-04-04 16:44:48,435] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 300500, global step 4798874: learning rate 0.0000
[2019-04-04 16:44:48,906] A3C_AGENT_WORKER-Thread-15 INFO:Local step 300000, global step 4799096: loss 20.0312
[2019-04-04 16:44:48,910] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 300000, global step 4799096: learning rate 0.0000
[2019-04-04 16:44:50,710] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.76047704e-10 1.04088925e-11 1.46038303e-18 3.61394680e-17
 1.00000000e+00 3.16130021e-12 4.03777256e-18], sum to 1.0000
[2019-04-04 16:44:50,711] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9732
[2019-04-04 16:44:50,727] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.266666666666667, 79.66666666666667, 97.5, 700.1666666666667, 26.0, 25.04896987244061, 0.4998302699793242, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1514400.0000, 
sim time next is 1515000.0000, 
raw observation next is [6.733333333333333, 76.33333333333333, 95.0, 700.3333333333334, 26.0, 25.31016702865256, 0.5485871738812357, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.649122807017544, 0.7633333333333333, 0.31666666666666665, 0.7738489871086557, 0.6666666666666666, 0.6091805857210467, 0.6828623912937452, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8229897], dtype=float32), 0.7944687]. 
=============================================
[2019-04-04 16:44:50,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[98.12895 ]
 [98.528336]
 [98.757   ]
 [98.869576]
 [98.88165 ]], R is [[97.67086029]
 [97.69415283]
 [97.71720886]
 [97.74003601]
 [97.76263428]].
[2019-04-04 16:44:50,804] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-04 16:44:50,805] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 16:44:50,806] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:44:50,808] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 16:44:50,809] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 16:44:50,809] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:44:50,809] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:44:50,815] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run49
[2019-04-04 16:44:50,835] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run49
[2019-04-04 16:44:50,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run49
[2019-04-04 16:44:54,810] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18365623], dtype=float32), 0.24704894]
[2019-04-04 16:44:54,810] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [7.283333333333333, 95.5, 0.0, 0.0, 26.0, 20.59498313687235, -0.7180825955456754, 0.0, 1.0, 42150.86720822113]
[2019-04-04 16:44:54,810] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 16:44:54,810] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.41242606e-08 1.44589185e-08 1.72718421e-13 3.69327052e-12
 9.99999881e-01 5.23974064e-09 2.21929300e-13], sampled 0.206716806244999
[2019-04-04 16:46:24,698] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.18365623], dtype=float32), 0.24704894]
[2019-04-04 16:46:24,699] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [17.3, 36.0, 148.5, 758.5, 26.0, 27.77903286002793, 1.0640785523233, 1.0, 0.0, 0.0]
[2019-04-04 16:46:24,699] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 16:46:24,700] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.1050447e-11 6.0884310e-12 3.3111974e-18 4.3929561e-17 1.0000000e+00
 9.5271501e-13 8.6751262e-18], sampled 0.4386013263942711
[2019-04-04 16:46:31,307] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5866 239906820.8839 1605.2312
[2019-04-04 16:46:52,747] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 16:46:53,560] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 16:46:54,584] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 4800000, evaluation results [4800000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.586567219416, 239906820.88392106, 1605.2312036579478, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 16:46:55,094] A3C_AGENT_WORKER-Thread-11 INFO:Local step 300500, global step 4800246: loss 0.3067
[2019-04-04 16:46:55,095] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 300500, global step 4800246: learning rate 0.0000
[2019-04-04 16:46:55,780] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4629850e-10 6.0768321e-11 6.5841544e-16 2.2072010e-15 1.0000000e+00
 4.5255927e-11 5.3741175e-16], sum to 1.0000
[2019-04-04 16:46:55,780] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9622
[2019-04-04 16:46:55,824] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.266666666666667, 91.0, 0.0, 0.0, 26.0, 24.93535655499545, 0.4584292571313853, 0.0, 1.0, 199564.5892163587], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1455600.0000, 
sim time next is 1456200.0000, 
raw observation next is [1.35, 90.5, 0.0, 0.0, 26.0, 24.98537416015688, 0.4961658579151891, 0.0, 1.0, 128543.7826799539], 
processed observation next is [1.0, 0.8695652173913043, 0.5000000000000001, 0.905, 0.0, 0.0, 0.6666666666666666, 0.5821145133464066, 0.665388619305063, 0.0, 1.0, 0.6121132508569234], 
reward next is 0.3879, 
noisyNet noise sample is [array([-3.0735075], dtype=float32), -0.302619]. 
=============================================
[2019-04-04 16:46:55,900] A3C_AGENT_WORKER-Thread-4 INFO:Local step 300000, global step 4800697: loss 20.0542
[2019-04-04 16:46:55,901] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 300000, global step 4800697: learning rate 0.0000
[2019-04-04 16:46:56,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1481460e-09 1.0333176e-10 1.2220789e-16 2.2524518e-15 1.0000000e+00
 5.0343937e-11 6.2044295e-16], sum to 1.0000
[2019-04-04 16:46:56,438] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1844
[2019-04-04 16:46:56,451] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.9, 80.16666666666667, 0.0, 0.0, 26.0, 25.424915302128, 0.5362061785335087, 0.0, 1.0, 82056.19092323219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1563000.0000, 
sim time next is 1563600.0000, 
raw observation next is [4.800000000000001, 81.33333333333334, 0.0, 0.0, 26.0, 25.48307272449965, 0.5435235552717846, 0.0, 1.0, 21687.0193496955], 
processed observation next is [1.0, 0.08695652173913043, 0.5955678670360112, 0.8133333333333335, 0.0, 0.0, 0.6666666666666666, 0.6235893937083041, 0.6811745184239282, 0.0, 1.0, 0.10327152071283571], 
reward next is 0.8967, 
noisyNet noise sample is [array([0.218472], dtype=float32), 0.3916552]. 
=============================================
[2019-04-04 16:46:57,072] A3C_AGENT_WORKER-Thread-10 INFO:Local step 300000, global step 4801374: loss 20.0710
[2019-04-04 16:46:57,074] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 300000, global step 4801375: learning rate 0.0000
[2019-04-04 16:46:57,161] A3C_AGENT_WORKER-Thread-16 INFO:Local step 300000, global step 4801411: loss 20.0867
[2019-04-04 16:46:57,161] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 300000, global step 4801411: learning rate 0.0000
[2019-04-04 16:46:57,367] A3C_AGENT_WORKER-Thread-3 INFO:Local step 300000, global step 4801498: loss 20.0891
[2019-04-04 16:46:57,372] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 300000, global step 4801498: learning rate 0.0000
[2019-04-04 16:46:58,667] A3C_AGENT_WORKER-Thread-20 INFO:Local step 300000, global step 4802160: loss 20.0892
[2019-04-04 16:46:58,669] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 300000, global step 4802160: learning rate 0.0000
[2019-04-04 16:46:59,490] A3C_AGENT_WORKER-Thread-6 INFO:Local step 300000, global step 4802660: loss 20.0757
[2019-04-04 16:46:59,496] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 300000, global step 4802660: learning rate 0.0000
[2019-04-04 16:47:00,481] A3C_AGENT_WORKER-Thread-12 INFO:Local step 300000, global step 4803238: loss 20.0764
[2019-04-04 16:47:00,484] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 300000, global step 4803239: learning rate 0.0000
[2019-04-04 16:47:00,913] A3C_AGENT_WORKER-Thread-17 INFO:Local step 300500, global step 4803451: loss 0.3191
[2019-04-04 16:47:00,914] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 300500, global step 4803451: learning rate 0.0000
[2019-04-04 16:47:01,205] A3C_AGENT_WORKER-Thread-14 INFO:Local step 300000, global step 4803611: loss 20.0935
[2019-04-04 16:47:01,207] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 300000, global step 4803611: learning rate 0.0000
[2019-04-04 16:47:01,800] A3C_AGENT_WORKER-Thread-19 INFO:Local step 300000, global step 4803914: loss 20.0653
[2019-04-04 16:47:01,802] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 300000, global step 4803915: learning rate 0.0000
[2019-04-04 16:47:03,779] A3C_AGENT_WORKER-Thread-5 INFO:Local step 300000, global step 4804957: loss 20.0749
[2019-04-04 16:47:03,780] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 300000, global step 4804957: learning rate 0.0000
[2019-04-04 16:47:04,208] A3C_AGENT_WORKER-Thread-18 INFO:Local step 300000, global step 4805142: loss 20.0800
[2019-04-04 16:47:04,209] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 300000, global step 4805143: learning rate 0.0000
[2019-04-04 16:47:05,311] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0466471e-09 5.6867278e-10 2.8793587e-15 3.7971471e-14 1.0000000e+00
 9.1122672e-11 2.0479804e-15], sum to 1.0000
[2019-04-04 16:47:05,311] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7564
[2019-04-04 16:47:05,335] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 82.0, 0.0, 0.0, 26.0, 24.71751718405249, 0.2319262873615746, 0.0, 1.0, 45585.15786827578], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1810800.0000, 
sim time next is 1811400.0000, 
raw observation next is [-5.0, 81.50000000000001, 0.0, 0.0, 26.0, 24.68498847170027, 0.2254813499281989, 0.0, 1.0, 45534.38447946731], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.8150000000000002, 0.0, 0.0, 0.6666666666666666, 0.557082372641689, 0.5751604499760663, 0.0, 1.0, 0.21683040228317768], 
reward next is 0.7832, 
noisyNet noise sample is [array([-0.30887908], dtype=float32), 2.7137291]. 
=============================================
[2019-04-04 16:47:06,218] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7244694e-11 5.8491480e-12 1.2791109e-17 2.4918476e-16 1.0000000e+00
 5.2343594e-12 2.1680444e-17], sum to 1.0000
[2019-04-04 16:47:06,219] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9505
[2019-04-04 16:47:06,251] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.6, 93.0, 0.0, 0.0, 26.0, 25.62783776066102, 0.5848993508354791, 0.0, 1.0, 33347.55825022262], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1645200.0000, 
sim time next is 1645800.0000, 
raw observation next is [6.7, 93.5, 0.0, 0.0, 26.0, 25.64872269358687, 0.5800164200824448, 0.0, 1.0, 19200.725859169], 
processed observation next is [1.0, 0.043478260869565216, 0.6481994459833795, 0.935, 0.0, 0.0, 0.6666666666666666, 0.6373935577989057, 0.6933388066941483, 0.0, 1.0, 0.09143202790080476], 
reward next is 0.9086, 
noisyNet noise sample is [array([-0.25541022], dtype=float32), 0.25061613]. 
=============================================
[2019-04-04 16:47:09,290] A3C_AGENT_WORKER-Thread-2 INFO:Local step 301000, global step 4807265: loss 0.0051
[2019-04-04 16:47:09,290] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 301000, global step 4807265: learning rate 0.0000
[2019-04-04 16:47:11,042] A3C_AGENT_WORKER-Thread-15 INFO:Local step 300500, global step 4807902: loss 0.3409
[2019-04-04 16:47:11,043] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 300500, global step 4807902: learning rate 0.0000
[2019-04-04 16:47:13,018] A3C_AGENT_WORKER-Thread-13 INFO:Local step 301000, global step 4808636: loss 0.0049
[2019-04-04 16:47:13,020] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 301000, global step 4808636: learning rate 0.0000
[2019-04-04 16:47:14,984] A3C_AGENT_WORKER-Thread-4 INFO:Local step 300500, global step 4809276: loss 0.3502
[2019-04-04 16:47:14,995] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 300500, global step 4809276: learning rate 0.0000
[2019-04-04 16:47:15,596] A3C_AGENT_WORKER-Thread-10 INFO:Local step 300500, global step 4809509: loss 0.3513
[2019-04-04 16:47:15,597] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 300500, global step 4809510: learning rate 0.0000
[2019-04-04 16:47:15,617] A3C_AGENT_WORKER-Thread-3 INFO:Local step 300500, global step 4809518: loss 0.3485
[2019-04-04 16:47:15,618] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 300500, global step 4809519: learning rate 0.0000
[2019-04-04 16:47:15,764] A3C_AGENT_WORKER-Thread-11 INFO:Local step 301000, global step 4809577: loss 0.0036
[2019-04-04 16:47:15,765] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 301000, global step 4809577: learning rate 0.0000
[2019-04-04 16:47:15,871] A3C_AGENT_WORKER-Thread-16 INFO:Local step 300500, global step 4809613: loss 0.3546
[2019-04-04 16:47:15,872] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 300500, global step 4809613: learning rate 0.0000
[2019-04-04 16:47:17,443] A3C_AGENT_WORKER-Thread-20 INFO:Local step 300500, global step 4810176: loss 0.3520
[2019-04-04 16:47:17,444] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 300500, global step 4810176: learning rate 0.0000
[2019-04-04 16:47:18,452] A3C_AGENT_WORKER-Thread-6 INFO:Local step 300500, global step 4810541: loss 0.3544
[2019-04-04 16:47:18,452] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 300500, global step 4810541: learning rate 0.0000
[2019-04-04 16:47:19,483] A3C_AGENT_WORKER-Thread-12 INFO:Local step 300500, global step 4810927: loss 0.3533
[2019-04-04 16:47:19,485] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 300500, global step 4810928: learning rate 0.0000
[2019-04-04 16:47:20,268] A3C_AGENT_WORKER-Thread-14 INFO:Local step 300500, global step 4811160: loss 0.3491
[2019-04-04 16:47:20,268] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 300500, global step 4811160: learning rate 0.0000
[2019-04-04 16:47:20,692] A3C_AGENT_WORKER-Thread-19 INFO:Local step 300500, global step 4811306: loss 0.3478
[2019-04-04 16:47:20,697] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 300500, global step 4811306: learning rate 0.0000
[2019-04-04 16:47:22,225] A3C_AGENT_WORKER-Thread-17 INFO:Local step 301000, global step 4811920: loss 0.0040
[2019-04-04 16:47:22,229] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 301000, global step 4811921: learning rate 0.0000
[2019-04-04 16:47:22,596] A3C_AGENT_WORKER-Thread-5 INFO:Local step 300500, global step 4812050: loss 0.3472
[2019-04-04 16:47:22,596] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 300500, global step 4812050: learning rate 0.0000
[2019-04-04 16:47:23,411] A3C_AGENT_WORKER-Thread-18 INFO:Local step 300500, global step 4812339: loss 0.3424
[2019-04-04 16:47:23,412] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 300500, global step 4812339: learning rate 0.0000
[2019-04-04 16:47:24,662] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.9986169e-09 1.1287622e-09 5.8919680e-15 4.2239297e-14 1.0000000e+00
 9.6589214e-10 4.7806279e-14], sum to 1.0000
[2019-04-04 16:47:24,663] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0834
[2019-04-04 16:47:24,703] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3, 46.0, 79.0, 58.0, 26.0, 24.95386236344752, 0.2846462888304614, 0.0, 1.0, 49397.71835914705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2392200.0000, 
sim time next is 2392800.0000, 
raw observation next is [-0.4, 45.66666666666667, 66.83333333333334, 51.0, 26.0, 24.94797735317678, 0.2844648081251537, 0.0, 1.0, 45660.27675235552], 
processed observation next is [0.0, 0.6956521739130435, 0.45152354570637127, 0.4566666666666667, 0.2227777777777778, 0.056353591160221, 0.6666666666666666, 0.5789981127647316, 0.5948216027083846, 0.0, 1.0, 0.21742988929693105], 
reward next is 0.7826, 
noisyNet noise sample is [array([-0.7092638], dtype=float32), -1.7188022]. 
=============================================
[2019-04-04 16:47:27,436] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6646942e-08 1.3364327e-09 4.4782307e-14 4.8957361e-13 1.0000000e+00
 1.3991277e-09 2.4344792e-14], sum to 1.0000
[2019-04-04 16:47:27,437] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9837
[2019-04-04 16:47:27,451] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 73.66666666666667, 0.0, 0.0, 26.0, 24.93001285240996, 0.2926861874892148, 0.0, 1.0, 44254.91274642575], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2241600.0000, 
sim time next is 2242200.0000, 
raw observation next is [-6.1, 74.33333333333333, 0.0, 0.0, 26.0, 24.85463032071881, 0.2794598561294723, 0.0, 1.0, 44247.95537163718], 
processed observation next is [1.0, 0.9565217391304348, 0.29362880886426596, 0.7433333333333333, 0.0, 0.0, 0.6666666666666666, 0.5712191933932343, 0.5931532853764908, 0.0, 1.0, 0.21070454938874847], 
reward next is 0.7893, 
noisyNet noise sample is [array([-1.5076073], dtype=float32), 0.48833874]. 
=============================================
[2019-04-04 16:47:30,832] A3C_AGENT_WORKER-Thread-2 INFO:Local step 301500, global step 4815014: loss 0.0775
[2019-04-04 16:47:30,833] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 301500, global step 4815014: learning rate 0.0000
[2019-04-04 16:47:32,557] A3C_AGENT_WORKER-Thread-15 INFO:Local step 301000, global step 4815630: loss 0.0067
[2019-04-04 16:47:32,565] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 301000, global step 4815630: learning rate 0.0000
[2019-04-04 16:47:33,892] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6290249e-09 4.1241002e-10 4.4041016e-15 2.0465444e-14 1.0000000e+00
 6.2064370e-10 3.3204488e-15], sum to 1.0000
[2019-04-04 16:47:33,893] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8457
[2019-04-04 16:47:33,931] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.383333333333333, 76.16666666666666, 236.3333333333333, 73.66666666666666, 26.0, 25.77627190120097, 0.3880408772220877, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2112600.0000, 
sim time next is 2113200.0000, 
raw observation next is [-7.3, 75.0, 250.5, 80.5, 26.0, 25.75074466167064, 0.3830382437846569, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.26038781163434904, 0.75, 0.835, 0.08895027624309393, 0.6666666666666666, 0.6458953884725535, 0.6276794145948856, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39262092], dtype=float32), -1.0509568]. 
=============================================
[2019-04-04 16:47:34,845] A3C_AGENT_WORKER-Thread-13 INFO:Local step 301500, global step 4816466: loss 0.0730
[2019-04-04 16:47:34,847] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 301500, global step 4816466: learning rate 0.0000
[2019-04-04 16:47:36,050] A3C_AGENT_WORKER-Thread-4 INFO:Local step 301000, global step 4816898: loss 0.0081
[2019-04-04 16:47:36,051] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 301000, global step 4816898: learning rate 0.0000
[2019-04-04 16:47:36,779] A3C_AGENT_WORKER-Thread-10 INFO:Local step 301000, global step 4817183: loss 0.0076
[2019-04-04 16:47:36,781] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 301000, global step 4817184: learning rate 0.0000
[2019-04-04 16:47:37,273] A3C_AGENT_WORKER-Thread-3 INFO:Local step 301000, global step 4817392: loss 0.0069
[2019-04-04 16:47:37,274] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 301000, global step 4817392: learning rate 0.0000
[2019-04-04 16:47:37,421] A3C_AGENT_WORKER-Thread-11 INFO:Local step 301500, global step 4817451: loss 0.0738
[2019-04-04 16:47:37,422] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 301500, global step 4817451: learning rate 0.0000
[2019-04-04 16:47:37,441] A3C_AGENT_WORKER-Thread-16 INFO:Local step 301000, global step 4817459: loss 0.0071
[2019-04-04 16:47:37,442] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 301000, global step 4817459: learning rate 0.0000
[2019-04-04 16:47:38,566] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1670039e-08 5.1010124e-10 2.2726915e-15 3.9958346e-14 1.0000000e+00
 6.5090555e-10 8.9500398e-15], sum to 1.0000
[2019-04-04 16:47:38,571] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2991
[2019-04-04 16:47:38,643] A3C_AGENT_WORKER-Thread-20 INFO:Local step 301000, global step 4817900: loss 0.0073
[2019-04-04 16:47:38,644] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.299999999999999, 79.0, 30.33333333333333, 5.333333333333334, 26.0, 25.21423283654788, 0.2905819926794698, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2620200.0000, 
sim time next is 2620800.0000, 
raw observation next is [-7.3, 79.0, 42.0, 4.0, 26.0, 25.35074890440337, 0.2980758577835568, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.26038781163434904, 0.79, 0.14, 0.004419889502762431, 0.6666666666666666, 0.6125624087002809, 0.5993586192611856, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3883855], dtype=float32), -0.54994017]. 
=============================================
[2019-04-04 16:47:38,644] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 301000, global step 4817900: learning rate 0.0000
[2019-04-04 16:47:40,235] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.5950270e-09 3.5733907e-09 7.3956866e-14 3.9201726e-13 1.0000000e+00
 6.4650324e-10 1.9197812e-13], sum to 1.0000
[2019-04-04 16:47:40,236] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7724
[2019-04-04 16:47:40,260] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.383333333333333, 53.33333333333334, 0.0, 0.0, 26.0, 24.07463913252268, 0.03434368486542034, 0.0, 1.0, 43575.22110387148], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2427000.0000, 
sim time next is 2427600.0000, 
raw observation next is [-7.466666666666667, 53.66666666666667, 0.0, 0.0, 26.0, 24.02108863818698, 0.027100669459567, 0.0, 1.0, 43605.94557248103], 
processed observation next is [0.0, 0.08695652173913043, 0.25577100646352724, 0.5366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5017573865155818, 0.5090335564865224, 0.0, 1.0, 0.2076473598689573], 
reward next is 0.7924, 
noisyNet noise sample is [array([-0.5253374], dtype=float32), 0.6583466]. 
=============================================
[2019-04-04 16:47:40,283] A3C_AGENT_WORKER-Thread-6 INFO:Local step 301000, global step 4818470: loss 0.0084
[2019-04-04 16:47:40,287] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 301000, global step 4818471: learning rate 0.0000
[2019-04-04 16:47:41,188] A3C_AGENT_WORKER-Thread-12 INFO:Local step 301000, global step 4818813: loss 0.0081
[2019-04-04 16:47:41,189] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 301000, global step 4818814: learning rate 0.0000
[2019-04-04 16:47:41,784] A3C_AGENT_WORKER-Thread-14 INFO:Local step 301000, global step 4819044: loss 0.0087
[2019-04-04 16:47:41,785] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 301000, global step 4819044: learning rate 0.0000
[2019-04-04 16:47:42,442] A3C_AGENT_WORKER-Thread-19 INFO:Local step 301000, global step 4819283: loss 0.0080
[2019-04-04 16:47:42,443] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 301000, global step 4819283: learning rate 0.0000
[2019-04-04 16:47:43,778] A3C_AGENT_WORKER-Thread-17 INFO:Local step 301500, global step 4819845: loss 0.0702
[2019-04-04 16:47:43,779] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 301500, global step 4819845: learning rate 0.0000
[2019-04-04 16:47:43,869] A3C_AGENT_WORKER-Thread-5 INFO:Local step 301000, global step 4819883: loss 0.0077
[2019-04-04 16:47:43,870] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 301000, global step 4819883: learning rate 0.0000
[2019-04-04 16:47:45,020] A3C_AGENT_WORKER-Thread-18 INFO:Local step 301000, global step 4820294: loss 0.0090
[2019-04-04 16:47:45,021] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 301000, global step 4820295: learning rate 0.0000
[2019-04-04 16:47:48,936] A3C_AGENT_WORKER-Thread-2 INFO:Local step 302000, global step 4821816: loss 0.0019
[2019-04-04 16:47:48,937] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 302000, global step 4821816: learning rate 0.0000
[2019-04-04 16:47:50,133] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3165114e-09 5.0732618e-10 6.5739184e-15 3.2177386e-14 1.0000000e+00
 1.4959110e-10 5.5649281e-15], sum to 1.0000
[2019-04-04 16:47:50,135] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6759
[2019-04-04 16:47:50,180] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.550000000000001, 82.5, 101.0, 39.0, 26.0, 25.59322600214106, 0.3054496590856162, 1.0, 1.0, 18737.60918249515], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2280600.0000, 
sim time next is 2281200.0000, 
raw observation next is [-7.266666666666667, 81.0, 113.8333333333333, 40.83333333333333, 26.0, 25.60375487123953, 0.3160806935487487, 1.0, 1.0, 18734.99220253694], 
processed observation next is [1.0, 0.391304347826087, 0.26131117266851345, 0.81, 0.3794444444444443, 0.04511970534069981, 0.6666666666666666, 0.6336462392699609, 0.6053602311829162, 1.0, 1.0, 0.08921424858350925], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.59339774], dtype=float32), -0.93906844]. 
=============================================
[2019-04-04 16:47:53,103] A3C_AGENT_WORKER-Thread-13 INFO:Local step 302000, global step 4823435: loss 0.0013
[2019-04-04 16:47:53,108] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 302000, global step 4823436: learning rate 0.0000
[2019-04-04 16:47:53,991] A3C_AGENT_WORKER-Thread-15 INFO:Local step 301500, global step 4823810: loss 0.0657
[2019-04-04 16:47:53,993] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 301500, global step 4823812: learning rate 0.0000
[2019-04-04 16:47:55,692] A3C_AGENT_WORKER-Thread-11 INFO:Local step 302000, global step 4824596: loss 0.0014
[2019-04-04 16:47:55,693] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 302000, global step 4824596: learning rate 0.0000
[2019-04-04 16:47:55,710] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.5732010e-08 7.2220137e-09 2.7131860e-14 3.5109510e-13 9.9999988e-01
 8.0869406e-10 5.2460274e-14], sum to 1.0000
[2019-04-04 16:47:55,711] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4833
[2019-04-04 16:47:55,755] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.95, 39.5, 76.0, 777.0, 26.0, 25.00895034969729, 0.2284379564589994, 0.0, 1.0, 18744.69522215566], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2457000.0000, 
sim time next is 2457600.0000, 
raw observation next is [-3.4, 38.33333333333334, 77.66666666666667, 785.6666666666666, 26.0, 24.95901718000691, 0.2305152567222996, 0.0, 1.0, 41246.86500088676], 
processed observation next is [0.0, 0.43478260869565216, 0.368421052631579, 0.3833333333333334, 0.2588888888888889, 0.8681399631675875, 0.6666666666666666, 0.5799180983339092, 0.5768384189074333, 0.0, 1.0, 0.19641364286136553], 
reward next is 0.8036, 
noisyNet noise sample is [array([1.2892883], dtype=float32), -2.1238265]. 
=============================================
[2019-04-04 16:47:55,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.4026011e-08 1.2965267e-08 1.3516135e-13 3.0058850e-12 9.9999988e-01
 5.4706488e-09 2.5475108e-13], sum to 1.0000
[2019-04-04 16:47:55,784] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9466
[2019-04-04 16:47:55,833] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.24630049368351, 0.09750474396487017, 0.0, 1.0, 41264.795268236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2786400.0000, 
sim time next is 2787000.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.27336726989168, 0.07513450017670341, 0.0, 1.0, 41590.83103436901], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5227806058243066, 0.5250448333922345, 0.0, 1.0, 0.19805157635413814], 
reward next is 0.8019, 
noisyNet noise sample is [array([1.5389524], dtype=float32), -0.03435215]. 
=============================================
[2019-04-04 16:47:55,839] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[72.345634]
 [72.39906 ]
 [72.46442 ]
 [72.53433 ]
 [72.59808 ]], R is [[72.35714722]
 [72.43707275]
 [72.51613617]
 [72.5945282 ]
 [72.67238617]].
[2019-04-04 16:47:57,467] A3C_AGENT_WORKER-Thread-4 INFO:Local step 301500, global step 4825267: loss 0.0654
[2019-04-04 16:47:57,472] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 301500, global step 4825268: learning rate 0.0000
[2019-04-04 16:47:58,354] A3C_AGENT_WORKER-Thread-10 INFO:Local step 301500, global step 4825649: loss 0.0640
[2019-04-04 16:47:58,362] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 301500, global step 4825649: learning rate 0.0000
[2019-04-04 16:47:58,881] A3C_AGENT_WORKER-Thread-16 INFO:Local step 301500, global step 4825878: loss 0.0628
[2019-04-04 16:47:58,881] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 301500, global step 4825878: learning rate 0.0000
[2019-04-04 16:47:58,927] A3C_AGENT_WORKER-Thread-3 INFO:Local step 301500, global step 4825891: loss 0.0639
[2019-04-04 16:47:58,927] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 301500, global step 4825891: learning rate 0.0000
[2019-04-04 16:47:59,640] A3C_AGENT_WORKER-Thread-20 INFO:Local step 301500, global step 4826205: loss 0.0629
[2019-04-04 16:47:59,641] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 301500, global step 4826205: learning rate 0.0000
[2019-04-04 16:48:01,685] A3C_AGENT_WORKER-Thread-6 INFO:Local step 301500, global step 4827124: loss 0.0608
[2019-04-04 16:48:01,685] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 301500, global step 4827124: learning rate 0.0000
[2019-04-04 16:48:01,749] A3C_AGENT_WORKER-Thread-12 INFO:Local step 301500, global step 4827151: loss 0.0608
[2019-04-04 16:48:01,750] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 301500, global step 4827151: learning rate 0.0000
[2019-04-04 16:48:02,188] A3C_AGENT_WORKER-Thread-17 INFO:Local step 302000, global step 4827347: loss 0.0006
[2019-04-04 16:48:02,192] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 302000, global step 4827347: learning rate 0.0000
[2019-04-04 16:48:03,355] A3C_AGENT_WORKER-Thread-14 INFO:Local step 301500, global step 4827950: loss 0.0593
[2019-04-04 16:48:03,358] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 301500, global step 4827951: learning rate 0.0000
[2019-04-04 16:48:03,459] A3C_AGENT_WORKER-Thread-19 INFO:Local step 301500, global step 4827998: loss 0.0586
[2019-04-04 16:48:03,460] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 301500, global step 4827998: learning rate 0.0000
[2019-04-04 16:48:05,248] A3C_AGENT_WORKER-Thread-5 INFO:Local step 301500, global step 4828829: loss 0.0572
[2019-04-04 16:48:05,248] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 301500, global step 4828829: learning rate 0.0000
[2019-04-04 16:48:06,048] A3C_AGENT_WORKER-Thread-18 INFO:Local step 301500, global step 4829195: loss 0.0568
[2019-04-04 16:48:06,048] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 301500, global step 4829195: learning rate 0.0000
[2019-04-04 16:48:07,323] A3C_AGENT_WORKER-Thread-2 INFO:Local step 302500, global step 4829697: loss 0.0567
[2019-04-04 16:48:07,327] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 302500, global step 4829699: learning rate 0.0000
[2019-04-04 16:48:11,658] A3C_AGENT_WORKER-Thread-13 INFO:Local step 302500, global step 4831688: loss 0.0533
[2019-04-04 16:48:11,661] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 302500, global step 4831689: learning rate 0.0000
[2019-04-04 16:48:11,673] A3C_AGENT_WORKER-Thread-15 INFO:Local step 302000, global step 4831696: loss 0.0002
[2019-04-04 16:48:11,673] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 302000, global step 4831696: learning rate 0.0000
[2019-04-04 16:48:14,066] A3C_AGENT_WORKER-Thread-11 INFO:Local step 302500, global step 4832681: loss 0.0664
[2019-04-04 16:48:14,076] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 302500, global step 4832687: learning rate 0.0000
[2019-04-04 16:48:15,039] A3C_AGENT_WORKER-Thread-4 INFO:Local step 302000, global step 4833082: loss 0.0002
[2019-04-04 16:48:15,041] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 302000, global step 4833083: learning rate 0.0000
[2019-04-04 16:48:15,807] A3C_AGENT_WORKER-Thread-10 INFO:Local step 302000, global step 4833470: loss 0.0002
[2019-04-04 16:48:15,808] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 302000, global step 4833470: learning rate 0.0000
[2019-04-04 16:48:16,319] A3C_AGENT_WORKER-Thread-3 INFO:Local step 302000, global step 4833695: loss 0.0002
[2019-04-04 16:48:16,320] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 302000, global step 4833695: learning rate 0.0000
[2019-04-04 16:48:16,406] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2723450e-08 1.1646550e-09 6.6412681e-15 1.4796480e-14 1.0000000e+00
 9.0030366e-10 3.9658942e-15], sum to 1.0000
[2019-04-04 16:48:16,406] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3961
[2019-04-04 16:48:16,458] A3C_AGENT_WORKER-Thread-16 INFO:Local step 302000, global step 4833763: loss 0.0002
[2019-04-04 16:48:16,458] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 302000, global step 4833763: learning rate 0.0000
[2019-04-04 16:48:16,462] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-14.83333333333333, 84.33333333333334, 53.33333333333334, 220.0, 26.0, 25.03073319510012, 0.2860967574205116, 1.0, 1.0, 58977.94175320859], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2707800.0000, 
sim time next is 2708400.0000, 
raw observation next is [-14.66666666666667, 85.66666666666667, 66.66666666666667, 275.0, 26.0, 25.2772563558138, 0.3587295791304703, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.05632502308402576, 0.8566666666666667, 0.22222222222222224, 0.30386740331491713, 0.6666666666666666, 0.6064380296511501, 0.6195765263768235, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27819112], dtype=float32), -0.5411416]. 
=============================================
[2019-04-04 16:48:17,095] A3C_AGENT_WORKER-Thread-20 INFO:Local step 302000, global step 4834060: loss 0.0002
[2019-04-04 16:48:17,096] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 302000, global step 4834060: learning rate 0.0000
[2019-04-04 16:48:17,717] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2085310e-07 4.0680934e-09 5.6507896e-13 3.6383422e-12 9.9999988e-01
 1.1070951e-08 1.3494471e-13], sum to 1.0000
[2019-04-04 16:48:17,719] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6643
[2019-04-04 16:48:17,795] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 26.0, 23.15334610897002, -0.00885021965184333, 1.0, 1.0, 203105.2441193045], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2705400.0000, 
sim time next is 2706000.0000, 
raw observation next is [-15.0, 83.0, 13.33333333333333, 54.99999999999999, 26.0, 23.95398929691405, 0.1047419193957855, 1.0, 1.0, 154108.8093087058], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.04444444444444443, 0.060773480662983416, 0.6666666666666666, 0.4961657747428374, 0.5349139731319285, 1.0, 1.0, 0.733851472898599], 
reward next is 0.2661, 
noisyNet noise sample is [array([-0.44011995], dtype=float32), 1.2693651]. 
=============================================
[2019-04-04 16:48:17,813] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[70.48671 ]
 [67.619316]
 [66.88356 ]
 [67.079865]
 [67.26952 ]], R is [[72.72497559]
 [72.03055573]
 [71.34653473]
 [71.42688751]
 [71.50669861]].
[2019-04-04 16:48:19,253] A3C_AGENT_WORKER-Thread-6 INFO:Local step 302000, global step 4835036: loss 0.0002
[2019-04-04 16:48:19,254] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 302000, global step 4835036: learning rate 0.0000
[2019-04-04 16:48:19,595] A3C_AGENT_WORKER-Thread-12 INFO:Local step 302000, global step 4835175: loss 0.0003
[2019-04-04 16:48:19,596] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 302000, global step 4835175: learning rate 0.0000
[2019-04-04 16:48:20,862] A3C_AGENT_WORKER-Thread-17 INFO:Local step 302500, global step 4835746: loss 0.0673
[2019-04-04 16:48:20,863] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 302500, global step 4835747: learning rate 0.0000
[2019-04-04 16:48:21,033] A3C_AGENT_WORKER-Thread-19 INFO:Local step 302000, global step 4835831: loss 0.0002
[2019-04-04 16:48:21,034] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 302000, global step 4835831: learning rate 0.0000
[2019-04-04 16:48:21,453] A3C_AGENT_WORKER-Thread-14 INFO:Local step 302000, global step 4836062: loss 0.0002
[2019-04-04 16:48:21,454] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 302000, global step 4836062: learning rate 0.0000
[2019-04-04 16:48:21,876] A3C_AGENT_WORKER-Thread-2 INFO:Local step 303000, global step 4836272: loss 0.0026
[2019-04-04 16:48:21,877] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 303000, global step 4836273: learning rate 0.0000
[2019-04-04 16:48:22,888] A3C_AGENT_WORKER-Thread-5 INFO:Local step 302000, global step 4836733: loss 0.0002
[2019-04-04 16:48:22,895] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 302000, global step 4836733: learning rate 0.0000
[2019-04-04 16:48:24,078] A3C_AGENT_WORKER-Thread-18 INFO:Local step 302000, global step 4837267: loss 0.0002
[2019-04-04 16:48:24,079] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 302000, global step 4837267: learning rate 0.0000
[2019-04-04 16:48:27,160] A3C_AGENT_WORKER-Thread-13 INFO:Local step 303000, global step 4838759: loss 0.0021
[2019-04-04 16:48:27,160] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 303000, global step 4838759: learning rate 0.0000
[2019-04-04 16:48:28,749] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5044737e-09 1.8831980e-10 1.9526987e-15 2.9835147e-14 1.0000000e+00
 1.3989403e-10 4.3164812e-15], sum to 1.0000
[2019-04-04 16:48:28,751] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8384
[2019-04-04 16:48:28,779] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.166666666666667, 55.83333333333334, 74.0, 602.6666666666667, 26.0, 25.13784360291061, 0.409603931801616, 0.0, 1.0, 18706.99855625932], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2994600.0000, 
sim time next is 2995200.0000, 
raw observation next is [-1.0, 55.0, 69.5, 570.5, 26.0, 25.1484996624935, 0.4078059366466626, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.55, 0.23166666666666666, 0.6303867403314917, 0.6666666666666666, 0.5957083052077916, 0.6359353122155542, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26334468], dtype=float32), -0.913802]. 
=============================================
[2019-04-04 16:48:29,090] A3C_AGENT_WORKER-Thread-11 INFO:Local step 303000, global step 4839591: loss 0.0013
[2019-04-04 16:48:29,091] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 303000, global step 4839591: learning rate 0.0000
[2019-04-04 16:48:30,370] A3C_AGENT_WORKER-Thread-15 INFO:Local step 302500, global step 4840174: loss 0.0567
[2019-04-04 16:48:30,372] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 302500, global step 4840174: learning rate 0.0000
[2019-04-04 16:48:31,166] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1890158e-08 2.1246276e-09 7.1884052e-15 1.2865567e-13 1.0000000e+00
 1.3550649e-09 2.7645377e-13], sum to 1.0000
[2019-04-04 16:48:31,167] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5733
[2019-04-04 16:48:31,195] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.99025001735498, 0.04833237358874289, 0.0, 1.0, 40194.17684719063], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3040200.0000, 
sim time next is 3040800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.96252467209148, 0.04195334377217189, 0.0, 1.0, 40210.04271984425], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.4968770560076233, 0.5139844479240573, 0.0, 1.0, 0.19147639390402024], 
reward next is 0.8085, 
noisyNet noise sample is [array([-1.051083], dtype=float32), 0.87309855]. 
=============================================
[2019-04-04 16:48:33,612] A3C_AGENT_WORKER-Thread-4 INFO:Local step 302500, global step 4841658: loss 0.0589
[2019-04-04 16:48:33,615] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 302500, global step 4841659: learning rate 0.0000
[2019-04-04 16:48:34,138] A3C_AGENT_WORKER-Thread-10 INFO:Local step 302500, global step 4841921: loss 0.0581
[2019-04-04 16:48:34,140] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 302500, global step 4841923: learning rate 0.0000
[2019-04-04 16:48:34,637] A3C_AGENT_WORKER-Thread-3 INFO:Local step 302500, global step 4842175: loss 0.0553
[2019-04-04 16:48:34,638] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 302500, global step 4842175: learning rate 0.0000
[2019-04-04 16:48:34,702] A3C_AGENT_WORKER-Thread-16 INFO:Local step 302500, global step 4842205: loss 0.0544
[2019-04-04 16:48:34,706] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 302500, global step 4842207: learning rate 0.0000
[2019-04-04 16:48:35,359] A3C_AGENT_WORKER-Thread-20 INFO:Local step 302500, global step 4842550: loss 0.0744
[2019-04-04 16:48:35,360] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 302500, global step 4842552: learning rate 0.0000
[2019-04-04 16:48:35,544] A3C_AGENT_WORKER-Thread-17 INFO:Local step 303000, global step 4842645: loss 0.0011
[2019-04-04 16:48:35,547] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 303000, global step 4842647: learning rate 0.0000
[2019-04-04 16:48:35,981] A3C_AGENT_WORKER-Thread-2 INFO:Local step 303500, global step 4842891: loss 0.1357
[2019-04-04 16:48:35,982] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 303500, global step 4842891: learning rate 0.0000
[2019-04-04 16:48:37,270] A3C_AGENT_WORKER-Thread-6 INFO:Local step 302500, global step 4843546: loss 0.0544
[2019-04-04 16:48:37,270] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 302500, global step 4843546: learning rate 0.0000
[2019-04-04 16:48:37,656] A3C_AGENT_WORKER-Thread-12 INFO:Local step 302500, global step 4843740: loss 0.0558
[2019-04-04 16:48:37,657] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 302500, global step 4843740: learning rate 0.0000
[2019-04-04 16:48:38,148] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6466082e-09 7.3212320e-10 2.4293201e-15 1.7190363e-14 1.0000000e+00
 1.3483456e-10 2.9394102e-15], sum to 1.0000
[2019-04-04 16:48:38,154] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0921
[2019-04-04 16:48:38,199] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8666666666666667, 88.66666666666666, 0.0, 0.0, 26.0, 25.00418680260571, 0.2799925288581113, 0.0, 1.0, 31470.32567364124], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3091200.0000, 
sim time next is 3091800.0000, 
raw observation next is [-0.9333333333333333, 90.33333333333334, 0.0, 0.0, 26.0, 25.00188626402732, 0.2774154948289259, 0.0, 1.0, 34676.40779888361], 
processed observation next is [0.0, 0.782608695652174, 0.4367497691597415, 0.9033333333333334, 0.0, 0.0, 0.6666666666666666, 0.5834905220022767, 0.5924718316096419, 0.0, 1.0, 0.16512575142325528], 
reward next is 0.8349, 
noisyNet noise sample is [array([-0.96034676], dtype=float32), -0.47095487]. 
=============================================
[2019-04-04 16:48:39,276] A3C_AGENT_WORKER-Thread-19 INFO:Local step 302500, global step 4844647: loss 0.0551
[2019-04-04 16:48:39,276] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 302500, global step 4844647: learning rate 0.0000
[2019-04-04 16:48:39,574] A3C_AGENT_WORKER-Thread-14 INFO:Local step 302500, global step 4844834: loss 0.0562
[2019-04-04 16:48:39,577] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 302500, global step 4844834: learning rate 0.0000
[2019-04-04 16:48:41,273] A3C_AGENT_WORKER-Thread-13 INFO:Local step 303500, global step 4845790: loss 0.1367
[2019-04-04 16:48:41,275] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 303500, global step 4845790: learning rate 0.0000
[2019-04-04 16:48:41,303] A3C_AGENT_WORKER-Thread-5 INFO:Local step 302500, global step 4845799: loss 0.0586
[2019-04-04 16:48:41,304] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 302500, global step 4845799: learning rate 0.0000
[2019-04-04 16:48:42,186] A3C_AGENT_WORKER-Thread-18 INFO:Local step 302500, global step 4846308: loss 0.0570
[2019-04-04 16:48:42,187] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 302500, global step 4846308: learning rate 0.0000
[2019-04-04 16:48:43,438] A3C_AGENT_WORKER-Thread-11 INFO:Local step 303500, global step 4847036: loss 0.1415
[2019-04-04 16:48:43,439] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 303500, global step 4847036: learning rate 0.0000
[2019-04-04 16:48:43,957] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1637767e-10 2.7596200e-11 2.2741809e-17 2.7615625e-15 1.0000000e+00
 4.7699579e-12 2.8598926e-17], sum to 1.0000
[2019-04-04 16:48:43,958] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3378
[2019-04-04 16:48:43,992] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 92.0, 93.0, 511.5, 26.0, 26.00474802424819, 0.606518018572629, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3229200.0000, 
sim time next is 3229800.0000, 
raw observation next is [-3.0, 92.0, 95.66666666666667, 558.6666666666666, 26.0, 26.0604134199242, 0.6153482490682416, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.3188888888888889, 0.6173112338858194, 0.6666666666666666, 0.6717011183270166, 0.7051160830227472, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4201362], dtype=float32), -0.07630799]. 
=============================================
[2019-04-04 16:48:45,050] A3C_AGENT_WORKER-Thread-15 INFO:Local step 303000, global step 4847983: loss 0.0003
[2019-04-04 16:48:45,051] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 303000, global step 4847984: learning rate 0.0000
[2019-04-04 16:48:48,174] A3C_AGENT_WORKER-Thread-4 INFO:Local step 303000, global step 4849591: loss 0.0002
[2019-04-04 16:48:48,177] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 303000, global step 4849591: learning rate 0.0000
[2019-04-04 16:48:48,480] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.40279023e-08 4.69462558e-09 7.32156577e-14 2.64575064e-13
 1.00000000e+00 6.27921626e-09 1.02865165e-13], sum to 1.0000
[2019-04-04 16:48:48,481] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5468
[2019-04-04 16:48:48,512] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83022884864196, 0.2940488865409481, 0.0, 1.0, 41180.09886232638], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3377400.0000, 
sim time next is 3378000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.81289862752483, 0.2943540865650595, 0.0, 1.0, 41165.2508936169], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5677415522937359, 0.5981180288550199, 0.0, 1.0, 0.19602500425531857], 
reward next is 0.8040, 
noisyNet noise sample is [array([-0.54110193], dtype=float32), 0.18523182]. 
=============================================
[2019-04-04 16:48:48,530] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[76.94825 ]
 [76.95876 ]
 [77.07647 ]
 [77.00999 ]
 [77.104164]], R is [[76.95837402]
 [76.99269104]
 [77.02669525]
 [77.06034851]
 [77.09356689]].
[2019-04-04 16:48:48,781] A3C_AGENT_WORKER-Thread-10 INFO:Local step 303000, global step 4849925: loss 0.0002
[2019-04-04 16:48:48,782] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 303000, global step 4849925: learning rate 0.0000
[2019-04-04 16:48:49,065] A3C_AGENT_WORKER-Thread-3 INFO:Local step 303000, global step 4850085: loss 0.0002
[2019-04-04 16:48:49,065] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 303000, global step 4850085: learning rate 0.0000
[2019-04-04 16:48:49,211] A3C_AGENT_WORKER-Thread-16 INFO:Local step 303000, global step 4850177: loss 0.0002
[2019-04-04 16:48:49,212] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 303000, global step 4850177: learning rate 0.0000
[2019-04-04 16:48:49,268] A3C_AGENT_WORKER-Thread-17 INFO:Local step 303500, global step 4850218: loss 0.1491
[2019-04-04 16:48:49,271] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 303500, global step 4850219: learning rate 0.0000
[2019-04-04 16:48:50,060] A3C_AGENT_WORKER-Thread-20 INFO:Local step 303000, global step 4850604: loss 0.0002
[2019-04-04 16:48:50,075] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 303000, global step 4850604: learning rate 0.0000
[2019-04-04 16:48:50,570] A3C_AGENT_WORKER-Thread-2 INFO:Local step 304000, global step 4850840: loss 0.0041
[2019-04-04 16:48:50,570] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 304000, global step 4850840: learning rate 0.0000
[2019-04-04 16:48:51,764] A3C_AGENT_WORKER-Thread-6 INFO:Local step 303000, global step 4851517: loss 0.0002
[2019-04-04 16:48:51,765] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 303000, global step 4851517: learning rate 0.0000
[2019-04-04 16:48:51,886] A3C_AGENT_WORKER-Thread-12 INFO:Local step 303000, global step 4851579: loss 0.0003
[2019-04-04 16:48:51,898] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 303000, global step 4851580: learning rate 0.0000
[2019-04-04 16:48:52,762] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.7459746e-09 9.1124458e-10 2.5923189e-15 7.3845511e-14 1.0000000e+00
 8.1594209e-10 1.0575092e-14], sum to 1.0000
[2019-04-04 16:48:52,765] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5172
[2019-04-04 16:48:52,790] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23692667929891, 0.3775295268104918, 0.0, 1.0, 41756.36943945606], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3478800.0000, 
sim time next is 3479400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23099438648503, 0.3779886089195019, 0.0, 1.0, 41685.16883067759], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6025828655404192, 0.6259962029731673, 0.0, 1.0, 0.19850080395560757], 
reward next is 0.8015, 
noisyNet noise sample is [array([0.04027525], dtype=float32), -3.5474293]. 
=============================================
[2019-04-04 16:48:53,746] A3C_AGENT_WORKER-Thread-19 INFO:Local step 303000, global step 4852558: loss 0.0003
[2019-04-04 16:48:53,748] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 303000, global step 4852560: learning rate 0.0000
[2019-04-04 16:48:54,064] A3C_AGENT_WORKER-Thread-14 INFO:Local step 303000, global step 4852722: loss 0.0002
[2019-04-04 16:48:54,066] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 303000, global step 4852723: learning rate 0.0000
[2019-04-04 16:48:55,630] A3C_AGENT_WORKER-Thread-13 INFO:Local step 304000, global step 4853532: loss 0.0044
[2019-04-04 16:48:55,634] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 304000, global step 4853533: learning rate 0.0000
[2019-04-04 16:48:55,925] A3C_AGENT_WORKER-Thread-5 INFO:Local step 303000, global step 4853709: loss 0.0002
[2019-04-04 16:48:55,927] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 303000, global step 4853709: learning rate 0.0000
[2019-04-04 16:48:56,257] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0628138e-09 3.2778301e-11 4.0419357e-17 1.3891201e-15 1.0000000e+00
 2.9378579e-11 1.3985571e-16], sum to 1.0000
[2019-04-04 16:48:56,258] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3130
[2019-04-04 16:48:56,272] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 49.0, 104.1666666666667, 785.1666666666667, 26.0, 26.54188320323332, 0.7003230480017675, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3507600.0000, 
sim time next is 3508200.0000, 
raw observation next is [3.0, 49.0, 102.0, 781.0, 26.0, 26.62328048626127, 0.7140530760316044, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.34, 0.8629834254143647, 0.6666666666666666, 0.7186067071884391, 0.7380176920105348, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8169419], dtype=float32), 0.9143158]. 
=============================================
[2019-04-04 16:48:57,027] A3C_AGENT_WORKER-Thread-18 INFO:Local step 303000, global step 4854301: loss 0.0002
[2019-04-04 16:48:57,027] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 303000, global step 4854301: learning rate 0.0000
[2019-04-04 16:48:58,074] A3C_AGENT_WORKER-Thread-11 INFO:Local step 304000, global step 4854862: loss 0.0049
[2019-04-04 16:48:58,077] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 304000, global step 4854864: learning rate 0.0000
[2019-04-04 16:48:59,149] A3C_AGENT_WORKER-Thread-15 INFO:Local step 303500, global step 4855447: loss 0.1434
[2019-04-04 16:48:59,152] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 303500, global step 4855447: learning rate 0.0000
[2019-04-04 16:49:02,205] A3C_AGENT_WORKER-Thread-4 INFO:Local step 303500, global step 4857141: loss 0.1374
[2019-04-04 16:49:02,206] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 303500, global step 4857141: learning rate 0.0000
[2019-04-04 16:49:02,595] A3C_AGENT_WORKER-Thread-10 INFO:Local step 303500, global step 4857380: loss 0.1386
[2019-04-04 16:49:02,597] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 303500, global step 4857381: learning rate 0.0000
[2019-04-04 16:49:03,143] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9190275e-10 1.6259158e-10 3.9038706e-16 3.5336806e-16 1.0000000e+00
 1.3364110e-11 1.0391653e-15], sum to 1.0000
[2019-04-04 16:49:03,146] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9808
[2019-04-04 16:49:03,157] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.0, 26.0, 95.0, 533.0, 26.0, 25.63594496966672, 0.4506231117098384, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3661200.0000, 
sim time next is 3661800.0000, 
raw observation next is [11.0, 26.33333333333334, 97.0, 576.3333333333334, 26.0, 25.70280951852881, 0.4562760602478344, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.2633333333333334, 0.3233333333333333, 0.6368324125230203, 0.6666666666666666, 0.6419007932107341, 0.6520920200826115, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.07610504], dtype=float32), -0.7311411]. 
=============================================
[2019-04-04 16:49:03,203] A3C_AGENT_WORKER-Thread-3 INFO:Local step 303500, global step 4857735: loss 0.1308
[2019-04-04 16:49:03,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 303500, global step 4857735: learning rate 0.0000
[2019-04-04 16:49:03,831] A3C_AGENT_WORKER-Thread-17 INFO:Local step 304000, global step 4858095: loss 0.0077
[2019-04-04 16:49:03,833] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 304000, global step 4858095: learning rate 0.0000
[2019-04-04 16:49:03,912] A3C_AGENT_WORKER-Thread-16 INFO:Local step 303500, global step 4858140: loss 0.1360
[2019-04-04 16:49:03,917] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 303500, global step 4858142: learning rate 0.0000
[2019-04-04 16:49:04,282] A3C_AGENT_WORKER-Thread-20 INFO:Local step 303500, global step 4858359: loss 0.1306
[2019-04-04 16:49:04,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 303500, global step 4858359: learning rate 0.0000
[2019-04-04 16:49:06,257] A3C_AGENT_WORKER-Thread-6 INFO:Local step 303500, global step 4859387: loss 0.1318
[2019-04-04 16:49:06,260] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 303500, global step 4859387: learning rate 0.0000
[2019-04-04 16:49:06,268] A3C_AGENT_WORKER-Thread-12 INFO:Local step 303500, global step 4859396: loss 0.1295
[2019-04-04 16:49:06,271] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 303500, global step 4859397: learning rate 0.0000
[2019-04-04 16:49:06,365] A3C_AGENT_WORKER-Thread-2 INFO:Local step 304500, global step 4859442: loss 1.2438
[2019-04-04 16:49:06,369] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 304500, global step 4859445: learning rate 0.0000
[2019-04-04 16:49:08,004] A3C_AGENT_WORKER-Thread-19 INFO:Local step 303500, global step 4860361: loss 0.1236
[2019-04-04 16:49:08,005] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 303500, global step 4860362: learning rate 0.0000
[2019-04-04 16:49:08,527] A3C_AGENT_WORKER-Thread-14 INFO:Local step 303500, global step 4860680: loss 0.1266
[2019-04-04 16:49:08,527] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 303500, global step 4860680: learning rate 0.0000
[2019-04-04 16:49:09,639] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.0977349e-09 2.8474928e-10 4.6241646e-15 8.3797994e-15 1.0000000e+00
 3.6438469e-10 7.3434495e-15], sum to 1.0000
[2019-04-04 16:49:09,640] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7070
[2019-04-04 16:49:09,670] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 35.0, 182.0, 728.3333333333333, 26.0, 25.13194263247293, 0.4030424101682185, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4194600.0000, 
sim time next is 4195200.0000, 
raw observation next is [2.0, 36.0, 198.0, 698.6666666666666, 26.0, 25.13047077047482, 0.4032956613567711, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.36, 0.66, 0.7720073664825046, 0.6666666666666666, 0.5942058975395682, 0.6344318871189237, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.93782425], dtype=float32), 0.49612805]. 
=============================================
[2019-04-04 16:49:10,815] A3C_AGENT_WORKER-Thread-5 INFO:Local step 303500, global step 4861880: loss 0.1231
[2019-04-04 16:49:10,816] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 303500, global step 4861880: learning rate 0.0000
[2019-04-04 16:49:11,116] A3C_AGENT_WORKER-Thread-13 INFO:Local step 304500, global step 4862041: loss 1.2283
[2019-04-04 16:49:11,118] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 304500, global step 4862042: learning rate 0.0000
[2019-04-04 16:49:11,734] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.7489587e-10 2.0611436e-11 4.8259884e-17 1.1101552e-15 1.0000000e+00
 5.6947898e-11 3.5777769e-16], sum to 1.0000
[2019-04-04 16:49:11,735] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4744
[2019-04-04 16:49:11,746] A3C_AGENT_WORKER-Thread-18 INFO:Local step 303500, global step 4862390: loss 0.1249
[2019-04-04 16:49:11,747] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.166666666666667, 50.5, 0.0, 0.0, 26.0, 25.48740151959917, 0.5273649084088394, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3869400.0000, 
sim time next is 3870000.0000, 
raw observation next is [1.0, 51.0, 0.0, 0.0, 26.0, 25.51357683259871, 0.5325310389194134, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.51, 0.0, 0.0, 0.6666666666666666, 0.6261314027165591, 0.6775103463064712, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6833489], dtype=float32), 1.627086]. 
=============================================
[2019-04-04 16:49:11,751] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 303500, global step 4862391: learning rate 0.0000
[2019-04-04 16:49:11,767] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[83.03025]
 [82.9185 ]
 [82.86618]
 [82.81401]
 [81.90355]], R is [[83.30606842]
 [83.4730072 ]
 [83.63827515]
 [83.80189514]
 [83.08131409]].
[2019-04-04 16:49:13,840] A3C_AGENT_WORKER-Thread-11 INFO:Local step 304500, global step 4863553: loss 1.2383
[2019-04-04 16:49:13,840] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 304500, global step 4863553: learning rate 0.0000
[2019-04-04 16:49:14,424] A3C_AGENT_WORKER-Thread-15 INFO:Local step 304000, global step 4863862: loss 0.0091
[2019-04-04 16:49:14,425] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 304000, global step 4863862: learning rate 0.0000
[2019-04-04 16:49:18,093] A3C_AGENT_WORKER-Thread-4 INFO:Local step 304000, global step 4865682: loss 0.0080
[2019-04-04 16:49:18,102] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 304000, global step 4865685: learning rate 0.0000
[2019-04-04 16:49:18,177] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8016695e-10 9.0433036e-11 4.0587791e-16 1.9009776e-15 1.0000000e+00
 6.5647349e-11 2.8063552e-16], sum to 1.0000
[2019-04-04 16:49:18,179] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3153
[2019-04-04 16:49:18,201] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 47.66666666666667, 261.1666666666666, 102.1666666666667, 26.0, 26.25926326193117, 0.4930044176514095, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4544400.0000, 
sim time next is 4545000.0000, 
raw observation next is [3.0, 47.0, 264.0, 113.0, 26.0, 25.85814968332861, 0.5500367451053122, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.47, 0.88, 0.12486187845303867, 0.6666666666666666, 0.6548458069440507, 0.6833455817017707, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7587538], dtype=float32), 0.5237872]. 
=============================================
[2019-04-04 16:49:18,204] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[88.94441 ]
 [89.00666 ]
 [88.844894]
 [88.594894]
 [88.15782 ]], R is [[88.90341187]
 [89.01438141]
 [89.12423706]
 [89.23299408]
 [89.34066772]].
[2019-04-04 16:49:18,237] A3C_AGENT_WORKER-Thread-10 INFO:Local step 304000, global step 4865748: loss 0.0080
[2019-04-04 16:49:18,238] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 304000, global step 4865748: learning rate 0.0000
[2019-04-04 16:49:18,687] A3C_AGENT_WORKER-Thread-3 INFO:Local step 304000, global step 4865998: loss 0.0075
[2019-04-04 16:49:18,687] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 304000, global step 4865998: learning rate 0.0000
[2019-04-04 16:49:19,245] A3C_AGENT_WORKER-Thread-16 INFO:Local step 304000, global step 4866271: loss 0.0088
[2019-04-04 16:49:19,248] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 304000, global step 4866272: learning rate 0.0000
[2019-04-04 16:49:19,984] A3C_AGENT_WORKER-Thread-2 INFO:Local step 305000, global step 4866586: loss 0.0627
[2019-04-04 16:49:19,984] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 305000, global step 4866586: learning rate 0.0000
[2019-04-04 16:49:20,226] A3C_AGENT_WORKER-Thread-20 INFO:Local step 304000, global step 4866714: loss 0.0077
[2019-04-04 16:49:20,229] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 304000, global step 4866716: learning rate 0.0000
[2019-04-04 16:49:20,341] A3C_AGENT_WORKER-Thread-17 INFO:Local step 304500, global step 4866776: loss 1.2421
[2019-04-04 16:49:20,342] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 304500, global step 4866776: learning rate 0.0000
[2019-04-04 16:49:21,725] A3C_AGENT_WORKER-Thread-12 INFO:Local step 304000, global step 4867502: loss 0.0072
[2019-04-04 16:49:21,727] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 304000, global step 4867502: learning rate 0.0000
[2019-04-04 16:49:21,803] A3C_AGENT_WORKER-Thread-6 INFO:Local step 304000, global step 4867540: loss 0.0069
[2019-04-04 16:49:21,804] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 304000, global step 4867540: learning rate 0.0000
[2019-04-04 16:49:23,208] A3C_AGENT_WORKER-Thread-19 INFO:Local step 304000, global step 4868282: loss 0.0079
[2019-04-04 16:49:23,209] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 304000, global step 4868282: learning rate 0.0000
[2019-04-04 16:49:23,619] A3C_AGENT_WORKER-Thread-14 INFO:Local step 304000, global step 4868527: loss 0.0069
[2019-04-04 16:49:23,620] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 304000, global step 4868528: learning rate 0.0000
[2019-04-04 16:49:24,828] A3C_AGENT_WORKER-Thread-13 INFO:Local step 305000, global step 4869119: loss 0.0585
[2019-04-04 16:49:24,829] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 305000, global step 4869120: learning rate 0.0000
[2019-04-04 16:49:25,849] A3C_AGENT_WORKER-Thread-5 INFO:Local step 304000, global step 4869645: loss 0.0087
[2019-04-04 16:49:25,849] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 304000, global step 4869645: learning rate 0.0000
[2019-04-04 16:49:26,853] A3C_AGENT_WORKER-Thread-11 INFO:Local step 305000, global step 4870230: loss 0.0510
[2019-04-04 16:49:26,855] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 305000, global step 4870230: learning rate 0.0000
[2019-04-04 16:49:26,959] A3C_AGENT_WORKER-Thread-18 INFO:Local step 304000, global step 4870296: loss 0.0107
[2019-04-04 16:49:26,962] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 304000, global step 4870298: learning rate 0.0000
[2019-04-04 16:49:29,199] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.8749460e-10 2.9797203e-11 1.1323183e-16 2.5201628e-16 1.0000000e+00
 7.5407305e-11 1.3702822e-17], sum to 1.0000
[2019-04-04 16:49:29,202] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6990
[2019-04-04 16:49:29,213] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.800000000000001, 49.33333333333334, 192.3333333333333, 634.6666666666666, 26.0, 26.11095016113443, 0.7378600795596673, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4630800.0000, 
sim time next is 4631400.0000, 
raw observation next is [4.85, 49.5, 203.0, 599.0, 26.0, 26.62709638031499, 0.7941345850164576, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5969529085872576, 0.495, 0.6766666666666666, 0.661878453038674, 0.6666666666666666, 0.7189246983595824, 0.7647115283388192, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16584323], dtype=float32), -1.5073806]. 
=============================================
[2019-04-04 16:49:29,287] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0564273e-09 9.3811910e-11 3.3538637e-16 2.3158372e-15 1.0000000e+00
 6.3162545e-11 3.5947544e-16], sum to 1.0000
[2019-04-04 16:49:29,290] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0055
[2019-04-04 16:49:29,313] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.0, 35.0, 98.0, 728.0, 26.0, 27.15546292257693, 0.7817550602941606, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4115400.0000, 
sim time next is 4116000.0000, 
raw observation next is [4.0, 35.0, 96.0, 711.5, 26.0, 27.22294263741497, 0.6742537293696985, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.32, 0.7861878453038674, 0.6666666666666666, 0.7685785531179142, 0.7247512431232329, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2694389], dtype=float32), -0.8405465]. 
=============================================
[2019-04-04 16:49:29,318] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[86.15526]
 [86.02419]
 [85.88025]
 [85.70726]
 [85.56967]], R is [[86.42491913]
 [86.56066895]
 [86.69506073]
 [86.82810974]
 [86.95983124]].
[2019-04-04 16:49:29,787] A3C_AGENT_WORKER-Thread-15 INFO:Local step 304500, global step 4871815: loss 1.2010
[2019-04-04 16:49:29,788] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 304500, global step 4871815: learning rate 0.0000
[2019-04-04 16:49:31,356] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9542704e-08 1.5701709e-09 6.0072634e-14 2.9416004e-13 1.0000000e+00
 4.6697450e-09 2.7781581e-14], sum to 1.0000
[2019-04-04 16:49:31,358] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1746
[2019-04-04 16:49:31,387] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 39.33333333333334, 0.0, 0.0, 26.0, 25.93131338103092, 0.5701738103436694, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4139400.0000, 
sim time next is 4140000.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 26.0, 25.89865916672775, 0.5579851871939586, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6582215972273125, 0.6859950623979861, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4462635], dtype=float32), -0.19270743]. 
=============================================
[2019-04-04 16:49:31,401] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.44184 ]
 [77.564285]
 [77.81877 ]
 [78.165115]
 [78.613594]], R is [[77.61600494]
 [77.83984375]
 [78.06144714]
 [78.28083038]
 [78.49802399]].
[2019-04-04 16:49:33,033] A3C_AGENT_WORKER-Thread-17 INFO:Local step 305000, global step 4873619: loss 0.0378
[2019-04-04 16:49:33,035] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 305000, global step 4873619: learning rate 0.0000
[2019-04-04 16:49:33,129] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5349541e-08 1.6680369e-09 4.4725115e-14 2.4642487e-13 1.0000000e+00
 1.7106412e-09 4.2861690e-14], sum to 1.0000
[2019-04-04 16:49:33,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0321
[2019-04-04 16:49:33,161] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.78805199118534, 0.2318685166996096, 0.0, 1.0, 39293.23576882304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4855800.0000, 
sim time next is 4856400.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.81219189236309, 0.2260503316753448, 0.0, 1.0, 39359.91731054642], 
processed observation next is [0.0, 0.21739130434782608, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.567682657696924, 0.5753501105584483, 0.0, 1.0, 0.18742817766926867], 
reward next is 0.8126, 
noisyNet noise sample is [array([-0.7433379], dtype=float32), 0.9891668]. 
=============================================
[2019-04-04 16:49:33,216] A3C_AGENT_WORKER-Thread-10 INFO:Local step 304500, global step 4873712: loss 1.1634
[2019-04-04 16:49:33,216] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 304500, global step 4873712: learning rate 0.0000
[2019-04-04 16:49:33,602] A3C_AGENT_WORKER-Thread-4 INFO:Local step 304500, global step 4873922: loss 1.1701
[2019-04-04 16:49:33,607] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 304500, global step 4873922: learning rate 0.0000
[2019-04-04 16:49:34,015] A3C_AGENT_WORKER-Thread-3 INFO:Local step 304500, global step 4874164: loss 1.1688
[2019-04-04 16:49:34,016] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 304500, global step 4874165: learning rate 0.0000
[2019-04-04 16:49:34,071] A3C_AGENT_WORKER-Thread-16 INFO:Local step 304500, global step 4874191: loss 1.1777
[2019-04-04 16:49:34,072] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 304500, global step 4874191: learning rate 0.0000
[2019-04-04 16:49:35,058] A3C_AGENT_WORKER-Thread-2 INFO:Local step 305500, global step 4874729: loss 0.0516
[2019-04-04 16:49:35,060] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 305500, global step 4874730: learning rate 0.0000
[2019-04-04 16:49:35,299] A3C_AGENT_WORKER-Thread-20 INFO:Local step 304500, global step 4874870: loss 1.1753
[2019-04-04 16:49:35,304] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 304500, global step 4874875: learning rate 0.0000
[2019-04-04 16:49:37,035] A3C_AGENT_WORKER-Thread-12 INFO:Local step 304500, global step 4875829: loss 1.1609
[2019-04-04 16:49:37,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 304500, global step 4875829: learning rate 0.0000
[2019-04-04 16:49:37,420] A3C_AGENT_WORKER-Thread-6 INFO:Local step 304500, global step 4876043: loss 1.1772
[2019-04-04 16:49:37,420] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 304500, global step 4876043: learning rate 0.0000
[2019-04-04 16:49:38,612] A3C_AGENT_WORKER-Thread-19 INFO:Local step 304500, global step 4876724: loss 1.1525
[2019-04-04 16:49:38,616] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 304500, global step 4876724: learning rate 0.0000
[2019-04-04 16:49:39,504] A3C_AGENT_WORKER-Thread-14 INFO:Local step 304500, global step 4877263: loss 1.1636
[2019-04-04 16:49:39,507] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 304500, global step 4877263: learning rate 0.0000
[2019-04-04 16:49:40,059] A3C_AGENT_WORKER-Thread-13 INFO:Local step 305500, global step 4877604: loss 0.0510
[2019-04-04 16:49:40,059] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 305500, global step 4877604: learning rate 0.0000
[2019-04-04 16:49:41,412] A3C_AGENT_WORKER-Thread-5 INFO:Local step 304500, global step 4878354: loss 1.1657
[2019-04-04 16:49:41,412] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 304500, global step 4878354: learning rate 0.0000
[2019-04-04 16:49:42,527] A3C_AGENT_WORKER-Thread-18 INFO:Local step 304500, global step 4878972: loss 1.1848
[2019-04-04 16:49:42,528] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 304500, global step 4878972: learning rate 0.0000
[2019-04-04 16:49:42,639] A3C_AGENT_WORKER-Thread-11 INFO:Local step 305500, global step 4879037: loss 0.0437
[2019-04-04 16:49:42,641] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 305500, global step 4879037: learning rate 0.0000
[2019-04-04 16:49:43,345] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9622163e-09 8.4433216e-10 1.0210272e-15 7.5164802e-14 1.0000000e+00
 6.2561616e-11 2.5582110e-15], sum to 1.0000
[2019-04-04 16:49:43,348] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7508
[2019-04-04 16:49:43,375] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.21940184572556, 0.3920462016441184, 0.0, 1.0, 40635.92982631167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4516800.0000, 
sim time next is 4517400.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.26261910436092, 0.3932946228505917, 0.0, 1.0, 40542.20440366853], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6052182586967433, 0.6310982076168639, 0.0, 1.0, 0.19305811620794538], 
reward next is 0.8069, 
noisyNet noise sample is [array([0.8977481], dtype=float32), 0.62742144]. 
=============================================
[2019-04-04 16:49:43,897] A3C_AGENT_WORKER-Thread-15 INFO:Local step 305000, global step 4879734: loss 0.0368
[2019-04-04 16:49:43,898] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 305000, global step 4879734: learning rate 0.0000
[2019-04-04 16:49:46,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:49:46,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:49:46,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run37
[2019-04-04 16:49:46,776] A3C_AGENT_WORKER-Thread-10 INFO:Local step 305000, global step 4881288: loss 0.0383
[2019-04-04 16:49:46,777] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 305000, global step 4881288: learning rate 0.0000
[2019-04-04 16:49:47,286] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.9285196e-11 1.3053311e-11 2.5966414e-17 1.3923608e-16 1.0000000e+00
 5.2460332e-12 1.5004835e-17], sum to 1.0000
[2019-04-04 16:49:47,287] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4323
[2019-04-04 16:49:47,295] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.13333333333333, 48.0, 0.0, 0.0, 26.0, 26.83811642268376, 0.9216668631310577, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4387200.0000, 
sim time next is 4387800.0000, 
raw observation next is [12.06666666666667, 49.0, 0.0, 0.0, 26.0, 27.14986332852823, 0.9328009684982668, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7968605724838413, 0.49, 0.0, 0.0, 0.6666666666666666, 0.7624886107106859, 0.8109336561660889, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6242927], dtype=float32), 1.0141976]. 
=============================================
[2019-04-04 16:49:47,708] A3C_AGENT_WORKER-Thread-4 INFO:Local step 305000, global step 4881797: loss 0.0334
[2019-04-04 16:49:47,709] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 305000, global step 4881797: learning rate 0.0000
[2019-04-04 16:49:47,842] A3C_AGENT_WORKER-Thread-3 INFO:Local step 305000, global step 4881880: loss 0.0389
[2019-04-04 16:49:47,843] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 305000, global step 4881880: learning rate 0.0000
[2019-04-04 16:49:47,865] A3C_AGENT_WORKER-Thread-16 INFO:Local step 305000, global step 4881887: loss 0.0348
[2019-04-04 16:49:47,867] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 305000, global step 4881888: learning rate 0.0000
[2019-04-04 16:49:48,827] A3C_AGENT_WORKER-Thread-17 INFO:Local step 305500, global step 4882401: loss 0.0353
[2019-04-04 16:49:48,829] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 305500, global step 4882401: learning rate 0.0000
[2019-04-04 16:49:48,832] A3C_AGENT_WORKER-Thread-20 INFO:Local step 305000, global step 4882401: loss 0.0387
[2019-04-04 16:49:48,832] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 305000, global step 4882402: learning rate 0.0000
[2019-04-04 16:49:50,103] A3C_AGENT_WORKER-Thread-12 INFO:Local step 305000, global step 4883046: loss 0.0409
[2019-04-04 16:49:50,104] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 305000, global step 4883046: learning rate 0.0000
[2019-04-04 16:49:50,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:49:50,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:49:50,639] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run37
[2019-04-04 16:49:51,239] A3C_AGENT_WORKER-Thread-6 INFO:Local step 305000, global step 4883654: loss 0.0400
[2019-04-04 16:49:51,244] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 305000, global step 4883657: learning rate 0.0000
[2019-04-04 16:49:52,202] A3C_AGENT_WORKER-Thread-19 INFO:Local step 305000, global step 4884146: loss 0.0400
[2019-04-04 16:49:52,204] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 305000, global step 4884146: learning rate 0.0000
[2019-04-04 16:49:52,660] A3C_AGENT_WORKER-Thread-14 INFO:Local step 305000, global step 4884399: loss 0.0374
[2019-04-04 16:49:52,663] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 305000, global step 4884400: learning rate 0.0000
[2019-04-04 16:49:53,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:49:53,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:49:53,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run37
[2019-04-04 16:49:54,047] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1785607e-09 4.7520732e-10 2.0800249e-15 2.9763870e-14 1.0000000e+00
 8.2813673e-11 3.6025124e-15], sum to 1.0000
[2019-04-04 16:49:54,049] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2316
[2019-04-04 16:49:54,066] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 26.0, 25.63348147529232, 0.4876797123600943, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4675200.0000, 
sim time next is 4675800.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 26.0, 25.61653354408977, 0.4855421202459424, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6347111286741475, 0.6618473734153142, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.949056], dtype=float32), -0.29324692]. 
=============================================
[2019-04-04 16:49:54,552] A3C_AGENT_WORKER-Thread-5 INFO:Local step 305000, global step 4885251: loss 0.0338
[2019-04-04 16:49:54,553] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 305000, global step 4885251: learning rate 0.0000
[2019-04-04 16:49:56,070] A3C_AGENT_WORKER-Thread-18 INFO:Local step 305000, global step 4885944: loss 0.0325
[2019-04-04 16:49:56,071] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 305000, global step 4885944: learning rate 0.0000
[2019-04-04 16:49:58,942] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9520530e-09 5.7412419e-10 1.4593772e-15 1.4688476e-14 1.0000000e+00
 1.6787378e-10 3.4011365e-15], sum to 1.0000
[2019-04-04 16:49:58,945] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0365
[2019-04-04 16:49:58,957] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 36.5, 87.0, 608.3333333333334, 26.0, 25.19802367992637, 0.4333867766649621, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4810200.0000, 
sim time next is 4810800.0000, 
raw observation next is [3.0, 36.0, 84.5, 578.6666666666666, 26.0, 25.19793143328351, 0.4325813444918387, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.36, 0.2816666666666667, 0.6394106813996316, 0.6666666666666666, 0.5998276194402926, 0.6441937814972796, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6078521], dtype=float32), 1.896817]. 
=============================================
[2019-04-04 16:49:59,132] A3C_AGENT_WORKER-Thread-15 INFO:Local step 305500, global step 4887404: loss 0.0483
[2019-04-04 16:49:59,133] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 305500, global step 4887404: learning rate 0.0000
[2019-04-04 16:49:59,138] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:49:59,138] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:49:59,167] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run37
[2019-04-04 16:49:59,960] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7604493e-09 8.5382712e-10 1.5700972e-15 2.9181188e-13 1.0000000e+00
 1.6145743e-10 4.8119489e-15], sum to 1.0000
[2019-04-04 16:49:59,960] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9495
[2019-04-04 16:49:59,976] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1666666666666666, 50.33333333333333, 0.0, 0.0, 26.0, 25.50495902081402, 0.4105167446581395, 0.0, 1.0, 30903.77747061111], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4827000.0000, 
sim time next is 4827600.0000, 
raw observation next is [0.0, 51.0, 0.0, 0.0, 26.0, 25.4648718879587, 0.4045498325654442, 0.0, 1.0, 55767.85284182994], 
processed observation next is [0.0, 0.9130434782608695, 0.46260387811634357, 0.51, 0.0, 0.0, 0.6666666666666666, 0.6220726573298917, 0.6348499441884814, 0.0, 1.0, 0.26556120400871397], 
reward next is 0.7344, 
noisyNet noise sample is [array([-1.8443849], dtype=float32), 0.528563]. 
=============================================
[2019-04-04 16:50:01,204] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2040784e-08 2.5121045e-09 1.4086031e-14 6.8043466e-13 1.0000000e+00
 9.1051316e-10 1.8653129e-14], sum to 1.0000
[2019-04-04 16:50:01,208] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6187
[2019-04-04 16:50:01,222] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.54940864293465, 0.3473805058519467, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4932600.0000, 
sim time next is 4933200.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.57961705036677, 0.3328556818694581, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6316347541972309, 0.6109518939564861, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39652652], dtype=float32), 1.4592131]. 
=============================================
[2019-04-04 16:50:01,491] A3C_AGENT_WORKER-Thread-10 INFO:Local step 305500, global step 4888535: loss 0.0472
[2019-04-04 16:50:01,491] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 305500, global step 4888535: learning rate 0.0000
[2019-04-04 16:50:02,903] A3C_AGENT_WORKER-Thread-4 INFO:Local step 305500, global step 4889112: loss 0.0424
[2019-04-04 16:50:02,905] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 305500, global step 4889112: learning rate 0.0000
[2019-04-04 16:50:03,091] A3C_AGENT_WORKER-Thread-16 INFO:Local step 305500, global step 4889203: loss 0.0416
[2019-04-04 16:50:03,093] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 305500, global step 4889203: learning rate 0.0000
[2019-04-04 16:50:03,325] A3C_AGENT_WORKER-Thread-3 INFO:Local step 305500, global step 4889317: loss 0.0451
[2019-04-04 16:50:03,331] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 305500, global step 4889317: learning rate 0.0000
[2019-04-04 16:50:04,239] A3C_AGENT_WORKER-Thread-20 INFO:Local step 305500, global step 4889767: loss 0.0457
[2019-04-04 16:50:04,240] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 305500, global step 4889767: learning rate 0.0000
[2019-04-04 16:50:05,074] A3C_AGENT_WORKER-Thread-12 INFO:Local step 305500, global step 4890196: loss 0.0476
[2019-04-04 16:50:05,075] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 305500, global step 4890196: learning rate 0.0000
[2019-04-04 16:50:06,702] A3C_AGENT_WORKER-Thread-6 INFO:Local step 305500, global step 4890994: loss 0.0438
[2019-04-04 16:50:06,703] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 305500, global step 4890995: learning rate 0.0000
[2019-04-04 16:50:06,765] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0786290e-09 7.8107770e-10 4.0855909e-16 1.8441442e-14 1.0000000e+00
 2.9624886e-10 1.1816143e-15], sum to 1.0000
[2019-04-04 16:50:06,765] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2860
[2019-04-04 16:50:06,782] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 30.0, 112.5, 760.0, 26.0, 26.43430883727958, 0.5379417559043583, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4960800.0000, 
sim time next is 4961400.0000, 
raw observation next is [1.333333333333333, 29.83333333333333, 114.0, 774.3333333333333, 26.0, 26.51137167673316, 0.5565829901663069, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4995383194829178, 0.2983333333333333, 0.38, 0.8556169429097605, 0.6666666666666666, 0.7092809730610966, 0.6855276633887689, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4598496], dtype=float32), 1.4266256]. 
=============================================
[2019-04-04 16:50:07,237] A3C_AGENT_WORKER-Thread-19 INFO:Local step 305500, global step 4891287: loss 0.0451
[2019-04-04 16:50:07,239] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 305500, global step 4891287: learning rate 0.0000
[2019-04-04 16:50:08,175] A3C_AGENT_WORKER-Thread-14 INFO:Local step 305500, global step 4891765: loss 0.0466
[2019-04-04 16:50:08,177] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 305500, global step 4891766: learning rate 0.0000
[2019-04-04 16:50:08,489] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.05293754e-09 2.53264132e-10 1.10415844e-14 3.67830698e-14
 1.00000000e+00 1.08946664e-10 1.00611886e-14], sum to 1.0000
[2019-04-04 16:50:08,489] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0129
[2019-04-04 16:50:08,502] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 34.5, 0.0, 0.0, 26.0, 25.45202965127808, 0.4859393109704604, 0.0, 1.0, 98311.23302522388], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5007000.0000, 
sim time next is 5007600.0000, 
raw observation next is [3.0, 34.0, 0.0, 0.0, 26.0, 25.4702141387761, 0.4962546240541746, 0.0, 1.0, 50400.99411265378], 
processed observation next is [1.0, 1.0, 0.5457063711911359, 0.34, 0.0, 0.0, 0.6666666666666666, 0.6225178448980083, 0.6654182080180582, 0.0, 1.0, 0.2400047338697799], 
reward next is 0.7600, 
noisyNet noise sample is [array([-0.9738113], dtype=float32), -0.8348266]. 
=============================================
[2019-04-04 16:50:09,375] A3C_AGENT_WORKER-Thread-5 INFO:Local step 305500, global step 4892438: loss 0.0461
[2019-04-04 16:50:09,380] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 305500, global step 4892438: learning rate 0.0000
[2019-04-04 16:50:09,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:50:09,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:09,610] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run37
[2019-04-04 16:50:10,623] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4026253e-09 5.7217037e-11 2.8653431e-16 3.0910814e-15 1.0000000e+00
 3.2762675e-11 1.9970076e-16], sum to 1.0000
[2019-04-04 16:50:10,624] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8737
[2019-04-04 16:50:10,646] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.4666666666666667, 95.83333333333333, 0.0, 0.0, 26.0, 24.53184807841787, 0.1954342517823075, 0.0, 1.0, 40262.71286686911], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 79800.0000, 
sim time next is 80400.0000, 
raw observation next is [0.4333333333333333, 95.66666666666666, 0.0, 0.0, 26.0, 24.51256389244658, 0.1915945943803838, 0.0, 1.0, 40207.55410797378], 
processed observation next is [0.0, 0.9565217391304348, 0.4746075715604802, 0.9566666666666666, 0.0, 0.0, 0.6666666666666666, 0.5427136577038816, 0.5638648647934613, 0.0, 1.0, 0.19146454337130372], 
reward next is 0.8085, 
noisyNet noise sample is [array([-0.44879693], dtype=float32), -0.03819927]. 
=============================================
[2019-04-04 16:50:11,010] A3C_AGENT_WORKER-Thread-18 INFO:Local step 305500, global step 4893172: loss 0.0465
[2019-04-04 16:50:11,013] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 305500, global step 4893173: learning rate 0.0000
[2019-04-04 16:50:11,480] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1587967e-11 8.3884158e-12 5.2719592e-17 1.3802612e-16 1.0000000e+00
 8.3832013e-12 1.8007553e-17], sum to 1.0000
[2019-04-04 16:50:11,480] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7833
[2019-04-04 16:50:11,513] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.5, 25.5, 92.0, 774.0, 26.0, 26.79393048728001, 0.811827658382259, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4980600.0000, 
sim time next is 4981200.0000, 
raw observation next is [8.666666666666668, 25.33333333333333, 88.66666666666667, 751.8333333333333, 26.0, 27.24008110892287, 0.8550570845494335, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7026777469990768, 0.2533333333333333, 0.29555555555555557, 0.8307550644567219, 0.6666666666666666, 0.7700067590769057, 0.7850190281831445, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00897855], dtype=float32), 0.022489138]. 
=============================================
[2019-04-04 16:50:11,542] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:50:11,543] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:11,587] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run37
[2019-04-04 16:50:12,445] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2139792e-09 1.5450988e-10 1.0987047e-14 1.8561351e-14 1.0000000e+00
 2.3272852e-10 7.4848880e-15], sum to 1.0000
[2019-04-04 16:50:12,448] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1712
[2019-04-04 16:50:12,462] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 25.80009934946773, 0.5672273438275154, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5004000.0000, 
sim time next is 5004600.0000, 
raw observation next is [3.0, 36.5, 0.0, 0.0, 26.0, 25.79380222322046, 0.5085262268064571, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.365, 0.0, 0.0, 0.6666666666666666, 0.6494835186017051, 0.669508742268819, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.488726], dtype=float32), -0.041335702]. 
=============================================
[2019-04-04 16:50:13,279] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:50:13,279] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:13,287] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run37
[2019-04-04 16:50:13,371] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:50:13,371] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:13,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run37
[2019-04-04 16:50:13,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:50:13,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:13,528] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run37
[2019-04-04 16:50:14,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:50:14,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:14,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run37
[2019-04-04 16:50:14,681] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.1701876e-09 1.0127330e-09 5.6721544e-15 1.5190797e-14 1.0000000e+00
 1.3837429e-09 1.6743080e-15], sum to 1.0000
[2019-04-04 16:50:14,682] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5939
[2019-04-04 16:50:14,731] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.333333333333333, 65.0, 65.33333333333334, 173.0, 26.0, 25.584944168691, 0.4155815591987598, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5038800.0000, 
sim time next is 5039400.0000, 
raw observation next is [-2.166666666666667, 65.0, 71.66666666666666, 245.0, 26.0, 25.58886088914432, 0.4229868723969737, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4025854108956602, 0.65, 0.23888888888888885, 0.27071823204419887, 0.6666666666666666, 0.6324050740953601, 0.6409956241323246, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19891971], dtype=float32), 0.30895507]. 
=============================================
[2019-04-04 16:50:15,280] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:50:15,280] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:15,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run37
[2019-04-04 16:50:16,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:50:16,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:16,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run37
[2019-04-04 16:50:17,702] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:50:17,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:17,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run37
[2019-04-04 16:50:18,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:50:18,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:18,834] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run37
[2019-04-04 16:50:19,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:50:19,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:19,888] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run37
[2019-04-04 16:50:20,493] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7486090e-05 4.6830723e-05 1.2910007e-08 1.5828061e-07 9.9989426e-01
 2.1249441e-05 2.2320920e-08], sum to 1.0000
[2019-04-04 16:50:20,494] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6378
[2019-04-04 16:50:20,548] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.199999999999999, 96.0, 0.0, 0.0, 23.0, 20.19772788719058, -0.8108784162640287, 0.0, 1.0, 44313.93123115478], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 6600.0000, 
sim time next is 7200.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 23.5, 20.26141812905402, -0.7999935389169884, 0.0, 1.0, 43963.97662851137], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 0.4583333333333333, 0.1884515107545018, 0.2333354870276705, 0.0, 1.0, 0.20935226965957796], 
reward next is 0.7906, 
noisyNet noise sample is [array([-1.7983837], dtype=float32), -0.11550718]. 
=============================================
[2019-04-04 16:50:22,764] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 16:50:22,764] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:22,767] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run37
[2019-04-04 16:50:34,841] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-04 16:50:34,844] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 16:50:34,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:34,857] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 16:50:34,857] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:34,857] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 16:50:34,858] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:50:34,862] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run50
[2019-04-04 16:50:34,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run50
[2019-04-04 16:50:34,903] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run50
[2019-04-04 16:51:24,594] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.18510182], dtype=float32), 0.24929453]
[2019-04-04 16:51:24,594] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.9, 86.0, 0.0, 0.0, 26.0, 25.74334967694442, 0.6085572286006818, 0.0, 1.0, 0.0]
[2019-04-04 16:51:24,594] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 16:51:24,594] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.0109835e-10 6.7238617e-11 1.3968935e-16 2.2070915e-15 1.0000000e+00
 4.5007068e-11 2.6550989e-16], sampled 0.35922359968349915
[2019-04-04 16:52:16,998] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.9534 239829784.3799 1604.9291
[2019-04-04 16:52:17,646] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18510182], dtype=float32), 0.24929453]
[2019-04-04 16:52:17,647] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.0, 39.66666666666666, 0.0, 0.0, 26.0, 25.06535661437523, 0.3343243286347033, 0.0, 1.0, 40684.96492953465]
[2019-04-04 16:52:17,647] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 16:52:17,648] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.14306635e-08 2.09707718e-09 6.12610819e-14 3.63820221e-13
 1.00000000e+00 1.49620283e-09 9.96678190e-14], sampled 0.2641602713967498
[2019-04-04 16:52:35,229] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.4981 263445398.1310 1557.1271
[2019-04-04 16:52:37,849] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 16:52:38,873] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 4900000, evaluation results [4900000.0, 7241.498104138273, 263445398.13096312, 1557.1271045198032, 7353.953407714569, 239829784.37993747, 1604.9291365554704, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 16:52:45,698] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0401611e-10 1.6368001e-10 3.5200367e-17 1.6887856e-15 1.0000000e+00
 5.1449806e-11 9.1208117e-17], sum to 1.0000
[2019-04-04 16:52:45,699] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8770
[2019-04-04 16:52:45,718] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.05, 87.0, 0.0, 0.0, 26.0, 24.84630544328738, 0.2332563628983642, 0.0, 1.0, 39710.04223858668], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 527400.0000, 
sim time next is 528000.0000, 
raw observation next is [3.966666666666667, 86.66666666666666, 0.0, 0.0, 26.0, 24.82465015029335, 0.2302493746450535, 0.0, 1.0, 39739.57616676564], 
processed observation next is [0.0, 0.08695652173913043, 0.5724838411819021, 0.8666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5687208458577793, 0.5767497915483512, 0.0, 1.0, 0.1892360769845983], 
reward next is 0.8108, 
noisyNet noise sample is [array([0.8476637], dtype=float32), -1.3084004]. 
=============================================
[2019-04-04 16:52:45,725] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[89.2374  ]
 [88.99213 ]
 [88.93244 ]
 [88.892426]
 [88.687195]], R is [[89.04259491]
 [88.96307373]
 [88.88448334]
 [88.80680847]
 [88.7300415 ]].
[2019-04-04 16:52:47,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5813303e-08 1.0939399e-09 1.0915486e-14 1.3068362e-14 1.0000000e+00
 5.3933669e-10 2.6761388e-15], sum to 1.0000
[2019-04-04 16:52:47,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9531
[2019-04-04 16:52:47,675] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.9, 59.5, 76.33333333333333, 0.0, 26.0, 24.78994611051256, 0.2964760848734195, 1.0, 1.0, 198943.2685379057], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 227400.0000, 
sim time next is 228000.0000, 
raw observation next is [-3.0, 60.0, 66.16666666666667, 0.0, 26.0, 25.2534299251173, 0.3364882490410864, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3795013850415513, 0.6, 0.22055555555555556, 0.0, 0.6666666666666666, 0.604452493759775, 0.6121627496803621, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6813816], dtype=float32), -1.5230621]. 
=============================================
[2019-04-04 16:52:47,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[82.246414]
 [81.6572  ]
 [82.10048 ]
 [82.47218 ]
 [82.79771 ]], R is [[81.92198944]
 [81.1554184 ]
 [81.34386444]
 [81.53042603]
 [81.71512604]].
[2019-04-04 16:52:49,919] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6067259e-09 3.6749204e-10 1.6387014e-15 6.9948836e-15 1.0000000e+00
 1.3402203e-10 8.6872557e-16], sum to 1.0000
[2019-04-04 16:52:49,920] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3610
[2019-04-04 16:52:49,967] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 68.5, 153.0, 0.0, 26.0, 25.27855284215401, 0.2277409304185019, 1.0, 1.0, 25377.78152282604], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 214200.0000, 
sim time next is 214800.0000, 
raw observation next is [-5.4, 67.33333333333333, 149.0, 0.0, 26.0, 25.29141196549566, 0.2285011865217718, 1.0, 1.0, 24851.81890387193], 
processed observation next is [1.0, 0.4782608695652174, 0.31301939058171746, 0.6733333333333333, 0.49666666666666665, 0.0, 0.6666666666666666, 0.6076176637913049, 0.576167062173924, 1.0, 1.0, 0.11834199478034252], 
reward next is 0.8817, 
noisyNet noise sample is [array([0.46138301], dtype=float32), 2.2214944]. 
=============================================
[2019-04-04 16:52:51,279] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.8568906e-09 5.8934668e-10 3.5480560e-14 3.8320651e-13 1.0000000e+00
 1.6267108e-09 1.1681294e-14], sum to 1.0000
[2019-04-04 16:52:51,279] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0813
[2019-04-04 16:52:51,352] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.38333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 25.2424814986382, 0.3430522021490092, 1.0, 1.0, 54637.56965533009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 328200.0000, 
sim time next is 328800.0000, 
raw observation next is [-12.46666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.25073407147145, 0.3357487041749059, 1.0, 1.0, 57003.38167839512], 
processed observation next is [1.0, 0.8260869565217391, 0.11726685133887339, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6042278392892874, 0.6119162347249686, 1.0, 1.0, 0.2714446746590244], 
reward next is 0.7286, 
noisyNet noise sample is [array([-1.0347098], dtype=float32), -0.076759145]. 
=============================================
[2019-04-04 16:53:02,815] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6139146e-09 9.1055087e-11 1.1330749e-15 1.3587055e-14 1.0000000e+00
 2.5523736e-10 3.1918804e-15], sum to 1.0000
[2019-04-04 16:53:02,822] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2488
[2019-04-04 16:53:02,845] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.3, 68.33333333333333, 0.0, 0.0, 26.0, 24.95982306431787, 0.2097840319022147, 0.0, 1.0, 42455.14576598249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 679800.0000, 
sim time next is 680400.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.95484227560024, 0.2056418771669031, 0.0, 1.0, 42391.69879571903], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5795701896333533, 0.5685472923889677, 0.0, 1.0, 0.2018652323605668], 
reward next is 0.7981, 
noisyNet noise sample is [array([-0.3243496], dtype=float32), -0.61874056]. 
=============================================
[2019-04-04 16:53:05,515] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6176716e-09 1.2308361e-10 8.5473539e-16 1.9571741e-14 1.0000000e+00
 1.0608008e-09 4.4848700e-15], sum to 1.0000
[2019-04-04 16:53:05,515] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6891
[2019-04-04 16:53:05,558] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 86.0, 14.5, 0.0, 26.0, 25.1967750251613, 0.3550552746588541, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 838800.0000, 
sim time next is 839400.0000, 
raw observation next is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 25.59400738424426, 0.3830410424894608, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6328339486870217, 0.6276803474964869, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9343168], dtype=float32), 0.23422682]. 
=============================================
[2019-04-04 16:53:06,435] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.0508667e-10 9.8861239e-11 2.9412256e-16 1.3664059e-14 1.0000000e+00
 1.6809158e-11 1.1968113e-15], sum to 1.0000
[2019-04-04 16:53:06,435] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2636
[2019-04-04 16:53:06,453] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.216666666666667, 87.66666666666667, 0.0, 0.0, 26.0, 24.89696569251651, 0.2399983508712857, 0.0, 1.0, 39653.4434080095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 526200.0000, 
sim time next is 526800.0000, 
raw observation next is [4.133333333333333, 87.33333333333334, 0.0, 0.0, 26.0, 24.8704347297067, 0.2365475800813415, 0.0, 1.0, 39681.04008186322], 
processed observation next is [0.0, 0.08695652173913043, 0.577100646352724, 0.8733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5725362274755584, 0.5788491933604472, 0.0, 1.0, 0.18895733372315818], 
reward next is 0.8110, 
noisyNet noise sample is [array([-1.0316778], dtype=float32), -0.43809277]. 
=============================================
[2019-04-04 16:53:11,616] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.0766468e-11 1.0842339e-11 1.4676272e-17 2.9272119e-16 1.0000000e+00
 2.7273847e-12 2.6079382e-17], sum to 1.0000
[2019-04-04 16:53:11,619] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1963
[2019-04-04 16:53:11,668] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.2, 89.66666666666667, 125.6666666666667, 103.1666666666667, 26.0, 25.03746571155731, 0.2794296629935397, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 552000.0000, 
sim time next is 552600.0000, 
raw observation next is [-0.3, 89.0, 144.0, 103.0, 26.0, 24.97624032362866, 0.2680202277206807, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.4542936288088643, 0.89, 0.48, 0.1138121546961326, 0.6666666666666666, 0.5813533603023883, 0.5893400759068935, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7177433], dtype=float32), 0.12339992]. 
=============================================
[2019-04-04 16:53:13,237] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.27020291e-09 1.16142686e-10 1.78193815e-15 1.17797397e-14
 1.00000000e+00 5.90059460e-11 3.48355916e-15], sum to 1.0000
[2019-04-04 16:53:13,238] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2382
[2019-04-04 16:53:13,258] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 26.0, 25.0371190713614, 0.2803696009345246, 0.0, 1.0, 43005.45613726814], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 594600.0000, 
sim time next is 595200.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 26.0, 25.00551628153359, 0.2778600155051025, 0.0, 1.0, 43015.25195883863], 
processed observation next is [0.0, 0.9130434782608695, 0.38504155124653744, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5837930234611326, 0.5926200051683675, 0.0, 1.0, 0.2048345331373268], 
reward next is 0.7952, 
noisyNet noise sample is [array([0.91359174], dtype=float32), 0.22906356]. 
=============================================
[2019-04-04 16:53:19,504] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.9046381e-11 4.7516518e-12 6.7930214e-18 7.4707035e-17 1.0000000e+00
 6.5856213e-13 1.4209110e-17], sum to 1.0000
[2019-04-04 16:53:19,504] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0335
[2019-04-04 16:53:19,525] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.51666666666667, 80.5, 0.0, 0.0, 26.0, 25.65646070664886, 0.6139388185114912, 0.0, 1.0, 18727.06860564762], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1147800.0000, 
sim time next is 1148400.0000, 
raw observation next is [12.7, 80.0, 0.0, 0.0, 26.0, 25.66621642022061, 0.614228779749405, 0.0, 1.0, 18726.33171693757], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6388513683517175, 0.7047429265831351, 0.0, 1.0, 0.08917300817589319], 
reward next is 0.9108, 
noisyNet noise sample is [array([1.8642215], dtype=float32), -0.6502692]. 
=============================================
[2019-04-04 16:53:20,456] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5996847e-09 3.7302866e-10 6.7388020e-16 4.4512759e-15 1.0000000e+00
 1.4183005e-10 1.1160068e-14], sum to 1.0000
[2019-04-04 16:53:20,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1328
[2019-04-04 16:53:20,507] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.566666666666667, 59.66666666666667, 165.1666666666667, 86.83333333333333, 26.0, 24.9352900209223, 0.243467679387637, 0.0, 1.0, 19643.5449899724], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 654000.0000, 
sim time next is 654600.0000, 
raw observation next is [-1.383333333333333, 59.83333333333333, 148.3333333333333, 80.66666666666667, 26.0, 24.93133057819118, 0.2372236059236256, 0.0, 1.0, 26320.0379849977], 
processed observation next is [0.0, 0.5652173913043478, 0.4242843951985227, 0.5983333333333333, 0.4944444444444443, 0.08913443830570902, 0.6666666666666666, 0.5776108815159317, 0.5790745353078752, 0.0, 1.0, 0.12533351421427477], 
reward next is 0.8747, 
noisyNet noise sample is [array([0.9204983], dtype=float32), 0.87973845]. 
=============================================
[2019-04-04 16:53:24,541] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.8624769e-10 2.8564177e-11 6.0545313e-17 6.5704552e-16 1.0000000e+00
 6.9577205e-11 7.1230223e-17], sum to 1.0000
[2019-04-04 16:53:24,542] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7064
[2019-04-04 16:53:24,553] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 50.0, 110.0, 611.0, 26.0, 25.58458720462538, 0.3591352820367154, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 738000.0000, 
sim time next is 738600.0000, 
raw observation next is [0.5, 49.16666666666667, 103.0, 665.0, 26.0, 25.50997982553285, 0.3399926094800987, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.4916666666666667, 0.3433333333333333, 0.7348066298342542, 0.6666666666666666, 0.6258316521277374, 0.6133308698266996, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.3616301], dtype=float32), -0.72309816]. 
=============================================
[2019-04-04 16:53:24,566] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.1032781e-09 1.3257569e-09 2.8164176e-14 2.8063809e-13 1.0000000e+00
 7.4234019e-10 7.5195452e-15], sum to 1.0000
[2019-04-04 16:53:24,567] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6871
[2019-04-04 16:53:24,578] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 74.5, 0.0, 0.0, 26.0, 24.36319691245002, 0.07668685805588317, 0.0, 1.0, 41106.44771959598], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 697800.0000, 
sim time next is 698400.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.36276874551253, 0.07188058947164724, 0.0, 1.0, 41116.06863819484], 
processed observation next is [1.0, 0.08695652173913043, 0.368421052631579, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5302307287927107, 0.523960196490549, 0.0, 1.0, 0.19579080303902305], 
reward next is 0.8042, 
noisyNet noise sample is [array([1.1482881], dtype=float32), -1.4584044]. 
=============================================
[2019-04-04 16:53:28,304] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.1016583e-09 2.7416996e-10 3.7277450e-15 4.1515805e-14 1.0000000e+00
 1.8772807e-10 3.1080959e-15], sum to 1.0000
[2019-04-04 16:53:28,304] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4134
[2019-04-04 16:53:28,362] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 53.0, 0.0, 0.0, 26.0, 24.96188072300628, 0.2984225747999765, 1.0, 1.0, 111901.7845437173], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 759600.0000, 
sim time next is 760200.0000, 
raw observation next is [-4.083333333333333, 53.83333333333334, 0.0, 0.0, 26.0, 24.93234608259138, 0.3342023010484018, 1.0, 1.0, 99703.99381198852], 
processed observation next is [1.0, 0.8260869565217391, 0.34949215143120965, 0.5383333333333334, 0.0, 0.0, 0.6666666666666666, 0.577695506882615, 0.611400767016134, 1.0, 1.0, 0.474780922914231], 
reward next is 0.5252, 
noisyNet noise sample is [array([1.5711963], dtype=float32), -0.1527031]. 
=============================================
[2019-04-04 16:53:39,485] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9532119e-12 4.9009684e-13 5.5606700e-20 1.9838246e-18 1.0000000e+00
 5.3287453e-13 2.3657280e-19], sum to 1.0000
[2019-04-04 16:53:39,487] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0917
[2019-04-04 16:53:39,500] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.2, 83.0, 11.5, 38.0, 26.0, 25.92345413178455, 0.6239938193512852, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1065600.0000, 
sim time next is 1066200.0000, 
raw observation next is [12.2, 83.0, 15.0, 48.33333333333334, 26.0, 25.90865406040388, 0.624266605813307, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.05, 0.05340699815837938, 0.6666666666666666, 0.6590545050336566, 0.7080888686044357, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2182368], dtype=float32), 0.88802147]. 
=============================================
[2019-04-04 16:53:39,911] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0433192e-09 2.2729561e-11 1.8550432e-16 4.0962988e-15 1.0000000e+00
 1.1983083e-10 2.4358516e-16], sum to 1.0000
[2019-04-04 16:53:39,916] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8169
[2019-04-04 16:53:39,929] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.783333333333333, 75.66666666666667, 0.0, 0.0, 26.0, 26.1512481019935, 0.7012561283104626, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1547400.0000, 
sim time next is 1548000.0000, 
raw observation next is [6.6, 76.0, 0.0, 0.0, 26.0, 26.11655614117463, 0.691711749110877, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6454293628808865, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6763796784312192, 0.730570583036959, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6602815], dtype=float32), 0.4990595]. 
=============================================
[2019-04-04 16:53:39,940] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[90.102005]
 [90.24809 ]
 [90.25985 ]
 [90.36054 ]
 [90.405594]], R is [[90.06556702]
 [90.16490936]
 [90.26325989]
 [90.36062622]
 [90.45702362]].
[2019-04-04 16:53:42,088] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.8717575e-11 3.7469975e-12 2.3112191e-17 9.2671956e-18 1.0000000e+00
 4.0268331e-12 5.9289353e-18], sum to 1.0000
[2019-04-04 16:53:42,089] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5415
[2019-04-04 16:53:42,097] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.116666666666667, 62.16666666666666, 205.3333333333333, 141.6666666666667, 26.0, 26.83624655116817, 0.7474978257745075, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1594200.0000, 
sim time next is 1594800.0000, 
raw observation next is [9.4, 61.0, 208.0, 168.5, 26.0, 26.82862529288714, 0.7603851354744388, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7229916897506927, 0.61, 0.6933333333333334, 0.1861878453038674, 0.6666666666666666, 0.7357187744072616, 0.7534617118248129, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02949073], dtype=float32), -2.0170925]. 
=============================================
[2019-04-04 16:53:47,259] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3599836e-11 5.7805293e-12 1.7541961e-17 1.2520616e-17 1.0000000e+00
 6.5444498e-13 1.5786349e-18], sum to 1.0000
[2019-04-04 16:53:47,263] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0495
[2019-04-04 16:53:47,281] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.53333333333333, 100.0, 29.66666666666666, 0.0, 26.0, 24.61645791400099, 0.4215881463821622, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1267800.0000, 
sim time next is 1268400.0000, 
raw observation next is [13.26666666666667, 100.0, 24.33333333333333, 0.0, 26.0, 24.608498924621, 0.4212793030965259, 0.0, 1.0, 23140.05110726789], 
processed observation next is [0.0, 0.6956521739130435, 0.8301015697137583, 1.0, 0.08111111111111109, 0.0, 0.6666666666666666, 0.5507082437184166, 0.6404264343655086, 0.0, 1.0, 0.11019071955841853], 
reward next is 0.8898, 
noisyNet noise sample is [array([0.25024432], dtype=float32), 0.85969025]. 
=============================================
[2019-04-04 16:53:51,567] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8057489e-10 1.0915857e-11 2.1165214e-17 3.0647674e-16 1.0000000e+00
 8.2684996e-12 1.1583659e-16], sum to 1.0000
[2019-04-04 16:53:51,569] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3492
[2019-04-04 16:53:51,636] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 101.1666666666667, 0.0, 26.0, 25.36536837020926, 0.5594327992261577, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1689600.0000, 
sim time next is 1690200.0000, 
raw observation next is [1.1, 88.0, 100.0, 0.0, 26.0, 25.79266749349259, 0.5837769572639754, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3333333333333333, 0.0, 0.6666666666666666, 0.6493889577910492, 0.6945923190879918, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46416473], dtype=float32), 0.29496527]. 
=============================================
[2019-04-04 16:53:53,987] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.0512310e-09 2.1503106e-10 7.9333159e-16 4.6573759e-15 1.0000000e+00
 1.4069036e-10 8.1211084e-16], sum to 1.0000
[2019-04-04 16:53:53,989] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1631
[2019-04-04 16:53:54,010] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.04777937633989, 0.4066964396111923, 0.0, 1.0, 38593.18033524558], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1406400.0000, 
sim time next is 1407000.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.04716412143498, 0.4072069496338493, 0.0, 1.0, 38615.67168707189], 
processed observation next is [1.0, 0.2608695652173913, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5872636767862485, 0.6357356498779497, 0.0, 1.0, 0.18388415089081853], 
reward next is 0.8161, 
noisyNet noise sample is [array([0.18321855], dtype=float32), -0.2908471]. 
=============================================
[2019-04-04 16:53:54,014] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.69215 ]
 [86.75625 ]
 [86.806145]
 [86.86088 ]
 [86.88521 ]], R is [[86.58506775]
 [86.53543854]
 [86.48648071]
 [86.43824768]
 [86.39060211]].
[2019-04-04 16:53:59,240] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3904120e-10 1.6495865e-11 1.0790358e-17 3.2186929e-16 1.0000000e+00
 3.8195325e-11 2.5466120e-17], sum to 1.0000
[2019-04-04 16:53:59,241] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8370
[2019-04-04 16:53:59,247] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 95.0, 91.0, 0.0, 26.0, 25.65528751957361, 0.4649038261857179, 1.0, 1.0, 23591.68746964911], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1424400.0000, 
sim time next is 1425000.0000, 
raw observation next is [0.0, 95.0, 92.0, 0.0, 26.0, 25.63836990023896, 0.4707126527115766, 1.0, 1.0, 18680.56354635593], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.30666666666666664, 0.0, 0.6666666666666666, 0.6365308250199133, 0.6569042175705255, 1.0, 1.0, 0.08895506450645681], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.97294223], dtype=float32), -0.45815384]. 
=============================================
[2019-04-04 16:53:59,250] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[91.60257 ]
 [91.52073 ]
 [91.368965]
 [91.404335]
 [91.42771 ]], R is [[91.60960388]
 [91.58116913]
 [91.51561737]
 [91.60046387]
 [91.6844635 ]].
[2019-04-04 16:53:59,842] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.8649110e-10 8.8717408e-11 5.7572538e-17 6.1544556e-16 1.0000000e+00
 9.2041609e-12 1.0396525e-16], sum to 1.0000
[2019-04-04 16:53:59,844] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9416
[2019-04-04 16:53:59,848] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.55, 70.0, 0.0, 0.0, 26.0, 25.84649702722609, 0.6171255763057442, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1625400.0000, 
sim time next is 1626000.0000, 
raw observation next is [8.266666666666666, 71.33333333333333, 0.0, 0.0, 26.0, 25.72284101592792, 0.603830405359901, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.6915974145891044, 0.7133333333333333, 0.0, 0.0, 0.6666666666666666, 0.6435700846606599, 0.7012768017866337, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6441398], dtype=float32), -0.86875737]. 
=============================================
[2019-04-04 16:53:59,856] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[89.72835]
 [91.22956]
 [93.02295]
 [92.87263]
 [92.48949]], R is [[90.73018646]
 [90.82288361]
 [90.91465759]
 [91.00550842]
 [91.09545135]].
[2019-04-04 16:54:02,752] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.8942733e-11 8.9788819e-12 3.3864632e-18 6.1017382e-17 1.0000000e+00
 2.1876062e-12 2.9392711e-18], sum to 1.0000
[2019-04-04 16:54:02,753] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9710
[2019-04-04 16:54:02,780] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.833333333333334, 63.33333333333333, 202.6666666666667, 114.8333333333333, 26.0, 26.78505112875403, 0.739586141722235, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1593600.0000, 
sim time next is 1594200.0000, 
raw observation next is [9.116666666666667, 62.16666666666666, 205.3333333333333, 141.6666666666667, 26.0, 26.83624278007356, 0.7474969011430944, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7151431209602955, 0.6216666666666666, 0.6844444444444443, 0.15653775322283614, 0.6666666666666666, 0.7363535650061301, 0.7491656337143647, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22427645], dtype=float32), -0.45405757]. 
=============================================
[2019-04-04 16:54:07,301] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.5281030e-11 1.3669867e-11 4.1874407e-17 1.3280730e-16 1.0000000e+00
 1.2131292e-11 9.3637813e-18], sum to 1.0000
[2019-04-04 16:54:07,301] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1750
[2019-04-04 16:54:07,309] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.583333333333334, 65.16666666666666, 0.0, 0.0, 26.0, 26.24670260780302, 0.6895663290040058, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1623000.0000, 
sim time next is 1623600.0000, 
raw observation next is [9.4, 66.0, 0.0, 0.0, 26.0, 26.18656358282908, 0.6745704674234201, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7229916897506927, 0.66, 0.0, 0.0, 0.6666666666666666, 0.6822136319024233, 0.7248568224744734, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5932076], dtype=float32), 0.36368513]. 
=============================================
[2019-04-04 16:54:07,539] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9980461e-10 1.4547724e-11 7.7786468e-18 1.2490479e-15 1.0000000e+00
 4.6794856e-11 9.1347051e-17], sum to 1.0000
[2019-04-04 16:54:07,539] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7548
[2019-04-04 16:54:07,621] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 24.3805760332082, 0.3326486565884603, 1.0, 1.0, 196524.2986362654], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1708800.0000, 
sim time next is 1709400.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 23.44710134580161, 0.3057654375826873, 1.0, 1.0, 198046.2192543068], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.45392511215013415, 0.6019218125275624, 1.0, 1.0, 0.9430772345443181], 
reward next is 0.0569, 
noisyNet noise sample is [array([0.5639175], dtype=float32), -1.0352871]. 
=============================================
[2019-04-04 16:54:11,634] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0850351e-09 5.6527927e-10 3.9394621e-15 2.0454751e-14 1.0000000e+00
 5.0519605e-10 2.5718216e-15], sum to 1.0000
[2019-04-04 16:54:11,635] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1208
[2019-04-04 16:54:11,699] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.899999999999999, 84.66666666666666, 0.0, 0.0, 26.0, 25.24233004398855, 0.3442988616232233, 0.0, 1.0, 18709.84991394465], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2058000.0000, 
sim time next is 2058600.0000, 
raw observation next is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 25.00858497520108, 0.3376474554095827, 1.0, 1.0, 141966.2870273289], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5840487479334234, 0.6125491518031942, 1.0, 1.0, 0.6760299382253756], 
reward next is 0.3240, 
noisyNet noise sample is [array([1.2409439], dtype=float32), -1.665756]. 
=============================================
[2019-04-04 16:54:12,967] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.9326937e-08 1.0286228e-08 1.5312312e-13 3.7472954e-13 9.9999988e-01
 5.9690386e-09 3.3255205e-14], sum to 1.0000
[2019-04-04 16:54:12,967] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9087
[2019-04-04 16:54:13,078] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 75.0, 0.0, 0.0, 26.0, 23.65786992076836, 0.04094790425871047, 1.0, 1.0, 202416.1885477032], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2186400.0000, 
sim time next is 2187000.0000, 
raw observation next is [-5.6, 75.0, 0.0, 0.0, 26.0, 23.92722970608666, 0.1457399139072753, 1.0, 1.0, 202712.3337481411], 
processed observation next is [1.0, 0.30434782608695654, 0.30747922437673136, 0.75, 0.0, 0.0, 0.6666666666666666, 0.493935808840555, 0.5485799713024251, 1.0, 1.0, 0.9652968273721004], 
reward next is 0.0347, 
noisyNet noise sample is [array([2.6619146], dtype=float32), -0.9542877]. 
=============================================
[2019-04-04 16:54:13,080] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.6029  ]
 [72.75966 ]
 [72.82865 ]
 [72.90061 ]
 [72.983986]], R is [[77.53334045]
 [76.79412079]
 [76.82647705]
 [76.85855865]
 [76.89043427]].
[2019-04-04 16:54:19,361] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.8182636e-09 1.2217406e-09 1.3703888e-14 6.7504128e-14 1.0000000e+00
 1.4901934e-09 2.4764416e-14], sum to 1.0000
[2019-04-04 16:54:19,363] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0444
[2019-04-04 16:54:19,417] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.833333333333333, 85.0, 0.0, 0.0, 26.0, 25.03765187662503, 0.2482258983203802, 0.0, 1.0, 38938.56548606185], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1878000.0000, 
sim time next is 1878600.0000, 
raw observation next is [-4.916666666666667, 85.5, 0.0, 0.0, 26.0, 25.02547651960243, 0.2454483366196155, 0.0, 1.0, 50271.58034294361], 
processed observation next is [0.0, 0.7391304347826086, 0.32640812557710064, 0.855, 0.0, 0.0, 0.6666666666666666, 0.585456376633536, 0.5818161122065385, 0.0, 1.0, 0.239388477823541], 
reward next is 0.7606, 
noisyNet noise sample is [array([-0.0900121], dtype=float32), 0.9053523]. 
=============================================
[2019-04-04 16:54:23,786] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.4869976e-09 3.3681891e-10 1.5261572e-14 1.0511551e-13 1.0000000e+00
 8.6770480e-10 1.5709817e-14], sum to 1.0000
[2019-04-04 16:54:23,790] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0582
[2019-04-04 16:54:23,812] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.3, 78.5, 0.0, 0.0, 26.0, 24.44135429479079, 0.1717425219312535, 0.0, 1.0, 45711.19108194017], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1816200.0000, 
sim time next is 1816800.0000, 
raw observation next is [-5.4, 78.33333333333334, 0.0, 0.0, 26.0, 24.40964691135578, 0.1645687950291285, 0.0, 1.0, 45781.04206349106], 
processed observation next is [0.0, 0.0, 0.31301939058171746, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5341372426129816, 0.5548562650097095, 0.0, 1.0, 0.2180049622071003], 
reward next is 0.7820, 
noisyNet noise sample is [array([0.2499565], dtype=float32), 0.20589612]. 
=============================================
[2019-04-04 16:54:30,139] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.7670022e-09 1.5919404e-09 3.3250007e-14 4.7096024e-13 1.0000000e+00
 7.1651529e-10 4.5953807e-14], sum to 1.0000
[2019-04-04 16:54:30,140] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9218
[2019-04-04 16:54:30,155] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.916666666666667, 41.5, 0.0, 0.0, 26.0, 24.79086778733903, 0.1936433812339575, 0.0, 1.0, 43038.905272408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2415000.0000, 
sim time next is 2415600.0000, 
raw observation next is [-5.0, 41.0, 0.0, 0.0, 26.0, 24.74325238308111, 0.1845365346066893, 0.0, 1.0, 43063.6215837738], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5619376985900925, 0.5615121782022298, 0.0, 1.0, 0.20506486468463714], 
reward next is 0.7949, 
noisyNet noise sample is [array([0.9609689], dtype=float32), -0.9327727]. 
=============================================
[2019-04-04 16:54:30,290] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2463149e-08 1.7744209e-09 1.2528883e-13 1.2815629e-13 1.0000000e+00
 9.3237782e-09 1.0390970e-13], sum to 1.0000
[2019-04-04 16:54:30,292] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1633
[2019-04-04 16:54:30,319] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.15917574545531, 0.05870301665495665, 0.0, 1.0, 41111.63103439956], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2008200.0000, 
sim time next is 2008800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.13805997751053, 0.05771806164269403, 0.0, 1.0, 41132.93142092844], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5115049981258775, 0.519239353880898, 0.0, 1.0, 0.19587110200442115], 
reward next is 0.8041, 
noisyNet noise sample is [array([-0.6515639], dtype=float32), -0.6693667]. 
=============================================
[2019-04-04 16:54:54,598] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2589894e-07 2.0511926e-08 1.7760421e-13 4.4538722e-12 9.9999976e-01
 1.0538475e-08 1.1349280e-12], sum to 1.0000
[2019-04-04 16:54:54,601] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7674
[2019-04-04 16:54:54,656] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.0, 84.33333333333333, 0.0, 0.0, 26.0, 23.34243849014157, -0.05429917930617045, 0.0, 1.0, 44092.66918749133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2695800.0000, 
sim time next is 2696400.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 26.0, 23.2799774964373, -0.06049691195667236, 0.0, 1.0, 43964.09177616692], 
processed observation next is [1.0, 0.21739130434782608, 0.04709141274238226, 0.83, 0.0, 0.0, 0.6666666666666666, 0.4399981247031084, 0.47983436268110924, 0.0, 1.0, 0.20935281798174724], 
reward next is 0.7906, 
noisyNet noise sample is [array([-0.50938237], dtype=float32), -0.42140597]. 
=============================================
[2019-04-04 16:54:57,076] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7804394e-08 1.8475075e-09 2.7245328e-14 8.2494504e-13 1.0000000e+00
 3.0866940e-10 4.5325658e-14], sum to 1.0000
[2019-04-04 16:54:57,078] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1299
[2019-04-04 16:54:57,171] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 69.0, 25.33333333333334, 0.0, 26.0, 24.04225058029202, 0.06965190160389696, 0.0, 1.0, 41994.14846304596], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2362200.0000, 
sim time next is 2362800.0000, 
raw observation next is [-3.4, 69.0, 31.16666666666667, 0.0, 26.0, 24.06963236796557, 0.1323117145377559, 0.0, 1.0, 202421.3760289058], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.1038888888888889, 0.0, 0.6666666666666666, 0.5058026973304642, 0.5441039048459186, 0.0, 1.0, 0.963911314423361], 
reward next is 0.0361, 
noisyNet noise sample is [array([-0.5539136], dtype=float32), 0.76875854]. 
=============================================
[2019-04-04 16:55:01,626] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5091461e-09 9.8101294e-10 1.9026110e-14 6.2785538e-14 1.0000000e+00
 3.5450465e-10 2.0136247e-14], sum to 1.0000
[2019-04-04 16:55:01,626] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7766
[2019-04-04 16:55:01,682] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.2, 46.33333333333334, 80.16666666666667, 105.1666666666667, 26.0, 24.96868172865641, 0.2869648870820452, 0.0, 1.0, 47475.62674452374], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2391600.0000, 
sim time next is 2392200.0000, 
raw observation next is [-0.3, 46.0, 79.0, 58.0, 26.0, 24.95386237099652, 0.2846462942148094, 0.0, 1.0, 49397.71799145922], 
processed observation next is [0.0, 0.6956521739130435, 0.4542936288088643, 0.46, 0.2633333333333333, 0.06408839779005525, 0.6666666666666666, 0.5794885309163768, 0.5948820980716031, 0.0, 1.0, 0.2352272285307582], 
reward next is 0.7648, 
noisyNet noise sample is [array([0.37144095], dtype=float32), -0.051126428]. 
=============================================
[2019-04-04 16:55:06,294] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8215450e-08 1.4771196e-09 2.0266558e-14 3.4416252e-13 1.0000000e+00
 1.9071502e-09 5.9551736e-14], sum to 1.0000
[2019-04-04 16:55:06,294] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2816
[2019-04-04 16:55:06,395] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.666666666666667, 64.0, 0.0, 0.0, 26.0, 23.46260121790863, -0.004136595505875511, 1.0, 1.0, 201982.8526594443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2791200.0000, 
sim time next is 2791800.0000, 
raw observation next is [-6.5, 64.0, 0.0, 0.0, 26.0, 23.67802455135429, 0.1016712753167981, 1.0, 1.0, 202976.258776585], 
processed observation next is [1.0, 0.30434782608695654, 0.28254847645429365, 0.64, 0.0, 0.0, 0.6666666666666666, 0.4731687126128576, 0.5338904251055994, 1.0, 1.0, 0.9665536132218333], 
reward next is 0.0334, 
noisyNet noise sample is [array([-0.15840392], dtype=float32), -1.7679693]. 
=============================================
[2019-04-04 16:55:17,154] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2917515e-09 2.1243995e-10 3.4281780e-15 1.9956276e-14 1.0000000e+00
 2.3756005e-10 4.4636205e-15], sum to 1.0000
[2019-04-04 16:55:17,154] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5109
[2019-04-04 16:55:17,198] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 56.5, 99.0, 635.0, 26.0, 25.33916219211443, 0.3257630710116592, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3058200.0000, 
sim time next is 3058800.0000, 
raw observation next is [-4.666666666666666, 55.66666666666667, 100.1666666666667, 655.6666666666667, 26.0, 25.31958311153661, 0.3236640492964549, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.33333333333333337, 0.5566666666666668, 0.333888888888889, 0.7244935543278086, 0.6666666666666666, 0.6099652592947175, 0.6078880164321516, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.62025964], dtype=float32), -1.9222283]. 
=============================================
[2019-04-04 16:55:17,359] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.8602540e-08 5.3127516e-09 3.4415987e-13 2.1588148e-12 9.9999988e-01
 1.0228357e-08 1.0405889e-12], sum to 1.0000
[2019-04-04 16:55:17,361] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2460
[2019-04-04 16:55:17,390] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 26.0, 23.27997749206298, -0.06049691310243375, 0.0, 1.0, 43964.09177933208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2696400.0000, 
sim time next is 2697000.0000, 
raw observation next is [-15.16666666666667, 83.0, 0.0, 0.0, 26.0, 23.27238229590573, -0.06870903711591075, 0.0, 1.0, 43825.26137628922], 
processed observation next is [1.0, 0.21739130434782608, 0.04247460757156039, 0.83, 0.0, 0.0, 0.6666666666666666, 0.4393651913254774, 0.47709698762802977, 0.0, 1.0, 0.2086917208394725], 
reward next is 0.7913, 
noisyNet noise sample is [array([-1.9187329], dtype=float32), -0.5678024]. 
=============================================
[2019-04-04 16:55:17,396] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[69.957565]
 [70.23169 ]
 [70.510414]
 [70.77903 ]
 [71.03704 ]], R is [[69.77764893]
 [69.87052155]
 [69.9618454 ]
 [70.05167389]
 [70.14012146]].
[2019-04-04 16:55:20,457] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2091456e-09 1.2353518e-10 7.7878591e-16 6.6844765e-15 1.0000000e+00
 7.9643410e-11 8.8004616e-16], sum to 1.0000
[2019-04-04 16:55:20,463] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9624
[2019-04-04 16:55:20,488] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.666666666666668, 64.0, 112.8333333333333, 763.1666666666667, 26.0, 26.04560143617544, 0.4791479462231248, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2719200.0000, 
sim time next is 2719800.0000, 
raw observation next is [-8.5, 64.0, 112.0, 781.0, 26.0, 26.03370081062282, 0.4770638060295586, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.22714681440443216, 0.64, 0.37333333333333335, 0.8629834254143647, 0.6666666666666666, 0.6694750675519016, 0.6590212686765196, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.152484], dtype=float32), -1.2066882]. 
=============================================
[2019-04-04 16:55:35,231] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3275437e-09 2.0554052e-10 1.9407046e-16 9.1794959e-15 1.0000000e+00
 5.1122207e-11 6.2302815e-16], sum to 1.0000
[2019-04-04 16:55:35,231] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4924
[2019-04-04 16:55:35,247] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 80.16666666666667, 0.0, 0.0, 26.0, 25.45119207206712, 0.4509447247087471, 0.0, 1.0, 41227.34796895939], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3459000.0000, 
sim time next is 3459600.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.43270634841892, 0.4489322256009139, 0.0, 1.0, 50079.0371324071], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6193921957015768, 0.6496440752003046, 0.0, 1.0, 0.23847160539241477], 
reward next is 0.7615, 
noisyNet noise sample is [array([0.58623785], dtype=float32), 0.3621205]. 
=============================================
[2019-04-04 16:55:40,108] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5401322e-10 3.6703231e-11 1.8286737e-17 8.4511908e-16 1.0000000e+00
 2.5945699e-11 1.1417080e-17], sum to 1.0000
[2019-04-04 16:55:40,109] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8904
[2019-04-04 16:55:40,117] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 49.0, 108.5, 793.5, 26.0, 26.31153041059682, 0.6666464287107673, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3506400.0000, 
sim time next is 3507000.0000, 
raw observation next is [3.0, 49.0, 106.3333333333333, 789.3333333333334, 26.0, 26.47726697181599, 0.6855328226270823, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.35444444444444434, 0.8721915285451197, 0.6666666666666666, 0.7064389143179991, 0.7285109408756941, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9749054], dtype=float32), -1.2421315]. 
=============================================
[2019-04-04 16:55:40,150] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[88.81349]
 [88.93682]
 [89.06882]
 [89.16219]
 [89.27192]], R is [[88.79613495]
 [88.90817261]
 [89.01908875]
 [89.03994751]
 [89.14955139]].
[2019-04-04 16:55:40,227] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.15750093e-10 1.58154670e-10 1.01172924e-16 1.20945548e-15
 1.00000000e+00 1.76213575e-11 1.27659618e-16], sum to 1.0000
[2019-04-04 16:55:40,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1525
[2019-04-04 16:55:40,239] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.0, 34.5, 116.0, 816.0, 26.0, 25.68814218039674, 0.4956772870269894, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3670200.0000, 
sim time next is 3670800.0000, 
raw observation next is [6.666666666666666, 38.0, 116.1666666666667, 818.1666666666666, 26.0, 25.64569438070848, 0.4869895585501338, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6472760849492153, 0.38, 0.38722222222222236, 0.9040515653775322, 0.6666666666666666, 0.6371411983923734, 0.6623298528500446, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39999542], dtype=float32), -0.55470574]. 
=============================================
[2019-04-04 16:55:41,571] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.1715511e-10 8.5243708e-11 1.0345061e-15 3.9405591e-15 1.0000000e+00
 8.9280638e-11 2.0559019e-15], sum to 1.0000
[2019-04-04 16:55:41,574] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0347
[2019-04-04 16:55:41,627] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 54.66666666666667, 114.6666666666667, 817.1666666666666, 26.0, 25.14811356438227, 0.4418691053361744, 0.0, 1.0, 45387.75364513187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3584400.0000, 
sim time next is 3585000.0000, 
raw observation next is [-3.166666666666667, 54.83333333333334, 115.3333333333333, 818.3333333333334, 26.0, 25.17218827665364, 0.4439655683758501, 0.0, 1.0, 18712.89609069381], 
processed observation next is [0.0, 0.4782608695652174, 0.3748845798707295, 0.5483333333333335, 0.3844444444444443, 0.9042357274401474, 0.6666666666666666, 0.5976823563878032, 0.6479885227919501, 0.0, 1.0, 0.08910902900330385], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.0292121], dtype=float32), -0.3561142]. 
=============================================
[2019-04-04 16:55:41,634] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[80.74902 ]
 [80.616455]
 [80.55098 ]
 [80.63887 ]
 [80.90283 ]], R is [[80.84056091]
 [80.81602478]
 [80.84596252]
 [80.94841003]
 [81.13892365]].
[2019-04-04 16:55:43,700] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0143704e-09 1.1140179e-11 4.0059417e-16 4.2577708e-15 1.0000000e+00
 2.9038579e-11 2.7761052e-16], sum to 1.0000
[2019-04-04 16:55:43,704] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3001
[2019-04-04 16:55:43,714] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 26.0, 25.69188738758305, 0.6138732900426146, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3200400.0000, 
sim time next is 3201000.0000, 
raw observation next is [0.8333333333333334, 100.0, 0.0, 0.0, 26.0, 25.7320868305913, 0.6054810572915618, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4856879039704525, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6443405692159416, 0.7018270190971873, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5343751], dtype=float32), 0.7245458]. 
=============================================
[2019-04-04 16:55:43,722] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[88.101166]
 [88.327675]
 [88.49622 ]
 [88.58828 ]
 [88.5712  ]], R is [[87.98155212]
 [88.10173798]
 [88.22071838]
 [88.2492218 ]
 [88.10250092]].
[2019-04-04 16:55:44,929] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4347186e-10 6.9372869e-11 3.9708706e-17 7.7087939e-16 1.0000000e+00
 1.8614844e-11 2.5750302e-16], sum to 1.0000
[2019-04-04 16:55:44,929] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7388
[2019-04-04 16:55:44,939] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 54.5, 116.0, 823.0, 26.0, 25.82052177809445, 0.5748211232295599, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3501000.0000, 
sim time next is 3501600.0000, 
raw observation next is [2.0, 53.66666666666667, 115.8333333333333, 820.1666666666667, 26.0, 25.90033803019212, 0.6095194687832198, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.5366666666666667, 0.386111111111111, 0.9062615101289135, 0.6666666666666666, 0.65836150251601, 0.7031731562610734, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6762369], dtype=float32), -0.3145689]. 
=============================================
[2019-04-04 16:55:47,005] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6353953e-08 2.1893878e-09 1.0495999e-14 5.2469287e-14 1.0000000e+00
 2.8943981e-10 2.6640804e-14], sum to 1.0000
[2019-04-04 16:55:47,009] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0656
[2019-04-04 16:55:47,024] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 67.0, 0.0, 0.0, 26.0, 25.17627161854339, 0.3825441792461352, 0.0, 1.0, 41110.46566156861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3554400.0000, 
sim time next is 3555000.0000, 
raw observation next is [-3.5, 68.0, 0.0, 0.0, 26.0, 25.13502250817505, 0.3741762359128018, 0.0, 1.0, 41174.69845658884], 
processed observation next is [0.0, 0.13043478260869565, 0.36565096952908593, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5945852090145877, 0.6247254119709339, 0.0, 1.0, 0.19606999265042305], 
reward next is 0.8039, 
noisyNet noise sample is [array([0.5607558], dtype=float32), 2.539202]. 
=============================================
[2019-04-04 16:55:47,037] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.494995]
 [80.57007 ]
 [80.46492 ]
 [80.56632 ]
 [80.57716 ]], R is [[80.42782593]
 [80.42778778]
 [80.42816162]
 [80.42892456]
 [80.43001556]].
[2019-04-04 16:55:49,828] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0251925e-09 1.1962581e-09 2.5541655e-14 1.5566921e-13 1.0000000e+00
 9.3705377e-10 6.6084941e-14], sum to 1.0000
[2019-04-04 16:55:49,830] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2706
[2019-04-04 16:55:49,848] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.94746109980454, 0.3584687252135063, 0.0, 1.0, 43830.60776523523], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3290400.0000, 
sim time next is 3291000.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.97882140395869, 0.3490230296160917, 0.0, 1.0, 43816.30591059841], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5815684503298909, 0.6163410098720306, 0.0, 1.0, 0.2086490757647543], 
reward next is 0.7914, 
noisyNet noise sample is [array([0.33254674], dtype=float32), -1.0146044]. 
=============================================
[2019-04-04 16:55:49,866] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[76.94922 ]
 [77.221344]
 [77.33757 ]
 [77.63034 ]
 [77.91069 ]], R is [[76.80245972]
 [76.82572174]
 [76.84868622]
 [76.87152863]
 [76.89432526]].
[2019-04-04 16:55:55,134] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3792905e-08 7.8851714e-10 6.8949812e-15 4.4421776e-14 1.0000000e+00
 6.4464639e-10 1.8166861e-14], sum to 1.0000
[2019-04-04 16:55:55,134] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0822
[2019-04-04 16:55:55,186] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.33333333333333, 81.66666666666667, 72.0, 345.6666666666667, 26.0, 25.55142329745074, 0.4403667320572422, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3313200.0000, 
sim time next is 3313800.0000, 
raw observation next is [-10.0, 80.5, 86.0, 396.0, 26.0, 25.72915626877759, 0.459144687709697, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.18559556786703602, 0.805, 0.2866666666666667, 0.4375690607734807, 0.6666666666666666, 0.6440963557314658, 0.6530482292365657, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1959471], dtype=float32), -0.54562855]. 
=============================================
[2019-04-04 16:55:58,559] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4125167e-10 1.0915878e-11 1.3725272e-16 4.5400542e-16 1.0000000e+00
 1.7035803e-11 2.1703708e-16], sum to 1.0000
[2019-04-04 16:55:58,561] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8795
[2019-04-04 16:55:58,581] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 47.0, 117.0, 817.0, 26.0, 25.1051329160887, 0.4623929742812975, 1.0, 1.0, 90562.98532565584], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3414600.0000, 
sim time next is 3415200.0000, 
raw observation next is [3.0, 47.66666666666666, 116.3333333333333, 815.1666666666667, 26.0, 25.38758250208641, 0.5158306877449257, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.47666666666666657, 0.38777777777777767, 0.9007366482504605, 0.6666666666666666, 0.6156318751738675, 0.6719435625816419, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.224836], dtype=float32), -0.09813068]. 
=============================================
[2019-04-04 16:56:06,669] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.39907358e-10 1.14886836e-10 1.67539332e-16 8.72835011e-15
 1.00000000e+00 3.57582124e-11 3.23124117e-16], sum to 1.0000
[2019-04-04 16:56:06,669] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3293
[2019-04-04 16:56:06,678] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.33333333333333, 26.66666666666667, 109.3333333333333, 746.3333333333334, 26.0, 25.63768891983306, 0.4747121639896116, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3666000.0000, 
sim time next is 3666600.0000, 
raw observation next is [11.5, 26.0, 111.0, 763.0, 26.0, 25.62683240736084, 0.4811839163261178, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.7811634349030472, 0.26, 0.37, 0.8430939226519337, 0.6666666666666666, 0.6355693672800701, 0.6603946387753726, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5993243], dtype=float32), 0.2215297]. 
=============================================
[2019-04-04 16:56:15,001] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7243342e-09 2.3041485e-11 1.1505709e-16 4.0533965e-15 1.0000000e+00
 1.0160216e-10 1.2092796e-16], sum to 1.0000
[2019-04-04 16:56:15,003] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0435
[2019-04-04 16:56:15,017] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.99290571452758, 0.5694483012288162, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3781800.0000, 
sim time next is 3782400.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.92539653392171, 0.5530822477849118, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6604497111601425, 0.6843607492616371, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38119593], dtype=float32), 0.12491503]. 
=============================================
[2019-04-04 16:56:15,600] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.6360781e-10 7.6537554e-11 1.0251681e-15 5.3339054e-15 1.0000000e+00
 1.2451407e-10 1.2984130e-15], sum to 1.0000
[2019-04-04 16:56:15,602] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9144
[2019-04-04 16:56:15,617] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.6, 60.0, 47.0, 392.0, 26.0, 25.59331168686748, 0.4482434910416688, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4294800.0000, 
sim time next is 4295400.0000, 
raw observation next is [6.533333333333333, 60.66666666666666, 39.33333333333333, 337.3333333333333, 26.0, 25.58059430754336, 0.4365920854852909, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.6435826408125578, 0.6066666666666666, 0.1311111111111111, 0.372744014732965, 0.6666666666666666, 0.6317161922952801, 0.6455306951617636, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03983302], dtype=float32), -0.50809366]. 
=============================================
[2019-04-04 16:56:16,154] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.1055700e-10 2.2088280e-10 1.3728832e-16 3.3727913e-14 1.0000000e+00
 1.4420100e-10 9.1729105e-16], sum to 1.0000
[2019-04-04 16:56:16,159] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5663
[2019-04-04 16:56:16,171] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 25.3230739354803, 0.5137757114480604, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4129200.0000, 
sim time next is 4129800.0000, 
raw observation next is [2.666666666666667, 38.0, 0.0, 0.0, 26.0, 25.49774353065771, 0.5297653481538313, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5364727608494922, 0.38, 0.0, 0.0, 0.6666666666666666, 0.6248119608881426, 0.6765884493846105, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12773758], dtype=float32), 0.7144217]. 
=============================================
[2019-04-04 16:56:16,540] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8006803e-09 1.2426109e-11 7.1869720e-17 7.2936684e-16 1.0000000e+00
 3.8403811e-11 3.8903697e-17], sum to 1.0000
[2019-04-04 16:56:16,544] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7635
[2019-04-04 16:56:16,554] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333334, 90.0, 117.6666666666667, 0.9999999999999998, 26.0, 26.19004805470792, 0.5645050499531185, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4452000.0000, 
sim time next is 4452600.0000, 
raw observation next is [0.1666666666666666, 91.0, 133.3333333333333, 2.0, 26.0, 26.07073131649383, 0.5615854157564512, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4672206832871654, 0.91, 0.4444444444444443, 0.0022099447513812156, 0.6666666666666666, 0.6725609430411525, 0.6871951385854836, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36152413], dtype=float32), -0.7564646]. 
=============================================
[2019-04-04 16:56:19,431] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-04 16:56:19,436] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 16:56:19,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:56:19,438] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 16:56:19,438] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:56:19,439] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 16:56:19,439] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 16:56:20,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run51
[2019-04-04 16:56:20,189] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run51
[2019-04-04 16:56:20,219] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/6/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run51
[2019-04-04 16:56:38,602] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.18438871], dtype=float32), 0.24921937]
[2019-04-04 16:56:38,603] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-12.43333333333333, 68.66666666666666, 0.0, 0.0, 26.0, 22.59630486343371, -0.2873771512641142, 0.0, 1.0, 43973.4713802359]
[2019-04-04 16:56:38,603] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 16:56:38,604] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.4638978e-07 3.2728053e-08 1.5035696e-12 1.2143674e-11 9.9999976e-01
 1.5373979e-08 2.0684596e-12], sampled 0.8090250930721722
[2019-04-04 16:57:28,217] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.18438871], dtype=float32), 0.24921937]
[2019-04-04 16:57:28,217] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-5.449999999999999, 51.5, 0.0, 0.0, 26.0, 25.17676064588405, 0.4182704488654544, 0.0, 1.0, 94406.89193503428]
[2019-04-04 16:57:28,217] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 16:57:28,219] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.0132455e-08 1.3786716e-09 4.9977143e-14 1.9540084e-13 1.0000000e+00
 1.3294819e-09 7.8756114e-14], sampled 0.9818785452263378
[2019-04-04 16:57:59,974] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4172 239942393.0563 1605.0250
[2019-04-04 16:58:21,945] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.4099 263463930.4480 1556.9858
[2019-04-04 16:58:24,879] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.9116 275748554.9316 1233.1025
[2019-04-04 16:58:25,904] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 5000000, evaluation results [5000000.0, 7241.409855009758, 263463930.4479512, 1556.9858411143289, 7353.417175922201, 239942393.05634084, 1605.024971488376, 7182.911643182633, 275748554.9316463, 1233.102527933016]
