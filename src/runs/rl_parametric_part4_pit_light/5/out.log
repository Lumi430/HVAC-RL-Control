Using TensorFlow backend.
[2019-04-04 05:13:08,250] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v1', activation='linear', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-v1', eval_act_func='part4_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=100000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=1e-05, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=5000000, metric_func='part4_v1', model_dir='None', model_param=[12, 1], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_v1', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=10, test_env=['Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'], test_mode='Multiple', train_act_func='part4_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=10.0, weight_initer='glorot_uniform', window_len=20)
[2019-04-04 05:13:08,250] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-04 05:13:08.283913: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-04 05:13:25,017] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-04 05:13:25,018] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-v1', 'Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'] ...
[2019-04-04 05:13:25,044] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation worker starts!
[2019-04-04 05:13:25,068] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation worker starts!
[2019-04-04 05:13:25,091] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation worker starts!
[2019-04-04 05:13:25,091] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:25,092] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-04 05:13:25,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:25,158] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run1
[2019-04-04 05:13:26,093] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:26,095] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-04 05:13:26,168] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:26,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run1
[2019-04-04 05:13:27,096] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:27,097] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-04 05:13:27,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:27,171] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run1
[2019-04-04 05:13:28,097] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:28,097] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-04 05:13:28,187] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:28,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run1
[2019-04-04 05:13:29,098] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:29,099] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-04 05:13:29,188] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:29,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run1
[2019-04-04 05:13:29,569] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 05:13:29,569] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:13:29,569] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:13:29,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:29,570] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:13:29,570] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:29,570] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:29,573] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run1
[2019-04-04 05:13:29,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run1
[2019-04-04 05:13:29,581] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run1
[2019-04-04 05:13:30,100] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:30,101] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-04 05:13:30,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:30,222] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run1
[2019-04-04 05:13:31,101] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:31,102] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-04 05:13:31,224] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:31,226] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run1
[2019-04-04 05:13:32,102] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:32,103] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-04 05:13:32,215] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:32,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run1
[2019-04-04 05:13:33,104] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:33,105] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-04 05:13:33,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:33,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run1
[2019-04-04 05:13:34,106] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:34,107] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-04 05:13:34,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:34,371] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run1
[2019-04-04 05:13:35,108] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:35,108] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-04 05:13:35,222] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:35,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run1
[2019-04-04 05:13:36,110] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:36,111] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-04 05:13:36,226] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:36,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run1
[2019-04-04 05:13:37,112] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:37,113] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-04 05:13:37,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:37,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run1
[2019-04-04 05:13:38,114] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:38,115] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-04 05:13:38,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:38,287] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run1
[2019-04-04 05:13:39,116] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:39,116] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-04 05:13:39,240] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:39,242] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run1
[2019-04-04 05:13:40,117] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 05:13:40,123] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-04 05:13:40,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:13:40,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run1
[2019-04-04 05:14:33,234] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-04 05:14:33,235] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.222599030166667, 96.11719546666666, 0.0, 0.0, 26.0, 23.29514176643838, -0.02594578462669513, 0.0, 1.0, 45743.9204688861]
[2019-04-04 05:14:33,235] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:14:33,236] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.22861683 0.17593575 0.0385988  0.14613995 0.10386191 0.06147041
 0.2453763 ], sampled 0.12186502490560602
[2019-04-04 05:16:13,791] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7480.3338 210743906.7704 897.3598
[2019-04-04 05:16:42,712] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7349.6539 235745039.2262 671.5045
[2019-04-04 05:16:50,532] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7278.2800 248228164.4309 552.4712
[2019-04-04 05:16:51,560] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 7349.653927433918, 235745039.2261806, 671.5044586872558, 7480.333773954255, 210743906.77041996, 897.3597660661917, 7278.279971242198, 248228164.4309072, 552.4712304012481]
[2019-04-04 05:17:03,452] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.19336326 0.11198441 0.0357073  0.15882362 0.09565109 0.04337852
 0.36109176], sum to 1.0000
[2019-04-04 05:17:03,453] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7016
[2019-04-04 05:17:03,638] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.4666666666666667, 95.83333333333333, 0.0, 0.0, 25.5, 22.85858499059557, -0.0975204438825833, 0.0, 1.0, 85560.91119580866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 79800.0000, 
sim time next is 80400.0000, 
raw observation next is [0.4333333333333333, 95.66666666666666, 0.0, 0.0, 26.0, 23.02660562003519, -0.07647089850056453, 0.0, 1.0, 56997.53786695484], 
processed observation next is [0.0, 0.9565217391304348, 0.4746075715604802, 0.9566666666666666, 0.0, 0.0, 0.6666666666666666, 0.41888380166959926, 0.47450970049981184, 0.0, 1.0, 0.2714168469854992], 
reward next is 0.7286, 
noisyNet noise sample is [array([0.42475963], dtype=float32), -0.8974916]. 
=============================================
[2019-04-04 05:17:10,371] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.16378334 0.46842772 0.02230733 0.04968018 0.06283212 0.14327104
 0.08969833], sum to 1.0000
[2019-04-04 05:17:10,372] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7658
[2019-04-04 05:17:10,534] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.3, 67.33333333333334, 0.0, 0.0, 20.0, 19.70879436304731, -0.8999445293512157, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 157800.0000, 
sim time next is 158400.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 20.5, 19.58319678385455, -0.9103740457447254, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.20833333333333334, 0.13193306532121252, 0.19654198475175821, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([1.5604562], dtype=float32), 1.085609]. 
=============================================
[2019-04-04 05:17:22,346] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7726: loss -1.4762
[2019-04-04 05:17:22,418] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7726: learning rate 0.0000
[2019-04-04 05:17:22,901] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 7832: loss 23.1222
[2019-04-04 05:17:22,902] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 7832: learning rate 0.0000
[2019-04-04 05:17:22,924] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 7839: loss -0.2551
[2019-04-04 05:17:22,924] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 7839: learning rate 0.0000
[2019-04-04 05:17:23,067] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 7868: loss -1.5902
[2019-04-04 05:17:23,085] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 7868: learning rate 0.0000
[2019-04-04 05:17:23,235] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7909: loss -0.4175
[2019-04-04 05:17:23,236] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7909: learning rate 0.0000
[2019-04-04 05:17:23,255] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7915: loss -0.4130
[2019-04-04 05:17:23,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7915: learning rate 0.0000
[2019-04-04 05:17:23,291] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 7921: loss 26.5287
[2019-04-04 05:17:23,292] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 7922: learning rate 0.0000
[2019-04-04 05:17:23,424] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7957: loss -0.0891
[2019-04-04 05:17:23,485] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7971: learning rate 0.0000
[2019-04-04 05:17:23,608] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8005: loss 0.8118
[2019-04-04 05:17:23,609] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8005: learning rate 0.0000
[2019-04-04 05:17:23,865] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8073: loss 22.8871
[2019-04-04 05:17:23,866] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8073: learning rate 0.0000
[2019-04-04 05:17:23,868] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8074: loss -1.3684
[2019-04-04 05:17:23,869] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8074: learning rate 0.0000
[2019-04-04 05:17:24,063] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8125: loss -0.4291
[2019-04-04 05:17:24,064] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8125: learning rate 0.0000
[2019-04-04 05:17:24,319] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 8203: loss -1.1104
[2019-04-04 05:17:24,327] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 8204: learning rate 0.0000
[2019-04-04 05:17:24,330] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 8208: loss -0.2577
[2019-04-04 05:17:24,342] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 8211: learning rate 0.0000
[2019-04-04 05:17:24,541] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8265: loss -0.5093
[2019-04-04 05:17:24,544] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8265: learning rate 0.0000
[2019-04-04 05:17:24,860] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.22470692 0.44608483 0.01457518 0.01791812 0.05778875 0.20014514
 0.03878113], sum to 1.0000
[2019-04-04 05:17:24,860] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2368
[2019-04-04 05:17:25,022] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.23333333333333, 46.66666666666666, 20.0, 201.0, 21.0, 20.13962458577445, -0.9361106806382313, 1.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 319200.0000, 
sim time next is 319800.0000, 
raw observation next is [-10.41666666666667, 47.83333333333334, 16.0, 162.0, 20.0, 20.14867820868335, -0.9287292686688765, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.17405355493998145, 0.47833333333333344, 0.05333333333333334, 0.17900552486187846, 0.16666666666666666, 0.17905651739027917, 0.1904235771103745, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.94958925], dtype=float32), -0.4665035]. 
=============================================
[2019-04-04 05:17:25,172] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8448: loss -1.9920
[2019-04-04 05:17:25,173] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8449: learning rate 0.0000
[2019-04-04 05:17:27,873] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.14687434 0.5445415  0.01863454 0.0311311  0.02389677 0.15898797
 0.07593376], sum to 1.0000
[2019-04-04 05:17:27,873] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8319
[2019-04-04 05:17:28,012] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-14.3, 68.0, 0.0, 0.0, 19.0, 19.30116613624287, -1.085716965422529, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 348000.0000, 
sim time next is 348600.0000, 
raw observation next is [-14.4, 68.5, 0.0, 0.0, 20.0, 19.27922935935688, -1.089007581209449, 0.0, 1.0, 119741.7650980296], 
processed observation next is [1.0, 0.0, 0.0637119113573407, 0.685, 0.0, 0.0, 0.16666666666666666, 0.10660244661307328, 0.13699747293018363, 0.0, 1.0, 0.5701988814191886], 
reward next is 0.4298, 
noisyNet noise sample is [array([-1.1257678], dtype=float32), -0.41317162]. 
=============================================
[2019-04-04 05:17:31,715] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.20007026 0.45713937 0.02136689 0.02179447 0.03022409 0.21175347
 0.05765144], sum to 1.0000
[2019-04-04 05:17:31,715] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6788
[2019-04-04 05:17:31,865] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.23333333333333, 82.0, 36.66666666666666, 694.5, 23.0, 21.18692095829133, -0.7249467878348453, 1.0, 1.0, 202729.2547282882], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 379200.0000, 
sim time next is 379800.0000, 
raw observation next is [-15.05, 78.0, 39.0, 738.0, 23.0, 21.38036078695306, -0.6241983553937793, 1.0, 1.0, 199360.0702264847], 
processed observation next is [1.0, 0.391304347826087, 0.0457063711911357, 0.78, 0.13, 0.8154696132596685, 0.4166666666666667, 0.2816967322460882, 0.2919338815354069, 1.0, 1.0, 0.9493336677451653], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.47087133], dtype=float32), 1.9892129]. 
=============================================
[2019-04-04 05:17:37,654] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.1425965  0.63931423 0.00990439 0.017194   0.02918352 0.12701021
 0.03479712], sum to 1.0000
[2019-04-04 05:17:37,655] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6416
[2019-04-04 05:17:37,829] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 19.0, 19.2848222787996, -1.15309649401888, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 437400.0000, 
sim time next is 438000.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 19.0, 19.13323684042145, -1.178189114072649, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 0.08333333333333333, 0.09443640336845416, 0.10727029530911698, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.89977187], dtype=float32), -1.3126472]. 
=============================================
[2019-04-04 05:17:37,853] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[2.3054512]
 [2.1224887]
 [2.1864536]
 [2.1005356]
 [2.2264504]], R is [[3.31488156]
 [4.28173256]
 [5.23891544]
 [6.1865263 ]
 [7.12466097]].
[2019-04-04 05:17:46,666] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.18581851 0.5974517  0.00553093 0.01203942 0.00789811 0.15992655
 0.03133478], sum to 1.0000
[2019-04-04 05:17:46,667] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1280
[2019-04-04 05:17:46,893] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [2.516666666666667, 92.66666666666667, 0.0, 0.0, 19.0, 19.15205566738197, -1.060131716320466, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 510600.0000, 
sim time next is 511200.0000, 
raw observation next is [2.7, 92.0, 0.0, 0.0, 19.0, 19.29464146013148, -1.05478032724689, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5373961218836566, 0.92, 0.0, 0.0, 0.08333333333333333, 0.10788678834429, 0.14840655758436996, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.4558728], dtype=float32), 0.1298342]. 
=============================================
[2019-04-04 05:17:47,569] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.298255   0.5683807  0.00777848 0.01532593 0.01020908 0.04437685
 0.05567388], sum to 1.0000
[2019-04-04 05:17:47,569] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1335
[2019-04-04 05:17:47,801] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [4.766666666666667, 88.66666666666667, 0.0, 0.0, 19.0, 19.74782452143128, -0.9619991913868766, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 523200.0000, 
sim time next is 523800.0000, 
raw observation next is [4.65, 88.5, 0.0, 0.0, 19.0, 19.88375825200712, -0.959315646170749, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.5914127423822716, 0.885, 0.0, 0.0, 0.08333333333333333, 0.1569798543339266, 0.18022811794308366, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03493192], dtype=float32), -0.33443668]. 
=============================================
[2019-04-04 05:17:48,986] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.29401547 0.32927683 0.02398555 0.06339309 0.02719624 0.10248396
 0.15964879], sum to 1.0000
[2019-04-04 05:17:48,986] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5537
[2019-04-04 05:17:49,004] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.333333333333333, 83.0, 0.0, 0.0, 19.0, 19.65372961985057, -0.991609504530815, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 534000.0000, 
sim time next is 534600.0000, 
raw observation next is [2.15, 83.5, 0.0, 0.0, 19.0, 19.6475940332394, -1.001451755317986, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.5221606648199446, 0.835, 0.0, 0.0, 0.08333333333333333, 0.13729950276995004, 0.166182748227338, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4764012], dtype=float32), -0.4809196]. 
=============================================
[2019-04-04 05:17:51,699] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15240: loss 42.9892
[2019-04-04 05:17:51,699] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15240: learning rate 0.0000
[2019-04-04 05:17:53,108] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 15549: loss 30.0749
[2019-04-04 05:17:53,137] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 15549: learning rate 0.0000
[2019-04-04 05:17:53,719] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 15682: loss 29.6121
[2019-04-04 05:17:53,753] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 15682: learning rate 0.0000
[2019-04-04 05:17:54,462] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 15897: loss 27.8188
[2019-04-04 05:17:54,507] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 15897: learning rate 0.0000
[2019-04-04 05:17:54,585] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 15930: loss 18.1512
[2019-04-04 05:17:54,588] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 15930: learning rate 0.0000
[2019-04-04 05:17:54,671] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15961: loss 19.4617
[2019-04-04 05:17:54,673] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15961: learning rate 0.0000
[2019-04-04 05:17:54,733] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15975: loss 18.7359
[2019-04-04 05:17:54,736] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15976: learning rate 0.0000
[2019-04-04 05:17:54,836] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 16016: loss 17.5035
[2019-04-04 05:17:54,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 16016: learning rate 0.0000
[2019-04-04 05:17:54,989] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 16061: loss 27.6499
[2019-04-04 05:17:54,989] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 16061: learning rate 0.0000
[2019-04-04 05:17:55,201] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 16140: loss 13.7464
[2019-04-04 05:17:55,206] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 16142: learning rate 0.0000
[2019-04-04 05:17:55,350] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 16188: loss 31.3117
[2019-04-04 05:17:55,356] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 16189: learning rate 0.0000
[2019-04-04 05:17:55,387] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16197: loss 31.0378
[2019-04-04 05:17:55,388] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16197: learning rate 0.0000
[2019-04-04 05:17:55,405] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16204: loss 15.7117
[2019-04-04 05:17:55,408] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16205: learning rate 0.0000
[2019-04-04 05:17:55,496] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16239: loss 43.0478
[2019-04-04 05:17:55,498] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16239: learning rate 0.0000
[2019-04-04 05:17:55,616] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16279: loss 34.6357
[2019-04-04 05:17:55,616] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16279: learning rate 0.0000
[2019-04-04 05:17:56,442] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16491: loss 28.2155
[2019-04-04 05:17:56,525] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16491: learning rate 0.0000
[2019-04-04 05:18:00,108] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.23193821 0.40803838 0.01803457 0.03594642 0.0411161  0.18442695
 0.08049934], sum to 1.0000
[2019-04-04 05:18:00,120] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1640
[2019-04-04 05:18:00,160] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-2.633333333333334, 60.66666666666667, 104.3333333333333, 79.33333333333334, 19.0, 18.81917789806627, -1.128635349033708, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 648600.0000, 
sim time next is 649200.0000, 
raw observation next is [-2.566666666666667, 60.33333333333334, 108.1666666666667, 89.66666666666667, 19.0, 18.87529820340734, -1.126702774628982, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.39150507848568794, 0.6033333333333334, 0.3605555555555557, 0.0990791896869245, 0.08333333333333333, 0.07294151695061164, 0.12443240845700602, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7698718], dtype=float32), -1.0691742]. 
=============================================
[2019-04-04 05:18:15,760] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.24936266 0.5993286  0.00135511 0.00521043 0.00648383 0.10964315
 0.02861625], sum to 1.0000
[2019-04-04 05:18:15,761] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2104
[2019-04-04 05:18:15,898] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-2.716666666666667, 79.16666666666667, 0.0, 0.0, 19.0, 19.27029030338328, -1.095652581292419, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 861000.0000, 
sim time next is 861600.0000, 
raw observation next is [-2.633333333333333, 79.33333333333334, 0.0, 0.0, 19.0, 19.22285083434564, -1.108236975081754, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.38965835641735924, 0.7933333333333334, 0.0, 0.0, 0.08333333333333333, 0.10190423619546991, 0.13058767497274867, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4312077], dtype=float32), -1.3920128]. 
=============================================
[2019-04-04 05:18:17,095] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.24022324 0.630355   0.00142593 0.00122861 0.00324634 0.11700789
 0.00651302], sum to 1.0000
[2019-04-04 05:18:17,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9011
[2019-04-04 05:18:17,125] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 19.0, 18.6390673873485, -1.16337784344673, 0.0, 1.0, 76461.96268320965], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 852000.0000, 
sim time next is 852600.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 20.0, 18.66113604886217, -1.143704802485237, 0.0, 1.0, 199629.7679001166], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.16666666666666666, 0.05509467073851404, 0.11876506583825434, 0.0, 1.0, 0.9506179423815077], 
reward next is 0.0494, 
noisyNet noise sample is [array([0.2522503], dtype=float32), 0.7583123]. 
=============================================
[2019-04-04 05:18:17,629] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 22969: loss -0.9075
[2019-04-04 05:18:17,638] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 22969: learning rate 0.0000
[2019-04-04 05:18:18,396] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23284: loss -0.8605
[2019-04-04 05:18:18,399] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23285: learning rate 0.0000
[2019-04-04 05:18:19,198] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 23612: loss -0.4620
[2019-04-04 05:18:19,199] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 23612: learning rate 0.0000
[2019-04-04 05:18:19,879] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23840: loss 0.1874
[2019-04-04 05:18:19,880] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23840: learning rate 0.0000
[2019-04-04 05:18:19,937] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 23859: loss -0.3906
[2019-04-04 05:18:19,939] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 23859: learning rate 0.0000
[2019-04-04 05:18:20,278] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 23985: loss -0.2690
[2019-04-04 05:18:20,285] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 23985: learning rate 0.0000
[2019-04-04 05:18:20,474] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 24059: loss -0.2412
[2019-04-04 05:18:20,475] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 24059: learning rate 0.0000
[2019-04-04 05:18:20,778] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 24183: loss -0.8279
[2019-04-04 05:18:20,779] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 24183: learning rate 0.0000
[2019-04-04 05:18:20,792] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24188: loss -0.4755
[2019-04-04 05:18:20,796] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24190: learning rate 0.0000
[2019-04-04 05:18:20,847] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24211: loss 0.5452
[2019-04-04 05:18:20,848] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24211: learning rate 0.0000
[2019-04-04 05:18:20,884] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 24230: loss -0.3763
[2019-04-04 05:18:20,889] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 24230: learning rate 0.0000
[2019-04-04 05:18:20,928] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 24235: loss -0.2520
[2019-04-04 05:18:20,933] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 24235: learning rate 0.0000
[2019-04-04 05:18:21,376] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 24424: loss -0.7070
[2019-04-04 05:18:21,402] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 24430: learning rate 0.0000
[2019-04-04 05:18:21,508] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24474: loss -0.7002
[2019-04-04 05:18:21,509] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24474: learning rate 0.0000
[2019-04-04 05:18:21,943] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 24658: loss -1.2091
[2019-04-04 05:18:21,944] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 24658: learning rate 0.0000
[2019-04-04 05:18:22,271] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 24809: loss 0.0448
[2019-04-04 05:18:22,277] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 24810: learning rate 0.0000
[2019-04-04 05:18:22,849] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.26919326 0.6334396  0.00067733 0.00125866 0.00382056 0.08699539
 0.00461532], sum to 1.0000
[2019-04-04 05:18:22,849] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5734
[2019-04-04 05:18:22,996] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 19.0, 18.48049793229593, -1.179231032644889, 0.0, 1.0, 23641.13564606764], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 936600.0000, 
sim time next is 937200.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 19.0, 18.42930643048508, -1.172095992457151, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.08333333333333333, 0.03577553587375674, 0.10930133584761632, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([0.37314552], dtype=float32), 0.6545218]. 
=============================================
[2019-04-04 05:18:28,708] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8947917e-01 6.6194224e-01 2.0779404e-04 4.5483533e-04 1.6359908e-03
 4.2564306e-02 3.7157240e-03], sum to 1.0000
[2019-04-04 05:18:28,715] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6661
[2019-04-04 05:18:28,857] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.4, 80.33333333333333, 0.0, 0.0, 19.0, 21.63109359407662, -0.485720641155131, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1019400.0000, 
sim time next is 1020000.0000, 
raw observation next is [14.4, 79.66666666666667, 0.0, 0.0, 19.0, 21.55219647905657, -0.5048138507346993, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.2960163732547141, 0.3317287164217669, 1.0, 1.0, 0.0], 
reward next is 0.9519, 
noisyNet noise sample is [array([1.2224027], dtype=float32), -0.21277627]. 
=============================================
[2019-04-04 05:18:28,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[6.4489145]
 [6.4945507]
 [6.3173876]
 [6.299608 ]
 [6.2962513]], R is [[ 7.26687765]
 [ 8.1942091 ]
 [ 9.11226749]
 [10.02114487]
 [10.92093372]].
[2019-04-04 05:18:31,008] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3431354e-01 6.0311109e-01 5.2395347e-04 1.4506649e-03 3.3132979e-03
 4.8222192e-02 9.0652080e-03], sum to 1.0000
[2019-04-04 05:18:31,034] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5244
[2019-04-04 05:18:31,045] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.2, 83.0, 48.0, 124.0, 19.0, 20.60053638153321, -0.7013733217113374, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1068600.0000, 
sim time next is 1069200.0000, 
raw observation next is [12.2, 83.0, 61.0, 151.5, 19.0, 20.65375326846013, -0.6862373603681782, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.8005540166204987, 0.83, 0.20333333333333334, 0.16740331491712707, 0.08333333333333333, 0.22114610570501073, 0.2712542132106073, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.51430297], dtype=float32), -1.0787599]. 
=============================================
[2019-04-04 05:18:33,739] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8224367e-01 6.7356694e-01 1.9331179e-04 3.0742338e-04 1.8910378e-03
 3.9655920e-02 2.1416508e-03], sum to 1.0000
[2019-04-04 05:18:33,745] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6922
[2019-04-04 05:18:33,751] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.03333333333333, 76.66666666666667, 111.6666666666667, 38.99999999999999, 19.0, 21.4883873257585, -0.5461880377684155, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1074000.0000, 
sim time next is 1074600.0000, 
raw observation next is [14.4, 75.0, 114.0, 0.0, 19.0, 21.56281435845921, -0.5338498022284918, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8614958448753465, 0.75, 0.38, 0.0, 0.08333333333333333, 0.2969011965382675, 0.32205006592383606, 1.0, 1.0, 0.0], 
reward next is 0.6615, 
noisyNet noise sample is [array([-1.4189107], dtype=float32), 1.4595164]. 
=============================================
[2019-04-04 05:18:33,992] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0845699e-01 6.2735742e-01 1.5289198e-04 5.9691124e-04 1.3036900e-03
 5.3881109e-02 8.2509490e-03], sum to 1.0000
[2019-04-04 05:18:34,002] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3999
[2019-04-04 05:18:34,160] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.61666666666667, 64.33333333333334, 0.0, 0.0, 19.0, 20.75749892788465, -0.5539599126542695, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1116600.0000, 
sim time next is 1117200.0000, 
raw observation next is [12.53333333333333, 64.66666666666667, 0.0, 0.0, 19.0, 20.71938736835728, -0.5618453286741931, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8097876269621421, 0.6466666666666667, 0.0, 0.0, 0.08333333333333333, 0.22661561402977318, 0.312718223775269, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.393538], dtype=float32), 2.781794]. 
=============================================
[2019-04-04 05:18:34,461] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.34831074 0.5036272  0.0022123  0.00666317 0.01121261 0.09141876
 0.03655526], sum to 1.0000
[2019-04-04 05:18:34,464] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3079
[2019-04-04 05:18:34,612] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [11.18333333333333, 77.0, 0.0, 0.0, 19.0, 19.54933413044524, -0.826897704212079, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1138200.0000, 
sim time next is 1138800.0000, 
raw observation next is [11.26666666666667, 77.0, 0.0, 0.0, 19.0, 19.52820912032507, -0.8325547043482365, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.7746999076638967, 0.77, 0.0, 0.0, 0.08333333333333333, 0.12735076002708912, 0.2224817652172545, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0659114], dtype=float32), -0.024080869]. 
=============================================
[2019-04-04 05:18:35,086] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 30140: loss 18.1045
[2019-04-04 05:18:35,089] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 30140: learning rate 0.0000
[2019-04-04 05:18:36,719] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 30970: loss 29.7825
[2019-04-04 05:18:36,720] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 30970: learning rate 0.0000
[2019-04-04 05:18:37,950] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 31499: loss 23.0823
[2019-04-04 05:18:37,951] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 31499: learning rate 0.0000
[2019-04-04 05:18:38,003] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 31531: loss 22.2523
[2019-04-04 05:18:38,004] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 31532: learning rate 0.0000
[2019-04-04 05:18:38,095] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 31573: loss 23.7420
[2019-04-04 05:18:38,100] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 31575: learning rate 0.0000
[2019-04-04 05:18:38,854] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 31948: loss 17.0339
[2019-04-04 05:18:38,855] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 31948: learning rate 0.0000
[2019-04-04 05:18:38,937] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31978: loss 19.7516
[2019-04-04 05:18:38,937] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31978: learning rate 0.0000
[2019-04-04 05:18:39,086] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 32040: loss 17.1960
[2019-04-04 05:18:39,092] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 32040: learning rate 0.0000
[2019-04-04 05:18:39,429] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.0460821e-01 3.8360444e-01 3.7276838e-04 2.0257693e-03 4.0147128e-03
 8.8252634e-02 1.7121438e-02], sum to 1.0000
[2019-04-04 05:18:39,433] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0368
[2019-04-04 05:18:39,576] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [7.383333333333333, 96.0, 0.0, 0.0, 19.0, 20.0848436455073, -0.6468720999436749, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1277400.0000, 
sim time next is 1278000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 20.03819606930146, -0.6565211819399621, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.1698496724417883, 0.2811596060200126, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29180086], dtype=float32), -0.88636965]. 
=============================================
[2019-04-04 05:18:39,598] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[6.4268036]
 [6.4464216]
 [6.453364 ]
 [6.4550986]
 [6.468003 ]], R is [[ 7.37203789]
 [ 8.29831696]
 [ 9.21533394]
 [10.12318039]
 [11.02194881]].
[2019-04-04 05:18:39,616] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 32224: loss 16.9168
[2019-04-04 05:18:39,649] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 32225: learning rate 0.0000
[2019-04-04 05:18:39,695] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 32248: loss 29.2672
[2019-04-04 05:18:39,696] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 32248: learning rate 0.0000
[2019-04-04 05:18:40,250] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 32503: loss 20.7058
[2019-04-04 05:18:40,251] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 32503: learning rate 0.0000
[2019-04-04 05:18:40,392] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32580: loss 16.8346
[2019-04-04 05:18:40,392] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32580: learning rate 0.0000
[2019-04-04 05:18:40,426] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32598: loss 23.2273
[2019-04-04 05:18:40,427] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32600: learning rate 0.0000
[2019-04-04 05:18:40,494] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 32640: loss 24.9756
[2019-04-04 05:18:40,497] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 32641: learning rate 0.0000
[2019-04-04 05:18:40,554] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 32678: loss 20.5436
[2019-04-04 05:18:40,556] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 32678: learning rate 0.0000
[2019-04-04 05:18:40,805] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.74380505 0.18099362 0.00146184 0.0030926  0.00446556 0.03867693
 0.0275044 ], sum to 1.0000
[2019-04-04 05:18:40,806] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2767
[2019-04-04 05:18:40,811] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [16.26666666666667, 77.0, 0.0, 0.0, 19.0, 20.52282407835602, -0.6048133899742184, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1208400.0000, 
sim time next is 1209000.0000, 
raw observation next is [16.18333333333333, 77.5, 0.0, 0.0, 19.0, 20.50354084213637, -0.6081332911937009, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 1.0, 0.9108956602031394, 0.775, 0.0, 0.0, 0.08333333333333333, 0.20862840351136422, 0.29728890293543303, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1324642], dtype=float32), -1.1417627]. 
=============================================
[2019-04-04 05:18:40,842] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32827: loss 17.9832
[2019-04-04 05:18:40,843] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32827: learning rate 0.0000
[2019-04-04 05:18:40,858] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[4.91802  ]
 [4.9134083]
 [4.9055247]
 [4.8921666]
 [4.8960805]], R is [[5.87953186]
 [6.82073641]
 [7.75252914]
 [8.67500401]
 [9.58825397]].
[2019-04-04 05:18:48,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2881924e-01 8.2558322e-01 5.4928445e-05 7.2609822e-05 6.8502245e-04
 4.4009283e-02 7.7572226e-04], sum to 1.0000
[2019-04-04 05:18:48,581] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2726
[2019-04-04 05:18:48,589] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 92.0, 72.0, 0.0, 19.0, 19.88703575067272, -0.8887493232340621, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1434600.0000, 
sim time next is 1435200.0000, 
raw observation next is [1.1, 92.0, 67.66666666666667, 0.0, 19.0, 20.05780714341922, -0.8652794358853141, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.22555555555555556, 0.0, 0.08333333333333333, 0.17148392861826847, 0.21157352137156196, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9107574], dtype=float32), 0.68339676]. 
=============================================
[2019-04-04 05:18:50,088] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5299585e-01 7.6442575e-01 1.3402149e-04 3.3743356e-04 6.0616294e-04
 7.8971460e-02 2.5293233e-03], sum to 1.0000
[2019-04-04 05:18:50,092] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0677
[2019-04-04 05:18:50,187] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 19.79523115059722, -0.8841136227305081, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1409400.0000, 
sim time next is 1410000.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 19.7928298980624, -0.8980571561231088, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.14940249150520005, 0.2006476146256304, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.830939], dtype=float32), 0.11946494]. 
=============================================
[2019-04-04 05:18:50,203] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[8.111511]
 [8.156117]
 [8.478693]
 [8.558594]
 [8.592348]], R is [[ 7.7910614 ]
 [ 8.71315098]
 [ 8.62601948]
 [ 9.53975964]
 [10.44436169]].
[2019-04-04 05:18:51,559] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 38178: loss -0.9289
[2019-04-04 05:18:51,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 38179: learning rate 0.0000
[2019-04-04 05:18:52,470] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7128812e-01 7.6168579e-01 8.9632180e-05 1.0804479e-04 7.8943931e-04
 6.4585432e-02 1.4534746e-03], sum to 1.0000
[2019-04-04 05:18:52,470] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8807
[2019-04-04 05:18:52,476] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 92.0, 54.66666666666666, 0.0, 19.0, 19.8633789677635, -0.9568371432993089, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1437000.0000, 
sim time next is 1437600.0000, 
raw observation next is [1.1, 92.0, 50.33333333333333, 0.0, 19.0, 19.80987733062884, -0.9571033952584513, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.16777777777777778, 0.0, 0.08333333333333333, 0.15082311088573666, 0.18096553491384956, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.58721614], dtype=float32), -0.25335902]. 
=============================================
[2019-04-04 05:18:53,099] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 39156: loss -0.6248
[2019-04-04 05:18:53,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 39156: learning rate 0.0000
[2019-04-04 05:18:53,412] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 39341: loss -0.7866
[2019-04-04 05:18:53,414] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 39342: learning rate 0.0000
[2019-04-04 05:18:54,012] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 39681: loss -0.5233
[2019-04-04 05:18:54,015] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 39683: learning rate 0.0000
[2019-04-04 05:18:54,016] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 39683: loss -1.1600
[2019-04-04 05:18:54,018] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 39683: learning rate 0.0000
[2019-04-04 05:18:54,086] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 39718: loss -0.7106
[2019-04-04 05:18:54,088] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 39718: learning rate 0.0000
[2019-04-04 05:18:54,751] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 40089: loss -0.5992
[2019-04-04 05:18:54,751] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 40090: learning rate 0.0000
[2019-04-04 05:18:54,931] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2021784e-01 8.2546049e-01 9.7555814e-05 1.5210859e-04 5.8207533e-04
 5.2192841e-02 1.2969550e-03], sum to 1.0000
[2019-04-04 05:18:54,931] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0609
[2019-04-04 05:18:54,949] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [2.2, 94.66666666666667, 0.0, 0.0, 19.0, 18.58919182663755, -1.067025865356282, 0.0, 1.0, 160475.4837085069], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1480800.0000, 
sim time next is 1481400.0000, 
raw observation next is [2.2, 95.0, 0.0, 0.0, 19.0, 18.76027894810509, -1.05295407237861, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.95, 0.0, 0.0, 0.08333333333333333, 0.06335657900875742, 0.14901530920713002, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2754605], dtype=float32), -0.1599458]. 
=============================================
[2019-04-04 05:18:54,970] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 40226: loss -0.8653
[2019-04-04 05:18:54,974] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 40228: learning rate 0.0000
[2019-04-04 05:18:55,154] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 40324: loss -1.0269
[2019-04-04 05:18:55,156] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 40324: learning rate 0.0000
[2019-04-04 05:18:55,273] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 40391: loss 0.0071
[2019-04-04 05:18:55,274] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 40392: loss -0.0010
[2019-04-04 05:18:55,274] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 40392: learning rate 0.0000
[2019-04-04 05:18:55,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 40393: learning rate 0.0000
[2019-04-04 05:18:55,293] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 40403: loss -0.4938
[2019-04-04 05:18:55,296] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 40403: learning rate 0.0000
[2019-04-04 05:18:55,309] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40414: loss -1.3099
[2019-04-04 05:18:55,311] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40414: learning rate 0.0000
[2019-04-04 05:18:55,447] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1796443e-01 5.9875149e-01 1.2836045e-04 2.1339592e-04 7.9868932e-04
 7.9129681e-02 3.0139347e-03], sum to 1.0000
[2019-04-04 05:18:55,447] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 40496: loss -0.4874
[2019-04-04 05:18:55,448] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 40496: learning rate 0.0000
[2019-04-04 05:18:55,451] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6804
[2019-04-04 05:18:55,459] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 100.0, 51.33333333333334, 0.0, 19.0, 19.87076805718073, -0.9341435250617178, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1503600.0000, 
sim time next is 1504200.0000, 
raw observation next is [2.1, 100.0, 55.66666666666666, 0.0, 19.0, 19.9654003156474, -0.9248088872507867, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5207756232686982, 1.0, 0.18555555555555553, 0.0, 0.08333333333333333, 0.16378335963728338, 0.19173037091640444, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.42245582], dtype=float32), 0.053965915]. 
=============================================
[2019-04-04 05:18:55,847] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 40733: loss -0.6294
[2019-04-04 05:18:55,851] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 40733: learning rate 0.0000
[2019-04-04 05:18:56,041] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 40854: loss -0.8489
[2019-04-04 05:18:56,042] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 40854: learning rate 0.0000
[2019-04-04 05:18:57,463] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2118990e-01 8.3548653e-01 3.6640249e-05 5.0266510e-05 6.2211038e-04
 4.2079195e-02 5.3542235e-04], sum to 1.0000
[2019-04-04 05:18:57,464] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6809
[2019-04-04 05:18:57,475] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.7, 53.0, 149.0, 124.0, 19.0, 21.00706950652909, -0.6311614353250747, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1600200.0000, 
sim time next is 1600800.0000, 
raw observation next is [13.06666666666667, 51.66666666666667, 153.5, 103.3333333333333, 19.0, 21.18212106047059, -0.6181518467080823, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8245614035087722, 0.5166666666666667, 0.5116666666666667, 0.11418047882136276, 0.08333333333333333, 0.26517675503921573, 0.2939493844306392, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.13651957], dtype=float32), 2.1641202]. 
=============================================
[2019-04-04 05:18:57,540] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9504744e-01 7.7202779e-01 1.8823966e-05 3.2318170e-05 4.1072626e-04
 3.1713400e-02 7.4950478e-04], sum to 1.0000
[2019-04-04 05:18:57,544] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3905
[2019-04-04 05:18:57,552] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.666666666666668, 65.0, 0.0, 0.0, 19.0, 20.75352786980433, -0.6496484501188975, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1538400.0000, 
sim time next is 1539000.0000, 
raw observation next is [8.3, 67.0, 0.0, 0.0, 19.0, 20.65575898948396, -0.6602130940012122, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.6925207756232689, 0.67, 0.0, 0.0, 0.08333333333333333, 0.22131324912366335, 0.2799289686662626, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49797034], dtype=float32), -0.058204707]. 
=============================================
[2019-04-04 05:18:57,567] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[9.821643]
 [9.792215]
 [9.786323]
 [9.743386]
 [9.703148]], R is [[10.97382641]
 [10.86408806]
 [10.75544739]
 [10.64789295]
 [10.54141426]].
[2019-04-04 05:19:00,469] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2647428e-01 8.4044915e-01 1.2842728e-05 1.5837857e-05 3.5501955e-04
 3.2433122e-02 2.5971580e-04], sum to 1.0000
[2019-04-04 05:19:00,470] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0548
[2019-04-04 05:19:00,475] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [12.75, 52.5, 50.0, 37.0, 19.0, 22.57218246457413, -0.3249426943166783, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1614600.0000, 
sim time next is 1615200.0000, 
raw observation next is [12.56666666666667, 53.0, 41.83333333333334, 30.83333333333334, 19.0, 22.73186350674742, -0.3056060176840389, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8107109879963068, 0.53, 0.13944444444444448, 0.03406998158379374, 0.08333333333333333, 0.39432195889561833, 0.3981313274386537, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7634895], dtype=float32), 1.032728]. 
=============================================
[2019-04-04 05:19:03,104] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4692457e-01 7.8725493e-01 1.6808475e-05 2.3311857e-05 1.9683427e-04
 6.5305196e-02 2.7842057e-04], sum to 1.0000
[2019-04-04 05:19:03,105] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1865
[2019-04-04 05:19:03,113] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.516666666666667, 82.16666666666666, 45.33333333333334, 0.0, 19.0, 19.91519819388229, -0.9228161576598858, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1698600.0000, 
sim time next is 1699200.0000, 
raw observation next is [1.6, 81.0, 41.5, 0.0, 19.0, 19.95976619407753, -0.9163622490285878, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.6956521739130435, 0.5069252077562327, 0.81, 0.13833333333333334, 0.0, 0.08333333333333333, 0.1633138495064609, 0.19454591699047075, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6719109], dtype=float32), 1.4521586]. 
=============================================
[2019-04-04 05:19:04,676] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6539055e-01 7.9969370e-01 2.0965164e-05 5.2197796e-05 2.5473561e-04
 3.3964995e-02 6.2275270e-04], sum to 1.0000
[2019-04-04 05:19:04,677] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8714
[2019-04-04 05:19:04,703] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 18.69006174299125, -1.089787034077502, 0.0, 1.0, 100258.3404476187], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1730400.0000, 
sim time next is 1731000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 18.69451571577291, -1.083760902083005, 0.0, 1.0, 65929.14799638154], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.057876309647742374, 0.13874636597233167, 0.0, 1.0, 0.31394832379229304], 
reward next is 0.6861, 
noisyNet noise sample is [array([1.7382538], dtype=float32), -0.25173038]. 
=============================================
[2019-04-04 05:19:04,737] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[ 9.761574]
 [ 9.86496 ]
 [10.171764]
 [10.306572]
 [10.42164 ]], R is [[10.39952755]
 [10.81811142]
 [11.70993042]
 [12.59283161]
 [13.46690369]].
[2019-04-04 05:19:04,789] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4427103e-01 8.0241394e-01 1.0560017e-05 2.7393706e-05 2.2578353e-04
 5.2528597e-02 5.2271859e-04], sum to 1.0000
[2019-04-04 05:19:04,790] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1266
[2019-04-04 05:19:04,799] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 18.99962837305486, -0.9981358549117475, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1719600.0000, 
sim time next is 1720200.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 18.97205924310253, -1.005974955428058, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.08100493692521098, 0.16467501485731403, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.69383], dtype=float32), 1.2645073]. 
=============================================
[2019-04-04 05:19:05,550] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7865905e-01 6.2048745e-01 4.6377565e-04 1.2620636e-03 3.5942183e-03
 8.2242236e-02 1.3291216e-02], sum to 1.0000
[2019-04-04 05:19:05,569] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1099
[2019-04-04 05:19:05,593] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-1.616666666666667, 87.0, 0.0, 0.0, 19.0, 18.74654483229564, -1.095446360197782, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1752600.0000, 
sim time next is 1753200.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 19.0, 18.73446774715726, -1.103989688685794, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 0.08333333333333333, 0.06120564559643841, 0.1320034371047353, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5329636], dtype=float32), 2.5769136]. 
=============================================
[2019-04-04 05:19:05,676] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5004128e-01 8.2240188e-01 8.1920134e-06 3.0778308e-05 1.4476378e-04
 2.6870267e-02 5.0279632e-04], sum to 1.0000
[2019-04-04 05:19:05,679] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8541
[2019-04-04 05:19:05,694] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 19.03527020193125, -0.9779369772101548, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1720800.0000, 
sim time next is 1721400.0000, 
raw observation next is [0.4166666666666667, 92.5, 0.0, 0.0, 19.0, 19.00622807198394, -1.04588401665733, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.47414589104339805, 0.925, 0.0, 0.0, 0.08333333333333333, 0.08385233933199505, 0.15137199444755667, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1173432], dtype=float32), 0.73033273]. 
=============================================
[2019-04-04 05:19:06,405] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 46811: loss 15.3597
[2019-04-04 05:19:06,410] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 46812: learning rate 0.0000
[2019-04-04 05:19:08,663] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 47502: loss 4.1067
[2019-04-04 05:19:08,668] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 47504: learning rate 0.0000
[2019-04-04 05:19:09,603] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 47748: loss 14.2111
[2019-04-04 05:19:09,603] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 47748: learning rate 0.0000
[2019-04-04 05:19:09,846] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 47808: loss 10.3239
[2019-04-04 05:19:09,855] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 47809: learning rate 0.0000
[2019-04-04 05:19:10,256] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 47919: loss 9.6088
[2019-04-04 05:19:10,256] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 47919: learning rate 0.0000
[2019-04-04 05:19:10,729] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 48067: loss 7.1522
[2019-04-04 05:19:10,731] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 48067: learning rate 0.0000
[2019-04-04 05:19:10,817] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48092: loss 5.6122
[2019-04-04 05:19:10,818] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48092: learning rate 0.0000
[2019-04-04 05:19:11,156] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48214: loss 17.3131
[2019-04-04 05:19:11,156] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48214: learning rate 0.0000
[2019-04-04 05:19:11,247] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 48245: loss 21.7624
[2019-04-04 05:19:11,248] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 48245: learning rate 0.0000
[2019-04-04 05:19:11,473] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 48322: loss 10.7718
[2019-04-04 05:19:11,474] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 48322: learning rate 0.0000
[2019-04-04 05:19:11,505] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 48336: loss 12.7179
[2019-04-04 05:19:11,508] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48336: loss 20.5480
[2019-04-04 05:19:11,508] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 48336: learning rate 0.0000
[2019-04-04 05:19:11,511] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48337: learning rate 0.0000
[2019-04-04 05:19:11,846] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 48469: loss 8.9080
[2019-04-04 05:19:11,846] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 48469: learning rate 0.0000
[2019-04-04 05:19:11,909] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 48495: loss 9.2602
[2019-04-04 05:19:11,910] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 48495: learning rate 0.0000
[2019-04-04 05:19:12,043] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 48564: loss 21.9321
[2019-04-04 05:19:12,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 48564: learning rate 0.0000
[2019-04-04 05:19:12,078] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48584: loss 21.9345
[2019-04-04 05:19:12,079] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48584: learning rate 0.0000
[2019-04-04 05:19:22,249] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4341401e-01 7.9435802e-01 1.7446366e-05 1.3248139e-05 2.3336904e-04
 6.1687525e-02 2.7635810e-04], sum to 1.0000
[2019-04-04 05:19:22,250] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1222
[2019-04-04 05:19:22,292] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-7.566666666666666, 80.16666666666667, 113.6666666666667, 444.6666666666667, 19.0, 20.68462497169015, -0.8772593556907419, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1936200.0000, 
sim time next is 1936800.0000, 
raw observation next is [-7.3, 79.0, 128.0, 392.5, 19.0, 20.67459905833444, -0.8673059240451019, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.26038781163434904, 0.79, 0.4266666666666667, 0.43370165745856354, 0.08333333333333333, 0.22288325486120333, 0.21089802531829938, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9291848], dtype=float32), -0.47098532]. 
=============================================
[2019-04-04 05:19:22,528] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0521235e-01 8.4416837e-01 1.4016222e-05 1.0378056e-05 2.4430850e-04
 5.0023951e-02 3.2665100e-04], sum to 1.0000
[2019-04-04 05:19:22,531] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7715
[2019-04-04 05:19:22,546] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 62.0, 52.0, 0.0, 19.0, 19.74576598775392, -1.048937564887283, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1958400.0000, 
sim time next is 1959000.0000, 
raw observation next is [-2.983333333333333, 64.16666666666667, 44.66666666666666, 0.0, 19.0, 19.77985814317752, -1.051063367286996, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.37996306555863346, 0.6416666666666667, 0.14888888888888885, 0.0, 0.08333333333333333, 0.14832151193146004, 0.149645544237668, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3048091], dtype=float32), 0.73627895]. 
=============================================
[2019-04-04 05:19:22,556] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[10.844182]
 [10.827713]
 [10.814715]
 [10.806968]
 [10.788887]], R is [[10.73869228]
 [10.63130569]
 [10.52499294]
 [10.41974354]
 [10.31554604]].
[2019-04-04 05:19:24,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9510750e-01 7.3792624e-01 2.0857440e-05 3.7922058e-05 4.8195504e-04
 6.5642953e-02 7.8257831e-04], sum to 1.0000
[2019-04-04 05:19:24,962] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4998
[2019-04-04 05:19:24,974] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.033333333333333, 86.16666666666667, 42.33333333333333, 0.0, 19.0, 19.18408833684999, -1.174526783655339, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2019000.0000, 
sim time next is 2019600.0000, 
raw observation next is [-6.0, 86.0, 49.0, 0.0, 19.0, 19.2325529000415, -1.177037892175379, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.296398891966759, 0.86, 0.16333333333333333, 0.0, 0.08333333333333333, 0.10271274167012508, 0.10765403594154033, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8291914], dtype=float32), -0.57872146]. 
=============================================
[2019-04-04 05:19:26,469] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 54385: loss 4.9287
[2019-04-04 05:19:26,471] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 54387: learning rate 0.0000
[2019-04-04 05:19:28,277] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 55133: loss 4.7360
[2019-04-04 05:19:28,278] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 55133: learning rate 0.0000
[2019-04-04 05:19:28,369] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 55169: loss 7.3062
[2019-04-04 05:19:28,371] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 55169: learning rate 0.0000
[2019-04-04 05:19:29,419] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 55623: loss 3.8939
[2019-04-04 05:19:29,419] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 55623: learning rate 0.0000
[2019-04-04 05:19:29,698] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 55758: loss 2.5608
[2019-04-04 05:19:29,700] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 55760: learning rate 0.0000
[2019-04-04 05:19:29,802] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55808: loss 4.0717
[2019-04-04 05:19:29,809] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55810: learning rate 0.0000
[2019-04-04 05:19:29,860] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 55829: loss 9.3730
[2019-04-04 05:19:29,861] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 55829: learning rate 0.0000
[2019-04-04 05:19:30,192] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 56000: loss 6.7382
[2019-04-04 05:19:30,193] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 56000: learning rate 0.0000
[2019-04-04 05:19:30,294] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 56056: loss 4.5903
[2019-04-04 05:19:30,295] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 56057: learning rate 0.0000
[2019-04-04 05:19:30,876] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56345: loss 8.3576
[2019-04-04 05:19:30,880] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56347: learning rate 0.0000
[2019-04-04 05:19:30,997] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 56403: loss 9.1889
[2019-04-04 05:19:31,006] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 56403: learning rate 0.0000
[2019-04-04 05:19:31,026] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 56419: loss 9.8159
[2019-04-04 05:19:31,028] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 56420: learning rate 0.0000
[2019-04-04 05:19:31,089] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56440: loss 3.5474
[2019-04-04 05:19:31,089] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56440: learning rate 0.0000
[2019-04-04 05:19:31,143] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 56456: loss 3.7181
[2019-04-04 05:19:31,146] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 56456: learning rate 0.0000
[2019-04-04 05:19:31,182] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 56469: loss 4.1144
[2019-04-04 05:19:31,184] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 56469: learning rate 0.0000
[2019-04-04 05:19:31,789] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 56754: loss 4.0919
[2019-04-04 05:19:31,790] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 56754: learning rate 0.0000
[2019-04-04 05:19:34,315] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.9425206e-02 8.4624934e-01 3.4209694e-05 4.9833136e-05 4.4394727e-04
 6.3037440e-02 7.6001720e-04], sum to 1.0000
[2019-04-04 05:19:34,316] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6779
[2019-04-04 05:19:34,428] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 18.66105037494569, -1.240352553762116, 0.0, 1.0, 56836.84464206619], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2171400.0000, 
sim time next is 2172000.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 18.62374571673226, -1.252891408984425, 0.0, 1.0, 63149.22487155971], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.05197880972768824, 0.08236953033852501, 0.0, 1.0, 0.3007105946264748], 
reward next is 0.6993, 
noisyNet noise sample is [array([-0.95718545], dtype=float32), -1.0798757]. 
=============================================
[2019-04-04 05:19:34,445] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[10.254709]
 [10.425042]
 [10.396499]
 [10.482293]
 [10.559766]], R is [[10.69822121]
 [11.32058716]
 [11.96202183]
 [12.8424015 ]
 [13.71397781]].
[2019-04-04 05:19:36,472] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3947705e-01 6.8947130e-01 9.4325542e-06 1.1594662e-05 2.0258232e-04
 7.0379287e-02 4.4875083e-04], sum to 1.0000
[2019-04-04 05:19:36,472] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4942
[2019-04-04 05:19:36,546] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 19.0, 18.51059214853593, -1.156670092955611, 1.0, 1.0, 153780.2330774483], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2232000.0000, 
sim time next is 2232600.0000, 
raw observation next is [-5.0, 70.5, 0.0, 0.0, 19.0, 18.62191489189916, -1.128828018563803, 0.0, 1.0, 83870.65129078114], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.705, 0.0, 0.0, 0.08333333333333333, 0.051826240991596606, 0.12372399381206563, 0.0, 1.0, 0.39938405376562447], 
reward next is 0.6006, 
noisyNet noise sample is [array([2.751941], dtype=float32), -0.8967522]. 
=============================================
[2019-04-04 05:19:39,008] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.18620284e-01 8.23245049e-01 7.60572584e-06 1.31734887e-05
 1.68161205e-04 5.75275607e-02 4.18116164e-04], sum to 1.0000
[2019-04-04 05:19:39,015] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0823
[2019-04-04 05:19:39,028] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-6.366666666666667, 76.0, 0.0, 0.0, 19.0, 18.63953591963089, -1.192658301267917, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2244000.0000, 
sim time next is 2244600.0000, 
raw observation next is [-6.45, 76.5, 0.0, 0.0, 19.0, 18.64665039548954, -1.203664155295166, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.28393351800554023, 0.765, 0.0, 0.0, 0.08333333333333333, 0.05388753295746174, 0.09877861490161137, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.71274203], dtype=float32), -0.50050634]. 
=============================================
[2019-04-04 05:19:39,879] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1190667e-01 7.4449825e-01 5.1101970e-06 1.3605609e-05 1.6768424e-04
 4.3090645e-02 3.1803173e-04], sum to 1.0000
[2019-04-04 05:19:39,879] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1574
[2019-04-04 05:19:39,910] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-6.366666666666667, 76.0, 0.0, 0.0, 19.0, 18.69746655412209, -1.173190235814414, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2244000.0000, 
sim time next is 2244600.0000, 
raw observation next is [-6.45, 76.5, 0.0, 0.0, 19.0, 18.71345265367121, -1.183436381078435, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.28393351800554023, 0.765, 0.0, 0.0, 0.08333333333333333, 0.059454387805934296, 0.10552120630718835, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29992998], dtype=float32), 0.7001378]. 
=============================================
[2019-04-04 05:19:44,540] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 62511: loss 21.4504
[2019-04-04 05:19:44,543] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 62511: learning rate 0.0000
[2019-04-04 05:19:46,935] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 63547: loss 20.4544
[2019-04-04 05:19:46,936] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 63547: learning rate 0.0000
[2019-04-04 05:19:47,120] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 63606: loss 13.3174
[2019-04-04 05:19:47,120] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 63606: learning rate 0.0000
[2019-04-04 05:19:47,377] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 63699: loss 15.6000
[2019-04-04 05:19:47,378] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 63699: learning rate 0.0000
[2019-04-04 05:19:47,860] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 63864: loss 25.1140
[2019-04-04 05:19:47,861] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 63864: learning rate 0.0000
[2019-04-04 05:19:48,152] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63967: loss 9.6227
[2019-04-04 05:19:48,154] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63967: learning rate 0.0000
[2019-04-04 05:19:48,231] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63987: loss 11.9713
[2019-04-04 05:19:48,232] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63988: learning rate 0.0000
[2019-04-04 05:19:48,496] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 64092: loss 23.4412
[2019-04-04 05:19:48,497] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 64092: learning rate 0.0000
[2019-04-04 05:19:48,526] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 64105: loss 22.5640
[2019-04-04 05:19:48,530] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 64108: learning rate 0.0000
[2019-04-04 05:19:48,597] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64136: loss 13.4775
[2019-04-04 05:19:48,602] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64136: learning rate 0.0000
[2019-04-04 05:19:48,880] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 64268: loss 17.4772
[2019-04-04 05:19:48,880] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 64268: learning rate 0.0000
[2019-04-04 05:19:49,085] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 64365: loss 22.4001
[2019-04-04 05:19:49,086] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 64366: learning rate 0.0000
[2019-04-04 05:19:49,300] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64476: loss 9.9053
[2019-04-04 05:19:49,302] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64477: learning rate 0.0000
[2019-04-04 05:19:49,550] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64612: loss 18.2125
[2019-04-04 05:19:49,552] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64612: learning rate 0.0000
[2019-04-04 05:19:49,558] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0695319e-01 6.2639695e-01 8.8820751e-05 1.8554891e-04 1.4286330e-03
 6.1794732e-02 3.1521060e-03], sum to 1.0000
[2019-04-04 05:19:49,558] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5620
[2019-04-04 05:19:49,587] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 46.33333333333334, 0.0, 0.0, 19.0, 18.63696566270988, -1.244000895704859, 0.0, 1.0, 47319.76312653125], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2421600.0000, 
sim time next is 2422200.0000, 
raw observation next is [-6.1, 47.16666666666666, 0.0, 0.0, 19.0, 18.60517403380609, -1.248816232848742, 0.0, 1.0, 58801.70022079741], 
processed observation next is [0.0, 0.0, 0.29362880886426596, 0.47166666666666657, 0.0, 0.0, 0.08333333333333333, 0.050431169483840854, 0.08372792238375266, 0.0, 1.0, 0.28000809628951145], 
reward next is 0.7200, 
noisyNet noise sample is [array([-0.48005813], dtype=float32), 0.013188514]. 
=============================================
[2019-04-04 05:19:49,778] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 64734: loss 13.0464
[2019-04-04 05:19:49,779] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 64735: learning rate 0.0000
[2019-04-04 05:19:49,812] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 64747: loss 19.0312
[2019-04-04 05:19:49,815] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 64750: learning rate 0.0000
[2019-04-04 05:19:49,868] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7443296e-01 6.3579452e-01 7.3017552e-05 1.9739666e-04 1.3148549e-03
 8.3751358e-02 4.4359034e-03], sum to 1.0000
[2019-04-04 05:19:49,869] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1446
[2019-04-04 05:19:49,885] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-3.233333333333333, 42.16666666666666, 0.0, 0.0, 19.0, 18.85054699726108, -1.203989791934452, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2404200.0000, 
sim time next is 2404800.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 19.0, 18.76027228798944, -1.223163718057291, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.08333333333333333, 0.06335602399912006, 0.09227876064756968, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16441022], dtype=float32), -1.3337275]. 
=============================================
[2019-04-04 05:19:50,777] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3597646e-01 6.8798363e-01 1.4663425e-04 3.1430757e-04 2.2043458e-03
 6.8409145e-02 4.9655000e-03], sum to 1.0000
[2019-04-04 05:19:50,777] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9990
[2019-04-04 05:19:50,791] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 60.0, 0.0, 0.0, 19.0, 18.4145371267289, -1.304682433878279, 0.0, 1.0, 45927.92442887301], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 2445600.0000, 
sim time next is 2446200.0000, 
raw observation next is [-9.5, 59.5, 0.0, 0.0, 21.0, 18.42766953888573, -1.30296322438974, 0.0, 1.0, 68120.58353209423], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.595, 0.0, 0.0, 0.25, 0.03563912824047755, 0.06567892520342, 0.0, 1.0, 0.3243837311052106], 
reward next is 0.6756, 
noisyNet noise sample is [array([0.13017088], dtype=float32), -1.1662952]. 
=============================================
[2019-04-04 05:19:59,843] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.3129659e-02 8.8592607e-01 2.8153352e-06 5.2729974e-06 8.2286038e-05
 3.0714311e-02 1.3956046e-04], sum to 1.0000
[2019-04-04 05:19:59,846] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2818
[2019-04-04 05:19:59,858] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.199999999999999, 78.83333333333334, 0.0, 0.0, 19.0, 18.55999692539304, -1.254514297363052, 0.0, 1.0, 43134.55665745587], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2616600.0000, 
sim time next is 2617200.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 19.0, 18.60047824890112, -1.259147894720697, 0.0, 1.0, 21536.82368690731], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.79, 0.0, 0.0, 0.08333333333333333, 0.05003985407509326, 0.08028403509310096, 0.0, 1.0, 0.10255630327098719], 
reward next is 0.8974, 
noisyNet noise sample is [array([-0.18688577], dtype=float32), -0.30143404]. 
=============================================
[2019-04-04 05:20:00,009] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 70180: loss 7.2186
[2019-04-04 05:20:00,011] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 70180: learning rate 0.0000
[2019-04-04 05:20:01,692] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 70970: loss 5.5252
[2019-04-04 05:20:01,693] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 70970: learning rate 0.0000
[2019-04-04 05:20:02,558] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 71405: loss 10.5619
[2019-04-04 05:20:02,559] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 71407: learning rate 0.0000
[2019-04-04 05:20:02,863] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4500, global step 71557: loss 7.0616
[2019-04-04 05:20:02,866] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 71558: loss 7.1011
[2019-04-04 05:20:02,868] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 71558: learning rate 0.0000
[2019-04-04 05:20:02,869] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4500, global step 71558: learning rate 0.0000
[2019-04-04 05:20:03,158] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 71706: loss 12.5383
[2019-04-04 05:20:03,159] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 71706: learning rate 0.0000
[2019-04-04 05:20:03,767] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71953: loss 7.5668
[2019-04-04 05:20:03,767] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71953: learning rate 0.0000
[2019-04-04 05:20:03,781] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 71960: loss 6.9341
[2019-04-04 05:20:03,786] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 71962: learning rate 0.0000
[2019-04-04 05:20:03,855] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.7756536e-02 9.3128753e-01 5.1563120e-07 8.6122526e-07 2.5780715e-05
 1.0864899e-02 6.3773448e-05], sum to 1.0000
[2019-04-04 05:20:03,855] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5753
[2019-04-04 05:20:03,885] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 60.66666666666666, 0.0, 0.0, 19.0, 18.57360080933153, -1.133191753529354, 0.0, 1.0, 196459.5805270132], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2756400.0000, 
sim time next is 2757000.0000, 
raw observation next is [-6.0, 59.83333333333334, 0.0, 0.0, 19.0, 18.5681200099698, -1.092009837144585, 0.0, 1.0, 170600.5812748499], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.5983333333333334, 0.0, 0.0, 0.08333333333333333, 0.047343334164149965, 0.135996720951805, 0.0, 1.0, 0.812383720356428], 
reward next is 0.1876, 
noisyNet noise sample is [array([0.3599094], dtype=float32), -0.4738671]. 
=============================================
[2019-04-04 05:20:03,892] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72008: loss 7.5084
[2019-04-04 05:20:03,892] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72008: learning rate 0.0000
[2019-04-04 05:20:03,903] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[13.956954]
 [13.949106]
 [13.507959]
 [13.608302]
 [13.868179]], R is [[14.13157749]
 [14.05473995]
 [13.97982216]
 [14.84002399]
 [15.69162369]].
[2019-04-04 05:20:04,076] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 72095: loss 9.5547
[2019-04-04 05:20:04,078] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 72095: learning rate 0.0000
[2019-04-04 05:20:04,252] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 72165: loss 21.6683
[2019-04-04 05:20:04,255] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 72165: learning rate 0.0000
[2019-04-04 05:20:04,514] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 72275: loss 7.1193
[2019-04-04 05:20:04,514] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 72275: learning rate 0.0000
[2019-04-04 05:20:04,782] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72387: loss 6.8813
[2019-04-04 05:20:04,782] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72387: learning rate 0.0000
[2019-04-04 05:20:04,865] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 72418: loss 7.1971
[2019-04-04 05:20:04,866] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 72418: learning rate 0.0000
[2019-04-04 05:20:05,410] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 72631: loss 16.0907
[2019-04-04 05:20:05,410] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 72631: learning rate 0.0000
[2019-04-04 05:20:05,590] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 72716: loss 9.1539
[2019-04-04 05:20:05,593] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 72716: learning rate 0.0000
[2019-04-04 05:20:13,612] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.6102980e-02 9.2583215e-01 6.3096832e-07 1.1643463e-06 2.1497282e-05
 1.7969260e-02 7.2245661e-05], sum to 1.0000
[2019-04-04 05:20:13,616] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5938
[2019-04-04 05:20:13,649] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 19.0, 18.94024238714521, -1.170034986097187, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2877000.0000, 
sim time next is 2877600.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 19.0, 18.9102967461528, -1.183889286859529, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.0, 0.0, 0.08333333333333333, 0.0758580621793999, 0.10537023771349034, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.38709852], dtype=float32), -0.19873077]. 
=============================================
[2019-04-04 05:20:13,891] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.8632348e-02 9.1836947e-01 1.4003873e-06 2.6302628e-06 3.4027500e-05
 2.2861419e-02 9.8728975e-05], sum to 1.0000
[2019-04-04 05:20:13,894] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4829
[2019-04-04 05:20:13,941] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 93.0, 34.99999999999999, 69.99999999999999, 19.0, 18.9717755954209, -1.102713931029581, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2879400.0000, 
sim time next is 2880000.0000, 
raw observation next is [2.0, 93.0, 52.5, 91.5, 19.0, 19.24204987142414, -1.088931474424025, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.518005540166205, 0.93, 0.175, 0.1011049723756906, 0.08333333333333333, 0.10350415595201164, 0.13702284185865832, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.38709852], dtype=float32), -0.19873077]. 
=============================================
[2019-04-04 05:20:13,955] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[13.50828 ]
 [13.82907 ]
 [13.791802]
 [13.964812]
 [14.407527]], R is [[13.36920834]
 [13.23551655]
 [13.10316181]
 [12.97213078]
 [12.84240913]].
[2019-04-04 05:20:15,211] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.20658815e-01 8.38950753e-01 2.33895707e-06 5.25121823e-06
 6.31945950e-05 4.00787555e-02 2.40847672e-04], sum to 1.0000
[2019-04-04 05:20:15,214] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7351
[2019-04-04 05:20:15,236] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.8333333333333334, 94.16666666666666, 67.66666666666666, 51.99999999999999, 19.0, 19.21846397999908, -1.138866726455048, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2884200.0000, 
sim time next is 2884800.0000, 
raw observation next is [0.6666666666666667, 95.33333333333334, 58.33333333333333, 25.99999999999999, 19.0, 19.25246700939699, -1.131998062564636, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4810710987996307, 0.9533333333333335, 0.19444444444444442, 0.028729281767955788, 0.08333333333333333, 0.10437225078308258, 0.12266731247845468, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.26135355], dtype=float32), 0.35680106]. 
=============================================
[2019-04-04 05:20:15,570] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1993081e-01 7.1984202e-01 4.9307502e-05 6.4602929e-05 1.2717136e-03
 5.7791390e-02 1.0500782e-03], sum to 1.0000
[2019-04-04 05:20:15,571] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4159
[2019-04-04 05:20:15,590] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 60.0, 104.0, 767.3333333333334, 19.0, 19.10439501149283, -0.9869537983521156, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2988600.0000, 
sim time next is 2989200.0000, 
raw observation next is [-2.0, 60.00000000000001, 102.5, 759.1666666666667, 19.0, 19.17041400832982, -0.9783128969984869, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6000000000000001, 0.3416666666666667, 0.8388581952117865, 0.08333333333333333, 0.09753450069415177, 0.17389570100050436, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8204638], dtype=float32), 0.089569375]. 
=============================================
[2019-04-04 05:20:16,203] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 78310: loss 20.6584
[2019-04-04 05:20:16,204] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 78310: learning rate 0.0000
[2019-04-04 05:20:18,326] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 79310: loss 20.6359
[2019-04-04 05:20:18,327] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 79313: learning rate 0.0000
[2019-04-04 05:20:19,177] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 79590: loss 26.7333
[2019-04-04 05:20:19,178] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 79590: learning rate 0.0000
[2019-04-04 05:20:19,366] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 79640: loss 19.3525
[2019-04-04 05:20:19,393] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 79642: learning rate 0.0000
[2019-04-04 05:20:19,605] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5000, global step 79703: loss 16.3210
[2019-04-04 05:20:19,608] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5000, global step 79703: learning rate 0.0000
[2019-04-04 05:20:20,138] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 79911: loss 16.2511
[2019-04-04 05:20:20,138] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 79911: learning rate 0.0000
[2019-04-04 05:20:20,376] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1378912e-01 7.2315615e-01 3.5868194e-05 7.7288445e-05 8.4738317e-04
 6.0331658e-02 1.7624605e-03], sum to 1.0000
[2019-04-04 05:20:20,376] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2327
[2019-04-04 05:20:20,430] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.166666666666667, 66.0, 204.0, 110.6666666666666, 19.0, 18.64559246652207, -1.14529200973452, 0.0, 1.0, 120906.5689419357], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2976600.0000, 
sim time next is 2977200.0000, 
raw observation next is [-3.0, 65.0, 217.0, 154.0, 19.0, 18.60097101151271, -1.113939027405831, 0.0, 1.0, 77256.72955456804], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.7233333333333334, 0.17016574585635358, 0.08333333333333333, 0.05008091762605904, 0.12868699086472302, 0.0, 1.0, 0.3678891883550859], 
reward next is 0.6321, 
noisyNet noise sample is [array([1.6718568], dtype=float32), -0.40187776]. 
=============================================
[2019-04-04 05:20:20,512] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80060: loss 21.0701
[2019-04-04 05:20:20,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80060: learning rate 0.0000
[2019-04-04 05:20:20,728] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2407955e-01 7.3623097e-01 4.8197326e-06 2.1642732e-05 3.6144364e-04
 3.8139515e-02 1.1620338e-03], sum to 1.0000
[2019-04-04 05:20:20,728] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0681
[2019-04-04 05:20:20,729] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 80130: loss 15.3837
[2019-04-04 05:20:20,730] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 80130: learning rate 0.0000
[2019-04-04 05:20:20,868] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 80143: loss 17.9408
[2019-04-04 05:20:20,902] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-3.583333333333333, 65.0, 0.0, 0.0, 19.0, 18.768132907752, -1.168465120713774, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3013800.0000, 
sim time next is 3014400.0000, 
raw observation next is [-3.666666666666667, 65.0, 0.0, 0.0, 19.0, 18.72235017253054, -1.182874234216095, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.3610341643582641, 0.65, 0.0, 0.0, 0.08333333333333333, 0.060195847710878304, 0.10570858859463501, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8115946], dtype=float32), -0.62143385]. 
=============================================
[2019-04-04 05:20:20,903] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 80143: learning rate 0.0000
[2019-04-04 05:20:21,179] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 80236: loss 26.0947
[2019-04-04 05:20:21,181] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 80238: loss 20.7076
[2019-04-04 05:20:21,184] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 80241: learning rate 0.0000
[2019-04-04 05:20:21,249] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 80242: learning rate 0.0000
[2019-04-04 05:20:21,389] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 80280: loss 20.3171
[2019-04-04 05:20:21,391] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 80282: learning rate 0.0000
[2019-04-04 05:20:22,003] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 80582: loss 26.8848
[2019-04-04 05:20:22,003] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 80582: learning rate 0.0000
[2019-04-04 05:20:22,129] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 80612: loss 25.8450
[2019-04-04 05:20:22,130] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 80612: learning rate 0.0000
[2019-04-04 05:20:22,528] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 80805: loss 24.1233
[2019-04-04 05:20:22,533] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 80806: learning rate 0.0000
[2019-04-04 05:20:23,132] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 81096: loss 25.8582
[2019-04-04 05:20:23,132] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 81096: learning rate 0.0000
[2019-04-04 05:20:25,167] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5199372e-01 6.7221880e-01 9.9997615e-06 1.3826346e-05 6.1992928e-04
 7.4605025e-02 5.3862855e-04], sum to 1.0000
[2019-04-04 05:20:25,205] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3504
[2019-04-04 05:20:25,236] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.1666666666666667, 39.16666666666666, 89.0, 707.0, 19.0, 19.86771127556716, -0.8713775684721798, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3078600.0000, 
sim time next is 3079200.0000, 
raw observation next is [0.3333333333333333, 39.33333333333334, 86.5, 690.0, 19.0, 19.87953584893389, -0.8720534030596706, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.4718374884579871, 0.3933333333333334, 0.28833333333333333, 0.7624309392265194, 0.08333333333333333, 0.15662798741115758, 0.20931553231344313, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2048146], dtype=float32), -0.77915746]. 
=============================================
[2019-04-04 05:20:25,777] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.0541365e-02 9.2465055e-01 1.1153539e-08 1.3016302e-08 6.6180705e-06
 1.4796017e-02 5.5256883e-06], sum to 1.0000
[2019-04-04 05:20:25,779] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2935
[2019-04-04 05:20:25,793] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.4, 99.33333333333334, 62.33333333333333, 529.0, 19.0, 23.32454906976338, -0.09808909484645993, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3169200.0000, 
sim time next is 3169800.0000, 
raw observation next is [6.3, 99.5, 58.0, 499.0, 19.0, 23.4841452440294, -0.07996557429905489, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6371191135734073, 0.995, 0.19333333333333333, 0.5513812154696133, 0.08333333333333333, 0.4570121036691166, 0.47334480856698175, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01386067], dtype=float32), 1.3530821]. 
=============================================
[2019-04-04 05:20:30,468] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2290557e-02 9.4692397e-01 9.8003618e-08 1.7478217e-07 6.5093318e-06
 1.0766268e-02 1.2489856e-05], sum to 1.0000
[2019-04-04 05:20:30,470] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2078
[2019-04-04 05:20:30,482] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 92.0, 0.0, 0.0, 19.0, 18.76680476666793, -1.015278883723362, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3223200.0000, 
sim time next is 3223800.0000, 
raw observation next is [-3.0, 92.0, 1.0, 82.0, 19.0, 19.03731016911996, -1.007733103109105, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.0033333333333333335, 0.09060773480662983, 0.08333333333333333, 0.08644251409332988, 0.16408896563029832, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.08060791], dtype=float32), -0.078324385]. 
=============================================
[2019-04-04 05:20:31,095] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 85572: loss 5.5842
[2019-04-04 05:20:31,099] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 85573: learning rate 0.0000
[2019-04-04 05:20:32,345] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.8891056e-02 9.4202560e-01 4.3305569e-08 8.5022201e-08 3.3033139e-06
 9.0668602e-03 1.2972339e-05], sum to 1.0000
[2019-04-04 05:20:32,345] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1193
[2019-04-04 05:20:32,358] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 94.66666666666666, 0.0, 0.0, 19.0, 18.76159677828025, -1.083600586133536, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3220800.0000, 
sim time next is 3221400.0000, 
raw observation next is [-3.0, 93.33333333333334, 0.0, 0.0, 19.0, 18.65676680328223, -1.085241994089071, 0.0, 1.0, 196542.0639825916], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.9333333333333335, 0.0, 0.0, 0.08333333333333333, 0.05473056694018593, 0.13825266863697636, 0.0, 1.0, 0.9359145903932933], 
reward next is 0.0641, 
noisyNet noise sample is [array([0.7038001], dtype=float32), 0.40471813]. 
=============================================
[2019-04-04 05:20:33,526] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 86963: loss 5.6264
[2019-04-04 05:20:33,527] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 86963: learning rate 0.0000
[2019-04-04 05:20:33,635] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.9088829e-02 8.9680266e-01 4.0645972e-08 3.5134313e-08 9.0153726e-06
 2.4091857e-02 7.5897938e-06], sum to 1.0000
[2019-04-04 05:20:33,644] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9238
[2019-04-04 05:20:33,659] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 73.0, 17.33333333333333, 171.8333333333333, 19.0, 21.64296083853628, -0.6019433774160751, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3259200.0000, 
sim time next is 3259800.0000, 
raw observation next is [-4.0, 71.0, 9.0, 104.0, 19.0, 21.29354441280066, -0.7122066472864796, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.71, 0.03, 0.11491712707182321, 0.08333333333333333, 0.274462034400055, 0.26259778423784014, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1388599], dtype=float32), -0.16993958]. 
=============================================
[2019-04-04 05:20:34,002] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 87232: loss 4.9556
[2019-04-04 05:20:34,005] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 87232: learning rate 0.0000
[2019-04-04 05:20:34,018] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 87239: loss 5.3820
[2019-04-04 05:20:34,022] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 87241: learning rate 0.0000
[2019-04-04 05:20:35,245] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87913: loss 5.3046
[2019-04-04 05:20:35,248] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87913: learning rate 0.0000
[2019-04-04 05:20:35,257] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 87920: loss 5.6714
[2019-04-04 05:20:35,259] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 87920: learning rate 0.0000
[2019-04-04 05:20:35,263] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 87923: loss 5.5562
[2019-04-04 05:20:35,265] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 87924: learning rate 0.0000
[2019-04-04 05:20:35,554] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5500, global step 88087: loss 11.7570
[2019-04-04 05:20:35,556] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5500, global step 88088: learning rate 0.0000
[2019-04-04 05:20:35,678] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 88152: loss 6.2816
[2019-04-04 05:20:35,679] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 88153: learning rate 0.0000
[2019-04-04 05:20:35,683] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 88154: loss 10.5899
[2019-04-04 05:20:35,683] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 88154: learning rate 0.0000
[2019-04-04 05:20:35,770] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88205: loss 6.0050
[2019-04-04 05:20:35,771] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88205: learning rate 0.0000
[2019-04-04 05:20:35,841] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 88239: loss 14.8690
[2019-04-04 05:20:35,843] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 88241: learning rate 0.0000
[2019-04-04 05:20:36,735] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 88690: loss 12.5272
[2019-04-04 05:20:36,738] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 88694: learning rate 0.0000
[2019-04-04 05:20:36,985] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 88825: loss 7.5928
[2019-04-04 05:20:36,996] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 88832: learning rate 0.0000
[2019-04-04 05:20:37,025] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 88846: loss 17.6024
[2019-04-04 05:20:37,026] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 88846: learning rate 0.0000
[2019-04-04 05:20:37,725] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 89223: loss 5.9471
[2019-04-04 05:20:37,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 89223: learning rate 0.0000
[2019-04-04 05:20:39,293] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.5773666e-02 9.1478592e-01 1.7624996e-07 2.0773044e-07 3.3365861e-05
 1.9390762e-02 1.5886804e-05], sum to 1.0000
[2019-04-04 05:20:39,295] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8747
[2019-04-04 05:20:39,310] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 49.0, 111.3333333333333, 800.8333333333334, 19.0, 20.51513762994152, -0.770111244900162, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3418800.0000, 
sim time next is 3419400.0000, 
raw observation next is [3.0, 49.0, 109.6666666666667, 795.6666666666666, 19.0, 20.65655585976879, -0.7444969242600138, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.3655555555555557, 0.8791896869244935, 0.08333333333333333, 0.22137965498073253, 0.2518343585799954, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.422746], dtype=float32), 0.19724585]. 
=============================================
[2019-04-04 05:20:40,134] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.7524772e-02 8.7996316e-01 7.2135629e-07 8.6892271e-07 3.8977618e-05
 3.2431114e-02 4.0455394e-05], sum to 1.0000
[2019-04-04 05:20:40,134] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9076
[2019-04-04 05:20:40,170] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.5, 58.0, 95.0, 579.3333333333334, 19.0, 20.16836461598389, -0.9020370232051885, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3402600.0000, 
sim time next is 3403200.0000, 
raw observation next is [-1.110223024625157e-16, 56.00000000000001, 97.0, 618.6666666666667, 19.0, 20.25194519043204, -0.8733859367553226, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.56, 0.3233333333333333, 0.6836095764272561, 0.08333333333333333, 0.18766209920266994, 0.20887135441489246, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0638294], dtype=float32), 1.1132302]. 
=============================================
[2019-04-04 05:20:45,581] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.0963619e-02 8.9192671e-01 1.0555200e-07 1.3556907e-07 1.5198604e-05
 2.7085492e-02 8.7825229e-06], sum to 1.0000
[2019-04-04 05:20:45,582] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6723
[2019-04-04 05:20:45,591] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 52.0, 115.5, 814.5, 19.0, 21.10696367765379, -0.6231389281788772, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3502800.0000, 
sim time next is 3503400.0000, 
raw observation next is [2.166666666666667, 51.5, 115.3333333333333, 811.6666666666666, 19.0, 21.23920465860445, -0.5981919286963726, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5226223453370269, 0.515, 0.3844444444444443, 0.8968692449355432, 0.08333333333333333, 0.2699337215503708, 0.30060269043454246, 1.0, 1.0, 0.0], 
reward next is 0.0181, 
noisyNet noise sample is [array([1.2767051], dtype=float32), -1.0721986]. 
=============================================
[2019-04-04 05:20:46,481] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 94028: loss 34.8073
[2019-04-04 05:20:46,485] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 94028: learning rate 0.0000
[2019-04-04 05:20:47,163] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.83426866e-02 9.56995070e-01 2.83069013e-09 1.40913095e-08
 1.15149749e-06 1.46590685e-02 1.97535178e-06], sum to 1.0000
[2019-04-04 05:20:47,166] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0768
[2019-04-04 05:20:47,176] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.833333333333333, 61.0, 0.0, 0.0, 19.0, 18.93455587192459, -1.023362350169011, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3541800.0000, 
sim time next is 3542400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 18.95694554934295, -1.027725084770025, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.0797454624452459, 0.157424971743325, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7967718], dtype=float32), 1.1489996]. 
=============================================
[2019-04-04 05:20:48,401] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 95093: loss 20.9682
[2019-04-04 05:20:48,403] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 95094: learning rate 0.0000
[2019-04-04 05:20:48,412] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 95101: loss 13.7028
[2019-04-04 05:20:48,414] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 95101: learning rate 0.0000
[2019-04-04 05:20:48,839] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 95340: loss 13.8350
[2019-04-04 05:20:48,843] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 95344: learning rate 0.0000
[2019-04-04 05:20:50,160] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 96000: loss 24.0066
[2019-04-04 05:20:50,161] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 96000: learning rate 0.0000
[2019-04-04 05:20:50,174] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 96006: loss 19.5270
[2019-04-04 05:20:50,181] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 96006: learning rate 0.0000
[2019-04-04 05:20:50,308] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 96095: loss 24.3696
[2019-04-04 05:20:50,313] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 96097: learning rate 0.0000
[2019-04-04 05:20:50,319] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6000, global step 96100: loss 13.4888
[2019-04-04 05:20:50,322] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6000, global step 96100: learning rate 0.0000
[2019-04-04 05:20:50,377] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 96141: loss 24.4309
[2019-04-04 05:20:50,381] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 96141: learning rate 0.0000
[2019-04-04 05:20:50,659] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 96296: loss 23.3817
[2019-04-04 05:20:50,662] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 96296: learning rate 0.0000
[2019-04-04 05:20:51,077] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 96534: loss 21.8911
[2019-04-04 05:20:51,079] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 96535: learning rate 0.0000
[2019-04-04 05:20:51,081] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96537: loss 11.7023
[2019-04-04 05:20:51,084] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96538: learning rate 0.0000
[2019-04-04 05:20:51,146] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.2331106e-02 8.6928970e-01 1.4951962e-06 3.2588521e-06 1.3346769e-04
 4.8096895e-02 1.4417403e-04], sum to 1.0000
[2019-04-04 05:20:51,151] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8479
[2019-04-04 05:20:51,179] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.833333333333333, 54.16666666666666, 113.0, 796.6666666666667, 19.0, 19.57384807355209, -0.9116522426258987, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3582600.0000, 
sim time next is 3583200.0000, 
raw observation next is [-3.666666666666667, 54.33333333333334, 113.5, 806.3333333333333, 19.0, 19.54691773135126, -0.9158364870844163, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.3610341643582641, 0.5433333333333334, 0.37833333333333335, 0.8909760589318599, 0.08333333333333333, 0.12890981094593842, 0.19472117097186123, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5995665], dtype=float32), 0.29954165]. 
=============================================
[2019-04-04 05:20:51,424] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 96746: loss 15.2841
[2019-04-04 05:20:51,425] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 96746: learning rate 0.0000
[2019-04-04 05:20:51,850] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0011253e-01 8.4523743e-01 4.0668601e-07 1.1239889e-06 7.1167633e-05
 5.4483522e-02 9.3840456e-05], sum to 1.0000
[2019-04-04 05:20:51,856] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6716
[2019-04-04 05:20:51,861] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 19.55509751760624, -1.000768052843196, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3618000.0000, 
sim time next is 3618600.0000, 
raw observation next is [-1.166666666666667, 43.33333333333333, 0.0, 0.0, 19.0, 19.51928866535828, -1.013536925944757, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.43028624192059095, 0.4333333333333333, 0.0, 0.0, 0.08333333333333333, 0.1266073887798568, 0.1621543580184143, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3471853], dtype=float32), -0.5282476]. 
=============================================
[2019-04-04 05:20:51,975] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 97111: loss 24.6077
[2019-04-04 05:20:51,976] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 97111: learning rate 0.0000
[2019-04-04 05:20:52,168] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 97241: loss 22.7762
[2019-04-04 05:20:52,172] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 97241: learning rate 0.0000
[2019-04-04 05:20:52,438] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7126986e-01 8.0985183e-01 7.5114417e-07 1.3417745e-06 8.7079672e-05
 1.8715633e-02 7.3476564e-05], sum to 1.0000
[2019-04-04 05:20:52,440] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0748
[2019-04-04 05:20:52,447] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 19.0, 18.891122476409, -1.186312862250142, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3634200.0000, 
sim time next is 3634800.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 19.0, 18.8515180031987, -1.194774123668318, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.7119113573407203, 0.25, 0.0, 0.0, 0.08333333333333333, 0.07095983359989155, 0.10174195877722732, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7614397], dtype=float32), 0.82431924]. 
=============================================
[2019-04-04 05:20:52,815] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 97669: loss 13.7553
[2019-04-04 05:20:52,815] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 97669: learning rate 0.0000
[2019-04-04 05:20:56,666] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9229595e-01 7.6263523e-01 4.4434099e-08 1.5147332e-07 3.2272823e-05
 4.5004275e-02 3.2008942e-05], sum to 1.0000
[2019-04-04 05:20:56,667] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0682
[2019-04-04 05:20:56,675] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [4.0, 59.0, 23.16666666666666, 224.5, 20.0, 20.28008800258386, -0.810061831781857, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3691200.0000, 
sim time next is 3691800.0000, 
raw observation next is [4.0, 59.0, 15.0, 165.0, 19.0, 20.22936600115735, -0.8237660729835712, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.05, 0.18232044198895028, 0.08333333333333333, 0.18578050009644573, 0.22541130900547626, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54562825], dtype=float32), -0.60654044]. 
=============================================
[2019-04-04 05:20:56,699] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-04 05:20:56,713] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:20:56,714] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:20:56,714] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:20:56,715] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:20:56,716] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:20:56,717] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:20:56,720] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run2
[2019-04-04 05:20:56,733] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run2
[2019-04-04 05:20:56,734] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run2
[2019-04-04 05:21:52,414] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.10061596], dtype=float32), 0.09043367]
[2019-04-04 05:21:52,415] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [9.5, 41.0, 122.5, 792.5, 19.0, 21.11610338460396, -0.6578110446351028, 1.0, 1.0, 0.0]
[2019-04-04 05:21:52,415] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:21:52,416] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.3131045e-01 8.3215123e-01 1.8081427e-07 2.0664177e-07 5.1492643e-05
 3.6462467e-02 2.3922154e-05], sampled 0.8096290357967993
[2019-04-04 05:22:33,725] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5688.9334 140659968.8773 -2066.0762
[2019-04-04 05:22:51,472] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5052.6781 170876367.0316 -2762.7723
[2019-04-04 05:23:04,899] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5143.4919 183524163.9020 -2586.3471
[2019-04-04 05:23:05,934] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 100000, evaluation results [100000.0, 5052.678105199641, 170876367.03162834, -2762.7723055058527, 5688.933389336098, 140659968.8772953, -2066.076217951976, 5143.4919022694085, 183524163.90200728, -2586.347057800924]
[2019-04-04 05:23:12,202] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 101760: loss 6.6156
[2019-04-04 05:23:12,214] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 101760: learning rate 0.0000
[2019-04-04 05:23:12,686] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4289939e-02 9.4617170e-01 6.6715877e-09 1.0579452e-08 4.9478781e-06
 9.5326388e-03 6.6338953e-07], sum to 1.0000
[2019-04-04 05:23:12,691] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8645
[2019-04-04 05:23:12,730] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.666666666666667, 46.0, 83.16666666666666, 689.3333333333333, 19.0, 21.59353925523112, -0.5470678051199809, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3858000.0000, 
sim time next is 3858600.0000, 
raw observation next is [2.833333333333333, 45.5, 79.33333333333334, 661.6666666666667, 19.0, 21.68332932269441, -0.5332018282976744, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.541089566020314, 0.455, 0.2644444444444445, 0.7311233885819522, 0.08333333333333333, 0.30694411022453405, 0.32226605723410856, 1.0, 1.0, 0.0], 
reward next is 0.6680, 
noisyNet noise sample is [array([-1.6259019], dtype=float32), -1.0646342]. 
=============================================
[2019-04-04 05:23:15,137] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 102535: loss 7.7093
[2019-04-04 05:23:15,138] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 102535: learning rate 0.0000
[2019-04-04 05:23:16,952] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 102913: loss 5.9371
[2019-04-04 05:23:16,953] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 102913: learning rate 0.0000
[2019-04-04 05:23:17,372] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 103015: loss 10.5724
[2019-04-04 05:23:17,385] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 103015: learning rate 0.0000
[2019-04-04 05:23:20,104] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103906: loss 5.1197
[2019-04-04 05:23:20,106] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103906: learning rate 0.0000
[2019-04-04 05:23:20,134] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 103911: loss 16.2027
[2019-04-04 05:23:20,135] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 103913: learning rate 0.0000
[2019-04-04 05:23:20,538] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6500, global step 104015: loss 20.7481
[2019-04-04 05:23:20,538] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6500, global step 104015: learning rate 0.0000
[2019-04-04 05:23:20,562] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 104018: loss 5.0927
[2019-04-04 05:23:20,563] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 104018: learning rate 0.0000
[2019-04-04 05:23:20,564] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 104018: loss 5.2512
[2019-04-04 05:23:20,595] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 104028: learning rate 0.0000
[2019-04-04 05:23:21,007] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 104133: loss 7.8126
[2019-04-04 05:23:21,007] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 104133: learning rate 0.0000
[2019-04-04 05:23:21,953] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 104402: loss 4.9905
[2019-04-04 05:23:21,971] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 104402: learning rate 0.0000
[2019-04-04 05:23:22,258] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104507: loss 8.4023
[2019-04-04 05:23:22,259] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104507: learning rate 0.0000
[2019-04-04 05:23:22,838] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 104649: loss 4.5722
[2019-04-04 05:23:22,839] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 104649: learning rate 0.0000
[2019-04-04 05:23:24,537] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 105089: loss 5.4948
[2019-04-04 05:23:24,539] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 105090: learning rate 0.0000
[2019-04-04 05:23:24,787] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 105150: loss 4.7063
[2019-04-04 05:23:24,787] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 105150: learning rate 0.0000
[2019-04-04 05:23:25,963] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 105529: loss 5.0094
[2019-04-04 05:23:25,964] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 105529: learning rate 0.0000
[2019-04-04 05:23:29,971] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5168400e-02 9.1903818e-01 7.9692398e-08 1.6945815e-07 6.7462902e-06
 3.5772301e-02 1.4110004e-05], sum to 1.0000
[2019-04-04 05:23:29,971] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1143
[2019-04-04 05:23:30,000] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-13.5, 66.0, 0.0, 0.0, 19.0, 18.42232432238178, -1.25580070428908, 1.0, 1.0, 73190.02330583251], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4001400.0000, 
sim time next is 4002000.0000, 
raw observation next is [-13.33333333333333, 65.0, 15.5, 73.99999999999999, 19.0, 18.4395715080929, -1.225908305582813, 1.0, 1.0, 54045.9049976534], 
processed observation next is [1.0, 0.30434782608695654, 0.09325946445060027, 0.65, 0.051666666666666666, 0.08176795580110496, 0.08333333333333333, 0.03663095900774168, 0.09136389813906232, 1.0, 1.0, 0.2573614523697781], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2461403], dtype=float32), -1.1745803]. 
=============================================
[2019-04-04 05:23:30,114] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[18.055641]
 [18.277437]
 [18.617542]
 [18.56413 ]
 [18.563278]], R is [[17.74489784]
 [17.56744957]
 [17.39177513]
 [17.81731606]
 [18.36064529]].
[2019-04-04 05:23:39,414] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 109691: loss 15.8346
[2019-04-04 05:23:39,428] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 109691: learning rate 0.0000
[2019-04-04 05:23:40,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8655941e-02 9.6113420e-01 3.2335250e-09 6.7428703e-09 3.0602564e-06
 2.0204622e-02 2.2143397e-06], sum to 1.0000
[2019-04-04 05:23:40,272] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3210
[2019-04-04 05:23:40,277] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 40.0, 0.0, 0.0, 19.0, 20.67734845690176, -0.7160331947499228, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4131000.0000, 
sim time next is 4131600.0000, 
raw observation next is [1.666666666666667, 41.0, 0.0, 0.0, 19.0, 20.57507451450831, -0.7325469328665067, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5087719298245615, 0.41, 0.0, 0.0, 0.08333333333333333, 0.2145895428756924, 0.25581768904449775, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.776233], dtype=float32), -1.620452]. 
=============================================
[2019-04-04 05:23:41,405] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 110449: loss 13.4191
[2019-04-04 05:23:41,407] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 110450: learning rate 0.0000
[2019-04-04 05:23:43,083] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 111171: loss 12.3844
[2019-04-04 05:23:43,084] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 111172: learning rate 0.0000
[2019-04-04 05:23:43,854] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 111504: loss 23.0739
[2019-04-04 05:23:43,855] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 111504: learning rate 0.0000
[2019-04-04 05:23:44,093] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.23597324e-01 8.25867116e-01 1.13154783e-06 1.74020943e-06
 1.48234190e-04 5.02739996e-02 1.10395347e-04], sum to 1.0000
[2019-04-04 05:23:44,098] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0246
[2019-04-04 05:23:44,104] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [1.166666666666667, 30.66666666666666, 118.3333333333333, 838.6666666666667, 19.0, 18.90644477332058, -1.061798272490051, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4191000.0000, 
sim time next is 4191600.0000, 
raw observation next is [1.333333333333333, 31.33333333333334, 118.1666666666667, 842.8333333333334, 19.0, 18.89822061138688, -1.057275639476345, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4995383194829178, 0.3133333333333334, 0.393888888888889, 0.9313075506445673, 0.08333333333333333, 0.07485171761557326, 0.14757478684121836, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18468717], dtype=float32), -0.64747286]. 
=============================================
[2019-04-04 05:23:44,568] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 111839: loss 25.7708
[2019-04-04 05:23:44,569] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 111839: learning rate 0.0000
[2019-04-04 05:23:44,658] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111888: loss 14.0466
[2019-04-04 05:23:44,658] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111888: learning rate 0.0000
[2019-04-04 05:23:44,933] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112002: loss 12.7480
[2019-04-04 05:23:44,937] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112004: learning rate 0.0000
[2019-04-04 05:23:45,061] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7000, global step 112077: loss 18.0629
[2019-04-04 05:23:45,062] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7000, global step 112079: learning rate 0.0000
[2019-04-04 05:23:45,415] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 112229: loss 24.0724
[2019-04-04 05:23:45,416] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 112229: learning rate 0.0000
[2019-04-04 05:23:45,783] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112398: loss 12.1538
[2019-04-04 05:23:45,784] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112398: learning rate 0.0000
[2019-04-04 05:23:46,004] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 112469: loss 15.6836
[2019-04-04 05:23:46,004] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 112469: learning rate 0.0000
[2019-04-04 05:23:46,025] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 112477: loss 12.2527
[2019-04-04 05:23:46,026] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 112477: learning rate 0.0000
[2019-04-04 05:23:46,828] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 112846: loss 12.3895
[2019-04-04 05:23:46,829] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 112846: learning rate 0.0000
[2019-04-04 05:23:47,353] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 113109: loss 27.9588
[2019-04-04 05:23:47,354] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 113109: learning rate 0.0000
[2019-04-04 05:23:47,972] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 113401: loss 11.8628
[2019-04-04 05:23:47,973] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 113401: learning rate 0.0000
[2019-04-04 05:23:48,262] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 113546: loss 11.7742
[2019-04-04 05:23:48,264] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 113546: learning rate 0.0000
[2019-04-04 05:23:51,660] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6319938e-02 9.2894936e-01 6.3931188e-10 8.2558804e-09 7.2605275e-07
 3.4726985e-02 2.9148409e-06], sum to 1.0000
[2019-04-04 05:23:51,662] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8241
[2019-04-04 05:23:51,673] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.433333333333334, 75.33333333333334, 0.0, 0.0, 19.0, 19.08578526656012, -1.068588693115826, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4317600.0000, 
sim time next is 4318200.0000, 
raw observation next is [4.45, 75.5, 0.0, 0.0, 19.0, 19.13296542771518, -1.071172844628266, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 1.0, 0.5858725761772854, 0.755, 0.0, 0.0, 0.08333333333333333, 0.09441378564293175, 0.14294238512391133, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00518457], dtype=float32), -0.0004780286]. 
=============================================
[2019-04-04 05:23:52,651] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2733884e-02 9.6152353e-01 1.3063458e-09 1.6456974e-09 9.8156545e-07
 5.7412395e-03 3.8533389e-07], sum to 1.0000
[2019-04-04 05:23:52,653] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6580
[2019-04-04 05:23:52,670] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.3, 38.0, 115.0, 780.0, 19.0, 21.50929879883261, -0.6331622100639156, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4357800.0000, 
sim time next is 4358400.0000, 
raw observation next is [11.73333333333333, 36.66666666666666, 115.8333333333333, 788.0, 19.0, 21.56431784266511, -0.6028091210796593, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7876269621421976, 0.3666666666666666, 0.386111111111111, 0.8707182320441988, 0.08333333333333333, 0.29702648688875904, 0.29906362630678024, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.653993], dtype=float32), 0.5679335]. 
=============================================
[2019-04-04 05:23:55,499] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 117463: loss 1.6149
[2019-04-04 05:23:55,500] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 117463: learning rate 0.0000
[2019-04-04 05:23:56,789] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 118127: loss 2.2509
[2019-04-04 05:23:56,791] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 118127: learning rate 0.0000
[2019-04-04 05:23:58,717] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 119023: loss 0.8838
[2019-04-04 05:23:58,751] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 119029: learning rate 0.0000
[2019-04-04 05:23:59,535] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 119385: loss 0.4203
[2019-04-04 05:23:59,536] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 119385: learning rate 0.0000
[2019-04-04 05:23:59,985] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.1357332e-03 9.8802865e-01 1.5717025e-11 3.8078558e-11 1.3246623e-08
 6.8355803e-03 1.5537822e-08], sum to 1.0000
[2019-04-04 05:23:59,996] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5417
[2019-04-04 05:24:00,013] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 19.21332694336537, -0.9385341839504484, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4485000.0000, 
sim time next is 4485600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 19.2220191797627, -0.9432621456968802, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.10183493164689168, 0.1855792847677066, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6312553], dtype=float32), -0.3446154]. 
=============================================
[2019-04-04 05:24:00,173] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 119666: loss 0.8152
[2019-04-04 05:24:00,174] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 119666: learning rate 0.0000
[2019-04-04 05:24:00,379] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119755: loss 0.8767
[2019-04-04 05:24:00,379] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119756: learning rate 0.0000
[2019-04-04 05:24:00,666] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 119877: loss 2.2735
[2019-04-04 05:24:00,670] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 119880: learning rate 0.0000
[2019-04-04 05:24:01,068] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7500, global step 119999: loss 0.8626
[2019-04-04 05:24:01,069] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7500, global step 120000: learning rate 0.0000
[2019-04-04 05:24:01,654] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120204: loss 0.9748
[2019-04-04 05:24:01,656] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120204: learning rate 0.0000
[2019-04-04 05:24:01,726] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 120227: loss 0.9693
[2019-04-04 05:24:01,735] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 120230: learning rate 0.0000
[2019-04-04 05:24:01,763] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120253: loss 0.9326
[2019-04-04 05:24:01,769] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120255: learning rate 0.0000
[2019-04-04 05:24:02,004] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3839499e-02 9.7773963e-01 6.1039168e-10 1.8399198e-09 3.6326682e-07
 8.4200455e-03 4.5973195e-07], sum to 1.0000
[2019-04-04 05:24:02,006] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0605
[2019-04-04 05:24:02,048] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6666666666666667, 72.83333333333334, 74.00000000000001, 44.00000000000001, 19.0, 18.7662046457299, -1.146819867229112, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4522200.0000, 
sim time next is 4522800.0000, 
raw observation next is [-0.5333333333333334, 72.66666666666667, 92.5, 55.0, 19.0, 18.69056301886896, -1.072890811852838, 1.0, 1.0, 18686.78184283338], 
processed observation next is [1.0, 0.34782608695652173, 0.44783010156971376, 0.7266666666666667, 0.30833333333333335, 0.06077348066298342, 0.08333333333333333, 0.05754691823908006, 0.14236972938238734, 1.0, 1.0, 0.08898467544206372], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4973995], dtype=float32), 0.011366721]. 
=============================================
[2019-04-04 05:24:02,108] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 120399: loss 0.4921
[2019-04-04 05:24:02,121] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 120413: learning rate 0.0000
[2019-04-04 05:24:02,844] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120721: loss 0.9646
[2019-04-04 05:24:02,846] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120721: learning rate 0.0000
[2019-04-04 05:24:03,984] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 121180: loss 2.2738
[2019-04-04 05:24:03,986] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 121180: learning rate 0.0000
[2019-04-04 05:24:04,873] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 121510: loss 0.6935
[2019-04-04 05:24:04,874] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 121511: learning rate 0.0000
[2019-04-04 05:24:05,135] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 121625: loss 2.4621
[2019-04-04 05:24:05,143] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 121625: learning rate 0.0000
[2019-04-04 05:24:07,791] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.1963190e-03 9.9046701e-01 4.7243287e-10 1.1540146e-09 1.6651300e-07
 4.3361713e-03 3.2944587e-07], sum to 1.0000
[2019-04-04 05:24:07,794] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8812
[2019-04-04 05:24:07,864] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 71.0, 82.00000000000001, 114.0, 19.0, 19.44879176797414, -1.01126043953116, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4608600.0000, 
sim time next is 4609200.0000, 
raw observation next is [-2.0, 71.0, 102.5, 142.5, 19.0, 19.55074864803789, -0.9493502103329785, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.3416666666666667, 0.1574585635359116, 0.08333333333333333, 0.1292290540031574, 0.18354992988900717, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0998843], dtype=float32), -0.6158371]. 
=============================================
[2019-04-04 05:24:10,152] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.0859596e-03 9.8476911e-01 3.8090361e-11 7.3945301e-11 1.5716128e-07
 7.1447752e-03 2.1612799e-08], sum to 1.0000
[2019-04-04 05:24:10,152] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0453
[2019-04-04 05:24:10,189] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.0, 43.0, 156.0, 138.0, 19.0, 23.19127561183473, -0.2931861416732311, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4636800.0000, 
sim time next is 4637400.0000, 
raw observation next is [5.866666666666667, 43.5, 143.0, 141.0, 19.0, 22.97252354435917, -0.2235519584780459, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6251154201292707, 0.435, 0.4766666666666667, 0.1558011049723757, 0.08333333333333333, 0.41437696202993085, 0.425482680507318, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.77999616], dtype=float32), 0.99583656]. 
=============================================
[2019-04-04 05:24:15,693] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 125582: loss 20.8830
[2019-04-04 05:24:15,714] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 125584: learning rate 0.0000
[2019-04-04 05:24:17,042] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 126181: loss 10.2363
[2019-04-04 05:24:17,044] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 126181: learning rate 0.0000
[2019-04-04 05:24:17,581] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7449449e-03 9.8663574e-01 5.9576823e-11 1.5874764e-10 6.8916407e-08
 9.6192816e-03 5.0612734e-08], sum to 1.0000
[2019-04-04 05:24:17,588] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4710
[2019-04-04 05:24:17,607] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 72.0, 88.16666666666667, 13.83333333333333, 19.0, 19.61541214998542, -0.9711515160169782, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4725600.0000, 
sim time next is 4726200.0000, 
raw observation next is [1.0, 72.0, 76.33333333333334, 16.66666666666666, 19.0, 19.86504532520826, -0.9527503328814803, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.72, 0.2544444444444445, 0.018416206261510124, 0.08333333333333333, 0.15542044376735506, 0.18241655570617324, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7471578], dtype=float32), 0.26510623]. 
=============================================
[2019-04-04 05:24:20,045] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 127374: loss 8.9300
[2019-04-04 05:24:20,047] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 127375: learning rate 0.0000
[2019-04-04 05:24:20,773] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 127645: loss 9.6327
[2019-04-04 05:24:20,774] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 127645: learning rate 0.0000
[2019-04-04 05:24:20,862] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 127679: loss 9.7835
[2019-04-04 05:24:20,869] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 127681: learning rate 0.0000
[2019-04-04 05:24:21,006] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 127741: loss 8.9823
[2019-04-04 05:24:21,007] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 127743: learning rate 0.0000
[2019-04-04 05:24:21,025] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 127748: loss 9.9175
[2019-04-04 05:24:21,027] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 127748: learning rate 0.0000
[2019-04-04 05:24:22,126] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8000, global step 128204: loss 9.5375
[2019-04-04 05:24:22,140] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8000, global step 128204: learning rate 0.0000
[2019-04-04 05:24:22,172] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 128220: loss 9.5230
[2019-04-04 05:24:22,203] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 128220: learning rate 0.0000
[2019-04-04 05:24:22,407] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128301: loss 17.2649
[2019-04-04 05:24:22,409] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128302: learning rate 0.0000
[2019-04-04 05:24:22,466] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 128329: loss 12.2510
[2019-04-04 05:24:22,482] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 128331: learning rate 0.0000
[2019-04-04 05:24:22,595] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 128358: loss 18.6720
[2019-04-04 05:24:22,617] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 128375: learning rate 0.0000
[2019-04-04 05:24:23,033] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3477340e-02 9.5847720e-01 3.1332085e-08 6.4918929e-08 3.1910927e-06
 1.8032487e-02 9.6807180e-06], sum to 1.0000
[2019-04-04 05:24:23,037] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8070
[2019-04-04 05:24:23,068] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 19.0, 18.6187603891203, -1.172160814148849, 0.0, 1.0, 66023.63860635064], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4851000.0000, 
sim time next is 4851600.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 19.0, 18.67104170383526, -1.165720255390422, 0.0, 1.0, 18723.05188225858], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.6, 0.0, 0.0, 0.08333333333333333, 0.055920141986271744, 0.111426581536526, 0.0, 1.0, 0.08915738991551704], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.94216377], dtype=float32), -1.3462666]. 
=============================================
[2019-04-04 05:24:23,667] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 128849: loss 9.7681
[2019-04-04 05:24:23,668] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 128849: learning rate 0.0000
[2019-04-04 05:24:24,316] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 129116: loss 9.5870
[2019-04-04 05:24:24,322] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 129116: learning rate 0.0000
[2019-04-04 05:24:24,965] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 129391: loss 8.6982
[2019-04-04 05:24:24,967] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 129392: learning rate 0.0000
[2019-04-04 05:24:25,312] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 129541: loss 9.4009
[2019-04-04 05:24:25,314] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 129541: learning rate 0.0000
[2019-04-04 05:24:28,874] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.8265894e-03 9.7392684e-01 3.8108756e-09 1.6367855e-08 1.6098168e-06
 1.6244000e-02 1.0313385e-06], sum to 1.0000
[2019-04-04 05:24:28,875] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4896
[2019-04-04 05:24:28,895] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 45.0, 187.3333333333333, 406.0, 19.0, 18.93343225799022, -1.060591815513661, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4893000.0000, 
sim time next is 4893600.0000, 
raw observation next is [3.0, 45.0, 175.1666666666667, 414.0, 19.0, 18.94900562276873, -1.05748807686339, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.583888888888889, 0.4574585635359116, 0.08333333333333333, 0.07908380189739421, 0.14750397437886997, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2174704], dtype=float32), -0.98083586]. 
=============================================
[2019-04-04 05:24:31,537] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5673858e-03 9.9331951e-01 2.9038250e-10 5.5924132e-10 1.4419456e-07
 4.1127936e-03 1.2159515e-07], sum to 1.0000
[2019-04-04 05:24:31,538] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0398
[2019-04-04 05:24:31,601] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.0, 18.77147357938611, -1.197126847088032, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4947000.0000, 
sim time next is 4947600.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 19.0, 18.86305054104855, -1.202042228247304, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.08333333333333333, 0.0719208784207126, 0.09931925725089867, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3053753], dtype=float32), 0.821832]. 
=============================================
[2019-04-04 05:24:32,801] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.4624099e-03 9.7874373e-01 1.6775655e-09 5.5318368e-09 5.2656247e-07
 1.5792800e-02 5.8878965e-07], sum to 1.0000
[2019-04-04 05:24:32,801] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4646
[2019-04-04 05:24:32,823] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.5, 42.5, 93.0, 560.0, 19.0, 18.98949002618755, -1.112336800130795, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4955400.0000, 
sim time next is 4956000.0000, 
raw observation next is [-1.333333333333333, 41.33333333333334, 95.5, 586.1666666666666, 19.0, 19.25697664849752, -1.088319774725337, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.42566943674976926, 0.41333333333333344, 0.31833333333333336, 0.6476979742173112, 0.08333333333333333, 0.10474805404146004, 0.137226741758221, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7033408], dtype=float32), -0.50553566]. 
=============================================
[2019-04-04 05:24:32,869] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[24.204374]
 [24.150076]
 [24.351374]
 [24.54088 ]
 [24.827763]], R is [[24.01328659]
 [23.77315331]
 [23.53542137]
 [23.3000679 ]
 [23.0670681 ]].
[2019-04-04 05:24:33,348] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0626944e-02 9.7681671e-01 1.4760330e-09 9.1146390e-10 3.9369968e-07
 1.2555769e-02 1.3960508e-07], sum to 1.0000
[2019-04-04 05:24:33,352] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7187
[2019-04-04 05:24:33,392] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.6666666666666667, 31.5, 111.0, 745.6666666666667, 19.0, 20.55833411043072, -0.8574565264367663, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4960200.0000, 
sim time next is 4960800.0000, 
raw observation next is [1.0, 30.0, 112.5, 760.0, 19.0, 20.67981136587954, -0.8341942536808453, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.3, 0.375, 0.8397790055248618, 0.08333333333333333, 0.22331761382329507, 0.22193524877305157, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.06117193], dtype=float32), -0.084110424]. 
=============================================
[2019-04-04 05:24:39,564] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1110416e-03 9.9532038e-01 3.8228342e-14 1.7647433e-13 6.3039907e-10
 3.5685608e-03 2.8568647e-10], sum to 1.0000
[2019-04-04 05:24:39,565] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9149
[2019-04-04 05:24:39,577] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.833333333333334, 19.0, 0.0, 0.0, 19.0, 23.16016145003768, -0.1197872471763108, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5083800.0000, 
sim time next is 5084400.0000, 
raw observation next is [9.666666666666668, 19.0, 0.0, 0.0, 19.0, 23.09300351704698, -0.1310285196405402, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7303785780240075, 0.19, 0.0, 0.0, 0.08333333333333333, 0.4244169597539151, 0.45632382678648664, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2466936], dtype=float32), 0.037003677]. 
=============================================
[2019-04-04 05:24:40,667] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:40,667] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:40,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run2
[2019-04-04 05:24:40,747] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3089819e-03 9.9697745e-01 4.3248187e-12 5.6000625e-12 2.2460276e-09
 1.7135427e-03 5.7646052e-09], sum to 1.0000
[2019-04-04 05:24:40,749] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0645
[2019-04-04 05:24:40,830] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.333333333333333, 40.0, 0.0, 0.0, 19.0, 19.26712296939731, -1.019323763344863, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5013600.0000, 
sim time next is 5014200.0000, 
raw observation next is [1.166666666666667, 40.0, 0.0, 0.0, 19.0, 19.24036211129533, -1.024252312004926, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.49492151431209613, 0.4, 0.0, 0.0, 0.08333333333333333, 0.1033635092746108, 0.15858256266502466, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4614791], dtype=float32), 0.2638985]. 
=============================================
[2019-04-04 05:24:41,735] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:41,735] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:41,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run2
[2019-04-04 05:24:46,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:46,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:46,395] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run2
[2019-04-04 05:24:48,834] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:48,834] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:48,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run2
[2019-04-04 05:24:49,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:49,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:49,359] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run2
[2019-04-04 05:24:49,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:49,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:49,787] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run2
[2019-04-04 05:24:49,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:49,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:49,882] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run2
[2019-04-04 05:24:50,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:50,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:50,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run2
[2019-04-04 05:24:50,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:50,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:50,583] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run2
[2019-04-04 05:24:50,666] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:50,667] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:50,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run2
[2019-04-04 05:24:50,760] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:50,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:50,763] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run2
[2019-04-04 05:24:50,782] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:50,782] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:50,784] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run2
[2019-04-04 05:24:52,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:52,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:52,383] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run2
[2019-04-04 05:24:52,750] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:52,751] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:52,752] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run2
[2019-04-04 05:24:53,534] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1779324e-02 9.6681279e-01 3.1079663e-11 1.1714073e-10 2.0126175e-08
 2.1407753e-02 1.3503477e-07], sum to 1.0000
[2019-04-04 05:24:53,534] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9476
[2019-04-04 05:24:53,557] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.616666666666667, 86.5, 0.0, 0.0, 19.0, 18.13141170397479, -1.206567932519005, 0.0, 1.0, 25154.5508771593], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 69000.0000, 
sim time next is 69600.0000, 
raw observation next is [3.433333333333334, 87.0, 0.0, 0.0, 19.0, 18.14715134057281, -1.19202637072009, 0.0, 1.0, 196217.9094192961], 
processed observation next is [0.0, 0.8260869565217391, 0.5577100646352725, 0.87, 0.0, 0.0, 0.08333333333333333, 0.01226261171440098, 0.10265787642663667, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([1.3072603], dtype=float32), -0.42817515]. 
=============================================
[2019-04-04 05:24:53,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:53,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:53,930] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run2
[2019-04-04 05:24:53,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:24:53,990] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:24:53,992] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run2
[2019-04-04 05:25:00,418] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0949898e-02 9.8192918e-01 7.3390512e-12 1.0160507e-10 2.1133808e-08
 7.1209236e-03 3.6118202e-08], sum to 1.0000
[2019-04-04 05:25:00,418] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7952
[2019-04-04 05:25:00,426] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.533333333333333, 86.0, 80.33333333333334, 0.0, 19.0, 18.36969520225293, -1.154793800925175, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 51600.0000, 
sim time next is 52200.0000, 
raw observation next is [7.45, 86.0, 79.0, 0.0, 19.0, 18.3630155484688, -1.150667436405909, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6689750692520776, 0.86, 0.2633333333333333, 0.0, 0.08333333333333333, 0.030251295705733188, 0.11644418786469697, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2848334], dtype=float32), -1.2059125]. 
=============================================
[2019-04-04 05:25:00,508] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7440126e-03 9.8762417e-01 4.5418672e-11 1.7081722e-10 9.2681223e-09
 1.0631715e-02 6.6882500e-08], sum to 1.0000
[2019-04-04 05:25:00,508] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2060
[2019-04-04 05:25:00,529] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.9, 76.66666666666667, 0.0, 0.0, 19.0, 18.69758751847357, -1.256285887010097, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 182400.0000, 
sim time next is 183000.0000, 
raw observation next is [-8.9, 77.33333333333333, 0.0, 0.0, 19.0, 18.59272834443814, -1.265313518862963, 0.0, 1.0, 35989.28007489999], 
processed observation next is [1.0, 0.08695652173913043, 0.21606648199445982, 0.7733333333333333, 0.0, 0.0, 0.08333333333333333, 0.04939402870317829, 0.07822882704567902, 0.0, 1.0, 0.17137752416619043], 
reward next is 0.8286, 
noisyNet noise sample is [array([1.2946098], dtype=float32), 0.10867956]. 
=============================================
[2019-04-04 05:25:00,546] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[29.053886]
 [29.500957]
 [29.851166]
 [30.370987]
 [30.64259 ]], R is [[29.22333717]
 [29.93110466]
 [30.63179398]
 [30.96513367]
 [31.44711113]].
[2019-04-04 05:25:43,744] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3304122e-03 9.9366522e-01 6.3453756e-13 2.6316557e-12 2.6308318e-09
 5.0044069e-03 2.5427209e-09], sum to 1.0000
[2019-04-04 05:25:43,744] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4713
[2019-04-04 05:25:43,771] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-13.56666666666667, 78.0, 0.0, 0.0, 19.0, 18.40823782136847, -1.277261204848308, 0.0, 1.0, 65652.34169033838], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 339600.0000, 
sim time next is 340200.0000, 
raw observation next is [-13.65, 76.0, 0.0, 0.0, 19.0, 18.44163446621511, -1.275035766479628, 0.0, 1.0, 39024.48549724666], 
processed observation next is [1.0, 0.9565217391304348, 0.08448753462603877, 0.76, 0.0, 0.0, 0.08333333333333333, 0.036802872184592474, 0.07498807784012403, 0.0, 1.0, 0.18583088332022218], 
reward next is 0.8142, 
noisyNet noise sample is [array([-0.29853618], dtype=float32), 0.03828192]. 
=============================================
[2019-04-04 05:25:53,777] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9995302e-03 9.9421352e-01 3.2466162e-11 1.9774599e-11 1.5440262e-08
 3.7869043e-03 9.0055376e-09], sum to 1.0000
[2019-04-04 05:25:53,778] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1802
[2019-04-04 05:25:53,811] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.4, 45.33333333333334, 0.0, 0.0, 19.0, 18.49569655361141, -1.264192674441696, 0.0, 1.0, 122512.2543792218], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 420000.0000, 
sim time next is 420600.0000, 
raw observation next is [-10.5, 46.16666666666667, 0.0, 0.0, 19.0, 18.48587086448135, -1.256783729702279, 0.0, 1.0, 82280.0823531861], 
processed observation next is [1.0, 0.8695652173913043, 0.17174515235457063, 0.4616666666666667, 0.0, 0.0, 0.08333333333333333, 0.040489238706779083, 0.08107209009924032, 0.0, 1.0, 0.3918099159675528], 
reward next is 0.6082, 
noisyNet noise sample is [array([-0.04721757], dtype=float32), -1.688965]. 
=============================================
[2019-04-04 05:25:56,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7135417e-03 9.9239606e-01 1.0304134e-09 1.1271811e-09 3.6742748e-07
 3.8896676e-03 3.0866374e-07], sum to 1.0000
[2019-04-04 05:25:56,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9307
[2019-04-04 05:25:56,801] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.766666666666667, 30.66666666666667, 92.0, 0.0, 19.0, 19.7345429126587, -1.140485926900843, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 469200.0000, 
sim time next is 469800.0000, 
raw observation next is [-3.4, 30.0, 98.0, 0.0, 19.0, 19.82057845565924, -1.142882087795997, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.368421052631579, 0.3, 0.32666666666666666, 0.0, 0.08333333333333333, 0.15171487130493677, 0.11903930406800096, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5865665], dtype=float32), -0.9209141]. 
=============================================
[2019-04-04 05:26:08,325] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1249014e-03 9.8239893e-01 4.7968914e-11 2.8530694e-10 4.4412403e-08
 1.2476004e-02 1.5464744e-07], sum to 1.0000
[2019-04-04 05:26:08,327] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6136
[2019-04-04 05:26:08,366] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 19.0, 18.42505292178442, -1.20368319060124, 0.0, 1.0, 28209.1560247825], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 562800.0000, 
sim time next is 563400.0000, 
raw observation next is [-1.0, 80.5, 130.0, 396.0, 19.0, 18.44121156768507, -1.194135243360762, 0.0, 1.0, 28223.49169726744], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.805, 0.43333333333333335, 0.4375690607734807, 0.08333333333333333, 0.03676763064042247, 0.10195491887974602, 0.0, 1.0, 0.13439757951079734], 
reward next is 0.8656, 
noisyNet noise sample is [array([0.32309735], dtype=float32), 1.2941217]. 
=============================================
[2019-04-04 05:26:11,057] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7925919e-03 9.8786354e-01 1.2908369e-12 1.5949174e-11 7.0903847e-09
 1.0343828e-02 1.8139048e-08], sum to 1.0000
[2019-04-04 05:26:11,057] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4333
[2019-04-04 05:26:11,105] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 19.0, 18.70214415882973, -1.185932129990747, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 588000.0000, 
sim time next is 588600.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 19.0, 18.69824960654801, -1.183051486261497, 0.0, 1.0, 111307.1693284006], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.08333333333333333, 0.05818746721233422, 0.10564950457950102, 0.0, 1.0, 0.5300341396590504], 
reward next is 0.4700, 
noisyNet noise sample is [array([-1.0443379], dtype=float32), 1.0701993]. 
=============================================
[2019-04-04 05:26:12,012] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.7650568e-04 9.9561286e-01 7.1525365e-13 8.3927492e-12 4.3762713e-09
 3.5106267e-03 6.8208603e-09], sum to 1.0000
[2019-04-04 05:26:12,014] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4837
[2019-04-04 05:26:12,071] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 19.0, 18.6762808599955, -1.202728155668298, 0.0, 1.0, 66770.56439962055], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 597000.0000, 
sim time next is 597600.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 19.0, 18.64767119511906, -1.200721574126423, 0.0, 1.0, 67632.9346922151], 
processed observation next is [0.0, 0.9565217391304348, 0.38504155124653744, 0.83, 0.0, 0.0, 0.08333333333333333, 0.053972599593254955, 0.09975947529119233, 0.0, 1.0, 0.32206159377245286], 
reward next is 0.6779, 
noisyNet noise sample is [array([0.34682995], dtype=float32), -1.6694826]. 
=============================================
[2019-04-04 05:26:28,848] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.4634887e-04 9.9698228e-01 4.3778875e-13 7.8303125e-13 8.4515622e-10
 2.3713962e-03 1.7504914e-09], sum to 1.0000
[2019-04-04 05:26:28,848] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1139
[2019-04-04 05:26:28,862] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.699999999999999, 82.0, 0.0, 0.0, 19.0, 19.01574495777317, -1.080132924702639, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 963600.0000, 
sim time next is 964200.0000, 
raw observation next is [7.7, 82.5, 0.0, 0.0, 19.0, 19.05709708665504, -1.069328357775826, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.825, 0.0, 0.0, 0.08333333333333333, 0.08809142388792009, 0.1435572140747247, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5629993], dtype=float32), 0.6448464]. 
=============================================
[2019-04-04 05:26:50,954] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0320964e-04 9.9873608e-01 1.8192231e-15 5.6265633e-15 3.0666927e-11
 1.1607066e-03 1.8846522e-11], sum to 1.0000
[2019-04-04 05:26:50,959] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2813
[2019-04-04 05:26:50,977] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 19.0, 19.51921469594007, -0.9134461652559126, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1018800.0000, 
sim time next is 1019400.0000, 
raw observation next is [14.4, 80.33333333333333, 0.0, 0.0, 19.0, 19.63429431515651, -0.9123211246922436, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.8033333333333332, 0.0, 0.0, 0.08333333333333333, 0.13619119292970913, 0.1958929584359188, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29830387], dtype=float32), -0.47935352]. 
=============================================
[2019-04-04 05:26:53,797] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0728629e-04 9.9784708e-01 2.1689837e-13 2.5607281e-13 2.4777461e-10
 1.4455841e-03 4.3330889e-10], sum to 1.0000
[2019-04-04 05:26:53,797] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8467
[2019-04-04 05:26:53,828] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.76666666666667, 73.33333333333334, 137.3333333333333, 35.83333333333333, 19.0, 20.98975527563296, -0.651239487174507, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1075200.0000, 
sim time next is 1075800.0000, 
raw observation next is [15.13333333333333, 71.66666666666667, 160.6666666666667, 71.66666666666666, 19.0, 21.06174484001611, -0.6344697065595496, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8818097876269622, 0.7166666666666667, 0.5355555555555557, 0.07918968692449355, 0.08333333333333333, 0.2551454033346759, 0.28851009781348347, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.846491], dtype=float32), -0.34814197]. 
=============================================
[2019-04-04 05:26:54,754] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.6812810e-04 9.9867767e-01 2.7303802e-15 3.3644918e-14 1.2985747e-11
 8.5424638e-04 1.7203197e-10], sum to 1.0000
[2019-04-04 05:26:54,754] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1847
[2019-04-04 05:26:54,764] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.8, 78.0, 0.0, 0.0, 19.0, 19.1323042167865, -1.004412934867689, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1044000.0000, 
sim time next is 1044600.0000, 
raw observation next is [13.9, 77.83333333333333, 0.0, 0.0, 19.0, 19.151004843401, -0.9962330657082633, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.847645429362881, 0.7783333333333333, 0.0, 0.0, 0.08333333333333333, 0.09591707028341674, 0.1679223114305789, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5878566], dtype=float32), 0.08679956]. 
=============================================
[2019-04-04 05:26:58,823] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2206978e-04 9.9945801e-01 1.3819155e-15 1.1020707e-14 1.4245891e-11
 3.1993818e-04 2.5973911e-11], sum to 1.0000
[2019-04-04 05:26:58,824] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0804
[2019-04-04 05:26:58,858] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [10.41666666666667, 77.33333333333334, 0.0, 0.0, 19.0, 19.76102875018787, -0.7858303338002618, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1127400.0000, 
sim time next is 1128000.0000, 
raw observation next is [10.33333333333333, 77.66666666666667, 0.0, 0.0, 19.0, 19.72146543264325, -0.7934169255373834, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.7488457987072946, 0.7766666666666667, 0.0, 0.0, 0.08333333333333333, 0.1434554527202708, 0.23552769148753885, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.93212473], dtype=float32), 1.5189497]. 
=============================================
[2019-04-04 05:26:58,924] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[41.193718]
 [41.954803]
 [42.913765]
 [43.408375]
 [43.944695]], R is [[41.15060043]
 [41.73909378]
 [42.32170486]
 [42.89848709]
 [43.4695015 ]].
[2019-04-04 05:27:06,775] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2058443e-02 9.7323507e-01 7.4606515e-10 2.5125131e-09 1.1458386e-07
 1.4705952e-02 4.7857930e-07], sum to 1.0000
[2019-04-04 05:27:06,775] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3939
[2019-04-04 05:27:06,813] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.25, 94.5, 0.0, 0.0, 19.0, 19.81972458518894, -0.7484963832218758, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1225800.0000, 
sim time next is 1226400.0000, 
raw observation next is [15.16666666666667, 95.0, 0.0, 0.0, 19.0, 19.80414856220969, -0.751283854316303, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8827331486611266, 0.95, 0.0, 0.0, 0.08333333333333333, 0.15034571351747417, 0.24957204856123236, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36647287], dtype=float32), -0.15937528]. 
=============================================
[2019-04-04 05:27:13,849] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.03665516e-04 9.99628305e-01 2.07243094e-14 6.80431799e-14
 5.70681662e-11 2.68011558e-04 9.43594439e-11], sum to 1.0000
[2019-04-04 05:27:13,849] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6564
[2019-04-04 05:27:13,861] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 92.0, 62.0, 0.0, 19.0, 20.27086549801782, -0.8228186881314729, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1349400.0000, 
sim time next is 1350000.0000, 
raw observation next is [1.1, 92.0, 57.5, 0.0, 19.0, 20.20867080308821, -0.8273234982783393, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.19166666666666668, 0.0, 0.08333333333333333, 0.18405590025735089, 0.2242255005738869, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.78134364], dtype=float32), 1.7912122]. 
=============================================
[2019-04-04 05:27:13,870] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[38.223717]
 [38.14444 ]
 [38.075527]
 [38.00402 ]
 [37.91615 ]], R is [[37.94382095]
 [37.56438446]
 [37.18873978]
 [36.81685257]
 [36.44868469]].
[2019-04-04 05:27:17,037] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.3632708e-04 9.9798089e-01 1.3508491e-13 4.9888056e-13 6.3612132e-10
 1.3827204e-03 5.7949889e-10], sum to 1.0000
[2019-04-04 05:27:17,040] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5600
[2019-04-04 05:27:17,056] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 100.0, 32.0, 0.0, 19.0, 19.46931348812261, -0.9851678865204966, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1414800.0000, 
sim time next is 1415400.0000, 
raw observation next is [-0.5, 99.16666666666667, 36.66666666666667, 0.0, 19.0, 19.56434291493792, -0.9876774638903454, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.44875346260387816, 0.9916666666666667, 0.12222222222222223, 0.0, 0.08333333333333333, 0.13036190957816007, 0.1707741787032182, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.09810232], dtype=float32), 0.02029203]. 
=============================================
[2019-04-04 05:27:21,623] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8346633e-04 9.9737513e-01 6.0279798e-14 4.1924840e-13 4.9498927e-10
 2.4413888e-03 2.3759494e-10], sum to 1.0000
[2019-04-04 05:27:21,625] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6227
[2019-04-04 05:27:21,638] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.2, 94.66666666666667, 0.0, 0.0, 19.0, 18.83416744848418, -1.09063154381336, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1480800.0000, 
sim time next is 1481400.0000, 
raw observation next is [2.2, 95.0, 0.0, 0.0, 19.0, 18.86572774508809, -1.099745358678843, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.95, 0.0, 0.0, 0.08333333333333333, 0.07214397875734087, 0.13341821377371899, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17246537], dtype=float32), -1.0052822]. 
=============================================
[2019-04-04 05:27:22,293] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.7330420e-05 9.9964559e-01 4.9880643e-16 4.4485781e-15 4.7039417e-12
 2.7700909e-04 1.1786683e-11], sum to 1.0000
[2019-04-04 05:27:22,294] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1232
[2019-04-04 05:27:22,302] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.9, 92.0, 0.0, 0.0, 19.0, 18.83690758037405, -1.106731524720623, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1474200.0000, 
sim time next is 1474800.0000, 
raw observation next is [2.0, 92.0, 0.0, 0.0, 19.0, 18.79107209034439, -1.118541033155549, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.92, 0.0, 0.0, 0.08333333333333333, 0.0659226741953658, 0.12715298894815033, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32886934], dtype=float32), 0.16292354]. 
=============================================
[2019-04-04 05:27:27,793] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.0234265e-05 9.9959522e-01 3.2641584e-15 1.9028966e-14 2.3028118e-11
 3.1458502e-04 3.3348681e-11], sum to 1.0000
[2019-04-04 05:27:27,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0556
[2019-04-04 05:27:27,822] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 18.5745194120856, -1.069475693847816, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1655400.0000, 
sim time next is 1656000.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 18.59583868530932, -1.024286128266967, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.049653223775776745, 0.15857129057767763, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([0.51224065], dtype=float32), 0.68320817]. 
=============================================
[2019-04-04 05:27:27,841] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[40.64476 ]
 [40.110535]
 [39.58864 ]
 [39.516636]
 [39.42654 ]], R is [[40.83510971]
 [40.49238586]
 [40.15309143]
 [40.75156021]
 [41.34404373]].
[2019-04-04 05:27:29,583] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0871499e-04 9.9969733e-01 1.7027422e-16 1.3207109e-15 2.8215568e-12
 1.9399884e-04 2.6369319e-12], sum to 1.0000
[2019-04-04 05:27:29,587] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1020
[2019-04-04 05:27:29,600] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.800000000000001, 94.0, 0.0, 0.0, 19.0, 18.97658432317798, -1.001215414455597, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1646400.0000, 
sim time next is 1647000.0000, 
raw observation next is [6.9, 94.5, 0.0, 0.0, 19.0, 18.95352327888621, -0.9966454383868458, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6537396121883658, 0.945, 0.0, 0.0, 0.08333333333333333, 0.07946027324051741, 0.16778485387105138, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01292703], dtype=float32), -1.1731879]. 
=============================================
[2019-04-04 05:27:29,608] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[45.282406]
 [45.84702 ]
 [46.276226]
 [46.837475]
 [47.239918]], R is [[45.4255867 ]
 [45.97133255]
 [46.51161957]
 [47.04650497]
 [47.57604218]].
[2019-04-04 05:27:50,561] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.18567463e-04 9.96794283e-01 1.24931685e-14 1.48910384e-13
 7.92746077e-11 3.08711478e-03 1.09410127e-10], sum to 1.0000
[2019-04-04 05:27:50,564] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7143
[2019-04-04 05:27:50,584] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.600000000000001, 83.0, 0.0, 0.0, 19.0, 18.56219287378615, -1.220983216567054, 0.0, 1.0, 51612.75301499625], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1995000.0000, 
sim time next is 1995600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 18.59217777588961, -1.207966142318685, 0.0, 1.0, 27983.86295132746], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.04934814799080082, 0.09734461922710502, 0.0, 1.0, 0.1332564902444165], 
reward next is 0.8667, 
noisyNet noise sample is [array([0.62969726], dtype=float32), -0.44400403]. 
=============================================
[2019-04-04 05:27:53,016] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1921955e-03 9.9618030e-01 2.6754465e-12 5.7611077e-12 4.6217057e-09
 2.6275667e-03 8.1976994e-09], sum to 1.0000
[2019-04-04 05:27:53,016] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4983
[2019-04-04 05:27:53,027] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.866666666666667, 77.66666666666667, 148.5, 0.0, 19.0, 19.16142137001569, -1.199922856540927, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2029200.0000, 
sim time next is 2029800.0000, 
raw observation next is [-4.683333333333333, 76.33333333333333, 150.0, 0.0, 19.0, 19.14888389546017, -1.200110438372681, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3328716528162512, 0.7633333333333333, 0.5, 0.0, 0.08333333333333333, 0.0957403246216808, 0.09996318720910631, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4108241], dtype=float32), -0.39848912]. 
=============================================
[2019-04-04 05:27:54,304] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7840427e-04 9.9778557e-01 8.9319285e-14 3.1035125e-13 2.9831038e-10
 1.9359342e-03 3.2496306e-10], sum to 1.0000
[2019-04-04 05:27:54,304] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9572
[2019-04-04 05:27:54,340] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.899999999999999, 82.0, 0.0, 0.0, 19.0, 19.07196444409971, -1.187893557592948, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2050800.0000, 
sim time next is 2051400.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 19.0, 18.85182830742632, -1.260831420239953, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.0, 0.0, 0.08333333333333333, 0.07098569228552669, 0.07972285992001564, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8316784], dtype=float32), 1.3789107]. 
=============================================
[2019-04-04 05:28:01,285] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0268069e-05 9.9953032e-01 1.9297848e-16 1.8965726e-15 6.3926763e-12
 4.4945048e-04 7.5100490e-12], sum to 1.0000
[2019-04-04 05:28:01,286] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8767
[2019-04-04 05:28:01,311] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 19.0, 18.49249268531485, -1.203820854778169, 0.0, 1.0, 35423.25587703498], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2156400.0000, 
sim time next is 2157000.0000, 
raw observation next is [-7.3, 82.00000000000001, 0.0, 0.0, 19.0, 18.53941105027364, -1.204139183840578, 0.0, 1.0, 18745.32905473601], 
processed observation next is [1.0, 1.0, 0.26038781163434904, 0.8200000000000002, 0.0, 0.0, 0.08333333333333333, 0.04495092085613658, 0.09862027205314068, 0.0, 1.0, 0.08926347168921908], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.5327167], dtype=float32), 1.5777403]. 
=============================================
[2019-04-04 05:28:01,322] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[46.65711 ]
 [46.439426]
 [46.189205]
 [45.599304]
 [45.17585 ]], R is [[46.94868088]
 [47.31051254]
 [47.43712234]
 [47.38461685]
 [47.38927078]].
[2019-04-04 05:28:01,344] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4885040e-03 9.9364811e-01 2.2768837e-11 9.8588034e-11 1.2567932e-08
 4.8633628e-03 5.3934357e-08], sum to 1.0000
[2019-04-04 05:28:01,345] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9646
[2019-04-04 05:28:01,360] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 69.0, 37.0, 0.0, 19.0, 18.70357503959298, -1.239881052223806, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2363400.0000, 
sim time next is 2364000.0000, 
raw observation next is [-3.4, 69.0, 51.0, 59.99999999999999, 19.0, 18.62688233963337, -1.248365041480854, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.17, 0.06629834254143646, 0.08333333333333333, 0.052240194969447394, 0.08387831950638198, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7109742], dtype=float32), 1.46759]. 
=============================================
[2019-04-04 05:28:01,369] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[32.657867]
 [32.70842 ]
 [32.802998]
 [32.892685]
 [32.80354 ]], R is [[33.26549911]
 [33.93284607]
 [34.5935173 ]
 [35.24758148]
 [35.65869522]].
[2019-04-04 05:28:07,915] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.0608476e-04 9.9505448e-01 4.1763357e-13 2.9466935e-12 8.2054719e-10
 4.5394795e-03 5.4465199e-10], sum to 1.0000
[2019-04-04 05:28:07,915] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8761
[2019-04-04 05:28:07,929] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 19.0, 18.48909832305502, -1.273150263170365, 0.0, 1.0, 31688.37186586768], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2266800.0000, 
sim time next is 2267400.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 19.0, 18.46743060237626, -1.269301038977506, 0.0, 1.0, 49423.46870602354], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.08333333333333333, 0.03895255019802176, 0.0768996536741647, 0.0, 1.0, 0.23534985098106445], 
reward next is 0.7647, 
noisyNet noise sample is [array([-0.51577854], dtype=float32), 0.029160202]. 
=============================================
[2019-04-04 05:28:09,923] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2100934e-04 9.9916053e-01 8.8823886e-14 2.0171110e-13 2.3916616e-10
 5.1851955e-04 2.8637329e-10], sum to 1.0000
[2019-04-04 05:28:09,924] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0813
[2019-04-04 05:28:09,937] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 18.69195709553274, -1.210556953357237, 0.0, 1.0, 20125.94461093267], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2338200.0000, 
sim time next is 2338800.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 19.0, 18.65798278776436, -1.215594700242265, 0.0, 1.0, 44745.70454202637], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.08333333333333333, 0.05483189898036341, 0.0948017665859117, 0.0, 1.0, 0.2130747835334589], 
reward next is 0.7869, 
noisyNet noise sample is [array([0.65442044], dtype=float32), 1.3986809]. 
=============================================
[2019-04-04 05:28:12,437] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 05:28:12,439] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:28:12,439] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:28:12,441] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:28:12,441] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:28:12,442] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run3
[2019-04-04 05:28:12,443] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:28:12,443] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:28:12,467] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run3
[2019-04-04 05:28:12,476] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run3
[2019-04-04 05:28:45,405] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.16885483], dtype=float32), 0.14922875]
[2019-04-04 05:28:45,405] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [8.087332322, 56.98750905666667, 0.0, 0.0, 19.0, 20.26538056857213, -0.7539581424885244, 0.0, 1.0, 0.0]
[2019-04-04 05:28:45,405] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:28:45,406] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.6158508e-04 9.9546921e-01 7.3036317e-14 3.5615995e-13 5.3878269e-10
 3.7691758e-03 6.6009759e-10], sampled 0.4784835156763746
[2019-04-04 05:29:24,131] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5549.5454 143128454.6826 -2187.6983
[2019-04-04 05:29:37,238] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5065.6019 170165733.9032 -2809.9336
[2019-04-04 05:29:42,467] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5091.8053 184888589.7605 -2741.9188
[2019-04-04 05:29:43,490] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 200000, evaluation results [200000.0, 5065.601859255716, 170165733.90322918, -2809.933649904267, 5549.545351728289, 143128454.68258724, -2187.698288516906, 5091.805264953821, 184888589.76047477, -2741.918840158975]
[2019-04-04 05:29:44,310] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1002046e-03 9.9235040e-01 1.2676341e-11 9.3534611e-11 9.5746246e-09
 5.5492590e-03 1.3101121e-08], sum to 1.0000
[2019-04-04 05:29:44,311] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7858
[2019-04-04 05:29:44,360] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 62.0, 153.0, 378.0, 19.0, 18.72658437063251, -1.165446323970345, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2372400.0000, 
sim time next is 2373000.0000, 
raw observation next is [-2.116666666666667, 59.50000000000001, 157.6666666666667, 354.0, 19.0, 18.7059120866657, -1.164902973248859, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.40397045244690677, 0.5950000000000001, 0.5255555555555557, 0.3911602209944751, 0.08333333333333333, 0.05882600722214152, 0.11169900891704702, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6287396], dtype=float32), -1.7911912]. 
=============================================
[2019-04-04 05:29:44,377] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[32.359905]
 [32.39077 ]
 [32.497982]
 [32.615726]
 [32.729507]], R is [[32.98993683]
 [33.66003799]
 [34.32343674]
 [34.98020172]
 [35.63040161]].
[2019-04-04 05:29:58,431] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5447040e-04 9.9547833e-01 1.2830718e-13 8.4199574e-13 4.5696405e-10
 3.9672265e-03 5.5854865e-10], sum to 1.0000
[2019-04-04 05:29:58,434] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7367
[2019-04-04 05:29:58,441] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.2, 60.5, 0.0, 0.0, 19.0, 18.59563042273502, -1.162571065739653, 1.0, 1.0, 54411.87559254889], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2661000.0000, 
sim time next is 2661600.0000, 
raw observation next is [-1.2, 61.00000000000001, 0.0, 0.0, 19.0, 18.6012060133286, -1.16827745292125, 0.0, 1.0, 34849.0322018012], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.6100000000000001, 0.0, 0.0, 0.08333333333333333, 0.05010050111071666, 0.11057418235958331, 0.0, 1.0, 0.16594777238952954], 
reward next is 0.8341, 
noisyNet noise sample is [array([-0.11516146], dtype=float32), 0.97162986]. 
=============================================
[2019-04-04 05:30:00,301] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3277123e-03 9.9631256e-01 1.2673896e-12 4.7964623e-12 2.4940823e-09
 2.3597428e-03 2.0348110e-09], sum to 1.0000
[2019-04-04 05:30:00,301] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3572
[2019-04-04 05:30:00,311] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.666666666666667, 34.66666666666666, 0.0, 0.0, 19.0, 19.32695840951155, -1.118987736562763, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2832000.0000, 
sim time next is 2832600.0000, 
raw observation next is [3.333333333333333, 35.83333333333334, 0.0, 0.0, 19.0, 19.06958516473932, -1.153938253140516, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5549399815327793, 0.35833333333333345, 0.0, 0.0, 0.08333333333333333, 0.08913209706160992, 0.115353915619828, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.38867742], dtype=float32), 1.6369114]. 
=============================================
[2019-04-04 05:30:00,909] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8800970e-04 9.9830073e-01 4.3076857e-14 3.0163272e-13 1.2330621e-10
 1.5112228e-03 2.1752360e-10], sum to 1.0000
[2019-04-04 05:30:00,910] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8357
[2019-04-04 05:30:00,925] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 19.0, 18.39816462984365, -1.20418815141122, 0.0, 1.0, 54734.27561164692], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2685600.0000, 
sim time next is 2686200.0000, 
raw observation next is [-11.31666666666667, 77.16666666666667, 0.0, 0.0, 19.0, 18.54438589619855, -1.196097511460322, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.14912280701754377, 0.7716666666666667, 0.0, 0.0, 0.08333333333333333, 0.04536549134987903, 0.10130082951322598, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09371776], dtype=float32), 1.0354981]. 
=============================================
[2019-04-04 05:30:10,455] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2938073e-04 9.9834108e-01 9.7703218e-15 5.5173134e-14 3.1176110e-11
 1.5295565e-03 2.1035761e-10], sum to 1.0000
[2019-04-04 05:30:10,455] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1173
[2019-04-04 05:30:10,506] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 97.66666666666666, 0.0, 0.0, 19.0, 18.96664663628495, -1.184964100971689, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2871600.0000, 
sim time next is 2872200.0000, 
raw observation next is [1.0, 98.83333333333334, 0.0, 0.0, 19.0, 18.87338246909203, -1.189268308087489, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9883333333333334, 0.0, 0.0, 0.08333333333333333, 0.07278187242433572, 0.10357723063750364, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3941739], dtype=float32), 1.5297194]. 
=============================================
[2019-04-04 05:30:15,189] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2417423e-03 9.7450054e-01 9.6722652e-13 8.1965606e-12 3.1091700e-09
 2.4257733e-02 8.8219965e-09], sum to 1.0000
[2019-04-04 05:30:15,189] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3399
[2019-04-04 05:30:15,202] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.833333333333333, 84.16666666666666, 0.0, 0.0, 19.0, 18.64204284504741, -1.152422946254227, 0.0, 1.0, 21517.82008002005], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2947800.0000, 
sim time next is 2948400.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 19.0, 18.64092281982411, -1.159646830920574, 0.0, 1.0, 27968.11654329405], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.08333333333333333, 0.0534102349853424, 0.11345105635980864, 0.0, 1.0, 0.1331815073490193], 
reward next is 0.8668, 
noisyNet noise sample is [array([0.79343873], dtype=float32), 1.0285869]. 
=============================================
[2019-04-04 05:30:34,124] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2176237e-05 9.9983656e-01 1.0377981e-17 1.5149212e-16 1.1628472e-12
 1.4119290e-04 4.3629317e-13], sum to 1.0000
[2019-04-04 05:30:34,125] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3622
[2019-04-04 05:30:34,144] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.5, 99.16666666666666, 66.66666666666666, 559.0, 19.0, 22.8407284626296, -0.2176289074973443, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3168600.0000, 
sim time next is 3169200.0000, 
raw observation next is [6.4, 99.33333333333334, 62.33333333333333, 529.0, 19.0, 22.91627430374742, -0.3268871458345899, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6398891966759004, 0.9933333333333334, 0.20777777777777776, 0.5845303867403315, 0.08333333333333333, 0.40968952531228514, 0.3910376180551367, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.353817], dtype=float32), -0.3030492]. 
=============================================
[2019-04-04 05:30:43,718] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7421432e-03 9.9143618e-01 1.1939040e-11 7.3406330e-11 2.1087450e-08
 6.8217358e-03 1.5615514e-08], sum to 1.0000
[2019-04-04 05:30:43,718] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0327
[2019-04-04 05:30:43,755] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.166666666666666, 71.16666666666667, 106.3333333333333, 656.6666666666666, 19.0, 20.01437100772188, -0.8951942126069218, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3318600.0000, 
sim time next is 3319200.0000, 
raw observation next is [-8.0, 70.0, 107.5, 677.5, 19.0, 20.13370466555087, -0.8767556371082762, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.24099722991689754, 0.7, 0.35833333333333334, 0.7486187845303868, 0.08333333333333333, 0.17780872212923912, 0.20774812096390793, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5704408], dtype=float32), -0.08172165]. 
=============================================
[2019-04-04 05:30:48,692] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4457539e-04 9.9764532e-01 2.7416654e-12 2.6218001e-12 7.4918525e-09
 1.9100920e-03 1.6872390e-09], sum to 1.0000
[2019-04-04 05:30:48,694] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8331
[2019-04-04 05:30:48,746] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 46.33333333333334, 116.6666666666667, 814.8333333333334, 19.0, 20.52647883210856, -0.8057933766987805, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3414000.0000, 
sim time next is 3414600.0000, 
raw observation next is [3.0, 47.0, 117.0, 817.0, 19.0, 20.28230029232627, -0.8338335987292841, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.47, 0.39, 0.9027624309392265, 0.08333333333333333, 0.1901916910271891, 0.22205546709023863, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3757421], dtype=float32), 0.32296613]. 
=============================================
[2019-04-04 05:30:49,976] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9938623e-05 9.9955887e-01 6.4470669e-16 5.9731518e-15 4.2659687e-12
 4.2118569e-04 4.5188163e-12], sum to 1.0000
[2019-04-04 05:30:49,980] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0639
[2019-04-04 05:30:49,992] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 19.16288740951303, -1.005487603826097, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3361200.0000, 
sim time next is 3361800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 19.14437212083234, -1.014413674107094, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.09536434340269502, 0.1618621086309687, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3740867], dtype=float32), -0.3709969]. 
=============================================
[2019-04-04 05:30:50,165] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3845372e-04 9.9852580e-01 1.2754743e-14 1.2092826e-13 2.6182270e-10
 1.3357450e-03 6.7487307e-11], sum to 1.0000
[2019-04-04 05:30:50,167] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4009
[2019-04-04 05:30:50,185] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.833333333333333, 69.0, 0.0, 0.0, 19.0, 20.34658957077916, -0.8184571296309917, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3435000.0000, 
sim time next is 3435600.0000, 
raw observation next is [1.666666666666667, 71.0, 0.0, 0.0, 19.0, 20.20992911664555, -0.8405580061679815, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5087719298245615, 0.71, 0.0, 0.0, 0.08333333333333333, 0.18416075972046256, 0.21981399794400616, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0820891], dtype=float32), 1.7302331]. 
=============================================
[2019-04-04 05:31:03,961] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5616501e-04 9.8666245e-01 2.1240336e-12 8.1956334e-12 1.8563469e-08
 1.2681349e-02 5.0824585e-09], sum to 1.0000
[2019-04-04 05:31:03,980] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7854
[2019-04-04 05:31:04,010] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 42.0, 99.33333333333333, 773.1666666666667, 19.0, 18.81300514553585, -1.057084254002664, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3595200.0000, 
sim time next is 3595800.0000, 
raw observation next is [-1.0, 42.0, 96.66666666666667, 758.3333333333333, 19.0, 18.80070891241428, -1.061801239593872, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.32222222222222224, 0.8379373848987108, 0.08333333333333333, 0.06672574270118996, 0.1460662534687093, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0350087], dtype=float32), -0.1795867]. 
=============================================
[2019-04-04 05:31:04,197] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.0729224e-04 9.9406141e-01 1.5351951e-12 1.6312047e-12 3.9305661e-09
 5.1312391e-03 5.9152266e-10], sum to 1.0000
[2019-04-04 05:31:04,201] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5148
[2019-04-04 05:31:04,242] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 48.0, 107.1666666666667, 789.0, 19.0, 21.28380281936305, -0.6249828063238517, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3853200.0000, 
sim time next is 3853800.0000, 
raw observation next is [2.0, 48.0, 106.0, 782.0, 19.0, 21.36290696136965, -0.611513719634924, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.35333333333333333, 0.8640883977900552, 0.08333333333333333, 0.2802422467808041, 0.29616209345502537, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.11371897], dtype=float32), -0.438827]. 
=============================================
[2019-04-04 05:31:06,897] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.7363605e-05 9.9755740e-01 9.6698069e-15 1.5953166e-13 5.3280168e-11
 2.3452190e-03 7.1373790e-10], sum to 1.0000
[2019-04-04 05:31:06,898] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7180
[2019-04-04 05:31:06,910] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 18.89247908224105, -1.106112284323805, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3616200.0000, 
sim time next is 3616800.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 18.90714475290983, -1.112627792259338, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.07559539607581911, 0.12912406924688735, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4860973], dtype=float32), -0.45698383]. 
=============================================
[2019-04-04 05:31:08,406] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.4386207e-04 9.9689758e-01 4.4235297e-12 2.8218523e-11 6.6825709e-09
 2.2585040e-03 1.4911095e-08], sum to 1.0000
[2019-04-04 05:31:08,407] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2955
[2019-04-04 05:31:08,420] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.033333333333333, 28.66666666666667, 0.0, 0.0, 19.0, 19.07010292559953, -1.162370569519976, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3642600.0000, 
sim time next is 3643200.0000, 
raw observation next is [8.0, 29.0, 0.0, 0.0, 19.0, 19.0257406656448, -1.171559008171903, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.6842105263157896, 0.29, 0.0, 0.0, 0.08333333333333333, 0.08547838880373337, 0.10948033060936568, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03881113], dtype=float32), 1.237887]. 
=============================================
[2019-04-04 05:31:09,096] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.3647545e-04 9.9516857e-01 9.5261115e-13 4.4864407e-12 2.1697526e-09
 4.2950297e-03 1.7205456e-09], sum to 1.0000
[2019-04-04 05:31:09,096] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9080
[2019-04-04 05:31:09,125] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 60.0, 117.0, 824.1666666666666, 19.0, 20.60156276089128, -0.7736829429140183, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3843600.0000, 
sim time next is 3844200.0000, 
raw observation next is [-1.0, 60.0, 117.0, 826.3333333333334, 19.0, 20.46017231313367, -0.7681069033555944, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.39, 0.9130755064456723, 0.08333333333333333, 0.2050143594278057, 0.2439643655481352, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5737109], dtype=float32), 0.96112376]. 
=============================================
[2019-04-04 05:31:17,892] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7908141e-04 9.9842322e-01 3.7048548e-15 1.3733132e-14 7.7064119e-11
 1.3976787e-03 8.9932896e-12], sum to 1.0000
[2019-04-04 05:31:17,894] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3242
[2019-04-04 05:31:17,901] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.3333333333333334, 54.00000000000001, 0.0, 0.0, 19.0, 19.78546264323631, -0.8768019973327146, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3874800.0000, 
sim time next is 3875400.0000, 
raw observation next is [0.0, 55.5, 0.0, 0.0, 19.0, 19.69751941173801, -0.8947178414496338, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.555, 0.0, 0.0, 0.08333333333333333, 0.14145995097816755, 0.20176071951678873, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30529344], dtype=float32), -1.1155053]. 
=============================================
[2019-04-04 05:31:19,914] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9453903e-04 9.9794525e-01 1.3087798e-12 3.0605889e-12 2.2585573e-09
 1.7601965e-03 1.5987003e-09], sum to 1.0000
[2019-04-04 05:31:19,914] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3585
[2019-04-04 05:31:19,927] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.166666666666667, 38.5, 0.0, 0.0, 19.0, 18.59211513296964, -1.215227281363797, 0.0, 1.0, 42771.62295630034], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4083000.0000, 
sim time next is 4083600.0000, 
raw observation next is [-4.333333333333334, 39.0, 0.0, 0.0, 19.0, 18.65377167607116, -1.214723609619758, 0.0, 1.0, 18729.17903055815], 
processed observation next is [1.0, 0.2608695652173913, 0.3425669436749769, 0.39, 0.0, 0.0, 0.08333333333333333, 0.05448097300592991, 0.09509213012674733, 0.0, 1.0, 0.08918656681218166], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.33370712], dtype=float32), 0.21903235]. 
=============================================
[2019-04-04 05:31:21,786] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2708828e-05 9.9882358e-01 1.4765140e-16 2.5347076e-15 1.5605601e-12
 1.1636936e-03 4.6487905e-12], sum to 1.0000
[2019-04-04 05:31:21,786] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9384
[2019-04-04 05:31:21,799] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 18.91139450725586, -1.084025458375559, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3890400.0000, 
sim time next is 3891000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 18.87072180521387, -1.096909354593351, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.07256015043448905, 0.134363548468883, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26550317], dtype=float32), -0.32958212]. 
=============================================
[2019-04-04 05:31:21,822] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[47.828762]
 [48.262424]
 [49.128544]
 [49.75304 ]
 [50.267582]], R is [[48.00053406]
 [48.5205307 ]
 [49.0353241 ]
 [49.54497147]
 [50.0495224 ]].
[2019-04-04 05:31:25,404] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5823776e-03 9.9477190e-01 1.6712086e-12 7.7388738e-12 3.4493921e-09
 3.6456934e-03 2.9586549e-09], sum to 1.0000
[2019-04-04 05:31:25,404] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2688
[2019-04-04 05:31:25,462] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 19.0, 19.37307038709149, -0.9987919057260003, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3955800.0000, 
sim time next is 3956400.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 19.0, 19.28789477855202, -1.018594503381808, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.296398891966759, 0.41, 0.0, 0.0, 0.08333333333333333, 0.10732456487933491, 0.16046849887273065, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.41603887], dtype=float32), -0.11850144]. 
=============================================
[2019-04-04 05:31:26,088] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.31691280e-03 9.81646717e-01 5.03400923e-12 1.12880235e-11
 1.34132687e-08 1.50364274e-02 8.36491321e-09], sum to 1.0000
[2019-04-04 05:31:26,089] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1803
[2019-04-04 05:31:26,114] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.833333333333333, 40.5, 0.0, 0.0, 19.0, 20.20660113678044, -0.8596625664315489, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3952200.0000, 
sim time next is 3952800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 19.0, 20.04953062611252, -0.8845559788045291, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.296398891966759, 0.41, 0.0, 0.0, 0.08333333333333333, 0.17079421884271007, 0.20514800706515698, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.17380874], dtype=float32), -0.23847534]. 
=============================================
[2019-04-04 05:31:28,020] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.8862944e-05 9.9878246e-01 2.0934132e-13 1.2689452e-12 4.5274193e-10
 1.1287231e-03 8.6397295e-10], sum to 1.0000
[2019-04-04 05:31:28,020] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2142
[2019-04-04 05:31:28,048] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-14.0, 69.0, 0.0, 0.0, 19.0, 18.43143214826602, -1.272294046781925, 0.0, 1.0, 58593.45321622936], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3999600.0000, 
sim time next is 4000200.0000, 
raw observation next is [-13.83333333333333, 68.0, 0.0, 0.0, 19.0, 18.4252344988089, -1.273905452354236, 0.0, 1.0, 53897.2134740198], 
processed observation next is [1.0, 0.30434782608695654, 0.07940904893813489, 0.68, 0.0, 0.0, 0.08333333333333333, 0.03543620823407512, 0.0753648492152547, 0.0, 1.0, 0.2566533974953324], 
reward next is 0.7433, 
noisyNet noise sample is [array([-1.393446], dtype=float32), -0.00016645307]. 
=============================================
[2019-04-04 05:31:30,234] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.43626030e-03 9.93152261e-01 1.07333985e-11 6.44373721e-11
 2.40961651e-08 5.41148521e-03 1.50913966e-08], sum to 1.0000
[2019-04-04 05:31:30,237] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5717
[2019-04-04 05:31:30,249] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.5, 35.0, 114.0, 774.0, 19.0, 18.77659759041335, -1.081989649435733, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4185000.0000, 
sim time next is 4185600.0000, 
raw observation next is [-1.333333333333333, 35.0, 114.8333333333333, 782.0, 19.0, 18.84503209926179, -1.074547885428548, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.42566943674976926, 0.35, 0.38277777777777766, 0.8640883977900552, 0.08333333333333333, 0.07041934160514914, 0.14181737152381735, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22516324], dtype=float32), 1.2925944]. 
=============================================
[2019-04-04 05:31:31,040] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.7993707e-05 9.9950004e-01 1.5685162e-15 1.0918605e-14 8.7558728e-12
 4.3190114e-04 2.6193962e-11], sum to 1.0000
[2019-04-04 05:31:31,040] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6540
[2019-04-04 05:31:31,055] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 37.0, 0.0, 0.0, 19.0, 18.69872204384611, -1.14511032844305, 0.0, 1.0, 18722.49167910872], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4060200.0000, 
sim time next is 4060800.0000, 
raw observation next is [-6.0, 37.0, 0.0, 0.0, 19.0, 18.70039576273653, -1.152714668909935, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.37, 0.0, 0.0, 0.08333333333333333, 0.058366313561377524, 0.11576177703002166, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02469289], dtype=float32), 0.86564225]. 
=============================================
[2019-04-04 05:31:31,840] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.7499258e-04 9.9340373e-01 3.3391016e-12 2.1879169e-11 1.0781071e-08
 5.7212296e-03 9.5758157e-09], sum to 1.0000
[2019-04-04 05:31:31,840] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3217
[2019-04-04 05:31:31,854] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 50.66666666666667, 0.0, 0.0, 19.0, 18.75262263097604, -1.166089206540619, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4173600.0000, 
sim time next is 4174200.0000, 
raw observation next is [-5.0, 51.5, 0.0, 0.0, 19.0, 18.79436725744627, -1.176923784753618, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.515, 0.0, 0.0, 0.08333333333333333, 0.06619727145385597, 0.10769207174879399, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.59025025], dtype=float32), -0.080027565]. 
=============================================
[2019-04-04 05:31:34,878] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3000715e-03 9.9517620e-01 6.3677917e-12 6.8677022e-12 2.0387539e-08
 3.5236913e-03 5.2915228e-09], sum to 1.0000
[2019-04-04 05:31:34,879] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1317
[2019-04-04 05:31:34,902] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.833333333333333, 34.33333333333334, 102.0, 761.0, 19.0, 21.55207931097982, -0.5833235338924583, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4114200.0000, 
sim time next is 4114800.0000, 
raw observation next is [4.0, 35.0, 100.0, 744.5, 19.0, 21.61711789557943, -0.5672505966116997, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.3333333333333333, 0.8226519337016575, 0.08333333333333333, 0.3014264912982858, 0.3109164677961001, 1.0, 1.0, 0.0], 
reward next is 0.3275, 
noisyNet noise sample is [array([-1.8299519], dtype=float32), 0.5294203]. 
=============================================
[2019-04-04 05:31:37,837] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.8859029e-04 9.9452764e-01 1.9806105e-11 8.9954974e-11 1.0008336e-08
 4.6836771e-03 2.1934225e-08], sum to 1.0000
[2019-04-04 05:31:37,837] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8555
[2019-04-04 05:31:37,855] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.0, 18.80343986006097, -1.150522401155384, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4161000.0000, 
sim time next is 4161600.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 19.0, 18.76177284814785, -1.161840470140395, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.5, 0.0, 0.0, 0.08333333333333333, 0.06348107067898745, 0.11271984328653499, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9368115], dtype=float32), 0.65813386]. 
=============================================
[2019-04-04 05:31:42,050] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8059385e-05 9.9905270e-01 7.8938468e-16 1.0286567e-15 3.1498114e-12
 9.0916775e-04 2.2867793e-12], sum to 1.0000
[2019-04-04 05:31:42,050] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4893
[2019-04-04 05:31:42,074] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 37.33333333333334, 0.0, 0.0, 19.0, 19.89190230419753, -0.8549053526508068, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4137600.0000, 
sim time next is 4138200.0000, 
raw observation next is [1.0, 38.0, 0.0, 0.0, 19.0, 19.83589833109607, -0.866514031135699, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.38, 0.0, 0.0, 0.08333333333333333, 0.1529915275913393, 0.21116198962143365, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1354675], dtype=float32), 0.7825473]. 
=============================================
[2019-04-04 05:31:44,281] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7774338e-04 9.9782926e-01 4.9800718e-13 4.0630528e-12 6.5586264e-10
 1.8929593e-03 2.0930797e-09], sum to 1.0000
[2019-04-04 05:31:44,283] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5219
[2019-04-04 05:31:44,304] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 51.0, 110.0, 53.0, 19.0, 18.87852279983666, -1.163547970654488, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4264200.0000, 
sim time next is 4264800.0000, 
raw observation next is [3.0, 51.66666666666666, 122.0, 66.0, 19.0, 18.89643539394013, -1.163040322642108, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.5166666666666666, 0.4066666666666667, 0.07292817679558011, 0.08333333333333333, 0.07470294949501088, 0.11231989245263067, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0219685], dtype=float32), -0.95943636]. 
=============================================
[2019-04-04 05:31:44,845] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6709247e-04 9.9721742e-01 3.9976060e-13 7.0193270e-12 3.0593359e-09
 2.3155250e-03 2.6464479e-09], sum to 1.0000
[2019-04-04 05:31:44,851] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4655
[2019-04-04 05:31:44,867] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.166666666666667, 53.16666666666667, 158.0, 105.0, 19.0, 18.73657439762587, -1.185153019166578, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4266600.0000, 
sim time next is 4267200.0000, 
raw observation next is [3.333333333333333, 53.33333333333334, 170.0, 118.0, 19.0, 18.76426461433485, -1.173181804697601, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.5549399815327793, 0.5333333333333334, 0.5666666666666667, 0.13038674033149172, 0.08333333333333333, 0.06368871786123738, 0.10893939843413299, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1230836], dtype=float32), 0.41633227]. 
=============================================
[2019-04-04 05:32:07,650] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.16340736e-04 9.99069035e-01 2.46372768e-15 1.10711018e-14
 1.57649727e-11 8.14641011e-04 1.90221363e-11], sum to 1.0000
[2019-04-04 05:32:07,651] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2118
[2019-04-04 05:32:07,773] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 19.31119775475598, -0.9715956935524511, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4473000.0000, 
sim time next is 4473600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 19.26427524252961, -0.9920302875523954, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.10535627021080085, 0.16932323748253486, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2950576], dtype=float32), 0.17831877]. 
=============================================
[2019-04-04 05:32:10,040] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0289081e-03 9.9684840e-01 1.1833235e-12 4.4785946e-12 4.8544986e-09
 2.1227163e-03 2.4948765e-09], sum to 1.0000
[2019-04-04 05:32:10,046] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4041
[2019-04-04 05:32:10,136] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.3333333333333333, 61.83333333333333, 152.3333333333333, 595.0, 19.0, 20.21310366648363, -0.8583975348208357, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4614600.0000, 
sim time next is 4615200.0000, 
raw observation next is [0.0, 60.0, 146.5, 638.0, 19.0, 20.31789798525219, -0.8367869541878235, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.6, 0.48833333333333334, 0.7049723756906078, 0.08333333333333333, 0.1931581654376826, 0.2210710152707255, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8138592], dtype=float32), -1.6823616]. 
=============================================
[2019-04-04 05:32:12,833] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.2491470e-04 9.9849260e-01 4.9820496e-14 5.2950960e-14 2.2930492e-10
 1.0824460e-03 2.8880941e-11], sum to 1.0000
[2019-04-04 05:32:12,835] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2867
[2019-04-04 05:32:12,858] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.166666666666667, 48.83333333333334, 197.6666666666667, 285.6666666666666, 19.0, 22.19263032637414, -0.4938411846098738, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4633800.0000, 
sim time next is 4634400.0000, 
raw observation next is [5.333333333333334, 47.66666666666667, 196.3333333333333, 207.3333333333333, 19.0, 22.02260240901855, -0.44836267210869, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6103416435826409, 0.47666666666666674, 0.6544444444444443, 0.22909760589318595, 0.08333333333333333, 0.3352168674182125, 0.35054577596377, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21788648], dtype=float32), -0.9197661]. 
=============================================
[2019-04-04 05:32:22,244] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2733580e-05 9.9960846e-01 1.7626847e-16 2.3605705e-15 4.6909815e-12
 3.4879314e-04 2.4629233e-12], sum to 1.0000
[2019-04-04 05:32:22,258] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0409
[2019-04-04 05:32:22,278] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.1, 64.66666666666667, 0.0, 0.0, 19.0, 18.71091110990069, -1.131055738656767, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4585800.0000, 
sim time next is 4586400.0000, 
raw observation next is [-0.2, 65.0, 0.0, 0.0, 19.0, 18.71765769765761, -1.129309447306772, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4570637119113574, 0.65, 0.0, 0.0, 0.08333333333333333, 0.059804808138134234, 0.12356351756440935, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.6361198], dtype=float32), 0.66948545]. 
=============================================
[2019-04-04 05:32:33,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3979459e-04 9.9657947e-01 2.0239492e-15 6.8293278e-14 4.6982092e-11
 3.2806867e-03 1.2171429e-10], sum to 1.0000
[2019-04-04 05:32:33,135] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7960
[2019-04-04 05:32:33,158] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.1666666666666667, 39.16666666666666, 0.0, 0.0, 19.0, 18.70517701719653, -1.219749251430869, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4921800.0000, 
sim time next is 4922400.0000, 
raw observation next is [0.3333333333333333, 39.33333333333334, 0.0, 0.0, 19.0, 18.62547609532584, -1.21692445410504, 0.0, 1.0, 196668.4396236362], 
processed observation next is [0.0, 1.0, 0.4718374884579871, 0.3933333333333334, 0.0, 0.0, 0.08333333333333333, 0.05212300794382004, 0.09435851529832003, 0.0, 1.0, 0.9365163791601724], 
reward next is 0.0635, 
noisyNet noise sample is [array([-1.0525161], dtype=float32), 0.0812974]. 
=============================================
[2019-04-04 05:32:38,844] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7649163e-04 9.9844557e-01 2.0383233e-13 1.0017472e-12 2.2745768e-10
 1.1779668e-03 3.7663361e-10], sum to 1.0000
[2019-04-04 05:32:38,856] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9982
[2019-04-04 05:32:38,968] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 92.0, 208.0, 6.0, 19.0, 20.2223719992721, -0.8971670227870431, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4703400.0000, 
sim time next is 4704000.0000, 
raw observation next is [0.0, 92.0, 208.8333333333333, 6.0, 19.0, 20.28568169839319, -0.8883830067362477, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.92, 0.6961111111111109, 0.0066298342541436465, 0.08333333333333333, 0.19047347486609922, 0.20387233108791744, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.920351], dtype=float32), -3.2223995]. 
=============================================
[2019-04-04 05:32:39,079] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[38.745487]
 [38.9211  ]
 [39.305126]
 [39.679523]
 [39.98161 ]], R is [[38.49499893]
 [38.1100502 ]
 [37.7289505 ]
 [37.35166168]
 [36.9781456 ]].
[2019-04-04 05:32:42,299] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5337954e-04 9.9822170e-01 1.2319629e-14 3.8119876e-13 3.2840241e-11
 1.6249104e-03 1.5774640e-10], sum to 1.0000
[2019-04-04 05:32:42,300] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5121
[2019-04-04 05:32:42,343] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.666666666666667, 78.0, 0.0, 0.0, 19.0, 18.62598759711731, -1.149645465056876, 0.0, 1.0, 45151.97572594587], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4760400.0000, 
sim time next is 4761000.0000, 
raw observation next is [-5.0, 81.5, 0.0, 0.0, 19.0, 18.61198553843607, -1.149725073122313, 0.0, 1.0, 45958.27286774683], 
processed observation next is [0.0, 0.08695652173913043, 0.32409972299168976, 0.815, 0.0, 0.0, 0.08333333333333333, 0.05099879486967248, 0.116758308959229, 0.0, 1.0, 0.21884891841784207], 
reward next is 0.7812, 
noisyNet noise sample is [array([0.23199216], dtype=float32), -0.14839008]. 
=============================================
[2019-04-04 05:32:42,348] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[41.586014]
 [42.51466 ]
 [43.17809 ]
 [44.86859 ]
 [46.06257 ]], R is [[41.66023636]
 [42.02862549]
 [42.37146759]
 [42.6893692 ]
 [43.13285828]].
[2019-04-04 05:32:53,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:32:53,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:32:53,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run3
[2019-04-04 05:32:55,333] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8890426e-04 9.9457693e-01 5.5672775e-14 6.7774703e-13 9.9805375e-10
 5.2341586e-03 6.4427397e-10], sum to 1.0000
[2019-04-04 05:32:55,333] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5889
[2019-04-04 05:32:55,403] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 45.0, 122.3333333333333, 352.0, 19.0, 19.00797364543029, -1.057577869788185, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4896600.0000, 
sim time next is 4897200.0000, 
raw observation next is [3.0, 45.0, 112.1666666666667, 334.5, 19.0, 19.01590806849514, -1.051242874341065, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.373888888888889, 0.3696132596685083, 0.08333333333333333, 0.08465900570792832, 0.14958570855297837, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5397588], dtype=float32), -0.7681287]. 
=============================================
[2019-04-04 05:33:00,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:00,774] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:00,775] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run3
[2019-04-04 05:33:05,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:05,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:05,114] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run3
[2019-04-04 05:33:07,109] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:07,109] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:07,111] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run3
[2019-04-04 05:33:10,110] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0107813e-06 9.9906796e-01 2.5677757e-17 1.7769914e-17 6.1309519e-13
 9.2909980e-04 7.4199241e-13], sum to 1.0000
[2019-04-04 05:33:10,150] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6629
[2019-04-04 05:33:10,201] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.333333333333333, 28.0, 0.0, 0.0, 19.0, 20.88424517959746, -0.6406550757682657, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4999800.0000, 
sim time next is 5000400.0000, 
raw observation next is [4.0, 29.0, 0.0, 0.0, 19.0, 20.81181778742769, -0.654759423954988, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5734072022160666, 0.29, 0.0, 0.0, 0.08333333333333333, 0.2343181489523074, 0.28174685868167065, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3398623], dtype=float32), -0.013235449]. 
=============================================
[2019-04-04 05:33:10,698] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8611131e-04 9.9907446e-01 9.9842396e-14 4.9235155e-13 1.7905435e-10
 7.3933974e-04 3.6329273e-10], sum to 1.0000
[2019-04-04 05:33:10,705] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1474
[2019-04-04 05:33:10,815] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 59.0, 90.66666666666667, 461.0, 19.0, 18.98163331069148, -1.026601637243808, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5041200.0000, 
sim time next is 5041800.0000, 
raw observation next is [-0.5, 56.0, 97.0, 533.0, 19.0, 19.3302364427521, -0.9755223063540334, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44875346260387816, 0.56, 0.3233333333333333, 0.5889502762430939, 0.08333333333333333, 0.11085303689600827, 0.17482589788198888, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2810569], dtype=float32), -1.3231435]. 
=============================================
[2019-04-04 05:33:14,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:14,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:14,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run3
[2019-04-04 05:33:14,447] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:14,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:14,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run3
[2019-04-04 05:33:16,260] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.43521533e-05 9.99514699e-01 1.17234505e-17 1.37899611e-17
 6.57965165e-13 4.50980122e-04 2.11265982e-13], sum to 1.0000
[2019-04-04 05:33:16,267] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4936
[2019-04-04 05:33:16,298] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [10.16666666666667, 18.66666666666667, 0.0, 0.0, 19.0, 22.62326608908175, -0.2549848608098177, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5082600.0000, 
sim time next is 5083200.0000, 
raw observation next is [10.0, 19.0, 0.0, 0.0, 19.0, 22.5356901978859, -0.2683796333441298, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.739612188365651, 0.19, 0.0, 0.0, 0.08333333333333333, 0.3779741831571582, 0.4105401222186234, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2743908], dtype=float32), 1.9152912]. 
=============================================
[2019-04-04 05:33:16,532] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:16,532] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:16,535] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run3
[2019-04-04 05:33:17,517] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:17,517] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:17,519] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run3
[2019-04-04 05:33:17,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:17,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:17,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run3
[2019-04-04 05:33:18,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:18,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:18,266] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run3
[2019-04-04 05:33:18,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:18,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:18,715] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run3
[2019-04-04 05:33:18,895] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:18,896] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:18,898] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run3
[2019-04-04 05:33:21,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:21,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:21,503] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run3
[2019-04-04 05:33:21,589] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9693707e-04 9.9343079e-01 3.6834514e-13 2.3866774e-12 1.4671362e-09
 6.1722593e-03 1.0734331e-09], sum to 1.0000
[2019-04-04 05:33:21,589] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0941
[2019-04-04 05:33:21,649] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.700000000000001, 63.0, 55.33333333333334, 38.0, 19.0, 19.05228472527205, -1.172576221286469, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 142800.0000, 
sim time next is 143400.0000, 
raw observation next is [-6.7, 63.5, 49.66666666666667, 31.0, 19.0, 19.02966048998066, -1.244764666486458, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.2770083102493075, 0.635, 0.16555555555555557, 0.03425414364640884, 0.08333333333333333, 0.08580504083172169, 0.085078444504514, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.07129897], dtype=float32), 0.8751552]. 
=============================================
[2019-04-04 05:33:21,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:21,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:21,731] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run3
[2019-04-04 05:33:21,879] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:21,879] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:21,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run3
[2019-04-04 05:33:22,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:33:22,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:33:22,695] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run3
[2019-04-04 05:33:32,306] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.0365072e-06 9.9963415e-01 2.9175014e-17 7.2401968e-16 1.2048896e-12
 3.5877220e-04 3.9285289e-12], sum to 1.0000
[2019-04-04 05:33:32,398] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2304
[2019-04-04 05:33:32,407] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 18.83013531229178, -1.103549832830086, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 21600.0000, 
sim time next is 22200.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 18.76648084733214, -1.1147857657512, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.063873403944345, 0.12840474474960004, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1667771], dtype=float32), -1.5570451]. 
=============================================
[2019-04-04 05:33:45,966] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8514169e-04 9.9539846e-01 3.4911315e-13 4.7497843e-12 6.0769034e-10
 4.4164173e-03 7.1379658e-10], sum to 1.0000
[2019-04-04 05:33:45,966] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9578
[2019-04-04 05:33:46,017] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.483333333333334, 78.0, 49.0, 197.3333333333333, 19.0, 19.24570251055155, -1.157415523563147, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 204600.0000, 
sim time next is 205200.0000, 
raw observation next is [-8.4, 78.0, 56.5, 148.0, 19.0, 19.22879249052435, -1.158150717666289, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2299168975069252, 0.78, 0.18833333333333332, 0.16353591160220995, 0.08333333333333333, 0.10239937421036238, 0.11394976077790364, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6460111], dtype=float32), 0.80937713]. 
=============================================
[2019-04-04 05:33:52,917] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.8405854e-05 9.9942940e-01 1.0505227e-14 3.9123479e-14 4.9537222e-11
 5.0221523e-04 1.1703367e-10], sum to 1.0000
[2019-04-04 05:33:52,917] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9320
[2019-04-04 05:33:53,003] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 19.0, 18.43742266627677, -1.420327466308766, 1.0, 1.0, 84344.94475798802], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 238200.0000, 
sim time next is 238800.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 18.72778598697461, -1.258692098681581, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.06064883224788412, 0.08043596710613965, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3837878], dtype=float32), 0.23853594]. 
=============================================
[2019-04-04 05:33:57,438] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5561947e-04 9.9484599e-01 1.9770938e-13 2.7600692e-13 4.3348314e-10
 4.7983574e-03 6.4328032e-10], sum to 1.0000
[2019-04-04 05:33:57,439] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6147
[2019-04-04 05:33:57,513] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-11.9, 59.00000000000001, 0.0, 0.0, 19.0, 19.14528237304384, -1.157671736474978, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 325200.0000, 
sim time next is 325800.0000, 
raw observation next is [-12.0, 60.0, 0.0, 0.0, 19.0, 19.24328023921306, -1.189721883757495, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.13019390581717452, 0.6, 0.0, 0.0, 0.08333333333333333, 0.10360668660108836, 0.10342603874750167, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2156031], dtype=float32), 0.8713155]. 
=============================================
[2019-04-04 05:34:01,060] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0838593e-04 9.9717629e-01 1.1983005e-13 5.6222843e-13 1.1631394e-10
 2.7153343e-03 1.7074082e-10], sum to 1.0000
[2019-04-04 05:34:01,060] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0938
[2019-04-04 05:34:01,076] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-15.9, 75.5, 0.0, 0.0, 19.0, 18.04031554405707, -1.417437328715653, 0.0, 1.0, 51750.3184850029], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 365400.0000, 
sim time next is 366000.0000, 
raw observation next is [-16.0, 76.33333333333334, 0.0, 0.0, 19.0, 18.04803696498204, -1.427437709325737, 0.0, 1.0, 51716.08373741334], 
processed observation next is [1.0, 0.21739130434782608, 0.01939058171745151, 0.7633333333333334, 0.0, 0.0, 0.08333333333333333, 0.004003080415169909, 0.024187430224754298, 0.0, 1.0, 0.246267065416254], 
reward next is 0.7537, 
noisyNet noise sample is [array([1.1966106], dtype=float32), 2.509738]. 
=============================================
[2019-04-04 05:34:01,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[41.964405]
 [41.967594]
 [41.90057 ]
 [41.820698]
 [41.67308 ]], R is [[42.31395721]
 [42.64439011]
 [42.9712944 ]
 [43.2946434 ]
 [43.61273956]].
[2019-04-04 05:34:09,189] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5992188e-03 9.9251467e-01 6.9980848e-11 8.7122476e-11 2.3738400e-08
 5.8861440e-03 1.9426560e-08], sum to 1.0000
[2019-04-04 05:34:09,189] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7742
[2019-04-04 05:34:09,196] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.45, 26.5, 129.0, 0.0, 19.0, 19.65969120742843, -1.175621335052364, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 477000.0000, 
sim time next is 477600.0000, 
raw observation next is [-1.366666666666667, 27.0, 127.3333333333333, 0.0, 19.0, 19.56977215004592, -1.194038688987044, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.42474607571560485, 0.27, 0.42444444444444435, 0.0, 0.08333333333333333, 0.1308143458371601, 0.10198710367098533, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6812206], dtype=float32), -1.7015358]. 
=============================================
[2019-04-04 05:34:16,564] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.9335812e-05 9.9910802e-01 8.0227275e-17 7.5704581e-16 2.3046664e-12
 8.1267848e-04 4.0961496e-12], sum to 1.0000
[2019-04-04 05:34:16,565] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4035
[2019-04-04 05:34:16,589] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 19.0, 18.64376128863967, -1.229987356776685, 0.0, 1.0, 18729.47386064146], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 683400.0000, 
sim time next is 684000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 18.70249230115314, -1.233843098228245, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.058541025096094934, 0.0887189672572517, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9572295], dtype=float32), 1.1559044]. 
=============================================
[2019-04-04 05:34:16,592] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[52.64915 ]
 [52.685627]
 [52.63979 ]
 [52.249683]
 [51.803722]], R is [[53.3513031 ]
 [53.72860336]
 [54.00695038]
 [54.18661118]
 [54.266716  ]].
[2019-04-04 05:34:21,184] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.33163366e-05 9.97223854e-01 2.31873101e-14 4.51600400e-13
 1.03069275e-10 2.72293226e-03 5.62409452e-10], sum to 1.0000
[2019-04-04 05:34:21,192] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6865
[2019-04-04 05:34:21,261] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 69.0, 115.6666666666667, 42.5, 19.0, 18.46597728183989, -1.261374045775079, 0.0, 1.0, 25337.68674911825], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 638400.0000, 
sim time next is 639000.0000, 
raw observation next is [-3.9, 68.0, 135.0, 51.0, 19.0, 18.47072440759472, -1.255725051129106, 0.0, 1.0, 32036.90475171667], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.68, 0.45, 0.056353591160221, 0.08333333333333333, 0.03922703396622662, 0.08142498295696465, 0.0, 1.0, 0.1525566892938889], 
reward next is 0.8474, 
noisyNet noise sample is [array([0.13759917], dtype=float32), -0.41704664]. 
=============================================
[2019-04-04 05:34:21,269] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[43.76482 ]
 [43.80114 ]
 [43.831894]
 [43.857758]
 [43.9412  ]], R is [[44.13831711]
 [44.57627869]
 [45.04130936]
 [45.57975769]
 [46.1239624 ]].
[2019-04-04 05:34:22,326] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3173174e-04 9.9616879e-01 1.3989974e-14 3.3736132e-13 1.7293247e-10
 3.6995132e-03 1.7225105e-10], sum to 1.0000
[2019-04-04 05:34:22,326] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3570
[2019-04-04 05:34:22,364] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 75.0, 0.0, 0.0, 19.0, 18.49672385211306, -1.245044556966563, 0.0, 1.0, 75749.6056298157], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 619200.0000, 
sim time next is 619800.0000, 
raw observation next is [-4.5, 73.83333333333333, 0.0, 0.0, 19.0, 18.51300509623702, -1.239714546070425, 0.0, 1.0, 48210.90933121795], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.7383333333333333, 0.0, 0.0, 0.08333333333333333, 0.04275042468641832, 0.08676181797652498, 0.0, 1.0, 0.22957575872008545], 
reward next is 0.7704, 
noisyNet noise sample is [array([0.07911182], dtype=float32), -0.42936552]. 
=============================================
[2019-04-04 05:34:24,253] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.0827824e-05 9.9736089e-01 1.3251601e-15 4.4229479e-14 2.7323144e-11
 2.5782590e-03 6.7376049e-11], sum to 1.0000
[2019-04-04 05:34:24,253] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3595
[2019-04-04 05:34:24,299] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 54.0, 55.0, 26.5, 19.0, 18.53975623858699, -1.247333626883501, 0.0, 1.0, 26578.62049392708], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 662400.0000, 
sim time next is 663000.0000, 
raw observation next is [-0.7, 54.5, 45.66666666666666, 22.66666666666666, 19.0, 18.54851425146912, -1.250200666896276, 0.0, 1.0, 29211.54791215431], 
processed observation next is [0.0, 0.6956521739130435, 0.443213296398892, 0.545, 0.1522222222222222, 0.02504604051565377, 0.08333333333333333, 0.045709520955759864, 0.08326644436790802, 0.0, 1.0, 0.1391026091054967], 
reward next is 0.8609, 
noisyNet noise sample is [array([1.1955892], dtype=float32), 0.15633832]. 
=============================================
[2019-04-04 05:34:24,340] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[46.733913]
 [46.499977]
 [46.322746]
 [46.140633]
 [45.984592]], R is [[47.36315918]
 [47.76296234]
 [48.15917969]
 [48.55200195]
 [48.94192886]].
[2019-04-04 05:34:28,046] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.0342401e-05 9.9343699e-01 2.2864123e-15 6.3084886e-14 5.5312328e-11
 6.5025548e-03 1.1233765e-10], sum to 1.0000
[2019-04-04 05:34:28,046] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8283
[2019-04-04 05:34:28,128] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.2, 57.0, 13.5, 8.5, 19.0, 18.49312882124493, -1.26608362753777, 0.0, 1.0, 32914.27421168716], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 666000.0000, 
sim time next is 666600.0000, 
raw observation next is [-1.2, 57.0, 0.0, 0.0, 19.0, 18.48637271576222, -1.264897135536096, 0.0, 1.0, 30729.54804853928], 
processed observation next is [0.0, 0.7391304347826086, 0.42936288088642666, 0.57, 0.0, 0.0, 0.08333333333333333, 0.04053105964685155, 0.07836762148796798, 0.0, 1.0, 0.14633118118352037], 
reward next is 0.8537, 
noisyNet noise sample is [array([-0.10169099], dtype=float32), -2.0174048]. 
=============================================
[2019-04-04 05:34:30,402] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.0605405e-04 9.9643338e-01 1.9650414e-12 3.5538215e-12 1.5089641e-09
 2.6604759e-03 1.2386951e-09], sum to 1.0000
[2019-04-04 05:34:30,402] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1456
[2019-04-04 05:34:30,475] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 64.5, 102.3333333333333, 542.0, 19.0, 19.70380591715818, -1.061657281587192, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 731400.0000, 
sim time next is 732000.0000, 
raw observation next is [-0.6, 63.00000000000001, 93.16666666666666, 660.5000000000001, 19.0, 19.76055194042807, -1.04928255591309, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.6300000000000001, 0.31055555555555553, 0.7298342541436466, 0.08333333333333333, 0.1467126617023391, 0.15023914802896998, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6529224], dtype=float32), 0.10411241]. 
=============================================
[2019-04-04 05:34:30,495] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[38.15536 ]
 [38.102005]
 [38.04579 ]
 [38.03724 ]
 [38.04346 ]], R is [[37.83156967]
 [37.4532547 ]
 [37.07872391]
 [36.70793533]
 [36.34085464]].
[2019-04-04 05:34:41,361] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.1035047e-05 9.9916220e-01 4.3645189e-15 8.9143060e-15 1.0386514e-11
 7.7680807e-04 2.1025121e-11], sum to 1.0000
[2019-04-04 05:34:41,368] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4100
[2019-04-04 05:34:41,376] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.45, 77.5, 0.0, 0.0, 19.0, 18.69427630400907, -1.22388719993616, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 876600.0000, 
sim time next is 877200.0000, 
raw observation next is [-1.366666666666667, 77.0, 0.0, 0.0, 19.0, 18.71677690147135, -1.225280835366173, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.42474607571560485, 0.77, 0.0, 0.0, 0.08333333333333333, 0.05973140845594571, 0.09157305487794232, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6784756], dtype=float32), 0.65943843]. 
=============================================
[2019-04-04 05:34:44,117] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.1139297e-06 9.9985826e-01 4.1539912e-19 1.5282627e-17 1.5632310e-13
 1.3358607e-04 7.9138532e-14], sum to 1.0000
[2019-04-04 05:34:44,122] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2162
[2019-04-04 05:34:44,192] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 19.0, 18.58351317877872, -1.228474267727556, 0.0, 1.0, 25054.77069327495], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 856200.0000, 
sim time next is 856800.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 19.0, 18.61195204073816, -1.222441690769323, 0.0, 1.0, 18733.47574532465], 
processed observation next is [1.0, 0.9565217391304348, 0.368421052631579, 0.83, 0.0, 0.0, 0.08333333333333333, 0.05099600339484667, 0.09251943641022568, 0.0, 1.0, 0.08920702735868881], 
reward next is 0.9108, 
noisyNet noise sample is [array([-2.357612], dtype=float32), -1.1828779]. 
=============================================
[2019-04-04 05:34:45,658] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6635152e-05 9.9906868e-01 2.4548332e-15 3.2088729e-14 1.2251638e-11
 8.9470518e-04 5.9800470e-11], sum to 1.0000
[2019-04-04 05:34:45,658] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8335
[2019-04-04 05:34:45,677] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.2, 76.0, 0.0, 0.0, 19.0, 18.76263376724013, -1.223898930947792, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 878400.0000, 
sim time next is 879000.0000, 
raw observation next is [-1.1, 75.33333333333334, 0.0, 0.0, 19.0, 18.80746833008395, -1.235865771270468, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.4321329639889197, 0.7533333333333334, 0.0, 0.0, 0.08333333333333333, 0.0672890275069958, 0.08804474290984403, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.638941], dtype=float32), -1.1720057]. 
=============================================
[2019-04-04 05:34:45,693] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[47.67616 ]
 [47.501663]
 [47.51343 ]
 [47.526466]
 [47.51345 ]], R is [[48.1945076 ]
 [48.71256256]
 [49.22543716]
 [49.73318481]
 [50.2358551 ]].
[2019-04-04 05:34:52,754] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 05:34:52,768] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:34:52,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:34:52,792] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:34:52,796] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:34:52,796] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:34:52,796] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:34:52,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run4
[2019-04-04 05:34:52,841] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run4
[2019-04-04 05:34:52,870] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run4
[2019-04-04 05:35:46,782] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.206674], dtype=float32), 0.17766061]
[2019-04-04 05:35:46,782] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.033333333333333, 78.33333333333334, 0.0, 0.0, 19.0, 18.72125758953457, -1.145872263999512, 0.0, 1.0, 0.0]
[2019-04-04 05:35:46,782] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:35:46,783] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.7629710e-05 9.9906570e-01 1.1419872e-16 1.4383803e-15 3.1161970e-12
 8.9666544e-04 5.3703973e-12], sampled 0.4889119297477862
[2019-04-04 05:36:18,562] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5583.9524 142465434.4950 -2172.9549
[2019-04-04 05:36:29,323] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5009.6703 169925095.8126 -2861.3607
[2019-04-04 05:36:35,588] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5093.0834 184770304.5347 -2749.1586
[2019-04-04 05:36:36,623] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 300000, evaluation results [300000.0, 5009.670341436831, 169925095.81259155, -2861.360665844522, 5583.952442224413, 142465434.49503028, -2172.9549369840015, 5093.083384326317, 184770304.53465173, -2749.1585527421703]
[2019-04-04 05:36:37,923] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4419027e-04 9.9950302e-01 9.7544494e-16 3.3309384e-14 2.1140319e-11
 3.5286162e-04 7.5835026e-11], sum to 1.0000
[2019-04-04 05:36:37,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4257
[2019-04-04 05:36:37,936] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 19.0, 19.89037194118112, -0.7592667101085259, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1197600.0000, 
sim time next is 1198200.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 19.0, 19.90848136510307, -0.7557433190222604, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.9529085872576178, 0.67, 0.0, 0.0, 0.08333333333333333, 0.15904011375858929, 0.2480855603259132, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2709583], dtype=float32), 1.8243918]. 
=============================================
[2019-04-04 05:36:46,120] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7682240e-06 9.9987888e-01 2.8577426e-20 7.6642787e-19 2.1411734e-14
 1.1925694e-04 1.6139135e-14], sum to 1.0000
[2019-04-04 05:36:46,125] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2189
[2019-04-04 05:36:46,131] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.3, 96.0, 0.0, 0.0, 19.0, 19.21310202286414, -0.8584215509070546, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1274400.0000, 
sim time next is 1275000.0000, 
raw observation next is [8.116666666666667, 96.0, 0.0, 0.0, 19.0, 19.1557040159004, -0.8710796710655103, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6874422899353648, 0.96, 0.0, 0.0, 0.08333333333333333, 0.09630866799170008, 0.2096401096448299, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7681705], dtype=float32), 0.9231888]. 
=============================================
[2019-04-04 05:36:46,151] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[64.01779 ]
 [64.01437 ]
 [64.0122  ]
 [63.99714 ]
 [63.952168]], R is [[64.33517456]
 [64.69181824]
 [65.04489899]
 [65.39444733]
 [65.7405014 ]].
[2019-04-04 05:36:49,552] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7746239e-06 9.9957472e-01 8.7584250e-21 1.1174800e-19 2.2126943e-14
 4.2248052e-04 2.8535219e-14], sum to 1.0000
[2019-04-04 05:36:49,555] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6034
[2019-04-04 05:36:49,563] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 19.0, 18.83674046264894, -0.9363098944509859, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1291800.0000, 
sim time next is 1292400.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 19.0, 18.82257147668232, -0.9439261054282649, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 1.0, 0.6149584487534627, 1.0, 0.0, 0.0, 0.08333333333333333, 0.06854762305686002, 0.185357964857245, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12674229], dtype=float32), 2.1378646]. 
=============================================
[2019-04-04 05:36:54,896] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.5481168e-04 9.9821019e-01 2.9134555e-14 1.8096451e-13 1.8419712e-10
 1.1349373e-03 1.8468962e-10], sum to 1.0000
[2019-04-04 05:36:54,897] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0664
[2019-04-04 05:36:54,904] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.366666666666667, 92.0, 63.83333333333333, 0.0, 19.0, 20.15467699565956, -0.8670797848974533, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1678800.0000, 
sim time next is 1679400.0000, 
raw observation next is [1.3, 92.0, 66.0, 0.0, 19.0, 20.18719118718509, -0.8735504530635406, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49861495844875353, 0.92, 0.22, 0.0, 0.08333333333333333, 0.1822659322654241, 0.20881651564548645, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.90065295], dtype=float32), -0.69033265]. 
=============================================
[2019-04-04 05:37:02,247] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.63944297e-06 9.99632478e-01 1.67229243e-18 9.24377395e-18
 7.02605969e-14 3.58904392e-04 1.11314224e-13], sum to 1.0000
[2019-04-04 05:37:02,250] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6965
[2019-04-04 05:37:02,265] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 19.0, 18.94653015178288, -1.011304799537714, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1558200.0000, 
sim time next is 1558800.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 19.0, 18.98955618074857, -1.02213688032609, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6011080332409973, 0.82, 0.0, 0.0, 0.08333333333333333, 0.0824630150623807, 0.15928770655797, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7323794], dtype=float32), -1.6662241]. 
=============================================
[2019-04-04 05:37:09,051] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.7536534e-05 9.9559033e-01 1.6123135e-16 1.5349645e-14 4.7568655e-12
 4.3621105e-03 1.4305282e-11], sum to 1.0000
[2019-04-04 05:37:09,053] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3784
[2019-04-04 05:37:09,107] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 82.00000000000001, 9.999999999999998, 0.0, 19.0, 18.69919091736595, -1.153617271658265, 0.0, 1.0, 107882.8897008369], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1789800.0000, 
sim time next is 1790400.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 19.0, 18.67776328935208, -1.140764569433306, 0.0, 1.0, 77177.46354607568], 
processed observation next is [0.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.0, 0.0, 0.08333333333333333, 0.056480274112673236, 0.11974514352223135, 0.0, 1.0, 0.367511731171789], 
reward next is 0.6325, 
noisyNet noise sample is [array([1.8806634], dtype=float32), 0.5634706]. 
=============================================
[2019-04-04 05:37:18,036] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.6752542e-05 9.9630570e-01 1.2807082e-14 5.9215511e-14 5.2354954e-11
 3.6175591e-03 1.6386228e-10], sum to 1.0000
[2019-04-04 05:37:18,052] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6980
[2019-04-04 05:37:18,085] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.199999999999999, 78.83333333333334, 24.66666666666666, 12.33333333333333, 19.0, 18.40783950759887, -1.276202905858813, 1.0, 1.0, 30230.82247466003], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2101800.0000, 
sim time next is 2102400.0000, 
raw observation next is [-7.3, 79.0, 36.5, 18.5, 19.0, 18.41274009932652, -1.253774387130476, 1.0, 1.0, 37063.15680435087], 
processed observation next is [1.0, 0.34782608695652173, 0.26038781163434904, 0.79, 0.12166666666666667, 0.020441988950276244, 0.08333333333333333, 0.03439500827720998, 0.08207520428984132, 1.0, 1.0, 0.17649122287786126], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0380661], dtype=float32), -0.43338305]. 
=============================================
[2019-04-04 05:37:20,289] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0062063e-05 9.9794990e-01 1.6789955e-16 1.8137252e-15 1.7497397e-12
 2.0299694e-03 3.8257067e-12], sum to 1.0000
[2019-04-04 05:37:20,289] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1008
[2019-04-04 05:37:20,344] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.583333333333333, 83.5, 10.33333333333333, 0.0, 19.0, 18.7912835542548, -1.172328465536742, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1876200.0000, 
sim time next is 1876800.0000, 
raw observation next is [-4.666666666666667, 84.0, 0.0, 0.0, 19.0, 18.85758977164196, -1.172808328789114, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.3333333333333333, 0.84, 0.0, 0.0, 0.08333333333333333, 0.07146581430349652, 0.10906389040362867, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04668724], dtype=float32), 1.2097006]. 
=============================================
[2019-04-04 05:37:21,495] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4882184e-04 9.9698776e-01 7.7634199e-13 2.5324666e-12 8.3964813e-10
 2.8635091e-03 1.6740331e-09], sum to 1.0000
[2019-04-04 05:37:21,496] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8982
[2019-04-04 05:37:21,517] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 79.0, 126.0, 0.0, 19.0, 19.24845984714262, -1.150615116278386, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2037600.0000, 
sim time next is 2038200.0000, 
raw observation next is [-4.0, 80.16666666666667, 118.6666666666667, 0.0, 19.0, 19.31113587585283, -1.148063953275903, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3518005540166205, 0.8016666666666667, 0.39555555555555566, 0.0, 0.08333333333333333, 0.10926132298773587, 0.11731201557469902, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.26464504], dtype=float32), -2.348823]. 
=============================================
[2019-04-04 05:37:33,496] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5262415e-05 9.9790907e-01 3.8567473e-15 1.4482666e-13 5.0923831e-11
 2.0256087e-03 1.2047585e-10], sum to 1.0000
[2019-04-04 05:37:33,496] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8679
[2019-04-04 05:37:33,509] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 19.0, 18.53272785570217, -1.261584576666713, 1.0, 1.0, 18735.61209810426], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2100600.0000, 
sim time next is 2101200.0000, 
raw observation next is [-7.1, 78.66666666666667, 0.0, 0.0, 19.0, 18.4859758181291, -1.275519535809108, 1.0, 1.0, 19355.11428114472], 
processed observation next is [1.0, 0.30434782608695654, 0.2659279778393352, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.04049798484409154, 0.07482682139696399, 1.0, 1.0, 0.09216721086259391], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9563394], dtype=float32), -1.0226535]. 
=============================================
[2019-04-04 05:37:36,570] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0023422e-04 9.9884027e-01 3.5437470e-17 7.3077556e-16 8.2687893e-13
 1.0594625e-03 9.4631594e-13], sum to 1.0000
[2019-04-04 05:37:36,575] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9807
[2019-04-04 05:37:36,595] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.3, 79.0, 0.0, 0.0, 19.0, 18.78252557711122, -1.21303372335799, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2163600.0000, 
sim time next is 2164200.0000, 
raw observation next is [-7.199999999999999, 78.83333333333334, 0.0, 0.0, 19.0, 18.69013594184383, -1.234582931119247, 0.0, 1.0, 18720.92120170936], 
processed observation next is [1.0, 0.043478260869565216, 0.26315789473684215, 0.7883333333333334, 0.0, 0.0, 0.08333333333333333, 0.057511328486985924, 0.0884723562935843, 0.0, 1.0, 0.08914724381766362], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.49396348], dtype=float32), -2.4981842]. 
=============================================
[2019-04-04 05:37:39,434] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4294062e-03 9.8535639e-01 8.0538001e-13 8.7545700e-12 1.4031258e-08
 1.3214226e-02 3.1483363e-09], sum to 1.0000
[2019-04-04 05:37:39,434] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9083
[2019-04-04 05:37:39,441] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.7, 27.83333333333334, 88.0, 836.3333333333334, 19.0, 18.70655345579146, -1.161199100477753, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2466600.0000, 
sim time next is 2467200.0000, 
raw observation next is [1.8, 27.66666666666667, 87.5, 834.1666666666667, 19.0, 18.71827845732563, -1.159400372368723, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5124653739612189, 0.2766666666666667, 0.2916666666666667, 0.921731123388582, 0.08333333333333333, 0.05985653811046928, 0.11353320921042569, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.749732], dtype=float32), -0.1433649]. 
=============================================
[2019-04-04 05:37:41,288] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0784075e-05 9.9974626e-01 3.6506562e-18 5.9939883e-17 1.7154046e-13
 2.3304723e-04 3.2675455e-13], sum to 1.0000
[2019-04-04 05:37:41,297] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6290
[2019-04-04 05:37:41,307] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.8, 72.33333333333334, 0.0, 0.0, 19.0, 18.71506357027214, -1.21905026527592, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2240400.0000, 
sim time next is 2241000.0000, 
raw observation next is [-5.9, 73.0, 0.0, 0.0, 19.0, 18.5481115614378, -1.228092378975838, 0.0, 1.0, 166925.9921162916], 
processed observation next is [1.0, 0.9565217391304348, 0.2991689750692521, 0.73, 0.0, 0.0, 0.08333333333333333, 0.04567596345314998, 0.09063587367472066, 0.0, 1.0, 0.7948856767442457], 
reward next is 0.2051, 
noisyNet noise sample is [array([-0.24635287], dtype=float32), -0.64876705]. 
=============================================
[2019-04-04 05:37:41,310] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[55.03702 ]
 [54.84317 ]
 [54.41315 ]
 [54.118877]
 [53.65537 ]], R is [[55.85388565]
 [56.29534912]
 [56.73239517]
 [57.16507339]
 [57.59342194]].
[2019-04-04 05:37:42,782] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5685936e-03 9.7838634e-01 2.1496369e-11 6.7810195e-11 1.7919101e-08
 1.7045062e-02 9.5110462e-09], sum to 1.0000
[2019-04-04 05:37:42,782] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5950
[2019-04-04 05:37:42,812] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.550000000000001, 82.5, 101.0, 39.0, 19.0, 19.47470787711543, -1.130738880957665, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2280600.0000, 
sim time next is 2281200.0000, 
raw observation next is [-7.266666666666667, 81.0, 113.8333333333333, 40.83333333333333, 19.0, 19.51249409469648, -1.119551271686287, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.26131117266851345, 0.81, 0.3794444444444443, 0.04511970534069981, 0.08333333333333333, 0.12604117455803987, 0.12681624277123768, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0433215], dtype=float32), 0.7360685]. 
=============================================
[2019-04-04 05:37:49,261] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.31029058e-04 9.98951197e-01 9.77096127e-15 2.67657966e-13
 1.16174556e-10 8.17779743e-04 1.58179650e-10], sum to 1.0000
[2019-04-04 05:37:49,263] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4955
[2019-04-04 05:37:49,283] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-14.3, 88.33333333333334, 0.0, 0.0, 19.0, 18.41618189070179, -1.23938692792686, 0.0, 1.0, 42193.76523894467], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2691600.0000, 
sim time next is 2692200.0000, 
raw observation next is [-14.65, 89.66666666666667, 0.0, 0.0, 19.0, 18.45497738419385, -1.241876696003103, 0.0, 1.0, 25091.75099661163], 
processed observation next is [1.0, 0.13043478260869565, 0.05678670360110801, 0.8966666666666667, 0.0, 0.0, 0.08333333333333333, 0.037914782016154135, 0.08604110133229903, 0.0, 1.0, 0.11948452855529348], 
reward next is 0.8805, 
noisyNet noise sample is [array([-1.2827139], dtype=float32), -0.43707573]. 
=============================================
[2019-04-04 05:37:52,421] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.5845002e-04 9.8895991e-01 1.1583055e-12 8.2229310e-12 1.5432546e-09
 1.0081663e-02 4.1634749e-09], sum to 1.0000
[2019-04-04 05:37:52,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7837
[2019-04-04 05:37:52,456] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.016666666666667, 35.16666666666666, 82.66666666666667, 811.6666666666667, 19.0, 19.36013982437995, -1.072995805147195, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2459400.0000, 
sim time next is 2460000.0000, 
raw observation next is [-1.733333333333333, 34.33333333333334, 84.33333333333334, 820.3333333333334, 19.0, 19.32117964338704, -1.075868592097731, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.41458910433979695, 0.34333333333333343, 0.28111111111111114, 0.9064456721915286, 0.08333333333333333, 0.1100983036155867, 0.14137713596742305, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1377362], dtype=float32), -0.2468011]. 
=============================================
[2019-04-04 05:37:52,459] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[40.645763]
 [40.7112  ]
 [40.70647 ]
 [40.73653 ]
 [40.64177 ]], R is [[41.48549652]
 [42.07064056]
 [42.64993286]
 [43.22343445]
 [43.79119873]].
[2019-04-04 05:37:52,617] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2740057e-04 9.9460185e-01 4.8598256e-13 3.6881895e-12 1.0393709e-09
 5.0707711e-03 2.7392240e-09], sum to 1.0000
[2019-04-04 05:37:52,620] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9915
[2019-04-04 05:37:52,639] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.733333333333334, 60.33333333333334, 0.0, 0.0, 19.0, 18.45574747300872, -1.305196285525254, 0.0, 1.0, 52493.39882802352], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2439600.0000, 
sim time next is 2440200.0000, 
raw observation next is [-8.816666666666666, 60.16666666666666, 0.0, 0.0, 19.0, 18.43923179711874, -1.304273462066964, 0.0, 1.0, 56975.57802783444], 
processed observation next is [0.0, 0.21739130434782608, 0.21837488457987075, 0.6016666666666666, 0.0, 0.0, 0.08333333333333333, 0.03660264975989502, 0.06524217931101199, 0.0, 1.0, 0.27131227632302113], 
reward next is 0.7287, 
noisyNet noise sample is [array([0.9114565], dtype=float32), -0.055344727]. 
=============================================
[2019-04-04 05:37:53,923] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.02922146e-03 9.53414261e-01 7.98540596e-13 1.09038585e-11
 7.18021109e-09 4.55563925e-02 3.89836652e-09], sum to 1.0000
[2019-04-04 05:37:53,924] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4111
[2019-04-04 05:37:53,938] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.1333333333333333, 30.0, 89.33333333333333, 842.3333333333334, 19.0, 18.85319892496901, -1.160988397595585, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2463600.0000, 
sim time next is 2464200.0000, 
raw observation next is [0.5, 29.5, 90.0, 845.0, 19.0, 18.80574528699722, -1.164251365342507, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4764542936288089, 0.295, 0.3, 0.9337016574585635, 0.08333333333333333, 0.06714544058310157, 0.11191621155249765, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6837877], dtype=float32), -0.31859258]. 
=============================================
[2019-04-04 05:37:55,158] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5367829e-05 9.9561161e-01 1.0195991e-15 3.9189907e-14 1.9413691e-11
 4.3430924e-03 3.9148802e-11], sum to 1.0000
[2019-04-04 05:37:55,161] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2618
[2019-04-04 05:37:55,176] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.7, 29.0, 0.0, 0.0, 19.0, 18.80439966668902, -1.206914714596361, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2491200.0000, 
sim time next is 2491800.0000, 
raw observation next is [-0.7833333333333333, 30.33333333333334, 0.0, 0.0, 19.0, 18.80154271302569, -1.214606033335052, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.44090489381348114, 0.3033333333333334, 0.0, 0.0, 0.08333333333333333, 0.06679522608547413, 0.09513132222164937, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1812869], dtype=float32), -0.320913]. 
=============================================
[2019-04-04 05:37:56,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1875319e-04 9.9707472e-01 1.2641730e-13 3.4533368e-13 2.2677006e-10
 2.7065729e-03 3.8973211e-10], sum to 1.0000
[2019-04-04 05:37:56,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0687
[2019-04-04 05:37:56,431] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 53.0, 0.0, 0.0, 19.0, 18.60106503947147, -1.253677059298305, 0.0, 1.0, 28145.91548787681], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2521800.0000, 
sim time next is 2522400.0000, 
raw observation next is [-2.1, 54.33333333333334, 0.0, 0.0, 19.0, 18.67664015398261, -1.248049646337164, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.404432132963989, 0.5433333333333334, 0.0, 0.0, 0.08333333333333333, 0.05638667949855088, 0.08398345122094535, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04010303], dtype=float32), 0.48938787]. 
=============================================
[2019-04-04 05:37:58,009] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2589078e-03 9.9092299e-01 1.2508075e-12 4.2985220e-12 7.6533739e-09
 6.8180519e-03 2.5136684e-09], sum to 1.0000
[2019-04-04 05:37:58,013] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7229
[2019-04-04 05:37:58,020] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.04999999999999999, 35.5, 0.0, 0.0, 19.0, 19.40194565213102, -1.079423358687011, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2572200.0000, 
sim time next is 2572800.0000, 
raw observation next is [-0.2333333333333333, 35.66666666666667, 0.0, 0.0, 19.0, 19.25899160283247, -1.095420451103005, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.456140350877193, 0.3566666666666667, 0.0, 0.0, 0.08333333333333333, 0.10491596690270584, 0.13485984963233166, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29949898], dtype=float32), 0.6894642]. 
=============================================
[2019-04-04 05:37:59,167] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2246306e-04 9.9835998e-01 2.0217571e-15 1.1349735e-14 6.4956473e-12
 1.4175874e-03 3.7569357e-11], sum to 1.0000
[2019-04-04 05:37:59,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6637
[2019-04-04 05:37:59,183] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.616666666666667, 78.83333333333333, 0.0, 0.0, 19.0, 18.50557927553607, -1.252401275380897, 0.0, 1.0, 26888.36167399518], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2613000.0000, 
sim time next is 2613600.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 19.0, 18.51758642757629, -1.257056497868947, 0.0, 1.0, 28893.39791200446], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.043132202298024204, 0.0809811673770177, 0.0, 1.0, 0.13758760910478315], 
reward next is 0.8624, 
noisyNet noise sample is [array([0.3783638], dtype=float32), -1.1898016]. 
=============================================
[2019-04-04 05:38:00,568] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0958607e-04 9.9678075e-01 6.2871961e-15 1.2739407e-13 5.6947513e-11
 3.0096194e-03 1.2314313e-10], sum to 1.0000
[2019-04-04 05:38:00,569] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2380
[2019-04-04 05:38:00,581] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 19.0, 18.69236784856773, -1.213590672081817, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2602800.0000, 
sim time next is 2603400.0000, 
raw observation next is [-5.100000000000001, 74.66666666666667, 0.0, 0.0, 19.0, 18.72266879079066, -1.220521673192011, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.32132963988919666, 0.7466666666666667, 0.0, 0.0, 0.08333333333333333, 0.06022239923255501, 0.09315944226932966, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49981642], dtype=float32), 0.8139535]. 
=============================================
[2019-04-04 05:38:01,445] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.8885202e-02 9.5131099e-01 1.2720247e-10 2.4404051e-10 9.6302074e-08
 1.9803721e-02 4.3469054e-08], sum to 1.0000
[2019-04-04 05:38:01,446] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6361
[2019-04-04 05:38:01,481] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 61.0, 144.6666666666667, 228.6666666666667, 19.0, 19.9089274014837, -1.018217117603677, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2798400.0000, 
sim time next is 2799000.0000, 
raw observation next is [-4.5, 59.5, 152.0, 233.0, 19.0, 19.97368348553302, -1.003013313960486, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3379501385041552, 0.595, 0.5066666666666667, 0.2574585635359116, 0.08333333333333333, 0.1644736237944183, 0.16566222867983801, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1500632], dtype=float32), 0.9611542]. 
=============================================
[2019-04-04 05:38:01,489] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[33.406418]
 [34.146908]
 [34.845936]
 [35.52472 ]
 [36.20174 ]], R is [[32.5295372 ]
 [32.20424271]
 [31.88220024]
 [31.56337929]
 [31.24774551]].
[2019-04-04 05:38:13,217] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1743239e-04 9.9735391e-01 5.3786969e-17 9.9899881e-16 3.9672983e-12
 2.4286832e-03 7.7315862e-12], sum to 1.0000
[2019-04-04 05:38:13,223] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6788
[2019-04-04 05:38:13,231] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.833333333333333, 65.0, 0.0, 0.0, 19.0, 18.85477591615991, -1.154775405008175, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3015600.0000, 
sim time next is 3016200.0000, 
raw observation next is [-3.916666666666667, 65.0, 0.0, 0.0, 19.0, 18.82284729589712, -1.164220305346751, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.3541089566020314, 0.65, 0.0, 0.0, 0.08333333333333333, 0.06857060799142663, 0.11192656488441632, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7416706], dtype=float32), 0.08771958]. 
=============================================
[2019-04-04 05:38:14,872] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2446241e-03 9.8833102e-01 1.7173005e-13 9.0489557e-13 7.1128536e-10
 9.4242906e-03 5.5798444e-10], sum to 1.0000
[2019-04-04 05:38:14,878] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6925
[2019-04-04 05:38:14,893] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 95.33333333333334, 60.16666666666667, 51.83333333333333, 19.0, 19.3858676715691, -1.119922378939745, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2911200.0000, 
sim time next is 2911800.0000, 
raw observation next is [2.0, 94.16666666666666, 49.33333333333334, 49.66666666666667, 19.0, 19.05425510867068, -1.13189018838129, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.9416666666666665, 0.16444444444444448, 0.05488029465930019, 0.08333333333333333, 0.0878545923892234, 0.12270327053957002, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2820138], dtype=float32), 0.90503544]. 
=============================================
[2019-04-04 05:38:16,345] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.8915215e-04 9.9757832e-01 6.1822373e-14 1.2197684e-12 2.4486030e-10
 1.9326197e-03 1.1312813e-09], sum to 1.0000
[2019-04-04 05:38:16,345] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8428
[2019-04-04 05:38:16,364] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 75.83333333333334, 0.0, 0.0, 19.0, 18.52810777090171, -1.244997507550197, 0.0, 1.0, 39699.72350671432], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3042600.0000, 
sim time next is 3043200.0000, 
raw observation next is [-6.0, 74.66666666666667, 0.0, 0.0, 19.0, 18.52417079979318, -1.242478794226104, 0.0, 1.0, 40795.22258883739], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.7466666666666667, 0.0, 0.0, 0.08333333333333333, 0.04368089998276492, 0.085840401924632, 0.0, 1.0, 0.19426296470874946], 
reward next is 0.8057, 
noisyNet noise sample is [array([0.90459096], dtype=float32), 1.2848568]. 
=============================================
[2019-04-04 05:38:17,894] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2014843e-04 9.9807429e-01 7.5189262e-17 4.4419585e-15 3.2334758e-12
 1.7055507e-03 2.8442405e-12], sum to 1.0000
[2019-04-04 05:38:17,894] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9108
[2019-04-04 05:38:17,912] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 19.05196537069813, -0.9683203560988193, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3208200.0000, 
sim time next is 3208800.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 19.11982290020307, -0.9639665041093816, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.09331857501692238, 0.17867783196353948, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0081383], dtype=float32), -1.0957882]. 
=============================================
[2019-04-04 05:38:25,521] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6486347e-03 9.8109794e-01 4.0324265e-13 3.6733398e-12 1.7077855e-09
 1.7253505e-02 1.4149643e-09], sum to 1.0000
[2019-04-04 05:38:25,521] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3364
[2019-04-04 05:38:25,700] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.5, 54.5, 111.0, 805.0, 19.0, 18.82350488899506, -1.12399837457735, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3065400.0000, 
sim time next is 3066000.0000, 
raw observation next is [-3.333333333333333, 54.66666666666667, 111.5, 807.0, 19.0, 18.75977608702513, -1.130693237369426, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.37026777469990774, 0.5466666666666667, 0.37166666666666665, 0.8917127071823204, 0.08333333333333333, 0.06331467391876089, 0.12310225421019132, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21834423], dtype=float32), 1.4672437]. 
=============================================
[2019-04-04 05:38:25,753] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[41.58285 ]
 [41.491856]
 [41.43854 ]
 [41.393852]
 [41.45376 ]], R is [[42.30744171]
 [42.8843689 ]
 [43.45552444]
 [44.02096939]
 [44.58076096]].
[2019-04-04 05:38:36,637] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0801728e-03 9.8851049e-01 7.8272380e-12 2.8720111e-11 6.3334196e-09
 9.4092973e-03 6.9074058e-09], sum to 1.0000
[2019-04-04 05:38:36,637] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9836
[2019-04-04 05:38:36,660] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-9.666666666666668, 79.33333333333333, 89.0, 432.5, 19.0, 19.2893679301001, -1.024363915780516, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3314400.0000, 
sim time next is 3315000.0000, 
raw observation next is [-9.333333333333332, 78.16666666666667, 92.0, 469.0, 19.0, 19.41143997858866, -0.9941817277782073, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.20406278855032323, 0.7816666666666667, 0.30666666666666664, 0.518232044198895, 0.08333333333333333, 0.11761999821572171, 0.16860609074059754, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.96228045], dtype=float32), 0.10924725]. 
=============================================
[2019-04-04 05:38:36,695] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[37.09773 ]
 [37.91863 ]
 [38.722248]
 [39.74126 ]
 [41.013298]], R is [[35.85695267]
 [35.49838257]
 [35.14339828]
 [34.79196548]
 [34.44404602]].
[2019-04-04 05:38:58,406] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8350428e-03 9.9099863e-01 4.6942992e-12 1.1258504e-11 1.3247322e-08
 6.1663091e-03 4.6656079e-09], sum to 1.0000
[2019-04-04 05:38:58,407] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8447
[2019-04-04 05:38:58,413] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 49.0, 108.5, 793.5, 19.0, 21.25281469236815, -0.6260350553095733, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3506400.0000, 
sim time next is 3507000.0000, 
raw observation next is [3.0, 49.0, 106.3333333333333, 789.3333333333334, 19.0, 21.27349256620376, -0.6188921830182093, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.35444444444444434, 0.8721915285451197, 0.08333333333333333, 0.2727910471836467, 0.2937026056605969, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.41954803], dtype=float32), -0.008616099]. 
=============================================
[2019-04-04 05:38:58,448] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[36.82047 ]
 [36.613632]
 [36.455887]
 [36.323883]
 [36.210777]], R is [[36.62236023]
 [36.25613785]
 [35.89357758]
 [35.53464127]
 [35.17929459]].
[2019-04-04 05:38:59,222] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2286027e-04 9.8922426e-01 1.2650423e-14 6.3904063e-13 1.7376313e-10
 1.0552826e-02 2.7684929e-10], sum to 1.0000
[2019-04-04 05:38:59,225] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0561
[2019-04-04 05:38:59,239] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.333333333333333, 70.0, 0.0, 0.0, 19.0, 18.57515707519028, -1.166537934928821, 0.0, 1.0, 38927.64314251126], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3568800.0000, 
sim time next is 3569400.0000, 
raw observation next is [-6.5, 70.0, 3.0, 121.0, 19.0, 18.57258598084913, -1.160097679852234, 0.0, 1.0, 39449.89909323274], 
processed observation next is [0.0, 0.30434782608695654, 0.28254847645429365, 0.7, 0.01, 0.13370165745856355, 0.08333333333333333, 0.04771549840409417, 0.11330077338258866, 0.0, 1.0, 0.18785666234872736], 
reward next is 0.8121, 
noisyNet noise sample is [array([-0.21622837], dtype=float32), -0.5317401]. 
=============================================
[2019-04-04 05:39:03,007] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1563429e-03 9.8797542e-01 2.4675869e-12 8.9768176e-12 1.4116181e-08
 6.8681678e-03 5.7657989e-09], sum to 1.0000
[2019-04-04 05:39:03,009] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4745
[2019-04-04 05:39:03,023] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.166666666666667, 65.5, 76.33333333333334, 640.3333333333334, 19.0, 21.35769341011772, -0.6287104568642781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3426600.0000, 
sim time next is 3427200.0000, 
raw observation next is [2.0, 67.0, 72.5, 608.5, 19.0, 21.40651696811678, -0.7033340489274731, 1.0, 1.0, 2668.630238604366], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.24166666666666667, 0.6723756906077348, 0.08333333333333333, 0.2838764140097316, 0.2655553170241756, 1.0, 1.0, 0.01270776304097317], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.35107237], dtype=float32), -0.75005144]. 
=============================================
[2019-04-04 05:39:05,013] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.5061751e-03 9.6374542e-01 8.7690681e-11 3.2432113e-10 3.6182328e-08
 2.7748283e-02 5.6953855e-08], sum to 1.0000
[2019-04-04 05:39:05,046] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0967
[2019-04-04 05:39:05,081] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.1666666666666666, 61.83333333333333, 102.3333333333333, 703.3333333333334, 19.0, 20.14394562015728, -0.8669997049914097, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3491400.0000, 
sim time next is 3492000.0000, 
raw observation next is [0.0, 60.0, 104.0, 720.0, 19.0, 20.28591082523232, -0.8436820657700791, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.6, 0.3466666666666667, 0.7955801104972375, 0.08333333333333333, 0.19049256876936008, 0.21877264474330696, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0838562], dtype=float32), -1.747716]. 
=============================================
[2019-04-04 05:39:05,085] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[34.516632]
 [35.403954]
 [36.46163 ]
 [37.201927]
 [38.228184]], R is [[33.476017  ]
 [33.14125824]
 [32.80984497]
 [32.48174667]
 [32.15692902]].
[2019-04-04 05:39:10,090] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.6254759e-04 9.9541360e-01 1.9012165e-15 1.3097380e-14 3.3130783e-11
 3.9239507e-03 2.6694907e-11], sum to 1.0000
[2019-04-04 05:39:10,090] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7475
[2019-04-04 05:39:10,096] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.3333333333333333, 68.66666666666666, 0.0, 0.0, 19.0, 20.11918991269625, -0.8141936386505506, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3527400.0000, 
sim time next is 3528000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 19.98584020507032, -0.8365791319211384, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.1654866837558601, 0.22114028935962052, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.61328566], dtype=float32), 1.9269981]. 
=============================================
[2019-04-04 05:39:10,106] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[48.152588]
 [46.853497]
 [45.703995]
 [44.400497]
 [42.855988]], R is [[49.74689484]
 [50.2494278 ]
 [50.74693298]
 [51.23946381]
 [51.72706985]].
[2019-04-04 05:39:19,496] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.32502628e-05 9.82068002e-01 3.04378782e-14 8.09728397e-14
 2.72531886e-10 1.78487059e-02 1.05944774e-10], sum to 1.0000
[2019-04-04 05:39:19,496] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0357
[2019-04-04 05:39:19,572] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 19.0, 18.55870352213859, -1.032684345877552, 0.0, 1.0, 112781.1397651602], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3961800.0000, 
sim time next is 3962400.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 19.0, 18.7404125453507, -1.009231625155633, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 0.08333333333333333, 0.06170104544589172, 0.16358945828145566, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0697155], dtype=float32), -0.5823342]. 
=============================================
[2019-04-04 05:39:24,878] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6837984e-05 9.9691260e-01 5.8826484e-18 1.2609940e-16 5.1098839e-13
 3.0605632e-03 1.3938964e-12], sum to 1.0000
[2019-04-04 05:39:24,881] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2673
[2019-04-04 05:39:24,901] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.166666666666667, 62.16666666666667, 0.0, 0.0, 19.0, 18.964040607886, -1.134888889029231, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3703800.0000, 
sim time next is 3704400.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 19.0, 18.90655606487606, -1.140569532376842, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.518005540166205, 0.62, 0.0, 0.0, 0.08333333333333333, 0.07554633873967169, 0.11981015587438597, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0066065], dtype=float32), -0.7809742]. 
=============================================
[2019-04-04 05:39:26,125] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.7014950e-04 9.9690777e-01 1.1552610e-14 5.8780780e-14 3.6317324e-11
 2.8221053e-03 1.3505856e-10], sum to 1.0000
[2019-04-04 05:39:26,125] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1977
[2019-04-04 05:39:26,193] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 18.62618022596659, -1.233266829393058, 0.0, 1.0, 196877.5086661895], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3733200.0000, 
sim time next is 3733800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 18.53683330700584, -1.215899421429531, 0.0, 1.0, 178609.4869797947], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.04473610891715326, 0.09470019285682303, 0.0, 1.0, 0.8505213665704509], 
reward next is 0.1495, 
noisyNet noise sample is [array([-1.6514783], dtype=float32), 1.3883498]. 
=============================================
[2019-04-04 05:39:27,966] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.2962588e-06 9.9760073e-01 7.8304589e-18 7.1731481e-17 2.6887203e-13
 2.3909239e-03 8.4100478e-13], sum to 1.0000
[2019-04-04 05:39:27,980] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7054
[2019-04-04 05:39:28,012] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 18.81931675585725, -1.109064627085169, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3717600.0000, 
sim time next is 3718200.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 19.0, 18.97317388578265, -1.102804903698309, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.08333333333333333, 0.08109782381522083, 0.13239836543389702, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.72545975], dtype=float32), 1.713454]. 
=============================================
[2019-04-04 05:39:41,545] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0650169e-04 9.9075890e-01 1.6392915e-14 2.5865009e-13 1.4155857e-10
 8.9345900e-03 1.8397159e-10], sum to 1.0000
[2019-04-04 05:39:41,545] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4438
[2019-04-04 05:39:41,594] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.5, 40.5, 160.0, 538.0, 19.0, 19.46710880281335, -0.9196201895040765, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4203000.0000, 
sim time next is 4203600.0000, 
raw observation next is [2.666666666666667, 39.33333333333333, 144.6666666666667, 540.0, 19.0, 19.6045735078476, -0.90751940220771, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5364727608494922, 0.3933333333333333, 0.4822222222222224, 0.5966850828729282, 0.08333333333333333, 0.13371445898730006, 0.19749353259743, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09145312], dtype=float32), -0.4700826]. 
=============================================
[2019-04-04 05:39:49,704] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.1242674e-06 9.9817669e-01 8.3191538e-20 5.7099550e-18 4.4324123e-14
 1.8191101e-03 1.9660281e-13], sum to 1.0000
[2019-04-04 05:39:49,704] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3879
[2019-04-04 05:39:49,758] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.433333333333334, 75.33333333333334, 0.0, 0.0, 19.0, 19.14638204977434, -1.078331983149547, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4317600.0000, 
sim time next is 4318200.0000, 
raw observation next is [4.45, 75.5, 0.0, 0.0, 19.0, 19.13877702427342, -1.08633785348525, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 1.0, 0.5858725761772854, 0.755, 0.0, 0.0, 0.08333333333333333, 0.09489808535611832, 0.13788738217158336, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5821869], dtype=float32), -0.18616508]. 
=============================================
[2019-04-04 05:39:52,695] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9987493e-02 9.0032846e-01 3.4307121e-10 4.5537535e-10 3.8880458e-07
 5.9683550e-02 1.0665668e-07], sum to 1.0000
[2019-04-04 05:39:52,697] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8337
[2019-04-04 05:39:52,716] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.5, 28.5, 0.0, 0.0, 19.0, 20.04370493196792, -0.8933399314062783, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4041000.0000, 
sim time next is 4041600.0000, 
raw observation next is [-3.666666666666667, 29.33333333333333, 0.0, 0.0, 19.0, 19.87496680294572, -0.9139843814626668, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3610341643582641, 0.2933333333333333, 0.0, 0.0, 0.08333333333333333, 0.1562472335788101, 0.1953385395124444, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.2426794], dtype=float32), -0.5527236]. 
=============================================
[2019-04-04 05:39:53,352] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6804352e-04 9.8433506e-01 3.0834941e-17 9.1201911e-16 2.0035818e-12
 1.5296856e-02 4.5191737e-12], sum to 1.0000
[2019-04-04 05:39:53,352] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1619
[2019-04-04 05:39:53,382] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.333333333333333, 62.66666666666667, 20.0, 190.0, 19.0, 20.31823809772764, -0.8215166487199087, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4297200.0000, 
sim time next is 4297800.0000, 
raw observation next is [6.266666666666667, 63.33333333333334, 16.0, 152.0, 19.0, 20.29216000821831, -0.8325292755544019, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.6361957525392429, 0.6333333333333334, 0.05333333333333334, 0.16795580110497238, 0.08333333333333333, 0.19101333401819254, 0.22249024148186602, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17812036], dtype=float32), 1.1226748]. 
=============================================
[2019-04-04 05:39:53,868] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3884876e-05 9.9750096e-01 8.6683512e-17 1.2078283e-15 7.8080754e-12
 2.4451269e-03 1.6942742e-11], sum to 1.0000
[2019-04-04 05:39:53,887] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6577
[2019-04-04 05:39:53,893] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.133333333333333, 42.66666666666666, 0.0, 0.0, 19.0, 19.33399684236293, -1.069785743947889, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4218000.0000, 
sim time next is 4218600.0000, 
raw observation next is [1.066666666666667, 42.83333333333334, 0.0, 0.0, 19.0, 19.2724347539764, -1.084768720566901, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.49215143120960303, 0.42833333333333345, 0.0, 0.0, 0.08333333333333333, 0.1060362294980332, 0.13841042647769966, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1301178], dtype=float32), 0.2359292]. 
=============================================
[2019-04-04 05:40:18,489] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7802400e-05 9.9910778e-01 3.5327165e-20 6.7964137e-19 1.5402458e-15
 8.7443460e-04 1.5505987e-14], sum to 1.0000
[2019-04-04 05:40:18,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2435
[2019-04-04 05:40:18,511] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.333333333333333, 79.33333333333334, 0.0, 0.0, 19.0, 18.59516968075586, -1.105439274630557, 0.0, 1.0, 66885.73674716415], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4749600.0000, 
sim time next is 4750200.0000, 
raw observation next is [-3.5, 80.5, 0.0, 0.0, 19.0, 18.62735714357428, -1.100739811756503, 0.0, 1.0, 30705.84922546448], 
processed observation next is [1.0, 1.0, 0.36565096952908593, 0.805, 0.0, 0.0, 0.08333333333333333, 0.052279761964523296, 0.13308672941449898, 0.0, 1.0, 0.14621832964506895], 
reward next is 0.8538, 
noisyNet noise sample is [array([-0.52523726], dtype=float32), 1.5652511]. 
=============================================
[2019-04-04 05:40:19,897] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0760897e-03 9.8904896e-01 6.4234570e-15 2.0338550e-13 7.7703385e-11
 9.8749474e-03 3.4306646e-10], sum to 1.0000
[2019-04-04 05:40:19,899] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8454
[2019-04-04 05:40:19,937] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 92.0, 63.0, 0.0, 19.0, 19.1325110074414, -1.051519064852539, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4696200.0000, 
sim time next is 4696800.0000, 
raw observation next is [0.0, 92.0, 71.66666666666666, 0.0, 19.0, 19.4296106277061, -1.037579986871351, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.23888888888888885, 0.0, 0.08333333333333333, 0.1191342189755084, 0.15414000437621636, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.71176183], dtype=float32), -0.32653767]. 
=============================================
[2019-04-04 05:40:28,713] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9011341e-04 9.8907125e-01 6.6583923e-17 1.7524143e-15 6.3386588e-12
 1.0638635e-02 1.2767618e-11], sum to 1.0000
[2019-04-04 05:40:28,713] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7891
[2019-04-04 05:40:28,748] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.8333333333333334, 47.66666666666667, 0.0, 0.0, 19.0, 19.24510382092817, -1.061408206505102, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4824600.0000, 
sim time next is 4825200.0000, 
raw observation next is [0.6666666666666667, 48.33333333333334, 0.0, 0.0, 19.0, 19.18456250012412, -1.075153178613404, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4810710987996307, 0.48333333333333345, 0.0, 0.0, 0.08333333333333333, 0.09871354167700996, 0.1416156071288653, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05747184], dtype=float32), -1.8877592]. 
=============================================
[2019-04-04 05:40:33,102] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2268551e-03 9.7648036e-01 1.1523678e-14 7.7379631e-14 1.1382757e-10
 2.2292782e-02 9.1293265e-11], sum to 1.0000
[2019-04-04 05:40:33,102] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4517
[2019-04-04 05:40:33,108] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.833333333333333, 52.83333333333334, 0.0, 0.0, 19.0, 21.25682524956362, -0.565092042918036, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4648200.0000, 
sim time next is 4648800.0000, 
raw observation next is [2.666666666666667, 52.66666666666667, 0.0, 0.0, 19.0, 21.16098863095493, -0.583750638907658, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5364727608494922, 0.5266666666666667, 0.0, 0.0, 0.08333333333333333, 0.2634157192462441, 0.30541645369744735, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2988841], dtype=float32), 0.68819344]. 
=============================================
[2019-04-04 05:40:35,374] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0981207e-05 9.9900264e-01 4.8599100e-19 1.0835914e-17 4.6291164e-14
 9.8645699e-04 7.0961676e-14], sum to 1.0000
[2019-04-04 05:40:35,384] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5509
[2019-04-04 05:40:35,393] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.166666666666667, 39.0, 0.0, 0.0, 19.0, 19.54185198500991, -0.9568564485920469, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5010600.0000, 
sim time next is 5011200.0000, 
raw observation next is [2.0, 40.0, 0.0, 0.0, 19.0, 19.47054207785662, -0.9729525856989496, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.4, 0.0, 0.0, 0.08333333333333333, 0.1225451731547184, 0.17568247143368346, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05647866], dtype=float32), -0.84763396]. 
=============================================
[2019-04-04 05:40:36,779] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6504518e-03 9.7947985e-01 6.4002118e-13 2.1548887e-12 2.0301765e-09
 1.7869756e-02 2.4493085e-09], sum to 1.0000
[2019-04-04 05:40:36,780] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6681
[2019-04-04 05:40:36,789] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 92.0, 208.8333333333333, 6.0, 19.0, 21.01853218645628, -0.7302900239076978, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4704000.0000, 
sim time next is 4704600.0000, 
raw observation next is [0.0, 92.0, 209.6666666666667, 6.0, 19.0, 21.06417620804876, -0.7236107015912014, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.92, 0.698888888888889, 0.0066298342541436465, 0.08333333333333333, 0.2553480173373967, 0.25879643280293285, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.11804507], dtype=float32), -1.1901076]. 
=============================================
[2019-04-04 05:40:39,182] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 05:40:39,188] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:40:39,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:40:39,192] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run5
[2019-04-04 05:40:39,205] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:40:39,208] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:40:39,209] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:40:39,211] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:40:39,213] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run5
[2019-04-04 05:40:39,233] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run5
[2019-04-04 05:40:48,037] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23425548], dtype=float32), 0.19401155]
[2019-04-04 05:40:48,037] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.5, 58.0, 0.0, 0.0, 19.0, 18.97993143472809, -1.08627054494492, 0.0, 1.0, 0.0]
[2019-04-04 05:40:48,037] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:40:48,038] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.3823673e-04 9.8858380e-01 1.3579915e-17 4.7593717e-16 2.4879497e-12
 1.1278003e-02 4.2055422e-12], sampled 0.3252178347221292
[2019-04-04 05:42:49,006] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23425548], dtype=float32), 0.19401155]
[2019-04-04 05:42:49,006] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.8, 83.0, 0.0, 0.0, 19.0, 18.96458984413746, -1.048322716246439, 0.0, 1.0, 0.0]
[2019-04-04 05:42:49,006] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:42:49,007] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.29770910e-04 9.95913804e-01 1.04603586e-16 2.28287195e-15
 2.94742746e-12 3.95641942e-03 1.07041173e-11], sampled 0.7132117964940865
[2019-04-04 05:42:50,650] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5622.5073 140638952.3560 -2124.6698
[2019-04-04 05:43:07,536] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5065.5382 169716815.4421 -2714.7941
[2019-04-04 05:43:13,483] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5155.4566 183047349.5963 -2635.9248
[2019-04-04 05:43:14,506] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 400000, evaluation results [400000.0, 5065.538183316079, 169716815.44206113, -2714.794144014607, 5622.507342109809, 140638952.35600382, -2124.6698170663785, 5155.456640194983, 183047349.59634444, -2635.9247691394]
[2019-04-04 05:43:14,939] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3574171e-03 9.7612017e-01 3.1686266e-14 4.3345880e-13 1.5529820e-10
 2.2522397e-02 2.5922720e-10], sum to 1.0000
[2019-04-04 05:43:14,945] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3334
[2019-04-04 05:43:14,952] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 92.0, 71.66666666666666, 0.0, 19.0, 19.77274349018896, -0.972145024641203, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4696800.0000, 
sim time next is 4697400.0000, 
raw observation next is [0.0, 92.0, 80.33333333333333, 0.0, 19.0, 19.85065788522282, -0.9633772678596241, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.2677777777777778, 0.0, 0.08333333333333333, 0.15422149043523495, 0.17887424404679197, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7248977], dtype=float32), 0.28717136]. 
=============================================
[2019-04-04 05:43:15,448] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8246170e-04 9.4351524e-01 2.1666842e-15 6.2796488e-14 1.4371066e-11
 5.6202300e-02 1.2364798e-10], sum to 1.0000
[2019-04-04 05:43:15,451] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9726
[2019-04-04 05:43:15,465] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.066666666666666, 92.33333333333334, 0.0, 0.0, 19.0, 18.54090984750235, -1.181222965632202, 0.0, 1.0, 37175.52687433704], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4774800.0000, 
sim time next is 4775400.0000, 
raw observation next is [-6.1, 92.5, 0.0, 0.0, 20.0, 18.55509286324215, -1.171163133542846, 0.0, 1.0, 128295.9843885768], 
processed observation next is [0.0, 0.2608695652173913, 0.29362880886426596, 0.925, 0.0, 0.0, 0.16666666666666666, 0.04625773860351264, 0.10961228881905132, 0.0, 1.0, 0.6109332589932229], 
reward next is 0.3891, 
noisyNet noise sample is [array([0.80209935], dtype=float32), -1.264303]. 
=============================================
[2019-04-04 05:43:16,142] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9879712e-04 9.9280554e-01 8.1336981e-17 1.4195267e-15 2.6376518e-12
 6.8956795e-03 6.6116322e-12], sum to 1.0000
[2019-04-04 05:43:16,146] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3992
[2019-04-04 05:43:16,165] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.8333333333333334, 40.5, 0.0, 0.0, 19.0, 18.95841637650371, -1.153713855904819, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4925400.0000, 
sim time next is 4926000.0000, 
raw observation next is [0.6666666666666667, 41.0, 0.0, 0.0, 19.0, 18.93925557510176, -1.163593023807277, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.4810710987996307, 0.41, 0.0, 0.0, 0.08333333333333333, 0.07827129792514675, 0.1121356587309077, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42500615], dtype=float32), 0.38717222]. 
=============================================
[2019-04-04 05:43:16,184] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[58.10319 ]
 [58.73335 ]
 [59.511013]
 [59.452847]
 [59.524708]], R is [[57.41005707]
 [57.83595657]
 [58.25759888]
 [58.67502213]
 [59.08827209]].
[2019-04-04 05:43:16,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:16,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:16,606] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run4
[2019-04-04 05:43:17,593] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1031686e-04 9.8788249e-01 1.1007019e-14 8.4413901e-14 2.5212241e-10
 1.1707286e-02 7.2087968e-11], sum to 1.0000
[2019-04-04 05:43:17,593] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6953
[2019-04-04 05:43:17,612] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.5, 55.5, 153.0, 730.0, 19.0, 18.67900482986557, -1.079102857963157, 0.0, 1.0, 18698.98334091295], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4789800.0000, 
sim time next is 4790400.0000, 
raw observation next is [-2.333333333333333, 52.33333333333333, 147.8333333333333, 748.1666666666667, 19.0, 18.68345824515885, -1.072610255645402, 0.0, 1.0, 18698.68718662848], 
processed observation next is [0.0, 0.43478260869565216, 0.3979686057248385, 0.5233333333333333, 0.4927777777777776, 0.8267034990791897, 0.08333333333333333, 0.056954853763237466, 0.14246324811819933, 0.0, 1.0, 0.0890413675553737], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.40860924], dtype=float32), -0.034948923]. 
=============================================
[2019-04-04 05:43:18,342] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9332179e-04 9.8466057e-01 5.0745516e-18 1.3099379e-16 4.4973419e-13
 1.5046072e-02 1.6835375e-12], sum to 1.0000
[2019-04-04 05:43:18,343] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8094
[2019-04-04 05:43:18,360] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 19.0, 18.73133646487727, -1.104454630366661, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4831800.0000, 
sim time next is 4832400.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 19.0, 18.85042760421664, -1.100139354246647, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.08333333333333333, 0.07086896701805341, 0.1332868819177843, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4018244], dtype=float32), 0.81928664]. 
=============================================
[2019-04-04 05:43:20,201] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:20,201] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:20,210] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run4
[2019-04-04 05:43:21,001] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0951957e-04 9.9159944e-01 9.6381948e-15 1.6454214e-13 1.5178724e-10
 8.1910510e-03 1.3496709e-10], sum to 1.0000
[2019-04-04 05:43:21,002] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7689
[2019-04-04 05:43:21,010] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.333333333333333, 45.33333333333333, 278.0, 389.3333333333334, 19.0, 19.67974293616768, -0.9235325319883932, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4884600.0000, 
sim time next is 4885200.0000, 
raw observation next is [1.4, 45.0, 276.5, 389.0, 19.0, 19.67900675074347, -0.9186503780497395, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5013850415512465, 0.45, 0.9216666666666666, 0.4298342541436464, 0.08333333333333333, 0.13991722922862238, 0.1937832073167535, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2520385], dtype=float32), 1.0124686]. 
=============================================
[2019-04-04 05:43:23,125] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2543557e-04 9.8943406e-01 1.2580047e-14 1.2617854e-13 1.8655691e-10
 1.0240480e-02 1.3581200e-10], sum to 1.0000
[2019-04-04 05:43:23,130] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6760
[2019-04-04 05:43:23,141] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 19.0, 18.76937303636783, -1.24112652662558, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4933800.0000, 
sim time next is 4934400.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 19.0, 18.65639909425967, -1.244175760974513, 0.0, 1.0, 196227.3825423508], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.08333333333333333, 0.054699924521639076, 0.08527474634182901, 0.0, 1.0, 0.9344161073445276], 
reward next is 0.0656, 
noisyNet noise sample is [array([-0.3691868], dtype=float32), -0.7573569]. 
=============================================
[2019-04-04 05:43:23,848] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.77700524e-04 9.90260541e-01 1.16254747e-14 1.06077975e-13
 1.08571339e-10 9.56177339e-03 2.92266128e-10], sum to 1.0000
[2019-04-04 05:43:23,848] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5147
[2019-04-04 05:43:23,861] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 47.33333333333333, 0.0, 0.0, 19.0, 18.80094314451436, -1.230407610876881, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4941600.0000, 
sim time next is 4942200.0000, 
raw observation next is [-2.0, 46.66666666666667, 0.0, 0.0, 19.0, 18.75693572249404, -1.241309634082527, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.40720221606648205, 0.46666666666666673, 0.0, 0.0, 0.08333333333333333, 0.06307797687450319, 0.08623012197249103, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.61706406], dtype=float32), -1.8078656]. 
=============================================
[2019-04-04 05:43:24,094] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:24,095] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:24,124] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run4
[2019-04-04 05:43:24,139] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:24,139] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:24,184] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run4
[2019-04-04 05:43:25,712] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.9256148e-06 9.9934369e-01 5.0832688e-20 3.1649938e-18 1.1116173e-14
 6.4940617e-04 5.0826154e-14], sum to 1.0000
[2019-04-04 05:43:25,712] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6810
[2019-04-04 05:43:25,718] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 35.0, 0.0, 0.0, 19.0, 20.08413172454592, -0.8418360888645203, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5006400.0000, 
sim time next is 5007000.0000, 
raw observation next is [3.0, 34.5, 0.0, 0.0, 19.0, 19.98583230333677, -0.8600029712356901, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.345, 0.0, 0.0, 0.08333333333333333, 0.16548602527806425, 0.21333234292143663, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28681633], dtype=float32), 0.6557479]. 
=============================================
[2019-04-04 05:43:25,754] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[65.81353 ]
 [65.67035 ]
 [65.35845 ]
 [65.180405]
 [64.855064]], R is [[66.11526489]
 [66.45410919]
 [66.78956604]
 [67.12167358]
 [67.45045471]].
[2019-04-04 05:43:26,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:26,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:26,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run4
[2019-04-04 05:43:27,047] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:27,048] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:27,066] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run4
[2019-04-04 05:43:29,114] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3315843e-04 9.9111432e-01 2.0127195e-15 2.3895721e-14 4.1263132e-11
 8.2525937e-03 7.7328794e-12], sum to 1.0000
[2019-04-04 05:43:29,115] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4096
[2019-04-04 05:43:29,133] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.0, 19.0, 86.0, 665.0, 19.0, 23.73978770281349, -0.0694352909813719, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5068800.0000, 
sim time next is 5069400.0000, 
raw observation next is [12.0, 18.66666666666667, 82.66666666666666, 638.3333333333334, 19.0, 23.79967461904217, -0.05406916435309059, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.1866666666666667, 0.2755555555555555, 0.705340699815838, 0.08333333333333333, 0.48330621825351433, 0.48197694521563644, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35985318], dtype=float32), 1.8443512]. 
=============================================
[2019-04-04 05:43:29,915] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4854087e-05 9.9646348e-01 7.7536986e-21 1.0367767e-19 5.5011080e-15
 3.5117366e-03 1.4628855e-14], sum to 1.0000
[2019-04-04 05:43:29,917] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0652
[2019-04-04 05:43:29,925] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.4, 20.0, 0.0, 0.0, 19.0, 21.64035150313219, -0.4797893581038966, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5094000.0000, 
sim time next is 5094600.0000, 
raw observation next is [8.350000000000001, 22.5, 0.0, 0.0, 19.0, 21.56086020874429, -0.4960938922315284, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6939058171745154, 0.225, 0.0, 0.0, 0.08333333333333333, 0.2967383507286909, 0.3346353692561572, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22885443], dtype=float32), -0.51757544]. 
=============================================
[2019-04-04 05:43:30,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:30,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:30,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run4
[2019-04-04 05:43:30,634] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:30,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:30,650] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run4
[2019-04-04 05:43:31,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:31,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:31,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run4
[2019-04-04 05:43:31,140] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:31,140] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:31,142] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run4
[2019-04-04 05:43:31,158] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:31,158] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:31,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run4
[2019-04-04 05:43:31,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:31,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:31,275] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run4
[2019-04-04 05:43:31,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:31,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:31,475] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run4
[2019-04-04 05:43:31,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:31,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:31,607] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run4
[2019-04-04 05:43:31,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:31,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:31,907] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run4
[2019-04-04 05:43:32,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:43:32,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:32,515] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run4
[2019-04-04 05:43:32,944] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4218960e-05 9.9752563e-01 8.7106885e-19 6.7293819e-18 7.2560881e-14
 2.4601293e-03 2.2804886e-13], sum to 1.0000
[2019-04-04 05:43:32,944] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3642
[2019-04-04 05:43:32,980] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 19.17677685334738, -1.041642572480249, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 15600.0000, 
sim time next is 16200.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 19.11312610230345, -1.054176227482964, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.09276050852528754, 0.14860792417234533, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45754933], dtype=float32), 0.41057536]. 
=============================================
[2019-04-04 05:43:50,802] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4906380e-04 9.8129934e-01 8.6235798e-15 1.1877405e-13 1.4821526e-10
 1.8351644e-02 8.6240577e-11], sum to 1.0000
[2019-04-04 05:43:50,803] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2299
[2019-04-04 05:43:50,902] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.38333333333333, 67.5, 0.0, 0.0, 19.0, 18.42688794345612, -1.307284610093908, 0.0, 1.0, 34844.84628785374], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 285000.0000, 
sim time next is 285600.0000, 
raw observation next is [-12.46666666666667, 68.0, 0.0, 0.0, 19.0, 18.36522724399304, -1.26122851516877, 1.0, 1.0, 106204.5651042335], 
processed observation next is [1.0, 0.30434782608695654, 0.11726685133887339, 0.68, 0.0, 0.0, 0.08333333333333333, 0.030435603666086664, 0.0795904949437433, 1.0, 1.0, 0.5057360243058738], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3733529], dtype=float32), -0.75968486]. 
=============================================
[2019-04-04 05:43:53,093] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9074559e-03 9.6177697e-01 6.1293847e-13 4.2257478e-12 4.1921493e-09
 3.5315536e-02 5.3082960e-09], sum to 1.0000
[2019-04-04 05:43:53,093] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0623
[2019-04-04 05:43:53,182] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 19.0, 18.87011493182534, -1.471153512259251, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 236400.0000, 
sim time next is 237000.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 18.16414715200368, -1.357926921210757, 1.0, 1.0, 196217.9094192962], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.0136789293336399, 0.047357692929747675, 1.0, 1.0, 0.9343709972347438], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5994948], dtype=float32), 0.34534588]. 
=============================================
[2019-04-04 05:43:53,189] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[42.74324 ]
 [42.551495]
 [42.35853 ]
 [42.120514]
 [40.661854]], R is [[43.54473114]
 [43.10928345]
 [42.67819214]
 [42.25141144]
 [41.82889938]].
[2019-04-04 05:43:53,959] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.7673268e-05 9.9481946e-01 2.0937826e-17 1.5575536e-16 1.7326028e-12
 5.1328903e-03 3.9273110e-13], sum to 1.0000
[2019-04-04 05:43:53,960] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9674
[2019-04-04 05:43:54,006] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 19.0, 18.37013954670277, -1.219914645335932, 0.0, 1.0, 114674.8433650028], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 247200.0000, 
sim time next is 247800.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 18.59923466161955, -1.199585449692675, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.049936221801629266, 0.100138183435775, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2595761], dtype=float32), -1.8230352]. 
=============================================
[2019-04-04 05:43:54,507] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1602077e-05 9.9562526e-01 5.6502593e-19 7.3292967e-18 9.0332974e-14
 4.3131607e-03 1.2143903e-13], sum to 1.0000
[2019-04-04 05:43:54,508] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0001
[2019-04-04 05:43:54,527] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 77.33333333333334, 0.0, 0.0, 19.0, 18.59001143890915, -1.228310816794671, 0.0, 1.0, 21390.48929828823], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 253200.0000, 
sim time next is 253800.0000, 
raw observation next is [-3.9, 78.5, 0.0, 0.0, 19.0, 18.60464950900823, -1.231278807171608, 0.0, 1.0, 22018.99271928301], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.785, 0.0, 0.0, 0.08333333333333333, 0.05038745908401913, 0.0895737309427973, 0.0, 1.0, 0.10485234628230006], 
reward next is 0.8951, 
noisyNet noise sample is [array([0.1378604], dtype=float32), 0.07285986]. 
=============================================
[2019-04-04 05:43:55,184] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7851633e-04 9.9216050e-01 1.2121840e-16 1.1498599e-15 2.3290040e-12
 7.6610553e-03 7.3032413e-12], sum to 1.0000
[2019-04-04 05:43:55,186] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1146
[2019-04-04 05:43:55,233] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-13.0, 78.66666666666667, 0.0, 0.0, 19.0, 18.49504102994221, -1.254313293622964, 0.0, 1.0, 74892.63362535971], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 336000.0000, 
sim time next is 336600.0000, 
raw observation next is [-13.1, 79.5, 0.0, 0.0, 19.0, 18.46598980485928, -1.254546557288813, 0.0, 1.0, 74349.50153783237], 
processed observation next is [1.0, 0.9130434782608695, 0.0997229916897507, 0.795, 0.0, 0.0, 0.08333333333333333, 0.03883248373827334, 0.08181781423706232, 0.0, 1.0, 0.3540452454182494], 
reward next is 0.6460, 
noisyNet noise sample is [array([-0.5924307], dtype=float32), 0.10244511]. 
=============================================
[2019-04-04 05:43:59,955] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.8569960e-03 8.6552370e-01 4.3345232e-12 1.0426399e-11 1.8820044e-08
 1.2561923e-01 5.8770477e-09], sum to 1.0000
[2019-04-04 05:43:59,955] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3154
[2019-04-04 05:44:00,054] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.0, 60.0, 0.0, 0.0, 19.0, 18.29171653001317, -1.232966210907855, 1.0, 1.0, 197151.3215368382], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 325800.0000, 
sim time next is 326400.0000, 
raw observation next is [-12.1, 61.0, 0.0, 0.0, 19.0, 18.37698107837315, -1.162214602028711, 1.0, 1.0, 198753.8234066807], 
processed observation next is [1.0, 0.782608695652174, 0.12742382271468145, 0.61, 0.0, 0.0, 0.08333333333333333, 0.031415089864429056, 0.11259513265709635, 1.0, 1.0, 0.9464467781270509], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.914352], dtype=float32), 1.3047857]. 
=============================================
[2019-04-04 05:44:03,607] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7879561e-03 9.1688168e-01 6.1757717e-13 3.8348305e-12 7.7132434e-10
 8.1330396e-02 1.4391695e-09], sum to 1.0000
[2019-04-04 05:44:03,609] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5073
[2019-04-04 05:44:03,633] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 19.0, 18.29074790528332, -1.324860180232381, 0.0, 1.0, 82547.56171172648], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 356400.0000, 
sim time next is 357000.0000, 
raw observation next is [-15.1, 69.66666666666667, 0.0, 0.0, 19.0, 18.26732058909901, -1.320489618868027, 0.0, 1.0, 63576.27690396355], 
processed observation next is [1.0, 0.13043478260869565, 0.0443213296398892, 0.6966666666666668, 0.0, 0.0, 0.08333333333333333, 0.022276715758250926, 0.05983679371065764, 0.0, 1.0, 0.30274417573315976], 
reward next is 0.6973, 
noisyNet noise sample is [array([0.01218874], dtype=float32), 1.5848633]. 
=============================================
[2019-04-04 05:44:03,671] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[45.78689 ]
 [46.21833 ]
 [46.640965]
 [47.190346]
 [47.91859 ]], R is [[45.342453  ]
 [45.49594498]
 [45.50146103]
 [45.76911926]
 [46.22000122]].
[2019-04-04 05:44:06,702] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.3411080e-05 9.4504863e-01 1.2426190e-20 3.1849400e-19 2.0822630e-15
 5.4897949e-02 3.1649384e-15], sum to 1.0000
[2019-04-04 05:44:06,708] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5365
[2019-04-04 05:44:06,724] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.8, 92.66666666666667, 0.0, 0.0, 19.0, 18.89167674464951, -1.164559594776586, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 511800.0000, 
sim time next is 512400.0000, 
raw observation next is [2.9, 93.33333333333334, 0.0, 0.0, 19.0, 18.93940160376997, -1.165023172001461, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5429362880886427, 0.9333333333333335, 0.0, 0.0, 0.08333333333333333, 0.07828346698083077, 0.11165894266617966, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.68288434], dtype=float32), 0.7258617]. 
=============================================
[2019-04-04 05:44:09,524] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4302679e-04 9.8144639e-01 1.1654692e-16 5.9101025e-15 5.7394866e-12
 1.8410632e-02 1.0071038e-11], sum to 1.0000
[2019-04-04 05:44:09,537] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0378
[2019-04-04 05:44:09,565] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 74.0, 0.0, 0.0, 19.0, 18.65595281931299, -1.262074400319354, 0.0, 1.0, 32501.61807781214], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 697200.0000, 
sim time next is 697800.0000, 
raw observation next is [-3.4, 74.5, 0.0, 0.0, 19.0, 18.61451383970989, -1.261452232020886, 0.0, 1.0, 54314.55067939285], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.745, 0.0, 0.0, 0.08333333333333333, 0.05120948664249081, 0.07951592265970464, 0.0, 1.0, 0.25864071752091833], 
reward next is 0.7414, 
noisyNet noise sample is [array([-0.60237473], dtype=float32), -1.7441444]. 
=============================================
[2019-04-04 05:44:14,083] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3850611e-03 9.1352254e-01 6.5980567e-15 1.1438238e-13 1.5076683e-10
 8.5092388e-02 2.4314797e-10], sum to 1.0000
[2019-04-04 05:44:14,095] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1254
[2019-04-04 05:44:14,118] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 65.5, 0.0, 0.0, 19.0, 19.00174888943655, -1.175902549566941, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 627000.0000, 
sim time next is 627600.0000, 
raw observation next is [-4.5, 66.0, 0.0, 0.0, 19.0, 18.95382555262589, -1.187641660513078, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.66, 0.0, 0.0, 0.08333333333333333, 0.07948546271882417, 0.10411944649564069, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6312653], dtype=float32), -1.1557132]. 
=============================================
[2019-04-04 05:44:23,952] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.9540589e-04 9.6944201e-01 8.0731269e-15 7.2947182e-14 1.5229061e-10
 2.9662518e-02 9.1218644e-11], sum to 1.0000
[2019-04-04 05:44:23,955] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5582
[2019-04-04 05:44:23,969] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.3, 73.0, 0.0, 0.0, 19.0, 18.78432459069702, -1.215321461966338, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 793800.0000, 
sim time next is 794400.0000, 
raw observation next is [-7.3, 72.33333333333333, 0.0, 0.0, 19.0, 18.68797945040168, -1.217182026413796, 0.0, 1.0, 72047.01570219455], 
processed observation next is [1.0, 0.17391304347826086, 0.26038781163434904, 0.7233333333333333, 0.0, 0.0, 0.08333333333333333, 0.05733162086680673, 0.094272657862068, 0.0, 1.0, 0.34308102715330735], 
reward next is 0.6569, 
noisyNet noise sample is [array([1.348554], dtype=float32), 0.12870827]. 
=============================================
[2019-04-04 05:44:28,188] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.2828735e-05 9.7876030e-01 3.8605881e-22 1.2076828e-20 1.2786097e-16
 2.1196900e-02 2.1963253e-15], sum to 1.0000
[2019-04-04 05:44:28,188] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8822
[2019-04-04 05:44:28,201] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 19.0, 19.61171468518004, -0.9609778202422374, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 946200.0000, 
sim time next is 946800.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 19.0, 19.56710532514784, -0.971079344036251, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.08333333333333333, 0.13059211042898658, 0.1763068853212497, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.326987], dtype=float32), 1.2106682]. 
=============================================
[2019-04-04 05:44:28,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.8694818e-04 8.3559871e-01 1.4173350e-15 4.2822411e-14 1.5943221e-11
 1.6381437e-01 5.6852859e-11], sum to 1.0000
[2019-04-04 05:44:28,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6727
[2019-04-04 05:44:28,383] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.466666666666667, 75.66666666666667, 0.0, 0.0, 19.0, 18.77271213695919, -1.248583678769136, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 708000.0000, 
sim time next is 708600.0000, 
raw observation next is [-2.383333333333333, 75.83333333333333, 0.0, 0.0, 19.0, 18.73277898270637, -1.256959253149187, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.3965835641735919, 0.7583333333333333, 0.0, 0.0, 0.08333333333333333, 0.061064915225530925, 0.0810135822836043, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04508622], dtype=float32), -0.20406903]. 
=============================================
[2019-04-04 05:44:28,738] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9817689e-04 9.3124723e-01 8.0058034e-16 1.1125568e-14 2.1712736e-11
 6.8554662e-02 2.4859515e-11], sum to 1.0000
[2019-04-04 05:44:28,739] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2202
[2019-04-04 05:44:28,753] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 18.7966539146876, -1.241553854837629, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 715800.0000, 
sim time next is 716400.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 18.74777163192014, -1.256575606463467, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.062314302660011776, 0.08114146451217767, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05388737], dtype=float32), -1.1487389]. 
=============================================
[2019-04-04 05:44:33,470] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0772320e-05 9.5331275e-01 5.3080938e-22 7.6247047e-20 8.4075977e-16
 4.6676479e-02 3.3886561e-15], sum to 1.0000
[2019-04-04 05:44:33,471] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1774
[2019-04-04 05:44:33,479] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.166666666666667, 93.66666666666666, 0.0, 0.0, 19.0, 18.94432434843803, -1.06480361754349, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 951600.0000, 
sim time next is 952200.0000, 
raw observation next is [5.25, 92.5, 0.0, 0.0, 19.0, 18.99302341140717, -1.069759549166591, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.60803324099723, 0.925, 0.0, 0.0, 0.08333333333333333, 0.0827519509505974, 0.14341348361113634, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.985844], dtype=float32), 1.6443759]. 
=============================================
[2019-04-04 05:44:34,940] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9621460e-04 9.6295840e-01 2.3261452e-16 1.3020057e-15 2.8628562e-12
 3.6745410e-02 1.6812099e-11], sum to 1.0000
[2019-04-04 05:44:34,941] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1081
[2019-04-04 05:44:34,960] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.1, 79.66666666666667, 0.0, 0.0, 19.0, 20.07713152746959, -0.7071874057393087, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1212600.0000, 
sim time next is 1213200.0000, 
raw observation next is [16.1, 80.0, 0.0, 0.0, 19.0, 20.0687635632396, -0.7107374337706217, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.9085872576177286, 0.8, 0.0, 0.0, 0.08333333333333333, 0.17239696360329995, 0.2630875220764594, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6100303], dtype=float32), 2.7684436]. 
=============================================
[2019-04-04 05:44:36,908] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.5549691e-04 9.2658025e-01 3.4339691e-14 2.5269034e-13 9.1632577e-11
 7.2664231e-02 3.0128433e-10], sum to 1.0000
[2019-04-04 05:44:36,911] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7027
[2019-04-04 05:44:36,926] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.55, 76.0, 29.0, 0.0, 20.0, 19.59033685233416, -1.02481012872557, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 894600.0000, 
sim time next is 895200.0000, 
raw observation next is [0.7333333333333334, 77.33333333333333, 32.16666666666666, 0.0, 19.0, 19.80236338713533, -1.003016392926707, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4829178208679595, 0.7733333333333333, 0.10722222222222219, 0.0, 0.08333333333333333, 0.15019694892794413, 0.1656612023577643, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.26170388], dtype=float32), 1.5293728]. 
=============================================
[2019-04-04 05:44:45,656] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7925191e-03 7.6394176e-01 3.1836825e-17 2.5628406e-16 9.1087051e-12
 2.3326565e-01 5.3973466e-12], sum to 1.0000
[2019-04-04 05:44:45,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7410
[2019-04-04 05:44:45,672] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.8, 60.0, 0.0, 0.0, 23.0, 21.59603278389997, -0.3734729962547762, 0.0, 1.0, 129541.655468093], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1108800.0000, 
sim time next is 1109400.0000, 
raw observation next is [13.71666666666667, 60.33333333333333, 0.0, 0.0, 22.0, 21.53191086757632, -0.3739836603607265, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.8425669436749772, 0.6033333333333333, 0.0, 0.0, 0.3333333333333333, 0.29432590563135985, 0.37533877987975783, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5391972], dtype=float32), 0.69235617]. 
=============================================
[2019-04-04 05:44:57,333] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.6188482e-03 2.6735798e-01 1.9809217e-14 3.4806283e-13 1.8094559e-10
 7.2802311e-01 7.1993622e-10], sum to 1.0000
[2019-04-04 05:44:57,334] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9414
[2019-04-04 05:44:57,343] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 36.0, 0.0, 19.0, 20.1932396658359, -0.7578513102403527, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1329000.0000, 
sim time next is 1329600.0000, 
raw observation next is [0.5, 92.0, 40.5, 0.0, 20.0, 20.21482565820552, -0.7392644858555709, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.135, 0.0, 0.16666666666666666, 0.18456880485045998, 0.2535785047148097, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6821502], dtype=float32), -0.82641673]. 
=============================================
[2019-04-04 05:45:04,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.4814136e-05 8.7204236e-01 2.2034513e-20 1.1871560e-18 5.6011972e-15
 1.2789284e-01 9.5425072e-15], sum to 1.0000
[2019-04-04 05:45:04,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6981
[2019-04-04 05:45:04,192] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 23.0, 20.60223283014625, -0.4802938375895833, 0.0, 1.0, 109683.803723136], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1401600.0000, 
sim time next is 1402200.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 22.0, 21.03230232920531, -0.4495464927247335, 0.0, 1.0, 67312.23087992298], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.3333333333333333, 0.2526918607671093, 0.35015116909175553, 0.0, 1.0, 0.320534432761538], 
reward next is 0.6795, 
noisyNet noise sample is [array([-1.0599493], dtype=float32), -0.69272095]. 
=============================================
[2019-04-04 05:45:04,548] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1482255e-04 6.4835322e-01 2.1150078e-20 1.5626786e-18 1.0616436e-14
 3.5113195e-01 6.7835277e-14], sum to 1.0000
[2019-04-04 05:45:04,556] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7525
[2019-04-04 05:45:04,622] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6000000000000001, 92.0, 92.0, 0.0, 24.0, 24.10751754259849, 0.05373070645108077, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1429800.0000, 
sim time next is 1430400.0000, 
raw observation next is [0.7000000000000001, 92.0, 91.0, 0.0, 25.0, 24.12877838379352, 0.08429248304004026, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4819944598337951, 0.92, 0.30333333333333334, 0.0, 0.5833333333333334, 0.5107315319827933, 0.5280974943466801, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20947038], dtype=float32), -1.5513173]. 
=============================================
[2019-04-04 05:45:15,211] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6078282e-04 4.1277444e-01 8.1565927e-23 1.7737188e-21 5.5170888e-16
 5.8706480e-01 6.0441465e-16], sum to 1.0000
[2019-04-04 05:45:15,212] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3603
[2019-04-04 05:45:15,259] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 88.00000000000001, 0.0, 0.0, 26.0, 25.10567210397735, 0.462638552056274, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1707000.0000, 
sim time next is 1707600.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 25.0, 25.52810587012954, 0.4810237056233502, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 0.5833333333333334, 0.6273421558441283, 0.6603412352077834, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.59086], dtype=float32), -1.0963882]. 
=============================================
[2019-04-04 05:45:16,887] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.54055908e-06 8.91500890e-01 1.72035970e-24 1.44101726e-22
 1.00860771e-17 1.08494505e-01 1.27240983e-17], sum to 1.0000
[2019-04-04 05:45:16,888] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9402
[2019-04-04 05:45:16,895] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.800000000000001, 94.0, 0.0, 0.0, 19.0, 24.02002517231562, 0.1840216064736334, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1646400.0000, 
sim time next is 1647000.0000, 
raw observation next is [6.9, 94.5, 0.0, 0.0, 20.0, 23.9752125358099, 0.1841680801372054, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6537396121883658, 0.945, 0.0, 0.0, 0.16666666666666666, 0.49793437798415824, 0.5613893600457351, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13457328], dtype=float32), -0.20708033]. 
=============================================
[2019-04-04 05:45:16,936] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[91.67749 ]
 [93.142426]
 [94.43857 ]
 [96.030594]
 [97.69458 ]], R is [[90.72899628]
 [90.82170868]
 [90.9134903 ]
 [91.00435638]
 [91.09431458]].
[2019-04-04 05:45:24,516] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.3421669e-07 5.3520180e-02 3.2138708e-27 8.8036926e-25 2.1737350e-19
 9.4647920e-01 1.0471081e-18], sum to 1.0000
[2019-04-04 05:45:24,518] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8244
[2019-04-04 05:45:24,560] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 24.0, 24.21646021090006, 0.225764292687532, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1717200.0000, 
sim time next is 1717800.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 25.0, 24.11839596727969, 0.2244533917000273, 0.0, 1.0, 196707.726922304], 
processed observation next is [1.0, 0.9130434782608695, 0.4764542936288089, 0.92, 0.0, 0.0, 0.5833333333333334, 0.5098663306066408, 0.5748177972333425, 0.0, 1.0, 0.9367034615347809], 
reward next is 0.0633, 
noisyNet noise sample is [array([-1.0296888], dtype=float32), 0.02598705]. 
=============================================
[2019-04-04 05:45:34,403] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8509488e-06 1.2433815e-01 4.3114944e-24 4.3349671e-22 7.3884057e-17
 8.7566000e-01 1.3838949e-16], sum to 1.0000
[2019-04-04 05:45:34,403] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7535
[2019-04-04 05:45:34,461] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.3, 73.0, 184.0, 81.0, 26.0, 25.23616754441992, 0.2692067563752372, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1855800.0000, 
sim time next is 1856400.0000, 
raw observation next is [-5.199999999999999, 72.33333333333333, 173.3333333333333, 67.5, 26.0, 25.22520225536974, 0.2571776913766498, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.31855955678670367, 0.7233333333333333, 0.5777777777777776, 0.07458563535911603, 0.6666666666666666, 0.6021001879474784, 0.5857258971255499, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02160269], dtype=float32), 1.2918088]. 
=============================================
[2019-04-04 05:45:39,982] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3643950e-04 2.8917542e-02 6.5000117e-20 3.4151299e-19 3.6861569e-14
 9.7094607e-01 2.4810500e-14], sum to 1.0000
[2019-04-04 05:45:39,983] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7904
[2019-04-04 05:45:40,025] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7994262e-06 2.2245617e-01 8.5285691e-29 6.0693257e-26 1.6228286e-19
 7.7754205e-01 3.6268556e-19], sum to 1.0000
[2019-04-04 05:45:40,026] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0117
[2019-04-04 05:45:40,055] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.55, 78.5, 208.0, 60.0, 26.0, 25.64714498640036, 0.3382553891600242, 1.0, 1.0, 32257.70813451719], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2111400.0000, 
sim time next is 2112000.0000, 
raw observation next is [-7.466666666666667, 77.33333333333334, 222.1666666666667, 66.83333333333333, 26.0, 25.63813953911679, 0.349078547685218, 1.0, 1.0, 32172.63958989218], 
processed observation next is [1.0, 0.43478260869565216, 0.25577100646352724, 0.7733333333333334, 0.7405555555555557, 0.07384898710865562, 0.6666666666666666, 0.6365116282597324, 0.6163595158950727, 1.0, 1.0, 0.15320304566615323], 
reward next is 0.8468, 
noisyNet noise sample is [array([1.4658891], dtype=float32), 0.5938461]. 
=============================================
[2019-04-04 05:45:40,061] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.40707831946285, 0.1591086257570002, 0.0, 1.0, 47571.28596688477], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1984200.0000, 
sim time next is 1984800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 25.0, 24.36405460703592, 0.1513653422167166, 0.0, 1.0, 44537.38856536237], 
processed observation next is [1.0, 1.0, 0.30747922437673136, 0.83, 0.0, 0.0, 0.5833333333333334, 0.5303378839196601, 0.5504551140722388, 0.0, 1.0, 0.21208280269220178], 
reward next is 0.7879, 
noisyNet noise sample is [array([-0.28450927], dtype=float32), -0.5711817]. 
=============================================
[2019-04-04 05:45:40,063] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[72.44647]
 [72.02867]
 [71.46419]
 [70.88718]
 [70.62822]], R is [[73.22855377]
 [73.342659  ]
 [73.45642853]
 [73.57402802]
 [73.70797729]].
[2019-04-04 05:45:40,085] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1512617e-06 5.3312844e-01 7.0480605e-22 3.5385875e-20 4.6586219e-16
 4.6686739e-01 1.0586480e-15], sum to 1.0000
[2019-04-04 05:45:40,089] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5525
[2019-04-04 05:45:40,113] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.733333333333334, 80.66666666666667, 0.0, 0.0, 26.0, 23.37608459409457, -0.1547006259889573, 0.0, 1.0, 45508.46741938918], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1917600.0000, 
sim time next is 1918200.0000, 
raw observation next is [-8.816666666666666, 81.33333333333334, 0.0, 0.0, 26.0, 23.35834761301326, -0.1530477287300403, 0.0, 1.0, 45494.26429417262], 
processed observation next is [1.0, 0.17391304347826086, 0.21837488457987075, 0.8133333333333335, 0.0, 0.0, 0.6666666666666666, 0.4465289677511051, 0.44898409042331994, 0.0, 1.0, 0.2166393537817744], 
reward next is 0.7834, 
noisyNet noise sample is [array([2.0693078], dtype=float32), 0.04784219]. 
=============================================
[2019-04-04 05:45:41,068] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.02820287e-07 7.05609620e-02 1.02448526e-26 7.41343523e-25
 1.23366482e-18 9.29438889e-01 8.54971503e-19], sum to 1.0000
[2019-04-04 05:45:41,069] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1557
[2019-04-04 05:45:41,085] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.566666666666666, 76.33333333333334, 0.0, 0.0, 22.0, 24.00384529168976, -0.01636915418071997, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1894800.0000, 
sim time next is 1895400.0000, 
raw observation next is [-6.75, 77.0, 0.0, 0.0, 23.0, 23.87466962150402, -0.04392653722971038, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.275623268698061, 0.77, 0.0, 0.0, 0.4166666666666667, 0.4895558017920016, 0.4853578209234299, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23949075], dtype=float32), -0.36027816]. 
=============================================
[2019-04-04 05:45:50,318] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5003927e-06 4.5405719e-02 4.4967448e-25 7.3235442e-23 1.1996921e-17
 9.5459080e-01 1.4860861e-17], sum to 1.0000
[2019-04-04 05:45:50,319] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8870
[2019-04-04 05:45:50,334] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.9, 78.33333333333334, 0.0, 0.0, 26.0, 24.14949585245221, 0.1057331911381979, 0.0, 1.0, 42710.54504508328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2166000.0000, 
sim time next is 2166600.0000, 
raw observation next is [-6.800000000000001, 78.16666666666666, 0.0, 0.0, 26.0, 24.11997072590663, 0.1053822497899632, 0.0, 1.0, 42728.87154931804], 
processed observation next is [1.0, 0.043478260869565216, 0.2742382271468144, 0.7816666666666666, 0.0, 0.0, 0.6666666666666666, 0.5099975604922191, 0.5351274165966544, 0.0, 1.0, 0.2034708169015145], 
reward next is 0.7965, 
noisyNet noise sample is [array([0.50956446], dtype=float32), 0.96185666]. 
=============================================
[2019-04-04 05:46:05,965] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.0154008e-06 2.5272327e-02 2.0216844e-24 4.2744609e-22 2.5955965e-17
 9.7472262e-01 4.0135803e-17], sum to 1.0000
[2019-04-04 05:46:05,967] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8759
[2019-04-04 05:46:05,982] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.75589114123627, -0.004537735097894046, 0.0, 1.0, 43464.49669088394], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2264400.0000, 
sim time next is 2265000.0000, 
raw observation next is [-8.9, 91.00000000000001, 0.0, 0.0, 26.0, 23.7078473596786, -0.01709791564542696, 0.0, 1.0, 43409.2951692919], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.9100000000000001, 0.0, 0.0, 0.6666666666666666, 0.47565394663988325, 0.49430069478485766, 0.0, 1.0, 0.20671092937758048], 
reward next is 0.7933, 
noisyNet noise sample is [array([-0.24038559], dtype=float32), -1.9064293]. 
=============================================
[2019-04-04 05:46:06,029] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[88.92791 ]
 [88.74433 ]
 [88.623634]
 [88.50089 ]
 [88.36788 ]], R is [[88.98212433]
 [88.8853302 ]
 [88.78927612]
 [88.69403076]
 [88.5995636 ]].
[2019-04-04 05:46:14,191] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.3221021e-07 2.4014395e-02 5.9901577e-24 1.5903628e-22 7.1649584e-17
 9.7598517e-01 2.4954118e-17], sum to 1.0000
[2019-04-04 05:46:14,194] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9947
[2019-04-04 05:46:14,249] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.3, 68.33333333333333, 93.0, 240.0, 26.0, 25.00309015300629, 0.2665837866436354, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2365800.0000, 
sim time next is 2366400.0000, 
raw observation next is [-3.2, 67.66666666666667, 107.0, 300.0, 26.0, 25.00037159218921, 0.2661181748595027, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.37396121883656513, 0.6766666666666667, 0.3566666666666667, 0.3314917127071823, 0.6666666666666666, 0.583364299349101, 0.5887060582865009, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-3.2427301], dtype=float32), -0.65517396]. 
=============================================
[2019-04-04 05:46:16,983] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1505618e-07 2.6966045e-02 1.7421604e-22 1.5276777e-20 9.9800214e-17
 9.7303367e-01 5.3109575e-16], sum to 1.0000
[2019-04-04 05:46:16,999] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7826
[2019-04-04 05:46:17,068] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.550000000000001, 82.5, 101.0, 39.0, 26.0, 25.66146859734492, 0.3137508378249624, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2280600.0000, 
sim time next is 2281200.0000, 
raw observation next is [-7.266666666666667, 81.0, 113.8333333333333, 40.83333333333333, 26.0, 25.66060457066418, 0.3206138784039594, 1.0, 1.0, 18735.98438697837], 
processed observation next is [1.0, 0.391304347826087, 0.26131117266851345, 0.81, 0.3794444444444443, 0.04511970534069981, 0.6666666666666666, 0.638383714222015, 0.6068712928013198, 1.0, 1.0, 0.08921897327132557], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.10758834], dtype=float32), 0.5398879]. 
=============================================
[2019-04-04 05:46:22,088] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8396811e-06 2.4664283e-02 2.2946725e-23 2.4585985e-21 2.7712202e-16
 9.7533184e-01 2.8010106e-16], sum to 1.0000
[2019-04-04 05:46:22,090] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0521
[2019-04-04 05:46:22,131] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 25.0, 24.3821454514087, 0.149023008875916, 0.0, 1.0, 34854.08637591456], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2348400.0000, 
sim time next is 2349000.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 26.0, 24.34471225917946, 0.14078616182747, 0.0, 1.0, 43116.12345063667], 
processed observation next is [0.0, 0.17391304347826086, 0.37673130193905824, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5287260215982883, 0.5469287206091566, 0.0, 1.0, 0.20531487357446032], 
reward next is 0.7947, 
noisyNet noise sample is [array([-0.82050824], dtype=float32), 1.0879929]. 
=============================================
[2019-04-04 05:46:22,147] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[88.42899]
 [88.42842]
 [88.29139]
 [88.13617]
 [88.05537]], R is [[88.45773315]
 [88.40718842]
 [88.33104706]
 [88.25614166]
 [88.18254089]].
[2019-04-04 05:46:29,548] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3329178e-06 3.6779027e-03 8.0424519e-24 1.2307569e-22 1.9926821e-17
 9.9632072e-01 1.0211069e-17], sum to 1.0000
[2019-04-04 05:46:29,549] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5091
[2019-04-04 05:46:29,622] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8666666666666667, 29.0, 89.5, 842.8333333333334, 26.0, 24.897519012259, 0.2618176545105352, 0.0, 1.0, 26681.64257736054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2464800.0000, 
sim time next is 2465400.0000, 
raw observation next is [1.233333333333333, 28.5, 89.0, 840.6666666666666, 26.0, 24.9139103858409, 0.2703917174301272, 0.0, 1.0, 18722.17305428591], 
processed observation next is [0.0, 0.5217391304347826, 0.49676823638042483, 0.285, 0.2966666666666667, 0.9289134438305708, 0.6666666666666666, 0.576159198820075, 0.5901305724767091, 0.0, 1.0, 0.0891532050204091], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.8974181], dtype=float32), -1.1703176]. 
=============================================
[2019-04-04 05:46:39,992] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3516343e-06 5.0281081e-02 1.4305922e-23 2.7003282e-22 3.4623428e-17
 9.4971752e-01 7.4425655e-17], sum to 1.0000
[2019-04-04 05:46:39,993] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1634
[2019-04-04 05:46:40,011] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.24911154776023, 0.09094492812676991, 0.0, 1.0, 41296.51627764424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2785200.0000, 
sim time next is 2785800.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.23291123971869, 0.09245043639076196, 0.0, 1.0, 41315.89765127803], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5194092699765575, 0.530816812130254, 0.0, 1.0, 0.19674236976799062], 
reward next is 0.8033, 
noisyNet noise sample is [array([-1.7714087], dtype=float32), -0.452253]. 
=============================================
[2019-04-04 05:46:43,326] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.5809476e-08 1.5211087e-03 7.9629182e-28 2.0176484e-26 2.0491624e-20
 9.9847883e-01 3.1849767e-19], sum to 1.0000
[2019-04-04 05:46:43,326] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6321
[2019-04-04 05:46:43,372] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 94.16666666666666, 49.33333333333334, 49.66666666666667, 26.0, 24.78552363866348, 0.3950607265224473, 1.0, 1.0, 195495.3377686448], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2911800.0000, 
sim time next is 2912400.0000, 
raw observation next is [2.0, 93.0, 38.5, 47.5, 26.0, 25.27465259734521, 0.4458792257440626, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.93, 0.12833333333333333, 0.052486187845303865, 0.6666666666666666, 0.6062210497787675, 0.6486264085813542, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2086065], dtype=float32), -1.4897935]. 
=============================================
[2019-04-04 05:47:01,990] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7835517e-08 1.2175181e-03 9.1426844e-30 1.4562029e-26 6.2975955e-20
 9.9878246e-01 4.5006344e-21], sum to 1.0000
[2019-04-04 05:47:02,065] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1561
[2019-04-04 05:47:02,104] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 26.0, 25.29334305349574, 0.3076525824218636, 0.0, 1.0, 64437.18920423542], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3117000.0000, 
sim time next is 3117600.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 26.0, 25.25429345454397, 0.3142601925397595, 0.0, 1.0, 48063.10237855317], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6045244545453308, 0.6047533975132532, 0.0, 1.0, 0.22887191608834845], 
reward next is 0.7711, 
noisyNet noise sample is [array([2.780069], dtype=float32), 0.14041652]. 
=============================================
[2019-04-04 05:47:02,156] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.02091214e-07 1.60877630e-02 3.45021783e-26 4.04029312e-24
 1.48466791e-18 9.83912170e-01 8.45399524e-19], sum to 1.0000
[2019-04-04 05:47:02,156] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6587
[2019-04-04 05:47:02,232] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 93.0, 105.0, 156.0, 26.0, 25.0688417133969, 0.3247960033915085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2881800.0000, 
sim time next is 2882400.0000, 
raw observation next is [1.333333333333333, 93.0, 95.66666666666666, 130.0, 26.0, 25.28106953124364, 0.3371818061819172, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4995383194829178, 0.93, 0.31888888888888883, 0.143646408839779, 0.6666666666666666, 0.6067557942703035, 0.6123939353939724, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5428771], dtype=float32), -0.9559945]. 
=============================================
[2019-04-04 05:47:15,307] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5452096e-08 4.7007296e-03 8.0332975e-29 2.0952703e-26 4.2514414e-21
 9.9529922e-01 4.9807091e-20], sum to 1.0000
[2019-04-04 05:47:15,307] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6761
[2019-04-04 05:47:15,319] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.87760886774894, 0.2550838779112772, 0.0, 1.0, 37976.02741179966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3023400.0000, 
sim time next is 3024000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.84209122726952, 0.2477195104229691, 0.0, 1.0, 37933.60540658345], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5701742689391267, 0.5825731701409896, 0.0, 1.0, 0.18063621622182593], 
reward next is 0.8194, 
noisyNet noise sample is [array([1.2270046], dtype=float32), 0.9376056]. 
=============================================
[2019-04-04 05:47:15,322] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[108.64557 ]
 [108.61801 ]
 [108.60518 ]
 [108.57619 ]
 [108.539116]], R is [[107.12797546]
 [106.87585449]
 [106.62605286]
 [106.37846375]
 [106.13299561]].
[2019-04-04 05:47:19,068] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.3332891e-08 1.1399004e-02 6.0579137e-26 4.2717061e-23 3.1918100e-18
 9.8860091e-01 9.4444844e-18], sum to 1.0000
[2019-04-04 05:47:19,068] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9324
[2019-04-04 05:47:19,094] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.45, 77.0, 0.0, 0.0, 26.0, 24.71338806156635, 0.2902910721315932, 0.0, 1.0, 43933.00043002111], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3295800.0000, 
sim time next is 3296400.0000, 
raw observation next is [-8.600000000000001, 77.0, 0.0, 0.0, 26.0, 24.72652934277325, 0.285347945401714, 0.0, 1.0, 43904.05069228941], 
processed observation next is [1.0, 0.13043478260869565, 0.22437673130193903, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5605441118977709, 0.5951159818005713, 0.0, 1.0, 0.20906690805852102], 
reward next is 0.7909, 
noisyNet noise sample is [array([0.23137653], dtype=float32), -0.45747882]. 
=============================================
[2019-04-04 05:47:25,991] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5391604e-08 5.2766064e-03 2.2257304e-27 2.8398680e-25 1.8753968e-20
 9.9472344e-01 1.3014948e-20], sum to 1.0000
[2019-04-04 05:47:25,991] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1959
[2019-04-04 05:47:26,082] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 100.0, 42.0, 237.0, 26.0, 25.37857982286307, 0.3731528876852558, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3139200.0000, 
sim time next is 3139800.0000, 
raw observation next is [6.166666666666666, 100.0, 55.66666666666668, 288.6666666666667, 26.0, 25.61256815313607, 0.3840330134504722, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6334256694367498, 1.0, 0.18555555555555558, 0.31896869244935544, 0.6666666666666666, 0.6343806794280059, 0.6280110044834907, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46986276], dtype=float32), 1.5064512]. 
=============================================
[2019-04-04 05:47:28,866] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.3878632e-10 6.1777909e-04 9.5274129e-30 8.5967055e-29 8.8905942e-22
 9.9938214e-01 3.1467516e-21], sum to 1.0000
[2019-04-04 05:47:28,867] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1244
[2019-04-04 05:47:28,887] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 100.0, 24.66666666666666, 243.6666666666666, 26.0, 26.88254264932761, 0.8866473809860579, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3172200.0000, 
sim time next is 3172800.0000, 
raw observation next is [6.0, 100.0, 16.33333333333333, 179.8333333333333, 26.0, 27.22105257203912, 0.8669291608852226, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.05444444444444443, 0.19871086556169423, 0.6666666666666666, 0.7684210476699267, 0.7889763869617409, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23617281], dtype=float32), -0.33836466]. 
=============================================
[2019-04-04 05:47:34,731] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6042798e-07 3.3845992e-03 4.5333558e-25 1.1809028e-23 3.2651019e-19
 9.9661511e-01 4.2227153e-18], sum to 1.0000
[2019-04-04 05:47:34,735] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3687
[2019-04-04 05:47:34,761] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 50.0, 35.5, 317.0, 26.0, 26.26987430320392, 0.6275006122053509, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3344400.0000, 
sim time next is 3345000.0000, 
raw observation next is [-2.166666666666667, 50.83333333333334, 27.33333333333333, 255.6666666666666, 26.0, 26.42944989108157, 0.4786507248175718, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4025854108956602, 0.5083333333333334, 0.0911111111111111, 0.2825046040515653, 0.6666666666666666, 0.702454157590131, 0.6595502416058573, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9958131], dtype=float32), 0.31985974]. 
=============================================
[2019-04-04 05:47:34,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[93.111206]
 [93.050476]
 [92.680954]
 [92.57616 ]
 [92.54881 ]], R is [[93.236763  ]
 [93.30439758]
 [93.37135315]
 [93.43763733]
 [93.50326538]].
[2019-04-04 05:47:42,901] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.2054777e-09 1.4631357e-03 1.0647469e-30 7.8732469e-28 2.8869388e-22
 9.9853683e-01 1.1344000e-21], sum to 1.0000
[2019-04-04 05:47:42,901] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3454
[2019-04-04 05:47:42,916] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333333, 74.0, 0.0, 0.0, 26.0, 25.93477093849637, 0.6160527254475244, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3532800.0000, 
sim time next is 3533400.0000, 
raw observation next is [-0.5, 75.0, 0.0, 0.0, 26.0, 25.91954644941708, 0.6034908483027345, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.44875346260387816, 0.75, 0.0, 0.0, 0.6666666666666666, 0.65996220411809, 0.7011636161009115, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2084343], dtype=float32), 0.10394491]. 
=============================================
[2019-04-04 05:47:45,862] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-04 05:47:45,864] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:47:45,864] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:47:45,865] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:47:45,865] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:47:45,865] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:47:45,868] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run6
[2019-04-04 05:47:45,868] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:47:45,887] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run6
[2019-04-04 05:47:45,913] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run6
[2019-04-04 05:48:31,281] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2609195], dtype=float32), 0.22129546]
[2019-04-04 05:48:31,281] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-12.65, 70.0, 0.0, 0.0, 26.0, 21.52142910198847, -0.5945210388364973, 0.0, 1.0, 46223.7263178092]
[2019-04-04 05:48:31,281] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:48:31,282] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.9358224e-06 2.2401510e-02 6.6842712e-21 3.0778351e-19 4.9752327e-15
 9.7759455e-01 1.3091901e-14], sampled 0.12246174130700249
[2019-04-04 05:50:38,978] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.2609195], dtype=float32), 0.22129546]
[2019-04-04 05:50:38,978] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.5127518185, 72.35463428, 0.0, 0.0, 26.0, 25.30732780274341, 0.418805219737115, 0.0, 1.0, 42511.45518893796]
[2019-04-04 05:50:38,978] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:50:38,979] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.5033500e-08 4.3503055e-03 2.5508352e-30 9.5233923e-28 1.2938261e-21
 9.9564964e-01 4.8933840e-21], sampled 0.2590827345386816
[2019-04-04 05:50:55,500] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7354.1599 239786428.7615 1605.0211
[2019-04-04 05:51:10,649] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.2609195], dtype=float32), 0.22129546]
[2019-04-04 05:51:10,649] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.0, 49.0, 255.5, 80.5, 26.0, 26.34411412787115, 0.6044364464371886, 1.0, 1.0, 0.0]
[2019-04-04 05:51:10,649] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 05:51:10,650] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.3765329e-07 2.2508739e-03 3.8470593e-24 2.7578385e-22 3.3418596e-17
 9.9774879e-01 5.2446632e-17], sampled 0.3872619104025873
[2019-04-04 05:51:24,019] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2666 263494012.4510 1551.7683
[2019-04-04 05:51:25,797] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.8912 275752850.6934 1233.0985
[2019-04-04 05:51:26,819] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 500000, evaluation results [500000.0, 7241.266607376075, 263494012.45102707, 1551.7682745179382, 7354.159863040235, 239786428.76154968, 1605.021052738359, 7182.891187174117, 275752850.6934435, 1233.0985381296578]
[2019-04-04 05:51:27,637] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.2795782e-07 1.3594608e-02 4.4329112e-25 3.3692473e-23 9.7559981e-18
 9.8640454e-01 7.5062240e-18], sum to 1.0000
[2019-04-04 05:51:27,638] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7270
[2019-04-04 05:51:27,652] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 24.49393642520126, 0.1679474332474085, 0.0, 1.0, 59984.30244789016], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3394800.0000, 
sim time next is 3395400.0000, 
raw observation next is [-2.833333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 24.44013217502221, 0.1665177050849973, 0.0, 1.0, 54301.67185725718], 
processed observation next is [1.0, 0.30434782608695654, 0.3841181902123731, 0.6416666666666667, 0.0, 0.0, 0.6666666666666666, 0.5366776812518509, 0.5555059016949991, 0.0, 1.0, 0.2585793897964627], 
reward next is 0.7414, 
noisyNet noise sample is [array([0.5028966], dtype=float32), 1.4530538]. 
=============================================
[2019-04-04 05:51:30,560] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.5775461e-08 5.8784260e-04 6.5486632e-29 1.3010184e-25 1.6689089e-19
 9.9941206e-01 1.1876938e-19], sum to 1.0000
[2019-04-04 05:51:30,566] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0521
[2019-04-04 05:51:30,588] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 67.0, 12.0, 121.0, 26.0, 25.97064668544399, 0.4865707021715385, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3432600.0000, 
sim time next is 3433200.0000, 
raw observation next is [2.0, 67.0, 10.0, 100.8333333333333, 26.0, 25.74166088629305, 0.5313208561257855, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.03333333333333333, 0.11141804788213625, 0.6666666666666666, 0.6451384071910876, 0.6771069520419285, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-3.1436932], dtype=float32), 1.3083872]. 
=============================================
[2019-04-04 05:51:36,101] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.3383910e-08 1.8623868e-02 3.4969780e-30 4.8916074e-28 1.6115299e-22
 9.8137617e-01 6.0382968e-22], sum to 1.0000
[2019-04-04 05:51:36,102] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4613
[2019-04-04 05:51:36,112] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666666, 76.0, 0.0, 0.0, 26.0, 25.87527882636609, 0.5954628304821813, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3534000.0000, 
sim time next is 3534600.0000, 
raw observation next is [-0.8333333333333334, 77.0, 0.0, 0.0, 26.0, 25.85847349116901, 0.5810492852924842, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.43951985226223456, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6548727909307509, 0.6936830950974947, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.97730976], dtype=float32), 1.3065423]. 
=============================================
[2019-04-04 05:51:36,911] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1972139e-07 1.7136498e-03 9.4738799e-26 2.3438354e-23 1.0339186e-17
 9.9828619e-01 2.8637268e-18], sum to 1.0000
[2019-04-04 05:51:36,912] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5823
[2019-04-04 05:51:36,922] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.333333333333333, 67.0, 115.6666666666667, 823.1666666666666, 26.0, 26.46133744069497, 0.5941054837398699, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3757200.0000, 
sim time next is 3757800.0000, 
raw observation next is [-2.166666666666667, 66.0, 116.3333333333333, 824.3333333333334, 26.0, 26.49260913241955, 0.5940571187469252, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4025854108956602, 0.66, 0.38777777777777767, 0.910865561694291, 0.6666666666666666, 0.707717427701629, 0.6980190395823085, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.81651336], dtype=float32), -0.35621172]. 
=============================================
[2019-04-04 05:51:44,476] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.0949621e-09 1.3586667e-02 1.0057809e-29 3.6631168e-27 5.1266740e-21
 9.8641336e-01 1.6066705e-20], sum to 1.0000
[2019-04-04 05:51:44,479] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4665
[2019-04-04 05:51:44,488] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.4145455403899, 0.4130761978036482, 0.0, 1.0, 56814.83319600484], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3708000.0000, 
sim time next is 3708600.0000, 
raw observation next is [-0.5, 70.83333333333333, 0.0, 0.0, 26.0, 25.42443906930221, 0.4132355833618191, 0.0, 1.0, 39838.74903476681], 
processed observation next is [0.0, 0.9565217391304348, 0.44875346260387816, 0.7083333333333333, 0.0, 0.0, 0.6666666666666666, 0.6187032557751841, 0.6377451944539397, 0.0, 1.0, 0.18970832873698482], 
reward next is 0.8103, 
noisyNet noise sample is [array([-1.4321716], dtype=float32), 1.6329465]. 
=============================================
[2019-04-04 05:51:44,588] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3684898e-07 8.5279429e-03 1.6466354e-27 4.4752584e-24 1.5241854e-19
 9.9147171e-01 1.2959004e-19], sum to 1.0000
[2019-04-04 05:51:44,589] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1112
[2019-04-04 05:51:44,596] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.0, 34.5, 116.0, 816.0, 26.0, 25.68695623030456, 0.495535420090679, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3670200.0000, 
sim time next is 3670800.0000, 
raw observation next is [6.666666666666666, 38.0, 116.1666666666667, 818.1666666666666, 26.0, 25.64470575144251, 0.4868682074981892, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6472760849492153, 0.38, 0.38722222222222236, 0.9040515653775322, 0.6666666666666666, 0.637058812620209, 0.6622894024993964, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.82723963], dtype=float32), 0.5031242]. 
=============================================
[2019-04-04 05:51:45,730] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8100649e-06 1.4382018e-02 2.7975772e-23 7.7730461e-22 1.3023943e-16
 9.8561513e-01 2.7559908e-16], sum to 1.0000
[2019-04-04 05:51:45,732] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2860
[2019-04-04 05:51:45,785] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 77.0, 105.5, 722.0, 26.0, 26.23809703629002, 0.5200454591480409, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3751200.0000, 
sim time next is 3751800.0000, 
raw observation next is [-3.0, 76.0, 107.3333333333333, 737.6666666666667, 26.0, 26.27845000362248, 0.5300780650497222, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.76, 0.3577777777777777, 0.8151012891344384, 0.6666666666666666, 0.6898708336352067, 0.6766926883499074, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20132817], dtype=float32), -0.14465103]. 
=============================================
[2019-04-04 05:51:46,920] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7585901e-06 1.2892603e-02 1.2426980e-22 6.7653366e-21 6.1230021e-16
 9.8710561e-01 1.2711631e-15], sum to 1.0000
[2019-04-04 05:51:46,920] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2547
[2019-04-04 05:51:46,984] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.0, 63.0, 0.0, 0.0, 26.0, 23.50910222174706, -0.02809055491341737, 0.0, 1.0, 43638.11984880888], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3996000.0000, 
sim time next is 3996600.0000, 
raw observation next is [-13.16666666666667, 64.0, 0.0, 0.0, 26.0, 23.48881666464938, -0.0313516688726893, 0.0, 1.0, 43534.28154892882], 
processed observation next is [1.0, 0.2608695652173913, 0.09787626962142189, 0.64, 0.0, 0.0, 0.6666666666666666, 0.45740138872078173, 0.4895494437091035, 0.0, 1.0, 0.20730610261394675], 
reward next is 0.7927, 
noisyNet noise sample is [array([0.2827665], dtype=float32), 0.044834357]. 
=============================================
[2019-04-04 05:51:48,179] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3511087e-09 6.4407750e-03 1.4898779e-29 1.2244432e-27 1.4037236e-20
 9.9355924e-01 7.6745656e-21], sum to 1.0000
[2019-04-04 05:51:48,181] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5317
[2019-04-04 05:51:48,190] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 73.0, 0.0, 0.0, 26.0, 25.64846364302577, 0.5375117809764561, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3792000.0000, 
sim time next is 3792600.0000, 
raw observation next is [-3.0, 74.0, 0.0, 0.0, 26.0, 25.60742899715851, 0.5234158566573931, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3795013850415513, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6339524164298759, 0.674471952219131, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4154758], dtype=float32), -1.5151134]. 
=============================================
[2019-04-04 05:51:50,013] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5832955e-08 2.0027994e-03 8.2425980e-30 2.2845895e-27 1.9536059e-20
 9.9799711e-01 3.0584581e-21], sum to 1.0000
[2019-04-04 05:51:50,021] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1176
[2019-04-04 05:51:50,037] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.333333333333334, 50.33333333333334, 0.0, 0.0, 26.0, 25.40016726069686, 0.4330247798813714, 0.0, 1.0, 43758.78389981162], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3968400.0000, 
sim time next is 3969000.0000, 
raw observation next is [-8.5, 51.0, 0.0, 0.0, 26.0, 25.32953792000773, 0.4202247025657764, 0.0, 1.0, 57031.63270574866], 
processed observation next is [1.0, 0.9565217391304348, 0.22714681440443216, 0.51, 0.0, 0.0, 0.6666666666666666, 0.6107948266673109, 0.6400749008552588, 0.0, 1.0, 0.2715792033607079], 
reward next is 0.7284, 
noisyNet noise sample is [array([-0.25021046], dtype=float32), -0.5659317]. 
=============================================
[2019-04-04 05:51:50,044] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[104.913826]
 [104.68092 ]
 [104.52855 ]
 [104.05373 ]
 [103.49754 ]], R is [[104.96796417]
 [104.70991516]
 [104.37326813]
 [104.01534271]
 [103.66519165]].
[2019-04-04 05:51:52,780] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5329077e-06 1.9437814e-02 2.8432709e-24 7.5422681e-22 8.1425681e-17
 9.8056066e-01 8.5583415e-17], sum to 1.0000
[2019-04-04 05:51:52,780] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1311
[2019-04-04 05:51:52,821] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.666666666666667, 60.00000000000001, 108.5, 759.1666666666667, 26.0, 26.51217918561598, 0.6128336708131833, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3838800.0000, 
sim time next is 3839400.0000, 
raw observation next is [-1.5, 60.0, 110.0, 775.0, 26.0, 26.54493979174135, 0.6199770863658679, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4210526315789474, 0.6, 0.36666666666666664, 0.856353591160221, 0.6666666666666666, 0.7120783159784457, 0.7066590287886226, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0813043], dtype=float32), 0.18815878]. 
=============================================
[2019-04-04 05:52:08,816] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.1361142e-08 1.6312118e-03 3.1801073e-28 2.8733741e-26 4.9867019e-20
 9.9836880e-01 4.1829490e-20], sum to 1.0000
[2019-04-04 05:52:08,816] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4754
[2019-04-04 05:52:08,847] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.47981951141787, 0.4156664330496556, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4219800.0000, 
sim time next is 4220400.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.56186818965018, 0.4130796113149337, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6301556824708484, 0.6376932037716446, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39446446], dtype=float32), 0.19564408]. 
=============================================
[2019-04-04 05:52:11,995] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.0484340e-09 6.3534413e-04 2.5535277e-28 5.5009509e-27 1.0024931e-20
 9.9936467e-01 2.2822110e-20], sum to 1.0000
[2019-04-04 05:52:11,996] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9354
[2019-04-04 05:52:12,010] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 74.0, 20.83333333333334, 45.83333333333334, 26.0, 25.69326878417208, 0.5000694545941821, 1.0, 1.0, 30661.87554571549], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4470000.0000, 
sim time next is 4470600.0000, 
raw observation next is [0.0, 73.0, 16.66666666666667, 36.66666666666667, 26.0, 25.36187328173199, 0.5177807932983912, 1.0, 1.0, 25746.912910828], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.73, 0.05555555555555557, 0.04051565377532229, 0.6666666666666666, 0.6134894401443326, 0.6725935977661304, 1.0, 1.0, 0.12260434719441904], 
reward next is 0.8774, 
noisyNet noise sample is [array([0.20370123], dtype=float32), -1.5207847]. 
=============================================
[2019-04-04 05:52:13,116] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4039848e-07 4.0730033e-02 8.1120376e-25 9.2917118e-23 6.0764946e-17
 9.5926958e-01 3.0844023e-17], sum to 1.0000
[2019-04-04 05:52:13,116] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0012
[2019-04-04 05:52:13,129] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.5, 48.0, 0.0, 0.0, 26.0, 25.25850071754806, 0.380559359929432, 0.0, 1.0, 39269.50416761979], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4156200.0000, 
sim time next is 4156800.0000, 
raw observation next is [-2.666666666666667, 48.66666666666666, 0.0, 0.0, 25.0, 25.23159354796385, 0.3699636339813457, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.38873499538319484, 0.4866666666666666, 0.0, 0.0, 0.5833333333333334, 0.6026327956636542, 0.6233212113271153, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3363236], dtype=float32), -0.722666]. 
=============================================
[2019-04-04 05:52:23,608] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2325327e-07 3.0377249e-03 3.0498039e-27 6.8587294e-25 1.0845828e-19
 9.9696213e-01 3.2912410e-19], sum to 1.0000
[2019-04-04 05:52:23,611] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2608
[2019-04-04 05:52:23,637] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.75, 69.5, 0.0, 0.0, 26.0, 25.58686255696051, 0.3779937056213281, 0.0, 1.0, 18736.74946086045], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4336200.0000, 
sim time next is 4336800.0000, 
raw observation next is [3.7, 69.33333333333334, 0.0, 0.0, 26.0, 25.55573871934882, 0.3640387563507005, 0.0, 1.0, 39614.42546925112], 
processed observation next is [1.0, 0.17391304347826086, 0.5650969529085873, 0.6933333333333335, 0.0, 0.0, 0.6666666666666666, 0.6296448932790684, 0.6213462521169002, 0.0, 1.0, 0.1886401212821482], 
reward next is 0.8114, 
noisyNet noise sample is [array([0.9538263], dtype=float32), -0.5151493]. 
=============================================
[2019-04-04 05:52:26,615] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2977681e-08 1.3229302e-04 3.5227464e-29 4.7972650e-26 4.7469072e-21
 9.9986768e-01 2.5555155e-20], sum to 1.0000
[2019-04-04 05:52:26,615] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3250
[2019-04-04 05:52:26,683] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 196.0, 6.0, 26.0, 25.37267582867382, 0.4711825253449062, 1.0, 1.0, 117355.2700420986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4455000.0000, 
sim time next is 4455600.0000, 
raw observation next is [0.0, 92.0, 177.5, 5.0, 26.0, 24.64543509194624, 0.4478175166597365, 1.0, 1.0, 196265.42042635], 
processed observation next is [1.0, 0.5652173913043478, 0.46260387811634357, 0.92, 0.5916666666666667, 0.0055248618784530384, 0.6666666666666666, 0.5537862576621867, 0.6492725055532454, 1.0, 1.0, 0.9345972401254762], 
reward next is 0.0654, 
noisyNet noise sample is [array([2.6580913], dtype=float32), -1.3316482]. 
=============================================
[2019-04-04 05:52:29,217] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4737166e-08 6.6558248e-03 6.7289612e-27 3.0475501e-25 5.4251576e-19
 9.9334407e-01 7.6301466e-20], sum to 1.0000
[2019-04-04 05:52:29,218] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0879
[2019-04-04 05:52:29,254] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666666, 73.0, 0.0, 0.0, 26.0, 25.28682365341658, 0.4478695009762808, 0.0, 1.0, 44383.60481285467], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4500600.0000, 
sim time next is 4501200.0000, 
raw observation next is [-0.7333333333333334, 73.0, 0.0, 0.0, 26.0, 25.33970993759515, 0.4447212760403434, 0.0, 1.0, 43175.09582074881], 
processed observation next is [1.0, 0.08695652173913043, 0.44228993536472766, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6116424947995958, 0.6482404253467812, 0.0, 1.0, 0.20559569438451814], 
reward next is 0.7944, 
noisyNet noise sample is [array([0.82525486], dtype=float32), 0.7834776]. 
=============================================
[2019-04-04 05:52:32,753] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.29145565e-08 3.75932967e-03 5.46147654e-28 1.70892833e-26
 4.77929384e-20 9.96240616e-01 4.55765144e-20], sum to 1.0000
[2019-04-04 05:52:32,757] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4108
[2019-04-04 05:52:32,764] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 114.0, 732.0, 26.0, 25.18174097781, 0.4459575546928868, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4806600.0000, 
sim time next is 4807200.0000, 
raw observation next is [3.0, 37.0, 105.5, 729.5, 26.0, 25.1876664057114, 0.4449004241384952, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.3516666666666667, 0.8060773480662984, 0.6666666666666666, 0.59897220047595, 0.6483001413794983, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.63850063], dtype=float32), -1.7671884]. 
=============================================
[2019-04-04 05:52:37,667] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7872278e-08 7.7374943e-04 7.9933480e-30 3.3327625e-28 1.4745037e-22
 9.9922621e-01 9.8287262e-22], sum to 1.0000
[2019-04-04 05:52:37,667] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8303
[2019-04-04 05:52:37,679] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.46007458818053, 0.6152619114726513, 0.0, 1.0, 173517.9656989556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4653600.0000, 
sim time next is 4654200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.5615942932229, 0.6452927072265807, 0.0, 1.0, 33962.69591591618], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.630132857768575, 0.7150975690755269, 0.0, 1.0, 0.16172712340912465], 
reward next is 0.8383, 
noisyNet noise sample is [array([0.5955334], dtype=float32), -0.24686658]. 
=============================================
[2019-04-04 05:52:48,566] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:52:48,568] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:52:48,580] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run5
[2019-04-04 05:52:49,375] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:52:49,376] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:52:49,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run5
[2019-04-04 05:52:49,608] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3240084e-08 1.9517265e-03 1.6754069e-29 3.9723634e-27 1.5137958e-20
 9.9804831e-01 1.3330701e-19], sum to 1.0000
[2019-04-04 05:52:49,608] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2607
[2019-04-04 05:52:49,666] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 42.0, 0.0, 0.0, 26.0, 25.0239319600215, 0.3399682803034105, 0.0, 1.0, 18701.77556522362], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4819200.0000, 
sim time next is 4819800.0000, 
raw observation next is [1.166666666666667, 42.5, 0.0, 0.0, 26.0, 25.01330284773766, 0.3333133213826164, 0.0, 1.0, 29739.47577905441], 
processed observation next is [0.0, 0.782608695652174, 0.49492151431209613, 0.425, 0.0, 0.0, 0.6666666666666666, 0.5844419039781382, 0.6111044404608722, 0.0, 1.0, 0.14161655132883053], 
reward next is 0.8584, 
noisyNet noise sample is [array([0.8862919], dtype=float32), 0.984918]. 
=============================================
[2019-04-04 05:52:49,721] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4008979e-09 4.2313868e-03 9.7092422e-27 3.5156131e-25 1.0689335e-19
 9.9576867e-01 4.1025051e-19], sum to 1.0000
[2019-04-04 05:52:49,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2428
[2019-04-04 05:52:49,759] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 45.5, 132.3333333333333, 802.6666666666666, 26.0, 25.18490362160565, 0.421754868509262, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4792200.0000, 
sim time next is 4792800.0000, 
raw observation next is [-1.0, 45.0, 127.1666666666667, 820.8333333333334, 26.0, 25.13951739656009, 0.4143043734632831, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4349030470914128, 0.45, 0.423888888888889, 0.9069981583793739, 0.6666666666666666, 0.5949597830466743, 0.6381014578210943, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4693093], dtype=float32), 0.14371926]. 
=============================================
[2019-04-04 05:52:53,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:52:53,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:52:53,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run5
[2019-04-04 05:52:54,042] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0257739e-08 7.9369126e-03 1.4307046e-28 1.8912754e-26 5.5520694e-19
 9.9206311e-01 6.1864101e-20], sum to 1.0000
[2019-04-04 05:52:54,042] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4502
[2019-04-04 05:52:54,067] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1666666666666666, 38.5, 0.0, 0.0, 26.0, 25.40931665781464, 0.3522041053464646, 0.0, 1.0, 54985.45883895373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4920600.0000, 
sim time next is 4921200.0000, 
raw observation next is [0.0, 39.0, 0.0, 0.0, 26.0, 25.40525360340703, 0.350408205042517, 0.0, 1.0, 47810.38720328847], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 0.39, 0.0, 0.0, 0.6666666666666666, 0.6171044669505857, 0.6168027350141724, 0.0, 1.0, 0.22766851049184986], 
reward next is 0.7723, 
noisyNet noise sample is [array([0.19702642], dtype=float32), -0.92265254]. 
=============================================
[2019-04-04 05:52:56,136] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:52:56,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:52:56,159] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run5
[2019-04-04 05:52:58,722] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:52:58,722] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:52:58,726] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run5
[2019-04-04 05:52:59,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:52:59,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:52:59,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run5
[2019-04-04 05:53:02,924] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3965145e-08 9.6871732e-03 7.2684171e-26 2.2368165e-24 4.1380881e-18
 9.9031281e-01 2.4010522e-18], sum to 1.0000
[2019-04-04 05:53:02,925] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9087
[2019-04-04 05:53:02,946] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.933333333333334, 77.33333333333334, 0.0, 0.0, 26.0, 23.94026207251007, 0.04749342988684104, 0.0, 1.0, 43703.79092034733], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 102000.0000, 
sim time next is 102600.0000, 
raw observation next is [-4.2, 76.5, 0.0, 0.0, 26.0, 23.91027382396179, 0.03676953609796049, 0.0, 1.0, 43774.59706152241], 
processed observation next is [1.0, 0.17391304347826086, 0.34626038781163443, 0.765, 0.0, 0.0, 0.6666666666666666, 0.49252281866348263, 0.5122565120326535, 0.0, 1.0, 0.20845046219772576], 
reward next is 0.7915, 
noisyNet noise sample is [array([-0.36913544], dtype=float32), -0.60025674]. 
=============================================
[2019-04-04 05:53:04,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:53:04,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:53:04,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run5
[2019-04-04 05:53:04,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:53:04,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:53:04,987] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run5
[2019-04-04 05:53:05,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:53:05,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:53:05,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run5
[2019-04-04 05:53:05,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:53:05,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:53:05,439] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run5
[2019-04-04 05:53:05,733] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:53:05,734] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:53:05,735] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run5
[2019-04-04 05:53:06,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:53:06,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:53:06,482] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run5
[2019-04-04 05:53:06,899] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:53:06,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:53:06,907] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run5
[2019-04-04 05:53:08,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:53:08,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:53:08,187] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run5
[2019-04-04 05:53:08,526] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:53:08,526] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:53:08,528] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run5
[2019-04-04 05:53:08,710] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:53:08,711] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:53:08,712] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run5
[2019-04-04 05:53:35,432] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3960920e-06 1.2424541e-02 6.0774317e-21 7.3184074e-20 1.8486947e-15
 9.8757404e-01 3.4701131e-15], sum to 1.0000
[2019-04-04 05:53:35,432] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7689
[2019-04-04 05:53:35,568] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.53333333333333, 80.0, 0.0, 0.0, 26.0, 22.68882597292298, -0.2506863568873711, 1.0, 1.0, 169289.5443173769], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 373200.0000, 
sim time next is 373800.0000, 
raw observation next is [-16.61666666666667, 80.5, 8.333333333333332, 192.3333333333333, 26.0, 23.02883823260426, -0.173822729899304, 1.0, 1.0, 117089.294047759], 
processed observation next is [1.0, 0.30434782608695654, 0.0023084025854107648, 0.805, 0.027777777777777773, 0.21252302025782682, 0.6666666666666666, 0.4190698527170218, 0.4420590900335653, 1.0, 1.0, 0.5575680668940904], 
reward next is 0.4424, 
noisyNet noise sample is [array([-2.1718402], dtype=float32), 1.0285071]. 
=============================================
[2019-04-04 05:53:41,927] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1321953e-07 1.6471619e-03 6.2219662e-23 9.8813489e-22 5.3928609e-17
 9.9835277e-01 1.3593072e-16], sum to 1.0000
[2019-04-04 05:53:41,927] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3467
[2019-04-04 05:53:41,948] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.25936816022655, -0.146349456629293, 0.0, 1.0, 48394.95784017861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 352800.0000, 
sim time next is 353400.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.19827743911543, -0.1698878152409582, 0.0, 1.0, 48615.25348867259], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.6666666666666666, 0.43318978659295243, 0.44337072825301393, 0.0, 1.0, 0.2315012070889171], 
reward next is 0.7685, 
noisyNet noise sample is [array([-0.50409245], dtype=float32), 0.8608275]. 
=============================================
[2019-04-04 05:53:42,701] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.60056413e-07 4.90524527e-03 8.15845549e-24 1.08252093e-21
 2.88974290e-16 9.95094299e-01 1.04631214e-16], sum to 1.0000
[2019-04-04 05:53:42,701] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3970
[2019-04-04 05:53:42,716] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 67.0, 0.0, 0.0, 26.0, 23.74920693613862, -0.02760653130043057, 0.0, 1.0, 45787.16202785919], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 270000.0000, 
sim time next is 270600.0000, 
raw observation next is [-9.0, 67.5, 0.0, 0.0, 26.0, 23.65739882296619, -0.04774756072133516, 0.0, 1.0, 45870.57622868446], 
processed observation next is [1.0, 0.13043478260869565, 0.21329639889196678, 0.675, 0.0, 0.0, 0.6666666666666666, 0.47144990191384917, 0.4840841464262216, 0.0, 1.0, 0.2184313153746879], 
reward next is 0.7816, 
noisyNet noise sample is [array([-0.40769798], dtype=float32), -0.61451006]. 
=============================================
[2019-04-04 05:53:48,520] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8858282e-07 1.5618762e-03 8.5026343e-26 1.3359808e-23 3.5267445e-18
 9.9843794e-01 1.0752721e-18], sum to 1.0000
[2019-04-04 05:53:48,520] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3726
[2019-04-04 05:53:48,582] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.4, 39.33333333333334, 0.0, 0.0, 26.0, 25.18428306445395, 0.3420072712947067, 1.0, 1.0, 67753.06078203549], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 409800.0000, 
sim time next is 410400.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.41733085419627, 0.3658895769135752, 1.0, 1.0, 23694.27018749247], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6181109045163558, 0.6219631923045251, 1.0, 1.0, 0.11282985803567844], 
reward next is 0.8872, 
noisyNet noise sample is [array([0.21506865], dtype=float32), -1.016105]. 
=============================================
[2019-04-04 05:53:58,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8529879e-07 2.6781308e-03 3.2700875e-25 1.7158441e-23 1.7748864e-18
 9.9732167e-01 3.0139475e-18], sum to 1.0000
[2019-04-04 05:53:58,172] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9323
[2019-04-04 05:53:58,242] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 71.0, 77.0, 25.5, 26.0, 25.13000115441331, 0.2542951885962458, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 637200.0000, 
sim time next is 637800.0000, 
raw observation next is [-3.9, 70.0, 96.33333333333334, 34.00000000000001, 26.0, 25.23752663695423, 0.2571583398129578, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.7, 0.3211111111111111, 0.03756906077348067, 0.6666666666666666, 0.6031272197461858, 0.5857194466043193, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2110996], dtype=float32), -0.4036059]. 
=============================================
[2019-04-04 05:53:58,966] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.2991957e-08 7.2222145e-04 6.3490015e-26 3.3732580e-24 2.7810482e-18
 9.9927777e-01 5.6003047e-18], sum to 1.0000
[2019-04-04 05:53:58,966] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6458
[2019-04-04 05:53:59,015] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.1, 42.83333333333334, 0.0, 0.0, 26.0, 25.1203031112209, 0.2923593060311882, 0.0, 1.0, 55079.2016919247], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 418200.0000, 
sim time next is 418800.0000, 
raw observation next is [-10.2, 43.66666666666667, 0.0, 0.0, 26.0, 25.1131177988557, 0.2822466234761107, 0.0, 1.0, 38192.77411239179], 
processed observation next is [1.0, 0.8695652173913043, 0.1800554016620499, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5927598165713084, 0.5940822078253702, 0.0, 1.0, 0.18187035291615136], 
reward next is 0.8181, 
noisyNet noise sample is [array([0.8424363], dtype=float32), 0.0155133065]. 
=============================================
[2019-04-04 05:53:59,178] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.3977041e-08 9.9628344e-03 8.4995535e-26 1.2818426e-24 3.8540756e-19
 9.9003708e-01 6.0507197e-19], sum to 1.0000
[2019-04-04 05:53:59,178] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1614
[2019-04-04 05:53:59,267] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.5, 46.16666666666667, 0.0, 0.0, 26.0, 24.86038313311936, 0.2176706066843299, 0.0, 1.0, 45646.376394648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 420600.0000, 
sim time next is 421200.0000, 
raw observation next is [-10.6, 47.0, 0.0, 0.0, 26.0, 24.74582757380704, 0.1974844220214273, 0.0, 1.0, 45900.07886693767], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.47, 0.0, 0.0, 0.6666666666666666, 0.5621522978172534, 0.5658281406738092, 0.0, 1.0, 0.21857180412827462], 
reward next is 0.7814, 
noisyNet noise sample is [array([-0.77187467], dtype=float32), 0.51295006]. 
=============================================
[2019-04-04 05:54:07,107] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1699931e-08 7.0387521e-03 4.5173318e-28 4.4990041e-26 2.7216191e-20
 9.9296123e-01 1.6527058e-19], sum to 1.0000
[2019-04-04 05:54:07,108] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7182
[2019-04-04 05:54:07,122] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.95539869406632, 0.2056945359603772, 0.0, 1.0, 42392.84878702131], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 680400.0000, 
sim time next is 681000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.93225291837367, 0.1983427404150694, 0.0, 1.0, 42325.23264635897], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5776877431978059, 0.5661142468050232, 0.0, 1.0, 0.20154872688742367], 
reward next is 0.7985, 
noisyNet noise sample is [array([-0.36885452], dtype=float32), 0.49548882]. 
=============================================
[2019-04-04 05:54:07,165] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[104.57412]
 [104.33844]
 [104.21561]
 [104.0625 ]
 [103.90546]], R is [[104.47187042]
 [104.22528076]
 [103.98085785]
 [103.73873901]
 [103.49782562]].
[2019-04-04 05:54:14,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3763783e-08 2.9952923e-04 1.2525201e-28 1.1203447e-25 4.5536143e-21
 9.9970043e-01 8.7325692e-21], sum to 1.0000
[2019-04-04 05:54:14,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8293
[2019-04-04 05:54:14,387] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.82372635599475, 0.1773104119185749, 0.0, 1.0, 41969.2535522118], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 684000.0000, 
sim time next is 684600.0000, 
raw observation next is [-3.483333333333333, 69.33333333333333, 0.0, 0.0, 26.0, 24.79292854856073, 0.1695371409790396, 0.0, 1.0, 41902.84840184404], 
processed observation next is [0.0, 0.9565217391304348, 0.3661126500461681, 0.6933333333333332, 0.0, 0.0, 0.6666666666666666, 0.5660773790467276, 0.5565123803263465, 0.0, 1.0, 0.1995373733421145], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.31522432], dtype=float32), -0.17328893]. 
=============================================
[2019-04-04 05:54:18,460] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4256889e-08 7.2915235e-04 1.7328233e-27 4.0712401e-25 9.8502335e-20
 9.9927086e-01 9.8049513e-19], sum to 1.0000
[2019-04-04 05:54:18,460] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0991
[2019-04-04 05:54:18,538] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.633333333333334, 60.66666666666667, 104.3333333333333, 79.33333333333334, 26.0, 24.88542801049785, 0.2244703572272895, 0.0, 1.0, 31812.93635484563], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 648600.0000, 
sim time next is 649200.0000, 
raw observation next is [-2.566666666666667, 60.33333333333334, 108.1666666666667, 89.66666666666667, 26.0, 24.89113649439282, 0.2251106109110878, 0.0, 1.0, 34772.73831935642], 
processed observation next is [0.0, 0.5217391304347826, 0.39150507848568794, 0.6033333333333334, 0.3605555555555557, 0.0990791896869245, 0.6666666666666666, 0.5742613745327351, 0.575036870303696, 0.0, 1.0, 0.16558446818741152], 
reward next is 0.8344, 
noisyNet noise sample is [array([-0.51019907], dtype=float32), -0.73341995]. 
=============================================
[2019-04-04 05:54:20,694] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.5241922e-08 1.9373121e-03 5.6674768e-29 6.5716150e-26 1.6256199e-20
 9.9806255e-01 2.4465255e-19], sum to 1.0000
[2019-04-04 05:54:20,694] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2149
[2019-04-04 05:54:20,712] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.60939689847279, 0.1328263213369576, 0.0, 1.0, 41513.77173117978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 688200.0000, 
sim time next is 688800.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.56920634825613, 0.1260181878953226, 0.0, 1.0, 41448.64050008085], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5474338623546776, 0.5420060626317742, 0.0, 1.0, 0.1973744785718136], 
reward next is 0.8026, 
noisyNet noise sample is [array([0.1960754], dtype=float32), 0.9738621]. 
=============================================
[2019-04-04 05:54:25,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.2324111e-07 5.5581885e-03 1.6676439e-24 3.9496610e-22 3.6443704e-18
 9.9444121e-01 9.4930341e-18], sum to 1.0000
[2019-04-04 05:54:25,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9787
[2019-04-04 05:54:25,401] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 25.18002114859148, 0.2550131515362249, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 718800.0000, 
sim time next is 719400.0000, 
raw observation next is [-2.3, 76.0, 9.666666666666664, 0.0, 26.0, 25.49501278423731, 0.2790404612938308, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.032222222222222215, 0.0, 0.6666666666666666, 0.6245843986864426, 0.5930134870979437, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05418379], dtype=float32), 0.29975852]. 
=============================================
[2019-04-04 05:54:32,311] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.1491739e-09 3.3633178e-04 1.3793675e-28 4.9708243e-26 3.6788829e-20
 9.9966371e-01 5.4799927e-20], sum to 1.0000
[2019-04-04 05:54:32,312] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2453
[2019-04-04 05:54:32,362] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.8, 93.0, 93.0, 0.0, 26.0, 25.32319701858302, 0.3855538095342765, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 914400.0000, 
sim time next is 915000.0000, 
raw observation next is [3.9, 93.0, 92.0, 0.0, 26.0, 25.71469237845768, 0.4186117930268121, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5706371191135734, 0.93, 0.30666666666666664, 0.0, 0.6666666666666666, 0.6428910315381401, 0.6395372643422707, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5849977], dtype=float32), -0.6724231]. 
=============================================
[2019-04-04 05:54:32,370] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[101.670166]
 [101.370346]
 [100.32531 ]
 [ 99.069466]
 [ 98.61277 ]], R is [[102.02733612]
 [102.00706482]
 [101.27353668]
 [100.36427307]
 [100.18299103]].
[2019-04-04 05:54:40,648] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.8956598e-08 8.1841613e-04 1.1479493e-26 7.3264101e-25 1.0855670e-20
 9.9918145e-01 2.5567261e-19], sum to 1.0000
[2019-04-04 05:54:40,652] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8710
[2019-04-04 05:54:40,706] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.62137997188465, 0.5839179717690014, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1323000.0000, 
sim time next is 1323600.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.84879212577605, 0.5807414285255282, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6540660104813375, 0.693580476175176, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0155295], dtype=float32), -1.0854466]. 
=============================================
[2019-04-04 05:54:42,230] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.72405707e-08 5.93344774e-03 1.16816135e-27 4.05253994e-25
 4.93277647e-21 9.94066477e-01 7.55687780e-20], sum to 1.0000
[2019-04-04 05:54:42,233] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5111
[2019-04-04 05:54:42,248] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 83.0, 0.0, 0.0, 26.0, 25.54641667852056, 0.4462592328722196, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 964800.0000, 
sim time next is 965400.0000, 
raw observation next is [7.883333333333334, 83.0, 0.0, 0.0, 26.0, 25.50183314596334, 0.430437061288145, 0.0, 1.0, 25766.6998794185], 
processed observation next is [1.0, 0.17391304347826086, 0.6809787626962143, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6251527621636116, 0.6434790204293817, 0.0, 1.0, 0.1226985708543738], 
reward next is 0.8773, 
noisyNet noise sample is [array([-0.59649503], dtype=float32), 0.6770113]. 
=============================================
[2019-04-04 05:54:44,674] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9689191e-08 1.5869832e-03 3.0501306e-28 1.2679498e-25 6.1747288e-20
 9.9841297e-01 2.0396622e-19], sum to 1.0000
[2019-04-04 05:54:44,677] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9595
[2019-04-04 05:54:44,725] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.84889374611249, 0.5807558533291405, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1323600.0000, 
sim time next is 1324200.0000, 
raw observation next is [1.1, 92.0, 5.999999999999998, 0.0, 26.0, 25.8570547729293, 0.5601512942262755, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.019999999999999993, 0.0, 0.6666666666666666, 0.654754564410775, 0.6867170980754252, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17855407], dtype=float32), 0.24743223]. 
=============================================
[2019-04-04 05:54:46,512] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9419709e-08 1.8470928e-03 6.1026196e-28 8.2948151e-26 2.7295215e-20
 9.9815291e-01 5.1777063e-20], sum to 1.0000
[2019-04-04 05:54:46,516] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8557
[2019-04-04 05:54:46,532] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.63333333333333, 60.66666666666667, 0.0, 0.0, 26.0, 25.6664595950897, 0.6485457064971132, 0.0, 1.0, 196244.1853905028], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1110000.0000, 
sim time next is 1110600.0000, 
raw observation next is [13.55, 61.0, 0.0, 0.0, 26.0, 25.62507165620186, 0.671548301064854, 0.0, 1.0, 149901.3395285958], 
processed observation next is [1.0, 0.8695652173913043, 0.8379501385041552, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6354226380168218, 0.7238494336882847, 0.0, 1.0, 0.7138159025171228], 
reward next is 0.2862, 
noisyNet noise sample is [array([-0.79756624], dtype=float32), -0.89558953]. 
=============================================
[2019-04-04 05:54:49,343] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1533264e-07 5.0601680e-03 3.0142143e-25 2.2047523e-23 2.5581690e-18
 9.9493968e-01 9.7866253e-17], sum to 1.0000
[2019-04-04 05:54:49,344] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9478
[2019-04-04 05:54:49,352] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.38333333333333, 64.66666666666667, 167.0, 0.0, 26.0, 25.08993010432312, 0.502443933672211, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1169400.0000, 
sim time next is 1170000.0000, 
raw observation next is [18.3, 65.0, 165.0, 0.0, 26.0, 25.07881700800527, 0.5005411337244349, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.55, 0.0, 0.6666666666666666, 0.5899014173337725, 0.6668470445748116, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41161963], dtype=float32), -0.5241631]. 
=============================================
[2019-04-04 05:54:49,366] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[88.439896]
 [88.306076]
 [88.17833 ]
 [88.89667 ]
 [89.9097  ]], R is [[88.75175476]
 [88.86423492]
 [88.97559357]
 [89.08583832]
 [89.19498444]].
[2019-04-04 05:54:53,628] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1350238e-09 2.2286405e-04 5.6437576e-32 2.1329260e-29 2.9309843e-23
 9.9977714e-01 4.9386979e-22], sum to 1.0000
[2019-04-04 05:54:53,631] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1028
[2019-04-04 05:54:53,646] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.73333333333333, 100.0, 15.83333333333333, 0.0, 26.0, 24.59671513125113, 0.4220676221159118, 0.0, 1.0, 27380.05091328887], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1269600.0000, 
sim time next is 1270200.0000, 
raw observation next is [12.46666666666667, 100.0, 12.66666666666667, 0.0, 26.0, 24.59374203408588, 0.4230319758482984, 0.0, 1.0, 27467.20055925251], 
processed observation next is [0.0, 0.6956521739130435, 0.8079409048938138, 1.0, 0.04222222222222223, 0.0, 0.6666666666666666, 0.5494785028404902, 0.6410106586160994, 0.0, 1.0, 0.13079619313929766], 
reward next is 0.8692, 
noisyNet noise sample is [array([-0.6296853], dtype=float32), 0.46659386]. 
=============================================
[2019-04-04 05:54:54,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1434150e-07 3.1015901e-03 3.5444560e-27 1.9933719e-26 4.3401737e-20
 9.9689817e-01 6.9268790e-20], sum to 1.0000
[2019-04-04 05:54:54,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5279
[2019-04-04 05:54:54,559] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.34599796900246, 0.4491986343549033, 0.0, 1.0, 36899.92205352296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1488600.0000, 
sim time next is 1489200.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.3278814255723, 0.4468538328092188, 0.0, 1.0, 36923.79378885091], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6106567854643584, 0.6489512776030729, 0.0, 1.0, 0.17582758947071864], 
reward next is 0.8242, 
noisyNet noise sample is [array([-2.101041], dtype=float32), 0.9379705]. 
=============================================
[2019-04-04 05:55:08,849] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.1502906e-09 4.3076236e-04 1.0720107e-27 1.1369012e-25 2.2044442e-20
 9.9956924e-01 8.8650710e-20], sum to 1.0000
[2019-04-04 05:55:08,871] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5857
[2019-04-04 05:55:08,976] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 83.66666666666667, 121.3333333333333, 0.0, 26.0, 24.94727968141316, 0.3460634644994933, 0.0, 1.0, 34110.39156545041], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1770600.0000, 
sim time next is 1771200.0000, 
raw observation next is [-2.3, 83.0, 122.5, 0.0, 26.0, 24.94534196840301, 0.3452277232366862, 0.0, 1.0, 39828.19625335765], 
processed observation next is [0.0, 0.5217391304347826, 0.3988919667590028, 0.83, 0.4083333333333333, 0.0, 0.6666666666666666, 0.5787784973669176, 0.6150759077455621, 0.0, 1.0, 0.1896580773969412], 
reward next is 0.8103, 
noisyNet noise sample is [array([0.7683826], dtype=float32), 0.0816255]. 
=============================================
[2019-04-04 05:55:39,676] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.7226255e-07 5.3228415e-04 7.9560117e-26 2.8029449e-23 2.3822855e-18
 9.9946684e-01 1.5380026e-17], sum to 1.0000
[2019-04-04 05:55:39,676] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5142
[2019-04-04 05:55:39,711] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 78.5, 0.0, 0.0, 26.0, 24.13181745227953, 0.05251711602799677, 0.0, 1.0, 45088.79561265646], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1902600.0000, 
sim time next is 1903200.0000, 
raw observation next is [-7.3, 77.33333333333334, 0.0, 0.0, 26.0, 24.13926200578038, 0.04694091792797208, 0.0, 1.0, 45104.21047853529], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5116051671483651, 0.5156469726426574, 0.0, 1.0, 0.21478195465969188], 
reward next is 0.7852, 
noisyNet noise sample is [array([-1.6031235], dtype=float32), -0.6292794]. 
=============================================
[2019-04-04 05:55:45,197] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7717963e-09 2.1228129e-04 6.5883439e-28 2.5046395e-24 1.9955528e-19
 9.9978775e-01 5.7194949e-19], sum to 1.0000
[2019-04-04 05:55:45,197] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9295
[2019-04-04 05:55:45,311] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 81.66666666666667, 110.0, 27.99999999999999, 26.0, 25.07885225736091, 0.2800462277947215, 0.0, 1.0, 21254.33111290567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1869000.0000, 
sim time next is 1869600.0000, 
raw observation next is [-4.5, 80.33333333333334, 91.0, 14.0, 26.0, 25.06288046194037, 0.2701985047540036, 0.0, 1.0, 38937.04404429552], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.8033333333333335, 0.30333333333333334, 0.015469613259668509, 0.6666666666666666, 0.5885733718283642, 0.5900661682513345, 0.0, 1.0, 0.1854144954490263], 
reward next is 0.8146, 
noisyNet noise sample is [array([0.7490795], dtype=float32), -0.5433978]. 
=============================================
[2019-04-04 05:56:01,137] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0725186e-06 5.0978973e-03 2.0461597e-23 2.6818923e-22 2.0012384e-17
 9.9490106e-01 3.5091893e-17], sum to 1.0000
[2019-04-04 05:56:01,137] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6845
[2019-04-04 05:56:01,174] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.10317290068479, 0.08814717150207259, 0.0, 1.0, 41601.84497799024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2002800.0000, 
sim time next is 2003400.0000, 
raw observation next is [-5.9, 85.0, 0.0, 0.0, 26.0, 24.20518195534429, 0.08934405356819247, 0.0, 1.0, 41329.9437061411], 
processed observation next is [1.0, 0.17391304347826086, 0.2991689750692521, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5170984962786909, 0.5297813511893975, 0.0, 1.0, 0.19680925574352906], 
reward next is 0.8032, 
noisyNet noise sample is [array([-1.0089719], dtype=float32), -0.71887624]. 
=============================================
[2019-04-04 05:56:09,587] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-04 05:56:09,605] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:56:09,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:09,607] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run7
[2019-04-04 05:56:09,662] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:56:09,663] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:09,665] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run7
[2019-04-04 05:56:09,705] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:56:09,705] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:09,707] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run7
[2019-04-04 05:59:26,194] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.6336 239896941.9356 1604.4269
[2019-04-04 05:59:57,064] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6546 263412524.4972 1552.1333
[2019-04-04 06:00:04,268] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.8420 275763179.2874 1233.6268
[2019-04-04 06:00:05,307] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 600000, evaluation results [600000.0, 7241.654645251345, 263412524.49722072, 1552.133287587248, 7353.63360983056, 239896941.9355824, 1604.4268913787394, 7182.842003393331, 275763179.28739864, 1233.626758324726]
[2019-04-04 06:00:22,032] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0770881e-08 3.8075034e-04 1.0596481e-25 1.3201743e-23 1.4363998e-18
 9.9961925e-01 1.7740822e-18], sum to 1.0000
[2019-04-04 06:00:22,032] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7892
[2019-04-04 06:00:22,101] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.566666666666667, 67.0, 141.3333333333333, 0.0, 26.0, 25.17975810739475, 0.2995550793721066, 1.0, 1.0, 60718.73921734166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2208000.0000, 
sim time next is 2208600.0000, 
raw observation next is [-3.65, 68.0, 144.0, 0.0, 26.0, 25.17144543155078, 0.3324681526042423, 1.0, 1.0, 53783.35090532493], 
processed observation next is [1.0, 0.5652173913043478, 0.3614958448753463, 0.68, 0.48, 0.0, 0.6666666666666666, 0.5976204526292316, 0.6108227175347475, 1.0, 1.0, 0.2561111947872616], 
reward next is 0.7439, 
noisyNet noise sample is [array([-0.54898554], dtype=float32), -1.6462191]. 
=============================================
[2019-04-04 06:00:29,673] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.58542218e-08 8.48318741e-04 1.23805866e-23 1.74630984e-22
 1.64517307e-17 9.99151587e-01 3.14489205e-17], sum to 1.0000
[2019-04-04 06:00:29,674] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6304
[2019-04-04 06:00:29,719] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.3, 86.83333333333333, 0.0, 0.0, 26.0, 24.05788445449637, 0.07265457053111019, 0.0, 1.0, 43638.95679705339], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2260200.0000, 
sim time next is 2260800.0000, 
raw observation next is [-8.4, 87.0, 0.0, 0.0, 26.0, 24.05927785081467, 0.06331720424816507, 0.0, 1.0, 43611.24096825542], 
processed observation next is [1.0, 0.17391304347826086, 0.2299168975069252, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5049398209012225, 0.5211057347493884, 0.0, 1.0, 0.20767257603931152], 
reward next is 0.7923, 
noisyNet noise sample is [array([0.14783043], dtype=float32), -0.8041116]. 
=============================================
[2019-04-04 06:00:32,796] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5025340e-06 3.0312988e-03 1.0037175e-23 1.0761634e-21 9.5300133e-17
 9.9696726e-01 4.3641088e-16], sum to 1.0000
[2019-04-04 06:00:32,798] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8382
[2019-04-04 06:00:32,856] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 54.0, 21.0, 5.999999999999998, 26.0, 25.25971841124982, 0.2055125178561582, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2533800.0000, 
sim time next is 2534400.0000, 
raw observation next is [-2.8, 54.0, 28.5, 9.0, 26.0, 25.17928157883103, 0.187843676434658, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.38504155124653744, 0.54, 0.095, 0.009944751381215469, 0.6666666666666666, 0.5982734649025859, 0.5626145588115526, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.5729303], dtype=float32), 1.760494]. 
=============================================
[2019-04-04 06:00:34,586] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8951140e-08 1.1601399e-04 3.7451090e-25 8.0521786e-24 9.7501516e-19
 9.9988389e-01 1.6761303e-18], sum to 1.0000
[2019-04-04 06:00:34,586] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9276
[2019-04-04 06:00:34,658] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.1, 63.0, 161.0, 110.0, 26.0, 25.69455685653794, 0.3492876817087946, 1.0, 1.0, 50758.58172208707], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2287800.0000, 
sim time next is 2288400.0000, 
raw observation next is [-3.8, 61.33333333333333, 177.8333333333333, 104.0, 26.0, 25.69060068817118, 0.3550907013840536, 1.0, 1.0, 32488.43516694608], 
processed observation next is [1.0, 0.4782608695652174, 0.3573407202216067, 0.6133333333333333, 0.5927777777777776, 0.11491712707182321, 0.6666666666666666, 0.6408833906809317, 0.6183635671280179, 1.0, 1.0, 0.15470683412831468], 
reward next is 0.8453, 
noisyNet noise sample is [array([-0.2332362], dtype=float32), -0.21399625]. 
=============================================
[2019-04-04 06:00:38,088] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6893426e-06 7.4537629e-03 9.6981424e-23 2.8088329e-21 8.1852678e-17
 9.9254358e-01 2.8049213e-16], sum to 1.0000
[2019-04-04 06:00:38,088] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7855
[2019-04-04 06:00:38,105] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.60555020221673, 0.2032078240305673, 0.0, 1.0, 39536.26178322114], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2343600.0000, 
sim time next is 2344200.0000, 
raw observation next is [-2.383333333333333, 62.5, 0.0, 0.0, 26.0, 24.56850200302861, 0.19616200422678, 0.0, 1.0, 39648.0614712327], 
processed observation next is [0.0, 0.13043478260869565, 0.3965835641735919, 0.625, 0.0, 0.0, 0.6666666666666666, 0.5473751669190507, 0.56538733474226, 0.0, 1.0, 0.18880029272015572], 
reward next is 0.8112, 
noisyNet noise sample is [array([-0.09719393], dtype=float32), -0.34997925]. 
=============================================
[2019-04-04 06:00:39,318] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.86828814e-07 1.52108865e-02 1.13789014e-24 1.75893490e-22
 7.29321176e-18 9.84788895e-01 2.25341949e-17], sum to 1.0000
[2019-04-04 06:00:39,357] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5400
[2019-04-04 06:00:39,388] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.72433868818727, 0.2396077605562844, 0.0, 1.0, 38721.3614804728], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2338800.0000, 
sim time next is 2339400.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.69679457090782, 0.2381130109635035, 0.0, 1.0, 38830.88764007483], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5580662142423183, 0.5793710036545011, 0.0, 1.0, 0.18490898876226108], 
reward next is 0.8151, 
noisyNet noise sample is [array([0.65096986], dtype=float32), 0.4677556]. 
=============================================
[2019-04-04 06:00:44,351] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5813326e-08 1.3558439e-03 2.5534042e-25 4.1569231e-23 9.6975495e-18
 9.9864417e-01 8.4828141e-18], sum to 1.0000
[2019-04-04 06:00:44,352] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9585
[2019-04-04 06:00:44,419] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8833333333333332, 31.83333333333334, 87.33333333333334, 834.3333333333334, 26.0, 25.0245916028935, 0.2618208491584732, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2461800.0000, 
sim time next is 2462400.0000, 
raw observation next is [-0.6, 31.0, 88.0, 837.0, 26.0, 25.00475277179702, 0.2548591455574808, 0.0, 1.0, 18722.65529910065], 
processed observation next is [0.0, 0.5217391304347826, 0.44598337950138506, 0.31, 0.29333333333333333, 0.9248618784530387, 0.6666666666666666, 0.5837293976497516, 0.5849530485191603, 0.0, 1.0, 0.08915550142428881], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.8965169], dtype=float32), 2.5219958]. 
=============================================
[2019-04-04 06:00:44,470] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.1119397e-08 5.9220428e-04 6.6145241e-26 6.8673008e-24 2.7328827e-18
 9.9940777e-01 2.2658036e-18], sum to 1.0000
[2019-04-04 06:00:44,470] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3060
[2019-04-04 06:00:44,494] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666667, 43.0, 0.0, 0.0, 26.0, 24.93857941814683, 0.2228946946695596, 0.0, 1.0, 42972.87509798988], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2413200.0000, 
sim time next is 2413800.0000, 
raw observation next is [-4.75, 42.5, 0.0, 0.0, 26.0, 24.89203598443555, 0.2124690183429165, 0.0, 1.0, 42991.607617088], 
processed observation next is [0.0, 0.9565217391304348, 0.3310249307479225, 0.425, 0.0, 0.0, 0.6666666666666666, 0.574336332036296, 0.5708230061143055, 0.0, 1.0, 0.20472194103375235], 
reward next is 0.7953, 
noisyNet noise sample is [array([-0.2566136], dtype=float32), 1.1337744]. 
=============================================
[2019-04-04 06:00:49,170] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.1865678e-07 1.3284025e-03 5.6352575e-23 5.6825826e-21 2.3536170e-17
 9.9867070e-01 6.0159291e-17], sum to 1.0000
[2019-04-04 06:00:49,179] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7966
[2019-04-04 06:00:49,193] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 48.16666666666667, 0.0, 0.0, 26.0, 24.9945782795892, 0.1781174425385504, 0.0, 1.0, 38414.01837847375], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2519400.0000, 
sim time next is 2520000.0000, 
raw observation next is [-1.7, 49.0, 0.0, 0.0, 26.0, 24.97581896108063, 0.169564104490948, 0.0, 1.0, 38366.72927758736], 
processed observation next is [1.0, 0.17391304347826086, 0.4155124653739613, 0.49, 0.0, 0.0, 0.6666666666666666, 0.5813182467567192, 0.5565213681636493, 0.0, 1.0, 0.1826987108456541], 
reward next is 0.8173, 
noisyNet noise sample is [array([-1.5153084], dtype=float32), -1.3180488]. 
=============================================
[2019-04-04 06:00:49,208] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[79.99217]
 [79.82175]
 [79.64193]
 [79.45517]
 [79.25096]], R is [[80.23342133]
 [80.24816895]
 [80.26271057]
 [80.27696228]
 [80.29080963]].
[2019-04-04 06:00:49,571] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.6877626e-08 2.9299548e-03 4.6021507e-28 4.8083577e-26 3.8724615e-19
 9.9706995e-01 1.1041547e-19], sum to 1.0000
[2019-04-04 06:00:49,571] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5598
[2019-04-04 06:00:49,627] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 61.5, 0.0, 0.0, 26.0, 25.21867867783507, 0.4400998924498689, 0.0, 1.0, 114357.9410690963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2752200.0000, 
sim time next is 2752800.0000, 
raw observation next is [-5.666666666666667, 62.33333333333333, 0.0, 0.0, 26.0, 25.28827250877396, 0.4537560900325645, 0.0, 1.0, 75137.46604340075], 
processed observation next is [1.0, 0.8695652173913043, 0.30563250230840255, 0.6233333333333333, 0.0, 0.0, 0.6666666666666666, 0.60735604239783, 0.6512520300108549, 0.0, 1.0, 0.3577974573495274], 
reward next is 0.6422, 
noisyNet noise sample is [array([0.7025382], dtype=float32), 0.7302687]. 
=============================================
[2019-04-04 06:00:51,261] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2923626e-08 1.3068992e-03 7.6294116e-28 5.2884377e-25 4.8747036e-19
 9.9869305e-01 1.0881125e-19], sum to 1.0000
[2019-04-04 06:00:51,263] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4034
[2019-04-04 06:00:51,303] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.566666666666667, 26.0, 13.0, 82.33333333333333, 26.0, 24.9145149098102, 0.2070659943960067, 0.0, 1.0, 75209.02982891817], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2481600.0000, 
sim time next is 2482200.0000, 
raw observation next is [2.2, 26.5, 0.0, 0.0, 26.0, 24.86001971877837, 0.2061552053278308, 0.0, 1.0, 90879.86720654953], 
processed observation next is [0.0, 0.7391304347826086, 0.5235457063711911, 0.265, 0.0, 0.0, 0.6666666666666666, 0.5716683098981976, 0.5687184017759436, 0.0, 1.0, 0.43276127241214063], 
reward next is 0.5672, 
noisyNet noise sample is [array([0.9672425], dtype=float32), -1.0694083]. 
=============================================
[2019-04-04 06:01:06,070] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9198966e-08 2.4114926e-03 1.1325029e-26 1.0618451e-25 1.8705747e-19
 9.9758852e-01 2.0660953e-19], sum to 1.0000
[2019-04-04 06:01:06,070] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0010
[2019-04-04 06:01:06,128] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.333333333333333, 28.0, 139.8333333333333, 28.83333333333333, 26.0, 25.87545612831176, 0.4280911902281646, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2816400.0000, 
sim time next is 2817000.0000, 
raw observation next is [6.5, 27.0, 118.0, 0.0, 25.0, 25.92092115921937, 0.412177983454548, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6426592797783934, 0.27, 0.3933333333333333, 0.0, 0.5833333333333334, 0.6600767632682807, 0.637392661151516, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8255746], dtype=float32), 1.1597562]. 
=============================================
[2019-04-04 06:01:06,158] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[98.681335]
 [98.94076 ]
 [99.06133 ]
 [99.21891 ]
 [99.266014]], R is [[98.28296661]
 [98.30014038]
 [98.31713867]
 [98.33396912]
 [98.35063171]].
[2019-04-04 06:01:07,507] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5749932e-09 4.9501890e-05 3.3506740e-27 1.1064087e-24 3.4610612e-19
 9.9995053e-01 8.9805975e-19], sum to 1.0000
[2019-04-04 06:01:07,507] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2092
[2019-04-04 06:01:07,572] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.166666666666666, 64.0, 112.3333333333333, 787.0, 26.0, 25.9868368806542, 0.4714441272343023, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2721000.0000, 
sim time next is 2721600.0000, 
raw observation next is [-8.0, 64.0, 112.5, 790.0, 26.0, 25.94378488818349, 0.471077169471096, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.24099722991689754, 0.64, 0.375, 0.8729281767955801, 0.6666666666666666, 0.6619820740152909, 0.657025723157032, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1740545], dtype=float32), -0.75174314]. 
=============================================
[2019-04-04 06:01:08,681] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3052970e-07 6.2198081e-04 3.3893586e-24 3.1844242e-22 4.7639426e-17
 9.9937785e-01 2.1191389e-16], sum to 1.0000
[2019-04-04 06:01:08,681] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9511
[2019-04-04 06:01:08,754] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.5, 70.0, 117.0, 674.0, 26.0, 26.07591317396581, 0.4683581833619745, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2716200.0000, 
sim time next is 2716800.0000, 
raw observation next is [-10.0, 68.0, 116.1666666666667, 691.8333333333334, 26.0, 26.08137368340477, 0.4731580784705756, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.18559556786703602, 0.68, 0.38722222222222236, 0.7644567219152855, 0.6666666666666666, 0.6734478069503975, 0.6577193594901919, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9533466], dtype=float32), 0.20410186]. 
=============================================
[2019-04-04 06:01:17,264] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6714873e-08 2.6674708e-04 1.2507816e-26 2.5911807e-24 3.9039996e-19
 9.9973327e-01 1.9269441e-19], sum to 1.0000
[2019-04-04 06:01:17,265] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8103
[2019-04-04 06:01:17,283] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333333, 97.66666666666667, 73.16666666666666, 0.0, 26.0, 25.41803066374165, 0.308908490253191, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2888400.0000, 
sim time next is 2889000.0000, 
raw observation next is [0.5, 96.5, 78.0, 0.0, 26.0, 25.42645350017683, 0.3101757671960457, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.4764542936288089, 0.965, 0.26, 0.0, 0.6666666666666666, 0.6188711250147358, 0.6033919223986819, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.7806847], dtype=float32), 0.7697413]. 
=============================================
[2019-04-04 06:01:17,300] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[92.31446 ]
 [91.59701 ]
 [91.18894 ]
 [90.865105]
 [90.60766 ]], R is [[92.91120911]
 [92.89315033]
 [92.87526703]
 [92.8575592 ]
 [92.84002686]].
[2019-04-04 06:01:19,962] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.0038955e-09 3.0965256e-04 5.3249796e-29 2.5602047e-27 6.0764728e-21
 9.9969041e-01 7.0828479e-21], sum to 1.0000
[2019-04-04 06:01:19,974] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7071
[2019-04-04 06:01:19,992] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 106.5, 729.5, 26.0, 26.80652596232314, 0.6775114540266006, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3147600.0000, 
sim time next is 3148200.0000, 
raw observation next is [7.0, 100.0, 108.0, 746.0, 26.0, 26.89012551555707, 0.6986957310228759, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.36, 0.8243093922651934, 0.6666666666666666, 0.7408437929630892, 0.7328985770076253, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5028934], dtype=float32), -0.5136889]. 
=============================================
[2019-04-04 06:01:20,212] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4875200e-09 2.4827817e-04 1.6728208e-29 1.1609652e-26 2.0857864e-21
 9.9975175e-01 7.6700815e-21], sum to 1.0000
[2019-04-04 06:01:20,214] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5009
[2019-04-04 06:01:20,257] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 100.0, 88.33333333333334, 0.0, 26.0, 25.44023881610655, 0.4164999991908954, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2904600.0000, 
sim time next is 2905200.0000, 
raw observation next is [2.0, 100.0, 87.5, 0.0, 26.0, 25.66071933724276, 0.438112255463475, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.2916666666666667, 0.0, 0.6666666666666666, 0.6383932781035634, 0.646037418487825, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3802376], dtype=float32), 0.6765738]. 
=============================================
[2019-04-04 06:01:28,485] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2471379e-10 1.2584002e-05 4.2251447e-31 1.5799624e-28 5.6621154e-23
 9.9998736e-01 3.8315461e-22], sum to 1.0000
[2019-04-04 06:01:28,485] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9061
[2019-04-04 06:01:28,493] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 109.0, 755.8333333333334, 26.0, 26.96257743583566, 0.7171823669672736, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3148800.0000, 
sim time next is 3149400.0000, 
raw observation next is [7.0, 100.0, 110.0, 765.6666666666666, 26.0, 27.01231623212872, 0.7301365307504549, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.36666666666666664, 0.8460405156537752, 0.6666666666666666, 0.7510263526773931, 0.743378843583485, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0760715], dtype=float32), -1.4698137]. 
=============================================
[2019-04-04 06:01:37,770] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5318945e-09 2.1936066e-04 2.9236350e-31 1.7669154e-29 1.6573889e-22
 9.9978060e-01 1.8500790e-23], sum to 1.0000
[2019-04-04 06:01:37,771] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7473
[2019-04-04 06:01:37,784] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 94.16666666666666, 0.0, 0.0, 26.0, 25.37794664837411, 0.6187706724660896, 0.0, 1.0, 116448.098369976], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3192600.0000, 
sim time next is 3193200.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.42330730252847, 0.631237726319784, 0.0, 1.0, 43237.39654256455], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6186089418773726, 0.710412575439928, 0.0, 1.0, 0.20589236448840262], 
reward next is 0.7941, 
noisyNet noise sample is [array([-0.77192944], dtype=float32), -1.1046067]. 
=============================================
[2019-04-04 06:01:44,532] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7271297e-08 3.3743918e-04 6.6564520e-29 2.1015677e-26 1.7802035e-21
 9.9966252e-01 1.1878096e-20], sum to 1.0000
[2019-04-04 06:01:44,537] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2897
[2019-04-04 06:01:44,581] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.30925408403305, 0.473819559630124, 0.0, 1.0, 61595.86933066668], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3278400.0000, 
sim time next is 3279000.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.29422396118892, 0.475774605737987, 0.0, 1.0, 50710.77976272576], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6078519967657433, 0.6585915352459957, 0.0, 1.0, 0.24147990363202743], 
reward next is 0.7585, 
noisyNet noise sample is [array([-1.2829788], dtype=float32), 1.3320789]. 
=============================================
[2019-04-04 06:01:44,606] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[101.02344]
 [100.49869]
 [100.21878]
 [100.27461]
 [100.11023]], R is [[100.75786591]
 [100.45697784]
 [100.14170837]
 [ 99.96867371]
 [ 99.6506958 ]].
[2019-04-04 06:01:47,210] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.07140585e-10 9.06611895e-05 3.31358390e-28 1.22934905e-26
 5.54121143e-21 9.99909282e-01 5.84806596e-21], sum to 1.0000
[2019-04-04 06:01:47,210] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9621
[2019-04-04 06:01:47,225] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.166666666666667, 50.0, 99.66666666666666, 726.0, 26.0, 26.51813652470935, 0.6924886574048088, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3336600.0000, 
sim time next is 3337200.0000, 
raw observation next is [-3.0, 50.0, 96.5, 713.0, 26.0, 26.61746737771821, 0.7016440475920095, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3795013850415513, 0.5, 0.32166666666666666, 0.7878453038674034, 0.6666666666666666, 0.7181222814765175, 0.7338813491973365, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15157476], dtype=float32), 0.15102549]. 
=============================================
[2019-04-04 06:01:52,139] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3415663e-08 4.1948707e-04 6.6202802e-28 1.8796525e-25 2.4932033e-19
 9.9958044e-01 2.3878688e-19], sum to 1.0000
[2019-04-04 06:01:52,140] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5482
[2019-04-04 06:01:52,195] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.333333333333333, 47.33333333333333, 82.5, 645.1666666666667, 26.0, 26.05094613401486, 0.6324834909364138, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3339600.0000, 
sim time next is 3340200.0000, 
raw observation next is [-2.166666666666667, 46.66666666666667, 78.0, 616.3333333333334, 26.0, 25.49607713594021, 0.599809059383435, 1.0, 1.0, 62338.498916518], 
processed observation next is [1.0, 0.6521739130434783, 0.4025854108956602, 0.46666666666666673, 0.26, 0.6810313075506446, 0.6666666666666666, 0.6246730946616843, 0.6999363531278117, 1.0, 1.0, 0.2968499948405619], 
reward next is 0.7032, 
noisyNet noise sample is [array([0.51796687], dtype=float32), 0.76707876]. 
=============================================
[2019-04-04 06:01:58,806] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.6651322e-09 4.0497156e-03 2.3459650e-29 4.8791309e-27 3.3421718e-21
 9.9595022e-01 2.0351346e-20], sum to 1.0000
[2019-04-04 06:01:58,808] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9713
[2019-04-04 06:01:58,825] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.32013308346173, 0.4704858923476922, 0.0, 1.0, 57546.80034075963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3451800.0000, 
sim time next is 3452400.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.35408937512011, 0.4752127027020796, 0.0, 1.0, 47010.98980464451], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6128407812600093, 0.6584042342340265, 0.0, 1.0, 0.22386185621259289], 
reward next is 0.7761, 
noisyNet noise sample is [array([0.39860132], dtype=float32), 1.0658213]. 
=============================================
[2019-04-04 06:02:02,003] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.0154584e-08 9.8170654e-04 4.2533580e-25 9.7234238e-23 3.8130498e-17
 9.9901819e-01 3.3528704e-17], sum to 1.0000
[2019-04-04 06:02:02,004] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5219
[2019-04-04 06:02:02,019] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.11391061113689, 0.2795087611241766, 0.0, 1.0, 41759.12336366907], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3736800.0000, 
sim time next is 3737400.0000, 
raw observation next is [-3.166666666666667, 67.0, 0.0, 0.0, 26.0, 25.09861177274912, 0.2763821177790255, 0.0, 1.0, 41865.74071432983], 
processed observation next is [1.0, 0.2608695652173913, 0.3748845798707295, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5915509810624267, 0.5921273725930085, 0.0, 1.0, 0.1993606700682373], 
reward next is 0.8006, 
noisyNet noise sample is [array([0.69364977], dtype=float32), 0.5762623]. 
=============================================
[2019-04-04 06:02:07,606] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1620242e-06 4.7353897e-03 6.4233038e-22 1.3929909e-20 8.1046473e-16
 9.9526238e-01 3.5036444e-15], sum to 1.0000
[2019-04-04 06:02:07,610] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2559
[2019-04-04 06:02:07,652] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.833333333333334, 52.33333333333334, 97.0, 616.6666666666666, 26.0, 25.89805473138899, 0.4881047852710563, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3921000.0000, 
sim time next is 3921600.0000, 
raw observation next is [-7.666666666666667, 51.66666666666667, 98.5, 654.3333333333334, 26.0, 25.88469323779433, 0.5090423366417314, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2502308402585411, 0.5166666666666667, 0.3283333333333333, 0.7230202578268877, 0.6666666666666666, 0.6570577698161942, 0.6696807788805771, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7323068], dtype=float32), -0.7565502]. 
=============================================
[2019-04-04 06:02:08,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0488244e-09 3.0866102e-04 3.0480553e-27 1.2634224e-26 2.1535182e-20
 9.9969137e-01 7.1609103e-20], sum to 1.0000
[2019-04-04 06:02:08,592] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0753
[2019-04-04 06:02:08,626] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.166666666666667, 39.16666666666666, 112.3333333333333, 812.0, 26.0, 26.73417077079701, 0.7390684075994516, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3937800.0000, 
sim time next is 3938400.0000, 
raw observation next is [-5.0, 38.0, 110.5, 806.0, 26.0, 26.8076004583077, 0.7534225659842736, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.32409972299168976, 0.38, 0.36833333333333335, 0.8906077348066298, 0.6666666666666666, 0.733966704858975, 0.7511408553280913, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24600886], dtype=float32), -1.0313112]. 
=============================================
[2019-04-04 06:02:15,112] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5519646e-08 2.9290800e-03 5.1428816e-26 1.3899249e-24 3.6394425e-18
 9.9707091e-01 1.1581041e-18], sum to 1.0000
[2019-04-04 06:02:15,112] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8427
[2019-04-04 06:02:15,127] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3423993e-09 1.1238820e-04 1.8722897e-28 8.4692378e-26 2.1361030e-20
 9.9988759e-01 1.6619810e-20], sum to 1.0000
[2019-04-04 06:02:15,128] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4455
[2019-04-04 06:02:15,143] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.8538984297193, 0.5715723466935079, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3954000.0000, 
sim time next is 3954600.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.83922891089884, 0.5487964354417183, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.6532690759082366, 0.6829321451472395, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.61179775], dtype=float32), -0.38677102]. 
=============================================
[2019-04-04 06:02:15,145] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.27847425734662, 0.4190184539928206, 0.0, 1.0, 43702.77859262818], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3801600.0000, 
sim time next is 3802200.0000, 
raw observation next is [-3.166666666666667, 72.0, 0.0, 0.0, 26.0, 25.25829029705752, 0.4121959177078708, 0.0, 1.0, 43738.4804636444], 
processed observation next is [1.0, 0.0, 0.3748845798707295, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6048575247547934, 0.6373986392359569, 0.0, 1.0, 0.20827847839830668], 
reward next is 0.7917, 
noisyNet noise sample is [array([1.3064318], dtype=float32), 1.225728]. 
=============================================
[2019-04-04 06:02:16,819] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.5973788e-08 7.8465242e-04 1.4510662e-25 1.2146809e-23 6.0958070e-18
 9.9921525e-01 1.5790758e-18], sum to 1.0000
[2019-04-04 06:02:16,819] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7323
[2019-04-04 06:02:16,837] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.40455442337111, 0.4445156020002252, 0.0, 1.0, 21906.98642344976], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3892200.0000, 
sim time next is 3892800.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.47853083689603, 0.4373451969438087, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6232109030746692, 0.6457817323146029, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.864937], dtype=float32), 0.015064774]. 
=============================================
[2019-04-04 06:02:23,137] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6043139e-07 3.9809812e-03 1.3179104e-23 4.6060042e-21 2.4472352e-16
 9.9601871e-01 7.3024727e-16], sum to 1.0000
[2019-04-04 06:02:23,137] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7472
[2019-04-04 06:02:23,202] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 45.0, 100.0, 574.0, 26.0, 25.56801391950669, 0.4203087178410156, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4179600.0000, 
sim time next is 4180200.0000, 
raw observation next is [-3.666666666666667, 43.33333333333334, 102.6666666666667, 602.6666666666667, 26.0, 25.56428012016243, 0.4194576420024394, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3610341643582641, 0.4333333333333334, 0.3422222222222223, 0.6659300184162064, 0.6666666666666666, 0.6303566766802025, 0.6398192140008131, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.99616957], dtype=float32), 0.10735275]. 
=============================================
[2019-04-04 06:02:28,918] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7508020e-07 2.8635557e-03 5.3070514e-25 1.2665132e-23 2.1583243e-18
 9.9713624e-01 6.6441521e-18], sum to 1.0000
[2019-04-04 06:02:28,924] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4562
[2019-04-04 06:02:28,934] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.416666666666667, 71.83333333333333, 0.0, 0.0, 26.0, 25.69538879939773, 0.3896986529641162, 0.0, 1.0, 18722.69496472033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4327800.0000, 
sim time next is 4328400.0000, 
raw observation next is [4.333333333333334, 71.66666666666667, 0.0, 0.0, 26.0, 25.62183494624909, 0.3779462749864169, 0.0, 1.0, 49771.15717284328], 
processed observation next is [1.0, 0.08695652173913043, 0.58264081255771, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6351529121874243, 0.6259820916621389, 0.0, 1.0, 0.23700551034687278], 
reward next is 0.7630, 
noisyNet noise sample is [array([-0.23866892], dtype=float32), -0.61808527]. 
=============================================
[2019-04-04 06:02:38,023] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.8745862e-08 9.7958569e-04 1.5259809e-24 5.5657772e-23 2.3541167e-18
 9.9902034e-01 1.5535157e-17], sum to 1.0000
[2019-04-04 06:02:38,024] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8306
[2019-04-04 06:02:38,039] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.95, 73.0, 0.0, 0.0, 26.0, 25.38424704858716, 0.4362934300438765, 0.0, 1.0, 55222.35025986011], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4505400.0000, 
sim time next is 4506000.0000, 
raw observation next is [-0.9333333333333333, 73.0, 0.0, 0.0, 26.0, 25.3876384300566, 0.4350208674484882, 0.0, 1.0, 46943.53703002853], 
processed observation next is [1.0, 0.13043478260869565, 0.4367497691597415, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6156365358380501, 0.6450069558161627, 0.0, 1.0, 0.22354065252394537], 
reward next is 0.7765, 
noisyNet noise sample is [array([-0.9317941], dtype=float32), 0.26833367]. 
=============================================
[2019-04-04 06:02:38,052] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[84.72313 ]
 [84.512665]
 [84.458694]
 [84.767296]
 [84.886215]], R is [[84.78649902]
 [84.67567444]
 [84.67677307]
 [84.74066162]
 [84.70904541]].
[2019-04-04 06:02:39,302] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.8462623e-09 3.9613951e-05 6.7582129e-28 1.3450584e-25 1.0331655e-19
 9.9996042e-01 1.0393123e-19], sum to 1.0000
[2019-04-04 06:02:39,333] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7753
[2019-04-04 06:02:39,369] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.45, 41.83333333333333, 0.0, 0.0, 26.0, 24.99179039508628, 0.3087320543659625, 0.0, 1.0, 40389.46740864294], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4215000.0000, 
sim time next is 4215600.0000, 
raw observation next is [1.4, 42.0, 0.0, 0.0, 26.0, 25.01216768390398, 0.3057954714576439, 0.0, 1.0, 22697.05146244504], 
processed observation next is [0.0, 0.8260869565217391, 0.5013850415512465, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5843473069919982, 0.6019318238192146, 0.0, 1.0, 0.10808119744021448], 
reward next is 0.8919, 
noisyNet noise sample is [array([-0.17360677], dtype=float32), 0.22524533]. 
=============================================
[2019-04-04 06:02:41,575] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.52340573e-09 8.80394946e-05 3.14066076e-27 1.44458613e-25
 1.03399464e-19 9.99911904e-01 5.53318172e-20], sum to 1.0000
[2019-04-04 06:02:41,579] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6940
[2019-04-04 06:02:41,629] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 24.93675917607679, 0.4268003030874172, 0.0, 1.0, 55673.2522126031], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4564200.0000, 
sim time next is 4564800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 24.99676716708466, 0.4305990102992079, 1.0, 1.0, 18704.32238008078], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5830639305903883, 0.6435330034330693, 1.0, 1.0, 0.08906820180990847], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.6573681], dtype=float32), -0.43883735]. 
=============================================
[2019-04-04 06:02:42,049] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0182710e-08 8.7898289e-04 1.1242525e-27 2.6491921e-26 4.6878881e-20
 9.9912101e-01 6.9365531e-20], sum to 1.0000
[2019-04-04 06:02:42,055] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0231
[2019-04-04 06:02:42,084] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 42.66666666666667, 182.5, 163.8333333333333, 26.0, 25.02716760465783, 0.3631035125824879, 0.0, 1.0, 55388.89196850319], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4200000.0000, 
sim time next is 4200600.0000, 
raw observation next is [2.0, 43.33333333333334, 178.0, 238.6666666666666, 26.0, 24.9850211640513, 0.3733346704258734, 0.0, 1.0, 57382.85317868873], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.4333333333333334, 0.5933333333333334, 0.26372007366482497, 0.6666666666666666, 0.582085097004275, 0.6244448901419578, 0.0, 1.0, 0.2732516818032797], 
reward next is 0.7267, 
noisyNet noise sample is [array([0.4407744], dtype=float32), 0.63146025]. 
=============================================
[2019-04-04 06:02:42,981] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.5368229e-09 1.1069558e-04 1.9158077e-29 5.2812312e-27 1.9453763e-21
 9.9988925e-01 1.7282416e-21], sum to 1.0000
[2019-04-04 06:02:42,981] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9537
[2019-04-04 06:02:42,991] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 52.0, 131.3333333333333, 825.3333333333334, 26.0, 25.27470678730015, 0.4261424997506143, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4279200.0000, 
sim time next is 4279800.0000, 
raw observation next is [7.0, 52.0, 142.6666666666667, 803.6666666666667, 26.0, 25.33284199248751, 0.4311396017529792, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.47555555555555573, 0.8880294659300185, 0.6666666666666666, 0.6110701660406258, 0.6437132005843264, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1557941], dtype=float32), 1.3380876]. 
=============================================
[2019-04-04 06:02:46,863] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5840067e-07 1.0434947e-03 7.1052486e-26 4.5915247e-24 4.9354605e-19
 9.9895632e-01 6.1987245e-19], sum to 1.0000
[2019-04-04 06:02:46,866] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5786
[2019-04-04 06:02:46,880] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.35, 73.5, 0.0, 0.0, 26.0, 25.68870925129472, 0.4318398083413971, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4325400.0000, 
sim time next is 4326000.0000, 
raw observation next is [4.4, 73.0, 0.0, 0.0, 26.0, 25.78291038291016, 0.4231767541541291, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.5844875346260389, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6485758652425133, 0.6410589180513764, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3677629], dtype=float32), -0.39296937]. 
=============================================
[2019-04-04 06:02:46,892] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[93.14598]
 [93.90862]
 [95.06513]
 [95.77294]
 [96.58702]], R is [[92.37622833]
 [92.45246887]
 [92.43869781]
 [92.4223938 ]
 [92.32054901]].
[2019-04-04 06:02:47,188] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1099478e-07 2.3928894e-02 4.5455609e-24 1.7532530e-23 3.8221402e-18
 9.7607076e-01 1.7277498e-17], sum to 1.0000
[2019-04-04 06:02:47,192] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2121
[2019-04-04 06:02:47,205] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.983333333333333, 70.83333333333334, 0.0, 0.0, 26.0, 25.43695654526017, 0.4116240566067293, 0.0, 1.0, 41363.15489776771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4331400.0000, 
sim time next is 4332000.0000, 
raw observation next is [3.966666666666667, 70.66666666666667, 0.0, 0.0, 26.0, 25.62647526805373, 0.4134425980023054, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5724838411819021, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6355396056711443, 0.6378141993341018, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3587239], dtype=float32), 0.8440884]. 
=============================================
[2019-04-04 06:02:47,221] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[89.42906 ]
 [89.45538 ]
 [89.44588 ]
 [89.596054]
 [89.8087  ]], R is [[89.4360199 ]
 [89.34469604]
 [89.1697464 ]
 [88.97920227]
 [88.61810303]].
[2019-04-04 06:02:47,444] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.5879530e-10 6.5320644e-05 3.2177827e-31 2.5464688e-29 1.2621041e-22
 9.9993467e-01 7.1031352e-23], sum to 1.0000
[2019-04-04 06:02:47,445] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3862
[2019-04-04 06:02:47,474] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.0, 36.5, 39.0, 0.0, 26.0, 28.48019941047833, 0.9271985870182181, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4379400.0000, 
sim time next is 4380000.0000, 
raw observation next is [13.0, 37.0, 34.0, 0.0, 26.0, 28.52839739222477, 1.10288048901423, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.37, 0.11333333333333333, 0.0, 0.6666666666666666, 0.877366449352064, 0.8676268296714099, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.74489665], dtype=float32), -0.24348855]. 
=============================================
[2019-04-04 06:02:47,500] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[104.96928 ]
 [105.462715]
 [105.80198 ]
 [106.1289  ]
 [106.55562 ]], R is [[104.46965027]
 [104.42495728]
 [104.38070679]
 [104.3368988 ]
 [104.29353333]].
[2019-04-04 06:02:48,633] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.1057065e-11 3.9376668e-05 7.2647773e-32 3.3807221e-29 4.9068114e-23
 9.9996066e-01 1.4411637e-22], sum to 1.0000
[2019-04-04 06:02:48,633] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6860
[2019-04-04 06:02:48,653] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.1, 31.66666666666666, 179.6666666666667, 524.1666666666667, 26.0, 28.3124380266884, 1.111063035584563, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4372800.0000, 
sim time next is 4373400.0000, 
raw observation next is [14.0, 31.83333333333333, 164.3333333333334, 419.3333333333334, 26.0, 27.58781309516073, 1.054755889184538, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8504155124653741, 0.3183333333333333, 0.547777777777778, 0.46335174953959496, 0.6666666666666666, 0.7989844245967275, 0.851585296394846, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0760566], dtype=float32), -0.47791925]. 
=============================================
[2019-04-04 06:02:50,543] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5867809e-09 4.4001569e-04 1.3830145e-25 3.0759138e-23 2.4581457e-18
 9.9955994e-01 1.5188925e-17], sum to 1.0000
[2019-04-04 06:02:50,557] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3305
[2019-04-04 06:02:50,573] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.533333333333333, 82.66666666666667, 127.5, 198.5, 26.0, 25.88765805281909, 0.5788032175666117, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4437600.0000, 
sim time next is 4438200.0000, 
raw observation next is [1.416666666666667, 83.33333333333333, 135.0, 165.0, 26.0, 26.03909742913084, 0.5998075403914693, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5018467220683288, 0.8333333333333333, 0.45, 0.18232044198895028, 0.6666666666666666, 0.6699247857609034, 0.6999358467971565, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2398651], dtype=float32), 0.59745103]. 
=============================================
[2019-04-04 06:02:58,332] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3148110e-09 3.0550556e-05 5.2891732e-28 5.0575664e-27 5.6837466e-21
 9.9996948e-01 1.9992750e-20], sum to 1.0000
[2019-04-04 06:02:58,336] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1471
[2019-04-04 06:02:58,356] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.39746454507859, 0.4757652707387041, 0.0, 1.0, 25321.10020927353], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4576800.0000, 
sim time next is 4577400.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.41165188907458, 0.4729090772899314, 0.0, 1.0, 22932.08546882027], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6176376574228817, 0.6576363590966438, 0.0, 1.0, 0.10920040699438224], 
reward next is 0.8908, 
noisyNet noise sample is [array([1.2043982], dtype=float32), -0.29804027]. 
=============================================
[2019-04-04 06:03:03,087] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3619362e-08 4.5693095e-04 3.5406801e-27 7.2571413e-26 4.5815773e-20
 9.9954307e-01 1.6456089e-19], sum to 1.0000
[2019-04-04 06:03:03,087] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4807
[2019-04-04 06:03:03,099] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 26.0, 25.45786265705645, 0.5258498825239083, 0.0, 1.0, 105702.5153670901], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4661400.0000, 
sim time next is 4662000.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 26.0, 25.4570173370297, 0.5356389323666113, 0.0, 1.0, 63254.91855720848], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6214181114191417, 0.6785463107888704, 0.0, 1.0, 0.30121389789146896], 
reward next is 0.6988, 
noisyNet noise sample is [array([-0.5483318], dtype=float32), 1.1843374]. 
=============================================
[2019-04-04 06:03:03,114] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[98.5126  ]
 [98.650314]
 [98.11733 ]
 [97.56383 ]
 [97.87341 ]], R is [[98.06575012]
 [97.58174896]
 [96.97636414]
 [96.62506866]
 [96.65882111]].
[2019-04-04 06:03:11,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:11,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:11,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run6
[2019-04-04 06:03:13,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:13,074] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:13,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run6
[2019-04-04 06:03:14,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:14,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:14,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run6
[2019-04-04 06:03:18,719] A3C_AGENT_WORKER-Thread-2 INFO:Local step 42500, global step 675804: loss 139.5438
[2019-04-04 06:03:18,720] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 42500, global step 675804: learning rate 0.0000
[2019-04-04 06:03:19,983] A3C_AGENT_WORKER-Thread-16 INFO:Local step 42500, global step 676364: loss 139.9810
[2019-04-04 06:03:19,985] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 42500, global step 676365: learning rate 0.0000
[2019-04-04 06:03:21,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:21,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:21,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run6
[2019-04-04 06:03:22,005] A3C_AGENT_WORKER-Thread-3 INFO:Local step 42500, global step 677138: loss 135.4609
[2019-04-04 06:03:22,006] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 42500, global step 677138: learning rate 0.0000
[2019-04-04 06:03:24,908] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:24,908] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:24,922] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run6
[2019-04-04 06:03:27,374] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:27,375] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:27,387] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run6
[2019-04-04 06:03:27,869] A3C_AGENT_WORKER-Thread-4 INFO:Local step 42500, global step 679446: loss 139.7330
[2019-04-04 06:03:27,897] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 42500, global step 679446: learning rate 0.0000
[2019-04-04 06:03:28,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:28,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:28,642] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run6
[2019-04-04 06:03:29,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:29,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:29,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run6
[2019-04-04 06:03:29,676] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:29,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:29,680] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run6
[2019-04-04 06:03:30,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:30,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:30,030] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run6
[2019-04-04 06:03:30,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:30,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:30,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run6
[2019-04-04 06:03:30,607] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:30,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:30,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run6
[2019-04-04 06:03:31,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:31,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:31,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run6
[2019-04-04 06:03:31,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:31,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:31,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run6
[2019-04-04 06:03:32,090] A3C_AGENT_WORKER-Thread-18 INFO:Local step 42500, global step 680456: loss 137.8092
[2019-04-04 06:03:32,090] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 42500, global step 680456: learning rate 0.0000
[2019-04-04 06:03:33,474] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:33,474] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:33,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run6
[2019-04-04 06:03:34,668] A3C_AGENT_WORKER-Thread-6 INFO:Local step 42500, global step 680796: loss 138.8933
[2019-04-04 06:03:34,669] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 42500, global step 680796: learning rate 0.0000
[2019-04-04 06:03:34,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:03:34,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:03:34,812] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run6
[2019-04-04 06:03:36,079] A3C_AGENT_WORKER-Thread-15 INFO:Local step 42500, global step 680960: loss 140.7046
[2019-04-04 06:03:36,079] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 42500, global step 680960: learning rate 0.0000
[2019-04-04 06:03:37,380] A3C_AGENT_WORKER-Thread-11 INFO:Local step 42500, global step 681130: loss 136.9005
[2019-04-04 06:03:37,399] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 42500, global step 681130: learning rate 0.0000
[2019-04-04 06:03:39,060] A3C_AGENT_WORKER-Thread-14 INFO:Local step 42500, global step 681384: loss 141.0898
[2019-04-04 06:03:39,060] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 42500, global step 681384: learning rate 0.0000
[2019-04-04 06:03:39,092] A3C_AGENT_WORKER-Thread-12 INFO:Local step 42500, global step 681391: loss 140.6691
[2019-04-04 06:03:39,096] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 42500, global step 681391: learning rate 0.0000
[2019-04-04 06:03:39,200] A3C_AGENT_WORKER-Thread-5 INFO:Local step 42500, global step 681417: loss 138.0012
[2019-04-04 06:03:39,201] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 42500, global step 681417: learning rate 0.0000
[2019-04-04 06:03:39,279] A3C_AGENT_WORKER-Thread-17 INFO:Local step 42500, global step 681434: loss 143.6117
[2019-04-04 06:03:39,282] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 42500, global step 681434: learning rate 0.0000
[2019-04-04 06:03:40,197] A3C_AGENT_WORKER-Thread-10 INFO:Local step 42500, global step 681684: loss 136.4314
[2019-04-04 06:03:40,211] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 42500, global step 681684: learning rate 0.0000
[2019-04-04 06:03:41,078] A3C_AGENT_WORKER-Thread-20 INFO:Local step 42500, global step 681867: loss 145.8821
[2019-04-04 06:03:41,097] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 42500, global step 681867: learning rate 0.0000
[2019-04-04 06:03:42,356] A3C_AGENT_WORKER-Thread-19 INFO:Local step 42500, global step 682159: loss 136.0200
[2019-04-04 06:03:42,357] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 42500, global step 682159: learning rate 0.0000
[2019-04-04 06:03:43,442] A3C_AGENT_WORKER-Thread-13 INFO:Local step 42500, global step 682494: loss 136.5246
[2019-04-04 06:03:43,458] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 42500, global step 682494: learning rate 0.0000
[2019-04-04 06:03:45,858] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.1048649e-09 1.3794976e-04 4.0082731e-26 1.6259219e-24 2.8366688e-19
 9.9986207e-01 6.1783858e-20], sum to 1.0000
[2019-04-04 06:03:45,859] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8321
[2019-04-04 06:03:45,906] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 50.83333333333334, 98.33333333333333, 691.3333333333334, 26.0, 25.71785031513357, 0.4202671332625749, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 301800.0000, 
sim time next is 302400.0000, 
raw observation next is [-10.6, 49.0, 94.5, 708.0, 26.0, 25.91235581990787, 0.4385661230046006, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.1689750692520776, 0.49, 0.315, 0.7823204419889502, 0.6666666666666666, 0.6593629849923225, 0.6461887076682001, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6753995], dtype=float32), -0.767494]. 
=============================================
[2019-04-04 06:03:47,100] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43000, global step 683600: loss 0.0255
[2019-04-04 06:03:47,104] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43000, global step 683600: learning rate 0.0000
[2019-04-04 06:03:47,513] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43000, global step 683726: loss 0.0218
[2019-04-04 06:03:47,513] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43000, global step 683726: learning rate 0.0000
[2019-04-04 06:03:50,254] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43000, global step 684456: loss 0.0165
[2019-04-04 06:03:50,256] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43000, global step 684456: learning rate 0.0000
[2019-04-04 06:03:56,584] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43000, global step 686232: loss 0.0147
[2019-04-04 06:03:56,585] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43000, global step 686232: learning rate 0.0000
[2019-04-04 06:04:01,720] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43000, global step 687630: loss 0.0192
[2019-04-04 06:04:01,721] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43000, global step 687630: learning rate 0.0000
[2019-04-04 06:04:03,288] A3C_AGENT_WORKER-Thread-6 INFO:Local step 43000, global step 688062: loss 0.0165
[2019-04-04 06:04:03,289] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 43000, global step 688062: learning rate 0.0000
[2019-04-04 06:04:04,647] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3324798e-08 1.9157187e-03 2.9161653e-27 6.3662421e-25 2.4169504e-19
 9.9808431e-01 2.9517568e-19], sum to 1.0000
[2019-04-04 06:04:04,647] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5714
[2019-04-04 06:04:04,666] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 76.16666666666667, 0.0, 0.0, 26.0, 24.49144451185372, 0.1662303540641819, 0.0, 1.0, 44191.81359928646], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 252600.0000, 
sim time next is 253200.0000, 
raw observation next is [-3.9, 77.33333333333334, 0.0, 0.0, 26.0, 24.46010652278052, 0.1589956193795719, 0.0, 1.0, 44179.52732662379], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.53834221023171, 0.5529985397931906, 0.0, 1.0, 0.2103787015553514], 
reward next is 0.7896, 
noisyNet noise sample is [array([0.30954435], dtype=float32), -1.7653644]. 
=============================================
[2019-04-04 06:04:04,873] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43000, global step 688505: loss 0.0155
[2019-04-04 06:04:04,874] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43000, global step 688505: learning rate 0.0000
[2019-04-04 06:04:05,316] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43000, global step 688625: loss 0.0159
[2019-04-04 06:04:05,316] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43000, global step 688625: learning rate 0.0000
[2019-04-04 06:04:07,088] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43000, global step 689174: loss 0.0353
[2019-04-04 06:04:07,088] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43000, global step 689174: learning rate 0.0000
[2019-04-04 06:04:07,315] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43000, global step 689254: loss 0.0161
[2019-04-04 06:04:07,316] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43000, global step 689254: learning rate 0.0000
[2019-04-04 06:04:07,469] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43000, global step 689299: loss 0.0159
[2019-04-04 06:04:07,470] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43000, global step 689299: learning rate 0.0000
[2019-04-04 06:04:08,206] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43000, global step 689512: loss 0.0152
[2019-04-04 06:04:08,210] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43000, global step 689513: learning rate 0.0000
[2019-04-04 06:04:08,483] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43000, global step 689593: loss 0.0190
[2019-04-04 06:04:08,484] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43000, global step 689593: learning rate 0.0000
[2019-04-04 06:04:09,564] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43000, global step 689902: loss 0.0184
[2019-04-04 06:04:09,564] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43000, global step 689902: learning rate 0.0000
[2019-04-04 06:04:10,592] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43000, global step 690205: loss 0.0343
[2019-04-04 06:04:10,592] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43000, global step 690205: learning rate 0.0000
[2019-04-04 06:04:11,078] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0935428e-09 2.5243948e-05 6.2602969e-27 3.1053588e-25 1.2594488e-19
 9.9997473e-01 1.3763598e-19], sum to 1.0000
[2019-04-04 06:04:11,078] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9122
[2019-04-04 06:04:11,122] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 42.0, 62.33333333333334, 437.5, 26.0, 26.44795444739054, 0.5527348883685553, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 315600.0000, 
sim time next is 316200.0000, 
raw observation next is [-9.5, 42.0, 54.66666666666667, 398.0000000000001, 26.0, 26.48664429258381, 0.5467998185575195, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.18222222222222223, 0.439779005524862, 0.6666666666666666, 0.7072203577153177, 0.6822666061858399, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48813567], dtype=float32), 0.17770688]. 
=============================================
[2019-04-04 06:04:11,597] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43000, global step 690432: loss 0.0161
[2019-04-04 06:04:11,633] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43000, global step 690432: learning rate 0.0000
[2019-04-04 06:04:14,040] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43500, global step 690987: loss 8.5464
[2019-04-04 06:04:14,065] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43500, global step 690991: learning rate 0.0000
[2019-04-04 06:04:15,882] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1000877e-06 2.0703042e-03 5.1743123e-20 1.2071511e-18 1.6688724e-14
 9.9792862e-01 6.7796754e-14], sum to 1.0000
[2019-04-04 06:04:15,883] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1080
[2019-04-04 06:04:16,029] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.2, 42.0, 0.0, 0.0, 26.0, 22.53868907378236, -0.2640888471119909, 1.0, 1.0, 202324.9148727898], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 458400.0000, 
sim time next is 459000.0000, 
raw observation next is [-8.1, 41.5, 0.0, 0.0, 26.0, 22.87396089058539, -0.1718418274959656, 1.0, 1.0, 203487.4415189996], 
processed observation next is [1.0, 0.30434782608695654, 0.23822714681440446, 0.415, 0.0, 0.0, 0.6666666666666666, 0.40616340754878255, 0.4427193908346781, 1.0, 1.0, 0.968987816757141], 
reward next is 0.0310, 
noisyNet noise sample is [array([-0.24790297], dtype=float32), 1.4420047]. 
=============================================
[2019-04-04 06:04:16,039] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[66.93776 ]
 [67.3025  ]
 [67.257996]
 [67.18605 ]
 [67.18026 ]], R is [[66.17633057]
 [65.55110931]
 [65.67643738]
 [65.80019379]
 [65.92266846]].
[2019-04-04 06:04:16,642] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43500, global step 691556: loss 7.7799
[2019-04-04 06:04:16,649] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43500, global step 691556: learning rate 0.0000
[2019-04-04 06:04:20,448] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43500, global step 692374: loss 8.4975
[2019-04-04 06:04:20,448] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43500, global step 692374: learning rate 0.0000
[2019-04-04 06:04:27,572] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43500, global step 694086: loss 8.2318
[2019-04-04 06:04:27,574] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43500, global step 694086: learning rate 0.0000
[2019-04-04 06:04:32,911] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5664634e-08 7.5604752e-05 5.5408160e-26 1.3125961e-24 6.8889999e-20
 9.9992442e-01 8.1109779e-19], sum to 1.0000
[2019-04-04 06:04:32,912] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6564
[2019-04-04 06:04:33,007] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 54.0, 82.0, 47.0, 26.0, 24.88113382376491, 0.2292650888588934, 0.0, 1.0, 35040.68037169329], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 658800.0000, 
sim time next is 659400.0000, 
raw observation next is [-0.6, 54.0, 82.33333333333334, 44.0, 26.0, 24.90535607594844, 0.2280270872937427, 0.0, 1.0, 24221.91279236272], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.2744444444444445, 0.04861878453038674, 0.6666666666666666, 0.57544633966237, 0.5760090290979142, 0.0, 1.0, 0.1153424418683939], 
reward next is 0.8847, 
noisyNet noise sample is [array([0.21606503], dtype=float32), -0.06748056]. 
=============================================
[2019-04-04 06:04:33,175] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43500, global step 695410: loss 8.2923
[2019-04-04 06:04:33,176] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43500, global step 695410: learning rate 0.0000
[2019-04-04 06:04:35,171] A3C_AGENT_WORKER-Thread-6 INFO:Local step 43500, global step 695836: loss 8.0815
[2019-04-04 06:04:35,172] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 43500, global step 695836: learning rate 0.0000
[2019-04-04 06:04:38,266] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43500, global step 696532: loss 8.0049
[2019-04-04 06:04:38,267] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43500, global step 696532: learning rate 0.0000
[2019-04-04 06:04:39,284] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43500, global step 696774: loss 8.6596
[2019-04-04 06:04:39,284] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43500, global step 696774: learning rate 0.0000
[2019-04-04 06:04:41,804] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43500, global step 697377: loss 7.7557
[2019-04-04 06:04:41,811] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43500, global step 697378: learning rate 0.0000
[2019-04-04 06:04:41,881] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43500, global step 697395: loss 7.9650
[2019-04-04 06:04:41,897] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43500, global step 697404: learning rate 0.0000
[2019-04-04 06:04:42,014] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43500, global step 697427: loss 8.3921
[2019-04-04 06:04:42,014] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43500, global step 697427: learning rate 0.0000
[2019-04-04 06:04:42,385] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43500, global step 697515: loss 8.7942
[2019-04-04 06:04:42,385] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43500, global step 697515: learning rate 0.0000
[2019-04-04 06:04:42,832] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43500, global step 697632: loss 8.1563
[2019-04-04 06:04:42,834] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43500, global step 697632: learning rate 0.0000
[2019-04-04 06:04:45,018] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43500, global step 698194: loss 8.0571
[2019-04-04 06:04:45,034] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43500, global step 698194: learning rate 0.0000
[2019-04-04 06:04:45,638] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43500, global step 698323: loss 7.9035
[2019-04-04 06:04:45,638] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43500, global step 698323: learning rate 0.0000
[2019-04-04 06:04:46,731] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43500, global step 698570: loss 8.2881
[2019-04-04 06:04:46,732] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43500, global step 698570: learning rate 0.0000
[2019-04-04 06:04:47,981] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44000, global step 698881: loss 1.9523
[2019-04-04 06:04:47,987] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44000, global step 698883: learning rate 0.0000
[2019-04-04 06:04:49,437] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44000, global step 699208: loss 1.9781
[2019-04-04 06:04:49,441] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44000, global step 699208: learning rate 0.0000
[2019-04-04 06:04:52,196] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.35101030e-09 1.33764464e-04 2.09694843e-28 6.84133226e-27
 1.19126925e-20 9.99866247e-01 6.42661175e-21], sum to 1.0000
[2019-04-04 06:04:52,196] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6377
[2019-04-04 06:04:52,217] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333334, 45.66666666666667, 81.5, 723.8333333333333, 26.0, 25.8369665905087, 0.4438218008294426, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 742800.0000, 
sim time next is 743400.0000, 
raw observation next is [0.25, 46.0, 80.0, 714.0, 26.0, 25.89073588865145, 0.4519577484267781, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46952908587257625, 0.46, 0.26666666666666666, 0.7889502762430939, 0.6666666666666666, 0.6575613240542874, 0.650652582808926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12429218], dtype=float32), 0.44515136]. 
=============================================
[2019-04-04 06:04:52,678] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 06:04:52,681] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:04:52,683] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:04:52,683] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:04:52,684] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:04:52,684] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:04:52,685] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:04:52,695] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run8
[2019-04-04 06:04:52,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run8
[2019-04-04 06:04:52,745] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run8
[2019-04-04 06:06:28,413] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23864964], dtype=float32), 0.21247216]
[2019-04-04 06:06:28,413] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.8, 71.0, 0.0, 0.0, 26.0, 24.29544593278925, 0.1616419625924318, 0.0, 1.0, 43556.17266288694]
[2019-04-04 06:06:28,413] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:06:28,414] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.4808932e-07 8.6862530e-04 2.1344023e-23 1.4390079e-21 6.2547381e-17
 9.9913114e-01 2.0061482e-16], sampled 0.17365945153274642
[2019-04-04 06:06:40,320] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23864964], dtype=float32), 0.21247216]
[2019-04-04 06:06:40,320] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.733333333333333, 70.33333333333333, 193.0, 172.6666666666667, 26.0, 25.07028636449231, 0.2588235700273016, 1.0, 1.0, 66824.73768910089]
[2019-04-04 06:06:40,320] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:06:40,321] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.4094801e-08 2.5742454e-04 6.3124398e-25 3.3459482e-23 5.4798246e-18
 9.9974257e-01 7.4428661e-18], sampled 0.20394594851073233
[2019-04-04 06:06:54,897] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.23864964], dtype=float32), 0.21247216]
[2019-04-04 06:06:54,898] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-7.466666666666667, 53.66666666666667, 0.0, 0.0, 26.0, 24.02159513305618, 0.02720677270596002, 0.0, 1.0, 43605.61144062008]
[2019-04-04 06:06:54,898] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 06:06:54,899] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.5862951e-06 1.7931053e-03 7.6210315e-21 3.2398527e-19 3.9326016e-15
 9.9820530e-01 9.5521278e-15], sampled 0.11559112748770972
[2019-04-04 06:07:23,586] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.23864964], dtype=float32), 0.21247216]
[2019-04-04 06:07:23,586] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.0110600196666667, 78.99755470666668, 0.0, 0.0, 26.0, 24.84537138666552, 0.3452757113649023, 0.0, 1.0, 80016.06304915548]
[2019-04-04 06:07:23,586] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 06:07:23,587] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.3056337e-09 5.5148510e-05 1.2095745e-28 1.7203150e-26 5.6046975e-21
 9.9994481e-01 2.4582173e-20], sampled 0.20868274084197325
[2019-04-04 06:07:34,615] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.23864964], dtype=float32), 0.21247216]
[2019-04-04 06:07:34,615] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.986520274, 100.0, 0.0, 0.0, 26.0, 25.288070802528, 0.5579489245276187, 0.0, 1.0, 197179.0215735397]
[2019-04-04 06:07:34,615] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 06:07:34,616] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [9.8905728e-10 3.3103894e-05 1.5780372e-30 3.7950322e-28 2.8152280e-22
 9.9996686e-01 8.6623885e-22], sampled 0.8613597241690886
[2019-04-04 06:07:43,860] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23864964], dtype=float32), 0.21247216]
[2019-04-04 06:07:43,860] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-5.6, 76.0, 0.0, 0.0, 26.0, 24.90093543199729, 0.3094137982352905, 0.0, 1.0, 38818.53785200914]
[2019-04-04 06:07:43,860] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:07:43,861] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.3415546e-07 7.2933274e-04 5.8280906e-24 3.9900330e-22 2.7493947e-17
 9.9927038e-01 7.0533069e-17], sampled 0.3711248104662638
[2019-04-04 06:07:53,117] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23864964], dtype=float32), 0.21247216]
[2019-04-04 06:07:53,117] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [17.2, 42.0, 0.0, 0.0, 26.0, 28.30306366650414, 1.215334508200715, 0.0, 0.0, 0.0]
[2019-04-04 06:07:53,117] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:07:53,118] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.5398995e-08 1.3345359e-04 4.9868381e-26 2.8718205e-24 6.1290728e-19
 9.9986649e-01 1.5336065e-18], sampled 0.5898166180557702
[2019-04-04 06:08:04,920] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.1953 239988981.1793 1605.0358
[2019-04-04 06:08:33,700] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.4347 263458723.1113 1557.0294
[2019-04-04 06:08:36,381] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4931 275836454.3630 1233.0070
[2019-04-04 06:08:37,413] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 700000, evaluation results [700000.0, 7241.434651850746, 263458723.11134383, 1557.0294065928176, 7353.19532771786, 239988981.17925218, 1605.0357703192992, 7182.493074461691, 275836454.3630484, 1233.0069646789339]
[2019-04-04 06:08:38,797] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44000, global step 700411: loss 1.9801
[2019-04-04 06:08:38,808] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44000, global step 700411: learning rate 0.0000
[2019-04-04 06:08:39,483] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.93654125e-07 2.61221605e-04 2.41758139e-24 1.02126286e-22
 3.43087552e-17 9.99738514e-01 1.14436103e-17], sum to 1.0000
[2019-04-04 06:08:39,483] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5006
[2019-04-04 06:08:39,547] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 73.0, 0.0, 0.0, 26.0, 24.4345397412308, 0.08821284161070825, 0.0, 1.0, 41006.07513246287], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 696000.0000, 
sim time next is 696600.0000, 
raw observation next is [-3.4, 73.5, 0.0, 0.0, 26.0, 24.40081073142991, 0.08533755253983083, 0.0, 1.0, 41048.56145559185], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.735, 0.0, 0.0, 0.6666666666666666, 0.5334008942858258, 0.5284458508466102, 0.0, 1.0, 0.1954693402647231], 
reward next is 0.8045, 
noisyNet noise sample is [array([0.47781727], dtype=float32), 0.6132367]. 
=============================================
[2019-04-04 06:08:44,690] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44000, global step 701971: loss 1.8475
[2019-04-04 06:08:44,712] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44000, global step 701971: learning rate 0.0000
[2019-04-04 06:08:50,227] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.6565471e-08 1.8445097e-04 7.7206624e-26 1.3403520e-24 4.8487638e-20
 9.9981552e-01 1.8376771e-18], sum to 1.0000
[2019-04-04 06:08:50,227] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2187
[2019-04-04 06:08:50,237] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.2, 59.0, 0.0, 0.0, 26.0, 25.91248564560666, 0.6797838021601533, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1107600.0000, 
sim time next is 1108200.0000, 
raw observation next is [14.0, 59.5, 0.0, 0.0, 26.0, 25.83986129189341, 0.6691590216417237, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8504155124653741, 0.595, 0.0, 0.0, 0.6666666666666666, 0.6533217743244508, 0.7230530072139079, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12477633], dtype=float32), -0.23407747]. 
=============================================
[2019-04-04 06:08:53,314] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8597746e-09 2.4325793e-05 7.8234464e-27 2.3479340e-25 2.2239022e-19
 9.9997568e-01 5.3465145e-20], sum to 1.0000
[2019-04-04 06:08:53,314] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3270
[2019-04-04 06:08:53,378] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 84.66666666666666, 0.0, 0.0, 26.0, 24.80948302211409, 0.3005150624284587, 1.0, 1.0, 153271.1042121678], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 844800.0000, 
sim time next is 845400.0000, 
raw observation next is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 24.93457749558057, 0.3115382998230027, 1.0, 1.0, 24846.08992576134], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5778814579650474, 0.6038460999410009, 1.0, 1.0, 0.11831471393219685], 
reward next is 0.8817, 
noisyNet noise sample is [array([-0.07188825], dtype=float32), 0.77040213]. 
=============================================
[2019-04-04 06:08:53,412] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44000, global step 703907: loss 1.9647
[2019-04-04 06:08:53,414] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44000, global step 703908: learning rate 0.0000
[2019-04-04 06:08:53,715] A3C_AGENT_WORKER-Thread-6 INFO:Local step 44000, global step 703997: loss 1.8924
[2019-04-04 06:08:53,717] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 44000, global step 703999: learning rate 0.0000
[2019-04-04 06:08:55,841] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44000, global step 704677: loss 1.8927
[2019-04-04 06:08:55,844] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44000, global step 704677: learning rate 0.0000
[2019-04-04 06:08:56,117] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44000, global step 704802: loss 1.9200
[2019-04-04 06:08:56,132] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44000, global step 704805: learning rate 0.0000
[2019-04-04 06:08:57,113] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44500, global step 705221: loss 8.9996
[2019-04-04 06:08:57,114] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44500, global step 705221: learning rate 0.0000
[2019-04-04 06:08:57,733] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44500, global step 705468: loss 8.9307
[2019-04-04 06:08:57,736] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44500, global step 705469: learning rate 0.0000
[2019-04-04 06:08:58,128] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44000, global step 705649: loss 1.9018
[2019-04-04 06:08:58,130] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44000, global step 705649: learning rate 0.0000
[2019-04-04 06:08:58,207] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44000, global step 705683: loss 1.8918
[2019-04-04 06:08:58,208] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44000, global step 705683: learning rate 0.0000
[2019-04-04 06:08:58,880] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44000, global step 706013: loss 1.8638
[2019-04-04 06:08:58,882] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44000, global step 706013: learning rate 0.0000
[2019-04-04 06:08:58,929] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44000, global step 706035: loss 1.8458
[2019-04-04 06:08:58,931] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44000, global step 706036: learning rate 0.0000
[2019-04-04 06:08:59,070] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44000, global step 706114: loss 1.8665
[2019-04-04 06:08:59,072] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44000, global step 706115: learning rate 0.0000
[2019-04-04 06:09:00,565] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6517493e-09 1.4398300e-05 3.8050436e-30 1.8793573e-28 1.5999437e-21
 9.9998558e-01 1.6133168e-21], sum to 1.0000
[2019-04-04 06:09:00,565] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4333
[2019-04-04 06:09:00,581] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 98.66666666666667, 0.0, 0.0, 26.0, 25.09237854473398, 0.3862772984570366, 0.0, 1.0, 40655.08494792281], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 940800.0000, 
sim time next is 941400.0000, 
raw observation next is [5.0, 98.0, 0.0, 0.0, 26.0, 25.12011067460802, 0.3900545262635955, 0.0, 1.0, 40376.59048629083], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.98, 0.0, 0.0, 0.6666666666666666, 0.5933425562173351, 0.6300181754211985, 0.0, 1.0, 0.1922694785061468], 
reward next is 0.8077, 
noisyNet noise sample is [array([-1.8695179], dtype=float32), -0.9230731]. 
=============================================
[2019-04-04 06:09:00,583] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44500, global step 706801: loss 8.7160
[2019-04-04 06:09:00,584] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44500, global step 706802: learning rate 0.0000
[2019-04-04 06:09:00,737] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44000, global step 706868: loss 1.7820
[2019-04-04 06:09:00,737] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44000, global step 706868: learning rate 0.0000
[2019-04-04 06:09:01,221] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44000, global step 707096: loss 1.9059
[2019-04-04 06:09:01,223] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44000, global step 707097: learning rate 0.0000
[2019-04-04 06:09:01,579] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44000, global step 707291: loss 1.8256
[2019-04-04 06:09:01,580] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44000, global step 707291: learning rate 0.0000
[2019-04-04 06:09:03,852] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44500, global step 708475: loss 9.1099
[2019-04-04 06:09:03,855] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44500, global step 708475: learning rate 0.0000
[2019-04-04 06:09:04,827] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.5700744e-09 5.3483676e-05 9.0334675e-28 4.2573889e-26 1.2998028e-19
 9.9994648e-01 1.2181486e-19], sum to 1.0000
[2019-04-04 06:09:04,834] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5852
[2019-04-04 06:09:04,848] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.08333333333333, 78.66666666666667, 0.0, 0.0, 26.0, 25.65754393261718, 0.6333576381643966, 0.0, 1.0, 24871.49275255252], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1129800.0000, 
sim time next is 1130400.0000, 
raw observation next is [10.0, 79.0, 0.0, 0.0, 26.0, 25.64733421324107, 0.631170094676709, 0.0, 1.0, 29608.10622584632], 
processed observation next is [0.0, 0.08695652173913043, 0.739612188365651, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6372778511034225, 0.710390031558903, 0.0, 1.0, 0.14099098202783963], 
reward next is 0.8590, 
noisyNet noise sample is [array([0.37271863], dtype=float32), 0.60383356]. 
=============================================
[2019-04-04 06:09:05,400] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.61631275e-09 2.22641043e-04 1.00721344e-26 2.39075795e-24
 6.85140777e-19 9.99777377e-01 1.20173898e-19], sum to 1.0000
[2019-04-04 06:09:05,406] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4214
[2019-04-04 06:09:05,423] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.91666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.6385988287335, 0.6292653591446592, 0.0, 1.0, 21432.21110848697], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1133400.0000, 
sim time next is 1134000.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.6437720731439, 0.6282781818989502, 0.0, 1.0, 20036.27574748692], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.6666666666666666, 0.636981006095325, 0.7094260606329833, 0.0, 1.0, 0.09541083689279486], 
reward next is 0.9046, 
noisyNet noise sample is [array([-1.8088005], dtype=float32), -0.038978245]. 
=============================================
[2019-04-04 06:09:05,441] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[92.423615]
 [92.73086 ]
 [92.71167 ]
 [92.52797 ]
 [93.00933 ]], R is [[92.40848541]
 [92.38233948]
 [92.34344482]
 [92.28736115]
 [92.21642303]].
[2019-04-04 06:09:07,864] A3C_AGENT_WORKER-Thread-6 INFO:Local step 44500, global step 710875: loss 8.2708
[2019-04-04 06:09:07,866] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 44500, global step 710876: learning rate 0.0000
[2019-04-04 06:09:08,136] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44500, global step 711043: loss 8.4493
[2019-04-04 06:09:08,139] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44500, global step 711043: learning rate 0.0000
[2019-04-04 06:09:09,649] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44500, global step 711938: loss 8.5794
[2019-04-04 06:09:09,650] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44500, global step 711938: learning rate 0.0000
[2019-04-04 06:09:10,467] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44500, global step 712409: loss 9.2576
[2019-04-04 06:09:10,469] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44500, global step 712410: learning rate 0.0000
[2019-04-04 06:09:11,932] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44500, global step 713177: loss 8.6653
[2019-04-04 06:09:11,934] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44500, global step 713178: learning rate 0.0000
[2019-04-04 06:09:12,232] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0699336e-09 2.9579200e-05 8.5993013e-30 1.4156518e-27 1.9299081e-21
 9.9997044e-01 1.6104462e-20], sum to 1.0000
[2019-04-04 06:09:12,235] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0486
[2019-04-04 06:09:12,251] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.38954215477984, 0.484307577684104, 0.0, 1.0, 30946.39543961612], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1468200.0000, 
sim time next is 1468800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.36930350294399, 0.4808580390386901, 0.0, 1.0, 43466.02257089813], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6141086252453324, 0.6602860130128967, 0.0, 1.0, 0.20698105986141968], 
reward next is 0.7930, 
noisyNet noise sample is [array([-1.375891], dtype=float32), 0.35444972]. 
=============================================
[2019-04-04 06:09:12,753] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44500, global step 713648: loss 8.3922
[2019-04-04 06:09:12,763] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44500, global step 713649: learning rate 0.0000
[2019-04-04 06:09:13,123] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44500, global step 713869: loss 7.8542
[2019-04-04 06:09:13,124] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44500, global step 713869: learning rate 0.0000
[2019-04-04 06:09:13,333] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44500, global step 713988: loss 9.0952
[2019-04-04 06:09:13,336] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44500, global step 713988: learning rate 0.0000
[2019-04-04 06:09:13,985] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44500, global step 714334: loss 8.6465
[2019-04-04 06:09:13,991] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44500, global step 714334: learning rate 0.0000
[2019-04-04 06:09:14,619] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45000, global step 714671: loss 9.8862
[2019-04-04 06:09:14,620] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45000, global step 714671: learning rate 0.0000
[2019-04-04 06:09:14,807] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45000, global step 714780: loss 9.9520
[2019-04-04 06:09:14,809] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45000, global step 714781: learning rate 0.0000
[2019-04-04 06:09:14,911] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44500, global step 714839: loss 8.8961
[2019-04-04 06:09:14,914] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44500, global step 714839: learning rate 0.0000
[2019-04-04 06:09:15,329] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44500, global step 715096: loss 8.6198
[2019-04-04 06:09:15,333] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44500, global step 715096: learning rate 0.0000
[2019-04-04 06:09:15,343] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.8670790e-11 1.4320819e-05 1.9419674e-29 8.9022899e-27 5.3190586e-22
 9.9998569e-01 3.7364504e-21], sum to 1.0000
[2019-04-04 06:09:15,343] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7287
[2019-04-04 06:09:15,353] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 62.0, 0.0, 26.0, 25.81545261671846, 0.520789084358961, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1349400.0000, 
sim time next is 1350000.0000, 
raw observation next is [1.1, 92.0, 57.5, 0.0, 26.0, 25.74016726660579, 0.5141834657957186, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.19166666666666668, 0.0, 0.6666666666666666, 0.645013938883816, 0.6713944885985729, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.62088317], dtype=float32), 0.031084316]. 
=============================================
[2019-04-04 06:09:15,364] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44500, global step 715109: loss 8.9385
[2019-04-04 06:09:15,367] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44500, global step 715109: learning rate 0.0000
[2019-04-04 06:09:15,382] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[96.434235]
 [96.5097  ]
 [96.59369 ]
 [96.65648 ]
 [96.64268 ]], R is [[96.40843201]
 [96.4443512 ]
 [96.4799118 ]
 [96.51511383]
 [96.5499649 ]].
[2019-04-04 06:09:17,985] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45000, global step 716411: loss 9.9505
[2019-04-04 06:09:17,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45000, global step 716411: learning rate 0.0000
[2019-04-04 06:09:18,614] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7487806e-08 1.5047812e-04 1.0762957e-25 2.8681272e-23 6.9964580e-19
 9.9984944e-01 3.0355097e-17], sum to 1.0000
[2019-04-04 06:09:18,614] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7231
[2019-04-04 06:09:18,657] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6, 92.0, 27.0, 0.0, 26.0, 26.06316224974613, 0.5932996691796845, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1327800.0000, 
sim time next is 1328400.0000, 
raw observation next is [0.5, 92.0, 31.5, 0.0, 26.0, 26.08001955381378, 0.5891367127430506, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.105, 0.0, 0.6666666666666666, 0.673334962817815, 0.6963789042476836, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7383627], dtype=float32), -0.54197216]. 
=============================================
[2019-04-04 06:09:21,138] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45000, global step 717855: loss 10.0860
[2019-04-04 06:09:21,141] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45000, global step 717855: learning rate 0.0000
[2019-04-04 06:09:25,810] A3C_AGENT_WORKER-Thread-6 INFO:Local step 45000, global step 719842: loss 10.2104
[2019-04-04 06:09:25,812] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 45000, global step 719842: learning rate 0.0000
[2019-04-04 06:09:25,898] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45000, global step 719875: loss 10.1539
[2019-04-04 06:09:25,901] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45000, global step 719876: learning rate 0.0000
[2019-04-04 06:09:27,540] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45000, global step 720558: loss 10.1784
[2019-04-04 06:09:27,544] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45000, global step 720559: learning rate 0.0000
[2019-04-04 06:09:27,896] A3C_AGENT_WORKER-Thread-11 INFO:Local step 45000, global step 720703: loss 10.1390
[2019-04-04 06:09:27,897] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 45000, global step 720703: learning rate 0.0000
[2019-04-04 06:09:29,010] A3C_AGENT_WORKER-Thread-12 INFO:Local step 45000, global step 721177: loss 10.2069
[2019-04-04 06:09:29,011] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 45000, global step 721177: learning rate 0.0000
[2019-04-04 06:09:30,798] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45000, global step 721939: loss 10.1916
[2019-04-04 06:09:30,802] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45000, global step 721942: learning rate 0.0000
[2019-04-04 06:09:30,984] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45000, global step 722014: loss 10.1809
[2019-04-04 06:09:30,986] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45000, global step 722015: learning rate 0.0000
[2019-04-04 06:09:31,545] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45000, global step 722295: loss 10.1939
[2019-04-04 06:09:31,546] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45000, global step 722296: learning rate 0.0000
[2019-04-04 06:09:31,815] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9808653e-08 1.5138798e-04 4.0994226e-26 6.9220193e-25 5.3100467e-19
 9.9984860e-01 1.3700008e-18], sum to 1.0000
[2019-04-04 06:09:31,815] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1311
[2019-04-04 06:09:31,847] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.15, 72.0, 115.0, 136.0, 26.0, 26.33567756024462, 0.6556511418257677, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1589400.0000, 
sim time next is 1590000.0000, 
raw observation next is [7.333333333333333, 70.66666666666666, 129.1666666666667, 128.0, 26.0, 26.46342150832989, 0.6695646882115723, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6657433056325024, 0.7066666666666666, 0.4305555555555557, 0.1414364640883978, 0.6666666666666666, 0.7052851256941576, 0.7231882294038575, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.5966082], dtype=float32), -0.3676247]. 
=============================================
[2019-04-04 06:09:31,860] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[91.29126]
 [90.62685]
 [90.18809]
 [89.80605]
 [89.37582]], R is [[92.07583618]
 [92.15507507]
 [92.23352814]
 [92.31119537]
 [92.38808441]].
[2019-04-04 06:09:32,092] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45000, global step 722531: loss 10.1936
[2019-04-04 06:09:32,093] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45000, global step 722532: learning rate 0.0000
[2019-04-04 06:09:32,909] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45000, global step 722915: loss 10.1355
[2019-04-04 06:09:32,910] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45000, global step 722915: learning rate 0.0000
[2019-04-04 06:09:32,988] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45500, global step 722954: loss 0.2229
[2019-04-04 06:09:33,000] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45500, global step 722954: learning rate 0.0000
[2019-04-04 06:09:33,103] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45500, global step 723017: loss 0.1901
[2019-04-04 06:09:33,104] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45500, global step 723017: learning rate 0.0000
[2019-04-04 06:09:33,263] A3C_AGENT_WORKER-Thread-13 INFO:Local step 45000, global step 723107: loss 10.1694
[2019-04-04 06:09:33,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 45000, global step 723107: learning rate 0.0000
[2019-04-04 06:09:33,514] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45000, global step 723234: loss 10.2345
[2019-04-04 06:09:33,523] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45000, global step 723237: learning rate 0.0000
[2019-04-04 06:09:35,115] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.4377015e-10 2.8050659e-04 6.6589642e-29 1.9207157e-27 8.8374555e-22
 9.9971944e-01 5.1027827e-21], sum to 1.0000
[2019-04-04 06:09:35,115] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2113
[2019-04-04 06:09:35,129] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.06666666666667, 51.66666666666667, 153.5, 103.3333333333333, 26.0, 26.71860031560794, 0.7474593279529773, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1600800.0000, 
sim time next is 1601400.0000, 
raw observation next is [13.43333333333334, 50.33333333333334, 158.0, 82.66666666666667, 26.0, 25.89623605730259, 0.6939322520412746, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8347183748845802, 0.5033333333333334, 0.5266666666666666, 0.09134438305709025, 0.6666666666666666, 0.6580196714418826, 0.731310750680425, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.137523], dtype=float32), 1.562227]. 
=============================================
[2019-04-04 06:09:35,877] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6806917e-08 1.3483172e-04 3.7649065e-25 6.8514952e-24 5.7725238e-19
 9.9986517e-01 1.1425817e-18], sum to 1.0000
[2019-04-04 06:09:35,877] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2425
[2019-04-04 06:09:35,918] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 76.0, 76.0, 85.5, 26.0, 26.16468301751373, 0.595964482753825, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1587600.0000, 
sim time next is 1588200.0000, 
raw observation next is [6.783333333333333, 74.66666666666667, 89.0, 102.3333333333333, 26.0, 26.15963128860751, 0.6083159690940153, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6505078485687905, 0.7466666666666667, 0.2966666666666667, 0.11307550644567216, 0.6666666666666666, 0.6799692740506259, 0.7027719896980051, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16610138], dtype=float32), -0.7879227]. 
=============================================
[2019-04-04 06:09:36,700] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45500, global step 724689: loss 0.1803
[2019-04-04 06:09:36,700] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45500, global step 724689: learning rate 0.0000
[2019-04-04 06:09:39,453] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45500, global step 725877: loss 0.2213
[2019-04-04 06:09:39,454] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45500, global step 725877: learning rate 0.0000
[2019-04-04 06:09:40,750] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3377799e-07 1.1195728e-03 2.0736562e-24 4.3704520e-22 6.1803833e-18
 9.9888033e-01 5.0905036e-17], sum to 1.0000
[2019-04-04 06:09:40,758] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2875
[2019-04-04 06:09:40,789] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8999999999999999, 85.0, 0.0, 0.0, 26.0, 24.9832295360646, 0.3633679061152366, 0.0, 1.0, 43639.16630849725], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1747800.0000, 
sim time next is 1748400.0000, 
raw observation next is [-1.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.97014360400497, 0.3563161401357644, 0.0, 1.0, 43686.58955474001], 
processed observation next is [0.0, 0.21739130434782608, 0.4349030470914128, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5808453003337476, 0.6187720467119214, 0.0, 1.0, 0.20803137883209527], 
reward next is 0.7920, 
noisyNet noise sample is [array([0.01568546], dtype=float32), -0.3680925]. 
=============================================
[2019-04-04 06:09:44,553] A3C_AGENT_WORKER-Thread-6 INFO:Local step 45500, global step 727531: loss 0.1793
[2019-04-04 06:09:44,567] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 45500, global step 727531: learning rate 0.0000
[2019-04-04 06:09:45,263] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45500, global step 727767: loss 0.1411
[2019-04-04 06:09:45,276] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45500, global step 727768: learning rate 0.0000
[2019-04-04 06:09:45,971] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45500, global step 728017: loss 0.1704
[2019-04-04 06:09:45,972] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45500, global step 728017: learning rate 0.0000
[2019-04-04 06:09:46,996] A3C_AGENT_WORKER-Thread-11 INFO:Local step 45500, global step 728338: loss 0.1594
[2019-04-04 06:09:46,996] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 45500, global step 728338: learning rate 0.0000
[2019-04-04 06:09:48,340] A3C_AGENT_WORKER-Thread-12 INFO:Local step 45500, global step 728772: loss 0.1084
[2019-04-04 06:09:48,348] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 45500, global step 728775: learning rate 0.0000
[2019-04-04 06:09:50,996] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45500, global step 729420: loss 0.1484
[2019-04-04 06:09:50,997] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45500, global step 729420: learning rate 0.0000
[2019-04-04 06:09:51,009] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45500, global step 729423: loss 0.1234
[2019-04-04 06:09:51,017] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45500, global step 729423: learning rate 0.0000
[2019-04-04 06:09:51,796] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45500, global step 729633: loss 0.1449
[2019-04-04 06:09:51,797] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45500, global step 729633: learning rate 0.0000
[2019-04-04 06:09:52,465] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45500, global step 729833: loss 0.1136
[2019-04-04 06:09:52,466] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45500, global step 729833: learning rate 0.0000
[2019-04-04 06:09:53,455] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45500, global step 730167: loss 0.1120
[2019-04-04 06:09:53,458] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45500, global step 730167: learning rate 0.0000
[2019-04-04 06:09:53,698] A3C_AGENT_WORKER-Thread-13 INFO:Local step 45500, global step 730245: loss 0.0617
[2019-04-04 06:09:53,704] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 45500, global step 730246: learning rate 0.0000
[2019-04-04 06:09:54,138] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45500, global step 730403: loss 0.1699
[2019-04-04 06:09:54,170] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45500, global step 730404: learning rate 0.0000
[2019-04-04 06:09:58,548] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46000, global step 731659: loss 10.8604
[2019-04-04 06:09:58,549] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46000, global step 731659: learning rate 0.0000
[2019-04-04 06:10:00,074] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46000, global step 732075: loss 10.9572
[2019-04-04 06:10:00,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46000, global step 732075: learning rate 0.0000
[2019-04-04 06:10:04,238] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46000, global step 733381: loss 10.9519
[2019-04-04 06:10:04,240] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46000, global step 733381: learning rate 0.0000
[2019-04-04 06:10:06,202] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46000, global step 733892: loss 10.9175
[2019-04-04 06:10:06,202] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46000, global step 733892: learning rate 0.0000
[2019-04-04 06:10:11,685] A3C_AGENT_WORKER-Thread-6 INFO:Local step 46000, global step 735490: loss 10.7251
[2019-04-04 06:10:11,685] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 46000, global step 735490: learning rate 0.0000
[2019-04-04 06:10:12,792] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46000, global step 735810: loss 10.6650
[2019-04-04 06:10:12,792] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46000, global step 735810: learning rate 0.0000
[2019-04-04 06:10:12,904] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46000, global step 735839: loss 10.8870
[2019-04-04 06:10:12,905] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46000, global step 735839: learning rate 0.0000
[2019-04-04 06:10:13,518] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5350224e-09 1.0597259e-05 5.7311315e-27 5.7456285e-25 3.3613156e-19
 9.9998939e-01 3.7712071e-19], sum to 1.0000
[2019-04-04 06:10:13,518] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7663
[2019-04-04 06:10:13,582] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.0, 17.0, 0.0, 26.0, 25.36565693212208, 0.3422171360435586, 1.0, 1.0, 63747.99060445418], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2048400.0000, 
sim time next is 2049000.0000, 
raw observation next is [-3.9, 82.00000000000001, 12.0, 0.0, 26.0, 25.47169150429094, 0.3547565872703812, 1.0, 1.0, 45740.63538042069], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8200000000000002, 0.04, 0.0, 0.6666666666666666, 0.6226409586909117, 0.6182521957567938, 1.0, 1.0, 0.2178125494305747], 
reward next is 0.7822, 
noisyNet noise sample is [array([-1.4612846], dtype=float32), 1.61149]. 
=============================================
[2019-04-04 06:10:13,585] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[87.56043 ]
 [87.43307 ]
 [87.51023 ]
 [87.52354 ]
 [87.655556]], R is [[87.43078613]
 [87.25292206]
 [87.38039398]
 [87.5065918 ]
 [87.63152313]].
[2019-04-04 06:10:15,028] A3C_AGENT_WORKER-Thread-11 INFO:Local step 46000, global step 736345: loss 10.9763
[2019-04-04 06:10:15,030] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 46000, global step 736345: learning rate 0.0000
[2019-04-04 06:10:15,635] A3C_AGENT_WORKER-Thread-12 INFO:Local step 46000, global step 736488: loss 10.8222
[2019-04-04 06:10:15,635] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 46000, global step 736488: learning rate 0.0000
[2019-04-04 06:10:16,048] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2116621e-09 2.7123140e-04 1.0084798e-25 2.1097740e-24 1.1584520e-19
 9.9972874e-01 1.2188599e-18], sum to 1.0000
[2019-04-04 06:10:16,048] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8399
[2019-04-04 06:10:16,096] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 43.0, 129.5, 51.0, 26.0, 26.29828678407197, 0.4989949578800422, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2300400.0000, 
sim time next is 2301000.0000, 
raw observation next is [0.9166666666666667, 43.16666666666667, 132.3333333333333, 48.0, 26.0, 26.32061608001089, 0.3976476424000848, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.48799630655586346, 0.4316666666666667, 0.44111111111111095, 0.05303867403314917, 0.6666666666666666, 0.6933846733342408, 0.6325492141333616, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.74586475], dtype=float32), 0.20938595]. 
=============================================
[2019-04-04 06:10:16,118] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[86.801506]
 [86.85712 ]
 [87.03647 ]
 [87.08336 ]
 [87.29413 ]], R is [[86.81606293]
 [86.94790649]
 [87.07843018]
 [87.20764923]
 [87.33557129]].
[2019-04-04 06:10:17,473] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.3316151e-07 1.9489520e-03 1.2217556e-22 1.2705230e-20 3.0617912e-17
 9.9805045e-01 3.0325818e-16], sum to 1.0000
[2019-04-04 06:10:17,473] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5995
[2019-04-04 06:10:17,564] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.800000000000001, 78.16666666666667, 0.0, 0.0, 26.0, 23.77552181323423, 0.01075878671936281, 0.0, 1.0, 43331.41479895149], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2099400.0000, 
sim time next is 2100000.0000, 
raw observation next is [-6.9, 78.33333333333334, 0.0, 0.0, 26.0, 23.73380497697644, 0.09466382332186196, 1.0, 1.0, 202393.2885542702], 
processed observation next is [1.0, 0.30434782608695654, 0.27146814404432135, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.47781708141470336, 0.531554607773954, 1.0, 1.0, 0.9637775645441438], 
reward next is 0.0362, 
noisyNet noise sample is [array([-1.2194451], dtype=float32), -0.57811034]. 
=============================================
[2019-04-04 06:10:17,569] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[77.2988 ]
 [77.43401]
 [77.52176]
 [77.65509]
 [77.77881]], R is [[76.03729248]
 [76.07057953]
 [76.10301971]
 [76.13470459]
 [76.16576385]].
[2019-04-04 06:10:17,618] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2527065e-08 4.7187506e-05 3.1647739e-25 2.3990909e-23 8.6529327e-19
 9.9995279e-01 1.5408934e-18], sum to 1.0000
[2019-04-04 06:10:17,618] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5256
[2019-04-04 06:10:17,659] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.533333333333333, 68.66666666666667, 0.0, 0.0, 26.0, 25.8685402695092, 0.4811061478335497, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2226000.0000, 
sim time next is 2226600.0000, 
raw observation next is [-4.55, 69.0, 0.0, 0.0, 26.0, 25.85488014048521, 0.4684737380609079, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3365650969529086, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6545733450404342, 0.6561579126869693, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9187749], dtype=float32), 0.44746834]. 
=============================================
[2019-04-04 06:10:17,764] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46000, global step 737165: loss 10.9958
[2019-04-04 06:10:17,765] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46000, global step 737165: learning rate 0.0000
[2019-04-04 06:10:18,425] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7513705e-06 2.8687136e-03 5.6761695e-22 7.2518246e-21 2.2203690e-16
 9.9712950e-01 5.4754645e-15], sum to 1.0000
[2019-04-04 06:10:18,425] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9174
[2019-04-04 06:10:18,466] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 83.0, 0.0, 0.0, 26.0, 24.06238039124386, 0.07260431153634873, 0.0, 1.0, 43563.57318006135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2095200.0000, 
sim time next is 2095800.0000, 
raw observation next is [-6.700000000000001, 82.16666666666667, 0.0, 0.0, 26.0, 24.05786858922713, 0.06129139463406599, 0.0, 1.0, 43596.2306622401], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.8216666666666668, 0.0, 0.0, 0.6666666666666666, 0.5048223824355942, 0.520430464878022, 0.0, 1.0, 0.20760109839161953], 
reward next is 0.7924, 
noisyNet noise sample is [array([-1.6903746], dtype=float32), -1.7798595]. 
=============================================
[2019-04-04 06:10:18,622] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.5901525e-09 9.4482428e-05 1.6221126e-26 7.7725454e-25 1.5723430e-19
 9.9990547e-01 1.3757979e-18], sum to 1.0000
[2019-04-04 06:10:18,622] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6738
[2019-04-04 06:10:18,682] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 65.33333333333334, 149.3333333333333, 22.33333333333333, 26.0, 25.24475514425873, 0.401526023892266, 1.0, 1.0, 31310.22843743411], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2121600.0000, 
sim time next is 2122200.0000, 
raw observation next is [-5.9, 66.0, 149.0, 0.0, 26.0, 25.5503901156684, 0.4209791432703194, 1.0, 1.0, 29307.98644392355], 
processed observation next is [1.0, 0.5652173913043478, 0.2991689750692521, 0.66, 0.49666666666666665, 0.0, 0.6666666666666666, 0.6291991763057, 0.6403263810901064, 1.0, 1.0, 0.13956184020915977], 
reward next is 0.8604, 
noisyNet noise sample is [array([0.78076726], dtype=float32), -0.15787522]. 
=============================================
[2019-04-04 06:10:19,014] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46000, global step 737582: loss 11.0361
[2019-04-04 06:10:19,026] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46000, global step 737582: learning rate 0.0000
[2019-04-04 06:10:19,375] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46000, global step 737689: loss 10.8070
[2019-04-04 06:10:19,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46000, global step 737689: learning rate 0.0000
[2019-04-04 06:10:19,721] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46000, global step 737778: loss 10.8634
[2019-04-04 06:10:19,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46000, global step 737778: learning rate 0.0000
[2019-04-04 06:10:20,437] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46000, global step 737988: loss 10.8128
[2019-04-04 06:10:20,437] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46000, global step 737988: learning rate 0.0000
[2019-04-04 06:10:20,568] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.4335426e-08 2.0043919e-04 6.7369613e-24 3.3876118e-22 4.4953729e-17
 9.9979955e-01 2.8797231e-17], sum to 1.0000
[2019-04-04 06:10:20,568] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9471
[2019-04-04 06:10:20,627] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.55, 78.5, 208.0, 60.0, 26.0, 25.84738089435647, 0.3909166663317867, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2111400.0000, 
sim time next is 2112000.0000, 
raw observation next is [-7.466666666666667, 77.33333333333334, 222.1666666666667, 66.83333333333333, 26.0, 25.79892350761287, 0.3892293534672677, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.25577100646352724, 0.7733333333333334, 0.7405555555555557, 0.07384898710865562, 0.6666666666666666, 0.6499102923010724, 0.6297431178224225, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.60116285], dtype=float32), -1.656449]. 
=============================================
[2019-04-04 06:10:20,690] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[82.352455]
 [81.48512 ]
 [80.689   ]
 [79.95327 ]
 [79.18933 ]], R is [[83.01881409]
 [83.18862915]
 [83.35674286]
 [83.5231781 ]
 [83.68795013]].
[2019-04-04 06:10:20,708] A3C_AGENT_WORKER-Thread-13 INFO:Local step 46000, global step 738058: loss 10.8637
[2019-04-04 06:10:20,713] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 46000, global step 738058: learning rate 0.0000
[2019-04-04 06:10:21,384] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46000, global step 738250: loss 10.8589
[2019-04-04 06:10:21,387] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46000, global step 738250: learning rate 0.0000
[2019-04-04 06:10:25,518] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1647672e-08 1.9371531e-04 6.0733415e-25 2.3010698e-24 5.5539574e-19
 9.9980634e-01 1.1805817e-18], sum to 1.0000
[2019-04-04 06:10:25,530] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2341
[2019-04-04 06:10:25,615] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 69.5, 0.0, 0.0, 26.0, 24.62713538878796, 0.2786355995641899, 1.0, 1.0, 198788.7899095495], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2223000.0000, 
sim time next is 2223600.0000, 
raw observation next is [-4.5, 69.0, 0.0, 0.0, 26.0, 24.81157682598011, 0.400433480970356, 1.0, 1.0, 191048.2295170949], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5676314021650093, 0.6334778269901187, 1.0, 1.0, 0.9097534738909281], 
reward next is 0.0902, 
noisyNet noise sample is [array([-0.18189558], dtype=float32), -0.45654285]. 
=============================================
[2019-04-04 06:10:25,961] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46500, global step 739528: loss 0.0167
[2019-04-04 06:10:25,965] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46500, global step 739528: learning rate 0.0000
[2019-04-04 06:10:26,156] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2609040e-08 6.8973291e-05 3.0923500e-25 4.4587144e-24 6.7930936e-19
 9.9993098e-01 1.6469267e-18], sum to 1.0000
[2019-04-04 06:10:26,156] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3995
[2019-04-04 06:10:26,219] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 68.5, 138.0, 0.0, 26.0, 25.66215744115175, 0.3579155172674018, 1.0, 1.0, 25368.25756871401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2202600.0000, 
sim time next is 2203200.0000, 
raw observation next is [-3.9, 68.0, 135.5, 0.0, 26.0, 25.62264203618989, 0.353830645535727, 1.0, 1.0, 24510.10913096105], 
processed observation next is [1.0, 0.5217391304347826, 0.3545706371191136, 0.68, 0.45166666666666666, 0.0, 0.6666666666666666, 0.635220169682491, 0.617943548511909, 1.0, 1.0, 0.11671480538552881], 
reward next is 0.8833, 
noisyNet noise sample is [array([0.21047936], dtype=float32), 1.3722825]. 
=============================================
[2019-04-04 06:10:28,127] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46500, global step 740210: loss 0.0137
[2019-04-04 06:10:28,128] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46500, global step 740210: learning rate 0.0000
[2019-04-04 06:10:32,294] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46500, global step 741333: loss 0.0122
[2019-04-04 06:10:32,303] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46500, global step 741334: learning rate 0.0000
[2019-04-04 06:10:33,933] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46500, global step 741920: loss 0.0188
[2019-04-04 06:10:33,936] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46500, global step 741922: learning rate 0.0000
[2019-04-04 06:10:39,187] A3C_AGENT_WORKER-Thread-6 INFO:Local step 46500, global step 743542: loss 0.0164
[2019-04-04 06:10:39,201] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 46500, global step 743542: learning rate 0.0000
[2019-04-04 06:10:39,763] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46500, global step 743736: loss 0.0242
[2019-04-04 06:10:39,764] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46500, global step 743736: learning rate 0.0000
[2019-04-04 06:10:39,940] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7088549e-06 5.1957392e-03 3.2632254e-23 4.3199762e-21 2.5545622e-16
 9.9480259e-01 1.1013976e-16], sum to 1.0000
[2019-04-04 06:10:39,945] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3157
[2019-04-04 06:10:39,962] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 41.0, 0.0, 0.0, 26.0, 24.74300503949522, 0.1844517321850286, 0.0, 1.0, 43063.77544063202], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2415600.0000, 
sim time next is 2416200.0000, 
raw observation next is [-5.100000000000001, 41.33333333333334, 0.0, 0.0, 26.0, 24.69810595506224, 0.1756576601200357, 0.0, 1.0, 43088.99602211146], 
processed observation next is [0.0, 1.0, 0.32132963988919666, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.5581754962551866, 0.5585525533733452, 0.0, 1.0, 0.2051856953433879], 
reward next is 0.7948, 
noisyNet noise sample is [array([-0.11278755], dtype=float32), -0.15720037]. 
=============================================
[2019-04-04 06:10:40,615] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46500, global step 744069: loss 0.0153
[2019-04-04 06:10:40,621] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46500, global step 744069: learning rate 0.0000
[2019-04-04 06:10:42,528] A3C_AGENT_WORKER-Thread-12 INFO:Local step 46500, global step 744743: loss 0.0163
[2019-04-04 06:10:42,531] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 46500, global step 744743: learning rate 0.0000
[2019-04-04 06:10:42,559] A3C_AGENT_WORKER-Thread-11 INFO:Local step 46500, global step 744758: loss 0.0236
[2019-04-04 06:10:42,560] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 46500, global step 744758: learning rate 0.0000
[2019-04-04 06:10:44,782] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46500, global step 745416: loss 0.0121
[2019-04-04 06:10:44,783] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46500, global step 745416: learning rate 0.0000
[2019-04-04 06:10:45,890] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46500, global step 745743: loss 0.0203
[2019-04-04 06:10:45,890] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46500, global step 745743: learning rate 0.0000
[2019-04-04 06:10:46,084] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46500, global step 745796: loss 0.0218
[2019-04-04 06:10:46,098] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46500, global step 745796: learning rate 0.0000
[2019-04-04 06:10:46,771] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46500, global step 746048: loss 0.0183
[2019-04-04 06:10:46,772] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46500, global step 746048: learning rate 0.0000
[2019-04-04 06:10:47,561] A3C_AGENT_WORKER-Thread-13 INFO:Local step 46500, global step 746388: loss 0.0369
[2019-04-04 06:10:47,566] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 46500, global step 746388: learning rate 0.0000
[2019-04-04 06:10:47,623] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46500, global step 746412: loss 0.0091
[2019-04-04 06:10:47,624] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46500, global step 746412: learning rate 0.0000
[2019-04-04 06:10:49,675] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46500, global step 747118: loss 0.0161
[2019-04-04 06:10:49,678] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46500, global step 747118: learning rate 0.0000
[2019-04-04 06:10:49,680] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47000, global step 747121: loss 18.5497
[2019-04-04 06:10:49,681] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47000, global step 747121: learning rate 0.0000
[2019-04-04 06:10:51,259] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47000, global step 747667: loss 18.8278
[2019-04-04 06:10:51,259] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47000, global step 747667: learning rate 0.0000
[2019-04-04 06:10:53,388] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.2892401e-07 8.6964708e-04 6.0238196e-24 1.7135220e-22 5.1229486e-17
 9.9912995e-01 1.5272552e-16], sum to 1.0000
[2019-04-04 06:10:53,388] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5874
[2019-04-04 06:10:53,415] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8, 34.33333333333334, 0.0, 0.0, 26.0, 25.18030487273926, 0.2474285966817191, 0.0, 1.0, 40254.10053551765], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2500800.0000, 
sim time next is 2501400.0000, 
raw observation next is [-0.7, 34.66666666666666, 0.0, 0.0, 26.0, 25.16359720709114, 0.2445629678183597, 0.0, 1.0, 40176.50639818102], 
processed observation next is [0.0, 0.9565217391304348, 0.443213296398892, 0.34666666666666657, 0.0, 0.0, 0.6666666666666666, 0.5969664339242616, 0.5815209892727865, 0.0, 1.0, 0.19131669713419536], 
reward next is 0.8087, 
noisyNet noise sample is [array([1.1998134], dtype=float32), 0.74324]. 
=============================================
[2019-04-04 06:10:55,204] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.0273193e-09 4.9538437e-05 3.4948514e-27 3.2302545e-24 2.8707215e-19
 9.9995041e-01 7.8388443e-19], sum to 1.0000
[2019-04-04 06:10:55,205] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8665
[2019-04-04 06:10:55,223] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 61.5, 0.0, 0.0, 26.0, 25.36828732785947, 0.4428031052674175, 0.0, 1.0, 67423.61083901985], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2755800.0000, 
sim time next is 2756400.0000, 
raw observation next is [-6.0, 60.66666666666666, 0.0, 0.0, 26.0, 25.34793503117735, 0.4407314494607242, 0.0, 1.0, 57528.3029498839], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.6066666666666666, 0.0, 0.0, 0.6666666666666666, 0.6123279192647791, 0.6469104831535747, 0.0, 1.0, 0.27394429976135193], 
reward next is 0.7261, 
noisyNet noise sample is [array([-0.15799472], dtype=float32), -2.05605]. 
=============================================
[2019-04-04 06:10:55,458] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47000, global step 749253: loss 18.9418
[2019-04-04 06:10:55,460] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47000, global step 749253: learning rate 0.0000
[2019-04-04 06:10:56,388] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47000, global step 749586: loss 18.9550
[2019-04-04 06:10:56,388] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47000, global step 749586: learning rate 0.0000
[2019-04-04 06:11:01,351] A3C_AGENT_WORKER-Thread-6 INFO:Local step 47000, global step 751391: loss 19.1803
[2019-04-04 06:11:01,352] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 47000, global step 751391: learning rate 0.0000
[2019-04-04 06:11:01,873] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47000, global step 751548: loss 19.1115
[2019-04-04 06:11:01,877] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47000, global step 751549: learning rate 0.0000
[2019-04-04 06:11:02,760] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47000, global step 751820: loss 19.1394
[2019-04-04 06:11:02,761] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47000, global step 751820: learning rate 0.0000
[2019-04-04 06:11:05,159] A3C_AGENT_WORKER-Thread-12 INFO:Local step 47000, global step 752601: loss 19.6663
[2019-04-04 06:11:05,160] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 47000, global step 752601: learning rate 0.0000
[2019-04-04 06:11:05,228] A3C_AGENT_WORKER-Thread-11 INFO:Local step 47000, global step 752617: loss 19.4944
[2019-04-04 06:11:05,228] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 47000, global step 752617: learning rate 0.0000
[2019-04-04 06:11:07,352] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47000, global step 753427: loss 19.3308
[2019-04-04 06:11:07,352] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47000, global step 753427: learning rate 0.0000
[2019-04-04 06:11:08,254] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2509690e-06 3.2128387e-03 4.3955376e-21 1.6391768e-19 3.6939069e-15
 9.9678385e-01 9.7625274e-15], sum to 1.0000
[2019-04-04 06:11:08,254] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9424
[2019-04-04 06:11:08,299] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.66666666666667, 83.0, 0.0, 0.0, 26.0, 22.95738874421112, -0.1456924347986542, 0.0, 1.0, 43243.24196779371], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2701200.0000, 
sim time next is 2701800.0000, 
raw observation next is [-15.5, 83.0, 0.0, 0.0, 26.0, 22.92625018096442, -0.1497138188891635, 0.0, 1.0, 43197.48847723324], 
processed observation next is [1.0, 0.2608695652173913, 0.033240997229916885, 0.83, 0.0, 0.0, 0.6666666666666666, 0.4105208484137017, 0.4500953937036121, 0.0, 1.0, 0.20570232608206304], 
reward next is 0.7943, 
noisyNet noise sample is [array([-1.2697103], dtype=float32), -1.4405484]. 
=============================================
[2019-04-04 06:11:08,319] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47000, global step 753743: loss 19.4562
[2019-04-04 06:11:08,319] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47000, global step 753743: learning rate 0.0000
[2019-04-04 06:11:08,454] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47000, global step 753779: loss 19.5848
[2019-04-04 06:11:08,455] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47000, global step 753779: learning rate 0.0000
[2019-04-04 06:11:08,714] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47000, global step 753845: loss 19.4874
[2019-04-04 06:11:08,716] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47000, global step 753845: learning rate 0.0000
[2019-04-04 06:11:09,779] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47000, global step 754193: loss 19.7138
[2019-04-04 06:11:09,779] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47000, global step 754193: learning rate 0.0000
[2019-04-04 06:11:10,197] A3C_AGENT_WORKER-Thread-13 INFO:Local step 47000, global step 754338: loss 19.7372
[2019-04-04 06:11:10,198] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 47000, global step 754338: learning rate 0.0000
[2019-04-04 06:11:12,111] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47000, global step 754890: loss 19.5549
[2019-04-04 06:11:12,113] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47000, global step 754890: learning rate 0.0000
[2019-04-04 06:11:13,307] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47500, global step 755339: loss 0.2114
[2019-04-04 06:11:13,314] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47500, global step 755339: learning rate 0.0000
[2019-04-04 06:11:14,536] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47500, global step 755844: loss 0.1717
[2019-04-04 06:11:14,537] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47500, global step 755844: learning rate 0.0000
[2019-04-04 06:11:16,215] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.4323268e-07 3.5071333e-03 5.3504270e-24 1.2956131e-21 2.2856964e-16
 9.9649256e-01 2.0014380e-16], sum to 1.0000
[2019-04-04 06:11:16,219] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8835
[2019-04-04 06:11:16,273] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 154.0, 132.0, 26.0, 25.25196781232526, 0.339938283986141, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2971800.0000, 
sim time next is 2972400.0000, 
raw observation next is [-4.0, 71.0, 158.0, 114.0, 26.0, 25.17885222532276, 0.3210317011490205, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.5266666666666666, 0.12596685082872927, 0.6666666666666666, 0.5982376854435634, 0.6070105670496735, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.57721674], dtype=float32), 0.9312504]. 
=============================================
[2019-04-04 06:11:16,493] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.16410409e-09 1.06845677e-04 1.60452690e-28 1.03837884e-26
 2.45269412e-21 9.99893188e-01 2.03629782e-20], sum to 1.0000
[2019-04-04 06:11:16,493] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1222
[2019-04-04 06:11:16,514] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 32.5, 249.0, 173.0, 26.0, 25.97319885765159, 0.4756244244541358, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2813400.0000, 
sim time next is 2814000.0000, 
raw observation next is [5.333333333333334, 31.66666666666667, 227.1666666666667, 144.1666666666667, 26.0, 25.99308415904012, 0.3211442151097871, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6103416435826409, 0.3166666666666667, 0.7572222222222224, 0.15930018416206268, 0.6666666666666666, 0.6660903465866767, 0.6070480717032624, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36193964], dtype=float32), -0.2995041]. 
=============================================
[2019-04-04 06:11:16,517] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[94.04162 ]
 [94.01974 ]
 [93.998985]
 [93.8428  ]
 [93.48378 ]], R is [[93.97299957]
 [94.03327179]
 [94.09294128]
 [94.15201569]
 [94.210495  ]].
[2019-04-04 06:11:19,474] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47500, global step 757559: loss 0.1667
[2019-04-04 06:11:19,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47500, global step 757560: learning rate 0.0000
[2019-04-04 06:11:19,942] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47500, global step 757753: loss 0.1494
[2019-04-04 06:11:19,945] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47500, global step 757754: learning rate 0.0000
[2019-04-04 06:11:22,917] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1785853e-09 1.7783551e-04 1.3902170e-26 4.8940911e-24 4.0195829e-18
 9.9982220e-01 8.9995059e-19], sum to 1.0000
[2019-04-04 06:11:22,920] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6978
[2019-04-04 06:11:22,966] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 193.5, 623.1666666666667, 26.0, 25.04757696066347, 0.3867317728141685, 0.0, 1.0, 20940.67870347741], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2982000.0000, 
sim time next is 2982600.0000, 
raw observation next is [-3.0, 65.0, 181.0, 691.0, 26.0, 25.04999383182351, 0.3924562803695279, 0.0, 1.0, 27283.26691975335], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.6033333333333334, 0.7635359116022099, 0.6666666666666666, 0.5874994859852926, 0.630818760123176, 0.0, 1.0, 0.12992031866549214], 
reward next is 0.8701, 
noisyNet noise sample is [array([-0.01983131], dtype=float32), 0.12860768]. 
=============================================
[2019-04-04 06:11:24,173] A3C_AGENT_WORKER-Thread-6 INFO:Local step 47500, global step 759285: loss 0.1574
[2019-04-04 06:11:24,176] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 47500, global step 759285: learning rate 0.0000
[2019-04-04 06:11:25,293] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47500, global step 759673: loss 0.1010
[2019-04-04 06:11:25,294] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47500, global step 759673: learning rate 0.0000
[2019-04-04 06:11:25,416] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47500, global step 759722: loss 0.1513
[2019-04-04 06:11:25,416] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47500, global step 759722: learning rate 0.0000
[2019-04-04 06:11:28,361] A3C_AGENT_WORKER-Thread-11 INFO:Local step 47500, global step 760821: loss 0.1009
[2019-04-04 06:11:28,362] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 47500, global step 760821: learning rate 0.0000
[2019-04-04 06:11:28,588] A3C_AGENT_WORKER-Thread-12 INFO:Local step 47500, global step 760894: loss 0.1283
[2019-04-04 06:11:28,589] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 47500, global step 760894: learning rate 0.0000
[2019-04-04 06:11:30,909] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47500, global step 761784: loss 0.1226
[2019-04-04 06:11:30,911] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47500, global step 761784: learning rate 0.0000
[2019-04-04 06:11:30,969] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47500, global step 761802: loss 0.0967
[2019-04-04 06:11:30,969] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47500, global step 761802: learning rate 0.0000
[2019-04-04 06:11:31,610] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47500, global step 762029: loss 0.1183
[2019-04-04 06:11:31,611] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47500, global step 762029: learning rate 0.0000
[2019-04-04 06:11:31,877] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47500, global step 762148: loss 0.0966
[2019-04-04 06:11:31,881] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47500, global step 762149: learning rate 0.0000
[2019-04-04 06:11:32,867] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48000, global step 762590: loss 1.2026
[2019-04-04 06:11:32,868] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48000, global step 762590: learning rate 0.0000
[2019-04-04 06:11:33,555] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47500, global step 762847: loss 0.1064
[2019-04-04 06:11:33,557] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47500, global step 762848: learning rate 0.0000
[2019-04-04 06:11:33,949] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48000, global step 762976: loss 1.3092
[2019-04-04 06:11:33,953] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48000, global step 762979: learning rate 0.0000
[2019-04-04 06:11:34,071] A3C_AGENT_WORKER-Thread-13 INFO:Local step 47500, global step 763019: loss 0.1348
[2019-04-04 06:11:34,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 47500, global step 763023: learning rate 0.0000
[2019-04-04 06:11:35,238] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47500, global step 763510: loss 0.1359
[2019-04-04 06:11:35,243] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47500, global step 763511: learning rate 0.0000
[2019-04-04 06:11:37,684] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5690574e-07 2.9798137e-04 7.0853831e-22 8.7054394e-21 1.6619861e-16
 9.9970168e-01 4.4597513e-16], sum to 1.0000
[2019-04-04 06:11:37,684] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1890
[2019-04-04 06:11:37,700] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.93334940797499, 0.3466928299329954, 0.0, 1.0, 43831.65763537659], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3291600.0000, 
sim time next is 3292200.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.95677949156726, 0.3461936236139267, 0.0, 1.0, 43832.80598121206], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5797316242972718, 0.6153978745379756, 0.0, 1.0, 0.20872764752958126], 
reward next is 0.7913, 
noisyNet noise sample is [array([-0.24342442], dtype=float32), -0.46569008]. 
=============================================
[2019-04-04 06:11:37,953] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6780574e-07 4.2872055e-04 4.8442364e-26 6.0662662e-24 2.6964918e-18
 9.9957114e-01 3.9112532e-18], sum to 1.0000
[2019-04-04 06:11:37,955] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0015
[2019-04-04 06:11:37,974] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 100.0, 0.0, 0.0, 26.0, 25.36164410709058, 0.3415423608629287, 0.0, 1.0, 48824.3055426429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3105000.0000, 
sim time next is 3105600.0000, 
raw observation next is [-0.3333333333333334, 100.0, 0.0, 0.0, 26.0, 25.39687906283746, 0.3466822119536452, 0.0, 1.0, 30933.60397651363], 
processed observation next is [0.0, 0.9565217391304348, 0.4533702677747, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6164065885697884, 0.6155607373178817, 0.0, 1.0, 0.14730287607863635], 
reward next is 0.8527, 
noisyNet noise sample is [array([0.96271515], dtype=float32), 0.28123364]. 
=============================================
[2019-04-04 06:11:38,458] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48000, global step 764910: loss 1.2270
[2019-04-04 06:11:38,459] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48000, global step 764911: learning rate 0.0000
[2019-04-04 06:11:38,951] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48000, global step 765113: loss 1.3329
[2019-04-04 06:11:38,951] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48000, global step 765113: learning rate 0.0000
[2019-04-04 06:11:42,569] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3450785e-08 6.6194742e-04 4.9516870e-24 2.8072182e-22 2.3991222e-18
 9.9933797e-01 3.0347321e-17], sum to 1.0000
[2019-04-04 06:11:42,570] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2459
[2019-04-04 06:11:42,583] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.55520416083046, 0.4576233662689859, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3462000.0000, 
sim time next is 3462600.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.56393483783097, 0.4487755882772793, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6303279031525809, 0.6495918627590931, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.815711], dtype=float32), 0.6195464]. 
=============================================
[2019-04-04 06:11:42,683] A3C_AGENT_WORKER-Thread-6 INFO:Local step 48000, global step 766892: loss 1.1385
[2019-04-04 06:11:42,685] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 48000, global step 766892: learning rate 0.0000
[2019-04-04 06:11:42,990] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.6379948e-08 2.5080625e-04 6.2846340e-25 1.6207605e-23 1.7553482e-18
 9.9974912e-01 3.6908140e-17], sum to 1.0000
[2019-04-04 06:11:43,000] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3955
[2019-04-04 06:11:43,033] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.32460288833064, 0.4890968652060338, 0.0, 1.0, 40585.42946939551], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3217200.0000, 
sim time next is 3217800.0000, 
raw observation next is [-2.833333333333333, 100.0, 0.0, 0.0, 26.0, 25.29981128862834, 0.4869705062278956, 0.0, 1.0, 40613.80346552752], 
processed observation next is [1.0, 0.21739130434782608, 0.3841181902123731, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6083176073856951, 0.6623235020759651, 0.0, 1.0, 0.1933990641215596], 
reward next is 0.8066, 
noisyNet noise sample is [array([0.9784178], dtype=float32), -0.24588656]. 
=============================================
[2019-04-04 06:11:43,306] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48000, global step 767177: loss 1.1067
[2019-04-04 06:11:43,307] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48000, global step 767177: learning rate 0.0000
[2019-04-04 06:11:43,656] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48000, global step 767314: loss 1.1763
[2019-04-04 06:11:43,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48000, global step 767314: learning rate 0.0000
[2019-04-04 06:11:46,633] A3C_AGENT_WORKER-Thread-11 INFO:Local step 48000, global step 768659: loss 1.1133
[2019-04-04 06:11:46,653] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 48000, global step 768660: learning rate 0.0000
[2019-04-04 06:11:46,990] A3C_AGENT_WORKER-Thread-12 INFO:Local step 48000, global step 768809: loss 1.0973
[2019-04-04 06:11:46,992] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 48000, global step 768810: learning rate 0.0000
[2019-04-04 06:11:48,597] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48000, global step 769546: loss 1.0867
[2019-04-04 06:11:48,597] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48000, global step 769546: learning rate 0.0000
[2019-04-04 06:11:49,166] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48000, global step 769758: loss 1.0754
[2019-04-04 06:11:49,167] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48000, global step 769758: learning rate 0.0000
[2019-04-04 06:11:49,500] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48000, global step 769860: loss 1.0466
[2019-04-04 06:11:49,503] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48000, global step 769860: learning rate 0.0000
[2019-04-04 06:11:49,584] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5958337e-10 5.8536407e-06 1.1092344e-30 5.9637350e-28 1.0052309e-21
 9.9999416e-01 1.1784663e-21], sum to 1.0000
[2019-04-04 06:11:49,588] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1732
[2019-04-04 06:11:49,620] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.333333333333333, 71.0, 87.66666666666667, 699.8333333333334, 26.0, 26.29840338693683, 0.7526853084697347, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3252000.0000, 
sim time next is 3252600.0000, 
raw observation next is [-2.5, 71.0, 85.0, 686.0, 26.0, 25.60751862671651, 0.7154380551639071, 1.0, 1.0, 115883.8543788877], 
processed observation next is [1.0, 0.6521739130434783, 0.39335180055401664, 0.71, 0.2833333333333333, 0.7580110497237569, 0.6666666666666666, 0.6339598855597091, 0.7384793517213023, 1.0, 1.0, 0.5518278779947033], 
reward next is 0.4482, 
noisyNet noise sample is [array([0.51373184], dtype=float32), 0.3005923]. 
=============================================
[2019-04-04 06:11:49,705] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48000, global step 769933: loss 1.1236
[2019-04-04 06:11:49,706] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48000, global step 769933: learning rate 0.0000
[2019-04-04 06:11:51,076] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48000, global step 770535: loss 0.9848
[2019-04-04 06:11:51,077] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48000, global step 770535: learning rate 0.0000
[2019-04-04 06:11:51,887] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48500, global step 770895: loss 3.1639
[2019-04-04 06:11:51,889] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48500, global step 770897: learning rate 0.0000
[2019-04-04 06:11:52,222] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1631845e-08 2.9939049e-04 8.0118778e-26 1.2157810e-23 1.5363375e-19
 9.9970067e-01 4.0017455e-18], sum to 1.0000
[2019-04-04 06:11:52,224] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9095
[2019-04-04 06:11:52,255] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 66.0, 0.0, 0.0, 26.0, 25.22165143045865, 0.4759634372434239, 0.0, 1.0, 56899.93412604265], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3538800.0000, 
sim time next is 3539400.0000, 
raw observation next is [-1.166666666666667, 65.0, 0.0, 0.0, 26.0, 25.30388657374186, 0.4805748597028569, 0.0, 1.0, 47456.43481646994], 
processed observation next is [1.0, 1.0, 0.43028624192059095, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6086572144784883, 0.6601916199009523, 0.0, 1.0, 0.22598302293557115], 
reward next is 0.7740, 
noisyNet noise sample is [array([0.44468498], dtype=float32), -0.21197824]. 
=============================================
[2019-04-04 06:11:52,382] A3C_AGENT_WORKER-Thread-13 INFO:Local step 48000, global step 771111: loss 1.1031
[2019-04-04 06:11:52,384] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 48000, global step 771111: learning rate 0.0000
[2019-04-04 06:11:53,093] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48500, global step 771417: loss 3.2059
[2019-04-04 06:11:53,094] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48500, global step 771417: learning rate 0.0000
[2019-04-04 06:11:53,355] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48000, global step 771536: loss 1.0318
[2019-04-04 06:11:53,355] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48000, global step 771536: learning rate 0.0000
[2019-04-04 06:11:56,388] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48500, global step 772932: loss 3.4393
[2019-04-04 06:11:56,389] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48500, global step 772932: learning rate 0.0000
[2019-04-04 06:11:56,743] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8448664e-08 1.9685820e-05 6.0686711e-27 5.2161830e-25 2.7037034e-19
 9.9998033e-01 1.1487353e-19], sum to 1.0000
[2019-04-04 06:11:56,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2879
[2019-04-04 06:11:56,756] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 25.14789494947285, 0.3408291611224761, 0.0, 1.0, 29163.85987489338], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3693600.0000, 
sim time next is 3694200.0000, 
raw observation next is [4.0, 59.0, 0.0, 0.0, 26.0, 25.11291955142321, 0.3322537413554645, 0.0, 1.0, 36956.64435880775], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5927432959519342, 0.6107512471184882, 0.0, 1.0, 0.1759840207562274], 
reward next is 0.8240, 
noisyNet noise sample is [array([-1.0287662], dtype=float32), -0.7871662]. 
=============================================
[2019-04-04 06:11:56,852] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.2058275e-07 2.0192382e-04 2.3238461e-22 1.6897862e-20 1.2998095e-16
 9.9979764e-01 2.0922639e-15], sum to 1.0000
[2019-04-04 06:11:56,859] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8833
[2019-04-04 06:11:56,872] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 67.0, 0.0, 0.0, 26.0, 25.17627209942164, 0.382544300586, 0.0, 1.0, 41110.46567439308], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3554400.0000, 
sim time next is 3555000.0000, 
raw observation next is [-3.5, 68.0, 0.0, 0.0, 26.0, 25.13502300107291, 0.3741763613597147, 0.0, 1.0, 41174.69843271246], 
processed observation next is [0.0, 0.13043478260869565, 0.36565096952908593, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5945852500894091, 0.6247254537865715, 0.0, 1.0, 0.196069992536726], 
reward next is 0.8039, 
noisyNet noise sample is [array([0.3211759], dtype=float32), 0.04680871]. 
=============================================
[2019-04-04 06:11:56,884] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[74.237946]
 [74.17208 ]
 [74.31636 ]
 [74.17925 ]
 [74.33505 ]], R is [[74.3615036 ]
 [74.42212677]
 [74.4825592 ]
 [74.54277802]
 [74.6027298 ]].
[2019-04-04 06:11:56,903] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48500, global step 773180: loss 3.4426
[2019-04-04 06:11:56,903] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48500, global step 773180: learning rate 0.0000
[2019-04-04 06:11:59,335] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8778428e-09 7.7039513e-05 1.1843087e-28 1.0568937e-25 1.6400093e-20
 9.9992299e-01 2.4780873e-19], sum to 1.0000
[2019-04-04 06:11:59,335] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8777
[2019-04-04 06:11:59,356] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 113.6666666666667, 807.8333333333334, 26.0, 25.85990903094046, 0.5738635133229024, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3417600.0000, 
sim time next is 3418200.0000, 
raw observation next is [3.0, 49.0, 113.0, 806.0, 26.0, 26.0727545566747, 0.5983163279595022, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.37666666666666665, 0.8906077348066298, 0.6666666666666666, 0.6727295463895583, 0.6994387759865007, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4354178], dtype=float32), -0.06585473]. 
=============================================
[2019-04-04 06:12:00,720] A3C_AGENT_WORKER-Thread-6 INFO:Local step 48500, global step 774975: loss 3.7117
[2019-04-04 06:12:00,721] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 48500, global step 774975: learning rate 0.0000
[2019-04-04 06:12:00,822] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.1561833e-10 3.0343472e-05 7.5878736e-28 6.6063120e-26 4.0878904e-20
 9.9996960e-01 7.8655900e-20], sum to 1.0000
[2019-04-04 06:12:00,825] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2399
[2019-04-04 06:12:00,837] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 42.33333333333334, 70.66666666666667, 576.3333333333333, 26.0, 25.32315696646668, 0.4531210238781937, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3600600.0000, 
sim time next is 3601200.0000, 
raw observation next is [0.0, 41.66666666666667, 66.83333333333333, 545.6666666666666, 26.0, 25.30576459706464, 0.4471428729929647, 0.0, 1.0, 18684.31268463445], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.41666666666666674, 0.22277777777777777, 0.6029465930018416, 0.6666666666666666, 0.6088137164220534, 0.6490476243309883, 0.0, 1.0, 0.08897291754587833], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.9210563], dtype=float32), 1.4953312]. 
=============================================
[2019-04-04 06:12:00,917] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.6365047e-07 1.1518280e-04 4.2597278e-24 6.7389012e-22 3.4894694e-17
 9.9988425e-01 7.7929566e-17], sum to 1.0000
[2019-04-04 06:12:00,918] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7219
[2019-04-04 06:12:00,943] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.55021242398824, 0.427099199140927, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3616200.0000, 
sim time next is 3616800.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.55561754299051, 0.4168361954541355, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6296347952492093, 0.6389453984847119, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08722556], dtype=float32), 0.5440218]. 
=============================================
[2019-04-04 06:12:01,159] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3949826e-08 8.7508008e-05 7.4620689e-26 3.3261646e-24 4.1188704e-19
 9.9991250e-01 4.8013967e-19], sum to 1.0000
[2019-04-04 06:12:01,162] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5474
[2019-04-04 06:12:01,189] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 68.0, 0.0, 0.0, 26.0, 25.11664125722509, 0.5039825663787925, 0.0, 1.0, 170014.2064958527], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3789000.0000, 
sim time next is 3789600.0000, 
raw observation next is [-2.666666666666667, 69.0, 0.0, 0.0, 26.0, 25.25448015596739, 0.5337245178122253, 0.0, 1.0, 91098.94632670368], 
processed observation next is [1.0, 0.8695652173913043, 0.38873499538319484, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6045400129972824, 0.6779081726040751, 0.0, 1.0, 0.43380450631763656], 
reward next is 0.5662, 
noisyNet noise sample is [array([-0.9056345], dtype=float32), 0.32645944]. 
=============================================
[2019-04-04 06:12:01,535] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48500, global step 775423: loss 3.7054
[2019-04-04 06:12:01,536] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48500, global step 775424: learning rate 0.0000
[2019-04-04 06:12:01,664] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48500, global step 775489: loss 3.6193
[2019-04-04 06:12:01,666] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48500, global step 775489: learning rate 0.0000
[2019-04-04 06:12:02,891] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.3823763e-09 1.3870845e-04 1.5802591e-26 4.4104132e-24 1.6023612e-18
 9.9986124e-01 1.0401815e-18], sum to 1.0000
[2019-04-04 06:12:02,891] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6575
[2019-04-04 06:12:02,916] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 70.0, 0.0, 0.0, 26.0, 25.27229899187494, 0.4555869952282722, 0.0, 1.0, 141509.5159302643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3537600.0000, 
sim time next is 3538200.0000, 
raw observation next is [-1.0, 68.0, 0.0, 0.0, 26.0, 25.20352379889593, 0.4619342024750977, 0.0, 1.0, 83863.67258092863], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6002936499079942, 0.6539780674916992, 0.0, 1.0, 0.3993508218139459], 
reward next is 0.6006, 
noisyNet noise sample is [array([0.7094148], dtype=float32), 1.8475854]. 
=============================================
[2019-04-04 06:12:04,327] A3C_AGENT_WORKER-Thread-11 INFO:Local step 48500, global step 776877: loss 3.6115
[2019-04-04 06:12:04,330] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 48500, global step 776879: learning rate 0.0000
[2019-04-04 06:12:04,567] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6688364e-08 1.4956803e-04 4.1638276e-26 2.0582066e-23 8.5150543e-18
 9.9985039e-01 2.3256228e-18], sum to 1.0000
[2019-04-04 06:12:04,571] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1567
[2019-04-04 06:12:04,582] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 63.66666666666667, 0.0, 0.0, 26.0, 25.6634492321771, 0.4234753086416709, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3705000.0000, 
sim time next is 3705600.0000, 
raw observation next is [1.333333333333333, 65.33333333333334, 0.0, 0.0, 26.0, 25.59543944602963, 0.4086955829939262, 0.0, 1.0, 47950.31605350872], 
processed observation next is [0.0, 0.9130434782608695, 0.4995383194829178, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6329532871691358, 0.6362318609979755, 0.0, 1.0, 0.22833483835004154], 
reward next is 0.7717, 
noisyNet noise sample is [array([0.6995344], dtype=float32), -0.6240425]. 
=============================================
[2019-04-04 06:12:05,114] A3C_AGENT_WORKER-Thread-12 INFO:Local step 48500, global step 777269: loss 3.3680
[2019-04-04 06:12:05,118] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 48500, global step 777271: learning rate 0.0000
[2019-04-04 06:12:06,085] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48500, global step 777762: loss 3.5256
[2019-04-04 06:12:06,087] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48500, global step 777762: learning rate 0.0000
[2019-04-04 06:12:07,214] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49000, global step 778350: loss 0.3192
[2019-04-04 06:12:07,215] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49000, global step 778350: learning rate 0.0000
[2019-04-04 06:12:07,306] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48500, global step 778390: loss 3.6990
[2019-04-04 06:12:07,307] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48500, global step 778390: learning rate 0.0000
[2019-04-04 06:12:07,343] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48500, global step 778406: loss 3.6241
[2019-04-04 06:12:07,344] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48500, global step 778406: learning rate 0.0000
[2019-04-04 06:12:07,405] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48500, global step 778431: loss 3.5375
[2019-04-04 06:12:07,405] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48500, global step 778431: learning rate 0.0000
[2019-04-04 06:12:08,127] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49000, global step 778799: loss 0.2897
[2019-04-04 06:12:08,129] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49000, global step 778799: learning rate 0.0000
[2019-04-04 06:12:08,338] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48500, global step 778909: loss 3.8030
[2019-04-04 06:12:08,340] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48500, global step 778909: learning rate 0.0000
[2019-04-04 06:12:09,940] A3C_AGENT_WORKER-Thread-13 INFO:Local step 48500, global step 779789: loss 3.3606
[2019-04-04 06:12:09,941] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 48500, global step 779789: learning rate 0.0000
[2019-04-04 06:12:10,843] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48500, global step 780302: loss 3.5076
[2019-04-04 06:12:10,845] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48500, global step 780302: learning rate 0.0000
[2019-04-04 06:12:10,948] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49000, global step 780345: loss 0.3307
[2019-04-04 06:12:10,949] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49000, global step 780346: learning rate 0.0000
[2019-04-04 06:12:11,487] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49000, global step 780618: loss 0.2741
[2019-04-04 06:12:11,491] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49000, global step 780619: learning rate 0.0000
[2019-04-04 06:12:13,964] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7749039e-09 4.4490804e-05 3.9683852e-28 4.3375103e-25 1.1686538e-19
 9.9995553e-01 2.4174023e-19], sum to 1.0000
[2019-04-04 06:12:13,973] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6934
[2019-04-04 06:12:13,988] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.84998289508305, 0.5733000812396906, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3781200.0000, 
sim time next is 3781800.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.98894029761963, 0.5689746964760617, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.665745024801636, 0.6896582321586872, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.070923], dtype=float32), 0.0024670349]. 
=============================================
[2019-04-04 06:12:14,733] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7374629e-09 2.8655777e-05 1.5974723e-25 5.5682260e-24 1.7441631e-18
 9.9997139e-01 3.2675232e-19], sum to 1.0000
[2019-04-04 06:12:14,733] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9298
[2019-04-04 06:12:14,767] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.166666666666666, 40.66666666666667, 114.3333333333333, 792.6666666666667, 26.0, 26.56645813340465, 0.5868459338279127, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4013400.0000, 
sim time next is 4014000.0000, 
raw observation next is [-8.0, 40.0, 115.5, 798.5, 26.0, 26.56571794931542, 0.5813518106606016, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.24099722991689754, 0.4, 0.385, 0.8823204419889503, 0.6666666666666666, 0.7138098291096183, 0.6937839368868671, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49866968], dtype=float32), 0.26882157]. 
=============================================
[2019-04-04 06:12:14,774] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.799194]
 [81.97513 ]
 [81.16762 ]
 [79.9728  ]
 [78.75788 ]], R is [[83.77246094]
 [83.93473816]
 [84.09539032]
 [84.25444031]
 [84.41189575]].
[2019-04-04 06:12:15,122] A3C_AGENT_WORKER-Thread-6 INFO:Local step 49000, global step 782459: loss 0.3270
[2019-04-04 06:12:15,123] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 49000, global step 782459: learning rate 0.0000
[2019-04-04 06:12:16,197] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49000, global step 782967: loss 0.3357
[2019-04-04 06:12:16,198] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49000, global step 782967: learning rate 0.0000
[2019-04-04 06:12:16,221] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49000, global step 782982: loss 0.3577
[2019-04-04 06:12:16,227] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49000, global step 782982: learning rate 0.0000
[2019-04-04 06:12:16,304] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1183890e-08 2.2884901e-04 3.9654958e-25 7.4226975e-24 2.4974312e-18
 9.9977118e-01 3.8980034e-18], sum to 1.0000
[2019-04-04 06:12:16,304] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3355
[2019-04-04 06:12:16,331] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.5, 49.0, 113.0, 775.0, 26.0, 26.36120169811101, 0.5936307284870668, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3925800.0000, 
sim time next is 3926400.0000, 
raw observation next is [-6.333333333333334, 49.0, 114.1666666666667, 780.8333333333334, 26.0, 26.37314560665301, 0.588467063567534, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.28716528162511545, 0.49, 0.38055555555555565, 0.8627992633517496, 0.6666666666666666, 0.697762133887751, 0.6961556878558447, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4558403], dtype=float32), 0.47751927]. 
=============================================
[2019-04-04 06:12:16,570] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6933766e-08 5.8138707e-05 8.1146887e-24 1.3437232e-21 1.3659231e-17
 9.9994183e-01 8.6285208e-17], sum to 1.0000
[2019-04-04 06:12:16,574] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5732
[2019-04-04 06:12:16,628] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 77.0, 19.33333333333333, 198.6666666666667, 26.0, 25.22511925983268, 0.3692579781352471, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3829200.0000, 
sim time next is 3829800.0000, 
raw observation next is [-5.0, 77.0, 33.66666666666666, 248.3333333333333, 26.0, 25.36977682409198, 0.3788150183661025, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.77, 0.1122222222222222, 0.2744014732965009, 0.6666666666666666, 0.6141480686743316, 0.6262716727887009, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6228091], dtype=float32), 0.46464205]. 
=============================================
[2019-04-04 06:12:19,429] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9828435e-07 1.6017504e-03 1.8512388e-22 3.8770969e-21 1.6921744e-16
 9.9839777e-01 4.2525686e-16], sum to 1.0000
[2019-04-04 06:12:19,430] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7811
[2019-04-04 06:12:19,430] A3C_AGENT_WORKER-Thread-11 INFO:Local step 49000, global step 784430: loss 0.3163
[2019-04-04 06:12:19,432] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 49000, global step 784430: learning rate 0.0000
[2019-04-04 06:12:19,449] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 68.0, 0.0, 0.0, 26.0, 25.36342304079366, 0.3984679127075108, 0.0, 1.0, 44931.80739794585], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3900600.0000, 
sim time next is 3901200.0000, 
raw observation next is [-2.666666666666667, 69.0, 0.0, 0.0, 26.0, 25.39162471550193, 0.3927092391568145, 0.0, 1.0, 30920.40174493905], 
processed observation next is [1.0, 0.13043478260869565, 0.38873499538319484, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6159687262918275, 0.6309030797189382, 0.0, 1.0, 0.14724000830923356], 
reward next is 0.8528, 
noisyNet noise sample is [array([1.236564], dtype=float32), -0.41857207]. 
=============================================
[2019-04-04 06:12:20,009] A3C_AGENT_WORKER-Thread-12 INFO:Local step 49000, global step 784695: loss 0.3287
[2019-04-04 06:12:20,013] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 49000, global step 784697: learning rate 0.0000
[2019-04-04 06:12:20,901] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49000, global step 785176: loss 0.2740
[2019-04-04 06:12:20,902] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49000, global step 785176: learning rate 0.0000
[2019-04-04 06:12:21,717] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49000, global step 785565: loss -0.7934
[2019-04-04 06:12:21,722] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49000, global step 785565: learning rate 0.0000
[2019-04-04 06:12:21,943] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49000, global step 785656: loss 0.3400
[2019-04-04 06:12:21,951] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49000, global step 785657: learning rate 0.0000
[2019-04-04 06:12:22,552] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49000, global step 785883: loss 0.2701
[2019-04-04 06:12:22,554] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49000, global step 785883: learning rate 0.0000
[2019-04-04 06:12:23,311] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49000, global step 786215: loss 0.3490
[2019-04-04 06:12:23,313] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49000, global step 786215: learning rate 0.0000
[2019-04-04 06:12:23,333] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6195082e-07 1.5198952e-03 8.6625125e-23 5.1169885e-21 4.4820298e-17
 9.9847990e-01 1.2495392e-16], sum to 1.0000
[2019-04-04 06:12:23,338] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4112
[2019-04-04 06:12:23,355] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 26.0, 24.88506612223065, 0.2680536518789357, 0.0, 1.0, 42333.46402087956], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3912000.0000, 
sim time next is 3912600.0000, 
raw observation next is [-6.833333333333334, 63.16666666666666, 0.0, 0.0, 26.0, 24.80077828578789, 0.2531035386903315, 0.0, 1.0, 42487.47367722607], 
processed observation next is [1.0, 0.2608695652173913, 0.27331486611265005, 0.6316666666666666, 0.0, 0.0, 0.6666666666666666, 0.5667315238156574, 0.5843678462301105, 0.0, 1.0, 0.20232130322488606], 
reward next is 0.7977, 
noisyNet noise sample is [array([0.01000679], dtype=float32), -0.49484453]. 
=============================================
[2019-04-04 06:12:24,336] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0639995e-07 3.4655922e-04 4.8624146e-23 1.8058768e-21 3.1809498e-17
 9.9965298e-01 6.6363263e-16], sum to 1.0000
[2019-04-04 06:12:24,336] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2743
[2019-04-04 06:12:24,352] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.69351874105588, 0.4247497358012894, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3897600.0000, 
sim time next is 3898200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.5500288488873, 0.3981939965438541, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6291690707406085, 0.6327313321812847, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2243277], dtype=float32), -0.5158511]. 
=============================================
[2019-04-04 06:12:24,973] A3C_AGENT_WORKER-Thread-13 INFO:Local step 49000, global step 786995: loss 0.3337
[2019-04-04 06:12:24,975] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 49000, global step 786995: learning rate 0.0000
[2019-04-04 06:12:25,293] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49500, global step 787137: loss 2.7429
[2019-04-04 06:12:25,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49500, global step 787137: learning rate 0.0000
[2019-04-04 06:12:25,639] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49500, global step 787286: loss 2.5121
[2019-04-04 06:12:25,642] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49500, global step 787287: learning rate 0.0000
[2019-04-04 06:12:25,892] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49000, global step 787413: loss 0.3343
[2019-04-04 06:12:25,897] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49000, global step 787413: learning rate 0.0000
[2019-04-04 06:12:27,771] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.2417529e-09 2.5449271e-04 5.5705949e-26 4.1243821e-24 7.3272693e-19
 9.9974555e-01 3.0708749e-19], sum to 1.0000
[2019-04-04 06:12:27,772] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9632
[2019-04-04 06:12:27,822] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.78709189107945, 0.5751106130311092, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3952800.0000, 
sim time next is 3953400.0000, 
raw observation next is [-6.0, 41.00000000000001, 0.0, 0.0, 26.0, 25.80538104606053, 0.5799407896152916, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.296398891966759, 0.4100000000000001, 0.0, 0.0, 0.6666666666666666, 0.650448420505044, 0.6933135965384305, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5387498], dtype=float32), 1.1081104]. 
=============================================
[2019-04-04 06:12:28,706] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49500, global step 788592: loss 2.5995
[2019-04-04 06:12:28,707] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49500, global step 788592: learning rate 0.0000
[2019-04-04 06:12:29,011] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49500, global step 788730: loss 2.8520
[2019-04-04 06:12:29,011] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49500, global step 788730: learning rate 0.0000
[2019-04-04 06:12:33,055] A3C_AGENT_WORKER-Thread-6 INFO:Local step 49500, global step 790808: loss 2.9746
[2019-04-04 06:12:33,056] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 49500, global step 790808: learning rate 0.0000
[2019-04-04 06:12:33,705] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49500, global step 791184: loss 3.0925
[2019-04-04 06:12:33,708] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49500, global step 791186: learning rate 0.0000
[2019-04-04 06:12:34,131] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0524347e-09 8.4900545e-05 1.5901043e-26 4.3118965e-25 1.1717587e-18
 9.9991512e-01 8.6140749e-19], sum to 1.0000
[2019-04-04 06:12:34,134] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0409
[2019-04-04 06:12:34,148] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49500, global step 791445: loss 2.8723
[2019-04-04 06:12:34,148] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49500, global step 791445: learning rate 0.0000
[2019-04-04 06:12:34,156] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333334, 30.66666666666667, 119.8333333333333, 808.1666666666666, 26.0, 26.73275664407376, 0.6444107884841558, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4101600.0000, 
sim time next is 4102200.0000, 
raw observation next is [0.0, 30.0, 121.0, 816.0, 26.0, 26.76764379842967, 0.4081250517698119, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.3, 0.4033333333333333, 0.901657458563536, 0.6666666666666666, 0.7306369832024725, 0.6360416839232707, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.912637], dtype=float32), 0.1318477]. 
=============================================
[2019-04-04 06:12:35,032] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.7812057e-07 8.0799666e-04 1.2269530e-20 5.0238789e-19 1.6355351e-15
 9.9919158e-01 8.5788479e-15], sum to 1.0000
[2019-04-04 06:12:35,032] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7020
[2019-04-04 06:12:35,046] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 38.0, 0.0, 0.0, 26.0, 25.04218951574663, 0.2847673027581989, 0.0, 1.0, 40731.18632411632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4071600.0000, 
sim time next is 4072200.0000, 
raw observation next is [-5.0, 38.5, 0.0, 0.0, 26.0, 25.04495589869435, 0.2811555249949327, 0.0, 1.0, 40678.61229560597], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.385, 0.0, 0.0, 0.6666666666666666, 0.5870796582245292, 0.5937185083316442, 0.0, 1.0, 0.19370767759812366], 
reward next is 0.8063, 
noisyNet noise sample is [array([0.6792571], dtype=float32), -0.51387495]. 
=============================================
[2019-04-04 06:12:35,475] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4401562e-10 1.0426938e-05 1.4562042e-29 7.6247929e-27 6.6482760e-21
 9.9998963e-01 8.3683102e-22], sum to 1.0000
[2019-04-04 06:12:35,476] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8942
[2019-04-04 06:12:35,490] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.833333333333333, 35.33333333333334, 93.33333333333333, 523.0, 26.0, 27.03508995232739, 0.7824987113340921, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4119000.0000, 
sim time next is 4119600.0000, 
raw observation next is [3.666666666666667, 35.66666666666667, 93.16666666666666, 480.0, 26.0, 27.26153807063741, 0.7997868262102088, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.564173591874423, 0.3566666666666667, 0.31055555555555553, 0.5303867403314917, 0.6666666666666666, 0.7717948392197842, 0.7665956087367363, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0127137], dtype=float32), 0.3116541]. 
=============================================
[2019-04-04 06:12:36,589] A3C_AGENT_WORKER-Thread-11 INFO:Local step 49500, global step 792827: loss 3.0348
[2019-04-04 06:12:36,590] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 49500, global step 792828: learning rate 0.0000
[2019-04-04 06:12:37,836] A3C_AGENT_WORKER-Thread-12 INFO:Local step 49500, global step 793171: loss 2.8757
[2019-04-04 06:12:37,837] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 49500, global step 793171: learning rate 0.0000
[2019-04-04 06:12:39,840] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49500, global step 793668: loss 3.1203
[2019-04-04 06:12:39,863] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49500, global step 793668: learning rate 0.0000
[2019-04-04 06:12:40,784] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49500, global step 793842: loss 2.9998
[2019-04-04 06:12:40,784] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49500, global step 793842: learning rate 0.0000
[2019-04-04 06:12:42,473] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50000, global step 794157: loss 0.0037
[2019-04-04 06:12:42,518] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50000, global step 794157: learning rate 0.0000
[2019-04-04 06:12:42,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.4671734e-07 1.0350102e-04 4.5060926e-24 1.2616414e-22 5.4132817e-18
 9.9989605e-01 1.4830915e-17], sum to 1.0000
[2019-04-04 06:12:42,631] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0244
[2019-04-04 06:12:42,677] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.25, 74.5, 0.0, 0.0, 26.0, 25.53303326583459, 0.4099569925614534, 0.0, 1.0, 19298.16890308583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4324200.0000, 
sim time next is 4324800.0000, 
raw observation next is [4.300000000000001, 74.0, 0.0, 0.0, 26.0, 25.57726594220734, 0.421043477359569, 0.0, 1.0, 18742.10101957603], 
processed observation next is [1.0, 0.043478260869565216, 0.5817174515235458, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6314388285172784, 0.640347825786523, 0.0, 1.0, 0.08924810009321918], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.87180156], dtype=float32), -0.07803697]. 
=============================================
[2019-04-04 06:12:42,966] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50000, global step 794277: loss 0.0061
[2019-04-04 06:12:42,971] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50000, global step 794277: learning rate 0.0000
[2019-04-04 06:12:43,950] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49500, global step 794501: loss 2.9515
[2019-04-04 06:12:43,960] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49500, global step 794504: learning rate 0.0000
[2019-04-04 06:12:44,469] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49500, global step 794649: loss 2.9982
[2019-04-04 06:12:44,474] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49500, global step 794649: learning rate 0.0000
[2019-04-04 06:12:45,735] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49500, global step 794981: loss 3.0946
[2019-04-04 06:12:45,754] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49500, global step 794981: learning rate 0.0000
[2019-04-04 06:12:46,348] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.2497412e-09 3.9792430e-05 2.4829207e-26 2.1530988e-24 7.2400886e-19
 9.9996018e-01 1.4159401e-19], sum to 1.0000
[2019-04-04 06:12:46,348] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9821
[2019-04-04 06:12:46,437] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.533333333333333, 52.00000000000001, 104.5, 646.0, 26.0, 26.50350021080495, 0.631160512828167, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4353600.0000, 
sim time next is 4354200.0000, 
raw observation next is [8.15, 49.5, 107.0, 677.0, 26.0, 26.68465766716195, 0.6675799548902387, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6883656509695293, 0.495, 0.3566666666666667, 0.7480662983425415, 0.6666666666666666, 0.7237214722634958, 0.7225266516300796, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22901283], dtype=float32), 0.515635]. 
=============================================
[2019-04-04 06:12:47,363] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0277508e-08 1.9937594e-05 6.2986611e-26 3.2266308e-24 2.9776718e-19
 9.9998009e-01 3.8616803e-18], sum to 1.0000
[2019-04-04 06:12:47,363] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1440
[2019-04-04 06:12:47,502] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.15, 73.0, 0.0, 0.0, 26.0, 25.81003991853602, 0.4612206618418135, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4308600.0000, 
sim time next is 4309200.0000, 
raw observation next is [5.1, 73.0, 0.0, 0.0, 26.0, 25.78378357375292, 0.4502594174357924, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.6038781163434903, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6486486311460767, 0.6500864724785974, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7200878], dtype=float32), -1.3114734]. 
=============================================
[2019-04-04 06:12:49,671] A3C_AGENT_WORKER-Thread-13 INFO:Local step 49500, global step 795927: loss 3.2224
[2019-04-04 06:12:49,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 49500, global step 795927: learning rate 0.0000
[2019-04-04 06:12:50,067] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50000, global step 795997: loss 0.0017
[2019-04-04 06:12:50,067] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50000, global step 795997: learning rate 0.0000
[2019-04-04 06:12:50,904] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2423343e-07 4.1447527e-04 8.2502915e-24 7.5474816e-23 5.2519678e-18
 9.9958497e-01 3.9301392e-17], sum to 1.0000
[2019-04-04 06:12:50,930] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6626
[2019-04-04 06:12:50,994] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.7, 62.0, 0.0, 0.0, 26.0, 25.43721449109039, 0.4575515196546411, 0.0, 1.0, 44877.17141905148], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4581000.0000, 
sim time next is 4581600.0000, 
raw observation next is [0.6000000000000001, 62.33333333333334, 0.0, 0.0, 26.0, 25.44949611327487, 0.4587861232466033, 0.0, 1.0, 31299.45021562336], 
processed observation next is [1.0, 0.0, 0.479224376731302, 0.6233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6207913427729057, 0.6529287077488678, 0.0, 1.0, 0.14904500102677792], 
reward next is 0.8510, 
noisyNet noise sample is [array([-1.3148793], dtype=float32), 1.3737674]. 
=============================================
[2019-04-04 06:12:51,147] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49500, global step 796225: loss 3.1770
[2019-04-04 06:12:51,148] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49500, global step 796225: learning rate 0.0000
[2019-04-04 06:12:51,905] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50000, global step 796372: loss 0.0014
[2019-04-04 06:12:51,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50000, global step 796372: learning rate 0.0000
[2019-04-04 06:12:56,744] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.43328049e-10 1.01727666e-04 8.28946518e-27 6.78226072e-25
 1.79281187e-19 9.99898314e-01 3.12924623e-19], sum to 1.0000
[2019-04-04 06:12:56,794] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6582
[2019-04-04 06:12:56,811] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 55.0, 162.5, 713.0, 26.0, 25.26405545117223, 0.3998581072880603, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4273200.0000, 
sim time next is 4273800.0000, 
raw observation next is [5.333333333333334, 54.5, 148.6666666666667, 749.3333333333334, 26.0, 25.24751568316531, 0.398877895935303, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6103416435826409, 0.545, 0.4955555555555557, 0.8279926335174954, 0.6666666666666666, 0.6039596402637759, 0.632959298645101, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2089859], dtype=float32), 0.7401769]. 
=============================================
[2019-04-04 06:13:02,113] A3C_AGENT_WORKER-Thread-6 INFO:Local step 50000, global step 798711: loss 0.0021
[2019-04-04 06:13:02,118] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 50000, global step 798711: learning rate 0.0000
[2019-04-04 06:13:03,038] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50000, global step 798953: loss 0.0066
[2019-04-04 06:13:03,038] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50000, global step 798953: learning rate 0.0000
[2019-04-04 06:13:04,810] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50000, global step 799366: loss 0.0010
[2019-04-04 06:13:04,847] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50000, global step 799369: learning rate 0.0000
[2019-04-04 06:13:05,934] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2524636e-08 1.7390419e-05 1.1413400e-24 2.1137118e-22 8.7880837e-19
 9.9998260e-01 4.7360644e-18], sum to 1.0000
[2019-04-04 06:13:05,934] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8307
[2019-04-04 06:13:05,984] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8666666666666667, 72.33333333333333, 18.5, 11.0, 26.0, 25.59858239775777, 0.4296958115645728, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4520400.0000, 
sim time next is 4521000.0000, 
raw observation next is [-0.8333333333333334, 72.66666666666667, 36.99999999999999, 22.0, 26.0, 25.50969225842847, 0.4238443590012607, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.43951985226223456, 0.7266666666666667, 0.12333333333333331, 0.02430939226519337, 0.6666666666666666, 0.6258076882023724, 0.6412814530004202, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4412163], dtype=float32), -0.81271577]. 
=============================================
[2019-04-04 06:13:06,052] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.74818]
 [79.20508]
 [79.90855]
 [80.87353]
 [80.86642]], R is [[78.52939606]
 [78.74410248]
 [78.95666504]
 [79.167099  ]
 [79.1829071 ]].
[2019-04-04 06:13:06,460] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4643944e-10 3.8586886e-06 2.3578763e-30 1.3337518e-27 6.1766390e-22
 9.9999619e-01 1.1336984e-21], sum to 1.0000
[2019-04-04 06:13:06,514] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6709
[2019-04-04 06:13:06,525] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.35, 49.0, 139.0, 813.0, 26.0, 26.92106847010722, 0.7862564498579774, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4627800.0000, 
sim time next is 4628400.0000, 
raw observation next is [4.466666666666667, 49.0, 149.6666666666667, 777.3333333333333, 26.0, 27.01896712914341, 0.8003526254680701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5863342566943676, 0.49, 0.49888888888888905, 0.8589318600368323, 0.6666666666666666, 0.7515805940952841, 0.7667842084893567, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0739565], dtype=float32), 0.553201]. 
=============================================
[2019-04-04 06:13:07,503] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-04 06:13:07,536] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:13:07,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:13:07,537] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:13:07,537] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:13:07,538] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:13:07,539] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:13:07,541] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run9
[2019-04-04 06:13:07,563] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run9
[2019-04-04 06:13:07,598] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run9
[2019-04-04 06:13:23,710] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.23349573], dtype=float32), 0.2115741]
[2019-04-04 06:13:23,710] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-7.1, 68.66666666666666, 22.5, 2.5, 26.0, 25.82144672559662, 0.3884025804296958, 1.0, 1.0, 0.0]
[2019-04-04 06:13:23,710] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 06:13:23,712] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.8399376e-09 2.9702784e-05 2.8187045e-25 2.6611961e-23 1.9371983e-18
 9.9997032e-01 4.4613366e-18], sampled 0.2993055649378955
[2019-04-04 06:14:00,207] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.23349573], dtype=float32), 0.2115741]
[2019-04-04 06:14:00,207] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.1141273426666667, 53.33565894666667, 217.22475705, 515.4482726166666, 26.0, 25.0133346708112, 0.2938541798937624, 1.0, 1.0, 21603.01230786397]
[2019-04-04 06:14:00,207] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 06:14:00,209] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.6504025e-10 2.6644482e-05 7.3021706e-28 7.2694931e-26 2.8521637e-20
 9.9997330e-01 3.8488701e-20], sampled 0.8360966857082476
[2019-04-04 06:16:12,483] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4788 239929441.9855 1605.2183
[2019-04-04 06:16:45,195] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7199 263398829.5475 1553.6277
[2019-04-04 06:16:51,339] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.23349573], dtype=float32), 0.2115741]
[2019-04-04 06:16:51,339] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [9.513461080666666, 20.33143908, 104.7181571333333, 738.9955587499999, 26.0, 27.17733726824092, 0.8519025308204499, 1.0, 1.0, 0.0]
[2019-04-04 06:16:51,339] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 06:16:51,340] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.0685353e-11 6.7196670e-06 1.2651810e-30 2.9858643e-28 5.0169817e-22
 9.9999332e-01 5.4078942e-22], sampled 0.7379929789035936
[2019-04-04 06:16:51,928] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7369 275785242.9737 1233.4296
[2019-04-04 06:16:52,966] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 800000, evaluation results [800000.0, 7241.71985929765, 263398829.54749188, 1553.6276591358205, 7353.4788476881195, 239929441.9854965, 1605.2183420378155, 7182.736938220529, 275785242.97368807, 1233.4296437659439]
[2019-04-04 06:16:53,837] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1394609e-09 2.0580296e-06 7.7552447e-28 3.1472053e-26 1.1152691e-20
 9.9999797e-01 2.8266713e-20], sum to 1.0000
[2019-04-04 06:16:53,837] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6414
[2019-04-04 06:16:53,889] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.95123141217798, 0.4609642289288224, 1.0, 1.0, 112845.6844620491], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4475400.0000, 
sim time next is 4476000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.98514757580842, 0.4709466221124307, 1.0, 1.0, 47745.69397372045], 
processed observation next is [1.0, 0.8260869565217391, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5820956313173683, 0.6569822073708103, 1.0, 1.0, 0.2273604474939069], 
reward next is 0.7726, 
noisyNet noise sample is [array([0.05272601], dtype=float32), 0.7275995]. 
=============================================
[2019-04-04 06:16:53,894] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[89.03829 ]
 [89.15325 ]
 [89.414986]
 [88.79733 ]
 [88.20828 ]], R is [[88.66846466]
 [88.24442291]
 [87.76344299]
 [87.61771393]
 [87.64617157]].
[2019-04-04 06:16:57,443] A3C_AGENT_WORKER-Thread-11 INFO:Local step 50000, global step 800905: loss 0.0032
[2019-04-04 06:16:57,471] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 50000, global step 800905: learning rate 0.0000
[2019-04-04 06:16:57,647] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2597765e-08 4.9157470e-04 7.1387218e-24 1.5976063e-22 5.6017043e-18
 9.9950838e-01 7.1435930e-18], sum to 1.0000
[2019-04-04 06:16:57,648] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7885
[2019-04-04 06:16:57,686] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8666666666666667, 71.0, 0.0, 0.0, 26.0, 25.31388189097424, 0.4071070719695418, 0.0, 1.0, 41318.51509575532], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4512000.0000, 
sim time next is 4512600.0000, 
raw observation next is [-0.9, 71.0, 0.0, 0.0, 26.0, 25.30044538486537, 0.4022070225935473, 0.0, 1.0, 41236.80676242459], 
processed observation next is [1.0, 0.21739130434782608, 0.43767313019390586, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6083704487387808, 0.6340690075311824, 0.0, 1.0, 0.19636574648773614], 
reward next is 0.8036, 
noisyNet noise sample is [array([0.70328593], dtype=float32), -0.23525345]. 
=============================================
[2019-04-04 06:16:58,843] A3C_AGENT_WORKER-Thread-12 INFO:Local step 50000, global step 801208: loss 0.0017
[2019-04-04 06:16:58,853] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 50000, global step 801208: learning rate 0.0000
[2019-04-04 06:17:00,816] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50000, global step 801582: loss 0.0017
[2019-04-04 06:17:00,830] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50000, global step 801582: learning rate 0.0000
[2019-04-04 06:17:01,239] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50000, global step 801659: loss 0.0009
[2019-04-04 06:17:01,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50000, global step 801659: learning rate 0.0000
[2019-04-04 06:17:03,882] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50500, global step 802236: loss 0.5597
[2019-04-04 06:17:03,885] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50500, global step 802236: learning rate 0.0000
[2019-04-04 06:17:04,557] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50000, global step 802366: loss 0.0009
[2019-04-04 06:17:04,569] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50000, global step 802366: learning rate 0.0000
[2019-04-04 06:17:04,959] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50500, global step 802455: loss 0.4899
[2019-04-04 06:17:04,987] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50500, global step 802455: learning rate 0.0000
[2019-04-04 06:17:05,271] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50000, global step 802523: loss 0.0016
[2019-04-04 06:17:05,271] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50000, global step 802523: learning rate 0.0000
[2019-04-04 06:17:08,881] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50000, global step 803213: loss 0.0025
[2019-04-04 06:17:08,882] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50000, global step 803213: learning rate 0.0000
[2019-04-04 06:17:10,100] A3C_AGENT_WORKER-Thread-13 INFO:Local step 50000, global step 803472: loss 0.0036
[2019-04-04 06:17:10,144] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 50000, global step 803473: learning rate 0.0000
[2019-04-04 06:17:12,383] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50000, global step 804021: loss 0.0019
[2019-04-04 06:17:12,397] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50000, global step 804021: learning rate 0.0000
[2019-04-04 06:17:12,562] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50500, global step 804067: loss 0.4546
[2019-04-04 06:17:12,563] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50500, global step 804067: learning rate 0.0000
[2019-04-04 06:17:12,779] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5756461e-07 3.4779185e-04 9.3830161e-24 4.2704880e-22 1.6058630e-17
 9.9965191e-01 1.9686041e-16], sum to 1.0000
[2019-04-04 06:17:12,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2335
[2019-04-04 06:17:12,869] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 74.0, 0.0, 0.0, 26.0, 25.09679069096208, 0.364104907173487, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4606200.0000, 
sim time next is 4606800.0000, 
raw observation next is [-2.333333333333333, 73.0, 20.5, 28.49999999999999, 26.0, 25.26377847338908, 0.3859265800852148, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3979686057248385, 0.73, 0.06833333333333333, 0.03149171270718231, 0.6666666666666666, 0.6053148727824235, 0.6286421933617382, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10711619], dtype=float32), -0.03428351]. 
=============================================
[2019-04-04 06:17:15,504] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50500, global step 804735: loss 0.5435
[2019-04-04 06:17:15,620] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50500, global step 804735: learning rate 0.0000
[2019-04-04 06:17:20,012] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2712769e-07 9.0173783e-04 2.1115774e-21 6.2053739e-20 9.5436483e-16
 9.9909806e-01 1.5119849e-15], sum to 1.0000
[2019-04-04 06:17:20,013] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4554
[2019-04-04 06:17:20,023] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666666, 47.66666666666666, 0.0, 0.0, 26.0, 25.36087203199653, 0.315199987373151, 0.0, 1.0, 86472.38594549918], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4930800.0000, 
sim time next is 4931400.0000, 
raw observation next is [-0.8333333333333334, 48.83333333333334, 0.0, 0.0, 26.0, 25.33146974909534, 0.3210874268515309, 0.0, 1.0, 58919.32108012676], 
processed observation next is [1.0, 0.043478260869565216, 0.43951985226223456, 0.48833333333333345, 0.0, 0.0, 0.6666666666666666, 0.6109558124246117, 0.6070291422838436, 0.0, 1.0, 0.28056819561965124], 
reward next is 0.7194, 
noisyNet noise sample is [array([-1.3449173], dtype=float32), 1.4822248]. 
=============================================
[2019-04-04 06:17:22,419] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2372730e-09 5.8287485e-05 1.8272180e-28 5.8924236e-26 2.7781943e-20
 9.9994171e-01 1.1554149e-19], sum to 1.0000
[2019-04-04 06:17:22,421] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6250
[2019-04-04 06:17:22,459] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3, 49.5, 283.0, 308.0, 26.0, 25.02263287053498, 0.3506287411114891, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4879800.0000, 
sim time next is 4880400.0000, 
raw observation next is [0.5333333333333332, 48.66666666666667, 282.6666666666667, 321.6666666666667, 26.0, 25.04653328778171, 0.3523093860295414, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4773776546629733, 0.4866666666666667, 0.9422222222222223, 0.3554327808471455, 0.6666666666666666, 0.5872111073151425, 0.6174364620098471, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3946445], dtype=float32), -0.41339967]. 
=============================================
[2019-04-04 06:17:22,590] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2939158e-07 1.7343687e-04 2.0309838e-23 4.9068487e-21 1.2270926e-17
 9.9982613e-01 1.6533145e-16], sum to 1.0000
[2019-04-04 06:17:22,591] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6342
[2019-04-04 06:17:22,623] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 60.33333333333334, 0.0, 0.0, 26.0, 25.56097917457313, 0.4719973982159568, 0.0, 1.0, 57576.63282570455], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4672200.0000, 
sim time next is 4672800.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 26.0, 25.48368370370287, 0.4689119602478867, 0.0, 1.0, 83768.85957237695], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6236403086419058, 0.6563039867492956, 0.0, 1.0, 0.3988993312970331], 
reward next is 0.6011, 
noisyNet noise sample is [array([-0.91825646], dtype=float32), -0.5465004]. 
=============================================
[2019-04-04 06:17:23,061] A3C_AGENT_WORKER-Thread-6 INFO:Local step 50500, global step 806966: loss 0.5499
[2019-04-04 06:17:23,062] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 50500, global step 806967: learning rate 0.0000
[2019-04-04 06:17:23,258] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50500, global step 807062: loss 0.4141
[2019-04-04 06:17:23,260] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50500, global step 807062: learning rate 0.0000
[2019-04-04 06:17:24,223] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.5163101e-07 3.0406879e-04 6.9715528e-22 3.7388454e-20 5.4370950e-16
 9.9969530e-01 3.1694456e-16], sum to 1.0000
[2019-04-04 06:17:24,223] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0177
[2019-04-04 06:17:24,245] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.41478800641013, 0.2967142673784904, 0.0, 1.0, 65404.68242954472], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4934400.0000, 
sim time next is 4935000.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.30710418255406, 0.2983895448291753, 0.0, 1.0, 76017.77625550008], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6089253485461716, 0.5994631816097251, 0.0, 1.0, 0.3619894107404766], 
reward next is 0.6380, 
noisyNet noise sample is [array([-1.1288359], dtype=float32), -0.4144256]. 
=============================================
[2019-04-04 06:17:24,253] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[73.96246]
 [73.83819]
 [73.71852]
 [73.89775]
 [74.00386]], R is [[74.27145386]
 [74.21729279]
 [74.47512054]
 [74.73036957]
 [74.98307037]].
[2019-04-04 06:17:24,501] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50500, global step 807635: loss 0.5389
[2019-04-04 06:17:24,502] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50500, global step 807635: learning rate 0.0000
[2019-04-04 06:17:27,980] A3C_AGENT_WORKER-Thread-11 INFO:Local step 50500, global step 809087: loss 0.5103
[2019-04-04 06:17:27,982] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 50500, global step 809087: learning rate 0.0000
[2019-04-04 06:17:28,253] A3C_AGENT_WORKER-Thread-12 INFO:Local step 50500, global step 809236: loss 0.6292
[2019-04-04 06:17:28,259] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 50500, global step 809236: learning rate 0.0000
[2019-04-04 06:17:28,679] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50500, global step 809447: loss 0.5134
[2019-04-04 06:17:28,688] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50500, global step 809452: learning rate 0.0000
[2019-04-04 06:17:29,410] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50500, global step 809797: loss 0.5089
[2019-04-04 06:17:29,411] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50500, global step 809797: learning rate 0.0000
[2019-04-04 06:17:29,782] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:29,782] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:29,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run7
[2019-04-04 06:17:30,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:30,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:30,482] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run7
[2019-04-04 06:17:30,759] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50500, global step 810384: loss 0.4661
[2019-04-04 06:17:30,763] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50500, global step 810384: learning rate 0.0000
[2019-04-04 06:17:31,121] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50500, global step 810520: loss 0.4867
[2019-04-04 06:17:31,121] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50500, global step 810520: learning rate 0.0000
[2019-04-04 06:17:32,634] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50500, global step 811003: loss 0.6189
[2019-04-04 06:17:32,635] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50500, global step 811003: learning rate 0.0000
[2019-04-04 06:17:33,085] A3C_AGENT_WORKER-Thread-13 INFO:Local step 50500, global step 811166: loss 0.6404
[2019-04-04 06:17:33,086] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 50500, global step 811166: learning rate 0.0000
[2019-04-04 06:17:34,374] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:34,374] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:34,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run7
[2019-04-04 06:17:34,814] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50500, global step 811860: loss 0.5295
[2019-04-04 06:17:34,815] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50500, global step 811860: learning rate 0.0000
[2019-04-04 06:17:35,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:35,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:35,647] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run7
[2019-04-04 06:17:38,928] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.3068404e-08 9.5551943e-05 6.6949874e-23 2.3266159e-20 2.2904973e-16
 9.9990439e-01 3.1879840e-16], sum to 1.0000
[2019-04-04 06:17:38,928] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4223
[2019-04-04 06:17:38,948] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8333333333333334, 48.83333333333334, 0.0, 0.0, 26.0, 25.33017811471897, 0.3210001554460517, 0.0, 1.0, 58925.1081896035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4931400.0000, 
sim time next is 4932000.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.35106322794194, 0.3477768093744122, 0.0, 1.0, 45566.04925628424], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6125886023284949, 0.615925603124804, 0.0, 1.0, 0.21698118693468685], 
reward next is 0.7830, 
noisyNet noise sample is [array([0.40450692], dtype=float32), 0.30348256]. 
=============================================
[2019-04-04 06:17:38,961] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[75.70358]
 [76.11806]
 [76.31265]
 [76.39053]
 [77.1331 ]], R is [[75.77256012]
 [75.73423767]
 [75.56510162]
 [75.44102478]
 [75.55253601]].
[2019-04-04 06:17:40,762] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:40,762] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:40,766] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run7
[2019-04-04 06:17:40,913] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:40,919] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:40,926] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run7
[2019-04-04 06:17:41,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:41,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:41,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run7
[2019-04-04 06:17:45,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:45,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:45,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run7
[2019-04-04 06:17:46,051] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:46,051] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:46,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run7
[2019-04-04 06:17:46,559] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:46,560] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:46,563] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run7
[2019-04-04 06:17:47,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:47,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:47,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run7
[2019-04-04 06:17:47,819] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8906218e-03 5.8187373e-02 7.5485653e-09 3.3834578e-08 8.5982100e-07
 9.3891841e-01 2.7244625e-06], sum to 1.0000
[2019-04-04 06:17:47,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1742
[2019-04-04 06:17:47,857] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 26.0, 19.79309330263577, -0.8658175080472806, 0.0, 1.0, 45812.88074948737], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4200.0000, 
sim time next is 4800.0000, 
raw observation next is [7.200000000000001, 96.0, 0.0, 0.0, 26.0, 19.94391433406363, -0.846351700404918, 0.0, 1.0, 45066.61023796149], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.6666666666666666, 0.1619928611719693, 0.217882766531694, 0.0, 1.0, 0.2146029058950547], 
reward next is 0.7854, 
noisyNet noise sample is [array([-0.12535174], dtype=float32), 0.19098304]. 
=============================================
[2019-04-04 06:17:49,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:49,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:49,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run7
[2019-04-04 06:17:50,146] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7243987e-09 1.5946684e-05 6.2387800e-27 5.8934176e-25 1.2530198e-19
 9.9998403e-01 1.3936650e-19], sum to 1.0000
[2019-04-04 06:17:50,146] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0652
[2019-04-04 06:17:50,167] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.799999999999999, 19.0, 0.0, 0.0, 26.0, 26.88270275917261, 0.8037387131059955, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5089200.0000, 
sim time next is 5089800.0000, 
raw observation next is [8.75, 19.0, 0.0, 0.0, 26.0, 26.8275423285724, 0.7916268966595373, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7049861495844876, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7356285273810332, 0.7638756322198458, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12483517], dtype=float32), -0.92822343]. 
=============================================
[2019-04-04 06:17:50,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:50,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:50,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run7
[2019-04-04 06:17:51,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:51,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:51,864] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run7
[2019-04-04 06:17:51,923] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:51,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:51,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run7
[2019-04-04 06:17:53,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:17:53,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:53,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run7
[2019-04-04 06:18:02,279] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8078206e-08 1.3224281e-04 2.2208493e-25 7.2505417e-23 2.3047925e-19
 9.9986768e-01 1.9646303e-18], sum to 1.0000
[2019-04-04 06:18:02,293] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9285
[2019-04-04 06:18:02,378] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 26.66666666666666, 0.0, 26.0, 22.55460534052201, -0.2624309065402607, 0.0, 1.0, 64048.42369027868], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 31800.0000, 
sim time next is 32400.0000, 
raw observation next is [7.7, 93.0, 29.5, 0.0, 26.0, 22.73171182471858, -0.222913328600391, 0.0, 1.0, 60580.30904095196], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.09833333333333333, 0.0, 0.6666666666666666, 0.39430931872654834, 0.425695557133203, 0.0, 1.0, 0.2884776620997712], 
reward next is 0.7115, 
noisyNet noise sample is [array([-0.4682211], dtype=float32), 0.95365155]. 
=============================================
[2019-04-04 06:18:07,051] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.00681765e-08 2.58817927e-05 3.30342924e-25 1.90079017e-23
 2.53580282e-19 9.99974132e-01 2.14579688e-18], sum to 1.0000
[2019-04-04 06:18:07,052] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3969
[2019-04-04 06:18:07,099] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 86.0, 187.0, 24.5, 26.0, 25.32646957074257, 0.3040295553043096, 1.0, 1.0, 41635.36302870055], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 126000.0000, 
sim time next is 126600.0000, 
raw observation next is [-7.9, 81.83333333333334, 186.0, 20.66666666666666, 26.0, 25.34715948356043, 0.3113204195169277, 1.0, 1.0, 44125.76477164563], 
processed observation next is [1.0, 0.4782608695652174, 0.24376731301939059, 0.8183333333333335, 0.62, 0.022836095764272552, 0.6666666666666666, 0.6122632902967023, 0.6037734731723092, 1.0, 1.0, 0.2101226893887887], 
reward next is 0.7899, 
noisyNet noise sample is [array([0.2939232], dtype=float32), 0.641017]. 
=============================================
[2019-04-04 06:18:08,943] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5944684e-09 1.3516525e-05 8.8889654e-26 1.0832241e-24 7.8786362e-20
 9.9998653e-01 1.7336092e-18], sum to 1.0000
[2019-04-04 06:18:08,943] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3809
[2019-04-04 06:18:09,010] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.1, 68.66666666666666, 22.5, 2.5, 26.0, 25.81771882096145, 0.387817326325461, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 146400.0000, 
sim time next is 147000.0000, 
raw observation next is [-7.199999999999999, 69.83333333333334, 18.0, 2.0, 26.0, 25.46147670274628, 0.3616811807813434, 1.0, 1.0, 119530.7363133068], 
processed observation next is [1.0, 0.6956521739130435, 0.26315789473684215, 0.6983333333333335, 0.06, 0.0022099447513812156, 0.6666666666666666, 0.6217897252288566, 0.6205603935937811, 1.0, 1.0, 0.5691939824443181], 
reward next is 0.4308, 
noisyNet noise sample is [array([0.57101315], dtype=float32), 0.31776267]. 
=============================================
[2019-04-04 06:18:09,014] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[83.73807]
 [84.22521]
 [84.69747]
 [85.22925]
 [85.81861]], R is [[83.19316864]
 [83.36123657]
 [83.52762604]
 [83.69235229]
 [83.8554306 ]].
[2019-04-04 06:18:14,036] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5570924e-08 1.3733917e-04 1.2102621e-24 7.5001474e-23 5.4114073e-18
 9.9986255e-01 1.8984247e-17], sum to 1.0000
[2019-04-04 06:18:14,036] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4688
[2019-04-04 06:18:14,136] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.35115042121945, 0.2818327944821115, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 242400.0000, 
sim time next is 243000.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.17638258961341, 0.2443112712406353, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5980318824677843, 0.5814370904135451, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0695126], dtype=float32), -1.3393247]. 
=============================================
[2019-04-04 06:18:14,153] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[82.74166]
 [82.1208 ]
 [82.16154]
 [82.25773]
 [81.9902 ]], R is [[82.44577026]
 [82.621315  ]
 [82.79510498]
 [82.96715546]
 [82.48967743]].
[2019-04-04 06:18:33,107] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2036364e-08 4.7832605e-06 5.5505387e-26 7.1717672e-24 1.2435537e-18
 9.9999523e-01 2.6541504e-19], sum to 1.0000
[2019-04-04 06:18:33,120] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6050
[2019-04-04 06:18:33,163] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 42.0, 70.0, 477.0, 26.0, 26.26698164258647, 0.5286488384571079, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 315000.0000, 
sim time next is 315600.0000, 
raw observation next is [-9.5, 42.0, 62.33333333333334, 437.5, 26.0, 26.31216862155506, 0.524594860000917, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.2077777777777778, 0.48342541436464087, 0.6666666666666666, 0.6926807184629217, 0.674864953333639, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43810564], dtype=float32), 0.05057472]. 
=============================================
[2019-04-04 06:18:48,645] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6463597e-08 1.3802343e-04 1.1882481e-24 1.1363251e-22 1.7850090e-18
 9.9986196e-01 2.1005297e-17], sum to 1.0000
[2019-04-04 06:18:48,645] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2817
[2019-04-04 06:18:48,673] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 24.58916596622456, 0.1690088103355236, 0.0, 1.0, 40863.40705112919], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 543600.0000, 
sim time next is 544200.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 24.51892709558649, 0.1589714462884289, 0.0, 1.0, 40949.25479356242], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5432439246322076, 0.552990482096143, 0.0, 1.0, 0.19499645139791627], 
reward next is 0.8050, 
noisyNet noise sample is [array([0.6264711], dtype=float32), -0.632842]. 
=============================================
[2019-04-04 06:19:06,914] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3205556e-08 9.8523751e-06 3.7327407e-25 1.4857917e-23 6.2557876e-19
 9.9999011e-01 3.4994488e-18], sum to 1.0000
[2019-04-04 06:19:06,915] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3114
[2019-04-04 06:19:06,955] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 75.0, 99.0, 0.0, 26.0, 25.51681117791964, 0.2924334296013542, 1.0, 1.0, 27791.23479220192], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 822600.0000, 
sim time next is 823200.0000, 
raw observation next is [-4.5, 76.33333333333333, 97.66666666666666, 0.0, 26.0, 25.43876360062288, 0.2891471608115824, 1.0, 1.0, 27879.60439589681], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7633333333333333, 0.32555555555555554, 0.0, 0.6666666666666666, 0.6198969667185734, 0.5963823869371941, 1.0, 1.0, 0.13276002093284195], 
reward next is 0.8672, 
noisyNet noise sample is [array([-0.5100641], dtype=float32), -0.7825538]. 
=============================================
[2019-04-04 06:19:15,745] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6956071e-10 2.0658442e-06 1.7222827e-28 6.7473677e-27 1.6770033e-21
 9.9999797e-01 9.2287450e-21], sum to 1.0000
[2019-04-04 06:19:15,745] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3458
[2019-04-04 06:19:15,771] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.7, 94.0, 0.0, 0.0, 26.0, 25.48011423920481, 0.3524003881262924, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 927000.0000, 
sim time next is 927600.0000, 
raw observation next is [4.600000000000001, 94.66666666666667, 0.0, 0.0, 26.0, 25.33705819835486, 0.3268431547805539, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5900277008310251, 0.9466666666666668, 0.0, 0.0, 0.6666666666666666, 0.6114215165295717, 0.6089477182601847, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.61833334], dtype=float32), -0.75617325]. 
=============================================
[2019-04-04 06:19:17,964] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2725598e-08 1.6081536e-05 3.1412290e-23 3.6889000e-21 5.9114057e-18
 9.9998391e-01 1.5597548e-17], sum to 1.0000
[2019-04-04 06:19:17,964] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1714
[2019-04-04 06:19:18,019] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.9930620102489, 0.2482400263511124, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 891000.0000, 
sim time next is 891600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.13555910162744, 0.2494312091433544, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.59462992513562, 0.5831437363811182, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0345564], dtype=float32), -0.46028516]. 
=============================================
[2019-04-04 06:19:19,596] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9195691e-10 1.4265478e-05 2.5033687e-31 2.6032147e-28 2.8928145e-23
 9.9998569e-01 7.7787279e-22], sum to 1.0000
[2019-04-04 06:19:19,596] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3444
[2019-04-04 06:19:19,626] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.6, 65.0, 217.5, 266.0, 26.0, 27.17890211020765, 0.6305334108251802, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1080000.0000, 
sim time next is 1080600.0000, 
raw observation next is [16.88333333333334, 63.5, 205.3333333333333, 283.0, 26.0, 27.04366137955662, 0.8663217282759034, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.9302862419205914, 0.635, 0.6844444444444443, 0.312707182320442, 0.6666666666666666, 0.753638448296385, 0.7887739094253011, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05469556], dtype=float32), 0.61992824]. 
=============================================
[2019-04-04 06:19:22,107] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9397473e-08 7.1125578e-06 3.9950446e-25 9.0893508e-24 1.1311822e-18
 9.9999285e-01 4.5472273e-18], sum to 1.0000
[2019-04-04 06:19:22,107] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2355
[2019-04-04 06:19:22,138] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 82.66666666666667, 52.83333333333333, 0.0, 26.0, 25.47927906868908, 0.2881414135407098, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 898800.0000, 
sim time next is 899400.0000, 
raw observation next is [1.1, 83.33333333333333, 57.66666666666666, 0.0, 26.0, 25.43387517819533, 0.2876799390094029, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8333333333333333, 0.19222222222222218, 0.0, 0.6666666666666666, 0.6194895981829441, 0.5958933130031343, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9604209], dtype=float32), -0.51103663]. 
=============================================
[2019-04-04 06:19:23,173] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.3594948e-10 2.3463697e-06 9.7284722e-29 2.3667337e-26 6.9922808e-22
 9.9999762e-01 6.8095479e-21], sum to 1.0000
[2019-04-04 06:19:23,173] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7146
[2019-04-04 06:19:23,187] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.93333333333334, 81.0, 0.0, 0.0, 26.0, 25.68622608184874, 0.6220240837385966, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1063200.0000, 
sim time next is 1063800.0000, 
raw observation next is [12.75, 81.5, 0.0, 0.0, 26.0, 25.84790422118594, 0.6240824516997597, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.8157894736842106, 0.815, 0.0, 0.0, 0.6666666666666666, 0.6539920184321616, 0.70802748389992, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3085688], dtype=float32), 1.8126392]. 
=============================================
[2019-04-04 06:19:25,572] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6738106e-11 4.6618896e-07 1.8284993e-31 3.6075200e-28 1.3555330e-23
 9.9999952e-01 1.8111940e-22], sum to 1.0000
[2019-04-04 06:19:25,573] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5850
[2019-04-04 06:19:25,588] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 100.0, 89.0, 0.0, 26.0, 24.79277957964235, 0.4511744427698259, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1259400.0000, 
sim time next is 1260000.0000, 
raw observation next is [13.8, 100.0, 86.0, 0.0, 26.0, 24.77696482460115, 0.4488755749319795, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.2866666666666667, 0.0, 0.6666666666666666, 0.5647470687167626, 0.6496251916439931, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00405981], dtype=float32), 1.0906409]. 
=============================================
[2019-04-04 06:19:25,591] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[100.93221 ]
 [100.883705]
 [100.84764 ]
 [100.665   ]
 [ 99.74968 ]], R is [[100.98526764]
 [100.97541809]
 [100.96566772]
 [100.95600891]
 [100.94644928]].
[2019-04-04 06:19:30,065] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.0657202e-10 1.1695774e-06 3.4423696e-28 5.6082365e-26 7.5803651e-22
 9.9999881e-01 3.9577484e-20], sum to 1.0000
[2019-04-04 06:19:30,067] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2754
[2019-04-04 06:19:30,076] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 26.10095020766784, 0.6387238240720966, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1039200.0000, 
sim time next is 1039800.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 26.06472322028446, 0.6319364704010387, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6720602683570384, 0.7106454901336795, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2039636], dtype=float32), -0.18134266]. 
=============================================
[2019-04-04 06:19:33,098] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.7414015e-07 3.5628065e-05 1.5040074e-22 1.0045265e-20 7.1350588e-17
 9.9996412e-01 5.3981071e-16], sum to 1.0000
[2019-04-04 06:19:33,099] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1986
[2019-04-04 06:19:33,109] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.51666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 24.55067922493936, 0.3654513520551457, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1199400.0000, 
sim time next is 1200000.0000, 
raw observation next is [17.33333333333334, 69.66666666666667, 0.0, 0.0, 26.0, 24.52554860983278, 0.3596607959925437, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.9427516158818101, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.5437957174860649, 0.6198869319975145, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13159421], dtype=float32), 0.92219645]. 
=============================================
[2019-04-04 06:19:33,124] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.445366]
 [75.41517 ]
 [75.35703 ]
 [75.350365]
 [75.35349 ]], R is [[75.75635529]
 [75.99879456]
 [76.23880768]
 [76.47641754]
 [76.71165466]].
[2019-04-04 06:19:35,016] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.07209294e-09 1.55737837e-06 1.03482575e-27 3.17997325e-25
 1.18073144e-19 9.99998450e-01 2.00068430e-19], sum to 1.0000
[2019-04-04 06:19:35,017] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6087
[2019-04-04 06:19:35,038] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 26.0, 25.65304943569425, 0.6236652532665542, 0.0, 1.0, 20561.29429143689], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1135800.0000, 
sim time next is 1136400.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.64981425533423, 0.6213896459185754, 0.0, 1.0, 23863.72207321097], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6374845212778526, 0.7071298819728584, 0.0, 1.0, 0.1136367717771951], 
reward next is 0.8864, 
noisyNet noise sample is [array([1.4165798], dtype=float32), -1.1172106]. 
=============================================
[2019-04-04 06:19:41,445] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3035821e-09 4.7343733e-06 6.0760552e-28 4.7279818e-26 3.8422632e-21
 9.9999523e-01 9.1546360e-20], sum to 1.0000
[2019-04-04 06:19:41,451] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6571
[2019-04-04 06:19:41,573] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 9.0, 0.0, 26.0, 25.05375484092359, 0.4296938118634375, 1.0, 1.0, 127877.2384800802], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1357200.0000, 
sim time next is 1357800.0000, 
raw observation next is [0.5, 96.0, 5.999999999999998, 0.0, 26.0, 24.38308974745081, 0.4368673853398315, 1.0, 1.0, 197326.727735802], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.019999999999999993, 0.0, 0.6666666666666666, 0.5319241456209008, 0.6456224617799439, 1.0, 1.0, 0.9396510844562], 
reward next is 0.0603, 
noisyNet noise sample is [array([-0.31444052], dtype=float32), -0.8435699]. 
=============================================
[2019-04-04 06:19:46,927] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5151948e-10 1.1786785e-06 5.1034045e-31 1.7061195e-28 2.3601239e-22
 9.9999881e-01 1.7704314e-22], sum to 1.0000
[2019-04-04 06:19:46,928] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9391
[2019-04-04 06:19:46,939] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.066666666666666, 66.33333333333334, 83.33333333333334, 694.6666666666667, 26.0, 26.38150085396911, 0.6844326538871474, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1518000.0000, 
sim time next is 1518600.0000, 
raw observation next is [9.533333333333333, 64.66666666666666, 81.66666666666666, 688.3333333333333, 26.0, 26.49890315135366, 0.7083654259055728, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.7266851338873501, 0.6466666666666666, 0.2722222222222222, 0.7605893186003683, 0.6666666666666666, 0.7082419292794716, 0.7361218086351909, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.62991494], dtype=float32), -0.071518704]. 
=============================================
[2019-04-04 06:19:50,476] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.4884286e-09 9.2261098e-06 6.9791048e-27 5.8410397e-25 3.4825351e-20
 9.9999082e-01 1.1349273e-18], sum to 1.0000
[2019-04-04 06:19:50,476] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2199
[2019-04-04 06:19:50,512] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.00000000000001, 0.0, 0.0, 26.0, 25.22570183173767, 0.4614659657130392, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1707000.0000, 
sim time next is 1707600.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.26561854107821, 0.4539240098581332, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6054682117565177, 0.6513080032860444, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.5027324], dtype=float32), -0.8892419]. 
=============================================
[2019-04-04 06:19:54,618] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7969893e-09 3.2773835e-06 8.8027663e-27 5.2148755e-25 3.8370120e-20
 9.9999666e-01 1.2841998e-19], sum to 1.0000
[2019-04-04 06:19:54,623] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3269
[2019-04-04 06:19:54,636] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.416666666666667, 79.5, 0.0, 0.0, 26.0, 25.46960393664853, 0.4883101750919177, 0.0, 1.0, 66943.86124637657], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1579800.0000, 
sim time next is 1580400.0000, 
raw observation next is [5.5, 79.0, 0.0, 0.0, 26.0, 25.45265509301618, 0.4923488882121279, 0.0, 1.0, 55802.1572673192], 
processed observation next is [1.0, 0.30434782608695654, 0.6149584487534627, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6210545910846816, 0.6641162960707093, 0.0, 1.0, 0.2657245584158057], 
reward next is 0.7343, 
noisyNet noise sample is [array([-1.194192], dtype=float32), -0.6755632]. 
=============================================
[2019-04-04 06:20:04,142] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6891625e-09 5.3317473e-07 3.0452149e-28 5.1400929e-26 8.5585833e-21
 9.9999940e-01 1.1987134e-19], sum to 1.0000
[2019-04-04 06:20:04,170] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0189
[2019-04-04 06:20:04,188] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.25, 93.5, 0.0, 0.0, 26.0, 25.36254282977009, 0.4795915284125747, 0.0, 1.0, 45727.17319783988], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1726200.0000, 
sim time next is 1726800.0000, 
raw observation next is [0.3333333333333333, 93.0, 0.0, 0.0, 26.0, 25.35972591543482, 0.4775027574178108, 0.0, 1.0, 44091.09238731928], 
processed observation next is [1.0, 1.0, 0.4718374884579871, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6133104929529015, 0.6591675858059369, 0.0, 1.0, 0.20995758279675847], 
reward next is 0.7900, 
noisyNet noise sample is [array([0.5360055], dtype=float32), 1.416273]. 
=============================================
[2019-04-04 06:20:21,220] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.5279490e-09 4.8621572e-05 2.5031103e-24 3.3465847e-22 4.5543469e-18
 9.9995136e-01 2.1024572e-17], sum to 1.0000
[2019-04-04 06:20:21,220] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9888
[2019-04-04 06:20:21,270] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.75, 84.5, 0.0, 0.0, 26.0, 25.03683664174191, 0.2530669477885636, 0.0, 1.0, 32805.09497030931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1877400.0000, 
sim time next is 1878000.0000, 
raw observation next is [-4.833333333333333, 85.0, 0.0, 0.0, 26.0, 25.03765572497519, 0.2482293185222495, 0.0, 1.0, 38938.42504220147], 
processed observation next is [0.0, 0.7391304347826086, 0.3287165281625116, 0.85, 0.0, 0.0, 0.6666666666666666, 0.586471310414599, 0.5827431061740832, 0.0, 1.0, 0.18542107162953078], 
reward next is 0.8146, 
noisyNet noise sample is [array([1.1294675], dtype=float32), 0.33273962]. 
=============================================
[2019-04-04 06:20:21,274] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[81.4478 ]
 [81.64779]
 [81.75843]
 [81.88883]
 [82.00845]], R is [[81.2387085 ]
 [81.27011108]
 [81.21263123]
 [81.14186096]
 [81.07065582]].
[2019-04-04 06:20:27,359] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3580630e-08 3.4382992e-05 2.1618447e-22 1.0339036e-20 1.9499729e-16
 9.9996555e-01 3.0429709e-16], sum to 1.0000
[2019-04-04 06:20:27,360] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9261
[2019-04-04 06:20:27,390] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.45, 76.5, 0.0, 0.0, 26.0, 24.12438776405503, 0.0740321606459793, 0.0, 1.0, 42051.5037106599], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2176200.0000, 
sim time next is 2176800.0000, 
raw observation next is [-6.366666666666667, 76.0, 0.0, 0.0, 26.0, 24.0649242543263, 0.06490927945974866, 0.0, 1.0, 42038.93293083119], 
processed observation next is [1.0, 0.17391304347826086, 0.28624192059095105, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5054103545271916, 0.5216364264865828, 0.0, 1.0, 0.20018539490871998], 
reward next is 0.7998, 
noisyNet noise sample is [array([-0.52614474], dtype=float32), -1.4006218]. 
=============================================
[2019-04-04 06:20:31,441] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.04145738e-08 2.21841910e-05 1.04455944e-22 2.66432654e-21
 1.72187637e-17 9.99977708e-01 1.93946974e-16], sum to 1.0000
[2019-04-04 06:20:31,444] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6609
[2019-04-04 06:20:31,468] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.35721589021264, 0.1337890336100916, 0.0, 1.0, 41581.56989839852], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1996800.0000, 
sim time next is 1997400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.38253142109333, 0.1325628119235154, 0.0, 1.0, 41526.0390826245], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5318776184244441, 0.5441876039745052, 0.0, 1.0, 0.19774304325059286], 
reward next is 0.8023, 
noisyNet noise sample is [array([2.9014297], dtype=float32), -2.1982112]. 
=============================================
[2019-04-04 06:20:34,578] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2148026e-09 9.6893864e-06 4.9233746e-26 6.5741258e-24 8.5150727e-19
 9.9999034e-01 1.7399355e-19], sum to 1.0000
[2019-04-04 06:20:34,578] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4350
[2019-04-04 06:20:34,665] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 68.0, 137.0, 0.0, 26.0, 25.54703555204397, 0.3979217548194876, 1.0, 1.0, 53824.00896565457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2124000.0000, 
sim time next is 2124600.0000, 
raw observation next is [-5.5, 68.0, 133.0, 0.0, 26.0, 24.98169535511751, 0.4091404669186529, 1.0, 1.0, 185514.9710083472], 
processed observation next is [1.0, 0.6086956521739131, 0.3102493074792244, 0.68, 0.44333333333333336, 0.0, 0.6666666666666666, 0.5818079462597924, 0.636380155639551, 1.0, 1.0, 0.8834046238492724], 
reward next is 0.1166, 
noisyNet noise sample is [array([-0.73866737], dtype=float32), 0.3779452]. 
=============================================
[2019-04-04 06:20:42,547] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4591845e-07 5.4020453e-05 4.2546815e-23 7.3589262e-22 2.5669458e-17
 9.9994588e-01 4.0079871e-16], sum to 1.0000
[2019-04-04 06:20:42,547] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2568
[2019-04-04 06:20:42,573] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.25330206355686, 0.1093102275359787, 0.0, 1.0, 42168.9748489769], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2173200.0000, 
sim time next is 2173800.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.22837759437855, 0.097558348771982, 0.0, 1.0, 42168.90449988784], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5190314661982125, 0.5325194495906607, 0.0, 1.0, 0.20080430714232303], 
reward next is 0.7992, 
noisyNet noise sample is [array([0.22295587], dtype=float32), -1.0368268]. 
=============================================
[2019-04-04 06:20:52,063] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.1385941e-09 2.0905302e-06 3.3792432e-25 8.3680244e-23 7.9686827e-19
 9.9999785e-01 1.7332784e-18], sum to 1.0000
[2019-04-04 06:20:52,063] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6404
[2019-04-04 06:20:52,129] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666666, 70.16666666666667, 0.0, 0.0, 26.0, 25.41523390760798, 0.3803971752379064, 1.0, 1.0, 27581.76621042351], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2229000.0000, 
sim time next is 2229600.0000, 
raw observation next is [-4.733333333333333, 70.33333333333334, 0.0, 0.0, 26.0, 25.24432456146811, 0.3543647004996245, 1.0, 1.0, 32414.2687566261], 
processed observation next is [1.0, 0.8260869565217391, 0.3314866112650046, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.603693713455676, 0.6181215668332082, 1.0, 1.0, 0.1543536607458386], 
reward next is 0.8456, 
noisyNet noise sample is [array([-1.0392888], dtype=float32), 1.852253]. 
=============================================
[2019-04-04 06:20:57,300] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5721208e-08 1.6945509e-05 3.7817209e-23 1.8148375e-21 7.2045281e-17
 9.9998307e-01 4.4234210e-16], sum to 1.0000
[2019-04-04 06:20:57,300] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7845
[2019-04-04 06:20:57,312] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.1, 67.0, 0.0, 0.0, 26.0, 24.19917590236401, 0.09932681297684842, 0.0, 1.0, 41201.83459870917], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2356200.0000, 
sim time next is 2356800.0000, 
raw observation next is [-3.2, 67.66666666666667, 0.0, 0.0, 26.0, 24.1631449182729, 0.09106936154699662, 0.0, 1.0, 41264.00574051293], 
processed observation next is [0.0, 0.2608695652173913, 0.37396121883656513, 0.6766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5135954098560749, 0.5303564538489989, 0.0, 1.0, 0.19649526543101395], 
reward next is 0.8035, 
noisyNet noise sample is [array([0.09354167], dtype=float32), -1.169451]. 
=============================================
[2019-04-04 06:21:00,214] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9865237e-10 2.3731993e-06 1.5325498e-26 1.7702317e-24 1.5484158e-19
 9.9999762e-01 2.1371297e-19], sum to 1.0000
[2019-04-04 06:21:00,215] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4487
[2019-04-04 06:21:00,251] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 29.5, 90.0, 845.0, 26.0, 24.92205295254625, 0.2571189560207332, 0.0, 1.0, 32809.4078708699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2464200.0000, 
sim time next is 2464800.0000, 
raw observation next is [0.8666666666666667, 29.0, 89.5, 842.8333333333334, 26.0, 24.9122279818041, 0.2626999428029245, 0.0, 1.0, 26142.90290824145], 
processed observation next is [0.0, 0.5217391304347826, 0.4866112650046169, 0.29, 0.29833333333333334, 0.9313075506445673, 0.6666666666666666, 0.5760189984836751, 0.5875666476009749, 0.0, 1.0, 0.1244900138487688], 
reward next is 0.8755, 
noisyNet noise sample is [array([0.4552108], dtype=float32), -0.09343306]. 
=============================================
[2019-04-04 06:21:03,692] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.1781605e-08 2.3830333e-05 5.1158056e-22 2.7710066e-21 5.0182319e-16
 9.9997616e-01 2.3476595e-16], sum to 1.0000
[2019-04-04 06:21:03,692] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8198
[2019-04-04 06:21:03,715] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.88257075119769, 0.2227773537916344, 0.0, 1.0, 41674.07414121294], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2601000.0000, 
sim time next is 2601600.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.81874491951276, 0.2215162607299483, 0.0, 1.0, 41722.92493027822], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.56822874329273, 0.5738387535766495, 0.0, 1.0, 0.19868059490608675], 
reward next is 0.8013, 
noisyNet noise sample is [array([0.93536866], dtype=float32), -0.8264836]. 
=============================================
[2019-04-04 06:21:38,117] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9414890e-08 2.9224704e-05 5.5770709e-22 7.1535961e-21 3.1936729e-16
 9.9997067e-01 3.0115873e-16], sum to 1.0000
[2019-04-04 06:21:38,117] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0732
[2019-04-04 06:21:38,166] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.49551575721988, 0.166865508587327, 0.0, 1.0, 40775.44146261172], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2778000.0000, 
sim time next is 2778600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.49614079522166, 0.1591109708053124, 0.0, 1.0, 40748.17288898793], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5413450662684717, 0.5530369902684374, 0.0, 1.0, 0.19403891851899013], 
reward next is 0.8060, 
noisyNet noise sample is [array([0.07502413], dtype=float32), -0.192652]. 
=============================================
[2019-04-04 06:21:39,599] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4979975e-08 1.5336107e-06 3.2158871e-25 1.6943577e-23 3.6887482e-18
 9.9999845e-01 1.2755317e-18], sum to 1.0000
[2019-04-04 06:21:39,599] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3984
[2019-04-04 06:21:39,701] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.333333333333334, 60.66666666666667, 0.0, 0.0, 26.0, 25.19525505708761, 0.4245058345118302, 0.0, 1.0, 180548.9939412837], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2751600.0000, 
sim time next is 2752200.0000, 
raw observation next is [-5.5, 61.5, 0.0, 0.0, 26.0, 25.20117394618837, 0.4415041092874666, 0.0, 1.0, 112357.4432953431], 
processed observation next is [1.0, 0.8695652173913043, 0.3102493074792244, 0.615, 0.0, 0.0, 0.6666666666666666, 0.6000978288490307, 0.6471680364291555, 0.0, 1.0, 0.5350354442635386], 
reward next is 0.4650, 
noisyNet noise sample is [array([0.28529483], dtype=float32), 0.16661707]. 
=============================================
[2019-04-04 06:21:42,262] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7636941e-10 1.5153041e-06 9.9607479e-28 1.2499446e-26 8.8523108e-21
 9.9999845e-01 2.9794406e-20], sum to 1.0000
[2019-04-04 06:21:42,262] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4931
[2019-04-04 06:21:42,271] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7227619e-10 5.7570166e-08 4.7223215e-28 9.1602756e-26 8.1023052e-21
 1.0000000e+00 7.4837227e-21], sum to 1.0000
[2019-04-04 06:21:42,271] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0540
[2019-04-04 06:21:42,309] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 93.0, 16.83333333333333, 43.16666666666667, 26.0, 25.94565232616484, 0.4382785840831484, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2913600.0000, 
sim time next is 2914200.0000, 
raw observation next is [1.5, 93.0, 6.0, 41.0, 26.0, 25.77883906445022, 0.2576153290018643, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5041551246537397, 0.93, 0.02, 0.045303867403314914, 0.6666666666666666, 0.6482365887041851, 0.5858717763339548, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11220016], dtype=float32), -0.4169236]. 
=============================================
[2019-04-04 06:21:42,348] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 52.66666666666667, 88.66666666666667, 633.8333333333334, 26.0, 25.94745989398547, 0.5977342416611704, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2733600.0000, 
sim time next is 2734200.0000, 
raw observation next is [-3.5, 52.0, 86.0, 614.0, 26.0, 26.32949896040012, 0.6311462570544477, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.36565096952908593, 0.52, 0.2866666666666667, 0.6784530386740332, 0.6666666666666666, 0.6941249133666766, 0.7103820856848159, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.589744], dtype=float32), -1.2915851]. 
=============================================
[2019-04-04 06:21:44,136] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.65307756e-11 2.26894500e-07 8.94407451e-28 1.05125615e-26
 4.09511353e-21 9.99999762e-01 1.86923868e-20], sum to 1.0000
[2019-04-04 06:21:44,137] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7793
[2019-04-04 06:21:44,292] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 39.33333333333334, 349.0000000000001, 26.0, 25.0758259112575, 0.3750747270824077, 0.0, 1.0, 37866.90023417705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2998200.0000, 
sim time next is 2998800.0000, 
raw observation next is [-1.0, 55.0, 31.0, 286.5, 26.0, 25.09527687868712, 0.3689477574639232, 0.0, 1.0, 20023.81087529932], 
processed observation next is [0.0, 0.7391304347826086, 0.4349030470914128, 0.55, 0.10333333333333333, 0.3165745856353591, 0.6666666666666666, 0.5912730732239266, 0.6229825858213077, 0.0, 1.0, 0.09535148035856819], 
reward next is 0.9046, 
noisyNet noise sample is [array([-0.1532784], dtype=float32), 0.40726742]. 
=============================================
[2019-04-04 06:21:56,268] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.59837654e-11 1.91846524e-07 1.71031113e-28 1.56633725e-26
 1.99851865e-21 9.99999762e-01 1.13373334e-20], sum to 1.0000
[2019-04-04 06:21:56,268] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4275
[2019-04-04 06:21:56,365] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.166666666666667, 100.0, 145.6666666666667, 0.0, 26.0, 25.39355254606213, 0.3263325384146615, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2895000.0000, 
sim time next is 2895600.0000, 
raw observation next is [1.333333333333333, 100.0, 160.3333333333333, 0.0, 26.0, 25.41121988072686, 0.301115922470931, 1.0, 1.0, 9340.205835115268], 
processed observation next is [1.0, 0.5217391304347826, 0.4995383194829178, 1.0, 0.5344444444444443, 0.0, 0.6666666666666666, 0.6176016567272384, 0.600371974156977, 1.0, 1.0, 0.04447717064340604], 
reward next is 0.9555, 
noisyNet noise sample is [array([0.05367218], dtype=float32), -0.32469475]. 
=============================================
[2019-04-04 06:21:59,715] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4067172e-08 1.9118349e-06 8.0759517e-25 5.3778174e-24 1.8707355e-18
 9.9999809e-01 6.7057026e-18], sum to 1.0000
[2019-04-04 06:21:59,731] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0573
[2019-04-04 06:21:59,795] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 26.0, 25.52922249166748, 0.3364204751628453, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3115200.0000, 
sim time next is 3115800.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 26.0, 25.47587115412024, 0.3179941231317756, 0.0, 1.0, 24853.50738372486], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6229892628433534, 0.6059980410439252, 0.0, 1.0, 0.11835003516059457], 
reward next is 0.8816, 
noisyNet noise sample is [array([-0.8662842], dtype=float32), 0.3768453]. 
=============================================
[2019-04-04 06:22:15,339] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6396256e-08 9.3524994e-07 1.0573289e-25 7.1164176e-24 1.9309154e-19
 9.9999905e-01 6.7506727e-19], sum to 1.0000
[2019-04-04 06:22:15,339] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5435
[2019-04-04 06:22:15,365] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 86.5, 0.0, 0.0, 26.0, 25.6177396021083, 0.5522419238699535, 0.0, 1.0, 18732.31286142239], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3274200.0000, 
sim time next is 3274800.0000, 
raw observation next is [-5.666666666666667, 88.33333333333333, 0.0, 0.0, 26.0, 25.54293019508616, 0.5416544118410341, 0.0, 1.0, 61596.15408492569], 
processed observation next is [1.0, 0.9130434782608695, 0.30563250230840255, 0.8833333333333333, 0.0, 0.0, 0.6666666666666666, 0.6285775162571801, 0.680551470613678, 0.0, 1.0, 0.2933150194520271], 
reward next is 0.7067, 
noisyNet noise sample is [array([0.6913256], dtype=float32), -0.226922]. 
=============================================
[2019-04-04 06:22:17,105] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 06:22:17,105] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:22:17,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:22:17,120] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:22:17,120] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:22:17,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run10
[2019-04-04 06:22:17,125] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:22:17,137] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:22:17,165] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run10
[2019-04-04 06:22:17,199] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run10
[2019-04-04 06:23:05,631] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23149996], dtype=float32), 0.21395573]
[2019-04-04 06:23:05,631] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.200000000000001, 81.66666666666667, 142.5, 388.8333333333333, 26.0, 25.47367859386069, 0.3321729637555581, 1.0, 1.0, 0.0]
[2019-04-04 06:23:05,631] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:23:05,633] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.0589112e-11 2.0385220e-07 2.0423480e-29 3.0454619e-27 9.4005169e-22
 9.9999976e-01 3.0056072e-21], sampled 0.6320703113019168
[2019-04-04 06:23:20,822] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.23149996], dtype=float32), 0.21395573]
[2019-04-04 06:23:20,822] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [10.86666666666667, 75.0, 0.0, 0.0, 26.0, 25.65543824818624, 0.6420993041368822, 0.0, 1.0, 43457.94668744075]
[2019-04-04 06:23:20,822] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 06:23:20,823] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.3808791e-09 2.0702500e-06 2.4877647e-26 1.7948680e-24 1.4545859e-19
 9.9999797e-01 9.7687632e-19], sampled 0.04751280313294015
[2019-04-04 06:25:24,060] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4829 239928582.5118 1604.8457
[2019-04-04 06:25:36,946] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.23149996], dtype=float32), 0.21395573]
[2019-04-04 06:25:36,946] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.0, 72.0, 0.0, 0.0, 26.0, 25.66651516273137, 0.5630247803369824, 0.0, 1.0, 0.0]
[2019-04-04 06:25:36,947] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 06:25:36,948] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.9045291e-09 8.0229734e-07 2.9720211e-26 2.7124872e-24 1.5078425e-19
 9.9999917e-01 9.4499626e-19], sampled 0.4135968934044124
[2019-04-04 06:25:54,881] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6417 263415234.9355 1551.9605
[2019-04-04 06:25:57,540] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 06:25:58,565] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 900000, evaluation results [900000.0, 7241.641738402564, 263415234.93545747, 1551.9605289518147, 7353.482940420242, 239928582.51175264, 1604.845674400583, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 06:26:00,746] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3625171e-09 1.4741307e-06 6.0474570e-25 2.2722802e-23 1.6289178e-18
 9.9999857e-01 3.7101143e-18], sum to 1.0000
[2019-04-04 06:26:00,747] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5699
[2019-04-04 06:26:00,809] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 56.5, 99.0, 635.0, 26.0, 25.33916211697487, 0.3257630532219768, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3058200.0000, 
sim time next is 3058800.0000, 
raw observation next is [-4.666666666666666, 55.66666666666667, 100.1666666666667, 655.6666666666667, 26.0, 25.31958303626982, 0.3236640304274999, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.33333333333333337, 0.5566666666666668, 0.333888888888889, 0.7244935543278086, 0.6666666666666666, 0.6099652530224849, 0.6078880101425, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24906477], dtype=float32), -0.20733005]. 
=============================================
[2019-04-04 06:26:02,347] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.8801891e-09 1.3201001e-06 8.0992719e-27 1.5187136e-24 4.4938952e-20
 9.9999869e-01 1.4590216e-19], sum to 1.0000
[2019-04-04 06:26:02,348] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2292
[2019-04-04 06:26:02,366] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.833333333333333, 100.0, 0.0, 0.0, 26.0, 25.38944096592018, 0.3078523009753407, 0.0, 1.0, 85065.83354989595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3131400.0000, 
sim time next is 3132000.0000, 
raw observation next is [4.0, 100.0, 0.0, 0.0, 26.0, 25.39636090446602, 0.311497648574787, 0.0, 1.0, 64119.01223906534], 
processed observation next is [1.0, 0.2608695652173913, 0.5734072022160666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6163634087055018, 0.6038325495249289, 0.0, 1.0, 0.305328629709835], 
reward next is 0.6947, 
noisyNet noise sample is [array([-0.8995587], dtype=float32), 0.3149953]. 
=============================================
[2019-04-04 06:26:02,374] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[85.93621]
 [85.72733]
 [85.63095]
 [85.61352]
 [85.53295]], R is [[85.95672607]
 [85.69208527]
 [85.57758331]
 [85.61257172]
 [85.6186142 ]].
[2019-04-04 06:26:14,010] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8717496e-09 4.7064808e-07 6.3545088e-26 1.1583391e-23 1.3895299e-19
 9.9999952e-01 1.2964059e-18], sum to 1.0000
[2019-04-04 06:26:14,013] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4597
[2019-04-04 06:26:14,050] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.88942702494877, 0.5712751100965171, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3349200.0000, 
sim time next is 3349800.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.90944336648433, 0.5414143407891966, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6591202805403608, 0.6804714469297322, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4595963], dtype=float32), -0.99000734]. 
=============================================
[2019-04-04 06:26:15,129] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.9170141e-11 2.4337493e-07 8.6855065e-29 2.6980886e-27 1.8989928e-21
 9.9999976e-01 8.4991404e-22], sum to 1.0000
[2019-04-04 06:26:15,129] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6812
[2019-04-04 06:26:15,139] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.166666666666667, 60.33333333333334, 113.0, 796.6666666666667, 26.0, 26.16115157004628, 0.6015034181462208, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3496200.0000, 
sim time next is 3496800.0000, 
raw observation next is [1.333333333333333, 59.66666666666667, 114.0, 803.3333333333333, 26.0, 26.23762161784944, 0.6119123701892982, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4995383194829178, 0.5966666666666667, 0.38, 0.8876611418047882, 0.6666666666666666, 0.6864684681541201, 0.7039707900630994, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.52152985], dtype=float32), 0.23719612]. 
=============================================
[2019-04-04 06:26:21,425] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.4928297e-10 1.3163889e-06 2.6181485e-27 5.9954970e-26 7.4328182e-20
 9.9999869e-01 5.6944626e-20], sum to 1.0000
[2019-04-04 06:26:21,425] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3167
[2019-04-04 06:26:21,441] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333334, 42.66666666666666, 82.16666666666667, 668.3333333333333, 26.0, 25.34704215849514, 0.4725806044983346, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3598800.0000, 
sim time next is 3599400.0000, 
raw observation next is [-0.1666666666666666, 42.83333333333334, 78.33333333333334, 637.6666666666667, 26.0, 25.34988387714247, 0.4665290607798269, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.4579870729455217, 0.42833333333333345, 0.2611111111111111, 0.7046040515653776, 0.6666666666666666, 0.6124903230952059, 0.6555096869266089, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1048347], dtype=float32), 0.6122965]. 
=============================================
[2019-04-04 06:26:35,159] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.2870103e-08 5.3631584e-06 1.4622722e-23 1.2365358e-21 3.3366544e-17
 9.9999452e-01 1.8980750e-16], sum to 1.0000
[2019-04-04 06:26:35,159] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4334
[2019-04-04 06:26:35,171] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 73.0, 0.0, 0.0, 26.0, 25.09291562410308, 0.2654659490410358, 0.0, 1.0, 42293.90536208379], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3739200.0000, 
sim time next is 3739800.0000, 
raw observation next is [-3.833333333333333, 75.0, 0.0, 0.0, 26.0, 25.05436984381637, 0.2548502403632998, 0.0, 1.0, 42197.95491136018], 
processed observation next is [1.0, 0.2608695652173913, 0.3564173591874424, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5878641536513642, 0.5849500801211, 0.0, 1.0, 0.20094264243504847], 
reward next is 0.7991, 
noisyNet noise sample is [array([-0.11009035], dtype=float32), -0.14486115]. 
=============================================
[2019-04-04 06:26:40,291] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0285680e-08 3.2198716e-06 1.4032663e-23 1.9869571e-22 3.6448771e-18
 9.9999678e-01 3.6308487e-17], sum to 1.0000
[2019-04-04 06:26:40,291] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7683
[2019-04-04 06:26:40,317] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.00050135997165, 0.3142448118719176, 0.0, 1.0, 43747.47105875835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3816000.0000, 
sim time next is 3816600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.97929265991001, 0.3092603078161727, 0.0, 1.0, 43818.6596228243], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5816077216591674, 0.6030867692720575, 0.0, 1.0, 0.20866028391821095], 
reward next is 0.7913, 
noisyNet noise sample is [array([0.248014], dtype=float32), -1.4054132]. 
=============================================
[2019-04-04 06:26:44,325] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.4347694e-08 3.1787706e-06 5.2705589e-23 4.4657878e-22 3.8074470e-18
 9.9999678e-01 2.5069884e-17], sum to 1.0000
[2019-04-04 06:26:44,325] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3437
[2019-04-04 06:26:44,349] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.69747306426743, 0.4425194377114887, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3897000.0000, 
sim time next is 3897600.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.6967056843747, 0.4253063316039722, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6413921403645583, 0.6417687772013241, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9488599], dtype=float32), -2.1366858]. 
=============================================
[2019-04-04 06:26:44,960] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.6580401e-07 5.8923328e-05 1.0106987e-20 2.5678204e-19 3.4309696e-15
 9.9994040e-01 2.9926591e-15], sum to 1.0000
[2019-04-04 06:26:44,963] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1047
[2019-04-04 06:26:44,992] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 40.0, 0.0, 0.0, 26.0, 24.91851083041998, 0.2516113320085857, 0.0, 1.0, 40597.77830277554], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4074000.0000, 
sim time next is 4074600.0000, 
raw observation next is [-5.0, 40.5, 0.0, 0.0, 26.0, 24.8837263066771, 0.2478304266779348, 0.0, 1.0, 40564.09792262458], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.405, 0.0, 0.0, 0.6666666666666666, 0.5736438588897584, 0.5826101422259783, 0.0, 1.0, 0.19316237106011708], 
reward next is 0.8068, 
noisyNet noise sample is [array([0.3198036], dtype=float32), 0.14237802]. 
=============================================
[2019-04-04 06:26:45,175] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9055236e-07 2.5316438e-05 4.4621483e-21 1.1745846e-19 2.5423469e-15
 9.9997449e-01 5.1999276e-15], sum to 1.0000
[2019-04-04 06:26:45,178] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9969
[2019-04-04 06:26:45,195] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 24.707690352275, 0.2152618578183196, 0.0, 1.0, 40401.14808682749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4078200.0000, 
sim time next is 4078800.0000, 
raw observation next is [-4.0, 34.0, 0.0, 0.0, 26.0, 24.75111073192507, 0.215304669207465, 0.0, 1.0, 40344.25437721286], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.34, 0.0, 0.0, 0.6666666666666666, 0.5625925609937559, 0.571768223069155, 0.0, 1.0, 0.19211549703434697], 
reward next is 0.8079, 
noisyNet noise sample is [array([0.20950885], dtype=float32), 0.42427623]. 
=============================================
[2019-04-04 06:26:57,339] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2220455e-09 3.6103225e-07 9.3079344e-26 7.6095968e-24 1.9389324e-19
 9.9999964e-01 1.1181775e-18], sum to 1.0000
[2019-04-04 06:26:57,352] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0552
[2019-04-04 06:26:57,364] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.533333333333334, 68.0, 0.0, 0.0, 26.0, 25.68751329537675, 0.5596750773155598, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4425600.0000, 
sim time next is 4426200.0000, 
raw observation next is [3.4, 68.0, 0.0, 0.0, 26.0, 25.74375130430079, 0.5451938213236797, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.556786703601108, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6453126086917326, 0.6817312737745599, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31331015], dtype=float32), -1.5269772]. 
=============================================
[2019-04-04 06:26:57,448] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9839399e-08 3.0096257e-06 2.8613060e-22 4.3649873e-21 9.8001897e-17
 9.9999690e-01 5.5996607e-16], sum to 1.0000
[2019-04-04 06:26:57,449] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8950
[2019-04-04 06:26:57,460] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.166666666666667, 32.0, 0.0, 0.0, 26.0, 25.45984417683852, 0.4394693407686881, 0.0, 1.0, 48412.21203549852], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4054200.0000, 
sim time next is 4054800.0000, 
raw observation next is [-5.333333333333334, 33.0, 0.0, 0.0, 26.0, 25.45765671299551, 0.4237065604828656, 0.0, 1.0, 41746.09544941693], 
processed observation next is [1.0, 0.9565217391304348, 0.31486611265004616, 0.33, 0.0, 0.0, 0.6666666666666666, 0.6214713927496259, 0.6412355201609552, 0.0, 1.0, 0.1987909307115092], 
reward next is 0.8012, 
noisyNet noise sample is [array([-0.05962742], dtype=float32), 0.3038808]. 
=============================================
[2019-04-04 06:27:00,398] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.0747206e-10 5.5182295e-07 7.1200786e-26 1.4086864e-24 3.9646281e-19
 9.9999940e-01 1.1576881e-18], sum to 1.0000
[2019-04-04 06:27:00,400] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8266
[2019-04-04 06:27:00,416] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.9, 75.0, 0.0, 0.0, 26.0, 25.40046815165545, 0.3784697189184835, 0.0, 1.0, 48564.34569862109], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4345200.0000, 
sim time next is 4345800.0000, 
raw observation next is [2.916666666666667, 74.83333333333333, 0.0, 0.0, 26.0, 25.49288917732336, 0.3871422289215028, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.543397968605725, 0.7483333333333333, 0.0, 0.0, 0.6666666666666666, 0.6244074314436133, 0.6290474096405009, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20356755], dtype=float32), -0.32789868]. 
=============================================
[2019-04-04 06:27:02,637] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7074719e-10 3.2441609e-08 2.9269929e-28 3.3366573e-26 1.7837085e-20
 1.0000000e+00 1.1802103e-20], sum to 1.0000
[2019-04-04 06:27:02,643] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8941
[2019-04-04 06:27:02,650] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.33333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 27.48801167645907, 0.947884539681323, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4390800.0000, 
sim time next is 4391400.0000, 
raw observation next is [11.16666666666667, 56.66666666666667, 0.0, 0.0, 26.0, 27.37225832491883, 0.929524226151941, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7719298245614037, 0.5666666666666668, 0.0, 0.0, 0.6666666666666666, 0.781021527076569, 0.8098414087173137, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.96700263], dtype=float32), -0.6128893]. 
=============================================
[2019-04-04 06:27:06,241] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.9736822e-11 7.5830560e-09 5.5621507e-29 3.5351172e-27 2.5028539e-21
 1.0000000e+00 2.8694704e-21], sum to 1.0000
[2019-04-04 06:27:06,241] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7554
[2019-04-04 06:27:06,255] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.26666666666667, 46.0, 0.0, 0.0, 26.0, 27.50087824084913, 0.9538835391335029, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4386000.0000, 
sim time next is 4386600.0000, 
raw observation next is [12.2, 47.0, 0.0, 0.0, 26.0, 26.64157122367665, 0.871082559932887, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8005540166204987, 0.47, 0.0, 0.0, 0.6666666666666666, 0.7201309353063875, 0.7903608533109624, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00363713], dtype=float32), 0.13793616]. 
=============================================
[2019-04-04 06:27:15,554] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.2066352e-11 2.6403331e-08 3.0929125e-28 1.2325928e-26 9.0191767e-21
 1.0000000e+00 2.5893868e-20], sum to 1.0000
[2019-04-04 06:27:15,555] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9110
[2019-04-04 06:27:15,564] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.2, 47.0, 0.0, 0.0, 26.0, 27.94701596778084, 1.026292969253263, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4386600.0000, 
sim time next is 4387200.0000, 
raw observation next is [12.13333333333333, 48.0, 0.0, 0.0, 26.0, 27.86333960471612, 1.033211827629049, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7987072945521699, 0.48, 0.0, 0.0, 0.6666666666666666, 0.8219449670596767, 0.8444039425430163, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.72878486], dtype=float32), 1.8928672]. 
=============================================
[2019-04-04 06:27:35,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:35,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:35,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run8
[2019-04-04 06:27:41,137] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8376216e-08 3.3111297e-07 3.3477786e-23 5.5843002e-22 5.4843739e-17
 9.9999964e-01 1.2496760e-16], sum to 1.0000
[2019-04-04 06:27:41,138] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6782
[2019-04-04 06:27:41,150] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.26867982839558, 0.3478428656650838, 0.0, 1.0, 43365.11218136002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4840800.0000, 
sim time next is 4841400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.24791770460302, 0.3439339468943527, 0.0, 1.0, 40584.43816222229], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6039931420502516, 0.6146446489647842, 0.0, 1.0, 0.19325922934391568], 
reward next is 0.8067, 
noisyNet noise sample is [array([0.97552925], dtype=float32), -0.4792519]. 
=============================================
[2019-04-04 06:27:41,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:41,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:41,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run8
[2019-04-04 06:27:43,148] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9429691e-09 6.9381969e-08 5.4158698e-25 5.1891397e-23 7.6154259e-19
 9.9999988e-01 1.6170824e-18], sum to 1.0000
[2019-04-04 06:27:43,152] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4106
[2019-04-04 06:27:43,163] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.166666666666667, 35.66666666666667, 0.0, 0.0, 26.0, 25.78685543002248, 0.5740874304693787, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5003400.0000, 
sim time next is 5004000.0000, 
raw observation next is [3.0, 37.0, 0.0, 0.0, 26.0, 25.80009934946773, 0.5672273438275154, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.37, 0.0, 0.0, 0.6666666666666666, 0.6500082791223107, 0.6890757812758385, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.90320885], dtype=float32), -1.3607991]. 
=============================================
[2019-04-04 06:27:43,166] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.51043 ]
 [77.48237 ]
 [77.67792 ]
 [78.068596]
 [78.09211 ]], R is [[77.74582672]
 [77.96836853]
 [78.18868256]
 [78.40679932]
 [78.38626099]].
[2019-04-04 06:27:43,980] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:43,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:43,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run8
[2019-04-04 06:27:44,564] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:44,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:44,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run8
[2019-04-04 06:27:47,518] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:47,518] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:47,528] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run8
[2019-04-04 06:27:49,171] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:49,172] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:49,175] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run8
[2019-04-04 06:27:49,479] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:49,480] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:49,483] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run8
[2019-04-04 06:27:53,516] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:53,516] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:53,535] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run8
[2019-04-04 06:27:55,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:55,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:55,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run8
[2019-04-04 06:27:55,487] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:55,487] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:55,496] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run8
[2019-04-04 06:27:56,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:56,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:56,038] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run8
[2019-04-04 06:27:56,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:56,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:56,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run8
[2019-04-04 06:27:56,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:56,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:56,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run8
[2019-04-04 06:27:57,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:57,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:57,120] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run8
[2019-04-04 06:27:57,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:57,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:57,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run8
[2019-04-04 06:27:59,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:27:59,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:27:59,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run8
[2019-04-04 06:28:22,443] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4396887e-08 3.2430603e-07 3.8753817e-23 8.4295253e-22 8.7589866e-18
 9.9999964e-01 3.0994368e-17], sum to 1.0000
[2019-04-04 06:28:22,443] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3655
[2019-04-04 06:28:22,463] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.87357059400291, 0.04029114077661872, 0.0, 1.0, 44645.64677942594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 172200.0000, 
sim time next is 172800.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.83008055009801, 0.03011333086579835, 0.0, 1.0, 44590.69847431598], 
processed observation next is [1.0, 0.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.48584004584150087, 0.5100377769552661, 0.0, 1.0, 0.21233665940150465], 
reward next is 0.7877, 
noisyNet noise sample is [array([-0.9506427], dtype=float32), 1.2877712]. 
=============================================
[2019-04-04 06:28:28,721] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.91978842e-08 6.93876473e-07 1.54120228e-23 1.55356424e-21
 3.14037598e-17 9.99999285e-01 1.09706946e-16], sum to 1.0000
[2019-04-04 06:28:28,722] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5040
[2019-04-04 06:28:28,815] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.65, 76.0, 0.0, 0.0, 26.0, 24.08714378569324, 0.07836764827364345, 0.0, 1.0, 47073.79460252921], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 340200.0000, 
sim time next is 340800.0000, 
raw observation next is [-13.73333333333333, 74.0, 0.0, 0.0, 26.0, 24.05185046759327, 0.0637061982463409, 0.0, 1.0, 47062.97692735832], 
processed observation next is [1.0, 0.9565217391304348, 0.08217913204062795, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5043208722994391, 0.5212353994154469, 0.0, 1.0, 0.22410941393980152], 
reward next is 0.7759, 
noisyNet noise sample is [array([-0.45431215], dtype=float32), 0.8469325]. 
=============================================
[2019-04-04 06:28:37,865] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.6012968e-10 9.4519685e-08 8.1743970e-27 6.8075753e-25 2.3337464e-19
 9.9999988e-01 1.1515351e-19], sum to 1.0000
[2019-04-04 06:28:37,865] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1205
[2019-04-04 06:28:37,907] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.100000000000001, 38.66666666666667, 34.33333333333334, 656.3333333333334, 26.0, 26.20811688913422, 0.4764981768221332, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 402000.0000, 
sim time next is 402600.0000, 
raw observation next is [-9.0, 38.33333333333334, 31.66666666666666, 605.6666666666667, 26.0, 25.91610289816067, 0.3046761938644502, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.21329639889196678, 0.3833333333333334, 0.10555555555555554, 0.6692449355432781, 0.6666666666666666, 0.6596752415133892, 0.6015587312881501, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4422562], dtype=float32), -0.7730837]. 
=============================================
[2019-04-04 06:28:41,204] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3945528e-09 1.9608687e-07 5.2597812e-26 5.6002951e-24 1.2939303e-19
 9.9999976e-01 6.1659594e-19], sum to 1.0000
[2019-04-04 06:28:41,227] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3091
[2019-04-04 06:28:41,244] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.533333333333333, 88.33333333333334, 0.0, 0.0, 26.0, 24.9998989428409, 0.2528691069451376, 0.0, 1.0, 39560.32196027674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 524400.0000, 
sim time next is 525000.0000, 
raw observation next is [4.416666666666667, 88.16666666666667, 0.0, 0.0, 26.0, 24.96256765358017, 0.2478463894590247, 0.0, 1.0, 39596.05610361754], 
processed observation next is [0.0, 0.043478260869565216, 0.584949215143121, 0.8816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5802139711316808, 0.5826154631530082, 0.0, 1.0, 0.1885526481124645], 
reward next is 0.8114, 
noisyNet noise sample is [array([-0.23582074], dtype=float32), -0.87623084]. 
=============================================
[2019-04-04 06:28:41,259] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[81.923035]
 [81.75571 ]
 [81.75896 ]
 [81.794914]
 [82.12471 ]], R is [[82.07553101]
 [82.06639099]
 [82.05753326]
 [82.04904175]
 [82.03970337]].
[2019-04-04 06:28:49,751] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.2761336e-09 3.7929603e-08 9.2959937e-23 7.6765784e-22 4.6653524e-18
 1.0000000e+00 8.4837091e-18], sum to 1.0000
[2019-04-04 06:28:49,752] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1506
[2019-04-04 06:28:49,804] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.1, 42.83333333333334, 0.0, 0.0, 26.0, 25.12036376547763, 0.2923757005020381, 0.0, 1.0, 55060.72202614452], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 418200.0000, 
sim time next is 418800.0000, 
raw observation next is [-10.2, 43.66666666666667, 0.0, 0.0, 26.0, 25.1131569682317, 0.2822666378904206, 0.0, 1.0, 38216.88331318644], 
processed observation next is [1.0, 0.8695652173913043, 0.1800554016620499, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5927630806859749, 0.5940888792968069, 0.0, 1.0, 0.18198515863422113], 
reward next is 0.8180, 
noisyNet noise sample is [array([-1.902208], dtype=float32), 0.113134615]. 
=============================================
[2019-04-04 06:28:59,883] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.7568438e-12 1.1417633e-08 7.7145930e-30 1.1559449e-27 2.4898066e-22
 1.0000000e+00 4.5020981e-22], sum to 1.0000
[2019-04-04 06:28:59,887] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9285
[2019-04-04 06:28:59,913] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 49.16666666666667, 103.0, 665.0, 26.0, 25.50997938792285, 0.3399924374455612, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 738600.0000, 
sim time next is 739200.0000, 
raw observation next is [0.5, 48.33333333333334, 96.0, 719.0, 26.0, 25.31673303315982, 0.3605706793463859, 1.0, 1.0, 18680.50499868238], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.48333333333333345, 0.32, 0.7944751381215469, 0.6666666666666666, 0.6097277527633184, 0.6201902264487953, 1.0, 1.0, 0.08895478570801134], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.0216267], dtype=float32), -0.8883356]. 
=============================================
[2019-04-04 06:29:01,902] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.0554037e-09 1.4632811e-07 9.5098987e-25 2.4054459e-22 1.5557202e-18
 9.9999988e-01 5.7786366e-18], sum to 1.0000
[2019-04-04 06:29:01,902] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4362
[2019-04-04 06:29:01,916] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 71.0, 0.0, 0.0, 26.0, 24.49995022501756, 0.1140842723538506, 0.0, 1.0, 41302.65832248703], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 690000.0000, 
sim time next is 690600.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.46962022328115, 0.1085108139815505, 0.0, 1.0, 41223.53536298454], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5391350186067626, 0.5361702713271835, 0.0, 1.0, 0.19630254934754543], 
reward next is 0.8037, 
noisyNet noise sample is [array([-0.7161561], dtype=float32), -0.32697353]. 
=============================================
[2019-04-04 06:29:12,398] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0026768e-09 6.8374341e-08 4.0235353e-25 1.7846031e-24 3.0868680e-19
 9.9999988e-01 4.1819150e-18], sum to 1.0000
[2019-04-04 06:29:12,405] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4392
[2019-04-04 06:29:12,494] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.716666666666666, 55.66666666666667, 0.0, 0.0, 26.0, 24.79277580568702, 0.3376773905269992, 1.0, 1.0, 199395.166933704], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 755400.0000, 
sim time next is 756000.0000, 
raw observation next is [-3.9, 56.0, 0.0, 0.0, 26.0, 25.03470566629934, 0.3756736652217698, 1.0, 1.0, 20274.61552021976], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.56, 0.0, 0.0, 0.6666666666666666, 0.5862254721916118, 0.6252245550739233, 1.0, 1.0, 0.09654578819152267], 
reward next is 0.9035, 
noisyNet noise sample is [array([0.42119655], dtype=float32), -0.49860728]. 
=============================================
[2019-04-04 06:29:12,497] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.54761 ]
 [77.49318 ]
 [77.522415]
 [77.933495]
 [78.55318 ]], R is [[77.47496796]
 [76.75071716]
 [76.04109955]
 [75.92440033]
 [76.1651535 ]].
[2019-04-04 06:29:18,347] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2813326e-08 1.0724492e-06 1.1561885e-23 2.3631252e-21 5.7318682e-18
 9.9999893e-01 1.2068405e-16], sum to 1.0000
[2019-04-04 06:29:18,348] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6179
[2019-04-04 06:29:18,364] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.55, 74.5, 0.0, 0.0, 26.0, 23.93427343327935, 0.03514424756041483, 0.0, 1.0, 41202.29644451313], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 790200.0000, 
sim time next is 790800.0000, 
raw observation next is [-7.466666666666667, 74.66666666666667, 0.0, 0.0, 26.0, 23.90387299068281, 0.02540871415453019, 0.0, 1.0, 41224.52616851607], 
processed observation next is [1.0, 0.13043478260869565, 0.25577100646352724, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.4919894158902342, 0.5084695713848434, 0.0, 1.0, 0.19630726746912416], 
reward next is 0.8037, 
noisyNet noise sample is [array([0.28295478], dtype=float32), -0.46842822]. 
=============================================
[2019-04-04 06:29:19,682] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0273211e-10 2.5349831e-08 6.9877698e-28 6.7332175e-26 4.2625029e-21
 1.0000000e+00 9.1746027e-21], sum to 1.0000
[2019-04-04 06:29:19,682] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4112
[2019-04-04 06:29:19,737] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.5, 92.83333333333333, 18.0, 0.0, 26.0, 25.40585924682344, 0.4252915335922909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 979800.0000, 
sim time next is 980400.0000, 
raw observation next is [9.600000000000001, 92.66666666666667, 22.5, 0.0, 26.0, 25.33550761313693, 0.487207385585406, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7285318559556788, 0.9266666666666667, 0.075, 0.0, 0.6666666666666666, 0.6112923010947441, 0.662402461861802, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9942698], dtype=float32), 0.7716771]. 
=============================================
[2019-04-04 06:29:21,864] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.8312849e-11 2.5395697e-09 3.5833514e-30 2.6172533e-27 4.5919953e-23
 1.0000000e+00 3.6741271e-22], sum to 1.0000
[2019-04-04 06:29:21,865] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4698
[2019-04-04 06:29:21,887] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.58333333333333, 80.5, 0.0, 0.0, 26.0, 25.99987632360288, 0.5776844736780745, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1014600.0000, 
sim time next is 1015200.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 26.09443237362697, 0.5721203169892283, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6745360311355807, 0.6907067723297428, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4274394], dtype=float32), 0.43120036]. 
=============================================
[2019-04-04 06:29:25,591] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2739244e-10 4.0079758e-08 2.4698094e-29 3.0755970e-26 5.6315798e-22
 1.0000000e+00 6.6341327e-21], sum to 1.0000
[2019-04-04 06:29:25,593] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4080
[2019-04-04 06:29:25,618] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.9, 97.33333333333333, 0.0, 0.0, 26.0, 25.31268299776696, 0.5923362786962912, 0.0, 1.0, 40687.42214815506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1282800.0000, 
sim time next is 1283400.0000, 
raw observation next is [5.8, 98.0, 0.0, 0.0, 26.0, 25.40368155782577, 0.6048596937741478, 0.0, 1.0, 23113.67498513456], 
processed observation next is [0.0, 0.8695652173913043, 0.6232686980609419, 0.98, 0.0, 0.0, 0.6666666666666666, 0.6169734631521475, 0.7016198979247159, 0.0, 1.0, 0.11006511897683123], 
reward next is 0.8899, 
noisyNet noise sample is [array([0.23644216], dtype=float32), 0.10783836]. 
=============================================
[2019-04-04 06:29:26,214] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.8919922e-10 4.5558430e-08 4.1729384e-28 2.7551389e-26 6.9118404e-21
 1.0000000e+00 8.1506961e-21], sum to 1.0000
[2019-04-04 06:29:26,215] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6760
[2019-04-04 06:29:26,232] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 26.0, 25.62590840579982, 0.5808491872242142, 0.0, 1.0, 48050.12617574105], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1061400.0000, 
sim time next is 1062000.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 26.0, 25.60717674864063, 0.5865664387418789, 0.0, 1.0, 43732.28862991736], 
processed observation next is [1.0, 0.30434782608695654, 0.8310249307479226, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6339313957200524, 0.6955221462472929, 0.0, 1.0, 0.20824899347579695], 
reward next is 0.7918, 
noisyNet noise sample is [array([0.39750764], dtype=float32), -0.2985568]. 
=============================================
[2019-04-04 06:29:26,248] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[90.90348]
 [90.81128]
 [90.73429]
 [90.59388]
 [90.617  ]], R is [[90.88819122]
 [90.75049591]
 [90.61075592]
 [90.40341949]
 [90.49938965]].
[2019-04-04 06:29:29,590] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9413325e-10 2.0434536e-08 2.3176867e-28 3.9360603e-26 8.3666459e-21
 1.0000000e+00 6.4462557e-21], sum to 1.0000
[2019-04-04 06:29:29,590] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2471
[2019-04-04 06:29:29,607] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 96.66666666666666, 0.0, 0.0, 26.0, 25.07084299046677, 0.2688755455604734, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 929400.0000, 
sim time next is 930000.0000, 
raw observation next is [4.4, 97.33333333333333, 0.0, 0.0, 26.0, 24.86895971307145, 0.2533145789868047, 1.0, 1.0, 97064.194441834], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.9733333333333333, 0.0, 0.0, 0.6666666666666666, 0.5724133094226209, 0.5844381929956016, 1.0, 1.0, 0.46221044972301906], 
reward next is 0.5378, 
noisyNet noise sample is [array([1.8958852], dtype=float32), 0.17394073]. 
=============================================
[2019-04-04 06:29:29,620] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[88.68095]
 [88.75935]
 [88.86052]
 [88.95437]
 [89.20491]], R is [[88.48712921]
 [88.60225677]
 [88.7162323 ]
 [88.82907104]
 [88.94078064]].
[2019-04-04 06:29:32,941] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.23872179e-09 4.10551948e-08 1.27615575e-27 4.87782921e-25
 1.59830843e-20 1.00000000e+00 3.54975125e-20], sum to 1.0000
[2019-04-04 06:29:32,943] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4239
[2019-04-04 06:29:32,956] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.1, 92.0, 0.0, 0.0, 26.0, 25.4323113371299, 0.5768327516640165, 0.0, 1.0, 18763.94620164015], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1304400.0000, 
sim time next is 1305000.0000, 
raw observation next is [3.0, 92.0, 0.0, 0.0, 26.0, 25.47429166864557, 0.5771277229161814, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5457063711911359, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6228576390537975, 0.6923759076387271, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5332765], dtype=float32), -0.49854594]. 
=============================================
[2019-04-04 06:29:32,968] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.52091 ]
 [86.46597 ]
 [86.236984]
 [85.93604 ]
 [85.99973 ]], R is [[86.87704468]
 [86.91892242]
 [86.83532715]
 [86.70300293]
 [86.45030212]].
[2019-04-04 06:29:36,729] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3842623e-10 2.6320317e-08 7.8622834e-29 5.2974544e-27 2.0565073e-21
 1.0000000e+00 2.5972304e-21], sum to 1.0000
[2019-04-04 06:29:36,730] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9908
[2019-04-04 06:29:36,745] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.283333333333333, 96.0, 0.0, 0.0, 26.0, 24.94113261894057, 0.5467928851919767, 0.0, 1.0, 57056.28463881621], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1281000.0000, 
sim time next is 1281600.0000, 
raw observation next is [6.1, 96.0, 0.0, 0.0, 26.0, 25.08153702607306, 0.5646419600068259, 0.0, 1.0, 45396.75263667821], 
processed observation next is [0.0, 0.8695652173913043, 0.6315789473684211, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5901280855060884, 0.688213986668942, 0.0, 1.0, 0.21617501255561053], 
reward next is 0.7838, 
noisyNet noise sample is [array([0.09221969], dtype=float32), 1.5385882]. 
=============================================
[2019-04-04 06:29:40,686] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0499068e-08 4.3965892e-07 3.8179450e-25 3.0684533e-22 3.8688921e-18
 9.9999952e-01 2.0821415e-17], sum to 1.0000
[2019-04-04 06:29:40,707] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4652
[2019-04-04 06:29:40,720] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.3, 65.0, 163.0, 0.0, 26.0, 25.06707606266914, 0.4990339334045396, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1170600.0000, 
sim time next is 1171200.0000, 
raw observation next is [18.3, 65.0, 161.0, 0.0, 26.0, 25.05552281551257, 0.4982244114122328, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.5366666666666666, 0.0, 0.6666666666666666, 0.5879602346260476, 0.6660748038040776, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5642939], dtype=float32), 0.7123638]. 
=============================================
[2019-04-04 06:29:42,715] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.6978008e-11 1.0846451e-08 5.4607548e-28 8.4824451e-26 1.4205451e-21
 1.0000000e+00 9.4959987e-21], sum to 1.0000
[2019-04-04 06:29:42,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7702
[2019-04-04 06:29:42,765] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.333333333333334, 80.0, 0.0, 0.0, 26.0, 25.44032852103544, 0.4990318731000995, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1581600.0000, 
sim time next is 1582200.0000, 
raw observation next is [5.25, 80.5, 0.0, 0.0, 26.0, 25.51877657760667, 0.4951400730797249, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.60803324099723, 0.805, 0.0, 0.0, 0.6666666666666666, 0.6265647148005558, 0.665046691026575, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.94740564], dtype=float32), 0.8934375]. 
=============================================
[2019-04-04 06:29:45,286] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4308139e-10 1.0120692e-08 4.9722473e-27 3.4936687e-25 2.9797164e-21
 1.0000000e+00 2.9073415e-20], sum to 1.0000
[2019-04-04 06:29:45,286] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9311
[2019-04-04 06:29:45,363] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.45273500032571, 0.4729432189747944, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1409400.0000, 
sim time next is 1410000.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.56989485602632, 0.4634661032222374, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6308245713355266, 0.6544887010740791, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5175671], dtype=float32), 0.39518702]. 
=============================================
[2019-04-04 06:29:45,371] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[86.45326 ]
 [86.34392 ]
 [87.046524]
 [87.089935]
 [87.11808 ]], R is [[86.0411911 ]
 [86.1807785 ]
 [86.31896973]
 [86.27180481]
 [86.22512817]].
[2019-04-04 06:30:03,339] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3199913e-09 8.3076571e-08 9.7412464e-26 5.6149699e-24 1.8127526e-19
 9.9999988e-01 1.7247199e-19], sum to 1.0000
[2019-04-04 06:30:03,340] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3140
[2019-04-04 06:30:03,357] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 92.33333333333334, 0.0, 0.0, 26.0, 25.36106206291677, 0.4809078952714027, 0.0, 1.0, 45406.99926009733], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1476600.0000, 
sim time next is 1477200.0000, 
raw observation next is [2.2, 92.66666666666667, 0.0, 0.0, 26.0, 25.4355476339344, 0.4889172817397491, 0.0, 1.0, 18764.61214560379], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6196289694945335, 0.662972427246583, 0.0, 1.0, 0.08935529593144663], 
reward next is 0.9106, 
noisyNet noise sample is [array([0.73663604], dtype=float32), 0.6477175]. 
=============================================
[2019-04-04 06:30:06,555] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7329785e-10 1.8179325e-08 1.6836680e-27 1.0557862e-25 1.1345816e-20
 1.0000000e+00 3.3648647e-20], sum to 1.0000
[2019-04-04 06:30:06,555] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3067
[2019-04-04 06:30:06,622] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.1, 100.0, 55.66666666666666, 0.0, 26.0, 26.09507308475353, 0.5320292649022391, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1504200.0000, 
sim time next is 1504800.0000, 
raw observation next is [2.2, 100.0, 60.0, 0.0, 26.0, 26.129759101085, 0.5233572421458085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5235457063711911, 1.0, 0.2, 0.0, 0.6666666666666666, 0.6774799250904167, 0.6744524140486029, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.57200015], dtype=float32), -1.1733111]. 
=============================================
[2019-04-04 06:30:11,215] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.1495910e-10 3.9007901e-08 3.8375529e-27 1.7763366e-25 1.5301515e-20
 1.0000000e+00 1.4020370e-20], sum to 1.0000
[2019-04-04 06:30:11,215] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6254
[2019-04-04 06:30:11,247] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.533333333333333, 85.33333333333334, 0.0, 0.0, 26.0, 25.48338700745914, 0.5143972643350075, 0.0, 1.0, 72497.11905179311], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1568400.0000, 
sim time next is 1569000.0000, 
raw observation next is [4.566666666666666, 85.16666666666667, 0.0, 0.0, 26.0, 25.50506664695328, 0.5237436410865722, 0.0, 1.0, 39447.80464593598], 
processed observation next is [1.0, 0.13043478260869565, 0.5891043397968606, 0.8516666666666667, 0.0, 0.0, 0.6666666666666666, 0.6254222205794401, 0.6745812136955242, 0.0, 1.0, 0.18784668879017136], 
reward next is 0.8122, 
noisyNet noise sample is [array([0.9830495], dtype=float32), 1.2202829]. 
=============================================
[2019-04-04 06:30:11,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[85.28544 ]
 [85.161476]
 [85.10748 ]
 [85.11388 ]
 [84.69714 ]], R is [[85.32302856]
 [85.12458038]
 [85.03681946]
 [85.07958221]
 [85.1140213 ]].
[2019-04-04 06:30:15,863] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7252385e-10 9.0988515e-09 1.7123025e-28 2.4153257e-26 1.5471662e-21
 1.0000000e+00 2.2281317e-21], sum to 1.0000
[2019-04-04 06:30:15,863] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6688
[2019-04-04 06:30:15,906] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.66918675787471, 0.5408014082989966, 0.0, 1.0, 23016.20610904423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1658400.0000, 
sim time next is 1659000.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.5763325516335, 0.5430709819918879, 0.0, 1.0, 73679.03323563754], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6313610459694582, 0.6810236606639627, 0.0, 1.0, 0.3508525392173216], 
reward next is 0.6491, 
noisyNet noise sample is [array([-0.40227988], dtype=float32), 0.9091159]. 
=============================================
[2019-04-04 06:30:15,922] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[88.19247 ]
 [88.2253  ]
 [88.28596 ]
 [88.286575]
 [88.33317 ]], R is [[88.05407715]
 [88.06393433]
 [88.1832962 ]
 [88.3014679 ]
 [88.41845703]].
[2019-04-04 06:30:17,472] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5321352e-11 6.2957541e-09 3.7881119e-29 2.2155991e-26 2.5321152e-21
 1.0000000e+00 2.7551040e-21], sum to 1.0000
[2019-04-04 06:30:17,472] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4327
[2019-04-04 06:30:17,499] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.00000000000001, 86.66666666666667, 0.0, 26.0, 26.27952804149711, 0.6241519293494594, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1692600.0000, 
sim time next is 1693200.0000, 
raw observation next is [1.1, 88.0, 83.33333333333334, 0.0, 26.0, 26.30586506898782, 0.6246989675444394, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.88, 0.2777777777777778, 0.0, 0.6666666666666666, 0.6921554224156518, 0.7082329891814799, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3341812], dtype=float32), -0.21648583]. 
=============================================
[2019-04-04 06:30:17,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.3469996e-09 1.7062886e-08 5.6648786e-26 4.5451649e-24 8.8979327e-20
 1.0000000e+00 8.4999337e-19], sum to 1.0000
[2019-04-04 06:30:17,556] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7084
[2019-04-04 06:30:17,605] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.27926105035083, 0.4550232090596478, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1706400.0000, 
sim time next is 1707000.0000, 
raw observation next is [1.1, 88.00000000000001, 0.0, 0.0, 26.0, 25.22569947605835, 0.4614653420367969, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.8800000000000001, 0.0, 0.0, 0.6666666666666666, 0.6021416230048624, 0.6538217806789323, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.64775], dtype=float32), -3.9581141]. 
=============================================
[2019-04-04 06:30:17,617] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[83.88326 ]
 [84.055824]
 [84.27005 ]
 [84.61545 ]
 [84.835335]], R is [[83.83565521]
 [83.90834808]
 [84.06926727]
 [84.22857666]
 [84.3862915 ]].
[2019-04-04 06:30:18,392] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.6218127e-10 2.7838377e-07 3.7263586e-26 8.2811833e-24 2.0221326e-19
 9.9999976e-01 1.9507893e-18], sum to 1.0000
[2019-04-04 06:30:18,418] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1522
[2019-04-04 06:30:18,452] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.9425311726234, 0.366329931916942, 0.0, 1.0, 43610.34328494474], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1747200.0000, 
sim time next is 1747800.0000, 
raw observation next is [-0.8999999999999999, 85.0, 0.0, 0.0, 26.0, 24.98323078012751, 0.3633682249236067, 0.0, 1.0, 43639.16557613182], 
processed observation next is [0.0, 0.21739130434782608, 0.43767313019390586, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5819358983439592, 0.6211227416412023, 0.0, 1.0, 0.20780555036253245], 
reward next is 0.7922, 
noisyNet noise sample is [array([0.64479905], dtype=float32), 0.21386659]. 
=============================================
[2019-04-04 06:30:21,975] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.45357001e-09 1.70500343e-08 7.69905209e-27 2.21557600e-24
 1.32382656e-20 1.00000000e+00 4.14403838e-19], sum to 1.0000
[2019-04-04 06:30:21,976] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2690
[2019-04-04 06:30:22,019] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.42191303710545, 0.471303858052252, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1705800.0000, 
sim time next is 1706400.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.27926105035083, 0.4550232090596478, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6066050875292358, 0.6516744030198826, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.6209548], dtype=float32), -1.1302079]. 
=============================================
[2019-04-04 06:30:23,617] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-04 06:30:23,634] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:30:23,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:23,669] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:30:23,718] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:23,702] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run11
[2019-04-04 06:30:23,724] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:30:23,745] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:23,747] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run11
[2019-04-04 06:30:23,802] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run11
[2019-04-04 06:31:57,533] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23186076], dtype=float32), 0.21740289]
[2019-04-04 06:31:57,533] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [15.76666666666667, 58.16666666666667, 150.3333333333333, 482.6666666666666, 26.0, 25.95465985163105, 0.7276791122986491, 0.0, 1.0, 0.0]
[2019-04-04 06:31:57,533] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:31:57,534] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.3146372e-12 2.8077518e-09 3.9726603e-31 9.4265917e-29 4.0344366e-23
 1.0000000e+00 1.3469570e-22], sampled 0.44057607341540117
[2019-04-04 06:32:06,971] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23186076], dtype=float32), 0.21740289]
[2019-04-04 06:32:06,971] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.966666666666667, 67.0, 0.0, 0.0, 26.0, 25.39606765878476, 0.4977140692963776, 0.0, 1.0, 43036.24461037956]
[2019-04-04 06:32:06,971] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:32:06,972] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.4072986e-09 9.6038562e-08 7.6845905e-26 4.6171279e-24 2.3220332e-19
 9.9999988e-01 9.7340381e-19], sampled 0.1652809951926275
[2019-04-04 06:32:12,060] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23186076], dtype=float32), 0.21740289]
[2019-04-04 06:32:12,060] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.233333333333333, 64.66666666666667, 0.0, 0.0, 26.0, 24.95078135609, 0.4570963185837151, 1.0, 1.0, 37315.82458669235]
[2019-04-04 06:32:12,060] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:32:12,061] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.3678210e-10 3.0103440e-08 2.2855647e-25 8.0921304e-24 4.2326449e-19
 1.0000000e+00 9.9205746e-19], sampled 0.9845221470058504
[2019-04-04 06:33:40,492] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4829 239928582.5118 1604.8457
[2019-04-04 06:34:13,197] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6417 263415234.9355 1551.9605
[2019-04-04 06:34:16,981] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 06:34:18,040] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1000000, evaluation results [1000000.0, 7241.641738402564, 263415234.93545747, 1551.9605289518147, 7353.482940420242, 239928582.51175264, 1604.845674400583, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 06:34:30,999] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6584552e-08 1.1594624e-06 8.4799884e-23 8.5894209e-21 6.3127116e-17
 9.9999881e-01 1.8485349e-16], sum to 1.0000
[2019-04-04 06:34:30,999] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3853
[2019-04-04 06:34:31,030] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 23.35188681479036, -0.080959623893595, 0.0, 1.0, 47173.95685969316], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1840200.0000, 
sim time next is 1840800.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 23.32053703889784, -0.08742527369345564, 0.0, 1.0, 47192.67374697718], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.44337808657482, 0.47085824210218147, 0.0, 1.0, 0.22472701784274848], 
reward next is 0.7753, 
noisyNet noise sample is [array([-0.6712325], dtype=float32), -1.0519673]. 
=============================================
[2019-04-04 06:34:45,375] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.23669430e-10 1.24797594e-08 2.17149433e-26 4.85027735e-24
 3.24749654e-20 1.00000000e+00 9.35959387e-20], sum to 1.0000
[2019-04-04 06:34:45,376] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0415
[2019-04-04 06:34:45,458] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.566666666666666, 63.0, 132.8333333333333, 0.0, 26.0, 25.66003549865253, 0.3377458249828666, 1.0, 1.0, 23106.17957075634], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1950000.0000, 
sim time next is 1950600.0000, 
raw observation next is [-3.483333333333333, 62.5, 128.6666666666667, 0.0, 26.0, 25.69665225097055, 0.3450547280302962, 1.0, 1.0, 23003.98080399615], 
processed observation next is [1.0, 0.5652173913043478, 0.3661126500461681, 0.625, 0.42888888888888904, 0.0, 0.6666666666666666, 0.6413876875808793, 0.6150182426767654, 1.0, 1.0, 0.109542765733315], 
reward next is 0.8905, 
noisyNet noise sample is [array([1.8821515], dtype=float32), -0.5546907]. 
=============================================
[2019-04-04 06:34:48,300] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.4716002e-09 3.0853292e-07 1.5480378e-23 2.5940597e-22 7.2987522e-18
 9.9999964e-01 8.3279030e-18], sum to 1.0000
[2019-04-04 06:34:48,303] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4027
[2019-04-04 06:34:48,324] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.9, 85.0, 0.0, 0.0, 26.0, 24.20775087106731, 0.08949037669847333, 0.0, 1.0, 41328.10787200006], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2003400.0000, 
sim time next is 2004000.0000, 
raw observation next is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.2384020077733, 0.08508005896669135, 0.0, 1.0, 41093.73673040397], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5198668339811082, 0.5283600196555638, 0.0, 1.0, 0.1956844606209713], 
reward next is 0.8043, 
noisyNet noise sample is [array([-0.00656856], dtype=float32), 0.056918956]. 
=============================================
[2019-04-04 06:34:48,384] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[74.3758  ]
 [74.40926 ]
 [74.437126]
 [74.5002  ]
 [74.52024 ]], R is [[74.39679718]
 [74.4560318 ]
 [74.51337433]
 [74.57165527]
 [74.62945557]].
[2019-04-04 06:34:54,050] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1416255e-09 7.0460324e-08 9.2773494e-25 9.8494358e-23 4.8163641e-19
 9.9999988e-01 3.9273766e-18], sum to 1.0000
[2019-04-04 06:34:54,050] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7885
[2019-04-04 06:34:54,115] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 96.5, 0.0, 26.0, 25.52108212884843, 0.2911359684199192, 1.0, 1.0, 31832.56645898171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2024400.0000, 
sim time next is 2025000.0000, 
raw observation next is [-5.6, 83.0, 102.0, 0.0, 26.0, 25.49842948357249, 0.2959951047115577, 1.0, 1.0, 32437.77835512752], 
processed observation next is [1.0, 0.43478260869565216, 0.30747922437673136, 0.83, 0.34, 0.0, 0.6666666666666666, 0.6248691236310409, 0.5986650349038526, 1.0, 1.0, 0.15446561121489297], 
reward next is 0.8455, 
noisyNet noise sample is [array([-0.7305441], dtype=float32), -0.6811724]. 
=============================================
[2019-04-04 06:34:54,118] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[76.97788]
 [76.66354]
 [76.43965]
 [76.11678]
 [75.8666 ]], R is [[77.38265991]
 [77.4572525 ]
 [77.53794098]
 [77.63269806]
 [77.74365234]].
[2019-04-04 06:35:01,829] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.2061009e-10 1.1256066e-08 1.0345324e-25 7.6913142e-24 1.2754357e-19
 1.0000000e+00 1.9807080e-19], sum to 1.0000
[2019-04-04 06:35:01,830] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8125
[2019-04-04 06:35:01,898] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.02759441158878, 0.3730095664399344, 0.0, 1.0, 123450.2149355015], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2145600.0000, 
sim time next is 2146200.0000, 
raw observation next is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 25.03829442769483, 0.3901295052077132, 0.0, 1.0, 72001.69722823374], 
processed observation next is [1.0, 0.8695652173913043, 0.3074792243767313, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5865245356412357, 0.630043168402571, 0.0, 1.0, 0.34286522489635113], 
reward next is 0.6571, 
noisyNet noise sample is [array([0.59863794], dtype=float32), -0.14820361]. 
=============================================
[2019-04-04 06:35:02,109] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3904778e-09 2.9578706e-07 4.9456155e-25 1.2986450e-23 2.3730042e-18
 9.9999964e-01 7.3049081e-18], sum to 1.0000
[2019-04-04 06:35:02,113] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0426
[2019-04-04 06:35:02,187] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 65.0, 130.0, 405.0, 26.0, 25.10843577683385, 0.2964394507876238, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2368800.0000, 
sim time next is 2369400.0000, 
raw observation next is [-2.716666666666667, 64.5, 133.0, 420.0, 26.0, 25.04547068049989, 0.2871340569814284, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3873499538319483, 0.645, 0.44333333333333336, 0.46408839779005523, 0.6666666666666666, 0.5871225567083241, 0.5957113523271428, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.484693], dtype=float32), 0.68758273]. 
=============================================
[2019-04-04 06:35:07,437] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0215568e-09 1.0013110e-06 5.6899180e-23 1.3182914e-21 3.4671527e-17
 9.9999905e-01 6.0458472e-17], sum to 1.0000
[2019-04-04 06:35:07,437] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3814
[2019-04-04 06:35:07,473] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.13852170005622, 0.1116471860467236, 0.0, 1.0, 41189.56669257415], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2355000.0000, 
sim time next is 2355600.0000, 
raw observation next is [-3.0, 66.33333333333334, 0.0, 0.0, 26.0, 24.21362894256016, 0.1088185025260333, 0.0, 1.0, 41173.27378551433], 
processed observation next is [0.0, 0.2608695652173913, 0.3795013850415513, 0.6633333333333334, 0.0, 0.0, 0.6666666666666666, 0.5178024118800133, 0.5362728341753444, 0.0, 1.0, 0.19606320850244918], 
reward next is 0.8039, 
noisyNet noise sample is [array([0.06416215], dtype=float32), 0.848964]. 
=============================================
[2019-04-04 06:35:09,824] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1747680e-09 4.6963834e-08 2.1275634e-24 2.9063946e-23 2.0072398e-18
 1.0000000e+00 3.8204371e-18], sum to 1.0000
[2019-04-04 06:35:09,824] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2523
[2019-04-04 06:35:09,876] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.933333333333334, 70.83333333333334, 0.0, 0.0, 26.0, 25.0021967894088, 0.3785890143777702, 0.0, 1.0, 84415.33399310205], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2231400.0000, 
sim time next is 2232000.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 25.05332424933608, 0.3887733325942085, 1.0, 1.0, 33677.15949494702], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5877770207780065, 0.6295911108647362, 1.0, 1.0, 0.1603674261664144], 
reward next is 0.8396, 
noisyNet noise sample is [array([0.36307865], dtype=float32), -0.33209184]. 
=============================================
[2019-04-04 06:35:09,898] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.19063 ]
 [75.570755]
 [75.463264]
 [75.32099 ]
 [75.58608 ]], R is [[75.8704071 ]
 [75.70972443]
 [75.30636597]
 [75.02578735]
 [75.12117767]].
[2019-04-04 06:35:17,538] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3431010e-07 9.3579047e-07 1.5136035e-21 3.7741628e-20 2.0884574e-16
 9.9999893e-01 1.3263864e-15], sum to 1.0000
[2019-04-04 06:35:17,541] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7741
[2019-04-04 06:35:17,579] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.483333333333334, 60.83333333333334, 0.0, 0.0, 26.0, 23.34730422054916, -0.1210125519165511, 0.0, 1.0, 44337.34298199186], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2437800.0000, 
sim time next is 2438400.0000, 
raw observation next is [-8.566666666666666, 60.66666666666667, 0.0, 0.0, 26.0, 23.31678559082367, -0.1279216813058746, 0.0, 1.0, 44295.88722969563], 
processed observation next is [0.0, 0.21739130434782608, 0.22530009233610343, 0.6066666666666667, 0.0, 0.0, 0.6666666666666666, 0.44306546590197254, 0.4573594395647085, 0.0, 1.0, 0.21093279633188397], 
reward next is 0.7891, 
noisyNet noise sample is [array([0.0781113], dtype=float32), -0.650637]. 
=============================================
[2019-04-04 06:35:29,152] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.42823104e-08 8.70170766e-07 2.90020408e-21 3.39811644e-20
 6.24061979e-17 9.99999166e-01 2.63735064e-16], sum to 1.0000
[2019-04-04 06:35:29,152] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3082
[2019-04-04 06:35:29,188] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.16666666666667, 83.0, 0.0, 0.0, 26.0, 22.84545216427158, -0.1805547531361164, 0.0, 1.0, 43191.58627062643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2703000.0000, 
sim time next is 2703600.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.78028221356607, -0.1963134636358817, 0.0, 1.0, 43241.98676735981], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.6666666666666666, 0.39835685113050595, 0.43456217878803943, 0.0, 1.0, 0.2059142227017134], 
reward next is 0.7941, 
noisyNet noise sample is [array([0.88418907], dtype=float32), 0.9457474]. 
=============================================
[2019-04-04 06:35:41,195] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3860810e-08 2.0735803e-07 1.6916540e-23 4.5146174e-22 5.2538666e-18
 9.9999976e-01 6.8016845e-17], sum to 1.0000
[2019-04-04 06:35:41,197] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1307
[2019-04-04 06:35:41,228] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.1, 60.0, 0.0, 0.0, 26.0, 24.96418304879907, 0.2882028180706119, 0.0, 1.0, 41952.53973150285], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2589600.0000, 
sim time next is 2590200.0000, 
raw observation next is [-4.2, 60.5, 0.0, 0.0, 26.0, 24.91212441863546, 0.2826067812427869, 0.0, 1.0, 41907.53943325015], 
processed observation next is [1.0, 1.0, 0.34626038781163443, 0.605, 0.0, 0.0, 0.6666666666666666, 0.5760103682196217, 0.5942022604142623, 0.0, 1.0, 0.1995597115869055], 
reward next is 0.8004, 
noisyNet noise sample is [array([0.9688585], dtype=float32), 2.738206]. 
=============================================
[2019-04-04 06:35:42,121] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4728799e-10 4.2064798e-08 1.7592409e-26 1.1242950e-24 9.5873270e-20
 1.0000000e+00 2.4172516e-19], sum to 1.0000
[2019-04-04 06:35:42,122] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7182
[2019-04-04 06:35:42,146] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.92106868093198, 0.2595991851994986, 0.0, 1.0, 55605.37895615329], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2865600.0000, 
sim time next is 2866200.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.9216746468656, 0.2578790663546431, 0.0, 1.0, 55707.94438553952], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5768062205721334, 0.585959688784881, 0.0, 1.0, 0.2652759256454263], 
reward next is 0.7347, 
noisyNet noise sample is [array([-0.9749258], dtype=float32), 2.2767882]. 
=============================================
[2019-04-04 06:35:48,391] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2828791e-08 1.8458992e-07 3.3003182e-23 4.0761281e-22 1.2319400e-17
 9.9999976e-01 1.2722248e-17], sum to 1.0000
[2019-04-04 06:35:48,398] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4332
[2019-04-04 06:35:48,441] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 87.0, 0.0, 0.0, 26.0, 23.40160163833825, -0.03562714537551242, 0.0, 1.0, 44318.26056192593], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2694600.0000, 
sim time next is 2695200.0000, 
raw observation next is [-15.0, 85.66666666666666, 0.0, 0.0, 26.0, 23.35208161083843, -0.04195298078915421, 0.0, 1.0, 44216.51554865587], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.8566666666666666, 0.0, 0.0, 0.6666666666666666, 0.4460068009032024, 0.486015673070282, 0.0, 1.0, 0.21055483594598035], 
reward next is 0.7894, 
noisyNet noise sample is [array([-1.1485115], dtype=float32), 1.04274]. 
=============================================
[2019-04-04 06:35:49,824] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0598141e-08 3.2373129e-07 8.8678552e-24 2.3499025e-22 9.4726739e-18
 9.9999964e-01 3.2302647e-17], sum to 1.0000
[2019-04-04 06:35:49,825] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4655
[2019-04-04 06:35:49,863] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 26.0, 24.98195991395398, 0.3053281929024085, 0.0, 1.0, 55391.39491792454], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2852400.0000, 
sim time next is 2853000.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 26.0, 25.00591589878144, 0.3098523713169066, 0.0, 1.0, 54839.31370729444], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5838263248984532, 0.6032841237723022, 0.0, 1.0, 0.26113958908235446], 
reward next is 0.7389, 
noisyNet noise sample is [array([-0.22879599], dtype=float32), 1.6681526]. 
=============================================
[2019-04-04 06:35:49,890] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.01635]
 [76.27883]
 [76.5357 ]
 [77.07471]
 [76.97736]], R is [[75.64775085]
 [75.62750244]
 [75.60993958]
 [75.60522461]
 [75.62721252]].
[2019-04-04 06:35:50,427] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1120447e-09 2.9979329e-08 1.2869422e-24 8.8814385e-23 1.2796519e-18
 1.0000000e+00 2.0528971e-18], sum to 1.0000
[2019-04-04 06:35:50,427] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1558
[2019-04-04 06:35:50,522] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.333333333333334, 32.33333333333334, 0.0, 0.0, 26.0, 24.62605158435519, 0.1976765124467284, 1.0, 1.0, 196217.9094192962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2830800.0000, 
sim time next is 2831400.0000, 
raw observation next is [4.0, 33.5, 0.0, 0.0, 26.0, 23.53622986955531, 0.1331194530708159, 1.0, 1.0, 196605.2886036068], 
processed observation next is [1.0, 0.782608695652174, 0.5734072022160666, 0.335, 0.0, 0.0, 0.6666666666666666, 0.4613524891296092, 0.5443731510236053, 1.0, 1.0, 0.9362156600171753], 
reward next is 0.0638, 
noisyNet noise sample is [array([0.5809784], dtype=float32), -0.019157762]. 
=============================================
[2019-04-04 06:35:52,265] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.9604972e-11 1.7675638e-09 1.0425373e-28 1.4079821e-26 1.7819326e-21
 1.0000000e+00 5.7655054e-21], sum to 1.0000
[2019-04-04 06:35:52,265] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9819
[2019-04-04 06:35:52,304] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 77.0, 78.0, 26.0, 25.34201807389499, 0.31610219961765, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2883600.0000, 
sim time next is 2884200.0000, 
raw observation next is [0.8333333333333334, 94.16666666666666, 67.66666666666666, 51.99999999999999, 26.0, 25.39065949466492, 0.3090530359953247, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4856879039704525, 0.9416666666666665, 0.22555555555555554, 0.0574585635359116, 0.6666666666666666, 0.6158882912220767, 0.6030176786651082, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1105411], dtype=float32), 0.34829575]. 
=============================================
[2019-04-04 06:36:05,389] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1119299e-10 4.8428571e-08 1.4581282e-26 7.4168103e-24 1.6340850e-19
 1.0000000e+00 4.5346641e-19], sum to 1.0000
[2019-04-04 06:36:05,389] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7341
[2019-04-04 06:36:05,408] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.92150083109703, 0.2578535115490549, 0.0, 1.0, 55707.98851820728], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2866200.0000, 
sim time next is 2866800.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.90897858592363, 0.2502867983873431, 0.0, 1.0, 55732.55922907781], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5757482154936359, 0.583428932795781, 0.0, 1.0, 0.2653931391860848], 
reward next is 0.7346, 
noisyNet noise sample is [array([-0.94481456], dtype=float32), -0.19028272]. 
=============================================
[2019-04-04 06:36:07,374] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.3869555e-11 9.6111821e-09 6.8689682e-29 8.4331734e-27 7.9156075e-22
 1.0000000e+00 1.2581882e-20], sum to 1.0000
[2019-04-04 06:36:07,377] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5040
[2019-04-04 06:36:07,389] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 100.0, 160.3333333333333, 0.0, 26.0, 25.41083249829011, 0.3010179363596304, 1.0, 1.0, 9340.205835115268], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2895600.0000, 
sim time next is 2896200.0000, 
raw observation next is [1.5, 100.0, 175.0, 0.0, 26.0, 25.2464819603237, 0.3029934105875493, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.5217391304347826, 0.5041551246537397, 1.0, 0.5833333333333334, 0.0, 0.6666666666666666, 0.6038734966936415, 0.6009978035291831, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.74893093], dtype=float32), 1.1267282]. 
=============================================
[2019-04-04 06:36:11,264] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6469721e-09 2.4206014e-07 7.4405804e-25 6.0732483e-23 1.6211153e-18
 9.9999976e-01 2.2081148e-18], sum to 1.0000
[2019-04-04 06:36:11,264] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8818
[2019-04-04 06:36:11,308] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.3149930947504, 0.1959364919631471, 0.0, 1.0, 42946.87170733259], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2955000.0000, 
sim time next is 2955600.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.28780752491831, 0.1935089505011843, 0.0, 1.0, 42892.14186863689], 
processed observation next is [0.0, 0.21739130434782608, 0.3795013850415513, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5239839604098592, 0.5645029835003947, 0.0, 1.0, 0.20424829461255664], 
reward next is 0.7958, 
noisyNet noise sample is [array([-0.43394798], dtype=float32), -0.9038081]. 
=============================================
[2019-04-04 06:36:20,362] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3868616e-09 4.0905512e-08 6.0774179e-24 5.9102456e-23 1.2664043e-18
 1.0000000e+00 4.1659612e-18], sum to 1.0000
[2019-04-04 06:36:20,363] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9285
[2019-04-04 06:36:20,393] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.57171055093519, 0.5116365897872045, 1.0, 1.0, 31643.83781442364], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3352200.0000, 
sim time next is 3352800.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.54512288480654, 0.4952492178570551, 0.0, 1.0, 27162.38975319732], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6287602404005449, 0.6650830726190183, 0.0, 1.0, 0.12934471311046342], 
reward next is 0.8707, 
noisyNet noise sample is [array([0.24327108], dtype=float32), -0.025807979]. 
=============================================
[2019-04-04 06:36:22,174] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.3391339e-10 3.6441843e-08 7.5887991e-28 1.1783943e-25 1.4436374e-20
 1.0000000e+00 7.9768120e-21], sum to 1.0000
[2019-04-04 06:36:22,175] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6621
[2019-04-04 06:36:22,191] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.933333333333333, 100.0, 0.0, 0.0, 26.0, 25.26571143107743, 0.2988054857182996, 0.0, 1.0, 53785.88703564648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3127800.0000, 
sim time next is 3128400.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 26.0, 25.28956130203186, 0.3264436886657111, 0.0, 1.0, 54132.99915068083], 
processed observation next is [1.0, 0.21739130434782608, 0.5457063711911359, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6074634418359883, 0.6088145628885704, 0.0, 1.0, 0.2577761864318135], 
reward next is 0.7422, 
noisyNet noise sample is [array([-0.65485823], dtype=float32), 1.0535531]. 
=============================================
[2019-04-04 06:36:22,882] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0841765e-10 1.4384376e-08 4.7145099e-28 7.1671995e-26 1.7234950e-21
 1.0000000e+00 2.8362905e-20], sum to 1.0000
[2019-04-04 06:36:22,884] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5702
[2019-04-04 06:36:22,896] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 97.66666666666667, 0.0, 0.0, 26.0, 25.73821190903102, 0.5991541469110636, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3190800.0000, 
sim time next is 3191400.0000, 
raw observation next is [2.0, 96.5, 0.0, 0.0, 26.0, 25.54786952532562, 0.5904150264682198, 0.0, 1.0, 165034.9343781175], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.965, 0.0, 0.0, 0.6666666666666666, 0.6289891271104683, 0.6968050088227399, 0.0, 1.0, 0.7858806398957976], 
reward next is 0.2141, 
noisyNet noise sample is [array([0.639349], dtype=float32), 1.2656075]. 
=============================================
[2019-04-04 06:36:23,812] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.5284293e-11 2.6598119e-09 7.6017216e-28 3.5838509e-26 6.1166408e-21
 1.0000000e+00 4.9293728e-21], sum to 1.0000
[2019-04-04 06:36:23,812] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6142
[2019-04-04 06:36:23,864] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.9999999999999999, 52.0, 100.6666666666667, 675.6666666666666, 26.0, 26.42454892195974, 0.5762938295222073, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3404400.0000, 
sim time next is 3405000.0000, 
raw observation next is [1.5, 50.0, 102.3333333333333, 693.3333333333334, 26.0, 26.48737067929524, 0.5839664800785161, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5041551246537397, 0.5, 0.341111111111111, 0.7661141804788214, 0.6666666666666666, 0.7072808899412699, 0.6946554933595054, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.87936157], dtype=float32), 0.61004156]. 
=============================================
[2019-04-04 06:36:23,884] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.52591]
 [84.60317]
 [83.5264 ]
 [82.37904]
 [81.40236]], R is [[86.44962311]
 [86.58512878]
 [86.71927643]
 [86.8520813 ]
 [86.98355865]].
[2019-04-04 06:36:25,501] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.97831040e-10 4.51451285e-08 1.20778896e-26 3.55035331e-24
 5.23509685e-20 1.00000000e+00 8.45317685e-19], sum to 1.0000
[2019-04-04 06:36:25,505] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6486
[2019-04-04 06:36:25,592] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 0.0, 0.0, 26.0, 25.13807498244271, 0.5060226536471876, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3223200.0000, 
sim time next is 3223800.0000, 
raw observation next is [-3.0, 92.0, 1.0, 82.0, 26.0, 25.57202963728075, 0.5046734545695873, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.0033333333333333335, 0.09060773480662983, 0.6666666666666666, 0.6310024697733958, 0.6682244848565291, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.61669904], dtype=float32), 0.60931766]. 
=============================================
[2019-04-04 06:36:40,386] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.6839054e-11 4.8065054e-09 8.4505588e-28 9.9426266e-26 2.5415809e-20
 1.0000000e+00 6.9734953e-21], sum to 1.0000
[2019-04-04 06:36:40,392] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7753
[2019-04-04 06:36:40,410] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 67.0, 0.0, 0.0, 26.0, 25.71800251528163, 0.3730359529289249, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3433800.0000, 
sim time next is 3434400.0000, 
raw observation next is [2.0, 67.0, 0.0, 0.0, 26.0, 25.73252906748167, 0.4904597929728567, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6443774222901393, 0.6634865976576189, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33600104], dtype=float32), 0.10304752]. 
=============================================
[2019-04-04 06:36:57,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1191688e-09 4.2966182e-08 3.5847550e-25 3.1370507e-24 1.5805262e-19
 1.0000000e+00 7.3316787e-19], sum to 1.0000
[2019-04-04 06:36:57,910] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5273
[2019-04-04 06:36:57,927] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 76.0, 0.0, 0.0, 26.0, 25.43530183986806, 0.4682592843586817, 0.0, 1.0, 66494.05079217438], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3795000.0000, 
sim time next is 3795600.0000, 
raw observation next is [-3.0, 75.0, 0.0, 0.0, 26.0, 25.45886588856584, 0.4584494664845432, 0.0, 1.0, 37584.97524802411], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6215721573804865, 0.6528164888281811, 0.0, 1.0, 0.1789760726096386], 
reward next is 0.8210, 
noisyNet noise sample is [array([0.7971039], dtype=float32), 0.016409094]. 
=============================================
[2019-04-04 06:37:11,264] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.6225180e-11 5.9555294e-09 2.4222156e-28 1.3724887e-26 5.4397546e-21
 1.0000000e+00 1.5788080e-20], sum to 1.0000
[2019-04-04 06:37:11,275] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0153
[2019-04-04 06:37:11,312] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.5, 36.0, 93.0, 437.0, 26.0, 27.37713713323366, 0.5745391604330627, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4120200.0000, 
sim time next is 4120800.0000, 
raw observation next is [3.333333333333333, 36.33333333333333, 81.33333333333333, 373.6666666666667, 26.0, 27.33040898927599, 0.7739767051327252, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5549399815327793, 0.3633333333333333, 0.2711111111111111, 0.41289134438305714, 0.6666666666666666, 0.7775340824396659, 0.7579922350442417, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7861558], dtype=float32), -0.51701266]. 
=============================================
[2019-04-04 06:37:11,913] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1525981e-08 3.4086406e-07 3.6220931e-22 4.8238233e-21 2.0032864e-16
 9.9999964e-01 1.2106639e-16], sum to 1.0000
[2019-04-04 06:37:11,915] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7758
[2019-04-04 06:37:11,933] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.66666666666667, 61.33333333333334, 0.0, 0.0, 26.0, 24.62478141731963, 0.2344302918895754, 0.0, 1.0, 43769.715718567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3980400.0000, 
sim time next is 3981000.0000, 
raw observation next is [-11.83333333333333, 62.16666666666666, 0.0, 0.0, 26.0, 24.54963876121617, 0.2177932918793008, 0.0, 1.0, 43784.34116956188], 
processed observation next is [1.0, 0.043478260869565216, 0.13481071098799638, 0.6216666666666666, 0.0, 0.0, 0.6666666666666666, 0.5458032301013475, 0.5725977639597669, 0.0, 1.0, 0.20849686271219942], 
reward next is 0.7915, 
noisyNet noise sample is [array([0.01128974], dtype=float32), -0.6364903]. 
=============================================
[2019-04-04 06:37:11,974] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[67.23009]
 [67.07017]
 [66.95736]
 [67.09992]
 [67.19903]], R is [[67.41220093]
 [67.52965546]
 [67.64597321]
 [67.76100922]
 [67.87462616]].
[2019-04-04 06:37:26,811] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.5258017e-09 2.8972133e-07 3.7781487e-24 5.0737306e-23 2.7281576e-17
 9.9999976e-01 4.6237684e-18], sum to 1.0000
[2019-04-04 06:37:26,813] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1313
[2019-04-04 06:37:26,833] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.66666666666666, 0.0, 0.0, 26.0, 25.38172546124664, 0.3301112130326635, 0.0, 1.0, 34996.03634585295], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4252200.0000, 
sim time next is 4252800.0000, 
raw observation next is [3.0, 46.33333333333334, 0.0, 0.0, 26.0, 25.3899692348828, 0.3275504737406975, 0.0, 1.0, 32805.49659303593], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.46333333333333343, 0.0, 0.0, 0.6666666666666666, 0.6158307695735665, 0.6091834912468992, 0.0, 1.0, 0.15621665044302824], 
reward next is 0.8438, 
noisyNet noise sample is [array([-0.81353486], dtype=float32), -0.7965799]. 
=============================================
[2019-04-04 06:37:39,226] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6421231e-09 3.9249482e-08 1.6245652e-24 4.7311507e-23 1.9025344e-18
 1.0000000e+00 8.2951864e-18], sum to 1.0000
[2019-04-04 06:37:39,226] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1260
[2019-04-04 06:37:39,244] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.47718819696391, 0.507765451031077, 0.0, 1.0, 45632.54109091942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4667400.0000, 
sim time next is 4668000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.48417486052329, 0.5124546789644756, 0.0, 1.0, 33659.64364107633], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6236812383769408, 0.6708182263214919, 0.0, 1.0, 0.1602840173384587], 
reward next is 0.8397, 
noisyNet noise sample is [array([-0.2725186], dtype=float32), 0.9186062]. 
=============================================
[2019-04-04 06:37:39,257] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[77.353615]
 [77.84623 ]
 [78.20726 ]
 [78.61783 ]
 [79.50553 ]], R is [[77.2231369 ]
 [77.23360443]
 [77.16049194]
 [77.09397888]
 [77.11790466]].
[2019-04-04 06:37:39,864] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3321965e-11 1.2637511e-09 4.6783564e-28 5.3881308e-26 1.0489557e-21
 1.0000000e+00 3.7915827e-21], sum to 1.0000
[2019-04-04 06:37:39,864] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7228
[2019-04-04 06:37:39,894] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 65.5, 164.0, 509.0, 26.0, 26.10797153804938, 0.5696292845503337, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4613400.0000, 
sim time next is 4614000.0000, 
raw observation next is [-0.6666666666666667, 63.66666666666667, 158.1666666666667, 552.0, 26.0, 26.24375816658005, 0.5837525168222993, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.44413665743305636, 0.6366666666666667, 0.5272222222222224, 0.6099447513812155, 0.6666666666666666, 0.6869798472150043, 0.6945841722740997, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1798629], dtype=float32), 1.2591162]. 
=============================================
[2019-04-04 06:37:39,902] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[86.9922  ]
 [85.97973 ]
 [84.96992 ]
 [83.896065]
 [82.8287  ]], R is [[88.13880157]
 [88.25741577]
 [88.37483978]
 [88.49108887]
 [88.60617828]].
[2019-04-04 06:37:42,847] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.0588151e-10 6.8271390e-08 2.1982333e-24 7.4698817e-23 1.4088687e-18
 9.9999988e-01 1.2411573e-17], sum to 1.0000
[2019-04-04 06:37:42,847] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9116
[2019-04-04 06:37:42,874] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 67.33333333333333, 0.0, 0.0, 26.0, 24.81127718934649, 0.2322971703744712, 0.0, 1.0, 39244.21998023891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4855200.0000, 
sim time next is 4855800.0000, 
raw observation next is [-3.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.78805199118534, 0.2318685166996096, 0.0, 1.0, 39293.23576882304], 
processed observation next is [0.0, 0.17391304347826086, 0.3564173591874424, 0.6916666666666668, 0.0, 0.0, 0.6666666666666666, 0.5656709992654451, 0.5772895055665365, 0.0, 1.0, 0.18711064651820494], 
reward next is 0.8129, 
noisyNet noise sample is [array([-0.14421308], dtype=float32), 1.1127529]. 
=============================================
[2019-04-04 06:37:54,851] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.87760407e-09 5.99716943e-09 1.01953135e-25 1.43153149e-24
 1.15539851e-19 1.00000000e+00 2.12028505e-19], sum to 1.0000
[2019-04-04 06:37:54,852] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5548
[2019-04-04 06:37:54,895] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.07691521782707, 0.4731473005908312, 1.0, 1.0, 25704.85573417582], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4734600.0000, 
sim time next is 4735200.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.25993240487243, 0.4708894417617211, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6049943670727025, 0.656963147253907, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.4330738], dtype=float32), -0.89005107]. 
=============================================
[2019-04-04 06:37:57,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:37:57,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:37:57,167] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run9
[2019-04-04 06:37:58,157] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.35134221e-11 1.25395472e-09 1.16024655e-29 1.29410732e-28
 1.03694012e-22 1.00000000e+00 8.70095958e-22], sum to 1.0000
[2019-04-04 06:37:58,157] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6522
[2019-04-04 06:37:58,170] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.0, 26.0, 106.1666666666667, 811.5, 26.0, 27.634666734034, 0.8713692954286429, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4977600.0000, 
sim time next is 4978200.0000, 
raw observation next is [8.0, 26.0, 103.3333333333333, 804.0, 26.0, 27.70022328603158, 0.8858373440458788, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.34444444444444433, 0.8883977900552487, 0.6666666666666666, 0.8083519405026317, 0.7952791146819597, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27579886], dtype=float32), 0.8783186]. 
=============================================
[2019-04-04 06:37:59,791] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.2115409e-09 5.5922555e-07 1.9140730e-24 7.9083552e-23 3.3725380e-18
 9.9999940e-01 2.9732813e-18], sum to 1.0000
[2019-04-04 06:37:59,795] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8705
[2019-04-04 06:37:59,807] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 47.33333333333333, 0.0, 0.0, 26.0, 25.47871961936477, 0.3777987660440134, 0.0, 1.0, 29969.4676007684], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5031600.0000, 
sim time next is 5032200.0000, 
raw observation next is [-1.0, 46.66666666666667, 0.0, 0.0, 26.0, 25.46156483180701, 0.365748382351802, 0.0, 1.0, 42170.96417722209], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 0.46666666666666673, 0.0, 0.0, 0.6666666666666666, 0.6217970693172509, 0.6219161274506007, 0.0, 1.0, 0.20081411512962902], 
reward next is 0.7992, 
noisyNet noise sample is [array([0.2219996], dtype=float32), -0.3753779]. 
=============================================
[2019-04-04 06:38:00,305] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4610042e-08 3.9613568e-08 3.6601758e-24 8.7026466e-23 3.8260664e-18
 9.9999988e-01 6.6900776e-18], sum to 1.0000
[2019-04-04 06:38:00,306] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7094
[2019-04-04 06:38:00,316] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666667, 48.33333333333334, 0.0, 0.0, 26.0, 25.50561980709081, 0.4389407700509442, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4825200.0000, 
sim time next is 4825800.0000, 
raw observation next is [0.5, 49.0, 0.0, 0.0, 26.0, 25.53019112753844, 0.4306057922326614, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4764542936288089, 0.49, 0.0, 0.0, 0.6666666666666666, 0.62751592729487, 0.6435352640775538, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9574155], dtype=float32), -0.8307761]. 
=============================================
[2019-04-04 06:38:00,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:00,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:00,567] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run9
[2019-04-04 06:38:01,651] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:01,652] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:01,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run9
[2019-04-04 06:38:03,700] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.8486794e-12 8.1550997e-09 1.1137448e-28 1.7641122e-27 1.2878871e-21
 1.0000000e+00 2.1725935e-21], sum to 1.0000
[2019-04-04 06:38:03,701] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1088
[2019-04-04 06:38:03,732] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.066666666666667, 46.66666666666667, 281.6666666666666, 362.6666666666667, 26.0, 25.0530032667596, 0.3570284974588975, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4882200.0000, 
sim time next is 4882800.0000, 
raw observation next is [1.133333333333333, 46.33333333333334, 281.3333333333334, 376.3333333333333, 26.0, 25.09101525188339, 0.3629240998646095, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.49399815327793173, 0.46333333333333343, 0.937777777777778, 0.4158379373848987, 0.6666666666666666, 0.5909179376569492, 0.6209746999548699, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8555447], dtype=float32), -1.2746712]. 
=============================================
[2019-04-04 06:38:04,222] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:04,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:04,226] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run9
[2019-04-04 06:38:05,130] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:05,131] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:05,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run9
[2019-04-04 06:38:05,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:05,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:05,331] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run9
[2019-04-04 06:38:08,047] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:08,048] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:08,084] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run9
[2019-04-04 06:38:08,278] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.7143822e-09 1.0022799e-06 6.5664880e-24 2.6917711e-22 1.4769712e-17
 9.9999905e-01 1.5902996e-17], sum to 1.0000
[2019-04-04 06:38:08,280] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2899
[2019-04-04 06:38:08,307] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.166666666666667, 46.66666666666667, 0.0, 0.0, 26.0, 25.18812738545041, 0.2630615125128979, 0.0, 1.0, 38472.55132699708], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4943400.0000, 
sim time next is 4944000.0000, 
raw observation next is [-2.333333333333333, 47.33333333333334, 0.0, 0.0, 26.0, 25.17562078485857, 0.2606333184402299, 0.0, 1.0, 38529.60149090135], 
processed observation next is [1.0, 0.21739130434782608, 0.3979686057248385, 0.47333333333333344, 0.0, 0.0, 0.6666666666666666, 0.5979683987382142, 0.5868777728134099, 0.0, 1.0, 0.18347429281381594], 
reward next is 0.8165, 
noisyNet noise sample is [array([-0.22071391], dtype=float32), -0.49342206]. 
=============================================
[2019-04-04 06:38:08,318] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[76.64446 ]
 [76.780876]
 [76.902794]
 [77.034996]
 [77.11486 ]], R is [[76.60453796]
 [76.6552887 ]
 [76.70579529]
 [76.755867  ]
 [76.80567169]].
[2019-04-04 06:38:09,931] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9537378e-10 1.6278445e-08 1.3835323e-27 6.0504399e-26 6.9255648e-21
 1.0000000e+00 7.5177767e-20], sum to 1.0000
[2019-04-04 06:38:09,931] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6794
[2019-04-04 06:38:09,992] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 75.0, 0.0, 26.0, 24.27588328884637, 0.08595440321509079, 0.0, 1.0, 18776.34583035036], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 41400.0000, 
sim time next is 42000.0000, 
raw observation next is [7.699999999999999, 93.0, 78.5, 0.0, 26.0, 24.3000600866242, 0.08851961705748461, 0.0, 1.0, 18770.26966259547], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.26166666666666666, 0.0, 0.6666666666666666, 0.5250050072186833, 0.5295065390191616, 0.0, 1.0, 0.08938223648854986], 
reward next is 0.9106, 
noisyNet noise sample is [array([1.4396924], dtype=float32), 0.42555365]. 
=============================================
[2019-04-04 06:38:09,995] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[88.66888]
 [88.39538]
 [88.08982]
 [87.74909]
 [87.31884]], R is [[88.82377625]
 [88.84613037]
 [88.86823273]
 [88.76678467]
 [88.61082458]].
[2019-04-04 06:38:13,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:13,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:13,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run9
[2019-04-04 06:38:13,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:13,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:13,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run9
[2019-04-04 06:38:13,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:13,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:13,558] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run9
[2019-04-04 06:38:13,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:13,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:13,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run9
[2019-04-04 06:38:14,328] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.7913154e-10 5.7920269e-09 7.9804405e-27 9.8791536e-25 6.1748787e-20
 1.0000000e+00 7.7633175e-19], sum to 1.0000
[2019-04-04 06:38:14,329] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3285
[2019-04-04 06:38:14,358] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3666666666666667, 95.33333333333333, 0.0, 0.0, 26.0, 24.46568858223292, 0.1828724688388799, 0.0, 1.0, 40128.48567008018], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 81600.0000, 
sim time next is 82200.0000, 
raw observation next is [0.3333333333333333, 95.16666666666667, 0.0, 0.0, 26.0, 24.44135121582549, 0.1782999223532051, 0.0, 1.0, 40103.26819107019], 
processed observation next is [0.0, 0.9565217391304348, 0.4718374884579871, 0.9516666666666667, 0.0, 0.0, 0.6666666666666666, 0.5367792679854576, 0.5594333074510683, 0.0, 1.0, 0.19096794376700088], 
reward next is 0.8090, 
noisyNet noise sample is [array([0.4467952], dtype=float32), 0.04355372]. 
=============================================
[2019-04-04 06:38:16,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:16,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:16,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run9
[2019-04-04 06:38:16,416] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:16,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:16,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run9
[2019-04-04 06:38:16,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:16,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:16,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run9
[2019-04-04 06:38:16,652] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:16,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:16,659] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run9
[2019-04-04 06:38:17,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:38:17,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:38:17,536] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run9
[2019-04-04 06:38:18,747] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.0073126e-10 1.3586623e-07 7.1689248e-25 5.3230054e-24 1.3168369e-18
 9.9999988e-01 1.0700949e-18], sum to 1.0000
[2019-04-04 06:38:18,750] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8218
[2019-04-04 06:38:18,785] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.416666666666667, 74.83333333333333, 0.0, 0.0, 26.0, 23.37269961048132, -0.06395484839273313, 0.0, 1.0, 44885.9793918526], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 107400.0000, 
sim time next is 108000.0000, 
raw observation next is [-6.7, 75.0, 0.0, 0.0, 26.0, 23.31655173008694, -0.06945395021849109, 0.0, 1.0, 45096.50871361812], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.75, 0.0, 0.0, 0.6666666666666666, 0.4430459775072449, 0.476848683260503, 0.0, 1.0, 0.2147452795886577], 
reward next is 0.7853, 
noisyNet noise sample is [array([0.46749878], dtype=float32), -1.0677638]. 
=============================================
[2019-04-04 06:38:18,817] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[78.82914 ]
 [79.21943 ]
 [79.616234]
 [80.01067 ]
 [80.38148 ]], R is [[78.47703552]
 [78.47852325]
 [78.48092651]
 [78.48410797]
 [78.48801422]].
[2019-04-04 06:38:23,187] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6364440e-11 1.4420418e-08 6.4414919e-27 1.3864449e-25 3.2272681e-20
 1.0000000e+00 2.0993185e-20], sum to 1.0000
[2019-04-04 06:38:23,187] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4426
[2019-04-04 06:38:23,238] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.3, 65.16666666666667, 166.0, 209.6666666666666, 26.0, 25.48149516055089, 0.3686320781922086, 1.0, 1.0, 47384.68276971707], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 129000.0000, 
sim time next is 129600.0000, 
raw observation next is [-8.4, 61.0, 157.0, 308.0, 26.0, 25.57279313453146, 0.3915015088896288, 1.0, 1.0, 47602.27133373833], 
processed observation next is [1.0, 0.5217391304347826, 0.2299168975069252, 0.61, 0.5233333333333333, 0.34033149171270716, 0.6666666666666666, 0.6310660945442882, 0.6305005029632096, 1.0, 1.0, 0.2266774825416111], 
reward next is 0.7733, 
noisyNet noise sample is [array([-0.22747901], dtype=float32), 1.0570846]. 
=============================================
[2019-04-04 06:38:26,722] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1063975e-09 8.3368960e-08 6.2248864e-24 2.7639447e-22 6.6505358e-18
 9.9999988e-01 6.5946040e-18], sum to 1.0000
[2019-04-04 06:38:26,722] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5545
[2019-04-04 06:38:26,742] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 26.0, 24.14689698070215, 0.07457351043798917, 0.0, 1.0, 44846.55249728422], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 262800.0000, 
sim time next is 263400.0000, 
raw observation next is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 26.0, 24.12644312870159, 0.06553351205258591, 0.0, 1.0, 45023.73185709313], 
processed observation next is [1.0, 0.043478260869565216, 0.2742382271468144, 0.6766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5105369273917993, 0.5218445040175287, 0.0, 1.0, 0.21439872312901492], 
reward next is 0.7856, 
noisyNet noise sample is [array([-1.6619879], dtype=float32), -1.2953649]. 
=============================================
[2019-04-04 06:38:32,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9102572e-11 3.7546442e-09 7.2013121e-28 4.5256617e-26 1.6219181e-20
 1.0000000e+00 7.8494294e-21], sum to 1.0000
[2019-04-04 06:38:32,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8266
[2019-04-04 06:38:32,628] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.3, 61.0, 148.0, 406.3333333333334, 26.0, 25.70620261554609, 0.4025123837667401, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 130200.0000, 
sim time next is 130800.0000, 
raw observation next is [-8.2, 61.0, 139.0, 504.6666666666667, 26.0, 25.75329930744277, 0.3910490543323922, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.23545706371191139, 0.61, 0.4633333333333333, 0.5576427255985267, 0.6666666666666666, 0.6461082756202309, 0.630349684777464, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0964881], dtype=float32), -0.05650879]. 
=============================================
[2019-04-04 06:38:34,143] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.5770690e-09 1.3377188e-08 1.2523485e-24 5.9037555e-23 5.7875470e-19
 1.0000000e+00 1.4103422e-18], sum to 1.0000
[2019-04-04 06:38:34,143] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7239
[2019-04-04 06:38:34,194] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.1558622852685, 0.2234844781423096, 1.0, 1.0, 26494.27004339333], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 238800.0000, 
sim time next is 239400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.9860038516081, 0.212970498584752, 1.0, 1.0, 122702.7608786116], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5821669876340083, 0.5709901661949174, 1.0, 1.0, 0.5842988613267219], 
reward next is 0.4157, 
noisyNet noise sample is [array([0.33290625], dtype=float32), -1.5281044]. 
=============================================
[2019-04-04 06:38:52,320] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8042902e-09 3.7506233e-08 3.0310377e-24 1.8552347e-23 2.7805856e-18
 1.0000000e+00 2.9838954e-18], sum to 1.0000
[2019-04-04 06:38:52,320] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2215
[2019-04-04 06:38:52,387] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.93505525359205, 0.2622141931310366, 0.0, 1.0, 56469.16948570064], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 246000.0000, 
sim time next is 246600.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.9346255148106, 0.2545006369571178, 0.0, 1.0, 50356.39026899467], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5778854595675501, 0.5848335456523727, 0.0, 1.0, 0.23979233461426033], 
reward next is 0.7602, 
noisyNet noise sample is [array([1.0999316], dtype=float32), -2.022602]. 
=============================================
[2019-04-04 06:38:55,465] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.0952290e-11 3.5837724e-09 1.0597233e-27 6.3361881e-26 2.6542124e-20
 1.0000000e+00 2.0204974e-20], sum to 1.0000
[2019-04-04 06:38:55,465] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2836
[2019-04-04 06:38:55,516] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 44.0, 88.5, 627.0, 26.0, 26.43153438461409, 0.5367303142707391, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 309600.0000, 
sim time next is 310200.0000, 
raw observation next is [-9.5, 43.66666666666667, 86.33333333333333, 625.6666666666666, 26.0, 26.44130937963547, 0.5277890561496279, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4366666666666667, 0.28777777777777774, 0.6913443830570902, 0.6666666666666666, 0.7034424483029559, 0.6759296853832093, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43971133], dtype=float32), -0.49457192]. 
=============================================
[2019-04-04 06:39:10,924] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 06:39:10,925] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:39:10,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:39:10,927] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run12
[2019-04-04 06:39:10,947] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:39:10,948] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:39:10,950] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run12
[2019-04-04 06:39:10,989] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:39:10,990] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:39:10,994] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run12
[2019-04-04 06:40:13,099] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.23876569], dtype=float32), 0.22547978]
[2019-04-04 06:40:13,099] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.6, 76.5, 0.0, 0.0, 26.0, 25.29966398928072, 0.3585144545923444, 0.0, 1.0, 39215.82751132083]
[2019-04-04 06:40:13,100] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:40:13,101] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.8469438e-09 2.0338570e-08 6.7658652e-26 5.5360223e-24 2.0744106e-19
 1.0000000e+00 1.2663174e-18], sampled 0.519826136252802
[2019-04-04 06:42:06,888] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.23876569], dtype=float32), 0.22547978]
[2019-04-04 06:42:06,888] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.5, 70.83333333333333, 0.0, 0.0, 26.0, 25.4244486039148, 0.413240672948575, 0.0, 1.0, 39837.13643324989]
[2019-04-04 06:42:06,888] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 06:42:06,891] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.3120842e-09 2.7159283e-08 2.4723973e-25 1.1767784e-23 5.8407771e-19
 1.0000000e+00 2.7899888e-18], sampled 0.9969069210941619
[2019-04-04 06:42:22,831] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 06:42:52,016] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 06:42:57,135] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 06:42:58,169] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1100000, evaluation results [1100000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 06:43:10,193] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.8355809e-10 3.3281900e-08 8.5715662e-27 3.7630733e-25 8.9173353e-20
 1.0000000e+00 5.1195671e-19], sum to 1.0000
[2019-04-04 06:43:10,193] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4543
[2019-04-04 06:43:10,255] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 88.66666666666667, 0.0, 0.0, 26.0, 24.46133414601546, 0.1608401353832744, 0.0, 1.0, 40802.93854657298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 540600.0000, 
sim time next is 541200.0000, 
raw observation next is [0.9000000000000001, 89.33333333333334, 0.0, 0.0, 26.0, 24.42992724904748, 0.1843009722675572, 0.0, 1.0, 41107.49170045703], 
processed observation next is [0.0, 0.2608695652173913, 0.48753462603878117, 0.8933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5358272707539568, 0.561433657422519, 0.0, 1.0, 0.19574996047836682], 
reward next is 0.8043, 
noisyNet noise sample is [array([1.8429215], dtype=float32), -1.1217893]. 
=============================================
[2019-04-04 06:43:32,613] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.5318229e-10 5.3753695e-08 3.2202623e-25 1.7331246e-24 2.8965828e-19
 1.0000000e+00 1.5980052e-18], sum to 1.0000
[2019-04-04 06:43:32,614] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4098
[2019-04-04 06:43:32,631] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.43789615405709, 0.1560711570502064, 0.0, 1.0, 38573.80798759028], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 888000.0000, 
sim time next is 888600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.52043131390391, 0.1619086637764659, 0.0, 1.0, 38492.44582292824], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5433692761586592, 0.5539695545921554, 0.0, 1.0, 0.18329736106156305], 
reward next is 0.8167, 
noisyNet noise sample is [array([1.792049], dtype=float32), 1.4278524]. 
=============================================
[2019-04-04 06:43:42,798] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6060611e-09 2.0119570e-08 2.1922617e-25 6.1914951e-24 3.0591796e-19
 1.0000000e+00 1.1397362e-18], sum to 1.0000
[2019-04-04 06:43:42,799] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8746
[2019-04-04 06:43:42,811] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [19.3, 49.83333333333334, 124.0, 0.0, 26.0, 27.74944082325763, 1.003208077768333, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1090200.0000, 
sim time next is 1090800.0000, 
raw observation next is [19.4, 49.0, 116.0, 0.0, 26.0, 27.80683554454593, 1.002018541278363, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.38666666666666666, 0.0, 0.6666666666666666, 0.8172362953788275, 0.8340061804261211, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17598301], dtype=float32), -0.40349546]. 
=============================================
[2019-04-04 06:43:46,010] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.5477077e-09 2.6082790e-07 3.1223890e-24 5.4887972e-22 6.2702817e-18
 9.9999976e-01 2.6674517e-17], sum to 1.0000
[2019-04-04 06:43:46,011] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2357
[2019-04-04 06:43:46,016] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.38333333333333, 64.66666666666667, 167.0, 0.0, 26.0, 25.09039753101534, 0.5025375464824109, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1169400.0000, 
sim time next is 1170000.0000, 
raw observation next is [18.3, 65.0, 165.0, 0.0, 26.0, 25.07924716205681, 0.5006510767277981, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.55, 0.0, 0.6666666666666666, 0.5899372635047341, 0.6668836922425992, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4642965], dtype=float32), -0.3461371]. 
=============================================
[2019-04-04 06:43:46,026] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[75.52684 ]
 [75.616264]
 [75.69196 ]
 [76.46292 ]
 [77.52629 ]], R is [[75.69683838]
 [75.93987274]
 [76.18047333]
 [76.41867065]
 [76.65448761]].
[2019-04-04 06:43:53,616] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.4713556e-11 2.7580139e-08 3.0768714e-29 3.2410123e-27 4.3317072e-22
 1.0000000e+00 6.5380602e-21], sum to 1.0000
[2019-04-04 06:43:53,616] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4104
[2019-04-04 06:43:53,626] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.3, 77.16666666666667, 0.0, 0.0, 26.0, 25.95898682435795, 0.615944329529784, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1051800.0000, 
sim time next is 1052400.0000, 
raw observation next is [14.2, 77.33333333333334, 0.0, 0.0, 26.0, 25.90982350274386, 0.6021877691111638, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.8559556786703602, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.6591519585619882, 0.700729256370388, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9602785], dtype=float32), -0.8383508]. 
=============================================
[2019-04-04 06:43:56,144] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3234651e-11 1.0823428e-08 1.0189932e-28 1.4607001e-26 2.0698238e-21
 1.0000000e+00 1.1931609e-21], sum to 1.0000
[2019-04-04 06:43:56,145] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4256
[2019-04-04 06:43:56,155] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 26.0, 25.62678640425813, 0.5804110847965385, 0.0, 1.0, 48121.06188623726], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1061400.0000, 
sim time next is 1062000.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 26.0, 25.60644914512243, 0.5862622304403459, 0.0, 1.0, 44611.7739729213], 
processed observation next is [1.0, 0.30434782608695654, 0.8310249307479226, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6338707620935358, 0.6954207434801153, 0.0, 1.0, 0.21243701891867287], 
reward next is 0.7876, 
noisyNet noise sample is [array([0.17864035], dtype=float32), 0.7063232]. 
=============================================
[2019-04-04 06:43:56,162] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[92.72548 ]
 [92.62967 ]
 [92.548584]
 [92.45207 ]
 [92.53189 ]], R is [[92.67494202]
 [92.51904297]
 [92.36427307]
 [92.14556885]
 [92.22411346]].
[2019-04-04 06:44:02,891] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8109364e-10 4.3057882e-09 3.0814186e-28 1.1892908e-26 9.1570105e-22
 1.0000000e+00 3.4579296e-21], sum to 1.0000
[2019-04-04 06:44:02,892] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8902
[2019-04-04 06:44:02,917] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.3, 96.66666666666666, 99.0, 0.0, 26.0, 25.06009724144217, 0.4835450553174535, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1253400.0000, 
sim time next is 1254000.0000, 
raw observation next is [14.2, 97.33333333333333, 100.0, 0.0, 26.0, 25.04255237635704, 0.4811889263330959, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8559556786703602, 0.9733333333333333, 0.3333333333333333, 0.0, 0.6666666666666666, 0.58687936469642, 0.6603963087776986, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0148239], dtype=float32), -0.4609004]. 
=============================================
[2019-04-04 06:44:02,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[90.07126 ]
 [89.143295]
 [88.1317  ]
 [86.97142 ]
 [85.86193 ]], R is [[91.33745575]
 [91.4240799 ]
 [91.50984192]
 [91.59474182]
 [91.67879486]].
[2019-04-04 06:44:06,998] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2493649e-11 4.3490216e-09 4.7475545e-28 1.8465755e-26 3.4150111e-21
 1.0000000e+00 2.1313603e-21], sum to 1.0000
[2019-04-04 06:44:06,998] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0271
[2019-04-04 06:44:07,010] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.466666666666667, 85.66666666666667, 0.0, 0.0, 26.0, 25.59291219624846, 0.5175064495115711, 0.0, 1.0, 22442.60896930363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1567200.0000, 
sim time next is 1567800.0000, 
raw observation next is [4.5, 85.5, 0.0, 0.0, 26.0, 25.54548817401554, 0.507749648364297, 0.0, 1.0, 49668.69450602264], 
processed observation next is [1.0, 0.13043478260869565, 0.5872576177285319, 0.855, 0.0, 0.0, 0.6666666666666666, 0.6287906811679616, 0.6692498827880989, 0.0, 1.0, 0.2365175928858221], 
reward next is 0.7635, 
noisyNet noise sample is [array([-0.2886303], dtype=float32), -1.2304282]. 
=============================================
[2019-04-04 06:44:10,762] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2572400e-10 1.5805169e-08 8.3886074e-27 4.2051601e-25 9.2078456e-20
 1.0000000e+00 9.4756673e-20], sum to 1.0000
[2019-04-04 06:44:10,762] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4118
[2019-04-04 06:44:10,780] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.2, 89.66666666666667, 0.0, 0.0, 26.0, 25.19614758077373, 0.4151683824499144, 0.0, 1.0, 43074.70186752963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1740000.0000, 
sim time next is 1740600.0000, 
raw observation next is [-0.3, 89.0, 0.0, 0.0, 26.0, 25.17291965763053, 0.4097999231228776, 0.0, 1.0, 43110.46512117005], 
processed observation next is [0.0, 0.13043478260869565, 0.4542936288088643, 0.89, 0.0, 0.0, 0.6666666666666666, 0.5977433048025441, 0.6365999743742926, 0.0, 1.0, 0.2052879291484288], 
reward next is 0.7947, 
noisyNet noise sample is [array([0.39836845], dtype=float32), -1.1267635]. 
=============================================
[2019-04-04 06:44:13,006] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.03887698e-10 1.68854919e-09 1.72623354e-27 3.33961355e-25
 7.28530762e-21 1.00000000e+00 1.23113515e-20], sum to 1.0000
[2019-04-04 06:44:13,006] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6208
[2019-04-04 06:44:13,036] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.13333333333333, 62.66666666666667, 0.0, 0.0, 26.0, 26.51201382153531, 0.7318230662556747, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1621200.0000, 
sim time next is 1621800.0000, 
raw observation next is [9.95, 63.5, 0.0, 0.0, 26.0, 26.46368010883117, 0.7183042987649603, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7382271468144045, 0.635, 0.0, 0.0, 0.6666666666666666, 0.7053066757359309, 0.7394347662549867, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1473314], dtype=float32), -0.19823939]. 
=============================================
[2019-04-04 06:44:14,436] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2443506e-09 3.4241779e-08 2.0747593e-26 8.1940752e-25 4.3118127e-20
 1.0000000e+00 4.8088559e-19], sum to 1.0000
[2019-04-04 06:44:14,438] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4751
[2019-04-04 06:44:14,514] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.20181319108664, 0.3301094876567798, 1.0, 1.0, 22332.01250905557], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1708200.0000, 
sim time next is 1708800.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 24.38057802908627, 0.3326488490059551, 1.0, 1.0, 196524.289557986], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5317148357571891, 0.6108829496686518, 1.0, 1.0, 0.9358299502761238], 
reward next is 0.0642, 
noisyNet noise sample is [array([-0.04800258], dtype=float32), 0.40847814]. 
=============================================
[2019-04-04 06:44:15,990] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2303907e-10 6.3258581e-09 1.2381111e-26 1.2383769e-24 3.0928545e-20
 1.0000000e+00 3.9117503e-19], sum to 1.0000
[2019-04-04 06:44:15,996] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4389
[2019-04-04 06:44:16,012] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.48564855080937, 0.5398911094557138, 0.0, 1.0, 53866.41614994144], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1720800.0000, 
sim time next is 1721400.0000, 
raw observation next is [0.4166666666666667, 92.5, 0.0, 0.0, 26.0, 25.46671527503711, 0.4941724952729067, 0.0, 1.0, 55999.19084490597], 
processed observation next is [1.0, 0.9565217391304348, 0.47414589104339805, 0.925, 0.0, 0.0, 0.6666666666666666, 0.6222262729197592, 0.6647241650909689, 0.0, 1.0, 0.2666628135471713], 
reward next is 0.7333, 
noisyNet noise sample is [array([-0.05259746], dtype=float32), 0.24771523]. 
=============================================
[2019-04-04 06:44:21,487] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3192376e-11 2.2108901e-09 3.0836865e-29 2.4683802e-27 3.4485881e-22
 1.0000000e+00 7.4551117e-22], sum to 1.0000
[2019-04-04 06:44:21,490] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1197
[2019-04-04 06:44:21,517] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 49.0, 145.0, 0.0, 26.0, 27.41829751493451, 0.8731876646901257, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1607400.0000, 
sim time next is 1608000.0000, 
raw observation next is [13.8, 49.0, 133.8333333333333, 0.0, 26.0, 27.43684764887038, 0.7538987936961151, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.844875346260388, 0.49, 0.44611111111111096, 0.0, 0.6666666666666666, 0.7864039707391983, 0.751299597898705, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1398314], dtype=float32), -0.03340447]. 
=============================================
[2019-04-04 06:44:21,537] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[89.6304 ]
 [89.78904]
 [89.94444]
 [90.12577]
 [90.34001]], R is [[89.50198364]
 [89.60696411]
 [89.71089172]
 [89.81378174]
 [89.91564178]].
[2019-04-04 06:44:31,554] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5773412e-10 4.0772523e-09 1.9781704e-27 2.6758088e-25 6.8910982e-20
 1.0000000e+00 1.4851408e-19], sum to 1.0000
[2019-04-04 06:44:31,554] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2065
[2019-04-04 06:44:31,616] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 83.0, 122.5, 0.0, 26.0, 24.94534202345726, 0.3452277353457574, 0.0, 1.0, 39828.21078772815], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1771200.0000, 
sim time next is 1771800.0000, 
raw observation next is [-2.383333333333333, 83.0, 123.6666666666667, 0.0, 26.0, 24.94195322675493, 0.3456345889404283, 0.0, 1.0, 45480.9915011355], 
processed observation next is [0.0, 0.5217391304347826, 0.3965835641735919, 0.83, 0.4122222222222223, 0.0, 0.6666666666666666, 0.5784961022295775, 0.6152115296468095, 0.0, 1.0, 0.21657615000540714], 
reward next is 0.7834, 
noisyNet noise sample is [array([0.3785192], dtype=float32), 0.3661775]. 
=============================================
[2019-04-04 06:44:39,729] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4310897e-08 9.5012055e-08 1.9921079e-23 7.2720864e-22 2.1380581e-17
 9.9999988e-01 2.4277238e-17], sum to 1.0000
[2019-04-04 06:44:39,750] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5511
[2019-04-04 06:44:39,775] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.4867065371, 0.1906146497998635, 0.0, 1.0, 42754.11672610916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1986600.0000, 
sim time next is 1987200.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.45577426068806, 0.1849205436057038, 0.0, 1.0, 42651.76270200649], 
processed observation next is [1.0, 0.0, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5379811883906717, 0.5616401812019013, 0.0, 1.0, 0.20310363191431663], 
reward next is 0.7969, 
noisyNet noise sample is [array([0.3243339], dtype=float32), 1.7307287]. 
=============================================
[2019-04-04 06:44:45,364] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0699815e-09 1.0796872e-08 2.3631411e-25 1.8158706e-23 3.0689393e-19
 1.0000000e+00 1.1197315e-18], sum to 1.0000
[2019-04-04 06:44:45,364] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3159
[2019-04-04 06:44:45,407] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.13292700809092, 0.4035288624638246, 0.0, 1.0, 114464.4477491661], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2060400.0000, 
sim time next is 2061000.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.21336505025752, 0.4197677386864398, 0.0, 1.0, 77956.6933512163], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6011137541881265, 0.6399225795621466, 0.0, 1.0, 0.37122234929150616], 
reward next is 0.6288, 
noisyNet noise sample is [array([-0.06141569], dtype=float32), -0.5356442]. 
=============================================
[2019-04-04 06:44:45,417] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[78.85462 ]
 [78.864296]
 [78.73972 ]
 [77.97504 ]
 [78.04662 ]], R is [[78.83087158]
 [78.49749756]
 [78.48678589]
 [77.97767639]
 [77.52186584]].
[2019-04-04 06:44:49,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7415096e-10 3.9942240e-08 4.8560487e-26 1.1416784e-23 4.4730864e-20
 1.0000000e+00 1.0093998e-18], sum to 1.0000
[2019-04-04 06:44:49,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6880
[2019-04-04 06:44:49,412] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.05, 79.0, 147.0, 0.0, 26.0, 25.61352566353813, 0.3350822868157879, 1.0, 1.0, 24356.03195404432], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2028600.0000, 
sim time next is 2029200.0000, 
raw observation next is [-4.866666666666667, 77.66666666666667, 148.5, 0.0, 26.0, 25.64371393884452, 0.3401586759093906, 1.0, 1.0, 23531.2270374928], 
processed observation next is [1.0, 0.4782608695652174, 0.3277931671283472, 0.7766666666666667, 0.495, 0.0, 0.6666666666666666, 0.6369761615703767, 0.6133862253031303, 1.0, 1.0, 0.11205346208329905], 
reward next is 0.8879, 
noisyNet noise sample is [array([0.9367773], dtype=float32), -0.908365]. 
=============================================
[2019-04-04 06:44:54,989] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8096873e-09 1.1130282e-07 1.1645951e-24 1.7625819e-23 1.3340935e-18
 9.9999988e-01 7.0631619e-19], sum to 1.0000
[2019-04-04 06:44:54,989] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4457
[2019-04-04 06:44:55,015] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 91.0, 0.0, 0.0, 26.0, 24.41168842643322, 0.1649025212811111, 0.0, 1.0, 43575.61465411318], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2088000.0000, 
sim time next is 2088600.0000, 
raw observation next is [-5.7, 90.33333333333334, 0.0, 0.0, 26.0, 24.49503058595728, 0.1457477561932128, 0.0, 1.0, 43690.62747850252], 
processed observation next is [1.0, 0.17391304347826086, 0.30470914127423826, 0.9033333333333334, 0.0, 0.0, 0.6666666666666666, 0.5412525488297734, 0.5485825853977376, 0.0, 1.0, 0.2080506070404882], 
reward next is 0.7919, 
noisyNet noise sample is [array([-1.9657515], dtype=float32), 0.09269346]. 
=============================================
[2019-04-04 06:45:09,418] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9415959e-08 2.5290242e-07 5.7715721e-23 1.5612526e-21 1.7693236e-17
 9.9999976e-01 4.4749015e-17], sum to 1.0000
[2019-04-04 06:45:09,418] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4988
[2019-04-04 06:45:09,431] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 81.50000000000001, 0.0, 0.0, 26.0, 24.39418791201546, 0.1886713168485513, 0.0, 1.0, 42462.76034636118], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2160600.0000, 
sim time next is 2161200.0000, 
raw observation next is [-7.300000000000001, 81.0, 0.0, 0.0, 26.0, 24.34979197395594, 0.1924812192079093, 0.0, 1.0, 42484.3691143215], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.81, 0.0, 0.0, 0.6666666666666666, 0.5291493311629951, 0.5641604064026364, 0.0, 1.0, 0.20230651959200713], 
reward next is 0.7977, 
noisyNet noise sample is [array([-0.38560244], dtype=float32), 0.9613353]. 
=============================================
[2019-04-04 06:45:16,006] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3825989e-09 2.7031014e-08 4.1014866e-24 5.9535560e-23 4.4266314e-18
 1.0000000e+00 6.2134234e-18], sum to 1.0000
[2019-04-04 06:45:16,006] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2114
[2019-04-04 06:45:16,082] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.516666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 25.75740564117955, 0.4830596996029859, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2225400.0000, 
sim time next is 2226000.0000, 
raw observation next is [-4.533333333333333, 68.66666666666667, 0.0, 0.0, 26.0, 25.86854018149197, 0.4811061108108705, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3370267774699908, 0.6866666666666668, 0.0, 0.0, 0.6666666666666666, 0.6557116817909975, 0.6603687036036235, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.62383], dtype=float32), -0.26854536]. 
=============================================
[2019-04-04 06:45:16,123] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.888695]
 [75.35213 ]
 [75.8609  ]
 [76.40879 ]
 [76.31606 ]], R is [[74.87178802]
 [75.12306976]
 [75.37184143]
 [75.61812592]
 [74.95219421]].
[2019-04-04 06:45:17,591] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4056997e-09 6.7616433e-08 2.7847910e-24 7.1555446e-23 9.7528451e-19
 9.9999988e-01 1.1732945e-17], sum to 1.0000
[2019-04-04 06:45:17,591] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0959
[2019-04-04 06:45:17,647] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 69.0, 51.0, 59.99999999999999, 26.0, 24.87427359912463, 0.2868960890975108, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2364000.0000, 
sim time next is 2364600.0000, 
raw observation next is [-3.4, 69.0, 64.99999999999999, 120.0, 26.0, 25.20015590421547, 0.314459814883175, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.21666666666666662, 0.13259668508287292, 0.6666666666666666, 0.6000129920179559, 0.6048199382943916, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6881299], dtype=float32), -1.0440371]. 
=============================================
[2019-04-04 06:45:30,622] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3111200e-09 1.2960651e-07 2.0345783e-23 4.0179276e-22 5.6564360e-18
 9.9999988e-01 1.7700936e-17], sum to 1.0000
[2019-04-04 06:45:30,622] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1501
[2019-04-04 06:45:30,667] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 57.0, 0.0, 0.0, 26.0, 24.82041573340911, 0.1376097715205723, 0.0, 1.0, 38463.82929318355], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2526600.0000, 
sim time next is 2527200.0000, 
raw observation next is [-2.3, 57.0, 0.0, 0.0, 26.0, 24.78555306892745, 0.1295510213742476, 0.0, 1.0, 38543.3750348419], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.57, 0.0, 0.0, 0.6666666666666666, 0.5654627557439541, 0.5431836737914159, 0.0, 1.0, 0.18353988111829475], 
reward next is 0.8165, 
noisyNet noise sample is [array([0.44468167], dtype=float32), 0.6631887]. 
=============================================
[2019-04-04 06:45:31,757] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.2655003e-09 3.4050043e-08 9.3966057e-23 2.9984162e-21 5.8784245e-17
 1.0000000e+00 2.0853947e-16], sum to 1.0000
[2019-04-04 06:45:31,761] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2217
[2019-04-04 06:45:31,779] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.1356741353611, 0.2756762548485061, 0.0, 1.0, 43156.80929670222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2407200.0000, 
sim time next is 2407800.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.1096981319699, 0.2730478633123376, 0.0, 1.0, 43155.10214263996], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.592474844330825, 0.5910159544374459, 0.0, 1.0, 0.2055004863935236], 
reward next is 0.7945, 
noisyNet noise sample is [array([-1.6563935], dtype=float32), -0.3957826]. 
=============================================
[2019-04-04 06:45:40,495] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.04810695e-07 8.94232315e-08 8.08318936e-22 2.78212053e-20
 2.68100063e-16 9.99999762e-01 7.15813568e-16], sum to 1.0000
[2019-04-04 06:45:40,495] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5980
[2019-04-04 06:45:40,515] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 39.33333333333334, 0.0, 0.0, 26.0, 25.03307344637278, 0.1996521369650128, 0.0, 1.0, 39096.37161053139], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2510400.0000, 
sim time next is 2511000.0000, 
raw observation next is [-1.7, 39.0, 0.0, 0.0, 26.0, 24.98778296987645, 0.202517220981547, 0.0, 1.0, 39054.84183129635], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.39, 0.0, 0.0, 0.6666666666666666, 0.5823152474897041, 0.5675057403271824, 0.0, 1.0, 0.18597543729188737], 
reward next is 0.8140, 
noisyNet noise sample is [array([-0.0895939], dtype=float32), 1.0431651]. 
=============================================
[2019-04-04 06:45:40,550] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[68.59427 ]
 [68.59462 ]
 [68.203354]
 [68.026245]
 [68.103035]], R is [[69.1619873 ]
 [69.28419495]
 [69.40505219]
 [69.52462006]
 [69.64302826]].
[2019-04-04 06:45:50,133] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7498857e-09 2.0766073e-08 1.7723914e-24 2.2279765e-23 5.5055622e-19
 1.0000000e+00 4.1980587e-18], sum to 1.0000
[2019-04-04 06:45:50,134] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4849
[2019-04-04 06:45:50,176] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.1, 59.0, 0.0, 0.0, 26.0, 24.99423446366253, 0.3758765880198205, 1.0, 1.0, 92375.78786649849], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2659800.0000, 
sim time next is 2660400.0000, 
raw observation next is [-1.2, 60.0, 0.0, 0.0, 26.0, 25.06487058707528, 0.393459151136912, 1.0, 1.0, 20888.83668990738], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5887392155896066, 0.6311530503789706, 1.0, 1.0, 0.09947065090432086], 
reward next is 0.9005, 
noisyNet noise sample is [array([0.87934643], dtype=float32), 0.093758665]. 
=============================================
[2019-04-04 06:46:04,593] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.63361874e-09 2.81821038e-08 1.40299188e-24 8.42200272e-23
 1.40019447e-18 1.00000000e+00 1.26407855e-17], sum to 1.0000
[2019-04-04 06:46:04,593] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4762
[2019-04-04 06:46:04,615] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 26.0, 25.04974216687847, 0.2992491261823975, 0.0, 1.0, 54871.02458730967], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2851800.0000, 
sim time next is 2852400.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 26.0, 24.97827567412181, 0.3052005578794054, 0.0, 1.0, 55392.18963863256], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5815229728434842, 0.6017335192931351, 0.0, 1.0, 0.263772331612536], 
reward next is 0.7362, 
noisyNet noise sample is [array([-0.5028114], dtype=float32), 1.3702991]. 
=============================================
[2019-04-04 06:46:07,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4334741e-10 3.9287158e-08 1.7350621e-25 1.3018546e-23 1.1707987e-19
 1.0000000e+00 1.0659597e-18], sum to 1.0000
[2019-04-04 06:46:07,580] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4277
[2019-04-04 06:46:07,600] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1666666666666666, 66.66666666666667, 40.33333333333334, 353.3333333333334, 26.0, 25.05455729728862, 0.3281047937796117, 0.0, 1.0, 50390.1162321669], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3084600.0000, 
sim time next is 3085200.0000, 
raw observation next is [0.0, 72.0, 32.0, 287.0, 26.0, 25.07639644570135, 0.3250133602894094, 0.0, 1.0, 26652.12757225975], 
processed observation next is [0.0, 0.7391304347826086, 0.46260387811634357, 0.72, 0.10666666666666667, 0.31712707182320443, 0.6666666666666666, 0.5896997038084457, 0.6083377867631364, 0.0, 1.0, 0.1269148932012369], 
reward next is 0.8731, 
noisyNet noise sample is [array([0.7846129], dtype=float32), -0.2721366]. 
=============================================
[2019-04-04 06:46:22,895] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.6139607e-12 3.1698411e-10 4.3646453e-31 3.3235949e-28 2.2913461e-23
 1.0000000e+00 2.0259663e-23], sum to 1.0000
[2019-04-04 06:46:22,896] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0123
[2019-04-04 06:46:22,909] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.333333333333333, 75.83333333333334, 95.66666666666667, 741.3333333333334, 26.0, 26.93563551531232, 0.8402794966661351, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3250200.0000, 
sim time next is 3250800.0000, 
raw observation next is [-2.0, 71.0, 93.0, 727.5, 26.0, 26.9768530107386, 0.8500744717974863, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.40720221606648205, 0.71, 0.31, 0.8038674033149171, 0.6666666666666666, 0.7480710842282168, 0.7833581572658287, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.1764467], dtype=float32), 0.62178195]. 
=============================================
[2019-04-04 06:46:29,048] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0560837e-11 1.1135007e-09 7.5464498e-28 1.2532272e-25 9.1159237e-21
 1.0000000e+00 8.3957079e-21], sum to 1.0000
[2019-04-04 06:46:29,051] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0953
[2019-04-04 06:46:29,104] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.5, 73.5, 104.0, 615.0, 26.0, 26.20991110751052, 0.5344463694796584, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3317400.0000, 
sim time next is 3318000.0000, 
raw observation next is [-8.333333333333334, 72.33333333333333, 105.1666666666667, 635.8333333333333, 26.0, 26.21283003000963, 0.5469096885329494, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.23176361957525393, 0.7233333333333333, 0.3505555555555557, 0.7025782688766113, 0.6666666666666666, 0.6844025025008026, 0.6823032295109831, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40713626], dtype=float32), -0.54552776]. 
=============================================
[2019-04-04 06:46:29,109] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.41825 ]
 [82.52771 ]
 [81.59311 ]
 [80.707985]
 [79.65802 ]], R is [[84.25547028]
 [84.41291809]
 [84.56878662]
 [84.72309875]
 [84.87586975]].
[2019-04-04 06:46:31,889] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1474322e-11 1.8132315e-10 1.5291101e-31 9.5832240e-29 4.1118592e-23
 1.0000000e+00 1.0884671e-22], sum to 1.0000
[2019-04-04 06:46:31,892] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4900
[2019-04-04 06:46:31,910] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 90.33333333333334, 102.6666666666667, 776.1666666666667, 26.0, 26.82653675238075, 0.8238859196633371, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3248400.0000, 
sim time next is 3249000.0000, 
raw observation next is [-3.0, 85.5, 101.0, 769.0, 26.0, 26.89292948876345, 0.8343330492233875, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.855, 0.33666666666666667, 0.8497237569060774, 0.6666666666666666, 0.7410774573969542, 0.7781110164077959, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.306648], dtype=float32), -0.9714475]. 
=============================================
[2019-04-04 06:46:31,923] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[94.536026]
 [94.57921 ]
 [94.542725]
 [94.48351 ]
 [94.49132 ]], R is [[94.50669098]
 [94.56162262]
 [94.61600494]
 [94.66984558]
 [94.72314453]].
[2019-04-04 06:46:35,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8583430e-10 1.1789351e-08 5.7973591e-26 9.8663494e-25 1.1678056e-19
 1.0000000e+00 1.2847773e-19], sum to 1.0000
[2019-04-04 06:46:35,219] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7192
[2019-04-04 06:46:35,283] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.0, 77.0, 95.0, 505.5, 26.0, 26.02510287490513, 0.5086039640174768, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3315600.0000, 
sim time next is 3316200.0000, 
raw observation next is [-8.833333333333334, 75.83333333333334, 98.0, 542.0, 26.0, 26.12401958602967, 0.5232572575608084, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.21791320406278855, 0.7583333333333334, 0.32666666666666666, 0.5988950276243094, 0.6666666666666666, 0.6770016321691393, 0.6744190858536028, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.78254735], dtype=float32), 0.7071949]. 
=============================================
[2019-04-04 06:46:36,937] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5010498e-10 1.9710251e-09 9.5596422e-28 3.7993914e-26 3.5000939e-20
 1.0000000e+00 1.6186979e-20], sum to 1.0000
[2019-04-04 06:46:36,947] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1303
[2019-04-04 06:46:36,970] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 46.0, 73.5, 587.5, 26.0, 26.04204494118304, 0.6483160012345127, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3340800.0000, 
sim time next is 3341400.0000, 
raw observation next is [-2.0, 46.66666666666667, 69.0, 558.6666666666667, 26.0, 26.39040021824707, 0.6772872881692384, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.46666666666666673, 0.23, 0.6173112338858197, 0.6666666666666666, 0.6992000181872559, 0.7257624293897461, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.72501236], dtype=float32), -0.020522121]. 
=============================================
[2019-04-04 06:46:37,137] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9452763e-10 1.0409583e-08 4.6036693e-26 2.0304219e-24 2.7567257e-19
 1.0000000e+00 2.9376329e-19], sum to 1.0000
[2019-04-04 06:46:37,138] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7750
[2019-04-04 06:46:37,152] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 50.0, 35.5, 317.0, 26.0, 26.27075421792958, 0.6276345969310425, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3344400.0000, 
sim time next is 3345000.0000, 
raw observation next is [-2.166666666666667, 50.83333333333334, 27.33333333333333, 255.6666666666666, 26.0, 26.4302072879292, 0.4787709789219664, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4025854108956602, 0.5083333333333334, 0.0911111111111111, 0.2825046040515653, 0.6666666666666666, 0.7025172739941, 0.6595903263073222, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4180238], dtype=float32), 0.49996713]. 
=============================================
[2019-04-04 06:46:37,162] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[78.884636]
 [79.634766]
 [80.23413 ]
 [80.77068 ]
 [81.2589  ]], R is [[78.32532501]
 [78.54207611]
 [78.75665283]
 [78.96908569]
 [79.17939758]].
[2019-04-04 06:46:37,663] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.3504397e-12 3.8823504e-09 1.5156336e-28 2.9265592e-27 9.9782872e-22
 1.0000000e+00 3.4060341e-21], sum to 1.0000
[2019-04-04 06:46:37,663] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8098
[2019-04-04 06:46:37,675] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8333333333333334, 60.83333333333334, 111.0, 783.3333333333333, 26.0, 26.27458422646531, 0.5802662089988812, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3495000.0000, 
sim time next is 3495600.0000, 
raw observation next is [1.0, 61.0, 112.0, 790.0, 26.0, 26.17833164058857, 0.5841821870726277, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.61, 0.37333333333333335, 0.8729281767955801, 0.6666666666666666, 0.6815276367157143, 0.6947273956908759, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41380903], dtype=float32), -0.3855753]. 
=============================================
[2019-04-04 06:46:37,896] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7961771e-12 1.9863098e-09 4.5694768e-28 6.7606538e-26 2.3504335e-21
 1.0000000e+00 1.0419009e-21], sum to 1.0000
[2019-04-04 06:46:37,904] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2185
[2019-04-04 06:46:37,912] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.333333333333333, 54.0, 117.3333333333333, 809.1666666666667, 26.0, 26.1129545894693, 0.5635310563184135, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3328800.0000, 
sim time next is 3329400.0000, 
raw observation next is [-5.166666666666667, 54.0, 116.6666666666667, 807.3333333333334, 26.0, 26.01434219602002, 0.5536289920565809, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.31948291782086796, 0.54, 0.388888888888889, 0.8920810313075507, 0.6666666666666666, 0.6678618496683351, 0.6845429973521936, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16875105], dtype=float32), -0.20623688]. 
=============================================
[2019-04-04 06:46:38,460] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.49097043e-09 5.10411269e-08 3.03694034e-24 1.02029547e-22
 2.53506683e-18 1.00000000e+00 4.26301384e-18], sum to 1.0000
[2019-04-04 06:46:38,461] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5116
[2019-04-04 06:46:38,489] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.833333333333334, 70.0, 0.0, 0.0, 26.0, 25.24984955573755, 0.4146380922671717, 0.0, 1.0, 53192.88335064634], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3365400.0000, 
sim time next is 3366000.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 25.21184533863488, 0.407614938931439, 0.0, 1.0, 45726.63316379251], 
processed observation next is [1.0, 1.0, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6009871115529067, 0.6358716463104797, 0.0, 1.0, 0.21774587220853575], 
reward next is 0.7823, 
noisyNet noise sample is [array([0.3978941], dtype=float32), -0.8786251]. 
=============================================
[2019-04-04 06:46:38,506] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[74.45017]
 [74.37407]
 [74.32186]
 [74.26356]
 [74.38221]], R is [[74.707901  ]
 [74.70752716]
 [74.64253998]
 [74.64083862]
 [74.73059845]].
[2019-04-04 06:46:40,619] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1556075e-08 5.8887977e-08 5.1857334e-24 2.0805491e-22 5.8436542e-18
 9.9999988e-01 3.5861234e-17], sum to 1.0000
[2019-04-04 06:46:40,622] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9015
[2019-04-04 06:46:40,639] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.833333333333333, 76.0, 0.0, 0.0, 26.0, 25.05979014831362, 0.3686607670550245, 0.0, 1.0, 41564.77458977879], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3369000.0000, 
sim time next is 3369600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.02200919656767, 0.3635286962800535, 0.0, 1.0, 41524.74766254247], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5851674330473058, 0.6211762320933512, 0.0, 1.0, 0.1977368936311546], 
reward next is 0.8023, 
noisyNet noise sample is [array([-0.5618549], dtype=float32), -0.67813814]. 
=============================================
[2019-04-04 06:46:42,050] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8169297e-09 3.0520049e-08 2.2550454e-25 5.5534510e-24 5.2369172e-19
 1.0000000e+00 4.5803529e-19], sum to 1.0000
[2019-04-04 06:46:42,055] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4269
[2019-04-04 06:46:42,073] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.52053002137976, 0.462679162329499, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3461400.0000, 
sim time next is 3462000.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.55508371392682, 0.4575419343155134, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6295903094939016, 0.6525139781051711, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10803982], dtype=float32), 1.266901]. 
=============================================
[2019-04-04 06:46:42,101] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.49072]
 [81.35489]
 [81.30063]
 [81.19183]
 [81.25877]], R is [[81.73214722]
 [81.91482544]
 [81.79268646]
 [81.73796082]
 [81.68211365]].
[2019-04-04 06:46:46,538] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0606816e-09 9.1801930e-08 5.5887227e-26 1.7950425e-24 8.8265322e-20
 9.9999988e-01 2.0644557e-19], sum to 1.0000
[2019-04-04 06:46:46,540] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3439
[2019-04-04 06:46:46,563] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23650464590946, 0.3774352296668184, 0.0, 1.0, 41755.95581227756], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3478800.0000, 
sim time next is 3479400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23059252245127, 0.3778970001903093, 0.0, 1.0, 41684.80533278024], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6025493768709392, 0.6259656667301031, 0.0, 1.0, 0.19849907301323924], 
reward next is 0.8015, 
noisyNet noise sample is [array([-2.0141356], dtype=float32), 0.683122]. 
=============================================
[2019-04-04 06:46:52,098] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2364908e-09 1.6554011e-08 5.2723805e-25 9.7397714e-24 9.7593219e-19
 1.0000000e+00 7.6352943e-19], sum to 1.0000
[2019-04-04 06:46:52,102] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0148
[2019-04-04 06:46:52,121] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.85556696028615, 0.573776826355825, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3781200.0000, 
sim time next is 3781800.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.99293122730335, 0.5694563202751146, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6660776022752793, 0.6898187734250382, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.627769], dtype=float32), 0.778144]. 
=============================================
[2019-04-04 06:47:04,233] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6620511e-08 6.1453420e-08 1.3191161e-23 5.0580226e-22 3.2442943e-17
 9.9999988e-01 5.3375806e-18], sum to 1.0000
[2019-04-04 06:47:04,234] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4435
[2019-04-04 06:47:04,249] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 25.66666666666667, 13.33333333333334, 128.6666666666667, 26.0, 25.78283262287742, 0.5570570235917098, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4038600.0000, 
sim time next is 4039200.0000, 
raw observation next is [-3.0, 26.0, 0.0, 0.0, 26.0, 25.83972887366738, 0.5357399260910971, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.26, 0.0, 0.0, 0.6666666666666666, 0.6533107394722816, 0.6785799753636991, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5379665], dtype=float32), 0.16999193]. 
=============================================
[2019-04-04 06:47:13,892] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1483606e-09 8.3528882e-09 6.3764662e-25 5.6809850e-24 9.8339839e-20
 1.0000000e+00 5.8974426e-19], sum to 1.0000
[2019-04-04 06:47:13,894] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8286
[2019-04-04 06:47:13,950] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.66666666666667, 51.50000000000001, 98.33333333333334, 613.3333333333334, 26.0, 26.36439213961834, 0.5175667633386015, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4007400.0000, 
sim time next is 4008000.0000, 
raw observation next is [-10.33333333333333, 50.0, 99.66666666666667, 655.6666666666667, 26.0, 26.41668597662942, 0.5310935290196894, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.17636195752539252, 0.5, 0.33222222222222225, 0.7244935543278086, 0.6666666666666666, 0.7013904980524517, 0.6770311763398965, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.60521257], dtype=float32), -0.9719645]. 
=============================================
[2019-04-04 06:47:13,955] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[78.80501 ]
 [77.88275 ]
 [76.78597 ]
 [75.695274]
 [74.577736]], R is [[80.00325775]
 [80.20322418]
 [80.40119171]
 [80.59718323]
 [80.79121399]].
[2019-04-04 06:47:29,389] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-04 06:47:29,404] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:47:29,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:47:29,406] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:47:29,407] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:47:29,407] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:47:29,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run13
[2019-04-04 06:47:29,435] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:47:29,439] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run13
[2019-04-04 06:47:29,469] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run13
[2019-04-04 06:48:06,625] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2446773], dtype=float32), 0.2326885]
[2019-04-04 06:48:06,625] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-11.05, 66.0, 114.0, 560.0, 26.0, 25.09366596955653, 0.2082541173302635, 0.0, 1.0, 0.0]
[2019-04-04 06:48:06,625] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:48:06,626] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.9295135e-10 4.2445450e-08 1.7543094e-25 1.3517846e-23 4.0862862e-19
 1.0000000e+00 1.4285137e-18], sampled 0.5784456232074867
[2019-04-04 06:49:15,509] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2446773], dtype=float32), 0.2326885]
[2019-04-04 06:49:15,510] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.0, 56.0, 180.0, 274.0, 26.0, 25.01439484513588, 0.3602431508492294, 0.0, 1.0, 25366.64709453466]
[2019-04-04 06:49:15,510] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:49:15,511] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.15908184e-11 7.16912085e-09 1.24979815e-27 1.17493806e-25
 1.22979737e-20 1.00000000e+00 2.75356083e-20], sampled 0.2754733791250885
[2019-04-04 06:50:35,159] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 06:51:05,440] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 06:51:10,632] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 06:51:11,670] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1200000, evaluation results [1200000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 06:51:15,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5013742e-09 1.5648409e-08 1.1951680e-24 1.4364718e-23 1.5996580e-18
 1.0000000e+00 6.3170514e-18], sum to 1.0000
[2019-04-04 06:51:15,397] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6499
[2019-04-04 06:51:15,442] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 40.0, 0.0, 0.0, 26.0, 25.58570386539171, 0.5155753530748729, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4131000.0000, 
sim time next is 4131600.0000, 
raw observation next is [1.666666666666667, 41.0, 0.0, 0.0, 26.0, 25.56208041483414, 0.5064024655116791, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5087719298245615, 0.41, 0.0, 0.0, 0.6666666666666666, 0.6301733679028448, 0.6688008218372263, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3378313], dtype=float32), -0.03353483]. 
=============================================
[2019-04-04 06:51:32,656] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.9808390e-11 9.8678594e-09 1.8634231e-27 9.9416837e-25 4.1320825e-22
 1.0000000e+00 2.2749149e-20], sum to 1.0000
[2019-04-04 06:51:32,656] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4232
[2019-04-04 06:51:32,717] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4, 72.5, 111.0, 66.0, 26.0, 25.64502441241278, 0.4763909522253742, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4523400.0000, 
sim time next is 4524000.0000, 
raw observation next is [-0.2666666666666667, 72.33333333333334, 113.0, 55.0, 26.0, 25.87219935962792, 0.4868745305781863, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4552169898430287, 0.7233333333333334, 0.37666666666666665, 0.06077348066298342, 0.6666666666666666, 0.6560166133023267, 0.6622915101927288, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.97478414], dtype=float32), 0.10856144]. 
=============================================
[2019-04-04 06:51:32,778] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.83731 ]
 [85.24871 ]
 [84.884636]
 [84.12543 ]
 [83.77442 ]], R is [[86.65185547]
 [86.78533936]
 [86.9174881 ]
 [87.04831696]
 [87.17783356]].
[2019-04-04 06:51:35,523] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7490915e-09 2.6250678e-08 8.3630465e-27 1.7737433e-25 6.2022366e-21
 1.0000000e+00 3.1239225e-19], sum to 1.0000
[2019-04-04 06:51:35,523] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3213
[2019-04-04 06:51:35,533] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.45, 75.5, 0.0, 0.0, 26.0, 25.56261208545037, 0.4022590905078445, 0.0, 1.0, 34439.57085027609], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4318200.0000, 
sim time next is 4318800.0000, 
raw observation next is [4.466666666666667, 75.66666666666667, 0.0, 0.0, 26.0, 25.51420964680865, 0.3988155343136519, 0.0, 1.0, 58488.04852007503], 
processed observation next is [0.0, 1.0, 0.5863342566943676, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6261841372340541, 0.6329385114378839, 0.0, 1.0, 0.2785145167622621], 
reward next is 0.7215, 
noisyNet noise sample is [array([-0.44782138], dtype=float32), 0.9806492]. 
=============================================
[2019-04-04 06:51:45,491] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.5475710e-11 1.3665091e-08 4.9553166e-28 8.9822094e-26 3.6619274e-21
 1.0000000e+00 9.8873933e-21], sum to 1.0000
[2019-04-04 06:51:45,492] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7660
[2019-04-04 06:51:45,531] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 67.0, 0.0, 0.0, 26.0, 25.79771769075345, 0.5578601896163496, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4420800.0000, 
sim time next is 4421400.0000, 
raw observation next is [4.383333333333333, 67.16666666666667, 0.0, 0.0, 26.0, 25.71200708213254, 0.5545382376036295, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5840258541089567, 0.6716666666666667, 0.0, 0.0, 0.6666666666666666, 0.6426672568443784, 0.6848460792012099, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.260176], dtype=float32), 1.3526149]. 
=============================================
[2019-04-04 06:51:53,251] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6347108e-11 1.4823304e-08 3.5264971e-27 2.0682175e-25 6.6113699e-21
 1.0000000e+00 8.0475717e-20], sum to 1.0000
[2019-04-04 06:51:53,251] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5446
[2019-04-04 06:51:53,279] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 48.0, 131.0, 40.0, 26.0, 26.47423399040414, 0.6046649151141085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4550400.0000, 
sim time next is 4551000.0000, 
raw observation next is [2.0, 48.66666666666666, 123.6666666666667, 49.33333333333334, 26.0, 26.48688364259032, 0.4974069914102581, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.4866666666666666, 0.4122222222222223, 0.05451197053406999, 0.6666666666666666, 0.7072403035491934, 0.6658023304700861, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3139576], dtype=float32), -1.2065659]. 
=============================================
[2019-04-04 06:51:53,292] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.20724 ]
 [83.7795  ]
 [84.666046]
 [85.41683 ]
 [86.19856 ]], R is [[82.83377838]
 [83.00543976]
 [83.17538452]
 [83.34362793]
 [83.51019287]].
[2019-04-04 06:51:57,160] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9117944e-11 7.6729991e-09 4.6852858e-28 5.4750275e-26 1.0234730e-21
 1.0000000e+00 2.9745053e-20], sum to 1.0000
[2019-04-04 06:51:57,160] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2579
[2019-04-04 06:51:57,203] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.166666666666667, 60.83333333333333, 238.0, 174.0, 26.0, 25.37948826533074, 0.3553462653618915, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4873800.0000, 
sim time next is 4874400.0000, 
raw observation next is [-2.0, 60.0, 253.5, 171.5, 26.0, 25.33150535323743, 0.3481624988207654, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.40720221606648205, 0.6, 0.845, 0.18950276243093922, 0.6666666666666666, 0.6109587794364524, 0.6160541662735884, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2581695], dtype=float32), -0.099193774]. 
=============================================
[2019-04-04 06:52:05,450] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2944608e-09 1.1956672e-07 3.5647654e-24 2.5858106e-22 1.0864822e-18
 9.9999988e-01 1.1178797e-17], sum to 1.0000
[2019-04-04 06:52:05,450] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8255
[2019-04-04 06:52:05,454] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.450000000000001, 19.83333333333334, 0.0, 0.0, 26.0, 26.26711825977541, 0.6279293547742854, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5093400.0000, 
sim time next is 5094000.0000, 
raw observation next is [8.4, 20.0, 0.0, 0.0, 26.0, 26.18448605790664, 0.6094310384770645, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6952908587257619, 0.2, 0.0, 0.0, 0.6666666666666666, 0.6820405048255532, 0.7031436794923548, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.862367], dtype=float32), 0.39519212]. 
=============================================
[2019-04-04 06:52:05,460] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.030396]
 [78.06555 ]
 [78.4205  ]
 [78.40072 ]
 [78.450775]], R is [[78.0176239 ]
 [78.23744965]
 [78.45507812]
 [78.6705246 ]
 [78.88381958]].
[2019-04-04 06:52:06,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:06,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:06,624] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run10
[2019-04-04 06:52:10,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:10,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:10,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run10
[2019-04-04 06:52:12,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:12,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:12,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run10
[2019-04-04 06:52:14,186] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:14,186] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:14,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run10
[2019-04-04 06:52:14,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:14,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:14,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run10
[2019-04-04 06:52:16,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:16,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:16,470] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run10
[2019-04-04 06:52:18,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:18,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:18,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run10
[2019-04-04 06:52:19,478] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6100974e-09 1.9757310e-08 3.1890787e-24 9.5317735e-23 4.4907330e-19
 1.0000000e+00 5.5800123e-18], sum to 1.0000
[2019-04-04 06:52:19,478] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8358
[2019-04-04 06:52:19,513] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 36.5, 0.0, 0.0, 26.0, 25.79380222322046, 0.5085262268064571, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5004600.0000, 
sim time next is 5005200.0000, 
raw observation next is [3.0, 36.0, 0.0, 0.0, 26.0, 25.75404304750592, 0.4816756286067547, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6461702539588267, 0.6605585428689182, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24233666], dtype=float32), 1.5205195]. 
=============================================
[2019-04-04 06:52:21,245] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5323112e-12 1.7367714e-09 1.3982865e-29 9.2136402e-28 1.7174300e-22
 1.0000000e+00 8.7512902e-22], sum to 1.0000
[2019-04-04 06:52:21,251] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6688
[2019-04-04 06:52:21,269] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.666666666666667, 39.33333333333334, 115.3333333333333, 790.5, 26.0, 27.1456688162968, 0.7652338435082434, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5048400.0000, 
sim time next is 5049000.0000, 
raw observation next is [4.0, 38.5, 116.0, 809.0, 26.0, 27.25285938581884, 0.7854167583311381, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5734072022160666, 0.385, 0.38666666666666666, 0.8939226519337017, 0.6666666666666666, 0.7710716154849034, 0.7618055861103793, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12406192], dtype=float32), 1.3163961]. 
=============================================
[2019-04-04 06:52:21,300] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[93.426605]
 [93.2952  ]
 [93.08648 ]
 [92.84707 ]
 [92.60592 ]], R is [[93.45468903]
 [93.5201416 ]
 [93.58493805]
 [93.649086  ]
 [93.71259308]].
[2019-04-04 06:52:21,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0760472e-10 5.0448952e-09 6.3816610e-29 7.0294278e-27 2.5203967e-21
 1.0000000e+00 4.9608219e-20], sum to 1.0000
[2019-04-04 06:52:21,403] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0643
[2019-04-04 06:52:21,451] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.200000000000001, 87.16666666666667, 93.0, 0.0, 26.0, 24.37001090451936, 0.1240218079035052, 0.0, 1.0, 18751.9553466297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 46200.0000, 
sim time next is 46800.0000, 
raw observation next is [8.3, 86.0, 91.5, 0.0, 26.0, 24.38909694269353, 0.125530322252895, 0.0, 1.0, 18748.83488959337], 
processed observation next is [0.0, 0.5652173913043478, 0.6925207756232689, 0.86, 0.305, 0.0, 0.6666666666666666, 0.5324247452244609, 0.541843440750965, 0.0, 1.0, 0.08928016614092081], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.42559472], dtype=float32), 1.2751275]. 
=============================================
[2019-04-04 06:52:23,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:23,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:23,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run10
[2019-04-04 06:52:24,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:24,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:24,783] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run10
[2019-04-04 06:52:25,396] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:25,396] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:25,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run10
[2019-04-04 06:52:25,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:25,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:25,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run10
[2019-04-04 06:52:26,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:26,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:26,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run10
[2019-04-04 06:52:26,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:26,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:26,712] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run10
[2019-04-04 06:52:26,740] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:26,740] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:26,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run10
[2019-04-04 06:52:26,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:26,975] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:26,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run10
[2019-04-04 06:52:27,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:52:27,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:52:27,180] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run10
[2019-04-04 06:52:29,122] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.4591909e-10 7.6704527e-09 1.7734100e-27 1.5234439e-25 8.7556688e-21
 1.0000000e+00 1.3683533e-19], sum to 1.0000
[2019-04-04 06:52:29,122] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1516
[2019-04-04 06:52:29,216] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.866666666666667, 84.66666666666666, 15.0, 0.0, 26.0, 24.53955699601114, 0.1896098215059848, 0.0, 1.0, 34419.24430637214], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 60000.0000, 
sim time next is 60600.0000, 
raw observation next is [5.683333333333334, 85.33333333333334, 12.0, 0.0, 26.0, 24.54824014586847, 0.1890691172121965, 0.0, 1.0, 34829.88553126116], 
processed observation next is [0.0, 0.6956521739130435, 0.6200369344413666, 0.8533333333333334, 0.04, 0.0, 0.6666666666666666, 0.5456866788223724, 0.5630230390707321, 0.0, 1.0, 0.1658565977679103], 
reward next is 0.8341, 
noisyNet noise sample is [array([0.18135981], dtype=float32), -0.98718673]. 
=============================================
[2019-04-04 06:52:44,398] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5198017e-09 3.5204422e-08 2.3945672e-24 5.3226231e-23 5.6983195e-19
 1.0000000e+00 4.7660668e-19], sum to 1.0000
[2019-04-04 06:52:44,404] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2442
[2019-04-04 06:52:44,438] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 26.0, 24.14689698070215, 0.07457351043798917, 0.0, 1.0, 44846.55249728422], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 262800.0000, 
sim time next is 263400.0000, 
raw observation next is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 26.0, 24.12644312870159, 0.06553351205258591, 0.0, 1.0, 45023.73185709313], 
processed observation next is [1.0, 0.043478260869565216, 0.2742382271468144, 0.6766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5105369273917993, 0.5218445040175287, 0.0, 1.0, 0.21439872312901492], 
reward next is 0.7856, 
noisyNet noise sample is [array([-0.40828362], dtype=float32), 0.24370384]. 
=============================================
[2019-04-04 06:52:48,763] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.6456286e-11 5.3944809e-09 5.8556113e-27 2.3277190e-26 4.9809004e-21
 1.0000000e+00 3.6794745e-20], sum to 1.0000
[2019-04-04 06:52:48,764] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5043
[2019-04-04 06:52:48,811] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 61.0, 146.5, 169.0, 26.0, 25.90108012763567, 0.4337984057604154, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 138000.0000, 
sim time next is 138600.0000, 
raw observation next is [-6.7, 61.0, 148.0, 106.0, 26.0, 25.85933657541719, 0.4225229923978378, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.49333333333333335, 0.11712707182320442, 0.6666666666666666, 0.654944714618099, 0.6408409974659459, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5981429], dtype=float32), -1.4568717]. 
=============================================
[2019-04-04 06:52:58,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1965958e-09 1.3788328e-08 5.1209379e-25 2.6139261e-23 4.8204638e-19
 1.0000000e+00 1.4222172e-18], sum to 1.0000
[2019-04-04 06:52:58,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9743
[2019-04-04 06:52:58,841] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 63.5, 18.0, 0.0, 26.0, 25.29294299514745, 0.2733345677193959, 1.0, 1.0, 35724.52795636204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 232200.0000, 
sim time next is 232800.0000, 
raw observation next is [-3.4, 64.0, 15.0, 0.0, 26.0, 25.33269028382034, 0.2775016130794676, 1.0, 1.0, 32760.21878189845], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.64, 0.05, 0.0, 0.6666666666666666, 0.6110575236516951, 0.5925005376931559, 1.0, 1.0, 0.15600104181856406], 
reward next is 0.8440, 
noisyNet noise sample is [array([0.22081462], dtype=float32), 0.89968246]. 
=============================================
[2019-04-04 06:53:40,374] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.45681675e-10 1.69419891e-08 1.67410705e-26 1.23336075e-24
 1.34570015e-20 1.00000000e+00 1.65038546e-19], sum to 1.0000
[2019-04-04 06:53:40,374] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6211
[2019-04-04 06:53:40,438] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 98.5, 0.0, 26.0, 25.59600278783704, 0.2991288959471972, 1.0, 1.0, 18713.59845215913], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 817200.0000, 
sim time next is 817800.0000, 
raw observation next is [-4.5, 71.0, 102.3333333333333, 0.0, 26.0, 25.59265584894826, 0.3027295769841419, 1.0, 1.0, 18908.11554909791], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.341111111111111, 0.0, 0.6666666666666666, 0.6327213207456884, 0.6009098589947139, 1.0, 1.0, 0.09003864547189482], 
reward next is 0.9100, 
noisyNet noise sample is [array([-0.22886463], dtype=float32), 1.6355519]. 
=============================================
[2019-04-04 06:53:46,203] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6285541e-10 1.5188488e-09 2.4821958e-26 5.2339052e-25 3.7613893e-20
 1.0000000e+00 1.0117046e-19], sum to 1.0000
[2019-04-04 06:53:46,203] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7650
[2019-04-04 06:53:46,255] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 45.0, 76.5, 17.0, 26.0, 25.6706313650889, 0.2942878098602255, 1.0, 1.0, 23438.77793453249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 748800.0000, 
sim time next is 749400.0000, 
raw observation next is [-0.9666666666666666, 46.5, 73.66666666666666, 12.33333333333333, 26.0, 25.25647361079614, 0.3331393248570569, 1.0, 1.0, 30817.44615409394], 
processed observation next is [1.0, 0.6956521739130435, 0.43582640812557716, 0.465, 0.24555555555555553, 0.013627992633517492, 0.6666666666666666, 0.6047061342330116, 0.611046441619019, 1.0, 1.0, 0.14674974359092352], 
reward next is 0.8533, 
noisyNet noise sample is [array([1.2296671], dtype=float32), -1.169526]. 
=============================================
[2019-04-04 06:53:46,289] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4900437e-11 2.1423288e-09 6.1411647e-29 6.3304108e-27 3.8405210e-22
 1.0000000e+00 1.2670450e-21], sum to 1.0000
[2019-04-04 06:53:46,295] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2035
[2019-04-04 06:53:46,332] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 93.0, 36.0, 0.0, 26.0, 25.80429884652008, 0.4057066432272986, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 921600.0000, 
sim time next is 922200.0000, 
raw observation next is [4.5, 92.83333333333333, 30.0, 0.0, 26.0, 25.78394138106896, 0.4027152089433052, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5872576177285319, 0.9283333333333332, 0.1, 0.0, 0.6666666666666666, 0.6486617817557466, 0.6342384029811018, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32863173], dtype=float32), -0.7036947]. 
=============================================
[2019-04-04 06:53:47,538] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2566405e-11 1.6438081e-10 8.4174122e-30 7.2727588e-28 3.0783719e-23
 1.0000000e+00 1.6243384e-21], sum to 1.0000
[2019-04-04 06:53:47,539] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7561
[2019-04-04 06:53:47,609] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 25.61618044491278, 0.5174563185256645, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1018200.0000, 
sim time next is 1018800.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.587497513894, 0.518297422670269, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6322914594911667, 0.6727658075567563, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1702791], dtype=float32), -0.4560278]. 
=============================================
[2019-04-04 06:53:49,140] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9261845e-10 2.6556248e-09 1.0985209e-28 2.8670669e-26 2.5174647e-22
 1.0000000e+00 3.1320812e-21], sum to 1.0000
[2019-04-04 06:53:49,141] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2044
[2019-04-04 06:53:49,167] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 93.0, 54.0, 0.0, 26.0, 25.78008794517295, 0.4083096783016476, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 919800.0000, 
sim time next is 920400.0000, 
raw observation next is [4.4, 93.0, 48.0, 0.0, 26.0, 25.78810812525372, 0.4090478320034077, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5844875346260389, 0.93, 0.16, 0.0, 0.6666666666666666, 0.6490090104378101, 0.6363492773344692, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37938976], dtype=float32), 0.2919109]. 
=============================================
[2019-04-04 06:53:56,548] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1971572e-12 2.9665159e-09 9.7492897e-31 1.8594456e-28 2.8039269e-23
 1.0000000e+00 9.3924487e-23], sum to 1.0000
[2019-04-04 06:53:56,554] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3741
[2019-04-04 06:53:56,574] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.41666666666667, 87.16666666666667, 104.0, 0.0, 26.0, 26.65191340074291, 0.6680436545040082, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 989400.0000, 
sim time next is 990000.0000, 
raw observation next is [11.6, 86.0, 108.0, 0.0, 26.0, 26.67253445430246, 0.6693162272095708, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7839335180055402, 0.86, 0.36, 0.0, 0.6666666666666666, 0.7227112045252051, 0.723105409069857, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7106634], dtype=float32), 1.2480783]. 
=============================================
[2019-04-04 06:53:56,584] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[95.03225]
 [94.79497]
 [94.52167]
 [94.49521]
 [94.37387]], R is [[95.32470703]
 [95.37145996]
 [95.4177475 ]
 [95.46356964]
 [95.50893402]].
[2019-04-04 06:54:02,361] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2829245e-10 1.4731982e-08 1.7144137e-28 3.1660328e-26 2.5948042e-21
 1.0000000e+00 8.6240364e-21], sum to 1.0000
[2019-04-04 06:54:02,376] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1565
[2019-04-04 06:54:02,396] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.416666666666666, 83.16666666666667, 0.0, 0.0, 26.0, 25.46002329103499, 0.4564847396806493, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 957000.0000, 
sim time next is 957600.0000, 
raw observation next is [6.6, 82.0, 0.0, 0.0, 26.0, 25.55986937576786, 0.4567322552545068, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6454293628808865, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6299891146473217, 0.6522440850848356, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16550548], dtype=float32), -1.3184773]. 
=============================================
[2019-04-04 06:54:02,417] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0505144e-11 5.9439316e-09 5.4114190e-29 1.9330907e-27 5.6176545e-22
 1.0000000e+00 7.0589267e-21], sum to 1.0000
[2019-04-04 06:54:02,417] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6682
[2019-04-04 06:54:02,437] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 82.5, 0.0, 0.0, 26.0, 25.47565985958544, 0.4553486576804389, 0.0, 1.0, 18756.0684592832], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 964200.0000, 
sim time next is 964800.0000, 
raw observation next is [7.7, 83.0, 0.0, 0.0, 26.0, 25.53948580411682, 0.4476348772535063, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.6759002770083103, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6282904836764015, 0.6492116257511688, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4397882], dtype=float32), 0.30674672]. 
=============================================
[2019-04-04 06:54:02,683] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.9021202e-10 9.4867509e-09 1.4570480e-28 3.5354829e-26 9.8131015e-22
 1.0000000e+00 1.8986899e-20], sum to 1.0000
[2019-04-04 06:54:02,683] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9309
[2019-04-04 06:54:02,696] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.083333333333334, 94.83333333333333, 0.0, 0.0, 26.0, 25.26859548288974, 0.4137020424580526, 0.0, 1.0, 38220.3784454995], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 951000.0000, 
sim time next is 951600.0000, 
raw observation next is [5.166666666666667, 93.66666666666666, 0.0, 0.0, 26.0, 25.26637100401493, 0.4251574959975011, 0.0, 1.0, 38115.60745423881], 
processed observation next is [1.0, 0.0, 0.6057248384118191, 0.9366666666666665, 0.0, 0.0, 0.6666666666666666, 0.6055309170012441, 0.6417191653325004, 0.0, 1.0, 0.18150289263923242], 
reward next is 0.8185, 
noisyNet noise sample is [array([-0.22465314], dtype=float32), 0.280018]. 
=============================================
[2019-04-04 06:54:07,757] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.7192740e-11 1.3073681e-09 7.4926908e-29 3.7628827e-27 4.6386884e-22
 1.0000000e+00 1.0334976e-20], sum to 1.0000
[2019-04-04 06:54:07,757] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6804
[2019-04-04 06:54:07,767] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.66666666666667, 39.66666666666667, 0.0, 26.0, 25.69508525761356, 0.51119109103647, 1.0, 1.0, 78322.49076812725], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1352400.0000, 
sim time next is 1353000.0000, 
raw observation next is [1.1, 92.83333333333333, 35.33333333333334, 0.0, 26.0, 25.6984014526498, 0.5183014066665149, 1.0, 1.0, 47307.25568291208], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.9283333333333332, 0.11777777777777781, 0.0, 0.6666666666666666, 0.6415334543874834, 0.6727671355555049, 1.0, 1.0, 0.22527264610910513], 
reward next is 0.7747, 
noisyNet noise sample is [array([0.0044326], dtype=float32), 0.78123856]. 
=============================================
[2019-04-04 06:54:07,784] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[87.64687 ]
 [87.64329 ]
 [87.89824 ]
 [88.096146]
 [88.325386]], R is [[87.54264069]
 [87.29425049]
 [87.42131042]
 [87.54709625]
 [87.67162323]].
[2019-04-04 06:54:08,883] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4549825e-12 1.4860459e-10 3.9184770e-31 1.0695300e-29 4.4003198e-24
 1.0000000e+00 1.4867420e-23], sum to 1.0000
[2019-04-04 06:54:08,885] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6825
[2019-04-04 06:54:08,903] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.86666666666667, 68.33333333333334, 230.6666666666667, 179.1666666666667, 26.0, 27.3641311709984, 0.9629047030957963, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1077600.0000, 
sim time next is 1078200.0000, 
raw observation next is [16.05, 67.5, 254.0, 215.0, 26.0, 27.48533988829138, 0.6458381962412766, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.9072022160664821, 0.675, 0.8466666666666667, 0.23756906077348067, 0.6666666666666666, 0.7904449906909484, 0.7152793987470921, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7211437], dtype=float32), 0.107283056]. 
=============================================
[2019-04-04 06:54:09,021] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.8798274e-11 2.0823654e-09 5.8804521e-29 5.3817601e-27 9.3103358e-22
 1.0000000e+00 1.1583276e-21], sum to 1.0000
[2019-04-04 06:54:09,022] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7423
[2019-04-04 06:54:09,053] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 45.0, 0.0, 26.0, 26.03730379980853, 0.588597221355959, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1330200.0000, 
sim time next is 1330800.0000, 
raw observation next is [0.5, 92.0, 54.5, 0.0, 26.0, 26.08792225195037, 0.5895144679483473, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.18166666666666667, 0.0, 0.6666666666666666, 0.6739935209958642, 0.696504822649449, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45219257], dtype=float32), 0.53794676]. 
=============================================
[2019-04-04 06:54:16,217] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8466278e-11 1.7011371e-09 7.5299076e-30 1.3926346e-27 8.1099697e-22
 1.0000000e+00 2.1628277e-21], sum to 1.0000
[2019-04-04 06:54:16,219] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4810
[2019-04-04 06:54:16,225] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.0, 100.0, 19.0, 0.0, 26.0, 24.60079661398782, 0.4214553514662492, 0.0, 1.0, 26533.59884668281], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1269000.0000, 
sim time next is 1269600.0000, 
raw observation next is [12.73333333333333, 100.0, 15.83333333333333, 0.0, 26.0, 24.59580846656353, 0.4221732093195605, 0.0, 1.0, 27474.39249671723], 
processed observation next is [0.0, 0.6956521739130435, 0.8153277931671283, 1.0, 0.05277777777777777, 0.0, 0.6666666666666666, 0.5496507055469607, 0.6407244031065201, 0.0, 1.0, 0.13083044046055825], 
reward next is 0.8692, 
noisyNet noise sample is [array([-0.35778072], dtype=float32), -0.061144393]. 
=============================================
[2019-04-04 06:54:19,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1359828e-10 1.9296539e-08 3.0876447e-27 1.0370295e-25 4.6534544e-21
 1.0000000e+00 4.2610889e-21], sum to 1.0000
[2019-04-04 06:54:19,597] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7208
[2019-04-04 06:54:19,612] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.533333333333333, 73.66666666666666, 0.0, 0.0, 26.0, 25.26567745959006, 0.5811164690012379, 0.0, 1.0, 198415.2365098056], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1543200.0000, 
sim time next is 1543800.0000, 
raw observation next is [7.616666666666667, 73.83333333333334, 0.0, 0.0, 26.0, 25.30940493897585, 0.6307638643150252, 0.0, 1.0, 173753.9425738378], 
processed observation next is [1.0, 0.8695652173913043, 0.6735918744228995, 0.7383333333333334, 0.0, 0.0, 0.6666666666666666, 0.6091170782479874, 0.7102546214383417, 0.0, 1.0, 0.8273997265420847], 
reward next is 0.1726, 
noisyNet noise sample is [array([0.689259], dtype=float32), -0.11251976]. 
=============================================
[2019-04-04 06:54:24,014] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.1171499e-11 5.1538424e-10 4.2344829e-28 6.4456704e-27 1.5991553e-21
 1.0000000e+00 1.7047873e-21], sum to 1.0000
[2019-04-04 06:54:24,014] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8881
[2019-04-04 06:54:24,034] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.06666666666667, 55.33333333333333, 72.83333333333333, 24.33333333333334, 26.0, 26.71281975307496, 0.7404489735716903, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1528800.0000, 
sim time next is 1529400.0000, 
raw observation next is [10.78333333333333, 56.66666666666667, 58.66666666666667, 20.66666666666667, 26.0, 26.01043173268787, 0.6791675907406486, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7613111726685133, 0.5666666666666668, 0.19555555555555557, 0.022836095764272566, 0.6666666666666666, 0.6675359777239892, 0.7263891969135495, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6811429], dtype=float32), 0.8635499]. 
=============================================
[2019-04-04 06:54:26,398] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.3197102e-11 5.3712466e-09 7.1667546e-28 3.0570104e-26 8.7627487e-22
 1.0000000e+00 2.3164674e-20], sum to 1.0000
[2019-04-04 06:54:26,398] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4731
[2019-04-04 06:54:26,416] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 59.0, 0.0, 26.0, 26.01352269324047, 0.5393020086416019, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1436400.0000, 
sim time next is 1437000.0000, 
raw observation next is [1.1, 92.0, 54.66666666666666, 0.0, 26.0, 26.05053112176828, 0.5166125816028565, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.1822222222222222, 0.0, 0.6666666666666666, 0.67087759348069, 0.6722041938676188, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8260907], dtype=float32), 2.7539485]. 
=============================================
[2019-04-04 06:54:26,425] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[87.44456 ]
 [87.65934 ]
 [87.836754]
 [88.019936]
 [88.2313  ]], R is [[87.40698242]
 [87.53291321]
 [87.65758514]
 [87.78101349]
 [87.90320587]].
[2019-04-04 06:54:44,375] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.2412517e-10 7.8568769e-09 3.2729321e-26 1.7672392e-25 7.0462056e-20
 1.0000000e+00 2.6761320e-20], sum to 1.0000
[2019-04-04 06:54:44,375] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9741
[2019-04-04 06:54:44,392] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.4, 91.66666666666667, 0.0, 0.0, 26.0, 25.34224321382181, 0.455689660509276, 0.0, 1.0, 43483.40204423034], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1732800.0000, 
sim time next is 1733400.0000, 
raw observation next is [0.35, 91.5, 0.0, 0.0, 26.0, 25.32528733029473, 0.4566209919602346, 0.0, 1.0, 43096.09327019761], 
processed observation next is [0.0, 0.043478260869565216, 0.47229916897506935, 0.915, 0.0, 0.0, 0.6666666666666666, 0.6104406108578943, 0.6522069973200783, 0.0, 1.0, 0.20521949176284576], 
reward next is 0.7948, 
noisyNet noise sample is [array([0.90551704], dtype=float32), 0.501013]. 
=============================================
[2019-04-04 06:54:44,398] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2831725e-09 7.3085951e-09 4.7751673e-26 5.1187508e-25 1.2122106e-19
 1.0000000e+00 2.0214313e-19], sum to 1.0000
[2019-04-04 06:54:44,398] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7860
[2019-04-04 06:54:44,415] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 87.0, 9.33333333333333, 0.0, 26.0, 24.73919820919495, 0.3005155756906223, 0.0, 1.0, 44072.71268924503], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1756200.0000, 
sim time next is 1756800.0000, 
raw observation next is [-1.7, 87.0, 13.5, 0.0, 26.0, 24.71502098745777, 0.2948648387136878, 0.0, 1.0, 44102.54572467873], 
processed observation next is [0.0, 0.34782608695652173, 0.4155124653739613, 0.87, 0.045, 0.0, 0.6666666666666666, 0.5595850822881475, 0.5982882795712293, 0.0, 1.0, 0.21001212249847012], 
reward next is 0.7900, 
noisyNet noise sample is [array([-0.34001175], dtype=float32), 1.3140206]. 
=============================================
[2019-04-04 06:54:46,422] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7982877e-09 1.1963516e-07 2.2218534e-26 3.5658260e-24 3.6218627e-20
 9.9999988e-01 1.1245858e-18], sum to 1.0000
[2019-04-04 06:54:46,422] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1461
[2019-04-04 06:54:46,435] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.283333333333333, 87.0, 0.0, 0.0, 26.0, 24.88535198898978, 0.3384855960510033, 0.0, 1.0, 43869.36444674501], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1750200.0000, 
sim time next is 1750800.0000, 
raw observation next is [-1.366666666666667, 87.0, 0.0, 0.0, 26.0, 24.85728994280371, 0.3330268283482395, 0.0, 1.0, 43916.59201584577], 
processed observation next is [0.0, 0.2608695652173913, 0.42474607571560485, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5714408285669759, 0.6110089427827465, 0.0, 1.0, 0.2091266286468846], 
reward next is 0.7909, 
noisyNet noise sample is [array([-0.85528225], dtype=float32), -1.2002404]. 
=============================================
[2019-04-04 06:54:50,980] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5400563e-09 1.3026450e-07 2.3721107e-24 1.1366250e-22 2.1605631e-18
 9.9999988e-01 2.6991819e-18], sum to 1.0000
[2019-04-04 06:54:50,982] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2893
[2019-04-04 06:54:51,052] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.75, 71.0, 120.0, 0.0, 26.0, 24.96569670071434, 0.2507682187919532, 0.0, 1.0, 52881.06580605587], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1859400.0000, 
sim time next is 1860000.0000, 
raw observation next is [-4.666666666666667, 71.0, 128.3333333333333, 6.666666666666665, 26.0, 24.95513774760073, 0.2551423364577226, 0.0, 1.0, 54035.30031438558], 
processed observation next is [0.0, 0.5217391304347826, 0.3333333333333333, 0.71, 0.42777777777777765, 0.00736648250460405, 0.6666666666666666, 0.5795948123000608, 0.5850474454859075, 0.0, 1.0, 0.2573109538780266], 
reward next is 0.7427, 
noisyNet noise sample is [array([-1.4107034], dtype=float32), -0.9910183]. 
=============================================
[2019-04-04 06:54:51,074] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[77.29215 ]
 [77.638466]
 [77.909645]
 [78.24183 ]
 [78.536964]], R is [[77.00696564]
 [76.98508453]
 [77.00479126]
 [77.07762146]
 [77.1593399 ]].
[2019-04-04 06:54:55,317] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8717829e-09 2.0512166e-08 2.5447675e-24 1.3945253e-22 1.5111901e-18
 1.0000000e+00 1.3702978e-17], sum to 1.0000
[2019-04-04 06:54:55,317] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7482
[2019-04-04 06:54:55,338] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.9, 83.0, 0.0, 0.0, 26.0, 24.92183346597277, 0.2669506350329666, 0.0, 1.0, 43102.7837243636], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1981800.0000, 
sim time next is 1982400.0000, 
raw observation next is [-5.8, 83.0, 0.0, 0.0, 26.0, 24.81755632280268, 0.2524370352992329, 0.0, 1.0, 42935.35862838235], 
processed observation next is [1.0, 0.9565217391304348, 0.30193905817174516, 0.83, 0.0, 0.0, 0.6666666666666666, 0.56812969356689, 0.5841456784330776, 0.0, 1.0, 0.20445408870658263], 
reward next is 0.7955, 
noisyNet noise sample is [array([-1.252254], dtype=float32), 1.4126407]. 
=============================================
[2019-04-04 06:54:55,434] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.1617297e-10 6.1136248e-08 1.5057081e-23 5.7940988e-22 5.7099413e-18
 9.9999988e-01 1.7006726e-17], sum to 1.0000
[2019-04-04 06:54:55,434] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1573
[2019-04-04 06:54:55,471] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.983333333333333, 64.16666666666667, 44.66666666666666, 0.0, 26.0, 25.53946175851087, 0.3109184634532439, 1.0, 1.0, 30873.87871335168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1959000.0000, 
sim time next is 1959600.0000, 
raw observation next is [-3.166666666666667, 66.33333333333334, 37.33333333333333, 0.0, 26.0, 25.50084851172112, 0.3160404774809554, 1.0, 1.0, 33520.62074970889], 
processed observation next is [1.0, 0.6956521739130435, 0.3748845798707295, 0.6633333333333334, 0.12444444444444443, 0.0, 0.6666666666666666, 0.6250707093100933, 0.6053468258269851, 1.0, 1.0, 0.15962200357004233], 
reward next is 0.8404, 
noisyNet noise sample is [array([-0.6940196], dtype=float32), -0.4785181]. 
=============================================
[2019-04-04 06:55:06,782] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4202070e-09 2.4240983e-08 1.1673929e-24 1.0568067e-23 2.9534877e-19
 1.0000000e+00 3.1239599e-18], sum to 1.0000
[2019-04-04 06:55:06,782] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4552
[2019-04-04 06:55:06,828] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 80.5, 0.0, 0.0, 26.0, 25.24754887624329, 0.3825027389098225, 0.0, 1.0, 64349.57587871299], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1974600.0000, 
sim time next is 1975200.0000, 
raw observation next is [-5.6, 79.66666666666667, 0.0, 0.0, 26.0, 25.24228613084174, 0.3809376312977644, 0.0, 1.0, 51247.72346545119], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6035238442368117, 0.6269792104325881, 0.0, 1.0, 0.24403677840691043], 
reward next is 0.7560, 
noisyNet noise sample is [array([0.8024155], dtype=float32), -0.33145398]. 
=============================================
[2019-04-04 06:55:31,762] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.1449867e-08 6.1595819e-07 2.3913196e-21 5.1188967e-20 1.6026094e-15
 9.9999928e-01 4.3199678e-16], sum to 1.0000
[2019-04-04 06:55:31,762] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4829
[2019-04-04 06:55:31,790] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.933333333333334, 51.33333333333333, 0.0, 0.0, 26.0, 24.23664155420945, 0.06711914131752834, 0.0, 1.0, 43492.93544105245], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2425200.0000, 
sim time next is 2425800.0000, 
raw observation next is [-7.116666666666666, 52.16666666666667, 0.0, 0.0, 26.0, 24.18704178235494, 0.0560394105200401, 0.0, 1.0, 43527.38275079281], 
processed observation next is [0.0, 0.043478260869565216, 0.265466297322253, 0.5216666666666667, 0.0, 0.0, 0.6666666666666666, 0.5155868151962449, 0.5186798035066801, 0.0, 1.0, 0.20727325119425147], 
reward next is 0.7927, 
noisyNet noise sample is [array([-0.5412668], dtype=float32), 0.8514722]. 
=============================================
[2019-04-04 06:55:31,902] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8164087e-09 4.3892907e-08 1.3966228e-22 3.3335617e-21 2.0210401e-17
 1.0000000e+00 2.3130548e-16], sum to 1.0000
[2019-04-04 06:55:31,902] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4720
[2019-04-04 06:55:31,920] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15863164123137, 0.2810747596082467, 0.0, 1.0, 43113.74704714974], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2406600.0000, 
sim time next is 2407200.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.1356741353611, 0.2756762548485061, 0.0, 1.0, 43156.80929670222], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5946395112800916, 0.5918920849495021, 0.0, 1.0, 0.20550861569858198], 
reward next is 0.7945, 
noisyNet noise sample is [array([-1.2970166], dtype=float32), 0.5206102]. 
=============================================
[2019-04-04 06:55:39,468] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.9281816e-08 2.7692423e-07 2.0916986e-21 1.9331709e-20 7.9340094e-16
 9.9999964e-01 7.5844442e-16], sum to 1.0000
[2019-04-04 06:55:39,468] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6813
[2019-04-04 06:55:39,499] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8999999999999999, 34.0, 0.0, 0.0, 26.0, 25.20083063640665, 0.2509575226098986, 0.0, 1.0, 40297.89767341728], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2500200.0000, 
sim time next is 2500800.0000, 
raw observation next is [-0.8, 34.33333333333334, 0.0, 0.0, 26.0, 25.18230652042462, 0.2476604809804795, 0.0, 1.0, 40253.21457307939], 
processed observation next is [0.0, 0.9565217391304348, 0.4404432132963989, 0.34333333333333343, 0.0, 0.0, 0.6666666666666666, 0.5985255433687184, 0.5825534936601598, 0.0, 1.0, 0.19168197415752092], 
reward next is 0.8083, 
noisyNet noise sample is [array([1.0428747], dtype=float32), 0.94652104]. 
=============================================
[2019-04-04 06:55:51,694] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0148588e-09 5.2074832e-08 3.6774585e-25 1.7171735e-23 1.8563854e-19
 1.0000000e+00 2.1113673e-18], sum to 1.0000
[2019-04-04 06:55:51,705] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4480
[2019-04-04 06:55:51,822] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.199999999999999, 78.33333333333334, 53.66666666666667, 2.666666666666666, 26.0, 25.39764990733219, 0.2959647085795569, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2621400.0000, 
sim time next is 2622000.0000, 
raw observation next is [-7.100000000000001, 77.66666666666667, 65.33333333333334, 1.333333333333333, 26.0, 25.38083079004533, 0.3109320164790878, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.26592797783933514, 0.7766666666666667, 0.21777777777777782, 0.00147329650092081, 0.6666666666666666, 0.6150692325037775, 0.6036440054930293, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1740165], dtype=float32), -0.21196543]. 
=============================================
[2019-04-04 06:55:51,839] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[79.38566]
 [79.11486]
 [79.00016]
 [79.14601]
 [78.50398]], R is [[79.73729706]
 [79.93992615]
 [80.14052582]
 [80.33911896]
 [80.11621094]].
[2019-04-04 06:56:17,447] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.40086683e-11 4.38560438e-10 8.97168224e-29 1.25593536e-26
 8.49719770e-22 1.00000000e+00 1.46738902e-21], sum to 1.0000
[2019-04-04 06:56:17,448] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3763
[2019-04-04 06:56:17,537] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 100.0, 85.83333333333334, 0.0, 26.0, 25.84995105698346, 0.4497961165372451, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2906400.0000, 
sim time next is 2907000.0000, 
raw observation next is [2.0, 100.0, 85.0, 0.0, 26.0, 25.87597125659815, 0.454729115070575, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.2833333333333333, 0.0, 0.6666666666666666, 0.6563309380498458, 0.6515763716901917, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06977775], dtype=float32), -0.8444144]. 
=============================================
[2019-04-04 06:56:17,564] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[90.554886]
 [90.73862 ]
 [91.029015]
 [91.33749 ]
 [91.81169 ]], R is [[90.5499115 ]
 [90.64441681]
 [90.73797607]
 [90.83059692]
 [90.92229462]].
[2019-04-04 06:56:21,119] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9156100e-09 2.7769110e-08 2.6455945e-24 3.4250400e-23 1.1365321e-18
 1.0000000e+00 7.4429645e-18], sum to 1.0000
[2019-04-04 06:56:21,138] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4810
[2019-04-04 06:56:21,183] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 56.0, 0.0, 0.0, 26.0, 25.23345197637525, 0.3544679040056415, 0.0, 1.0, 66861.09250621637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2846400.0000, 
sim time next is 2847000.0000, 
raw observation next is [2.0, 59.0, 0.0, 0.0, 26.0, 25.21197385973766, 0.354721329943388, 0.0, 1.0, 51853.24727851586], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.59, 0.0, 0.0, 0.6666666666666666, 0.6009978216448051, 0.6182404433144627, 0.0, 1.0, 0.24692022513578982], 
reward next is 0.7531, 
noisyNet noise sample is [array([1.2074171], dtype=float32), -0.10894347]. 
=============================================
[2019-04-04 06:56:21,256] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.08097]
 [76.92672]
 [76.26541]
 [75.84471]
 [75.73213]], R is [[77.50435638]
 [77.41092682]
 [77.19821167]
 [77.17033386]
 [77.26498413]].
[2019-04-04 06:56:22,679] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6298165e-09 1.4242578e-08 1.3348533e-24 5.7763717e-24 7.1704792e-19
 1.0000000e+00 1.6551464e-18], sum to 1.0000
[2019-04-04 06:56:22,679] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2740
[2019-04-04 06:56:22,764] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 63.66666666666667, 0.0, 0.0, 26.0, 25.02916427204246, 0.3829862441665493, 0.0, 1.0, 189214.848730822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2665200.0000, 
sim time next is 2665800.0000, 
raw observation next is [-1.2, 64.0, 0.0, 0.0, 26.0, 25.03958214515298, 0.410299379423243, 0.0, 1.0, 118938.9716709948], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.64, 0.0, 0.0, 0.6666666666666666, 0.586631845429415, 0.6367664598077477, 0.0, 1.0, 0.5663760555761657], 
reward next is 0.4336, 
noisyNet noise sample is [array([-1.1854863], dtype=float32), 0.55468774]. 
=============================================
[2019-04-04 06:56:28,675] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0422889e-09 1.5465393e-08 7.8890981e-25 3.1306563e-23 5.1735365e-19
 1.0000000e+00 1.2303382e-18], sum to 1.0000
[2019-04-04 06:56:28,676] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9861
[2019-04-04 06:56:28,731] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.34743712311475, 0.4394078466682749, 0.0, 1.0, 48973.43977221643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2757600.0000, 
sim time next is 2758200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.3419254212427, 0.3884692412779556, 0.0, 1.0, 48232.8365881076], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.6118271184368916, 0.6294897470926518, 0.0, 1.0, 0.22968017422908382], 
reward next is 0.7703, 
noisyNet noise sample is [array([-0.18617103], dtype=float32), 1.2492188]. 
=============================================
[2019-04-04 06:56:31,843] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5015530e-11 6.4695965e-10 3.2809149e-29 1.7906561e-27 2.0286432e-22
 1.0000000e+00 3.8848008e-21], sum to 1.0000
[2019-04-04 06:56:31,843] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1738
[2019-04-04 06:56:31,888] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 100.0, 127.0, 0.0, 26.0, 25.44134627523492, 0.3419923323402259, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2901600.0000, 
sim time next is 2902200.0000, 
raw observation next is [2.0, 100.0, 114.6666666666667, 0.0, 26.0, 25.43550962027862, 0.234514157791524, 1.0, 1.0, 9340.205835115268], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 1.0, 0.38222222222222235, 0.0, 0.6666666666666666, 0.6196258016898849, 0.578171385930508, 1.0, 1.0, 0.04447717064340604], 
reward next is 0.9555, 
noisyNet noise sample is [array([-0.46420613], dtype=float32), -0.49012366]. 
=============================================
[2019-04-04 06:56:38,286] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 06:56:38,307] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:56:38,307] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:56:38,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run14
[2019-04-04 06:56:38,392] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:56:38,392] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:56:38,394] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run14
[2019-04-04 06:56:38,410] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:56:38,411] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:56:38,413] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run14
[2019-04-04 06:56:51,429] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.24429886], dtype=float32), 0.23419586]
[2019-04-04 06:56:51,429] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.8, 74.33333333333334, 0.0, 0.0, 26.0, 24.18535862120583, 0.0602223661512922, 0.0, 1.0, 55465.10598189761]
[2019-04-04 06:56:51,429] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:56:51,430] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.5061884e-10 2.2551681e-08 5.4845178e-26 3.9564920e-24 1.2677620e-19
 1.0000000e+00 6.6509648e-19], sampled 0.7769826890923667
[2019-04-04 06:56:57,794] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.24429886], dtype=float32), 0.23419586]
[2019-04-04 06:56:57,794] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [8.3, 93.0, 0.0, 0.0, 26.0, 24.57575317826463, 0.3086290927130805, 0.0, 1.0, 37762.7818069542]
[2019-04-04 06:56:57,795] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:56:57,796] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.7595506e-10 1.8044416e-09 4.0375826e-28 3.0010863e-26 1.9862143e-21
 1.0000000e+00 1.7171042e-20], sampled 0.4819807454258943
[2019-04-04 06:58:00,123] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.24429886], dtype=float32), 0.23419586]
[2019-04-04 06:58:00,123] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.08333333333333333, 94.5, 0.0, 0.0, 26.0, 25.37464940494044, 0.4818031087016688, 0.0, 1.0, 52911.10107254818]
[2019-04-04 06:58:00,123] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 06:58:00,124] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.8447145e-10 3.3535046e-09 2.3463591e-27 2.1632689e-25 1.2736556e-20
 1.0000000e+00 8.4270615e-20], sampled 0.0851629553889558
[2019-04-04 06:58:40,298] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.24429886], dtype=float32), 0.23419586]
[2019-04-04 06:58:40,298] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.033333333333333, 73.66666666666667, 0.0, 0.0, 26.0, 24.60349724577996, 0.2358972292228598, 0.0, 1.0, 41106.2837909406]
[2019-04-04 06:58:40,298] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:58:40,299] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.5768112e-10 2.6222454e-08 6.1018268e-26 3.4241502e-24 1.4456496e-19
 1.0000000e+00 6.3262122e-19], sampled 0.2475942891322901
[2019-04-04 06:59:30,066] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.24429886], dtype=float32), 0.23419586]
[2019-04-04 06:59:30,066] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.0, 63.33333333333334, 100.3333333333333, 607.8333333333333, 26.0, 25.82690932374593, 0.4877851708536844, 1.0, 1.0, 0.0]
[2019-04-04 06:59:30,066] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:59:30,067] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.6123371e-11 2.5136431e-09 1.9027716e-28 1.3399844e-26 2.2083901e-21
 1.0000000e+00 7.1305659e-21], sampled 0.8665090120198952
[2019-04-04 06:59:49,242] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7840 239865368.7420 1604.9560
[2019-04-04 07:00:18,916] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.4099 263463930.4480 1556.9858
[2019-04-04 07:00:22,320] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 07:00:23,356] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1300000, evaluation results [1300000.0, 7241.409855009758, 263463930.4479512, 1556.9858411143289, 7353.783958371476, 239865368.74198616, 1604.956008778044, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 07:00:25,583] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7376114e-10 1.1197501e-08 1.5815640e-26 9.1815156e-25 2.9615867e-20
 1.0000000e+00 4.3708368e-19], sum to 1.0000
[2019-04-04 07:00:25,583] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3653
[2019-04-04 07:00:25,608] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.833333333333333, 83.83333333333334, 0.0, 0.0, 26.0, 25.13414082462772, 0.3920150271776124, 0.0, 1.0, 43771.63623237996], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2933400.0000, 
sim time next is 2934000.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 25.08401743300012, 0.383035797943971, 0.0, 1.0, 43663.304490666], 
processed observation next is [1.0, 1.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5903347860833433, 0.627678599314657, 0.0, 1.0, 0.2079204975746], 
reward next is 0.7921, 
noisyNet noise sample is [array([-2.2253335], dtype=float32), -0.6651208]. 
=============================================
[2019-04-04 07:00:25,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[81.9205  ]
 [81.90237 ]
 [82.22712 ]
 [82.195984]
 [82.30242 ]], R is [[81.9693222 ]
 [81.94119263]
 [81.912323  ]
 [81.87999725]
 [81.84044647]].
[2019-04-04 07:00:36,792] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5897943e-10 3.6223857e-10 3.0282447e-29 7.9726813e-27 1.0556595e-21
 1.0000000e+00 6.8730066e-21], sum to 1.0000
[2019-04-04 07:00:36,806] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4357
[2019-04-04 07:00:36,824] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.60528811520976, 0.6151634927576999, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3195600.0000, 
sim time next is 3196200.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.58491608623974, 0.6035299880582771, 0.0, 1.0, 18738.23346040317], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6320763405199784, 0.7011766626860924, 0.0, 1.0, 0.089229683144777], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.49232513], dtype=float32), 1.5529817]. 
=============================================
[2019-04-04 07:00:41,588] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.7457776e-11 4.0848884e-09 1.1420283e-28 1.6836045e-26 1.3831513e-21
 1.0000000e+00 8.5754517e-21], sum to 1.0000
[2019-04-04 07:00:41,595] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2759
[2019-04-04 07:00:41,614] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 96.0, 0.0, 0.0, 26.0, 25.25594401660127, 0.4617036630417218, 0.0, 1.0, 40867.04330564953], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3220200.0000, 
sim time next is 3220800.0000, 
raw observation next is [-3.0, 94.66666666666666, 0.0, 0.0, 26.0, 25.20211554877169, 0.4525445584969119, 0.0, 1.0, 40946.87503450995], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.9466666666666665, 0.0, 0.0, 0.6666666666666666, 0.6001762957309742, 0.6508481861656373, 0.0, 1.0, 0.19498511921195213], 
reward next is 0.8050, 
noisyNet noise sample is [array([0.7765842], dtype=float32), 0.59093773]. 
=============================================
[2019-04-04 07:00:45,448] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.6072616e-09 2.8425683e-08 1.1294491e-24 5.0324044e-23 1.1497274e-18
 1.0000000e+00 2.8771912e-18], sum to 1.0000
[2019-04-04 07:00:45,450] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5263
[2019-04-04 07:00:45,497] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.44038713668873, 0.4883955209231823, 0.0, 1.0, 36045.67701928398], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3277200.0000, 
sim time next is 3277800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.38606946151899, 0.4808139264237869, 0.0, 1.0, 65253.22067029626], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6155057884599158, 0.660271308807929, 0.0, 1.0, 0.310729622239506], 
reward next is 0.6893, 
noisyNet noise sample is [array([0.2589369], dtype=float32), -0.3758913]. 
=============================================
[2019-04-04 07:00:50,675] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.6393692e-12 6.4433658e-10 1.8406731e-28 1.0440234e-25 5.6055815e-22
 1.0000000e+00 3.1371631e-21], sum to 1.0000
[2019-04-04 07:00:50,676] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9414
[2019-04-04 07:00:50,691] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 54.0, 117.0, 804.5, 26.0, 26.3572886505539, 0.6055162771466379, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3326400.0000, 
sim time next is 3327000.0000, 
raw observation next is [-5.833333333333333, 54.0, 117.3333333333333, 806.6666666666667, 26.0, 26.36121902072197, 0.6086080730007146, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.30101569713758086, 0.54, 0.391111111111111, 0.8913443830570903, 0.6666666666666666, 0.6967682517268307, 0.7028693576669048, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2565956], dtype=float32), -1.683797]. 
=============================================
[2019-04-04 07:00:50,697] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[84.15582 ]
 [84.303604]
 [84.43822 ]
 [84.56705 ]
 [84.688065]], R is [[84.1269455 ]
 [84.28567505]
 [84.44281769]
 [84.59838867]
 [84.75240326]].
[2019-04-04 07:00:52,945] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.7108134e-09 9.5123390e-08 6.1028094e-24 5.0243857e-23 1.9498744e-18
 9.9999988e-01 3.8329763e-18], sum to 1.0000
[2019-04-04 07:00:52,953] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1116
[2019-04-04 07:00:52,965] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 25.40878004174985, 0.4362064425398473, 0.0, 1.0, 53609.0351106581], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3364200.0000, 
sim time next is 3364800.0000, 
raw observation next is [-4.666666666666666, 69.0, 0.0, 0.0, 26.0, 25.32545850877167, 0.4243059700441039, 0.0, 1.0, 66761.02246608761], 
processed observation next is [1.0, 0.9565217391304348, 0.33333333333333337, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6104548757309726, 0.6414353233480347, 0.0, 1.0, 0.3179096307908934], 
reward next is 0.6821, 
noisyNet noise sample is [array([-0.0891268], dtype=float32), -0.9077043]. 
=============================================
[2019-04-04 07:01:00,516] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5328697e-09 1.1541957e-08 1.0381145e-24 1.4004213e-22 3.1762797e-18
 1.0000000e+00 6.5497778e-18], sum to 1.0000
[2019-04-04 07:01:00,518] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4247
[2019-04-04 07:01:00,527] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.90099420346925, 0.538287806892618, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3522600.0000, 
sim time next is 3523200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.74359539272513, 0.5130813314460573, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6452996160604275, 0.6710271104820191, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.92477363], dtype=float32), -1.7994369]. 
=============================================
[2019-04-04 07:01:10,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.0936599e-12 1.0322426e-09 2.0989004e-29 4.4570595e-27 5.9011244e-22
 1.0000000e+00 1.8118444e-22], sum to 1.0000
[2019-04-04 07:01:10,608] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8882
[2019-04-04 07:01:10,624] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.166666666666667, 60.33333333333334, 113.0, 796.6666666666667, 26.0, 26.16072635704249, 0.601349521418905, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3496200.0000, 
sim time next is 3496800.0000, 
raw observation next is [1.333333333333333, 59.66666666666667, 114.0, 803.3333333333333, 26.0, 26.23722040811439, 0.6117635831461522, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4995383194829178, 0.5966666666666667, 0.38, 0.8876611418047882, 0.6666666666666666, 0.6864350340095324, 0.7039211943820507, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.58323234], dtype=float32), -2.1187618]. 
=============================================
[2019-04-04 07:01:16,123] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.5150070e-10 8.8360871e-08 1.1209027e-25 3.0177466e-24 4.1612254e-19
 9.9999988e-01 5.5871656e-19], sum to 1.0000
[2019-04-04 07:01:16,123] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7231
[2019-04-04 07:01:16,143] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 69.0, 0.0, 0.0, 26.0, 25.3917708513505, 0.3928041016796573, 0.0, 1.0, 30848.26771207463], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3901200.0000, 
sim time next is 3901800.0000, 
raw observation next is [-2.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.36542869418707, 0.3838659434646214, 0.0, 1.0, 45349.45345172765], 
processed observation next is [1.0, 0.13043478260869565, 0.3841181902123731, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6137857245155892, 0.6279553144882072, 0.0, 1.0, 0.21594977834156023], 
reward next is 0.7841, 
noisyNet noise sample is [array([-1.4330873], dtype=float32), -0.51003253]. 
=============================================
[2019-04-04 07:01:24,869] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6827780e-09 3.0793760e-08 1.1408457e-25 3.9566581e-24 3.4276856e-20
 1.0000000e+00 5.5723306e-19], sum to 1.0000
[2019-04-04 07:01:24,869] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0989
[2019-04-04 07:01:24,884] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.30907238091333, 0.3380392414813653, 0.0, 1.0, 41044.95226634765], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3725400.0000, 
sim time next is 3726000.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.28968135283294, 0.3319272295585285, 0.0, 1.0, 41085.28771637758], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6074734460694117, 0.6106424098528428, 0.0, 1.0, 0.1956442272208456], 
reward next is 0.8044, 
noisyNet noise sample is [array([1.6554034], dtype=float32), -1.1744982]. 
=============================================
[2019-04-04 07:01:24,889] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[81.26245]
 [80.87291]
 [80.61594]
 [80.11327]
 [79.79181]], R is [[81.64226532]
 [81.63039398]
 [81.61873627]
 [81.60683441]
 [81.59320831]].
[2019-04-04 07:01:25,695] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2814814e-09 7.2581429e-08 1.0661608e-22 2.1185768e-21 9.0048920e-17
 9.9999988e-01 3.6280635e-17], sum to 1.0000
[2019-04-04 07:01:25,700] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5446
[2019-04-04 07:01:25,715] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 27.66666666666667, 0.0, 0.0, 26.0, 25.91480573377513, 0.5154280469552908, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4040400.0000, 
sim time next is 4041000.0000, 
raw observation next is [-3.5, 28.5, 0.0, 0.0, 26.0, 25.7644702638243, 0.4925345383293029, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.36565096952908593, 0.285, 0.0, 0.0, 0.6666666666666666, 0.647039188652025, 0.664178179443101, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.77511233], dtype=float32), 0.35882893]. 
=============================================
[2019-04-04 07:01:25,752] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[68.51391 ]
 [69.08659 ]
 [69.82298 ]
 [70.63531 ]
 [71.362206]], R is [[68.33349609]
 [68.65016174]
 [68.96366119]
 [69.27402496]
 [69.58128357]].
[2019-04-04 07:01:28,665] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.91175545e-10 1.35350604e-07 1.28183118e-25 1.44756607e-24
 9.15565636e-20 9.99999881e-01 1.01095216e-19], sum to 1.0000
[2019-04-04 07:01:28,666] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8740
[2019-04-04 07:01:28,684] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.06428033430126, 0.3404372349456304, 0.0, 1.0, 43785.76327474713], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3813600.0000, 
sim time next is 3814200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.09924230254862, 0.3343551313621562, 0.0, 1.0, 43622.83699860162], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5916035252123851, 0.6114517104540521, 0.0, 1.0, 0.20772779523143628], 
reward next is 0.7923, 
noisyNet noise sample is [array([-0.7254921], dtype=float32), -0.23065753]. 
=============================================
[2019-04-04 07:01:42,691] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7070693e-08 2.9365799e-07 2.3730085e-22 3.8319606e-21 4.5552816e-17
 9.9999964e-01 1.8173178e-16], sum to 1.0000
[2019-04-04 07:01:42,695] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3504
[2019-04-04 07:01:42,726] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-14.0, 69.0, 0.0, 0.0, 26.0, 23.38108600397202, -0.06589661753452641, 0.0, 1.0, 42986.45472361133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3999600.0000, 
sim time next is 4000200.0000, 
raw observation next is [-13.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.35659172684445, -0.07557156928792802, 0.0, 1.0, 42886.94895595982], 
processed observation next is [1.0, 0.30434782608695654, 0.07940904893813489, 0.68, 0.0, 0.0, 0.6666666666666666, 0.44638264390370413, 0.47480947690402403, 0.0, 1.0, 0.20422356645695153], 
reward next is 0.7958, 
noisyNet noise sample is [array([0.53190166], dtype=float32), -0.59310013]. 
=============================================
[2019-04-04 07:01:43,194] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1387261e-10 2.9497997e-08 1.7051203e-27 9.6428656e-26 3.6300777e-21
 1.0000000e+00 3.3013185e-20], sum to 1.0000
[2019-04-04 07:01:43,228] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3067
[2019-04-04 07:01:43,241] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.45, 70.0, 0.0, 0.0, 26.0, 25.48965385611009, 0.3818101643285208, 0.0, 1.0, 18756.1284979283], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4339800.0000, 
sim time next is 4340400.0000, 
raw observation next is [3.4, 70.33333333333333, 0.0, 0.0, 26.0, 25.48835197151182, 0.373227685589935, 0.0, 1.0, 18753.66731281949], 
processed observation next is [1.0, 0.21739130434782608, 0.556786703601108, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.6240293309593182, 0.6244092285299784, 0.0, 1.0, 0.0893031776800928], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.29257846], dtype=float32), -0.08228548]. 
=============================================
[2019-04-04 07:01:44,014] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3073430e-09 1.5342924e-07 1.8146179e-22 2.4562786e-22 5.3316006e-17
 9.9999988e-01 1.5031015e-17], sum to 1.0000
[2019-04-04 07:01:44,014] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2533
[2019-04-04 07:01:44,042] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 29.66666666666666, 0.0, 0.0, 26.0, 25.47806214298982, 0.5218837336807872, 0.0, 1.0, 51683.74007233168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4048800.0000, 
sim time next is 4049400.0000, 
raw observation next is [-4.0, 29.33333333333333, 0.0, 0.0, 26.0, 25.58069583396394, 0.5279882583461989, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.2933333333333333, 0.0, 0.0, 0.6666666666666666, 0.6317246528303283, 0.6759960861153996, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9147286], dtype=float32), -0.01892865]. 
=============================================
[2019-04-04 07:01:45,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.63425323e-09 2.27374301e-07 5.30106198e-23 4.03457621e-22
 1.34145294e-17 9.99999762e-01 1.94641934e-17], sum to 1.0000
[2019-04-04 07:01:45,425] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0483
[2019-04-04 07:01:45,474] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 29.66666666666666, 0.0, 0.0, 26.0, 25.47823706627354, 0.521852799170247, 0.0, 1.0, 51730.45138291249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4048800.0000, 
sim time next is 4049400.0000, 
raw observation next is [-4.0, 29.33333333333333, 0.0, 0.0, 26.0, 25.58058165517296, 0.5279482683358331, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.2933333333333333, 0.0, 0.0, 0.6666666666666666, 0.63171513793108, 0.6759827561119444, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3608832], dtype=float32), -1.6813853]. 
=============================================
[2019-04-04 07:01:51,707] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7877395e-10 1.5873628e-08 1.5465663e-26 8.8062978e-25 3.3662642e-20
 1.0000000e+00 1.1727296e-19], sum to 1.0000
[2019-04-04 07:01:51,708] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9704
[2019-04-04 07:01:51,743] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 40.66666666666667, 196.0, 282.3333333333333, 26.0, 25.17541322654792, 0.3826997191630079, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4198200.0000, 
sim time next is 4198800.0000, 
raw observation next is [2.0, 41.33333333333334, 191.5, 185.6666666666666, 26.0, 25.16148979747748, 0.3665817718249081, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.41333333333333344, 0.6383333333333333, 0.20515653775322276, 0.6666666666666666, 0.5967908164564566, 0.622193923941636, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10952532], dtype=float32), -0.3801218]. 
=============================================
[2019-04-04 07:01:55,486] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5450966e-09 3.2378555e-08 3.0758098e-25 3.2597418e-23 3.7865872e-18
 1.0000000e+00 8.8172879e-18], sum to 1.0000
[2019-04-04 07:01:55,489] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3097
[2019-04-04 07:01:55,503] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.54474468967427, 0.3842211106347724, 0.0, 1.0, 18742.97037632848], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4222200.0000, 
sim time next is 4222800.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.5024649982567, 0.377310904511942, 0.0, 1.0, 48366.25886642004], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6252054165213915, 0.6257703015039807, 0.0, 1.0, 0.23031551841152398], 
reward next is 0.7697, 
noisyNet noise sample is [array([1.2677429], dtype=float32), 1.2132064]. 
=============================================
[2019-04-04 07:02:00,093] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2367765e-09 1.6319325e-08 3.1021694e-25 6.7989151e-23 2.7502765e-19
 1.0000000e+00 2.4869733e-18], sum to 1.0000
[2019-04-04 07:02:00,095] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0508
[2019-04-04 07:02:00,109] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.53289610481542, 0.451113378670124, 1.0, 1.0, 19875.61658156659], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4557600.0000, 
sim time next is 4558200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.14977893791432, 0.4331085106042901, 1.0, 1.0, 60575.36979527318], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.59581491149286, 0.6443695035347634, 1.0, 1.0, 0.28845414188225327], 
reward next is 0.7115, 
noisyNet noise sample is [array([1.0993448], dtype=float32), -0.22052376]. 
=============================================
[2019-04-04 07:02:03,179] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.2788604e-10 8.4574125e-09 1.0282049e-27 1.4754576e-25 1.9293994e-20
 1.0000000e+00 4.7415107e-20], sum to 1.0000
[2019-04-04 07:02:03,179] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3538
[2019-04-04 07:02:03,194] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.333333333333334, 63.66666666666667, 0.0, 0.0, 26.0, 25.81154277640801, 0.6356297953130824, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4407600.0000, 
sim time next is 4408200.0000, 
raw observation next is [7.199999999999999, 64.0, 0.0, 0.0, 26.0, 25.82281218841719, 0.6343712131537292, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.662049861495845, 0.64, 0.0, 0.0, 0.6666666666666666, 0.6519010157014323, 0.711457071051243, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5238431], dtype=float32), 0.80568916]. 
=============================================
[2019-04-04 07:02:06,491] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6670890e-09 7.6387128e-09 8.7588158e-27 1.9373080e-25 1.5744596e-20
 1.0000000e+00 3.2530405e-19], sum to 1.0000
[2019-04-04 07:02:06,495] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9334
[2019-04-04 07:02:06,521] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.50678810841445, 0.5399563255586105, 0.0, 1.0, 57598.37979708827], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4485600.0000, 
sim time next is 4486200.0000, 
raw observation next is [-0.05, 72.0, 0.0, 0.0, 26.0, 25.46504218643415, 0.4939283943486331, 0.0, 1.0, 71001.07012544415], 
processed observation next is [1.0, 0.9565217391304348, 0.461218836565097, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6220868488695125, 0.6646427981162111, 0.0, 1.0, 0.33810033393068645], 
reward next is 0.6619, 
noisyNet noise sample is [array([0.8948209], dtype=float32), 0.44519255]. 
=============================================
[2019-04-04 07:02:16,215] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4267676e-10 4.2519265e-08 7.9661183e-26 1.3837726e-23 2.5700116e-19
 1.0000000e+00 1.9169005e-18], sum to 1.0000
[2019-04-04 07:02:16,215] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3220
[2019-04-04 07:02:16,228] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 25.01607497237546, 0.2819717651200404, 0.0, 1.0, 39176.05208616835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4849200.0000, 
sim time next is 4849800.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.98549736123022, 0.2753512409130817, 0.0, 1.0, 39195.20197238576], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5821247801025183, 0.5917837469710272, 0.0, 1.0, 0.18664381891612267], 
reward next is 0.8134, 
noisyNet noise sample is [array([-0.18179633], dtype=float32), -0.43203852]. 
=============================================
[2019-04-04 07:02:17,223] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3868858e-11 6.9871675e-10 6.1047742e-29 3.3631590e-27 5.2324359e-22
 1.0000000e+00 5.9891168e-22], sum to 1.0000
[2019-04-04 07:02:17,223] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6249
[2019-04-04 07:02:17,239] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.166666666666667, 48.83333333333334, 197.6666666666667, 285.6666666666666, 26.0, 27.46742365503773, 0.8968877006163617, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4633800.0000, 
sim time next is 4634400.0000, 
raw observation next is [5.333333333333334, 47.66666666666667, 196.3333333333333, 207.3333333333333, 26.0, 27.63500433830754, 0.677559699709814, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6103416435826409, 0.47666666666666674, 0.6544444444444443, 0.22909760589318595, 0.6666666666666666, 0.8029170281922949, 0.7258532332366047, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9627721], dtype=float32), -0.6349014]. 
=============================================
[2019-04-04 07:02:23,164] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8528432e-09 2.7503129e-08 3.8383932e-24 1.0684369e-22 3.0170195e-18
 1.0000000e+00 2.4066454e-17], sum to 1.0000
[2019-04-04 07:02:23,175] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5150
[2019-04-04 07:02:23,203] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.833333333333333, 44.50000000000001, 0.0, 0.0, 26.0, 24.99552284279526, 0.2973945721337654, 0.0, 1.0, 36928.86160605744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4903800.0000, 
sim time next is 4904400.0000, 
raw observation next is [1.666666666666667, 45.0, 0.0, 0.0, 26.0, 24.97835479977574, 0.2958173919015374, 0.0, 1.0, 42236.02370647708], 
processed observation next is [0.0, 0.782608695652174, 0.5087719298245615, 0.45, 0.0, 0.0, 0.6666666666666666, 0.5815295666479784, 0.5986057973005124, 0.0, 1.0, 0.2011239224117956], 
reward next is 0.7989, 
noisyNet noise sample is [array([-0.02927788], dtype=float32), 1.665424]. 
=============================================
[2019-04-04 07:02:26,092] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:26,092] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:26,111] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run11
[2019-04-04 07:02:26,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:26,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:26,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run11
[2019-04-04 07:02:26,989] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.7924713e-10 7.0768486e-08 2.1660762e-25 3.2885789e-24 4.2300618e-19
 9.9999988e-01 2.1777577e-18], sum to 1.0000
[2019-04-04 07:02:26,990] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7680
[2019-04-04 07:02:27,006] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 25.016246074017, 0.2819933795647833, 0.0, 1.0, 39176.37684463122], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4849200.0000, 
sim time next is 4849800.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.98567167088429, 0.2753736897993791, 0.0, 1.0, 39195.49813963148], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5821393059070242, 0.5917912299331264, 0.0, 1.0, 0.1866452292363404], 
reward next is 0.8134, 
noisyNet noise sample is [array([-2.1512926], dtype=float32), 0.94106126]. 
=============================================
[2019-04-04 07:02:31,138] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:31,138] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:31,142] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run11
[2019-04-04 07:02:31,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:31,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:31,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run11
[2019-04-04 07:02:33,501] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85000, global step 1356607: loss 0.0002
[2019-04-04 07:02:33,504] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85000, global step 1356607: learning rate 0.0000
[2019-04-04 07:02:33,626] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:33,626] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:33,638] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run11
[2019-04-04 07:02:33,920] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85000, global step 1356729: loss 0.0003
[2019-04-04 07:02:33,932] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85000, global step 1356729: learning rate 0.0000
[2019-04-04 07:02:34,915] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.0938953e-10 5.0150419e-08 1.7802033e-25 9.9228959e-24 2.1982979e-19
 1.0000000e+00 3.5793685e-19], sum to 1.0000
[2019-04-04 07:02:34,916] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6124
[2019-04-04 07:02:34,939] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 48.0, 0.0, 0.0, 26.0, 25.17309587889672, 0.250772466391443, 0.0, 1.0, 38555.57746059151], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4944600.0000, 
sim time next is 4945200.0000, 
raw observation next is [-2.666666666666667, 48.66666666666666, 0.0, 0.0, 26.0, 25.10936306507998, 0.2378977209856268, 0.0, 1.0, 38600.98620530344], 
processed observation next is [1.0, 0.21739130434782608, 0.38873499538319484, 0.4866666666666666, 0.0, 0.0, 0.6666666666666666, 0.5924469220899983, 0.5792992403285423, 0.0, 1.0, 0.18381422002525447], 
reward next is 0.8162, 
noisyNet noise sample is [array([2.6763022], dtype=float32), -0.29162988]. 
=============================================
[2019-04-04 07:02:35,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:35,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:35,627] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run11
[2019-04-04 07:02:37,517] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:37,518] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:37,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run11
[2019-04-04 07:02:39,015] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85000, global step 1358296: loss 0.0004
[2019-04-04 07:02:39,015] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85000, global step 1358296: learning rate 0.0000
[2019-04-04 07:02:39,180] A3C_AGENT_WORKER-Thread-6 INFO:Local step 85000, global step 1358363: loss 0.0002
[2019-04-04 07:02:39,180] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 85000, global step 1358363: learning rate 0.0000
[2019-04-04 07:02:41,709] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85000, global step 1359239: loss 0.0004
[2019-04-04 07:02:41,711] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85000, global step 1359239: learning rate 0.0000
[2019-04-04 07:02:43,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:43,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:43,063] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run11
[2019-04-04 07:02:43,208] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:43,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:43,222] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run11
[2019-04-04 07:02:43,546] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85000, global step 1359831: loss 0.0014
[2019-04-04 07:02:43,547] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85000, global step 1359831: learning rate 0.0000
[2019-04-04 07:02:44,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:44,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:44,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run11
[2019-04-04 07:02:45,177] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85000, global step 1360323: loss 0.0031
[2019-04-04 07:02:45,178] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85000, global step 1360323: learning rate 0.0000
[2019-04-04 07:02:45,199] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:45,199] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:45,203] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run11
[2019-04-04 07:02:45,244] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:45,246] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:45,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run11
[2019-04-04 07:02:45,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:45,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:45,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run11
[2019-04-04 07:02:45,707] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:45,707] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:45,710] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run11
[2019-04-04 07:02:45,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:45,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:45,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run11
[2019-04-04 07:02:47,593] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:02:47,593] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:02:47,604] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run11
[2019-04-04 07:02:52,253] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85000, global step 1361230: loss 0.0004
[2019-04-04 07:02:52,253] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85000, global step 1361230: learning rate 0.0000
[2019-04-04 07:02:52,324] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85000, global step 1361239: loss 0.0006
[2019-04-04 07:02:52,325] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85000, global step 1361239: learning rate 0.0000
[2019-04-04 07:02:53,174] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5263151e-11 3.5467524e-09 1.2215733e-27 5.9134808e-27 1.7079052e-21
 1.0000000e+00 1.8513906e-21], sum to 1.0000
[2019-04-04 07:02:53,174] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4454
[2019-04-04 07:02:53,242] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.616666666666667, 61.0, 136.0, 523.6666666666666, 26.0, 25.64353955327216, 0.4294092867374479, 1.0, 1.0, 34624.99955232429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 133800.0000, 
sim time next is 134400.0000, 
raw observation next is [-7.433333333333334, 61.0, 137.5, 503.8333333333334, 26.0, 25.68684713713468, 0.447377453292045, 1.0, 1.0, 32417.27688188678], 
processed observation next is [1.0, 0.5652173913043478, 0.2566943674976916, 0.61, 0.4583333333333333, 0.5567219152854513, 0.6666666666666666, 0.6405705947612234, 0.649125817764015, 1.0, 1.0, 0.1543679851518418], 
reward next is 0.8456, 
noisyNet noise sample is [array([-0.72803164], dtype=float32), 0.53635687]. 
=============================================
[2019-04-04 07:02:55,278] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.9032546e-10 3.5462943e-08 1.1753095e-26 1.8977881e-24 6.4551178e-20
 1.0000000e+00 4.5739624e-19], sum to 1.0000
[2019-04-04 07:02:55,280] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1961
[2019-04-04 07:02:55,294] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.02428530240748, -0.6260026937834978, 0.0, 1.0, 40681.43349759776], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 18000.0000, 
sim time next is 18600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.04394758394561, -0.6103791906739388, 0.0, 1.0, 40601.89671485911], 
processed observation next is [0.0, 0.21739130434782608, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.25366229866213413, 0.29654026977535375, 0.0, 1.0, 0.1933423653088529], 
reward next is 0.8067, 
noisyNet noise sample is [array([-1.4761614], dtype=float32), 0.3071889]. 
=============================================
[2019-04-04 07:02:55,599] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85000, global step 1361843: loss 0.0022
[2019-04-04 07:02:55,599] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85000, global step 1361843: learning rate 0.0000
[2019-04-04 07:02:55,658] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85000, global step 1361862: loss 0.0047
[2019-04-04 07:02:55,658] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85000, global step 1361862: learning rate 0.0000
[2019-04-04 07:02:55,669] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85000, global step 1361865: loss 0.0028
[2019-04-04 07:02:55,672] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85000, global step 1361865: learning rate 0.0000
[2019-04-04 07:02:55,773] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85000, global step 1361898: loss 0.0026
[2019-04-04 07:02:55,774] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85000, global step 1361898: learning rate 0.0000
[2019-04-04 07:02:55,775] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85000, global step 1361898: loss 0.0022
[2019-04-04 07:02:55,790] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85000, global step 1361899: learning rate 0.0000
[2019-04-04 07:02:55,912] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1177716e-09 1.3221642e-08 1.1773872e-26 1.8090870e-24 3.7004338e-20
 1.0000000e+00 3.0689629e-19], sum to 1.0000
[2019-04-04 07:02:55,912] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8477
[2019-04-04 07:02:55,928] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.38567164288263, -0.5295393678796717, 0.0, 1.0, 40263.15668689913], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 27000.0000, 
sim time next is 27600.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 26.0, 21.40726672070302, -0.5240088516819826, 0.0, 1.0, 40258.72242881158], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.2839388933919184, 0.32533038277267246, 0.0, 1.0, 0.19170820204195993], 
reward next is 0.8083, 
noisyNet noise sample is [array([0.05935724], dtype=float32), -1.337523]. 
=============================================
[2019-04-04 07:02:56,705] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85000, global step 1362112: loss 0.0015
[2019-04-04 07:02:56,707] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85000, global step 1362112: learning rate 0.0000
[2019-04-04 07:02:57,645] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85000, global step 1362349: loss 0.0016
[2019-04-04 07:02:57,676] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85000, global step 1362349: learning rate 0.0000
[2019-04-04 07:03:00,289] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1163030e-10 2.1287823e-08 1.7076720e-27 9.4411089e-26 4.7179086e-21
 1.0000000e+00 6.8720660e-20], sum to 1.0000
[2019-04-04 07:03:00,289] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9599
[2019-04-04 07:03:00,330] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.2, 81.66666666666667, 0.0, 0.0, 26.0, 23.99056931657923, 0.06263541970536164, 0.0, 1.0, 43449.8313563273], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 99600.0000, 
sim time next is 100200.0000, 
raw observation next is [-3.3, 80.33333333333334, 0.0, 0.0, 26.0, 23.96459018178291, 0.05550968205426974, 0.0, 1.0, 43542.20889582732], 
processed observation next is [1.0, 0.13043478260869565, 0.37119113573407203, 0.8033333333333335, 0.0, 0.0, 0.6666666666666666, 0.4970491818152425, 0.5185032273514233, 0.0, 1.0, 0.20734385188489202], 
reward next is 0.7927, 
noisyNet noise sample is [array([1.1304984], dtype=float32), 1.0491824]. 
=============================================
[2019-04-04 07:03:04,096] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85500, global step 1364134: loss 0.4435
[2019-04-04 07:03:04,097] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85500, global step 1364134: learning rate 0.0000
[2019-04-04 07:03:04,738] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85500, global step 1364287: loss 0.4335
[2019-04-04 07:03:04,739] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85500, global step 1364287: learning rate 0.0000
[2019-04-04 07:03:06,638] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8758266e-11 3.8684003e-10 2.4570398e-28 2.8286318e-26 6.9224445e-22
 1.0000000e+00 4.5830051e-21], sum to 1.0000
[2019-04-04 07:03:06,639] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9361
[2019-04-04 07:03:06,702] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 61.0, 157.0, 308.0, 26.0, 25.57279313453146, 0.3915015088896288, 1.0, 1.0, 47602.27133373833], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 129600.0000, 
sim time next is 130200.0000, 
raw observation next is [-8.3, 61.0, 148.0, 406.3333333333334, 26.0, 25.71197814627525, 0.4039285564674946, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.23268698060941828, 0.61, 0.49333333333333335, 0.44898710865561703, 0.6666666666666666, 0.6426648455229375, 0.6346428521558315, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13031258], dtype=float32), 0.31097895]. 
=============================================
[2019-04-04 07:03:10,136] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85500, global step 1365592: loss 0.4294
[2019-04-04 07:03:10,137] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85500, global step 1365592: learning rate 0.0000
[2019-04-04 07:03:10,624] A3C_AGENT_WORKER-Thread-6 INFO:Local step 85500, global step 1365708: loss 0.4348
[2019-04-04 07:03:10,625] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 85500, global step 1365708: learning rate 0.0000
[2019-04-04 07:03:13,347] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85500, global step 1366515: loss 0.4120
[2019-04-04 07:03:13,347] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85500, global step 1366515: learning rate 0.0000
[2019-04-04 07:03:14,839] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.3465957e-11 4.0131756e-09 1.0576728e-25 7.5424648e-24 2.7740262e-19
 1.0000000e+00 6.6680363e-19], sum to 1.0000
[2019-04-04 07:03:14,839] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5387
[2019-04-04 07:03:14,884] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 42.0, 70.0, 477.0, 26.0, 26.26698164258647, 0.5286488384571079, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 315000.0000, 
sim time next is 315600.0000, 
raw observation next is [-9.5, 42.0, 62.33333333333334, 437.5, 26.0, 26.31216862155506, 0.524594860000917, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.2077777777777778, 0.48342541436464087, 0.6666666666666666, 0.6926807184629217, 0.674864953333639, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.96208143], dtype=float32), -1.0884103]. 
=============================================
[2019-04-04 07:03:15,267] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85500, global step 1366940: loss 0.3730
[2019-04-04 07:03:15,268] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85500, global step 1366940: learning rate 0.0000
[2019-04-04 07:03:16,089] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85500, global step 1367136: loss 0.4087
[2019-04-04 07:03:16,091] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85500, global step 1367136: learning rate 0.0000
[2019-04-04 07:03:18,743] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2946777e-08 4.5502981e-07 6.1840864e-22 6.3824868e-21 2.0711156e-16
 9.9999952e-01 2.6438283e-16], sum to 1.0000
[2019-04-04 07:03:18,743] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3096
[2019-04-04 07:03:18,782] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.8, 74.66666666666667, 0.0, 0.0, 26.0, 22.18231208342469, -0.3911271355350534, 0.0, 1.0, 48635.37910432401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 364800.0000, 
sim time next is 365400.0000, 
raw observation next is [-15.9, 75.5, 0.0, 0.0, 26.0, 22.09253174417249, -0.3987561204159006, 0.0, 1.0, 48616.15553353441], 
processed observation next is [1.0, 0.21739130434782608, 0.02216066481994457, 0.755, 0.0, 0.0, 0.6666666666666666, 0.3410443120143742, 0.36708129319469984, 0.0, 1.0, 0.23150550254064003], 
reward next is 0.7685, 
noisyNet noise sample is [array([0.75066334], dtype=float32), -0.49063712]. 
=============================================
[2019-04-04 07:03:22,021] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9207618e-10 8.5367615e-09 2.5086891e-25 2.6127895e-23 1.6478448e-18
 1.0000000e+00 1.4310390e-18], sum to 1.0000
[2019-04-04 07:03:22,021] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5551
[2019-04-04 07:03:22,080] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 42.33333333333333, 78.0, 574.3333333333334, 26.0, 24.89936855297147, 0.3611095228450151, 1.0, 1.0, 198442.407540736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 312600.0000, 
sim time next is 313200.0000, 
raw observation next is [-9.5, 42.0, 76.0, 550.0, 26.0, 25.24815336145667, 0.4525637075971103, 1.0, 1.0, 112482.3136055023], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.25333333333333335, 0.6077348066298343, 0.6666666666666666, 0.6040127801213891, 0.6508545691990367, 1.0, 1.0, 0.5356300647881063], 
reward next is 0.4644, 
noisyNet noise sample is [array([-0.05819377], dtype=float32), 0.36364886]. 
=============================================
[2019-04-04 07:03:23,430] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85500, global step 1369103: loss 0.3731
[2019-04-04 07:03:23,431] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85500, global step 1369103: learning rate 0.0000
[2019-04-04 07:03:24,219] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85500, global step 1369281: loss 0.3670
[2019-04-04 07:03:24,219] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85500, global step 1369281: learning rate 0.0000
[2019-04-04 07:03:24,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2879581e-10 4.2712371e-09 2.3671007e-26 3.6558068e-25 3.4292549e-20
 1.0000000e+00 7.9896064e-20], sum to 1.0000
[2019-04-04 07:03:24,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2211
[2019-04-04 07:03:24,643] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 49.0, 94.5, 708.0, 26.0, 25.91247653886598, 0.4385979486473259, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 302400.0000, 
sim time next is 303000.0000, 
raw observation next is [-10.41666666666667, 48.16666666666667, 90.66666666666667, 724.6666666666666, 26.0, 26.00756785108211, 0.4440284567401139, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.17405355493998145, 0.4816666666666667, 0.3022222222222222, 0.8007366482504603, 0.6666666666666666, 0.6672973209235092, 0.6480094855800379, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.57255363], dtype=float32), -0.9460582]. 
=============================================
[2019-04-04 07:03:24,661] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[81.20419 ]
 [81.35191 ]
 [81.281525]
 [80.66971 ]
 [80.083534]], R is [[81.22154999]
 [81.40933228]
 [81.59523773]
 [81.12373352]
 [80.36549377]].
[2019-04-04 07:03:26,924] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85500, global step 1369996: loss 0.3622
[2019-04-04 07:03:26,925] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85500, global step 1369996: learning rate 0.0000
[2019-04-04 07:03:26,926] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4346780e-11 8.8602841e-09 5.6338591e-29 1.3278535e-26 5.4729923e-22
 1.0000000e+00 3.1744094e-21], sum to 1.0000
[2019-04-04 07:03:26,930] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1072
[2019-04-04 07:03:26,992] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 85.0, 77.0, 141.0, 26.0, 24.82087438287883, 0.2764568168466149, 0.0, 1.0, 30507.55761369906], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 556200.0000, 
sim time next is 556800.0000, 
raw observation next is [-0.6, 84.33333333333333, 79.0, 140.0, 26.0, 24.87275466540716, 0.2777922474071457, 0.0, 1.0, 18742.40347038912], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.8433333333333333, 0.2633333333333333, 0.15469613259668508, 0.6666666666666666, 0.5727295554505968, 0.5925974158023819, 0.0, 1.0, 0.08924954033518628], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.008468], dtype=float32), 0.047774516]. 
=============================================
[2019-04-04 07:03:27,108] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85500, global step 1370046: loss 0.4003
[2019-04-04 07:03:27,109] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85500, global step 1370046: learning rate 0.0000
[2019-04-04 07:03:27,179] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85500, global step 1370063: loss 0.3543
[2019-04-04 07:03:27,181] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85500, global step 1370063: learning rate 0.0000
[2019-04-04 07:03:27,424] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85500, global step 1370126: loss 0.3462
[2019-04-04 07:03:27,424] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85500, global step 1370126: learning rate 0.0000
[2019-04-04 07:03:27,642] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85500, global step 1370180: loss 0.3581
[2019-04-04 07:03:27,642] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85500, global step 1370180: learning rate 0.0000
[2019-04-04 07:03:28,120] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85500, global step 1370282: loss 0.3746
[2019-04-04 07:03:28,121] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85500, global step 1370282: learning rate 0.0000
[2019-04-04 07:03:28,801] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85500, global step 1370443: loss 0.3476
[2019-04-04 07:03:28,802] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85500, global step 1370443: learning rate 0.0000
[2019-04-04 07:03:29,144] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8083918e-09 3.5956948e-08 3.1075497e-23 2.2173368e-22 2.4966784e-18
 1.0000000e+00 2.0662859e-17], sum to 1.0000
[2019-04-04 07:03:29,144] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2522
[2019-04-04 07:03:29,177] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.48333333333333, 80.00000000000001, 0.0, 0.0, 26.0, 24.1753967952053, 0.09676477870640073, 0.0, 1.0, 47075.38040072779], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 339000.0000, 
sim time next is 339600.0000, 
raw observation next is [-13.56666666666667, 78.0, 0.0, 0.0, 26.0, 24.11660618192058, 0.08772107949052992, 0.0, 1.0, 47089.11008698367], 
processed observation next is [1.0, 0.9565217391304348, 0.08679593721144958, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5097171818267151, 0.5292403598301766, 0.0, 1.0, 0.22423385755706507], 
reward next is 0.7758, 
noisyNet noise sample is [array([2.645843], dtype=float32), 0.55197954]. 
=============================================
[2019-04-04 07:03:32,067] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86000, global step 1371485: loss 1.3229
[2019-04-04 07:03:32,070] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86000, global step 1371485: learning rate 0.0000
[2019-04-04 07:03:33,000] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86000, global step 1371698: loss 1.3525
[2019-04-04 07:03:33,012] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86000, global step 1371698: learning rate 0.0000
[2019-04-04 07:03:35,381] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2269139e-08 5.6182620e-07 9.2534308e-22 1.0794411e-20 1.2250653e-16
 9.9999940e-01 2.4414504e-16], sum to 1.0000
[2019-04-04 07:03:35,381] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6701
[2019-04-04 07:03:35,445] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.47100666587723, -0.1024363651668263, 0.0, 1.0, 45626.2713909184], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 438000.0000, 
sim time next is 438600.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.42712149837105, -0.1163161861369142, 0.0, 1.0, 45688.52046904894], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 0.6666666666666666, 0.4522601248642542, 0.46122793795436196, 0.0, 1.0, 0.21756438318594734], 
reward next is 0.7824, 
noisyNet noise sample is [array([-0.3759104], dtype=float32), -1.6894648]. 
=============================================
[2019-04-04 07:03:37,972] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86000, global step 1372956: loss 1.2765
[2019-04-04 07:03:37,974] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86000, global step 1372956: learning rate 0.0000
[2019-04-04 07:03:38,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8010186e-08 4.1379163e-08 1.4100273e-22 9.4465722e-21 2.0372029e-16
 9.9999988e-01 1.1334071e-16], sum to 1.0000
[2019-04-04 07:03:38,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1101
[2019-04-04 07:03:38,245] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.60628907505891, 0.3549069539082217, 1.0, 1.0, 41764.94630336553], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 411600.0000, 
sim time next is 412200.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.52947609459156, 0.3412758905877049, 1.0, 1.0, 45226.46459877052], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6274563412159632, 0.6137586301959016, 1.0, 1.0, 0.2153641171370025], 
reward next is 0.7846, 
noisyNet noise sample is [array([0.75377196], dtype=float32), -0.7108021]. 
=============================================
[2019-04-04 07:03:39,290] A3C_AGENT_WORKER-Thread-6 INFO:Local step 86000, global step 1373447: loss 1.3655
[2019-04-04 07:03:39,291] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 86000, global step 1373449: learning rate 0.0000
[2019-04-04 07:03:40,867] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86000, global step 1373930: loss 1.3213
[2019-04-04 07:03:40,871] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86000, global step 1373932: learning rate 0.0000
[2019-04-04 07:03:43,257] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86000, global step 1374619: loss 1.3236
[2019-04-04 07:03:43,263] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86000, global step 1374620: learning rate 0.0000
[2019-04-04 07:03:43,332] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86000, global step 1374645: loss 1.3161
[2019-04-04 07:03:43,339] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86000, global step 1374646: learning rate 0.0000
[2019-04-04 07:03:48,268] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0893516e-09 6.7016801e-09 9.3963768e-26 3.6945322e-23 5.8696571e-19
 1.0000000e+00 2.0792126e-18], sum to 1.0000
[2019-04-04 07:03:48,268] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6134
[2019-04-04 07:03:48,354] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.9, 87.0, 0.0, 0.0, 26.0, 24.96359027555758, 0.2877356933041705, 0.0, 1.0, 47974.6594760397], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 580800.0000, 
sim time next is 581400.0000, 
raw observation next is [-2.0, 87.0, 0.0, 0.0, 26.0, 24.94341702800877, 0.289773327911177, 0.0, 1.0, 56483.26170906942], 
processed observation next is [0.0, 0.7391304347826086, 0.40720221606648205, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5786180856673976, 0.5965911093037256, 0.0, 1.0, 0.26896791290033056], 
reward next is 0.7310, 
noisyNet noise sample is [array([-1.7977219], dtype=float32), 1.5896637]. 
=============================================
[2019-04-04 07:03:51,048] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86000, global step 1377171: loss 1.2895
[2019-04-04 07:03:51,049] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86000, global step 1377171: learning rate 0.0000
[2019-04-04 07:03:51,575] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86000, global step 1377324: loss 1.3505
[2019-04-04 07:03:51,576] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86000, global step 1377324: learning rate 0.0000
[2019-04-04 07:03:53,579] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86000, global step 1378067: loss 1.3340
[2019-04-04 07:03:53,580] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86000, global step 1378067: learning rate 0.0000
[2019-04-04 07:03:53,632] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86000, global step 1378089: loss 1.2857
[2019-04-04 07:03:53,633] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86000, global step 1378089: learning rate 0.0000
[2019-04-04 07:03:53,854] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3883343e-10 4.0625906e-08 4.3453708e-25 8.7311740e-24 2.1590626e-19
 1.0000000e+00 2.7552344e-18], sum to 1.0000
[2019-04-04 07:03:53,855] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3598
[2019-04-04 07:03:53,871] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 65.5, 0.0, 0.0, 26.0, 23.70191307578552, -0.02337590633469909, 0.0, 1.0, 43824.38578584997], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 627000.0000, 
sim time next is 627600.0000, 
raw observation next is [-4.5, 66.0, 0.0, 0.0, 26.0, 23.67483318084852, -0.02630854501692122, 0.0, 1.0, 43780.06652031217], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.66, 0.0, 0.0, 0.6666666666666666, 0.47290276507071, 0.4912304849943596, 0.0, 1.0, 0.20847650723958175], 
reward next is 0.7915, 
noisyNet noise sample is [array([0.30889902], dtype=float32), -0.067623936]. 
=============================================
[2019-04-04 07:03:53,909] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.7462047e-10 1.9080062e-08 8.1688531e-26 3.9327923e-24 8.8582403e-20
 1.0000000e+00 4.4610579e-19], sum to 1.0000
[2019-04-04 07:03:53,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0111
[2019-04-04 07:03:53,928] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.2, 81.66666666666667, 0.0, 0.0, 26.0, 24.81111438259661, 0.2513255749680333, 0.0, 1.0, 41484.55922090947], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 858000.0000, 
sim time next is 858600.0000, 
raw observation next is [-3.1, 81.0, 0.0, 0.0, 26.0, 24.78785344617673, 0.2468123486584247, 0.0, 1.0, 41401.11406636181], 
processed observation next is [1.0, 0.9565217391304348, 0.37673130193905824, 0.81, 0.0, 0.0, 0.6666666666666666, 0.5656544538480608, 0.5822707828861415, 0.0, 1.0, 0.19714816222077053], 
reward next is 0.8029, 
noisyNet noise sample is [array([0.7433562], dtype=float32), 0.26415402]. 
=============================================
[2019-04-04 07:03:54,137] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86000, global step 1378295: loss 1.3066
[2019-04-04 07:03:54,147] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86000, global step 1378296: learning rate 0.0000
[2019-04-04 07:03:54,149] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86000, global step 1378296: loss 1.3445
[2019-04-04 07:03:54,151] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86000, global step 1378297: learning rate 0.0000
[2019-04-04 07:03:54,172] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86000, global step 1378308: loss 1.3069
[2019-04-04 07:03:54,187] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86000, global step 1378311: learning rate 0.0000
[2019-04-04 07:03:54,582] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86000, global step 1378441: loss 1.3644
[2019-04-04 07:03:54,583] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86000, global step 1378441: learning rate 0.0000
[2019-04-04 07:03:55,301] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86000, global step 1378652: loss 1.2882
[2019-04-04 07:03:55,302] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86000, global step 1378652: learning rate 0.0000
[2019-04-04 07:03:56,939] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86500, global step 1379096: loss 0.4281
[2019-04-04 07:03:56,940] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86500, global step 1379096: learning rate 0.0000
[2019-04-04 07:03:58,448] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86500, global step 1379528: loss 0.4897
[2019-04-04 07:03:58,450] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86500, global step 1379528: learning rate 0.0000
[2019-04-04 07:04:01,677] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86500, global step 1380773: loss 0.4063
[2019-04-04 07:04:01,679] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86500, global step 1380773: learning rate 0.0000
[2019-04-04 07:04:03,755] A3C_AGENT_WORKER-Thread-6 INFO:Local step 86500, global step 1381581: loss 0.4564
[2019-04-04 07:04:03,756] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 86500, global step 1381582: learning rate 0.0000
[2019-04-04 07:04:04,697] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86500, global step 1382010: loss 0.3897
[2019-04-04 07:04:04,698] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86500, global step 1382010: learning rate 0.0000
[2019-04-04 07:04:06,842] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86500, global step 1382918: loss 0.4332
[2019-04-04 07:04:06,843] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86500, global step 1382918: learning rate 0.0000
[2019-04-04 07:04:06,981] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86500, global step 1382987: loss 0.4130
[2019-04-04 07:04:06,982] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86500, global step 1382987: learning rate 0.0000
[2019-04-04 07:04:09,195] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4619399e-11 6.5930377e-09 1.6146103e-26 1.8797387e-25 5.1741062e-20
 1.0000000e+00 6.2799953e-20], sum to 1.0000
[2019-04-04 07:04:09,195] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4071
[2019-04-04 07:04:09,249] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 75.0, 99.0, 0.0, 26.0, 25.51651139907253, 0.2923600184670425, 1.0, 1.0, 27797.11690792053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 822600.0000, 
sim time next is 823200.0000, 
raw observation next is [-4.5, 76.33333333333333, 97.66666666666666, 0.0, 26.0, 25.43855783902186, 0.2890959122264865, 1.0, 1.0, 27884.98502131235], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7633333333333333, 0.32555555555555554, 0.0, 0.6666666666666666, 0.6198798199184884, 0.5963653040754955, 1.0, 1.0, 0.13278564295863024], 
reward next is 0.8672, 
noisyNet noise sample is [array([-1.2990285], dtype=float32), 0.07636381]. 
=============================================
[2019-04-04 07:04:10,047] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.75836995e-08 1.55392129e-07 1.03531544e-22 4.27642683e-21
 1.57961123e-17 9.99999762e-01 9.83354628e-17], sum to 1.0000
[2019-04-04 07:04:10,051] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9321
[2019-04-04 07:04:10,057] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.55, 64.0, 80.0, 0.0, 26.0, 25.045771035041, 0.488817314077735, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1179000.0000, 
sim time next is 1179600.0000, 
raw observation next is [18.63333333333333, 63.66666666666667, 71.5, 0.0, 26.0, 25.03153388163404, 0.4916902128388969, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.9787626962142197, 0.6366666666666667, 0.23833333333333334, 0.0, 0.6666666666666666, 0.5859611568028367, 0.6638967376129656, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3650367], dtype=float32), -0.7123385]. 
=============================================
[2019-04-04 07:04:10,224] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0923874e-08 5.3070114e-08 2.9228733e-22 2.9336094e-21 7.0967638e-18
 9.9999988e-01 9.5436314e-16], sum to 1.0000
[2019-04-04 07:04:10,224] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4322
[2019-04-04 07:04:10,228] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.3, 64.0, 0.0, 0.0, 26.0, 24.87256115878378, 0.4418427866794986, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1186200.0000, 
sim time next is 1186800.0000, 
raw observation next is [18.3, 63.66666666666667, 0.0, 0.0, 26.0, 24.85237249198265, 0.4380180393866784, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5710310409985541, 0.6460060131288928, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23011792], dtype=float32), 0.5006034]. 
=============================================
[2019-04-04 07:04:10,286] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.5580839e-12 7.3610540e-10 9.1205775e-30 1.2955299e-28 2.1136588e-23
 1.0000000e+00 5.4038914e-21], sum to 1.0000
[2019-04-04 07:04:10,286] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0467
[2019-04-04 07:04:10,298] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 79.66666666666667, 0.0, 0.0, 26.0, 25.49085621194359, 0.4781583396824387, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1020000.0000, 
sim time next is 1020600.0000, 
raw observation next is [14.4, 79.0, 0.0, 0.0, 26.0, 25.33872862070843, 0.4565958604569877, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6115607183923691, 0.6521986201523292, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.85252357], dtype=float32), 1.0047429]. 
=============================================
[2019-04-04 07:04:10,448] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87000, global step 1384474: loss 1.4441
[2019-04-04 07:04:10,453] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87000, global step 1384477: learning rate 0.0000
[2019-04-04 07:04:11,309] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87000, global step 1384896: loss 1.4232
[2019-04-04 07:04:11,309] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87000, global step 1384896: learning rate 0.0000
[2019-04-04 07:04:14,077] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86500, global step 1386348: loss 0.4887
[2019-04-04 07:04:14,079] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86500, global step 1386348: learning rate 0.0000
[2019-04-04 07:04:14,399] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86500, global step 1386547: loss 0.4667
[2019-04-04 07:04:14,404] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86500, global step 1386549: learning rate 0.0000
[2019-04-04 07:04:14,703] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87000, global step 1386727: loss 1.3704
[2019-04-04 07:04:14,704] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87000, global step 1386727: learning rate 0.0000
[2019-04-04 07:04:15,878] A3C_AGENT_WORKER-Thread-6 INFO:Local step 87000, global step 1387435: loss 1.3973
[2019-04-04 07:04:15,879] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 87000, global step 1387435: learning rate 0.0000
[2019-04-04 07:04:16,361] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86500, global step 1387579: loss 0.4654
[2019-04-04 07:04:16,375] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86500, global step 1387582: learning rate 0.0000
[2019-04-04 07:04:16,599] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86500, global step 1387647: loss 0.5331
[2019-04-04 07:04:16,601] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86500, global step 1387648: learning rate 0.0000
[2019-04-04 07:04:16,770] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86500, global step 1387702: loss 0.4927
[2019-04-04 07:04:16,774] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86500, global step 1387702: learning rate 0.0000
[2019-04-04 07:04:16,857] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86500, global step 1387729: loss 0.5333
[2019-04-04 07:04:16,859] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86500, global step 1387729: learning rate 0.0000
[2019-04-04 07:04:17,110] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86500, global step 1387805: loss 0.4944
[2019-04-04 07:04:17,138] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86500, global step 1387805: learning rate 0.0000
[2019-04-04 07:04:18,337] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86500, global step 1388138: loss 0.5325
[2019-04-04 07:04:18,337] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86500, global step 1388138: learning rate 0.0000
[2019-04-04 07:04:18,886] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86500, global step 1388322: loss 0.5279
[2019-04-04 07:04:18,887] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86500, global step 1388322: learning rate 0.0000
[2019-04-04 07:04:18,984] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.0674648e-11 2.4043509e-10 2.0203879e-29 6.3195260e-28 2.8261122e-23
 1.0000000e+00 7.6182335e-22], sum to 1.0000
[2019-04-04 07:04:18,985] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5653
[2019-04-04 07:04:19,065] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 24.91540706424952, 0.3673267689391202, 0.0, 1.0, 46993.7519266228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 939000.0000, 
sim time next is 939600.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 25.00862546807605, 0.3763025320974342, 0.0, 1.0, 42459.59631102762], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.584052122339671, 0.6254341773658114, 0.0, 1.0, 0.2021885538620363], 
reward next is 0.7978, 
noisyNet noise sample is [array([0.38646132], dtype=float32), 1.1611743]. 
=============================================
[2019-04-04 07:04:19,156] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87000, global step 1388390: loss 1.3918
[2019-04-04 07:04:19,159] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87000, global step 1388390: learning rate 0.0000
[2019-04-04 07:04:22,220] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87000, global step 1389403: loss 1.3422
[2019-04-04 07:04:22,220] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87000, global step 1389403: learning rate 0.0000
[2019-04-04 07:04:23,052] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87000, global step 1389675: loss 1.3855
[2019-04-04 07:04:23,054] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87000, global step 1389675: learning rate 0.0000
[2019-04-04 07:04:33,376] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87500, global step 1393227: loss 1.7109
[2019-04-04 07:04:33,377] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87500, global step 1393227: learning rate 0.0000
[2019-04-04 07:04:33,461] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87500, global step 1393261: loss 1.8684
[2019-04-04 07:04:33,465] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87500, global step 1393263: learning rate 0.0000
[2019-04-04 07:04:34,321] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87000, global step 1393600: loss 1.3389
[2019-04-04 07:04:34,323] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87000, global step 1393602: learning rate 0.0000
[2019-04-04 07:04:34,819] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7482636e-11 8.5183496e-09 6.0598086e-29 6.9602384e-27 7.9441635e-22
 1.0000000e+00 5.3200101e-21], sum to 1.0000
[2019-04-04 07:04:34,821] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5322
[2019-04-04 07:04:34,865] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 22.66666666666666, 0.0, 26.0, 25.73793938760602, 0.4959952417330107, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1413600.0000, 
sim time next is 1414200.0000, 
raw observation next is [-0.6, 100.0, 27.33333333333333, 0.0, 26.0, 25.82972120261028, 0.5077200752036649, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.0911111111111111, 0.0, 0.6666666666666666, 0.65247676688419, 0.6692400250678884, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10598644], dtype=float32), 0.294505]. 
=============================================
[2019-04-04 07:04:35,972] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87000, global step 1394172: loss 1.4531
[2019-04-04 07:04:35,973] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87000, global step 1394172: learning rate 0.0000
[2019-04-04 07:04:38,208] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87000, global step 1395136: loss 1.4059
[2019-04-04 07:04:38,209] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87000, global step 1395136: learning rate 0.0000
[2019-04-04 07:04:38,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5168895e-12 5.7635369e-10 4.5749116e-31 1.9704562e-29 4.2391807e-23
 1.0000000e+00 5.3217705e-23], sum to 1.0000
[2019-04-04 07:04:38,255] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1218
[2019-04-04 07:04:38,263] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.53333333333333, 59.33333333333334, 76.66666666666666, 669.3333333333333, 26.0, 26.80299622464637, 0.7572537051599949, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1520400.0000, 
sim time next is 1521000.0000, 
raw observation next is [10.8, 57.5, 75.0, 663.0, 26.0, 26.86852677822456, 0.7757909295556048, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7617728531855957, 0.575, 0.25, 0.7325966850828729, 0.6666666666666666, 0.73904389818538, 0.7585969765185349, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2743876], dtype=float32), 0.12132903]. 
=============================================
[2019-04-04 07:04:38,275] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[95.2143 ]
 [95.69576]
 [96.22071]
 [96.7835 ]
 [97.50068]], R is [[94.84117889]
 [94.89276886]
 [94.94384003]
 [94.99440002]
 [95.04445648]].
[2019-04-04 07:04:38,408] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87000, global step 1395213: loss 1.3916
[2019-04-04 07:04:38,408] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87000, global step 1395213: learning rate 0.0000
[2019-04-04 07:04:38,541] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87500, global step 1395261: loss 1.8132
[2019-04-04 07:04:38,546] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87500, global step 1395261: learning rate 0.0000
[2019-04-04 07:04:38,567] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87000, global step 1395274: loss 1.3550
[2019-04-04 07:04:38,569] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87000, global step 1395274: learning rate 0.0000
[2019-04-04 07:04:38,834] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87000, global step 1395368: loss 1.3706
[2019-04-04 07:04:38,845] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87000, global step 1395368: learning rate 0.0000
[2019-04-04 07:04:39,178] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87000, global step 1395492: loss 1.3989
[2019-04-04 07:04:39,192] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87000, global step 1395492: learning rate 0.0000
[2019-04-04 07:04:40,077] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87000, global step 1395830: loss 1.3925
[2019-04-04 07:04:40,077] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87000, global step 1395830: learning rate 0.0000
[2019-04-04 07:04:40,711] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87000, global step 1396059: loss 1.3746
[2019-04-04 07:04:40,712] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87000, global step 1396059: learning rate 0.0000
[2019-04-04 07:04:42,152] A3C_AGENT_WORKER-Thread-6 INFO:Local step 87500, global step 1396513: loss 1.8249
[2019-04-04 07:04:42,153] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 87500, global step 1396513: learning rate 0.0000
[2019-04-04 07:04:44,431] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87500, global step 1397453: loss 1.6283
[2019-04-04 07:04:44,433] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87500, global step 1397454: learning rate 0.0000
[2019-04-04 07:04:46,366] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87500, global step 1398120: loss 1.6029
[2019-04-04 07:04:46,366] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87500, global step 1398120: learning rate 0.0000
[2019-04-04 07:04:47,612] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87500, global step 1398546: loss 1.5697
[2019-04-04 07:04:47,615] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87500, global step 1398547: learning rate 0.0000
[2019-04-04 07:04:51,973] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-04 07:04:51,981] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:04:51,982] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:04:51,984] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:04:51,984] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:04:51,998] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:04:51,998] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:04:52,000] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run15
[2019-04-04 07:04:52,024] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run15
[2019-04-04 07:04:52,043] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run15
[2019-04-04 07:05:24,775] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2457427], dtype=float32), 0.2382371]
[2019-04-04 07:05:24,775] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-12.7, 71.0, 73.0, 181.0, 26.0, 23.78022359320959, -0.03238971027099679, 0.0, 1.0, 88432.59186672996]
[2019-04-04 07:05:24,775] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 07:05:24,776] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.3885971e-09 1.8286127e-07 7.9185309e-23 1.9861911e-21 2.4400999e-17
 9.9999976e-01 9.1652446e-17], sampled 0.6710065707759095
[2019-04-04 07:07:03,019] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.2457427], dtype=float32), 0.2382371]
[2019-04-04 07:07:03,019] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.75, 50.5, 0.0, 0.0, 26.0, 24.28417937984926, 0.07833116900614592, 0.0, 1.0, 43461.36811603409]
[2019-04-04 07:07:03,019] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 07:07:03,020] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.8279338e-08 2.9732249e-07 4.0400124e-22 8.5897230e-21 1.5038245e-16
 9.9999964e-01 3.5225088e-16], sampled 0.9565720566851685
[2019-04-04 07:07:36,129] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.2457427], dtype=float32), 0.2382371]
[2019-04-04 07:07:36,129] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.693375531166667, 89.89518256333334, 0.0, 0.0, 26.0, 25.47124492789229, 0.5289797573248807, 0.0, 1.0, 31348.18712238892]
[2019-04-04 07:07:36,129] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 07:07:36,130] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.7091118e-10 5.0433848e-09 1.4221945e-27 1.0686350e-25 8.7670329e-21
 1.0000000e+00 5.5717535e-20], sampled 0.6327442463737937
[2019-04-04 07:08:06,094] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 07:08:39,010] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 07:08:44,193] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 07:08:45,230] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1400000, evaluation results [1400000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 07:08:50,859] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.8787085e-12 5.3923899e-10 1.4499887e-29 1.6129096e-28 3.6484981e-22
 1.0000000e+00 2.2591399e-21], sum to 1.0000
[2019-04-04 07:08:50,859] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3705
[2019-04-04 07:08:50,894] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.6, 100.0, 32.5, 0.0, 26.0, 25.82561488811113, 0.4933226096205901, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1501200.0000, 
sim time next is 1501800.0000, 
raw observation next is [1.7, 100.0, 37.33333333333334, 0.0, 26.0, 25.85638805619172, 0.4925092305785901, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5096952908587258, 1.0, 0.12444444444444448, 0.0, 0.6666666666666666, 0.6546990046826432, 0.6641697435261967, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10300634], dtype=float32), -0.61668724]. 
=============================================
[2019-04-04 07:08:51,182] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1361013e-10 8.0079747e-09 2.1902866e-27 4.0392065e-25 1.9971025e-20
 1.0000000e+00 1.5010831e-19], sum to 1.0000
[2019-04-04 07:08:51,182] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8713
[2019-04-04 07:08:51,291] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 83.0, 45.5, 0.0, 26.0, 25.18858768596008, 0.3570919612026413, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1760400.0000, 
sim time next is 1761000.0000, 
raw observation next is [-1.8, 83.66666666666667, 52.0, 0.0, 26.0, 25.12096787083316, 0.3442662861879655, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.41274238227146814, 0.8366666666666667, 0.17333333333333334, 0.0, 0.6666666666666666, 0.5934139892360966, 0.6147554287293219, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28656375], dtype=float32), 0.42311904]. 
=============================================
[2019-04-04 07:08:51,299] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[84.307335]
 [84.37235 ]
 [84.46972 ]
 [84.58878 ]
 [84.609245]], R is [[84.35664368]
 [84.51307678]
 [84.66794586]
 [84.82126617]
 [84.97305298]].
[2019-04-04 07:08:52,283] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88000, global step 1401639: loss 1.6091
[2019-04-04 07:08:52,321] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88000, global step 1401639: learning rate 0.0000
[2019-04-04 07:08:52,936] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87500, global step 1401793: loss 1.4386
[2019-04-04 07:08:52,937] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87500, global step 1401793: learning rate 0.0000
[2019-04-04 07:08:53,841] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88000, global step 1402003: loss 1.6206
[2019-04-04 07:08:53,849] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88000, global step 1402003: learning rate 0.0000
[2019-04-04 07:08:55,389] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87500, global step 1402367: loss 1.3460
[2019-04-04 07:08:55,390] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87500, global step 1402367: learning rate 0.0000
[2019-04-04 07:08:57,461] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87500, global step 1402883: loss 1.4018
[2019-04-04 07:08:57,462] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87500, global step 1402883: learning rate 0.0000
[2019-04-04 07:08:58,783] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87500, global step 1403224: loss 1.3997
[2019-04-04 07:08:58,784] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87500, global step 1403224: learning rate 0.0000
[2019-04-04 07:08:58,791] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88000, global step 1403226: loss 1.6965
[2019-04-04 07:08:58,802] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88000, global step 1403226: learning rate 0.0000
[2019-04-04 07:08:59,034] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87500, global step 1403293: loss 1.4246
[2019-04-04 07:08:59,034] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87500, global step 1403293: learning rate 0.0000
[2019-04-04 07:08:59,668] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87500, global step 1403455: loss 1.3488
[2019-04-04 07:08:59,683] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87500, global step 1403455: learning rate 0.0000
[2019-04-04 07:08:59,911] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87500, global step 1403528: loss 1.3619
[2019-04-04 07:08:59,914] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87500, global step 1403529: learning rate 0.0000
[2019-04-04 07:09:01,173] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87500, global step 1403882: loss 1.3226
[2019-04-04 07:09:01,192] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87500, global step 1403885: learning rate 0.0000
[2019-04-04 07:09:01,506] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87500, global step 1403978: loss 1.3516
[2019-04-04 07:09:01,507] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87500, global step 1403980: learning rate 0.0000
[2019-04-04 07:09:01,753] A3C_AGENT_WORKER-Thread-6 INFO:Local step 88000, global step 1404055: loss 1.7183
[2019-04-04 07:09:01,754] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 88000, global step 1404055: learning rate 0.0000
[2019-04-04 07:09:04,690] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.1618937e-11 8.0591676e-09 1.0769777e-28 1.1529112e-26 1.5394305e-21
 1.0000000e+00 9.1285205e-21], sum to 1.0000
[2019-04-04 07:09:04,690] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3397
[2019-04-04 07:09:04,736] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.333333333333334, 80.0, 0.0, 0.0, 26.0, 25.50746662137492, 0.4861641453599388, 0.0, 1.0, 69794.36335325766], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1579200.0000, 
sim time next is 1579800.0000, 
raw observation next is [5.416666666666667, 79.5, 0.0, 0.0, 26.0, 25.46960393664853, 0.4883101750919177, 0.0, 1.0, 66943.86124637657], 
processed observation next is [1.0, 0.2608695652173913, 0.6126500461680519, 0.795, 0.0, 0.0, 0.6666666666666666, 0.6224669947207108, 0.6627700583639725, 0.0, 1.0, 0.31878029164941224], 
reward next is 0.6812, 
noisyNet noise sample is [array([1.5699967], dtype=float32), -0.41323993]. 
=============================================
[2019-04-04 07:09:06,831] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88000, global step 1405251: loss 1.6756
[2019-04-04 07:09:06,853] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88000, global step 1405252: learning rate 0.0000
[2019-04-04 07:09:09,084] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9825541e-10 4.1139128e-09 2.2610125e-27 2.5968288e-25 5.2829404e-21
 1.0000000e+00 1.6076098e-19], sum to 1.0000
[2019-04-04 07:09:09,084] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4485
[2019-04-04 07:09:09,090] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88000, global step 1406214: loss 1.6879
[2019-04-04 07:09:09,093] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88000, global step 1406215: learning rate 0.0000
[2019-04-04 07:09:09,094] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.4, 66.0, 0.0, 0.0, 26.0, 26.18656358282908, 0.6745704674234201, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1623600.0000, 
sim time next is 1624200.0000, 
raw observation next is [9.116666666666667, 67.33333333333333, 0.0, 0.0, 26.0, 26.07684633698063, 0.6564377842249214, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7151431209602955, 0.6733333333333333, 0.0, 0.0, 0.6666666666666666, 0.6730705280817192, 0.7188125947416405, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05075001], dtype=float32), -1.206374]. 
=============================================
[2019-04-04 07:09:09,160] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88000, global step 1406231: loss 1.7139
[2019-04-04 07:09:09,167] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88000, global step 1406231: learning rate 0.0000
[2019-04-04 07:09:19,903] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88000, global step 1409554: loss 1.7299
[2019-04-04 07:09:19,904] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88000, global step 1409554: learning rate 0.0000
[2019-04-04 07:09:20,406] A3C_AGENT_WORKER-Thread-11 INFO:Local step 88000, global step 1409687: loss 1.7417
[2019-04-04 07:09:20,406] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 88000, global step 1409687: learning rate 0.0000
[2019-04-04 07:09:22,993] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88000, global step 1410364: loss 1.8179
[2019-04-04 07:09:23,002] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88000, global step 1410364: learning rate 0.0000
[2019-04-04 07:09:23,432] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88000, global step 1410509: loss 1.8233
[2019-04-04 07:09:23,443] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88000, global step 1410512: learning rate 0.0000
[2019-04-04 07:09:23,906] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88500, global step 1410675: loss 0.6823
[2019-04-04 07:09:23,909] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88500, global step 1410676: learning rate 0.0000
[2019-04-04 07:09:23,984] A3C_AGENT_WORKER-Thread-12 INFO:Local step 88000, global step 1410702: loss 1.8416
[2019-04-04 07:09:23,994] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 88000, global step 1410705: learning rate 0.0000
[2019-04-04 07:09:24,116] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0544786e-09 1.4970913e-08 6.9548847e-25 2.8702448e-23 4.5716075e-19
 1.0000000e+00 9.4980418e-18], sum to 1.0000
[2019-04-04 07:09:24,116] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0992
[2019-04-04 07:09:24,135] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 82.66666666666667, 0.0, 0.0, 26.0, 24.75033799253831, 0.2388588845515238, 0.0, 1.0, 45643.30429965832], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1810200.0000, 
sim time next is 1810800.0000, 
raw observation next is [-5.0, 82.0, 0.0, 0.0, 26.0, 24.71751718588985, 0.2319262878770381, 0.0, 1.0, 45585.15786711282], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5597930988241542, 0.5773087626256793, 0.0, 1.0, 0.21707218031958486], 
reward next is 0.7829, 
noisyNet noise sample is [array([-1.6878368], dtype=float32), -0.0030128646]. 
=============================================
[2019-04-04 07:09:24,332] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88000, global step 1410835: loss 1.8258
[2019-04-04 07:09:24,345] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88000, global step 1410835: learning rate 0.0000
[2019-04-04 07:09:24,393] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88000, global step 1410853: loss 1.8435
[2019-04-04 07:09:24,396] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88000, global step 1410854: learning rate 0.0000
[2019-04-04 07:09:24,413] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.2755793e-10 3.5032546e-08 1.9963358e-25 3.7766959e-23 1.4705518e-18
 1.0000000e+00 4.4437363e-18], sum to 1.0000
[2019-04-04 07:09:24,414] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5525
[2019-04-04 07:09:24,480] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 161.6666666666667, 33.33333333333334, 26.0, 24.98777258073243, 0.2702160698139288, 0.0, 1.0, 36466.86359836875], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1862400.0000, 
sim time next is 1863000.0000, 
raw observation next is [-4.5, 71.0, 170.0, 40.0, 26.0, 24.9948074162569, 0.2724042589806819, 0.0, 1.0, 37760.37446950815], 
processed observation next is [0.0, 0.5652173913043478, 0.3379501385041552, 0.71, 0.5666666666666667, 0.04419889502762431, 0.6666666666666666, 0.5829006180214084, 0.5908014196602273, 0.0, 1.0, 0.17981130699765785], 
reward next is 0.8202, 
noisyNet noise sample is [array([1.0440607], dtype=float32), 0.5689726]. 
=============================================
[2019-04-04 07:09:24,490] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.33706]
 [76.34476]
 [76.43226]
 [76.52042]
 [76.64447]], R is [[76.41494751]
 [76.47714233]
 [76.53582001]
 [76.57719421]
 [76.58069611]].
[2019-04-04 07:09:24,769] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88500, global step 1410987: loss 0.6596
[2019-04-04 07:09:24,769] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88500, global step 1410987: learning rate 0.0000
[2019-04-04 07:09:24,942] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88000, global step 1411042: loss 1.8616
[2019-04-04 07:09:24,957] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88000, global step 1411042: learning rate 0.0000
[2019-04-04 07:09:24,998] A3C_AGENT_WORKER-Thread-13 INFO:Local step 88000, global step 1411058: loss 1.8002
[2019-04-04 07:09:24,999] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 88000, global step 1411058: learning rate 0.0000
[2019-04-04 07:09:28,276] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88500, global step 1411860: loss 0.6970
[2019-04-04 07:09:28,278] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88500, global step 1411860: learning rate 0.0000
[2019-04-04 07:09:31,965] A3C_AGENT_WORKER-Thread-6 INFO:Local step 88500, global step 1412875: loss 0.6778
[2019-04-04 07:09:31,966] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 88500, global step 1412875: learning rate 0.0000
[2019-04-04 07:09:32,000] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1858029e-09 7.1398389e-08 1.9231430e-23 1.1952838e-21 1.0126669e-17
 9.9999988e-01 1.1269101e-16], sum to 1.0000
[2019-04-04 07:09:32,008] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0047
[2019-04-04 07:09:32,034] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 80.5, 0.0, 0.0, 26.0, 24.29085584415692, 0.08285143275370438, 0.0, 1.0, 44932.73832919998], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1899000.0000, 
sim time next is 1899600.0000, 
raw observation next is [-7.3, 81.0, 0.0, 0.0, 26.0, 24.25182171156786, 0.07490096067026299, 0.0, 1.0, 44953.50119253313], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.81, 0.0, 0.0, 0.6666666666666666, 0.520985142630655, 0.5249669868900877, 0.0, 1.0, 0.2140642913930149], 
reward next is 0.7859, 
noisyNet noise sample is [array([0.20171368], dtype=float32), -0.83711463]. 
=============================================
[2019-04-04 07:09:34,288] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88500, global step 1413617: loss 0.6437
[2019-04-04 07:09:34,290] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88500, global step 1413617: learning rate 0.0000
[2019-04-04 07:09:37,371] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88500, global step 1414455: loss 0.6547
[2019-04-04 07:09:37,373] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88500, global step 1414455: learning rate 0.0000
[2019-04-04 07:09:37,592] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88500, global step 1414511: loss 0.6612
[2019-04-04 07:09:37,593] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88500, global step 1414511: learning rate 0.0000
[2019-04-04 07:09:48,906] A3C_AGENT_WORKER-Thread-11 INFO:Local step 88500, global step 1417593: loss 0.6564
[2019-04-04 07:09:48,906] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 88500, global step 1417593: learning rate 0.0000
[2019-04-04 07:09:49,532] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9375904e-10 2.7722169e-08 3.1055321e-25 2.8006023e-23 1.5032835e-19
 1.0000000e+00 8.4466980e-19], sum to 1.0000
[2019-04-04 07:09:49,533] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2334
[2019-04-04 07:09:49,550] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 88.5, 0.0, 0.0, 26.0, 24.62489523821775, 0.2239866023834517, 0.0, 1.0, 42708.70646087905], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2079000.0000, 
sim time next is 2079600.0000, 
raw observation next is [-4.5, 87.66666666666666, 0.0, 0.0, 26.0, 24.6442237643293, 0.2168401080445682, 0.0, 1.0, 42711.76525830389], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.8766666666666666, 0.0, 0.0, 0.6666666666666666, 0.5536853136941083, 0.5722800360148561, 0.0, 1.0, 0.20338935837287564], 
reward next is 0.7966, 
noisyNet noise sample is [array([1.7762483], dtype=float32), 0.47076628]. 
=============================================
[2019-04-04 07:09:49,859] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88500, global step 1417939: loss 0.7233
[2019-04-04 07:09:49,860] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88500, global step 1417939: learning rate 0.0000
[2019-04-04 07:09:50,954] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88500, global step 1418282: loss 0.6425
[2019-04-04 07:09:50,955] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88500, global step 1418282: learning rate 0.0000
[2019-04-04 07:09:51,420] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89000, global step 1418382: loss 1.1961
[2019-04-04 07:09:51,442] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89000, global step 1418382: learning rate 0.0000
[2019-04-04 07:09:52,251] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88500, global step 1418611: loss 0.6326
[2019-04-04 07:09:52,251] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88500, global step 1418611: learning rate 0.0000
[2019-04-04 07:09:52,482] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89000, global step 1418682: loss 1.1555
[2019-04-04 07:09:52,484] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89000, global step 1418682: learning rate 0.0000
[2019-04-04 07:09:52,848] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88500, global step 1418785: loss 0.6771
[2019-04-04 07:09:52,849] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88500, global step 1418786: learning rate 0.0000
[2019-04-04 07:09:53,166] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.199105e-09 3.042705e-08 8.425722e-25 5.821679e-23 3.001910e-19
 1.000000e+00 4.335386e-18], sum to 1.0000
[2019-04-04 07:09:53,168] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8322
[2019-04-04 07:09:53,216] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.2, 77.0, 0.0, 0.0, 26.0, 25.49245182559808, 0.4043834773180189, 0.0, 1.0, 31743.99861523229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2143200.0000, 
sim time next is 2143800.0000, 
raw observation next is [-5.3, 78.5, 0.0, 0.0, 26.0, 25.38064546644452, 0.3814161330258899, 0.0, 1.0, 34459.72555863416], 
processed observation next is [1.0, 0.8260869565217391, 0.31578947368421056, 0.785, 0.0, 0.0, 0.6666666666666666, 0.6150537888703766, 0.62713871100863, 0.0, 1.0, 0.16409393123159124], 
reward next is 0.8359, 
noisyNet noise sample is [array([-2.0785756], dtype=float32), -2.7672007]. 
=============================================
[2019-04-04 07:09:53,300] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88500, global step 1418920: loss 0.6705
[2019-04-04 07:09:53,300] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88500, global step 1418920: learning rate 0.0000
[2019-04-04 07:09:53,592] A3C_AGENT_WORKER-Thread-12 INFO:Local step 88500, global step 1419006: loss 0.6614
[2019-04-04 07:09:53,593] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 88500, global step 1419006: learning rate 0.0000
[2019-04-04 07:09:53,850] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88500, global step 1419075: loss 0.6900
[2019-04-04 07:09:53,850] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88500, global step 1419075: learning rate 0.0000
[2019-04-04 07:09:53,982] A3C_AGENT_WORKER-Thread-13 INFO:Local step 88500, global step 1419117: loss 0.6699
[2019-04-04 07:09:53,984] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 88500, global step 1419119: learning rate 0.0000
[2019-04-04 07:09:56,188] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3151595e-08 1.4550688e-08 1.5629183e-25 9.4962382e-24 2.8352750e-19
 1.0000000e+00 1.1057832e-18], sum to 1.0000
[2019-04-04 07:09:56,189] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5420
[2019-04-04 07:09:56,235] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.13660494440588, 0.4101295452474757, 0.0, 1.0, 73821.4702849791], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2147400.0000, 
sim time next is 2148000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.1802260734843, 0.4149596043896009, 0.0, 1.0, 53245.41020231142], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5983521727903582, 0.638319868129867, 0.0, 1.0, 0.25354957239195913], 
reward next is 0.7465, 
noisyNet noise sample is [array([1.6929095], dtype=float32), 0.5182028]. 
=============================================
[2019-04-04 07:09:56,239] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[77.496315]
 [77.531456]
 [77.42279 ]
 [77.381386]
 [76.97826 ]], R is [[77.36618042]
 [77.24098969]
 [76.94555664]
 [76.83323669]
 [76.47704315]].
[2019-04-04 07:09:56,521] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89000, global step 1419814: loss 1.0993
[2019-04-04 07:09:56,522] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89000, global step 1419814: learning rate 0.0000
[2019-04-04 07:10:00,197] A3C_AGENT_WORKER-Thread-6 INFO:Local step 89000, global step 1420976: loss 1.0635
[2019-04-04 07:10:00,201] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 89000, global step 1420977: learning rate 0.0000
[2019-04-04 07:10:02,566] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89000, global step 1421653: loss 1.0407
[2019-04-04 07:10:02,567] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89000, global step 1421653: learning rate 0.0000
[2019-04-04 07:10:04,787] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89000, global step 1422345: loss 1.0714
[2019-04-04 07:10:04,796] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89000, global step 1422349: learning rate 0.0000
[2019-04-04 07:10:04,969] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89000, global step 1422421: loss 1.0562
[2019-04-04 07:10:04,984] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89000, global step 1422422: learning rate 0.0000
[2019-04-04 07:10:09,441] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1824506e-08 6.0135861e-08 1.3646767e-21 4.5083751e-20 4.3335709e-16
 9.9999988e-01 1.6947808e-15], sum to 1.0000
[2019-04-04 07:10:09,443] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4075
[2019-04-04 07:10:09,502] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.9166666666666667, 28.33333333333334, 0.0, 0.0, 26.0, 24.92738762874274, 0.2196868400637076, 0.0, 1.0, 18719.20271423861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2484600.0000, 
sim time next is 2485200.0000, 
raw observation next is [0.7333333333333335, 28.66666666666667, 0.0, 0.0, 26.0, 24.92754968502823, 0.2144898493446322, 0.0, 1.0, 28515.69409870524], 
processed observation next is [0.0, 0.782608695652174, 0.4829178208679595, 0.28666666666666674, 0.0, 0.0, 0.6666666666666666, 0.5772958070856857, 0.5714966164482107, 0.0, 1.0, 0.135789019517644], 
reward next is 0.8642, 
noisyNet noise sample is [array([-0.04046245], dtype=float32), 1.7953079]. 
=============================================
[2019-04-04 07:10:11,853] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.6850492e-09 2.9076643e-08 7.4853678e-24 5.7047025e-23 1.4650925e-18
 1.0000000e+00 5.4868003e-18], sum to 1.0000
[2019-04-04 07:10:11,856] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4638
[2019-04-04 07:10:11,927] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 54.66666666666667, 0.0, 0.0, 26.0, 25.45055753928966, 0.4287674455916695, 0.0, 1.0, 18759.99925482402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2323200.0000, 
sim time next is 2323800.0000, 
raw observation next is [-1.7, 55.0, 0.0, 0.0, 26.0, 25.43301926057366, 0.4261490673601802, 0.0, 1.0, 32336.74252528141], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6194182717144715, 0.6420496891200601, 0.0, 1.0, 0.15398448821562577], 
reward next is 0.8460, 
noisyNet noise sample is [array([-0.7496416], dtype=float32), -0.16075398]. 
=============================================
[2019-04-04 07:10:15,009] A3C_AGENT_WORKER-Thread-11 INFO:Local step 89000, global step 1425842: loss 0.9414
[2019-04-04 07:10:15,014] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 89000, global step 1425842: learning rate 0.0000
[2019-04-04 07:10:15,526] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89500, global step 1426008: loss 0.1545
[2019-04-04 07:10:15,526] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89500, global step 1426008: learning rate 0.0000
[2019-04-04 07:10:15,651] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89500, global step 1426049: loss 0.1640
[2019-04-04 07:10:15,651] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89500, global step 1426049: learning rate 0.0000
[2019-04-04 07:10:16,125] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89000, global step 1426197: loss 0.9443
[2019-04-04 07:10:16,129] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89000, global step 1426197: learning rate 0.0000
[2019-04-04 07:10:17,478] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89000, global step 1426640: loss 0.9249
[2019-04-04 07:10:17,479] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89000, global step 1426640: learning rate 0.0000
[2019-04-04 07:10:17,868] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0193179e-09 7.2190424e-08 1.3544730e-24 2.2833532e-23 3.5274256e-19
 9.9999988e-01 3.5120273e-18], sum to 1.0000
[2019-04-04 07:10:17,868] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3101
[2019-04-04 07:10:17,903] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.9270750740183, 0.23349944212263, 0.0, 1.0, 41630.43321408247], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2600400.0000, 
sim time next is 2601000.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.88257074650033, 0.2227773525614618, 0.0, 1.0, 41674.07414499095], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5735475622083609, 0.5742591175204873, 0.0, 1.0, 0.1984479721190045], 
reward next is 0.8016, 
noisyNet noise sample is [array([0.18687464], dtype=float32), -2.4018848]. 
=============================================
[2019-04-04 07:10:17,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.39531]
 [77.0807 ]
 [76.48489]
 [76.22812]
 [75.90547]], R is [[78.07003021]
 [78.09108734]
 [78.11210632]
 [78.13288879]
 [78.15325165]].
[2019-04-04 07:10:18,887] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89000, global step 1427088: loss 0.9140
[2019-04-04 07:10:18,888] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89000, global step 1427088: learning rate 0.0000
[2019-04-04 07:10:19,544] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89000, global step 1427366: loss 0.9328
[2019-04-04 07:10:19,549] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89000, global step 1427366: learning rate 0.0000
[2019-04-04 07:10:19,622] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89500, global step 1427396: loss 0.1584
[2019-04-04 07:10:19,623] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89500, global step 1427396: learning rate 0.0000
[2019-04-04 07:10:19,632] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89000, global step 1427396: loss 0.9516
[2019-04-04 07:10:19,633] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89000, global step 1427397: learning rate 0.0000
[2019-04-04 07:10:19,649] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89000, global step 1427408: loss 0.9292
[2019-04-04 07:10:19,650] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89000, global step 1427408: learning rate 0.0000
[2019-04-04 07:10:19,771] A3C_AGENT_WORKER-Thread-13 INFO:Local step 89000, global step 1427455: loss 0.9430
[2019-04-04 07:10:19,774] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 89000, global step 1427455: learning rate 0.0000
[2019-04-04 07:10:19,805] A3C_AGENT_WORKER-Thread-12 INFO:Local step 89000, global step 1427471: loss 0.9308
[2019-04-04 07:10:19,807] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 89000, global step 1427471: learning rate 0.0000
[2019-04-04 07:10:21,072] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.70397840e-10 4.58665328e-09 1.00451505e-23 6.03910383e-23
 6.62761354e-19 1.00000000e+00 1.02354099e-18], sum to 1.0000
[2019-04-04 07:10:21,072] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8086
[2019-04-04 07:10:21,139] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.833333333333333, 53.33333333333333, 0.0, 0.0, 26.0, 25.29737828327772, 0.4364248489401548, 1.0, 1.0, 53141.48801385392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2742600.0000, 
sim time next is 2743200.0000, 
raw observation next is [-4.0, 54.0, 0.0, 0.0, 26.0, 25.38912495428901, 0.470112617311387, 1.0, 1.0, 47680.30247422087], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.54, 0.0, 0.0, 0.6666666666666666, 0.6157604128574174, 0.6567042057704623, 1.0, 1.0, 0.22704905940105177], 
reward next is 0.7730, 
noisyNet noise sample is [array([-2.308724], dtype=float32), 0.2009751]. 
=============================================
[2019-04-04 07:10:22,733] A3C_AGENT_WORKER-Thread-6 INFO:Local step 89500, global step 1428413: loss 0.1633
[2019-04-04 07:10:22,736] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 89500, global step 1428413: learning rate 0.0000
[2019-04-04 07:10:25,437] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89500, global step 1429410: loss 0.1604
[2019-04-04 07:10:25,439] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89500, global step 1429410: learning rate 0.0000
[2019-04-04 07:10:27,296] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89500, global step 1430185: loss 0.1843
[2019-04-04 07:10:27,296] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89500, global step 1430185: learning rate 0.0000
[2019-04-04 07:10:28,296] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89500, global step 1430503: loss 0.1920
[2019-04-04 07:10:28,298] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89500, global step 1430504: learning rate 0.0000
[2019-04-04 07:10:35,541] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.1874472e-11 2.0579090e-09 5.1424438e-28 3.9499886e-26 6.8369164e-22
 1.0000000e+00 5.0027799e-21], sum to 1.0000
[2019-04-04 07:10:35,542] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2404
[2019-04-04 07:10:35,594] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.666666666666667, 33.33333333333334, 237.1666666666667, 258.3333333333333, 26.0, 25.86512982116034, 0.4808116166700439, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2812800.0000, 
sim time next is 2813400.0000, 
raw observation next is [5.0, 32.5, 249.0, 173.0, 26.0, 25.97315907993863, 0.4756492130876544, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6011080332409973, 0.325, 0.83, 0.19116022099447513, 0.6666666666666666, 0.664429923328219, 0.6585497376958848, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.119181], dtype=float32), -0.20253822]. 
=============================================
[2019-04-04 07:10:37,387] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.1945895e-10 7.7394056e-09 9.0545612e-25 2.2231205e-23 2.3078349e-18
 1.0000000e+00 1.4571555e-18], sum to 1.0000
[2019-04-04 07:10:37,391] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5956
[2019-04-04 07:10:37,412] A3C_AGENT_WORKER-Thread-11 INFO:Local step 89500, global step 1433762: loss 0.1820
[2019-04-04 07:10:37,413] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 89500, global step 1433762: learning rate 0.0000
[2019-04-04 07:10:37,422] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 63.33333333333333, 0.0, 0.0, 26.0, 25.16035634172581, 0.371897785112963, 0.0, 1.0, 63048.92552461978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3008400.0000, 
sim time next is 3009000.0000, 
raw observation next is [-2.833333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 25.31563443838453, 0.3785604459087589, 0.0, 1.0, 47608.1251173269], 
processed observation next is [0.0, 0.8260869565217391, 0.3841181902123731, 0.6416666666666667, 0.0, 0.0, 0.6666666666666666, 0.6096362031987109, 0.6261868153029196, 0.0, 1.0, 0.22670535770155664], 
reward next is 0.7733, 
noisyNet noise sample is [array([-1.154831], dtype=float32), 1.2200894]. 
=============================================
[2019-04-04 07:10:37,428] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.91238]
 [75.38763]
 [74.66375]
 [73.89771]
 [73.63649]], R is [[76.34529114]
 [76.28160858]
 [76.00912476]
 [75.42179108]
 [75.49346924]].
[2019-04-04 07:10:38,262] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90000, global step 1434107: loss 0.3157
[2019-04-04 07:10:38,262] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90000, global step 1434107: learning rate 0.0000
[2019-04-04 07:10:38,355] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90000, global step 1434148: loss 0.3117
[2019-04-04 07:10:38,362] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90000, global step 1434148: learning rate 0.0000
[2019-04-04 07:10:38,825] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89500, global step 1434350: loss 0.1737
[2019-04-04 07:10:38,828] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89500, global step 1434352: learning rate 0.0000
[2019-04-04 07:10:39,752] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89500, global step 1434698: loss 0.1609
[2019-04-04 07:10:39,754] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89500, global step 1434698: learning rate 0.0000
[2019-04-04 07:10:41,158] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89500, global step 1435030: loss 0.1651
[2019-04-04 07:10:41,158] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89500, global step 1435030: learning rate 0.0000
[2019-04-04 07:10:41,506] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89500, global step 1435129: loss 0.1548
[2019-04-04 07:10:41,509] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89500, global step 1435129: learning rate 0.0000
[2019-04-04 07:10:41,771] A3C_AGENT_WORKER-Thread-13 INFO:Local step 89500, global step 1435218: loss 0.1789
[2019-04-04 07:10:41,775] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 89500, global step 1435218: learning rate 0.0000
[2019-04-04 07:10:41,790] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89500, global step 1435225: loss 0.1860
[2019-04-04 07:10:41,800] A3C_AGENT_WORKER-Thread-12 INFO:Local step 89500, global step 1435229: loss 0.1884
[2019-04-04 07:10:41,803] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89500, global step 1435229: learning rate 0.0000
[2019-04-04 07:10:41,806] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 89500, global step 1435231: learning rate 0.0000
[2019-04-04 07:10:42,237] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89500, global step 1435367: loss 0.1554
[2019-04-04 07:10:42,238] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89500, global step 1435367: learning rate 0.0000
[2019-04-04 07:10:42,380] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90000, global step 1435413: loss 0.2891
[2019-04-04 07:10:42,380] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90000, global step 1435413: learning rate 0.0000
[2019-04-04 07:10:44,538] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0890279e-09 2.9470666e-08 1.1998916e-24 1.0846166e-22 3.2564299e-19
 1.0000000e+00 3.7484446e-18], sum to 1.0000
[2019-04-04 07:10:44,538] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4264
[2019-04-04 07:10:44,561] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.0438665949762, 0.3294276137154326, 0.0, 1.0, 45928.40450774256], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2760600.0000, 
sim time next is 2761200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.99014556331918, 0.3182289866161088, 0.0, 1.0, 45608.10403691819], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5825121302765984, 0.6060763288720362, 0.0, 1.0, 0.21718144779484852], 
reward next is 0.7828, 
noisyNet noise sample is [array([-0.9082668], dtype=float32), 0.9670363]. 
=============================================
[2019-04-04 07:10:45,430] A3C_AGENT_WORKER-Thread-6 INFO:Local step 90000, global step 1436479: loss 0.2756
[2019-04-04 07:10:45,430] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 90000, global step 1436479: learning rate 0.0000
[2019-04-04 07:10:47,065] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9577008e-10 3.6105166e-09 1.8332890e-26 8.1601059e-25 2.1715837e-20
 1.0000000e+00 1.5920476e-19], sum to 1.0000
[2019-04-04 07:10:47,065] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1889
[2019-04-04 07:10:47,124] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 119.0, 90.5, 26.0, 25.22410845245643, 0.3572380776155487, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2970000.0000, 
sim time next is 2970600.0000, 
raw observation next is [-4.0, 71.0, 130.6666666666667, 104.3333333333333, 26.0, 25.29266064542553, 0.3539504858172739, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.4355555555555557, 0.11528545119705337, 0.6666666666666666, 0.6077217204521276, 0.6179834952724247, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.72090477], dtype=float32), 1.7179794]. 
=============================================
[2019-04-04 07:10:47,922] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90000, global step 1437372: loss 0.2707
[2019-04-04 07:10:47,923] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90000, global step 1437372: learning rate 0.0000
[2019-04-04 07:10:49,768] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90000, global step 1438157: loss 0.2691
[2019-04-04 07:10:49,771] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90000, global step 1438157: learning rate 0.0000
[2019-04-04 07:10:50,199] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.9387019e-11 1.3923373e-08 8.1352953e-27 5.7865580e-25 7.1579861e-20
 1.0000000e+00 1.9266790e-19], sum to 1.0000
[2019-04-04 07:10:50,199] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1309
[2019-04-04 07:10:50,286] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 56.5, 99.0, 635.0, 26.0, 25.33916211697487, 0.3257630532219768, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3058200.0000, 
sim time next is 3058800.0000, 
raw observation next is [-4.666666666666666, 55.66666666666667, 100.1666666666667, 655.6666666666667, 26.0, 25.31958303626982, 0.3236640304274999, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.33333333333333337, 0.5566666666666668, 0.333888888888889, 0.7244935543278086, 0.6666666666666666, 0.6099652530224849, 0.6078880101425, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47218484], dtype=float32), 0.48802388]. 
=============================================
[2019-04-04 07:10:50,974] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3744955e-11 1.2242856e-09 3.6755437e-29 1.4837360e-27 1.7977308e-22
 1.0000000e+00 1.2031797e-21], sum to 1.0000
[2019-04-04 07:10:50,976] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0481
[2019-04-04 07:10:50,993] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 100.0, 0.0, 0.0, 26.0, 25.29984027166688, 0.4869744186932121, 0.0, 1.0, 40613.86007456197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3217800.0000, 
sim time next is 3218400.0000, 
raw observation next is [-3.0, 100.0, 0.0, 0.0, 26.0, 25.30830175422471, 0.4896551301587411, 0.0, 1.0, 40654.48466676121], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6090251461853926, 0.6632183767195804, 0.0, 1.0, 0.19359278412743433], 
reward next is 0.8064, 
noisyNet noise sample is [array([-0.7121994], dtype=float32), 1.6464026]. 
=============================================
[2019-04-04 07:10:51,484] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90000, global step 1438807: loss 0.2518
[2019-04-04 07:10:51,485] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90000, global step 1438808: learning rate 0.0000
[2019-04-04 07:10:53,220] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.0440750e-12 1.5395690e-09 4.9460601e-29 5.3016191e-27 3.1362043e-22
 1.0000000e+00 2.4524322e-21], sum to 1.0000
[2019-04-04 07:10:53,220] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9773
[2019-04-04 07:10:53,249] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333333, 97.66666666666667, 73.16666666666666, 0.0, 26.0, 25.41790403260164, 0.30890702952767, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2888400.0000, 
sim time next is 2889000.0000, 
raw observation next is [0.5, 96.5, 78.0, 0.0, 26.0, 25.42633743352595, 0.3101531151847965, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.4764542936288089, 0.965, 0.26, 0.0, 0.6666666666666666, 0.6188614527938293, 0.6033843717282655, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.19035524], dtype=float32), 0.20668039]. 
=============================================
[2019-04-04 07:10:53,254] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[87.248184]
 [87.65131 ]
 [88.23766 ]
 [88.85371 ]
 [89.54147 ]], R is [[86.81442261]
 [86.85733032]
 [86.89980316]
 [86.94184875]
 [86.98347473]].
[2019-04-04 07:10:58,033] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90500, global step 1441370: loss 0.0280
[2019-04-04 07:10:58,036] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90500, global step 1441370: learning rate 0.0000
[2019-04-04 07:10:58,390] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90500, global step 1441539: loss 0.0304
[2019-04-04 07:10:58,392] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90500, global step 1441539: learning rate 0.0000
[2019-04-04 07:10:59,964] A3C_AGENT_WORKER-Thread-11 INFO:Local step 90000, global step 1442129: loss 0.2162
[2019-04-04 07:10:59,967] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 90000, global step 1442129: learning rate 0.0000
[2019-04-04 07:11:01,438] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90000, global step 1442624: loss 0.2178
[2019-04-04 07:11:01,439] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90000, global step 1442624: learning rate 0.0000
[2019-04-04 07:11:01,856] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90500, global step 1442785: loss 0.0295
[2019-04-04 07:11:01,857] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90500, global step 1442785: learning rate 0.0000
[2019-04-04 07:11:03,348] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90000, global step 1443313: loss 0.2110
[2019-04-04 07:11:03,351] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90000, global step 1443313: learning rate 0.0000
[2019-04-04 07:11:03,367] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90000, global step 1443316: loss 0.1699
[2019-04-04 07:11:03,367] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90000, global step 1443316: learning rate 0.0000
[2019-04-04 07:11:04,070] A3C_AGENT_WORKER-Thread-12 INFO:Local step 90000, global step 1443578: loss 0.1959
[2019-04-04 07:11:04,071] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 90000, global step 1443578: learning rate 0.0000
[2019-04-04 07:11:04,517] A3C_AGENT_WORKER-Thread-13 INFO:Local step 90000, global step 1443787: loss 0.2183
[2019-04-04 07:11:04,521] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 90000, global step 1443788: learning rate 0.0000
[2019-04-04 07:11:04,527] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90000, global step 1443792: loss 0.1840
[2019-04-04 07:11:04,527] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90000, global step 1443792: loss 0.2325
[2019-04-04 07:11:04,528] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90000, global step 1443793: learning rate 0.0000
[2019-04-04 07:11:04,529] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90000, global step 1443793: learning rate 0.0000
[2019-04-04 07:11:04,636] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90000, global step 1443845: loss 0.1899
[2019-04-04 07:11:04,637] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90000, global step 1443845: learning rate 0.0000
[2019-04-04 07:11:04,662] A3C_AGENT_WORKER-Thread-6 INFO:Local step 90500, global step 1443856: loss 0.0343
[2019-04-04 07:11:04,667] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 90500, global step 1443857: learning rate 0.0000
[2019-04-04 07:11:06,852] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90500, global step 1444683: loss 0.0188
[2019-04-04 07:11:06,853] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90500, global step 1444683: learning rate 0.0000
[2019-04-04 07:11:09,544] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90500, global step 1445748: loss 0.0355
[2019-04-04 07:11:09,544] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90500, global step 1445748: learning rate 0.0000
[2019-04-04 07:11:10,054] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5938587e-10 5.2769504e-09 2.4855500e-26 7.6238619e-25 3.8372883e-20
 1.0000000e+00 6.8175470e-20], sum to 1.0000
[2019-04-04 07:11:10,056] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9782
[2019-04-04 07:11:10,090] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.833333333333333, 50.0, 108.6666666666667, 768.0, 26.0, 25.09000921893377, 0.5248307315196606, 1.0, 1.0, 196283.6229633117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3334200.0000, 
sim time next is 3334800.0000, 
raw observation next is [-3.666666666666667, 50.0, 107.3333333333333, 760.0, 26.0, 25.63472722284086, 0.6048344637402776, 1.0, 1.0, 70333.60784504382], 
processed observation next is [1.0, 0.6086956521739131, 0.3610341643582641, 0.5, 0.3577777777777777, 0.8397790055248618, 0.6666666666666666, 0.6362272685700715, 0.7016114879134259, 1.0, 1.0, 0.3349219421192563], 
reward next is 0.6651, 
noisyNet noise sample is [array([0.85349673], dtype=float32), -1.2457331]. 
=============================================
[2019-04-04 07:11:11,131] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90500, global step 1446520: loss 0.0343
[2019-04-04 07:11:11,131] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90500, global step 1446520: learning rate 0.0000
[2019-04-04 07:11:11,480] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.4765562e-11 6.2911325e-09 5.1758001e-29 2.8900388e-26 8.9890834e-22
 1.0000000e+00 2.4989818e-20], sum to 1.0000
[2019-04-04 07:11:11,482] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7144
[2019-04-04 07:11:11,515] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.833333333333333, 100.0, 0.0, 0.0, 26.0, 25.46699330531672, 0.3248607694226918, 0.0, 1.0, 18756.97900409074], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3120600.0000, 
sim time next is 3121200.0000, 
raw observation next is [2.0, 100.0, 0.0, 0.0, 26.0, 25.48563922086829, 0.2997385861561997, 0.0, 1.0, 23191.11797153286], 
processed observation next is [1.0, 0.13043478260869565, 0.518005540166205, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6238032684056908, 0.5999128620520665, 0.0, 1.0, 0.11043389510253744], 
reward next is 0.8896, 
noisyNet noise sample is [array([-0.18219522], dtype=float32), -0.041908547]. 
=============================================
[2019-04-04 07:11:12,842] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4214308e-11 5.5270305e-10 2.1553826e-28 4.4773727e-27 2.9789521e-23
 1.0000000e+00 3.0628196e-21], sum to 1.0000
[2019-04-04 07:11:12,844] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8772
[2019-04-04 07:11:12,858] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.1666666666666667, 100.0, 0.0, 0.0, 26.0, 25.36092244578596, 0.5514699523095034, 0.0, 1.0, 98813.99295274407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3204600.0000, 
sim time next is 3205200.0000, 
raw observation next is [-0.3333333333333333, 100.0, 0.0, 0.0, 26.0, 25.3783904239165, 0.5653839629524948, 0.0, 1.0, 60892.10093284738], 
processed observation next is [1.0, 0.08695652173913043, 0.4533702677747, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6148658686597083, 0.6884613209841649, 0.0, 1.0, 0.28996238539451136], 
reward next is 0.7100, 
noisyNet noise sample is [array([-0.0942646], dtype=float32), -0.19351049]. 
=============================================
[2019-04-04 07:11:16,268] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91000, global step 1449001: loss 0.0943
[2019-04-04 07:11:16,268] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91000, global step 1449001: learning rate 0.0000
[2019-04-04 07:11:16,941] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91000, global step 1449274: loss 0.0950
[2019-04-04 07:11:16,947] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91000, global step 1449275: learning rate 0.0000
[2019-04-04 07:11:19,143] A3C_AGENT_WORKER-Thread-11 INFO:Local step 90500, global step 1450266: loss 0.0322
[2019-04-04 07:11:19,144] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 90500, global step 1450266: learning rate 0.0000
[2019-04-04 07:11:19,879] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91000, global step 1450608: loss 0.0843
[2019-04-04 07:11:19,884] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91000, global step 1450608: learning rate 0.0000
[2019-04-04 07:11:20,410] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90500, global step 1450872: loss 0.0321
[2019-04-04 07:11:20,410] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90500, global step 1450872: learning rate 0.0000
[2019-04-04 07:11:20,954] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8825312e-10 4.0178342e-08 5.0969058e-26 7.6296844e-24 1.3131494e-19
 1.0000000e+00 6.0077518e-19], sum to 1.0000
[2019-04-04 07:11:20,970] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2882
[2019-04-04 07:11:20,994] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.600000000000001, 77.0, 0.0, 0.0, 26.0, 24.73162528478885, 0.2865222363334881, 0.0, 1.0, 43904.13835053565], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3296400.0000, 
sim time next is 3297000.0000, 
raw observation next is [-8.75, 77.0, 0.0, 0.0, 26.0, 24.72026030279573, 0.2874264433833935, 0.0, 1.0, 43879.22731328561], 
processed observation next is [1.0, 0.13043478260869565, 0.22022160664819945, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5600216918996441, 0.5958088144611312, 0.0, 1.0, 0.20894870149183625], 
reward next is 0.7911, 
noisyNet noise sample is [array([1.2087677], dtype=float32), -1.6732814]. 
=============================================
[2019-04-04 07:11:21,014] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[77.93627 ]
 [78.10381 ]
 [78.28614 ]
 [78.47269 ]
 [78.589516]], R is [[77.81246948]
 [77.82527924]
 [77.83782196]
 [77.85018158]
 [77.86248016]].
[2019-04-04 07:11:21,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.7871953e-10 3.5056541e-08 4.5650237e-25 3.4995413e-23 8.0243700e-19
 1.0000000e+00 5.7668563e-18], sum to 1.0000
[2019-04-04 07:11:21,272] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5449
[2019-04-04 07:11:21,296] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.816666666666666, 76.16666666666667, 0.0, 0.0, 26.0, 24.48966109779854, 0.2246383629734358, 0.0, 1.0, 43761.94857231114], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3300600.0000, 
sim time next is 3301200.0000, 
raw observation next is [-10.0, 76.0, 0.0, 0.0, 26.0, 24.48140527495197, 0.2209205188449593, 0.0, 1.0, 43706.5326040172], 
processed observation next is [1.0, 0.21739130434782608, 0.18559556786703602, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5401171062459976, 0.5736401729483197, 0.0, 1.0, 0.20812634573341524], 
reward next is 0.7919, 
noisyNet noise sample is [array([2.9507487], dtype=float32), 0.7386592]. 
=============================================
[2019-04-04 07:11:21,973] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90500, global step 1451528: loss 0.0259
[2019-04-04 07:11:21,975] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90500, global step 1451528: learning rate 0.0000
[2019-04-04 07:11:22,020] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90500, global step 1451546: loss 0.0493
[2019-04-04 07:11:22,021] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90500, global step 1451546: learning rate 0.0000
[2019-04-04 07:11:22,202] A3C_AGENT_WORKER-Thread-6 INFO:Local step 91000, global step 1451625: loss 0.0846
[2019-04-04 07:11:22,204] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 91000, global step 1451625: learning rate 0.0000
[2019-04-04 07:11:22,445] A3C_AGENT_WORKER-Thread-12 INFO:Local step 90500, global step 1451715: loss 0.0289
[2019-04-04 07:11:22,446] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 90500, global step 1451715: learning rate 0.0000
[2019-04-04 07:11:22,648] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8044861e-09 9.6650448e-09 1.6600103e-24 3.1024328e-23 1.2010858e-18
 1.0000000e+00 3.9393807e-18], sum to 1.0000
[2019-04-04 07:11:22,648] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2100
[2019-04-04 07:11:22,669] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.59385864104139, 0.5330749416413166, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3359400.0000, 
sim time next is 3360000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.60614969089775, 0.5235719533134628, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6338458075748127, 0.6745239844378209, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.73061985], dtype=float32), 0.59566796]. 
=============================================
[2019-04-04 07:11:22,712] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[74.163994]
 [74.30623 ]
 [74.39635 ]
 [74.270874]
 [73.94914 ]], R is [[74.30071259]
 [74.55770874]
 [74.81213379]
 [74.79257965]
 [74.62815094]].
[2019-04-04 07:11:22,740] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90500, global step 1451834: loss 0.0448
[2019-04-04 07:11:22,741] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90500, global step 1451834: learning rate 0.0000
[2019-04-04 07:11:22,837] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90500, global step 1451883: loss 0.0284
[2019-04-04 07:11:22,838] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90500, global step 1451883: learning rate 0.0000
[2019-04-04 07:11:23,256] A3C_AGENT_WORKER-Thread-13 INFO:Local step 90500, global step 1452069: loss 0.0432
[2019-04-04 07:11:23,257] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 90500, global step 1452069: learning rate 0.0000
[2019-04-04 07:11:23,341] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90500, global step 1452114: loss 0.0308
[2019-04-04 07:11:23,342] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90500, global step 1452114: learning rate 0.0000
[2019-04-04 07:11:24,810] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91000, global step 1452818: loss 0.0926
[2019-04-04 07:11:24,817] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91000, global step 1452819: learning rate 0.0000
[2019-04-04 07:11:27,731] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91000, global step 1454105: loss 0.0813
[2019-04-04 07:11:27,733] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91000, global step 1454105: learning rate 0.0000
[2019-04-04 07:11:29,034] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91000, global step 1454724: loss 0.0681
[2019-04-04 07:11:29,040] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91000, global step 1454725: learning rate 0.0000
[2019-04-04 07:11:33,237] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91500, global step 1456785: loss 0.1959
[2019-04-04 07:11:33,239] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91500, global step 1456785: learning rate 0.0000
[2019-04-04 07:11:33,472] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91500, global step 1456892: loss 0.2063
[2019-04-04 07:11:33,476] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91500, global step 1456892: learning rate 0.0000
[2019-04-04 07:11:33,681] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3023425e-10 3.6367961e-08 4.4459732e-26 1.1924544e-23 2.6674091e-19
 1.0000000e+00 8.4950069e-19], sum to 1.0000
[2019-04-04 07:11:33,682] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5094
[2019-04-04 07:11:33,696] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.33108362309497, 0.3520014068536717, 0.0, 1.0, 41486.70132221535], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3723600.0000, 
sim time next is 3724200.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.34626222189401, 0.3496198939241311, 0.0, 1.0, 41100.35627361521], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6121885184911674, 0.6165399646413771, 0.0, 1.0, 0.1957159822553105], 
reward next is 0.8043, 
noisyNet noise sample is [array([0.93127966], dtype=float32), 0.7637276]. 
=============================================
[2019-04-04 07:11:36,093] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91500, global step 1458094: loss 0.2076
[2019-04-04 07:11:36,095] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91500, global step 1458095: learning rate 0.0000
[2019-04-04 07:11:36,788] A3C_AGENT_WORKER-Thread-11 INFO:Local step 91000, global step 1458409: loss 0.0724
[2019-04-04 07:11:36,798] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 91000, global step 1458415: learning rate 0.0000
[2019-04-04 07:11:38,265] A3C_AGENT_WORKER-Thread-6 INFO:Local step 91500, global step 1458957: loss 0.2651
[2019-04-04 07:11:38,265] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 91500, global step 1458957: learning rate 0.0000
[2019-04-04 07:11:38,700] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91000, global step 1459149: loss 0.0726
[2019-04-04 07:11:38,711] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91000, global step 1459151: learning rate 0.0000
[2019-04-04 07:11:39,756] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91000, global step 1459618: loss 0.0706
[2019-04-04 07:11:39,757] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91000, global step 1459618: learning rate 0.0000
[2019-04-04 07:11:39,830] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91000, global step 1459653: loss 0.0680
[2019-04-04 07:11:39,830] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91000, global step 1459653: learning rate 0.0000
[2019-04-04 07:11:40,221] A3C_AGENT_WORKER-Thread-12 INFO:Local step 91000, global step 1459830: loss 0.0615
[2019-04-04 07:11:40,221] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 91000, global step 1459830: learning rate 0.0000
[2019-04-04 07:11:40,315] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91000, global step 1459875: loss 0.0579
[2019-04-04 07:11:40,316] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91000, global step 1459875: learning rate 0.0000
[2019-04-04 07:11:40,468] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91000, global step 1459954: loss 0.0636
[2019-04-04 07:11:40,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91000, global step 1459954: learning rate 0.0000
[2019-04-04 07:11:40,935] A3C_AGENT_WORKER-Thread-13 INFO:Local step 91000, global step 1460187: loss 0.0646
[2019-04-04 07:11:40,944] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 91000, global step 1460189: learning rate 0.0000
[2019-04-04 07:11:41,006] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91000, global step 1460221: loss 0.0643
[2019-04-04 07:11:41,018] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91000, global step 1460221: learning rate 0.0000
[2019-04-04 07:11:41,034] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8751943e-10 8.4994909e-09 3.3254278e-24 1.1869544e-22 5.8219387e-18
 1.0000000e+00 8.4118139e-18], sum to 1.0000
[2019-04-04 07:11:41,044] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3311
[2019-04-04 07:11:41,062] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 23.0, 67.0, 548.0, 26.0, 25.86548676110579, 0.6312424503687711, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4033800.0000, 
sim time next is 4034400.0000, 
raw observation next is [-1.666666666666667, 23.33333333333334, 59.16666666666667, 488.8333333333334, 26.0, 26.31194668630805, 0.6763719325845875, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4164358264081256, 0.2333333333333334, 0.19722222222222224, 0.5401473296500922, 0.6666666666666666, 0.6926622238590042, 0.7254573108615291, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7987938], dtype=float32), -0.19754645]. 
=============================================
[2019-04-04 07:11:41,600] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91500, global step 1460522: loss 0.2186
[2019-04-04 07:11:41,600] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91500, global step 1460522: learning rate 0.0000
[2019-04-04 07:11:44,236] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91500, global step 1461881: loss 0.2361
[2019-04-04 07:11:44,236] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91500, global step 1461881: learning rate 0.0000
[2019-04-04 07:11:45,372] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.7497717e-10 2.5088113e-08 4.3865390e-26 5.5640537e-24 6.9752548e-20
 1.0000000e+00 1.7921053e-19], sum to 1.0000
[2019-04-04 07:11:45,374] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4700
[2019-04-04 07:11:45,407] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.37348581207403, 0.3346825019808182, 0.0, 1.0, 46184.96270012982], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3730200.0000, 
sim time next is 3730800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.36898981755879, 0.3379578099860137, 0.0, 1.0, 43903.92484502491], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6140824847965659, 0.6126526033286712, 0.0, 1.0, 0.20906630878583288], 
reward next is 0.7909, 
noisyNet noise sample is [array([0.64356345], dtype=float32), 1.6289625]. 
=============================================
[2019-04-04 07:11:45,799] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91500, global step 1462620: loss 0.3035
[2019-04-04 07:11:45,799] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91500, global step 1462620: learning rate 0.0000
[2019-04-04 07:11:51,646] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92000, global step 1465222: loss 0.1747
[2019-04-04 07:11:51,647] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92000, global step 1465222: learning rate 0.0000
[2019-04-04 07:11:52,194] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92000, global step 1465466: loss 0.1887
[2019-04-04 07:11:52,196] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92000, global step 1465468: learning rate 0.0000
[2019-04-04 07:11:53,243] A3C_AGENT_WORKER-Thread-11 INFO:Local step 91500, global step 1465983: loss 0.2819
[2019-04-04 07:11:53,251] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 91500, global step 1465984: learning rate 0.0000
[2019-04-04 07:11:54,283] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92000, global step 1466487: loss 0.1683
[2019-04-04 07:11:54,285] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92000, global step 1466488: learning rate 0.0000
[2019-04-04 07:11:54,663] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91500, global step 1466675: loss 0.2519
[2019-04-04 07:11:54,666] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91500, global step 1466675: learning rate 0.0000
[2019-04-04 07:11:55,864] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91500, global step 1467186: loss 0.2849
[2019-04-04 07:11:55,864] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91500, global step 1467186: learning rate 0.0000
[2019-04-04 07:11:55,892] A3C_AGENT_WORKER-Thread-12 INFO:Local step 91500, global step 1467196: loss 0.3004
[2019-04-04 07:11:55,901] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 91500, global step 1467196: learning rate 0.0000
[2019-04-04 07:11:56,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4280168e-10 9.5467430e-09 1.3871961e-25 1.5530305e-23 6.0769465e-19
 1.0000000e+00 8.5482866e-19], sum to 1.0000
[2019-04-04 07:11:56,197] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2996
[2019-04-04 07:11:56,205] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.266666666666667, 63.33333333333334, 16.0, 152.0, 26.0, 25.39445219585863, 0.3787305941344465, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4297800.0000, 
sim time next is 4298400.0000, 
raw observation next is [6.2, 64.0, 0.0, 0.0, 26.0, 25.34250421633029, 0.3538878023453524, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6343490304709142, 0.64, 0.0, 0.0, 0.6666666666666666, 0.6118753513608576, 0.6179626007817841, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2052884], dtype=float32), 0.09868322]. 
=============================================
[2019-04-04 07:11:56,222] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91500, global step 1467341: loss 0.3037
[2019-04-04 07:11:56,222] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91500, global step 1467341: learning rate 0.0000
[2019-04-04 07:11:56,849] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91500, global step 1467567: loss 0.3113
[2019-04-04 07:11:56,849] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91500, global step 1467567: learning rate 0.0000
[2019-04-04 07:11:56,863] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91500, global step 1467571: loss 0.3116
[2019-04-04 07:11:56,874] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91500, global step 1467571: learning rate 0.0000
[2019-04-04 07:11:56,922] A3C_AGENT_WORKER-Thread-6 INFO:Local step 92000, global step 1467592: loss 0.1564
[2019-04-04 07:11:56,924] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 92000, global step 1467592: learning rate 0.0000
[2019-04-04 07:11:57,299] A3C_AGENT_WORKER-Thread-13 INFO:Local step 91500, global step 1467763: loss 0.2636
[2019-04-04 07:11:57,300] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 91500, global step 1467763: learning rate 0.0000
[2019-04-04 07:11:57,497] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91500, global step 1467857: loss 0.3150
[2019-04-04 07:11:57,498] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91500, global step 1467857: learning rate 0.0000
[2019-04-04 07:12:00,400] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92000, global step 1469138: loss 0.1788
[2019-04-04 07:12:00,401] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92000, global step 1469138: learning rate 0.0000
[2019-04-04 07:12:03,033] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92000, global step 1470273: loss 0.1687
[2019-04-04 07:12:03,036] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92000, global step 1470275: learning rate 0.0000
[2019-04-04 07:12:04,374] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92000, global step 1470894: loss 0.1518
[2019-04-04 07:12:04,375] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92000, global step 1470894: learning rate 0.0000
[2019-04-04 07:12:07,702] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92500, global step 1472447: loss 5.2088
[2019-04-04 07:12:07,703] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92500, global step 1472447: learning rate 0.0000
[2019-04-04 07:12:08,046] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92500, global step 1472616: loss 5.1360
[2019-04-04 07:12:08,051] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92500, global step 1472619: learning rate 0.0000
[2019-04-04 07:12:09,848] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92500, global step 1473500: loss 5.2134
[2019-04-04 07:12:09,855] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92500, global step 1473503: learning rate 0.0000
[2019-04-04 07:12:11,997] A3C_AGENT_WORKER-Thread-11 INFO:Local step 92000, global step 1474593: loss 0.1329
[2019-04-04 07:12:11,999] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 92000, global step 1474593: learning rate 0.0000
[2019-04-04 07:12:12,027] A3C_AGENT_WORKER-Thread-6 INFO:Local step 92500, global step 1474605: loss 5.2349
[2019-04-04 07:12:12,028] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 92500, global step 1474605: learning rate 0.0000
[2019-04-04 07:12:12,317] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.4078494e-11 1.5968578e-09 6.7544307e-29 1.1788838e-26 6.4321527e-22
 1.0000000e+00 7.7371744e-21], sum to 1.0000
[2019-04-04 07:12:12,318] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1830
[2019-04-04 07:12:12,337] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.566666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.85910287741843, 0.6059016709500306, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4411200.0000, 
sim time next is 4411800.0000, 
raw observation next is [6.449999999999999, 65.5, 0.0, 0.0, 26.0, 25.85565571002125, 0.5908387723529467, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6412742382271469, 0.655, 0.0, 0.0, 0.6666666666666666, 0.6546379758351041, 0.6969462574509823, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0008833], dtype=float32), 0.010359996]. 
=============================================
[2019-04-04 07:12:13,679] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92000, global step 1475349: loss 0.1056
[2019-04-04 07:12:13,682] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92000, global step 1475351: learning rate 0.0000
[2019-04-04 07:12:14,097] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2099466e-12 7.6488266e-10 1.3457511e-28 3.1076162e-26 2.2493184e-22
 1.0000000e+00 3.8829930e-21], sum to 1.0000
[2019-04-04 07:12:14,099] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2360
[2019-04-04 07:12:14,120] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.75, 49.16666666666667, 181.6666666666667, 670.3333333333333, 26.0, 26.66703225571231, 0.7773949792524202, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4630200.0000, 
sim time next is 4630800.0000, 
raw observation next is [4.800000000000001, 49.33333333333334, 192.3333333333333, 634.6666666666666, 26.0, 26.11097701770542, 0.7378672349960883, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5955678670360112, 0.4933333333333334, 0.641111111111111, 0.7012891344383057, 0.6666666666666666, 0.6759147514754517, 0.7459557449986961, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3055504], dtype=float32), -0.1775653]. 
=============================================
[2019-04-04 07:12:14,181] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92000, global step 1475608: loss 0.1276
[2019-04-04 07:12:14,182] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92000, global step 1475608: learning rate 0.0000
[2019-04-04 07:12:14,518] A3C_AGENT_WORKER-Thread-12 INFO:Local step 92000, global step 1475761: loss 0.1338
[2019-04-04 07:12:14,521] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 92000, global step 1475763: learning rate 0.0000
[2019-04-04 07:12:14,879] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92000, global step 1475951: loss 0.1130
[2019-04-04 07:12:14,880] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92000, global step 1475951: learning rate 0.0000
[2019-04-04 07:12:14,895] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92000, global step 1475959: loss 0.1168
[2019-04-04 07:12:14,897] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92000, global step 1475959: learning rate 0.0000
[2019-04-04 07:12:15,087] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92000, global step 1476076: loss 0.1257
[2019-04-04 07:12:15,089] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92000, global step 1476076: learning rate 0.0000
[2019-04-04 07:12:15,584] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92500, global step 1476337: loss 5.1585
[2019-04-04 07:12:15,587] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92500, global step 1476338: learning rate 0.0000
[2019-04-04 07:12:15,661] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92000, global step 1476382: loss 0.1185
[2019-04-04 07:12:15,668] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92000, global step 1476382: learning rate 0.0000
[2019-04-04 07:12:16,462] A3C_AGENT_WORKER-Thread-13 INFO:Local step 92000, global step 1476810: loss 0.1122
[2019-04-04 07:12:16,463] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 92000, global step 1476810: learning rate 0.0000
[2019-04-04 07:12:18,484] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92500, global step 1477873: loss 5.1127
[2019-04-04 07:12:18,487] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92500, global step 1477873: learning rate 0.0000
[2019-04-04 07:12:19,486] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92500, global step 1478460: loss 5.1125
[2019-04-04 07:12:19,490] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92500, global step 1478461: learning rate 0.0000
[2019-04-04 07:12:24,342] A3C_AGENT_WORKER-Thread-2 INFO:Local step 93000, global step 1481100: loss 3.5433
[2019-04-04 07:12:24,343] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 93000, global step 1481100: learning rate 0.0000
[2019-04-04 07:12:24,592] A3C_AGENT_WORKER-Thread-3 INFO:Local step 93000, global step 1481255: loss 3.5062
[2019-04-04 07:12:24,593] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 93000, global step 1481256: learning rate 0.0000
[2019-04-04 07:12:25,792] A3C_AGENT_WORKER-Thread-16 INFO:Local step 93000, global step 1481916: loss 3.5795
[2019-04-04 07:12:25,795] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 93000, global step 1481916: learning rate 0.0000
[2019-04-04 07:12:26,523] A3C_AGENT_WORKER-Thread-11 INFO:Local step 92500, global step 1482256: loss 5.0533
[2019-04-04 07:12:26,524] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 92500, global step 1482256: learning rate 0.0000
[2019-04-04 07:12:28,128] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92500, global step 1483025: loss 5.0651
[2019-04-04 07:12:28,130] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92500, global step 1483026: learning rate 0.0000
[2019-04-04 07:12:28,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0451694e-10 2.2286905e-08 2.3886164e-26 1.1077473e-24 2.5570337e-20
 1.0000000e+00 8.2325125e-19], sum to 1.0000
[2019-04-04 07:12:28,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1235
[2019-04-04 07:12:28,201] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 68.0, 141.0, 313.0, 26.0, 24.80498844368071, 0.3384364918073446, 0.0, 1.0, 8346.298164570897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4869000.0000, 
sim time next is 4869600.0000, 
raw observation next is [-3.333333333333333, 67.0, 152.0, 290.6666666666667, 26.0, 25.28698331957491, 0.3763680587597462, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.37026777469990774, 0.67, 0.5066666666666667, 0.3211786372007367, 0.6666666666666666, 0.607248609964576, 0.625456019586582, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5017413], dtype=float32), -1.3845991]. 
=============================================
[2019-04-04 07:12:28,260] A3C_AGENT_WORKER-Thread-6 INFO:Local step 93000, global step 1483094: loss 3.6263
[2019-04-04 07:12:28,261] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 93000, global step 1483094: learning rate 0.0000
[2019-04-04 07:12:28,736] A3C_AGENT_WORKER-Thread-12 INFO:Local step 92500, global step 1483310: loss 5.0621
[2019-04-04 07:12:28,737] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 92500, global step 1483310: learning rate 0.0000
[2019-04-04 07:12:28,852] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92500, global step 1483357: loss 5.0737
[2019-04-04 07:12:28,852] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92500, global step 1483357: learning rate 0.0000
[2019-04-04 07:12:28,860] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92500, global step 1483359: loss 5.0944
[2019-04-04 07:12:28,862] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92500, global step 1483360: learning rate 0.0000
[2019-04-04 07:12:29,697] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92500, global step 1483762: loss 5.0538
[2019-04-04 07:12:29,698] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92500, global step 1483762: learning rate 0.0000
[2019-04-04 07:12:29,991] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92500, global step 1483910: loss 5.0495
[2019-04-04 07:12:29,993] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92500, global step 1483910: learning rate 0.0000
[2019-04-04 07:12:30,110] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92500, global step 1483978: loss 5.1052
[2019-04-04 07:12:30,112] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92500, global step 1483978: learning rate 0.0000
[2019-04-04 07:12:31,332] A3C_AGENT_WORKER-Thread-13 INFO:Local step 92500, global step 1484576: loss 5.1015
[2019-04-04 07:12:31,332] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 92500, global step 1484576: learning rate 0.0000
[2019-04-04 07:12:31,836] A3C_AGENT_WORKER-Thread-4 INFO:Local step 93000, global step 1484848: loss 3.7181
[2019-04-04 07:12:31,838] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 93000, global step 1484848: learning rate 0.0000
[2019-04-04 07:12:34,534] A3C_AGENT_WORKER-Thread-15 INFO:Local step 93000, global step 1486190: loss 3.6935
[2019-04-04 07:12:34,534] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 93000, global step 1486190: learning rate 0.0000
[2019-04-04 07:12:36,055] A3C_AGENT_WORKER-Thread-18 INFO:Local step 93000, global step 1486998: loss 3.7096
[2019-04-04 07:12:36,056] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 93000, global step 1486999: learning rate 0.0000
[2019-04-04 07:12:36,156] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5913525e-10 4.0632084e-09 2.2336834e-26 1.2054059e-24 1.7245030e-19
 1.0000000e+00 1.9544161e-19], sum to 1.0000
[2019-04-04 07:12:36,156] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0430
[2019-04-04 07:12:36,166] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 67.33333333333333, 0.0, 0.0, 26.0, 24.81127718934649, 0.2322971703744712, 0.0, 1.0, 39244.21998023891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4855200.0000, 
sim time next is 4855800.0000, 
raw observation next is [-3.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.78805199118534, 0.2318685166996096, 0.0, 1.0, 39293.23576882304], 
processed observation next is [0.0, 0.17391304347826086, 0.3564173591874424, 0.6916666666666668, 0.0, 0.0, 0.6666666666666666, 0.5656709992654451, 0.5772895055665365, 0.0, 1.0, 0.18711064651820494], 
reward next is 0.8129, 
noisyNet noise sample is [array([-0.81426364], dtype=float32), 0.213451]. 
=============================================
[2019-04-04 07:12:39,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:12:39,416] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:12:39,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run12
[2019-04-04 07:12:39,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:12:39,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:12:39,623] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run12
[2019-04-04 07:12:40,065] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2999857e-09 3.6709171e-08 1.3997318e-23 4.8748206e-23 6.2515825e-18
 1.0000000e+00 9.6532838e-18], sum to 1.0000
[2019-04-04 07:12:40,066] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6987
[2019-04-04 07:12:40,087] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8333333333333334, 39.83333333333334, 0.0, 0.0, 26.0, 25.38381406927507, 0.3499340325211842, 0.0, 1.0, 37921.74697241068], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4924200.0000, 
sim time next is 4924800.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 26.0, 25.39884247016484, 0.3469016445444058, 0.0, 1.0, 30639.82740662543], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.4, 0.0, 0.0, 0.6666666666666666, 0.61657020584707, 0.6156338815148019, 0.0, 1.0, 0.14590394003154966], 
reward next is 0.8541, 
noisyNet noise sample is [array([-1.3995383], dtype=float32), 1.0893637]. 
=============================================
[2019-04-04 07:12:40,611] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:12:40,611] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:12:40,615] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run12
[2019-04-04 07:12:42,556] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:12:42,556] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:12:42,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run12
[2019-04-04 07:12:42,971] A3C_AGENT_WORKER-Thread-11 INFO:Local step 93000, global step 1489996: loss 3.6531
[2019-04-04 07:12:42,972] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 93000, global step 1489996: learning rate 0.0000
[2019-04-04 07:12:44,200] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6067903e-09 7.5282935e-09 1.2755542e-24 3.3831105e-23 3.2672430e-19
 1.0000000e+00 2.9790618e-18], sum to 1.0000
[2019-04-04 07:12:44,206] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2484
[2019-04-04 07:12:44,215] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.333333333333333, 25.0, 0.0, 0.0, 26.0, 25.91604813049745, 0.5552030826128559, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4998000.0000, 
sim time next is 4998600.0000, 
raw observation next is [5.0, 26.0, 0.0, 0.0, 26.0, 25.84312514588501, 0.5395104206158676, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 0.26, 0.0, 0.0, 0.6666666666666666, 0.6535937621570843, 0.6798368068719558, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5688063], dtype=float32), 0.38477102]. 
=============================================
[2019-04-04 07:12:44,739] A3C_AGENT_WORKER-Thread-10 INFO:Local step 93000, global step 1490552: loss 3.6851
[2019-04-04 07:12:44,740] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 93000, global step 1490552: learning rate 0.0000
[2019-04-04 07:12:45,015] A3C_AGENT_WORKER-Thread-14 INFO:Local step 93000, global step 1490647: loss 3.7070
[2019-04-04 07:12:45,017] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 93000, global step 1490649: learning rate 0.0000
[2019-04-04 07:12:45,055] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4907644e-08 5.3357962e-08 1.3930624e-23 2.5597537e-22 1.0351129e-17
 9.9999988e-01 1.8903527e-17], sum to 1.0000
[2019-04-04 07:12:45,055] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8230
[2019-04-04 07:12:45,106] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 44.33333333333334, 0.0, 0.0, 26.0, 24.98782354921254, 0.3462395876146511, 0.0, 1.0, 198825.9572201984], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4821600.0000, 
sim time next is 4822200.0000, 
raw observation next is [1.0, 45.0, 0.0, 0.0, 26.0, 25.02368101859592, 0.3753852027088445, 0.0, 1.0, 164404.3176276435], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.45, 0.0, 0.0, 0.6666666666666666, 0.5853067515496599, 0.6251284009029482, 0.0, 1.0, 0.7828777029887786], 
reward next is 0.2171, 
noisyNet noise sample is [array([-0.36299855], dtype=float32), 1.0335796]. 
=============================================
[2019-04-04 07:12:45,214] A3C_AGENT_WORKER-Thread-12 INFO:Local step 93000, global step 1490716: loss 3.6966
[2019-04-04 07:12:45,219] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 93000, global step 1490717: learning rate 0.0000
[2019-04-04 07:12:45,378] A3C_AGENT_WORKER-Thread-5 INFO:Local step 93000, global step 1490775: loss 3.6956
[2019-04-04 07:12:45,380] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 93000, global step 1490776: learning rate 0.0000
[2019-04-04 07:12:45,984] A3C_AGENT_WORKER-Thread-20 INFO:Local step 93000, global step 1491028: loss 3.6955
[2019-04-04 07:12:45,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 93000, global step 1491028: learning rate 0.0000
[2019-04-04 07:12:46,479] A3C_AGENT_WORKER-Thread-17 INFO:Local step 93000, global step 1491265: loss 3.7027
[2019-04-04 07:12:46,483] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 93000, global step 1491267: learning rate 0.0000
[2019-04-04 07:12:46,768] A3C_AGENT_WORKER-Thread-19 INFO:Local step 93000, global step 1491407: loss 3.7038
[2019-04-04 07:12:46,770] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 93000, global step 1491407: learning rate 0.0000
[2019-04-04 07:12:47,049] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1435096e-09 1.8952846e-07 1.9416663e-24 7.7653139e-23 1.1341757e-18
 9.9999976e-01 2.4431362e-17], sum to 1.0000
[2019-04-04 07:12:47,054] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1466
[2019-04-04 07:12:47,076] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 34.5, 65.66666666666667, 427.6666666666667, 26.0, 25.20335768509565, 0.4078852072919708, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4812600.0000, 
sim time next is 4813200.0000, 
raw observation next is [3.0, 34.0, 57.5, 367.0, 26.0, 25.17800924603628, 0.3973684725547551, 0.0, 1.0, 18680.41167023054], 
processed observation next is [0.0, 0.7391304347826086, 0.5457063711911359, 0.34, 0.19166666666666668, 0.40552486187845305, 0.6666666666666666, 0.5981674371696899, 0.6324561575182517, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.241505], dtype=float32), -1.7519027]. 
=============================================
[2019-04-04 07:12:47,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:12:47,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:12:47,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run12
[2019-04-04 07:12:47,813] A3C_AGENT_WORKER-Thread-13 INFO:Local step 93000, global step 1491827: loss 3.7402
[2019-04-04 07:12:47,815] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 93000, global step 1491828: learning rate 0.0000
[2019-04-04 07:12:49,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:12:49,786] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:12:49,811] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run12
[2019-04-04 07:12:49,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6036465e-11 5.8554495e-10 2.1165501e-28 8.2324828e-26 4.7678440e-21
 1.0000000e+00 1.4747748e-20], sum to 1.0000
[2019-04-04 07:12:49,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1002
[2019-04-04 07:12:50,035] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.416666666666667, 82.66666666666667, 28.66666666666666, 0.0, 26.0, 24.54627051744927, 0.1787405372246849, 0.0, 1.0, 28616.51865161897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 58200.0000, 
sim time next is 58800.0000, 
raw observation next is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 26.0, 24.5220054204006, 0.1773502594089305, 0.0, 1.0, 50689.92145719513], 
processed observation next is [0.0, 0.6956521739130435, 0.6352723915050786, 0.8333333333333335, 0.07777777777777777, 0.0, 0.6666666666666666, 0.54350045170005, 0.5591167531363102, 0.0, 1.0, 0.24138057836759585], 
reward next is 0.7586, 
noisyNet noise sample is [array([0.7716326], dtype=float32), 0.6014697]. 
=============================================
[2019-04-04 07:12:51,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:12:51,146] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:12:51,166] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run12
[2019-04-04 07:12:59,831] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.86622087e-10 6.51795418e-09 3.45942316e-27 2.79651068e-25
 1.14690514e-20 1.00000000e+00 7.43033338e-20], sum to 1.0000
[2019-04-04 07:12:59,832] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4603
[2019-04-04 07:12:59,961] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 21.0, 0.0, 26.0, 21.90281754382155, -0.3528837853276385, 0.0, 1.0, 100106.0882914798], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 30600.0000, 
sim time next is 31200.0000, 
raw observation next is [7.699999999999999, 93.0, 23.83333333333333, 0.0, 26.0, 22.2967728386749, -0.2985658701758409, 0.0, 1.0, 74399.88709481977], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.07944444444444443, 0.0, 0.6666666666666666, 0.3580644032229084, 0.4004780432747197, 0.0, 1.0, 0.35428517664199893], 
reward next is 0.6457, 
noisyNet noise sample is [array([-0.8718915], dtype=float32), -1.5416254]. 
=============================================
[2019-04-04 07:13:03,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:13:03,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:13:03,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run12
[2019-04-04 07:13:06,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:13:06,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:13:06,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run12
[2019-04-04 07:13:07,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:13:07,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:13:07,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run12
[2019-04-04 07:13:08,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:13:08,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:13:08,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run12
[2019-04-04 07:13:08,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:13:08,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:13:08,512] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run12
[2019-04-04 07:13:08,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:13:08,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:13:08,895] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run12
[2019-04-04 07:13:10,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:13:10,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:13:10,212] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run12
[2019-04-04 07:13:11,294] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:13:11,294] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:13:11,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run12
[2019-04-04 07:13:13,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:13:13,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:13:13,132] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run12
[2019-04-04 07:13:24,943] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5060628e-10 2.0957867e-08 1.2304699e-25 4.0078516e-24 1.1770007e-19
 1.0000000e+00 3.7996513e-19], sum to 1.0000
[2019-04-04 07:13:24,943] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8460
[2019-04-04 07:13:25,197] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.300000000000001, 68.0, 0.0, 0.0, 26.0, 23.03321753134231, -0.05391713875624674, 1.0, 1.0, 202334.7339287751], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 112800.0000, 
sim time next is 113400.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 26.0, 23.38490075641259, 0.0494828069588645, 1.0, 1.0, 203503.9368248485], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.6666666666666666, 0.44874172970104925, 0.5164942689862881, 1.0, 1.0, 0.9690663658326119], 
reward next is 0.0309, 
noisyNet noise sample is [array([-1.8754798], dtype=float32), 1.946834]. 
=============================================
[2019-04-04 07:13:28,649] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 07:13:28,657] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:13:28,658] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:13:28,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:13:28,658] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:13:28,659] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:13:28,749] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:13:28,751] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run16
[2019-04-04 07:13:28,696] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run16
[2019-04-04 07:13:28,687] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run16
[2019-04-04 07:14:12,249] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.25003555], dtype=float32), 0.24410982]
[2019-04-04 07:14:12,249] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-12.71666666666667, 56.66666666666666, 0.0, 0.0, 26.0, 23.75369689479327, -0.1122720489117265, 0.0, 1.0, 44505.22151166525]
[2019-04-04 07:14:12,249] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 07:14:12,250] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.2338911e-08 1.2871766e-07 5.9489498e-22 1.7915095e-20 1.8664282e-16
 9.9999988e-01 6.5789319e-16], sampled 0.8526393280891041
[2019-04-04 07:15:13,345] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.25003555], dtype=float32), 0.24410982]
[2019-04-04 07:15:13,345] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.2, 78.33333333333334, 0.0, 0.0, 26.0, 23.83139489404377, 0.01226114464958347, 0.0, 1.0, 41896.1695251752]
[2019-04-04 07:15:13,345] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 07:15:13,345] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.4503704e-10 2.7697963e-08 1.2798182e-25 6.6122680e-24 2.4912638e-19
 1.0000000e+00 1.0209672e-18], sampled 0.9467679344799147
[2019-04-04 07:16:37,977] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 07:17:11,102] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 07:17:17,828] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 07:17:18,855] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1500000, evaluation results [1500000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 07:17:18,865] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.7766928e-10 1.0388598e-08 5.4815923e-25 2.8360698e-23 1.5059648e-18
 1.0000000e+00 2.3149592e-18], sum to 1.0000
[2019-04-04 07:17:18,865] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1443
[2019-04-04 07:17:18,974] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 42.33333333333333, 78.0, 574.3333333333334, 26.0, 24.89950789361182, 0.3611204511225717, 1.0, 1.0, 198443.0709362943], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 312600.0000, 
sim time next is 313200.0000, 
raw observation next is [-9.5, 42.0, 76.0, 550.0, 26.0, 25.24829443167668, 0.4525628056663922, 1.0, 1.0, 112449.4600039761], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.25333333333333335, 0.6077348066298343, 0.6666666666666666, 0.6040245359730566, 0.650854268555464, 1.0, 1.0, 0.5354736190665529], 
reward next is 0.4645, 
noisyNet noise sample is [array([-1.0143942], dtype=float32), 1.5505326]. 
=============================================
[2019-04-04 07:17:20,428] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.1463175e-09 1.2047727e-07 9.7203630e-23 6.6235619e-21 1.6021817e-16
 9.9999988e-01 5.3621987e-16], sum to 1.0000
[2019-04-04 07:17:20,428] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8488
[2019-04-04 07:17:20,464] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.63930633921254, -0.5058469225587893, 0.0, 1.0, 49374.78555829616], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 370800.0000, 
sim time next is 371400.0000, 
raw observation next is [-16.28333333333333, 78.5, 0.0, 0.0, 26.0, 21.582729005907, -0.5246781563320168, 0.0, 1.0, 49481.76451707297], 
processed observation next is [1.0, 0.30434782608695654, 0.011542012927054512, 0.785, 0.0, 0.0, 0.6666666666666666, 0.2985607504922501, 0.3251072812226611, 0.0, 1.0, 0.23562745008129984], 
reward next is 0.7644, 
noisyNet noise sample is [array([0.7725132], dtype=float32), 1.6011505]. 
=============================================
[2019-04-04 07:17:25,847] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.2129770e-10 4.3925885e-09 1.6561574e-25 5.8879000e-23 3.6961327e-19
 1.0000000e+00 6.9555162e-19], sum to 1.0000
[2019-04-04 07:17:25,847] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9647
[2019-04-04 07:17:25,887] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 64.0, 44.0, 24.0, 26.0, 25.8439524094785, 0.4421882129995614, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 144000.0000, 
sim time next is 144600.0000, 
raw observation next is [-6.800000000000001, 65.16666666666667, 38.33333333333333, 17.0, 26.0, 25.945497285252, 0.4461277145023752, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.2742382271468144, 0.6516666666666667, 0.12777777777777777, 0.01878453038674033, 0.6666666666666666, 0.662124773771, 0.6487092381674584, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3039773], dtype=float32), 0.13232435]. 
=============================================
[2019-04-04 07:17:36,174] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.2615206e-11 4.2742934e-09 7.2135783e-27 2.9278351e-25 7.4744544e-20
 1.0000000e+00 2.7051892e-20], sum to 1.0000
[2019-04-04 07:17:36,174] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8469
[2019-04-04 07:17:36,228] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.766666666666667, 63.0, 143.6666666666667, 0.0, 26.0, 25.30118525031383, 0.3531879223909213, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 222000.0000, 
sim time next is 222600.0000, 
raw observation next is [-3.583333333333333, 62.5, 138.3333333333333, 0.0, 26.0, 25.82318322980164, 0.4000225537455197, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.36334256694367506, 0.625, 0.46111111111111097, 0.0, 0.6666666666666666, 0.6519319358168033, 0.6333408512485066, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6815991], dtype=float32), -0.5719737]. 
=============================================
[2019-04-04 07:17:40,750] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1785494e-08 1.7434955e-07 7.4176042e-23 2.0176230e-21 3.9443290e-17
 9.9999976e-01 7.5510684e-17], sum to 1.0000
[2019-04-04 07:17:40,751] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8312
[2019-04-04 07:17:40,784] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 48.66666666666666, 0.0, 0.0, 26.0, 24.39856174860302, 0.1286100157513348, 0.0, 1.0, 44914.31450863199], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 424200.0000, 
sim time next is 424800.0000, 
raw observation next is [-10.6, 49.0, 0.0, 0.0, 26.0, 24.35333446998447, 0.1174656972862317, 0.0, 1.0, 44894.88359849376], 
processed observation next is [1.0, 0.9565217391304348, 0.1689750692520776, 0.49, 0.0, 0.0, 0.6666666666666666, 0.5294445391653726, 0.539155232428744, 0.0, 1.0, 0.21378515999282743], 
reward next is 0.7862, 
noisyNet noise sample is [array([0.03051665], dtype=float32), -1.6689067]. 
=============================================
[2019-04-04 07:17:50,215] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3867869e-10 6.0959002e-09 3.3604001e-28 6.7389219e-26 1.0806530e-20
 1.0000000e+00 4.9823092e-20], sum to 1.0000
[2019-04-04 07:17:50,216] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9735
[2019-04-04 07:17:50,325] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333334, 91.66666666666667, 28.5, 87.5, 26.0, 24.30781230555079, 0.2210062385598772, 0.0, 1.0, 202446.3538373176], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 548400.0000, 
sim time next is 549000.0000, 
raw observation next is [0.25, 91.5, 34.0, 104.0, 26.0, 24.62309540276953, 0.2714430694683797, 0.0, 1.0, 10432.47604565788], 
processed observation next is [0.0, 0.34782608695652173, 0.46952908587257625, 0.915, 0.11333333333333333, 0.11491712707182321, 0.6666666666666666, 0.5519246168974608, 0.5904810231561265, 0.0, 1.0, 0.049678457360275624], 
reward next is 0.9503, 
noisyNet noise sample is [array([-0.23860975], dtype=float32), 2.437506]. 
=============================================
[2019-04-04 07:17:50,337] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[88.26355 ]
 [87.44032 ]
 [87.249535]
 [87.06639 ]
 [86.96089 ]], R is [[88.7017746 ]
 [87.8507309 ]
 [87.77714539]
 [87.70401764]
 [87.6313324 ]].
[2019-04-04 07:17:54,572] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.02874184e-09 1.11623155e-08 7.79383057e-24 5.08425208e-23
 2.99464880e-18 1.00000000e+00 1.43186966e-17], sum to 1.0000
[2019-04-04 07:17:54,573] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0905
[2019-04-04 07:17:54,669] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 36.66666666666667, 17.5, 338.6666666666667, 26.0, 26.15051961086881, 0.3373514625962482, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 405600.0000, 
sim time next is 406200.0000, 
raw observation next is [-8.9, 36.33333333333333, 14.0, 274.3333333333334, 26.0, 26.20408650872852, 0.4574131156816885, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.3633333333333333, 0.04666666666666667, 0.3031307550644568, 0.6666666666666666, 0.6836738757273766, 0.6524710385605629, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47795483], dtype=float32), 1.2979653]. 
=============================================
[2019-04-04 07:17:59,002] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6565459e-10 6.5776007e-09 1.2060585e-25 9.8499208e-24 2.6390293e-19
 1.0000000e+00 3.3528991e-19], sum to 1.0000
[2019-04-04 07:17:59,003] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9535
[2019-04-04 07:17:59,052] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.083333333333333, 53.83333333333334, 0.0, 0.0, 26.0, 24.93234350153359, 0.3342017379515538, 1.0, 1.0, 99703.41872540613], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 760200.0000, 
sim time next is 760800.0000, 
raw observation next is [-4.266666666666667, 54.66666666666667, 0.0, 0.0, 26.0, 25.08636492315068, 0.3291766630064899, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3444136657433057, 0.5466666666666667, 0.0, 0.0, 0.6666666666666666, 0.5905304102625566, 0.6097255543354966, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1286824], dtype=float32), 0.05139557]. 
=============================================
[2019-04-04 07:18:08,323] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0382012e-10 3.4637859e-09 3.1512722e-26 4.4568940e-25 1.1827883e-20
 1.0000000e+00 2.3391654e-19], sum to 1.0000
[2019-04-04 07:18:08,324] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7857
[2019-04-04 07:18:08,342] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 80.33333333333334, 0.0, 0.0, 26.0, 24.76938983877969, 0.2485672396483736, 0.0, 1.0, 41295.52495506514], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 859200.0000, 
sim time next is 859800.0000, 
raw observation next is [-2.9, 79.66666666666667, 0.0, 0.0, 26.0, 24.790060548924, 0.248015909947488, 0.0, 1.0, 41164.91678702298], 
processed observation next is [1.0, 0.9565217391304348, 0.38227146814404434, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.565838379077, 0.582671969982496, 0.0, 1.0, 0.196023413271538], 
reward next is 0.8040, 
noisyNet noise sample is [array([-1.1215057], dtype=float32), -0.64007163]. 
=============================================
[2019-04-04 07:18:13,064] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0512243e-11 1.1126133e-09 1.5397061e-29 1.8497533e-28 6.8578354e-23
 1.0000000e+00 2.8374686e-21], sum to 1.0000
[2019-04-04 07:18:13,071] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9146
[2019-04-04 07:18:13,103] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 93.0, 36.0, 0.0, 26.0, 25.80429893368935, 0.4057066860082339, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 921600.0000, 
sim time next is 922200.0000, 
raw observation next is [4.5, 92.83333333333333, 30.0, 0.0, 26.0, 25.78394147640775, 0.4027152533576151, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5872576177285319, 0.9283333333333332, 0.1, 0.0, 0.6666666666666666, 0.6486617897006459, 0.6342384177858716, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17389299], dtype=float32), 0.31309095]. 
=============================================
[2019-04-04 07:18:13,619] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0653141e-12 3.3480640e-10 9.9386303e-29 9.1661785e-27 2.2958619e-22
 1.0000000e+00 1.1979268e-21], sum to 1.0000
[2019-04-04 07:18:13,619] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1878
[2019-04-04 07:18:13,676] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 82.0, 48.0, 0.0, 26.0, 25.44412265636153, 0.2931817251208586, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 898200.0000, 
sim time next is 898800.0000, 
raw observation next is [1.1, 82.66666666666667, 52.83333333333333, 0.0, 26.0, 25.47928042190486, 0.288141582262684, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8266666666666667, 0.1761111111111111, 0.0, 0.6666666666666666, 0.6232733684920717, 0.5960471940875613, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7261693], dtype=float32), 1.3970358]. 
=============================================
[2019-04-04 07:18:28,528] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6430264e-10 7.6013052e-08 2.1020708e-24 3.8541532e-23 1.7817711e-19
 9.9999988e-01 1.1255214e-18], sum to 1.0000
[2019-04-04 07:18:28,530] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7361
[2019-04-04 07:18:28,549] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.56504155759393, -0.05507268030400409, 0.0, 1.0, 42092.28951618967], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 799200.0000, 
sim time next is 799800.0000, 
raw observation next is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 26.0, 23.54857932735182, -0.06419317982666743, 0.0, 1.0, 42109.28660812553], 
processed observation next is [1.0, 0.2608695652173913, 0.26315789473684215, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.4623816106126517, 0.47860227339111083, 0.0, 1.0, 0.20052041241964536], 
reward next is 0.7995, 
noisyNet noise sample is [array([0.8049654], dtype=float32), -0.72339576]. 
=============================================
[2019-04-04 07:18:35,094] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8492799e-10 1.1677899e-08 4.8974010e-27 1.1345268e-25 2.7948539e-20
 1.0000000e+00 5.5847338e-20], sum to 1.0000
[2019-04-04 07:18:35,094] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1528
[2019-04-04 07:18:35,192] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.99289755714832, 0.2482116820843802, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 891000.0000, 
sim time next is 891600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.13529510005398, 0.2494024886136979, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5946079250044983, 0.5831341628712327, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1230028], dtype=float32), -0.7264729]. 
=============================================
[2019-04-04 07:18:35,968] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4079075e-10 5.5441163e-09 2.0876753e-27 1.6078625e-25 7.1600310e-21
 1.0000000e+00 2.7762692e-20], sum to 1.0000
[2019-04-04 07:18:35,968] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3436
[2019-04-04 07:18:36,001] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.7, 72.66666666666666, 0.0, 0.0, 26.0, 24.54762870559213, 0.1655210247357712, 0.0, 1.0, 39041.28707113471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 881400.0000, 
sim time next is 882000.0000, 
raw observation next is [-0.6, 72.0, 0.0, 0.0, 26.0, 24.54187005852645, 0.1742994300125129, 0.0, 1.0, 39027.97059967624], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5451558382105374, 0.558099810004171, 0.0, 1.0, 0.18584747904607735], 
reward next is 0.8142, 
noisyNet noise sample is [array([-1.0694199], dtype=float32), -1.3868963]. 
=============================================
[2019-04-04 07:18:36,020] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[85.768234]
 [85.854195]
 [85.95014 ]
 [86.03629 ]
 [86.08546 ]], R is [[85.6558609 ]
 [85.61338806]
 [85.57135773]
 [85.52978516]
 [85.48873138]].
[2019-04-04 07:18:40,493] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4576599e-12 3.2148556e-10 2.5316194e-31 1.1652854e-29 1.0819053e-23
 1.0000000e+00 1.0905161e-22], sum to 1.0000
[2019-04-04 07:18:40,494] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8871
[2019-04-04 07:18:40,545] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.5, 92.83333333333333, 18.0, 0.0, 26.0, 25.40585924682344, 0.4252915335922909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 979800.0000, 
sim time next is 980400.0000, 
raw observation next is [9.600000000000001, 92.66666666666667, 22.5, 0.0, 26.0, 25.33550761313693, 0.487207385585406, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7285318559556788, 0.9266666666666667, 0.075, 0.0, 0.6666666666666666, 0.6112923010947441, 0.662402461861802, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00691661], dtype=float32), -0.36280385]. 
=============================================
[2019-04-04 07:18:45,915] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.0463704e-12 2.4312174e-10 1.8317693e-29 8.1826928e-28 4.9267172e-23
 1.0000000e+00 5.0912028e-22], sum to 1.0000
[2019-04-04 07:18:45,916] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2147
[2019-04-04 07:18:45,946] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.18834939936512, 0.5024070279682417, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1360200.0000, 
sim time next is 1360800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.29024530460687, 0.5044699608107384, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6075204420505725, 0.6681566536035795, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3585411], dtype=float32), 1.1366783]. 
=============================================
[2019-04-04 07:18:47,707] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.7071222e-10 6.2940848e-09 1.2037884e-24 2.1663266e-23 2.8478272e-19
 1.0000000e+00 1.7083119e-18], sum to 1.0000
[2019-04-04 07:18:47,708] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2699
[2019-04-04 07:18:47,730] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.26666666666667, 49.66666666666666, 29.33333333333334, 0.4999999999999999, 26.0, 27.90649249352284, 1.021717557065076, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1096800.0000, 
sim time next is 1097400.0000, 
raw observation next is [17.98333333333333, 49.83333333333334, 23.66666666666667, 0.9999999999999998, 26.0, 27.94627793729586, 1.026403469503265, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.9607571560480148, 0.4983333333333334, 0.07888888888888891, 0.0011049723756906074, 0.6666666666666666, 0.8288564947746551, 0.8421344898344216, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5209076], dtype=float32), -1.3202218]. 
=============================================
[2019-04-04 07:18:48,255] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2456852e-09 7.4284476e-09 6.8688707e-27 3.6286164e-25 1.1339283e-20
 1.0000000e+00 1.0408454e-19], sum to 1.0000
[2019-04-04 07:18:48,259] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5534
[2019-04-04 07:18:48,273] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.46666666666667, 61.33333333333333, 0.0, 0.0, 26.0, 25.63265146296164, 0.6896911752686113, 0.0, 1.0, 70277.48959640894], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1111200.0000, 
sim time next is 1111800.0000, 
raw observation next is [13.38333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 25.68546398119304, 0.699769835236555, 0.0, 1.0, 18722.47403270673], 
processed observation next is [1.0, 0.8695652173913043, 0.8333333333333334, 0.6166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6404553317660865, 0.7332566117455183, 0.0, 1.0, 0.08915463825098444], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.43072674], dtype=float32), -0.47877774]. 
=============================================
[2019-04-04 07:18:52,948] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.6994005e-09 8.9458212e-08 5.8957626e-24 1.6800791e-22 1.7090743e-18
 9.9999988e-01 6.3171261e-17], sum to 1.0000
[2019-04-04 07:18:52,951] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9373
[2019-04-04 07:18:52,959] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.56032005715784, 0.1603779260464825, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1231800.0000, 
sim time next is 1232400.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.53755866618503, 0.1601203817421797, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.46146322218208596, 0.5533734605807266, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09772493], dtype=float32), -0.2719826]. 
=============================================
[2019-04-04 07:18:54,184] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0729635e-11 1.8156618e-09 1.3781784e-30 7.2738686e-28 5.3013870e-23
 1.0000000e+00 1.9295823e-21], sum to 1.0000
[2019-04-04 07:18:54,186] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6297
[2019-04-04 07:18:54,203] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.9, 99.33333333333334, 99.0, 0.0, 26.0, 24.95041830661247, 0.4701366504162479, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1255800.0000, 
sim time next is 1256400.0000, 
raw observation next is [13.8, 100.0, 98.0, 0.0, 26.0, 24.91864395510834, 0.4657085209867924, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.32666666666666666, 0.0, 0.6666666666666666, 0.576553662925695, 0.6552361736622642, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7916828], dtype=float32), 0.044663165]. 
=============================================
[2019-04-04 07:18:56,476] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.7973628e-11 9.3877650e-10 1.1859319e-29 4.3415536e-28 1.7132486e-22
 1.0000000e+00 1.6456247e-21], sum to 1.0000
[2019-04-04 07:18:56,476] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1468
[2019-04-04 07:18:56,488] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.45001321078474, 0.5864774353251553, 0.0, 1.0, 56249.90509535497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1287000.0000, 
sim time next is 1287600.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.43356038561261, 0.5900047325583417, 0.0, 1.0, 53318.42179421529], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6194633654677174, 0.6966682441861138, 0.0, 1.0, 0.25389724663912044], 
reward next is 0.7461, 
noisyNet noise sample is [array([0.22277832], dtype=float32), 0.75560224]. 
=============================================
[2019-04-04 07:18:56,934] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6970099e-11 1.2841371e-09 3.6457998e-29 8.8657768e-28 5.9474528e-22
 1.0000000e+00 9.2734368e-22], sum to 1.0000
[2019-04-04 07:18:56,939] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6027
[2019-04-04 07:18:56,952] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.433333333333334, 92.0, 0.0, 0.0, 26.0, 25.33685701062646, 0.5142852670385287, 0.0, 1.0, 77920.49477839471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1318800.0000, 
sim time next is 1319400.0000, 
raw observation next is [1.35, 92.0, 0.0, 0.0, 26.0, 25.30625577425239, 0.5209274483787262, 0.0, 1.0, 56181.08774933109], 
processed observation next is [1.0, 0.2608695652173913, 0.5000000000000001, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6088546478543657, 0.6736424827929087, 0.0, 1.0, 0.267528989282529], 
reward next is 0.7325, 
noisyNet noise sample is [array([1.140203], dtype=float32), -1.8112508]. 
=============================================
[2019-04-04 07:19:12,165] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.6936308e-10 3.5828840e-09 9.8359322e-28 1.4272721e-25 4.9632420e-21
 1.0000000e+00 3.4059188e-20], sum to 1.0000
[2019-04-04 07:19:12,166] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6154
[2019-04-04 07:19:12,182] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.47616012046035, 0.5532878409240601, 0.0, 1.0, 58752.51618831168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1555200.0000, 
sim time next is 1555800.0000, 
raw observation next is [5.0, 82.00000000000001, 0.0, 0.0, 26.0, 25.45988619496555, 0.5550288023096991, 0.0, 1.0, 53153.68225356295], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.8200000000000002, 0.0, 0.0, 0.6666666666666666, 0.6216571829137957, 0.6850096007698997, 0.0, 1.0, 0.25311277263601406], 
reward next is 0.7469, 
noisyNet noise sample is [array([-0.3727314], dtype=float32), 1.0914072]. 
=============================================
[2019-04-04 07:19:13,360] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.5569829e-11 1.3271687e-09 7.3730954e-29 3.4182086e-26 1.3352628e-21
 1.0000000e+00 3.0904233e-21], sum to 1.0000
[2019-04-04 07:19:13,360] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4255
[2019-04-04 07:19:13,381] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 86.0, 0.0, 0.0, 26.0, 25.66085957059121, 0.591427411430271, 0.0, 1.0, 18725.65853694438], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1641600.0000, 
sim time next is 1642200.0000, 
raw observation next is [7.1, 87.16666666666667, 0.0, 0.0, 26.0, 25.62187566116577, 0.5828336527625786, 0.0, 1.0, 37722.59112820761], 
processed observation next is [1.0, 0.0, 0.6592797783933518, 0.8716666666666667, 0.0, 0.0, 0.6666666666666666, 0.6351563050971475, 0.6942778842541929, 0.0, 1.0, 0.17963138632479816], 
reward next is 0.8204, 
noisyNet noise sample is [array([-0.6430832], dtype=float32), -0.51520807]. 
=============================================
[2019-04-04 07:19:13,660] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2858873e-10 3.4550762e-09 5.3200089e-29 1.0801928e-25 2.0196255e-21
 1.0000000e+00 1.6874032e-20], sum to 1.0000
[2019-04-04 07:19:13,660] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2402
[2019-04-04 07:19:13,697] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 82.0, 0.0, 0.0, 26.0, 25.5898401825248, 0.583613518286071, 0.0, 1.0, 93418.84051648348], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1638000.0000, 
sim time next is 1638600.0000, 
raw observation next is [7.2, 82.66666666666667, 0.0, 0.0, 26.0, 25.53142472217041, 0.6088248626438326, 0.0, 1.0, 91388.5079616094], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6276187268475342, 0.7029416208812775, 0.0, 1.0, 0.43518337124575907], 
reward next is 0.5648, 
noisyNet noise sample is [array([0.5749986], dtype=float32), -1.0702617]. 
=============================================
[2019-04-04 07:19:28,372] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5040218e-10 7.9121278e-09 2.0682570e-25 5.9725925e-24 1.3745483e-18
 1.0000000e+00 6.4926337e-19], sum to 1.0000
[2019-04-04 07:19:28,372] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5293
[2019-04-04 07:19:28,435] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.266666666666667, 65.0, 212.0, 3.333333333333333, 26.0, 25.59660003855037, 0.3231061144412721, 1.0, 1.0, 33028.97887147822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1946400.0000, 
sim time next is 1947000.0000, 
raw observation next is [-4.083333333333333, 65.0, 197.0, 2.666666666666667, 26.0, 25.57463536907563, 0.3232821883294509, 1.0, 1.0, 25048.88026014361], 
processed observation next is [1.0, 0.5217391304347826, 0.34949215143120965, 0.65, 0.6566666666666666, 0.002946593001841621, 0.6666666666666666, 0.6312196140896358, 0.6077607294431503, 1.0, 1.0, 0.11928038219116005], 
reward next is 0.8807, 
noisyNet noise sample is [array([0.0331794], dtype=float32), 1.6824982]. 
=============================================
[2019-04-04 07:19:28,441] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.16327]
 [77.35657]
 [77.55966]
 [77.99025]
 [78.49582]], R is [[77.07276154]
 [77.1447525 ]
 [77.16930389]
 [77.39761353]
 [77.62363434]].
[2019-04-04 07:19:32,856] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9267630e-10 1.3114297e-08 5.6206013e-26 1.1497502e-24 1.2620921e-19
 1.0000000e+00 5.8210664e-20], sum to 1.0000
[2019-04-04 07:19:32,856] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6309
[2019-04-04 07:19:32,922] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 77.66666666666667, 154.6666666666667, 0.0, 26.0, 25.35074112545441, 0.2998232185498267, 1.0, 1.0, 43344.24290393168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2032800.0000, 
sim time next is 2033400.0000, 
raw observation next is [-4.5, 78.33333333333334, 153.3333333333333, 0.0, 26.0, 25.32275660885034, 0.3100822326544198, 1.0, 1.0, 35237.52014165145], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7833333333333334, 0.511111111111111, 0.0, 0.6666666666666666, 0.6102297174041951, 0.6033607442181399, 1.0, 1.0, 0.16779771496024498], 
reward next is 0.8322, 
noisyNet noise sample is [array([1.0868962], dtype=float32), -0.4290092]. 
=============================================
[2019-04-04 07:19:45,672] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.1708396e-10 3.8352024e-09 3.4093158e-25 6.3614793e-24 3.8438518e-20
 1.0000000e+00 6.7992401e-19], sum to 1.0000
[2019-04-04 07:19:45,672] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2558
[2019-04-04 07:19:45,752] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.05, 77.0, 0.0, 0.0, 26.0, 25.19496614397723, 0.3099923952029686, 1.0, 1.0, 25671.04860124557], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1971000.0000, 
sim time next is 1971600.0000, 
raw observation next is [-5.233333333333333, 79.0, 0.0, 0.0, 26.0, 24.97702204354103, 0.3044059430251977, 1.0, 1.0, 156878.4508863134], 
processed observation next is [1.0, 0.8260869565217391, 0.31763619575253926, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5814185036284192, 0.6014686476750659, 1.0, 1.0, 0.747040242315778], 
reward next is 0.2530, 
noisyNet noise sample is [array([-2.5801432], dtype=float32), 0.0024802277]. 
=============================================
[2019-04-04 07:19:51,173] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.8900872e-11 5.6957838e-09 1.4663277e-26 1.1384205e-24 2.2183713e-20
 1.0000000e+00 4.1607159e-20], sum to 1.0000
[2019-04-04 07:19:51,173] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0727
[2019-04-04 07:19:51,201] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.1, 87.66666666666666, 0.0, 0.0, 26.0, 24.27567799753179, 0.1103150694612215, 0.0, 1.0, 43443.14130125612], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2091000.0000, 
sim time next is 2091600.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.19877648308362, 0.09549846015103497, 0.0, 1.0, 43475.66520311099], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.516564706923635, 0.5318328200503449, 0.0, 1.0, 0.20702697715767138], 
reward next is 0.7930, 
noisyNet noise sample is [array([1.0138749], dtype=float32), 0.36288682]. 
=============================================
[2019-04-04 07:19:59,456] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9796607e-09 3.0216594e-08 5.4126897e-25 2.6364580e-23 6.6257932e-19
 1.0000000e+00 1.0177630e-18], sum to 1.0000
[2019-04-04 07:19:59,460] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9523
[2019-04-04 07:19:59,473] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.25329981673477, 0.1093096198800484, 0.0, 1.0, 42168.97647650785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2173200.0000, 
sim time next is 2173800.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.22837533319367, 0.09755773990990256, 0.0, 1.0, 42168.90614841494], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5190312777661393, 0.5325192466366342, 0.0, 1.0, 0.2008043149924521], 
reward next is 0.7992, 
noisyNet noise sample is [array([-1.2447766], dtype=float32), -1.4363261]. 
=============================================
[2019-04-04 07:20:01,886] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9885152e-10 1.5670742e-09 4.5406740e-25 2.0640569e-24 4.1932529e-20
 1.0000000e+00 2.3216615e-19], sum to 1.0000
[2019-04-04 07:20:01,887] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4048
[2019-04-04 07:20:01,930] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 72.5, 0.0, 0.0, 26.0, 25.91512733042816, 0.4684588385016191, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2140200.0000, 
sim time next is 2140800.0000, 
raw observation next is [-5.0, 73.0, 0.0, 0.0, 26.0, 25.72293611258085, 0.4431481174953646, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6435780093817375, 0.6477160391651215, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.71824753], dtype=float32), -1.7998948]. 
=============================================
[2019-04-04 07:20:02,300] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4044025e-09 9.9033075e-09 1.5244844e-25 3.8345736e-24 1.7900487e-19
 1.0000000e+00 2.0435057e-18], sum to 1.0000
[2019-04-04 07:20:02,302] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9373
[2019-04-04 07:20:02,370] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.4, 80.0, 0.0, 0.0, 26.0, 25.23365141413021, 0.3635708848595073, 1.0, 1.0, 37480.36876798054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2144400.0000, 
sim time next is 2145000.0000, 
raw observation next is [-5.5, 81.5, 0.0, 0.0, 26.0, 25.11017573346104, 0.358020895722406, 1.0, 1.0, 103662.2126479766], 
processed observation next is [1.0, 0.8260869565217391, 0.3102493074792244, 0.815, 0.0, 0.0, 0.6666666666666666, 0.5925146444550867, 0.6193402985741353, 1.0, 1.0, 0.4936295840379838], 
reward next is 0.5064, 
noisyNet noise sample is [array([-1.0345721], dtype=float32), 0.49411038]. 
=============================================
[2019-04-04 07:20:02,374] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.2833  ]
 [77.39881 ]
 [77.40359 ]
 [77.27092 ]
 [77.135735]], R is [[77.32725525]
 [77.37551117]
 [77.43766022]
 [77.51212311]
 [77.61443329]].
[2019-04-04 07:20:09,142] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9047370e-11 3.4043337e-09 1.5564740e-26 9.9752270e-25 2.1778388e-20
 1.0000000e+00 3.3991562e-20], sum to 1.0000
[2019-04-04 07:20:09,143] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0258
[2019-04-04 07:20:09,195] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 68.5, 132.3333333333333, 94.99999999999999, 26.0, 26.37663961691351, 0.5143729098261195, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2213400.0000, 
sim time next is 2214000.0000, 
raw observation next is [-3.9, 68.0, 138.5, 142.5, 26.0, 26.36612639064812, 0.51350136582106, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.68, 0.46166666666666667, 0.1574585635359116, 0.6666666666666666, 0.6971771992206767, 0.6711671219403533, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43835598], dtype=float32), -0.8529182]. 
=============================================
[2019-04-04 07:20:09,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[79.88511 ]
 [79.76286 ]
 [79.78211 ]
 [79.94408 ]
 [80.041115]], R is [[80.26176453]
 [80.45914459]
 [80.65455627]
 [80.84801483]
 [81.03953552]].
[2019-04-04 07:20:18,665] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3094944e-09 5.9175744e-09 3.1896245e-25 9.2572699e-25 2.3328298e-19
 1.0000000e+00 2.9805267e-19], sum to 1.0000
[2019-04-04 07:20:18,667] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6161
[2019-04-04 07:20:18,716] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 64.0, 0.0, 0.0, 26.0, 25.03958214514933, 0.4102993794276926, 0.0, 1.0, 118938.9716746606], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2665800.0000, 
sim time next is 2666400.0000, 
raw observation next is [-1.2, 64.33333333333333, 0.0, 0.0, 26.0, 25.11178003067335, 0.4280626650715583, 0.0, 1.0, 69899.3903720064], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.6433333333333333, 0.0, 0.0, 0.6666666666666666, 0.592648335889446, 0.6426875550238528, 0.0, 1.0, 0.33285423986669715], 
reward next is 0.6671, 
noisyNet noise sample is [array([0.2853108], dtype=float32), -0.87122893]. 
=============================================
[2019-04-04 07:20:28,228] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.8968264e-10 2.1594042e-08 4.3482062e-25 5.1331084e-23 1.2916677e-18
 1.0000000e+00 2.9944430e-18], sum to 1.0000
[2019-04-04 07:20:28,228] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0644
[2019-04-04 07:20:28,308] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.449999999999999, 46.5, 61.0, 665.0, 26.0, 25.30083131475046, 0.256318656889848, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2453400.0000, 
sim time next is 2454000.0000, 
raw observation next is [-6.166666666666666, 45.33333333333333, 63.5, 683.6666666666667, 26.0, 25.26752740531755, 0.251806577468374, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.29178208679593726, 0.4533333333333333, 0.21166666666666667, 0.7554327808471456, 0.6666666666666666, 0.6056272837764626, 0.5839355258227913, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5069466], dtype=float32), -0.6418027]. 
=============================================
[2019-04-04 07:20:28,318] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[75.791214]
 [75.50086 ]
 [75.14146 ]
 [74.78806 ]
 [74.29599 ]], R is [[76.15897369]
 [76.39738464]
 [76.63341522]
 [76.86708069]
 [77.09841156]].
[2019-04-04 07:20:33,006] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7981782e-09 4.6701750e-08 7.2741329e-24 1.4298519e-22 1.2511824e-18
 1.0000000e+00 8.5233162e-18], sum to 1.0000
[2019-04-04 07:20:33,007] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2480
[2019-04-04 07:20:33,089] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.15, 40.0, 0.0, 0.0, 26.0, 25.09983207445249, 0.3272427541395073, 0.0, 1.0, 18709.32261558274], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2575800.0000, 
sim time next is 2576400.0000, 
raw observation next is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 26.0, 25.00937089931414, 0.3249755771854634, 0.0, 1.0, 74562.45783694793], 
processed observation next is [1.0, 0.8260869565217391, 0.42566943674976926, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.5841142416095115, 0.6083251923951545, 0.0, 1.0, 0.35505932303308535], 
reward next is 0.6449, 
noisyNet noise sample is [array([-0.9556579], dtype=float32), 0.38931578]. 
=============================================
[2019-04-04 07:20:38,759] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.6516098e-12 6.4321676e-10 5.3055559e-30 1.2437768e-27 9.8055000e-23
 1.0000000e+00 1.6936448e-21], sum to 1.0000
[2019-04-04 07:20:38,763] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0234
[2019-04-04 07:20:38,773] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.5, 78.0, 0.0, 26.0, 25.4280325187245, 0.3103266125227443, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2889000.0000, 
sim time next is 2889600.0000, 
raw observation next is [0.6666666666666666, 95.33333333333334, 79.5, 0.0, 26.0, 25.42772464441933, 0.3132302270234381, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.4810710987996307, 0.9533333333333335, 0.265, 0.0, 0.6666666666666666, 0.6189770537016109, 0.6044100756744794, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.5468712], dtype=float32), -2.056798]. 
=============================================
[2019-04-04 07:20:45,811] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2528306e-10 2.3734323e-09 4.7689754e-27 5.4071795e-25 5.6078726e-21
 1.0000000e+00 5.5425205e-20], sum to 1.0000
[2019-04-04 07:20:45,812] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1262
[2019-04-04 07:20:45,842] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 75.5, 0.0, 0.0, 26.0, 25.1757887712816, 0.3090960631069823, 0.0, 1.0, 51550.41069902152], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2856600.0000, 
sim time next is 2857200.0000, 
raw observation next is [1.0, 76.66666666666667, 0.0, 0.0, 26.0, 25.13627731932959, 0.3070852976943839, 0.0, 1.0, 54032.04765298268], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7666666666666667, 0.0, 0.0, 0.6666666666666666, 0.5946897766107991, 0.602361765898128, 0.0, 1.0, 0.25729546501420325], 
reward next is 0.7427, 
noisyNet noise sample is [array([-0.9249623], dtype=float32), 1.1108254]. 
=============================================
[2019-04-04 07:21:04,160] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6061039e-09 1.6174239e-08 8.7772165e-26 4.2437088e-24 5.4345751e-19
 1.0000000e+00 4.7506921e-19], sum to 1.0000
[2019-04-04 07:21:04,162] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2829
[2019-04-04 07:21:04,181] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.45, 76.5, 0.0, 0.0, 26.0, 24.63961427954041, 0.2440490904792923, 0.0, 1.0, 43756.95965689836], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3299400.0000, 
sim time next is 3300000.0000, 
raw observation next is [-9.633333333333333, 76.33333333333333, 0.0, 0.0, 26.0, 24.55840876575703, 0.228827642586092, 0.0, 1.0, 43773.89270561271], 
processed observation next is [1.0, 0.17391304347826086, 0.19575253924284397, 0.7633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5465340638130858, 0.5762758808620306, 0.0, 1.0, 0.20844710812196526], 
reward next is 0.7916, 
noisyNet noise sample is [array([-0.8572598], dtype=float32), -0.65025747]. 
=============================================
[2019-04-04 07:21:04,186] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.53584 ]
 [78.66793 ]
 [78.76049 ]
 [78.87995 ]
 [78.979675]], R is [[78.39561462]
 [78.40328979]
 [78.41092682]
 [78.41729736]
 [78.42261505]].
[2019-04-04 07:21:12,593] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1451131e-10 2.9068365e-09 2.6068420e-26 3.8534751e-24 3.7644182e-20
 1.0000000e+00 5.3053023e-19], sum to 1.0000
[2019-04-04 07:21:12,593] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1874
[2019-04-04 07:21:12,639] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.87489639686132, 0.5971717201518153, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3267600.0000, 
sim time next is 3268200.0000, 
raw observation next is [-4.0, 70.0, 0.0, 0.0, 26.0, 25.79176387535801, 0.5753833251803645, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6493136562798343, 0.6917944417267882, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3714717], dtype=float32), 0.4785968]. 
=============================================
[2019-04-04 07:21:23,243] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.3298738e-10 2.3034039e-09 6.7066023e-28 6.1255678e-26 1.9503626e-21
 1.0000000e+00 2.3454278e-20], sum to 1.0000
[2019-04-04 07:21:23,255] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1738
[2019-04-04 07:21:23,427] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.38595093569916, 0.4573818600885849, 0.0, 1.0, 74233.69260913684], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3456000.0000, 
sim time next is 3456600.0000, 
raw observation next is [1.0, 84.83333333333334, 0.0, 0.0, 26.0, 25.3815881931991, 0.4622837915980054, 0.0, 1.0, 58552.1911969145], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.8483333333333334, 0.0, 0.0, 0.6666666666666666, 0.6151323494332583, 0.6540945971993352, 0.0, 1.0, 0.2788199580805452], 
reward next is 0.7212, 
noisyNet noise sample is [array([0.46076986], dtype=float32), 1.2764081]. 
=============================================
[2019-04-04 07:21:25,521] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2021215e-11 4.4747478e-10 1.7577096e-28 1.0500820e-26 6.3274846e-21
 1.0000000e+00 2.9356248e-21], sum to 1.0000
[2019-04-04 07:21:25,521] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5112
[2019-04-04 07:21:25,539] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 61.0, 112.0, 790.0, 26.0, 26.17833164058857, 0.5841821870726277, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3495600.0000, 
sim time next is 3496200.0000, 
raw observation next is [1.166666666666667, 60.33333333333334, 113.0, 796.6666666666667, 26.0, 26.1609878533207, 0.6014188847117291, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49492151431209613, 0.6033333333333334, 0.37666666666666665, 0.8802946593001842, 0.6666666666666666, 0.6800823211100585, 0.7004729615705764, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49683484], dtype=float32), 0.068827406]. 
=============================================
[2019-04-04 07:21:26,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.6229902e-10 5.6898988e-09 2.2164360e-26 1.8143395e-24 2.9429944e-19
 1.0000000e+00 1.4622608e-19], sum to 1.0000
[2019-04-04 07:21:26,366] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0535
[2019-04-04 07:21:26,390] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 66.33333333333334, 552.0, 26.0, 26.96337203305146, 0.517504942972744, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3514800.0000, 
sim time next is 3515400.0000, 
raw observation next is [3.0, 49.0, 62.0, 525.0, 26.0, 26.8902782335996, 0.7398299780098004, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.20666666666666667, 0.580110497237569, 0.6666666666666666, 0.7408565194666332, 0.7466099926699336, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5600443], dtype=float32), 2.6226404]. 
=============================================
[2019-04-04 07:21:43,988] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1968829e-10 1.7374715e-08 6.6337360e-26 5.7429098e-24 4.7075560e-20
 1.0000000e+00 5.2460348e-19], sum to 1.0000
[2019-04-04 07:21:43,993] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2330
[2019-04-04 07:21:44,053] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 70.0, 0.0, 0.0, 26.0, 25.28231489237695, 0.4221261581337699, 0.0, 1.0, 40966.71602326762], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3550200.0000, 
sim time next is 3550800.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 26.0, 25.26723646065329, 0.4136756566042714, 0.0, 1.0, 40859.11040471873], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6056030383877742, 0.6378918855347572, 0.0, 1.0, 0.19456719240342255], 
reward next is 0.8054, 
noisyNet noise sample is [array([-0.02975028], dtype=float32), -1.6306089]. 
=============================================
[2019-04-04 07:21:44,938] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0049589e-11 6.5429745e-10 1.0402581e-26 1.2346688e-25 2.8829012e-20
 1.0000000e+00 1.3951235e-19], sum to 1.0000
[2019-04-04 07:21:44,938] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4041
[2019-04-04 07:21:44,982] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 43.66666666666667, 67.83333333333333, 578.6666666666666, 26.0, 26.92839023523076, 0.6413746558506768, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3860400.0000, 
sim time next is 3861000.0000, 
raw observation next is [3.0, 43.0, 64.0, 551.0, 26.0, 26.40242173899308, 0.6788485938121918, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.43, 0.21333333333333335, 0.6088397790055249, 0.6666666666666666, 0.7002018115827567, 0.7262828646040639, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06966711], dtype=float32), 0.0153906]. 
=============================================
[2019-04-04 07:21:44,991] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[80.20918 ]
 [80.494995]
 [80.69836 ]
 [80.87705 ]
 [81.06539 ]], R is [[80.13253784]
 [80.3312149 ]
 [80.5279007 ]
 [80.72262573]
 [80.91539764]].
[2019-04-04 07:21:45,066] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4212591e-08 2.2212131e-08 1.3124606e-23 3.4816069e-22 2.0243965e-17
 1.0000000e+00 3.2098505e-17], sum to 1.0000
[2019-04-04 07:21:45,067] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8007
[2019-04-04 07:21:45,109] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 39.66666666666666, 46.66666666666667, 390.6666666666667, 26.0, 25.23645309574778, 0.4163115826418508, 0.0, 1.0, 18684.18084836957], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3603000.0000, 
sim time next is 3603600.0000, 
raw observation next is [0.0, 39.0, 38.5, 328.5, 26.0, 25.20366583107362, 0.4039575667974987, 0.0, 1.0, 19255.96717015655], 
processed observation next is [0.0, 0.7391304347826086, 0.46260387811634357, 0.39, 0.12833333333333333, 0.3629834254143646, 0.6666666666666666, 0.6003054859228018, 0.6346525222658329, 0.0, 1.0, 0.09169508176265023], 
reward next is 0.9083, 
noisyNet noise sample is [array([-0.5411913], dtype=float32), 0.28390256]. 
=============================================
[2019-04-04 07:21:54,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.4843242e-09 1.9118991e-07 9.8167631e-23 1.6640128e-21 1.3506442e-17
 9.9999976e-01 9.0977367e-17], sum to 1.0000
[2019-04-04 07:21:54,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0017
[2019-04-04 07:21:54,287] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 31.0, 0.0, 0.0, 26.0, 25.46098558328369, 0.4853110379540038, 0.0, 1.0, 64010.81647600716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4053600.0000, 
sim time next is 4054200.0000, 
raw observation next is [-5.166666666666667, 32.0, 0.0, 0.0, 26.0, 25.45984417683852, 0.4394693407686881, 0.0, 1.0, 48412.21203549852], 
processed observation next is [1.0, 0.9565217391304348, 0.31948291782086796, 0.32, 0.0, 0.0, 0.6666666666666666, 0.62165368140321, 0.6464897802562294, 0.0, 1.0, 0.23053434302618345], 
reward next is 0.7695, 
noisyNet noise sample is [array([-0.17473026], dtype=float32), -0.44895786]. 
=============================================
[2019-04-04 07:22:00,861] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-04 07:22:00,890] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:22:00,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:22:00,892] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run17
[2019-04-04 07:22:00,925] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:22:00,925] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:22:00,928] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run17
[2019-04-04 07:22:00,963] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:22:00,963] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:22:00,965] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run17
[2019-04-04 07:24:54,666] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.24732056], dtype=float32), 0.24367043]
[2019-04-04 07:24:54,666] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.452794911166666, 88.01930185333333, 0.0, 0.0, 26.0, 25.50165259485405, 0.4665508480896524, 0.0, 1.0, 0.0]
[2019-04-04 07:24:54,666] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 07:24:54,668] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.1763841e-10 1.9019120e-09 8.9157682e-28 6.0738426e-26 7.2781691e-21
 1.0000000e+00 4.0910411e-20], sampled 0.8710784556651628
[2019-04-04 07:25:10,220] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 07:25:39,943] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6037 263423224.0345 1559.2181
[2019-04-04 07:25:44,559] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 07:25:45,593] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1600000, evaluation results [1600000.0, 7241.603695074028, 263423224.0344576, 1559.2181108402822, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 07:25:50,395] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.8896874e-10 1.2697219e-08 8.6817078e-27 9.1326821e-25 6.1589021e-21
 1.0000000e+00 9.5157662e-20], sum to 1.0000
[2019-04-04 07:25:50,396] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6809
[2019-04-04 07:25:50,432] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 75.0, 0.0, 0.0, 26.0, 25.4224020661117, 0.3925592622634031, 0.0, 1.0, 40879.16923544391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3810000.0000, 
sim time next is 3810600.0000, 
raw observation next is [-4.0, 74.0, 0.0, 0.0, 26.0, 25.39492506809172, 0.376441460639678, 0.0, 1.0, 55896.68583041086], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.74, 0.0, 0.0, 0.6666666666666666, 0.61624375567431, 0.6254804868798927, 0.0, 1.0, 0.26617469443052794], 
reward next is 0.7338, 
noisyNet noise sample is [array([-0.474102], dtype=float32), -0.73225737]. 
=============================================
[2019-04-04 07:25:53,552] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2852030e-11 1.0303896e-09 7.5557540e-28 4.2898641e-26 4.1206253e-21
 1.0000000e+00 1.3557056e-21], sum to 1.0000
[2019-04-04 07:25:53,553] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4990
[2019-04-04 07:25:53,605] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.166666666666667, 50.5, 114.3333333333333, 827.6666666666666, 26.0, 26.59996562842151, 0.6879257412587333, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3849000.0000, 
sim time next is 3849600.0000, 
raw observation next is [1.333333333333333, 50.00000000000001, 113.6666666666667, 825.8333333333334, 26.0, 26.66097397205181, 0.5838884549287223, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4995383194829178, 0.5000000000000001, 0.378888888888889, 0.912523020257827, 0.6666666666666666, 0.7217478310043175, 0.6946294849762408, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30961522], dtype=float32), 0.68097126]. 
=============================================
[2019-04-04 07:25:54,521] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.7550146e-10 3.4758179e-09 3.1644647e-25 3.6068409e-24 2.2022260e-19
 1.0000000e+00 4.8352341e-19], sum to 1.0000
[2019-04-04 07:25:54,521] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4120
[2019-04-04 07:25:54,535] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 35.0, 0.0, 0.0, 26.0, 26.48121247232125, 0.6545426904012974, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4126800.0000, 
sim time next is 4127400.0000, 
raw observation next is [3.0, 35.5, 0.0, 0.0, 26.0, 26.45873456321669, 0.6189459304682, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.355, 0.0, 0.0, 0.6666666666666666, 0.7048945469347242, 0.7063153101560666, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0295137], dtype=float32), -0.35741192]. 
=============================================
[2019-04-04 07:25:56,319] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.53304072e-08 3.04246754e-07 8.91796052e-24 2.26958110e-22
 1.19267144e-17 9.99999642e-01 1.49389216e-17], sum to 1.0000
[2019-04-04 07:25:56,354] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8980
[2019-04-04 07:25:56,381] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.18022256450548, 0.1137711891745468, 0.0, 1.0, 43643.89971858619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3988800.0000, 
sim time next is 3989400.0000, 
raw observation next is [-12.16666666666667, 64.0, 0.0, 0.0, 26.0, 24.11401463913138, 0.1050442692036519, 0.0, 1.0, 43686.3789886024], 
processed observation next is [1.0, 0.17391304347826086, 0.12557710064635264, 0.64, 0.0, 0.0, 0.6666666666666666, 0.509501219927615, 0.5350147564012173, 0.0, 1.0, 0.20803037613620193], 
reward next is 0.7920, 
noisyNet noise sample is [array([-0.68406695], dtype=float32), -0.81125146]. 
=============================================
[2019-04-04 07:25:57,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.29293151e-09 1.00564264e-07 9.42068343e-24 4.53482990e-22
 3.23519664e-18 9.99999881e-01 2.21018094e-17], sum to 1.0000
[2019-04-04 07:25:57,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1993
[2019-04-04 07:25:57,821] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.33333333333333, 65.0, 0.0, 0.0, 26.0, 24.07974829941636, 0.09828364358355646, 0.0, 1.0, 43705.91617426986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3990000.0000, 
sim time next is 3990600.0000, 
raw observation next is [-12.5, 66.0, 0.0, 0.0, 26.0, 24.05354233622288, 0.08391329806530168, 0.0, 1.0, 43720.17626365295], 
processed observation next is [1.0, 0.17391304347826086, 0.11634349030470914, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5044618613519066, 0.5279710993551006, 0.0, 1.0, 0.20819131554120454], 
reward next is 0.7918, 
noisyNet noise sample is [array([-1.268419], dtype=float32), 0.54548943]. 
=============================================
[2019-04-04 07:26:11,612] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2570854e-09 1.7647046e-08 2.1847148e-25 1.0864920e-23 9.7165277e-19
 1.0000000e+00 6.1922459e-18], sum to 1.0000
[2019-04-04 07:26:11,613] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3594
[2019-04-04 07:26:11,628] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.57976119363457, 0.4045296673174064, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4221000.0000, 
sim time next is 4221600.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.57213524669004, 0.3944156496743926, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6310112705575032, 0.6314718832247975, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5078604], dtype=float32), -0.95836025]. 
=============================================
[2019-04-04 07:26:13,696] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1279364e-10 3.1790112e-08 2.5713362e-26 1.2688763e-24 3.5056811e-19
 1.0000000e+00 6.0300696e-19], sum to 1.0000
[2019-04-04 07:26:13,696] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4044
[2019-04-04 07:26:13,744] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.39921398017045, 0.3404419947913297, 0.0, 1.0, 37985.83364233407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4243200.0000, 
sim time next is 4243800.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.40748120061664, 0.3397867600097772, 0.0, 1.0, 33362.18384059054], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6172901000513867, 0.6132622533365925, 0.0, 1.0, 0.15886754209805018], 
reward next is 0.8411, 
noisyNet noise sample is [array([0.01310497], dtype=float32), 1.0459445]. 
=============================================
[2019-04-04 07:26:15,923] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0403352e-10 1.5167703e-09 3.2724936e-27 4.9516747e-26 8.3330417e-21
 1.0000000e+00 1.2591821e-20], sum to 1.0000
[2019-04-04 07:26:15,924] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7764
[2019-04-04 07:26:15,934] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 58.66666666666666, 0.0, 0.0, 26.0, 25.62326972020574, 0.4820320644490272, 0.0, 1.0, 21524.35804448908], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4671600.0000, 
sim time next is 4672200.0000, 
raw observation next is [2.0, 60.33333333333334, 0.0, 0.0, 26.0, 25.56099286369711, 0.4720195267377707, 0.0, 1.0, 57597.34617035068], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.6033333333333334, 0.0, 0.0, 0.6666666666666666, 0.6300827386414257, 0.6573398422459236, 0.0, 1.0, 0.27427307700166986], 
reward next is 0.7257, 
noisyNet noise sample is [array([2.0468416], dtype=float32), -0.100100204]. 
=============================================
[2019-04-04 07:26:16,078] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.50958870e-10 5.73798253e-09 3.70477277e-26 4.08688152e-25
 1.71072389e-19 1.00000000e+00 1.02055786e-19], sum to 1.0000
[2019-04-04 07:26:16,084] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7268
[2019-04-04 07:26:16,104] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.40776618966363, 0.3398202970348267, 0.0, 1.0, 33331.44045243564], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4243800.0000, 
sim time next is 4244400.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.41073943989607, 0.3392723952389513, 0.0, 1.0, 34306.5750763921], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6175616199913391, 0.6130907984129838, 0.0, 1.0, 0.16336464322091476], 
reward next is 0.8366, 
noisyNet noise sample is [array([-0.9927526], dtype=float32), -0.2964735]. 
=============================================
[2019-04-04 07:26:16,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2491130e-10 1.5072944e-08 4.0039048e-26 2.0696549e-24 1.7610130e-19
 1.0000000e+00 1.5877726e-18], sum to 1.0000
[2019-04-04 07:26:16,287] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8361
[2019-04-04 07:26:16,305] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.39747526377631, 0.3437628220448272, 0.0, 1.0, 37949.21193845302], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4247400.0000, 
sim time next is 4248000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.47051928010826, 0.3418094801457032, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6225432733423549, 0.613936493381901, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7474142], dtype=float32), -0.20218499]. 
=============================================
[2019-04-04 07:26:16,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[82.60278 ]
 [82.61487 ]
 [82.602936]
 [82.5986  ]
 [82.635994]], R is [[82.6365509 ]
 [82.62947845]
 [82.58060455]
 [82.51081085]
 [82.52442169]].
[2019-04-04 07:26:17,015] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0693213e-11 1.2667628e-09 2.3656640e-27 5.4091133e-27 4.5702953e-21
 1.0000000e+00 1.1115544e-21], sum to 1.0000
[2019-04-04 07:26:17,017] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7914
[2019-04-04 07:26:17,045] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.866666666666667, 43.5, 143.0, 141.0, 26.0, 26.96688639955699, 0.809846351441932, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4637400.0000, 
sim time next is 4638000.0000, 
raw observation next is [5.733333333333333, 44.0, 130.0, 144.0, 26.0, 26.269570764432, 0.7449678769019701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6214219759926132, 0.44, 0.43333333333333335, 0.1591160220994475, 0.6666666666666666, 0.6891308970360001, 0.74832262563399, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.055732], dtype=float32), 0.22480166]. 
=============================================
[2019-04-04 07:26:17,074] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[85.14634 ]
 [85.46818 ]
 [85.813896]
 [86.164185]
 [86.67348 ]], R is [[84.9620285 ]
 [85.1124115 ]
 [85.2612915 ]
 [85.40867615]
 [85.55458832]].
[2019-04-04 07:26:17,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.70929251e-10 4.78629669e-09 4.38720389e-28 7.77209060e-26
 1.28740145e-20 1.00000000e+00 8.69446068e-21], sum to 1.0000
[2019-04-04 07:26:17,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9021
[2019-04-04 07:26:17,394] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 75.0, 0.0, 0.0, 26.0, 25.55480292959541, 0.4214073607138883, 0.0, 1.0, 18741.38038797822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4316400.0000, 
sim time next is 4317000.0000, 
raw observation next is [4.416666666666667, 75.16666666666667, 0.0, 0.0, 26.0, 25.59554401059147, 0.4193999030019886, 0.0, 1.0, 18738.67630892201], 
processed observation next is [0.0, 1.0, 0.584949215143121, 0.7516666666666667, 0.0, 0.0, 0.6666666666666666, 0.6329620008826226, 0.6397999676673295, 0.0, 1.0, 0.08923179194724767], 
reward next is 0.9108, 
noisyNet noise sample is [array([1.2413925], dtype=float32), 0.3036464]. 
=============================================
[2019-04-04 07:26:17,399] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[88.44428 ]
 [88.45037 ]
 [88.47899 ]
 [88.540436]
 [88.59182 ]], R is [[88.46259308]
 [88.48872375]
 [88.51457977]
 [88.49597931]
 [88.47421265]].
[2019-04-04 07:26:19,106] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.08979714e-10 7.93970223e-09 1.12842731e-26 7.08456750e-25
 1.16984328e-19 1.00000000e+00 1.51802441e-19], sum to 1.0000
[2019-04-04 07:26:19,107] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5784
[2019-04-04 07:26:19,125] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.61279245102901, 0.457042859990931, 0.0, 1.0, 35613.53136014599], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4573200.0000, 
sim time next is 4573800.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.42786339721343, 0.4480591028134877, 0.0, 1.0, 131072.3234256753], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6189886164344524, 0.6493530342711625, 0.0, 1.0, 0.6241539210746443], 
reward next is 0.3758, 
noisyNet noise sample is [array([0.01216432], dtype=float32), -1.177769]. 
=============================================
[2019-04-04 07:26:26,219] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4204582e-10 1.1721793e-08 2.9207134e-28 6.4367297e-26 6.6814989e-21
 1.0000000e+00 1.4204591e-20], sum to 1.0000
[2019-04-04 07:26:26,219] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2222
[2019-04-04 07:26:26,239] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 73.0, 0.0, 0.0, 26.0, 25.37820716584761, 0.4536743103513864, 0.0, 1.0, 38845.08339354803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4503600.0000, 
sim time next is 4504200.0000, 
raw observation next is [-0.9833333333333334, 73.0, 0.0, 0.0, 26.0, 25.44115634329395, 0.4457775866279047, 0.0, 1.0, 18763.20130841812], 
processed observation next is [1.0, 0.13043478260869565, 0.43536472760849493, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6200963619411626, 0.6485925288759682, 0.0, 1.0, 0.0893485776591339], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.22597386], dtype=float32), -0.5489026]. 
=============================================
[2019-04-04 07:26:26,934] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9771427e-10 7.7766948e-09 3.0338935e-25 2.8521578e-24 1.4025520e-18
 1.0000000e+00 4.0717064e-19], sum to 1.0000
[2019-04-04 07:26:26,937] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7724
[2019-04-04 07:26:26,943] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 97.0, 727.0, 26.0, 25.18501721192654, 0.4432620315263363, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4807800.0000, 
sim time next is 4808400.0000, 
raw observation next is [3.0, 37.0, 94.5, 697.3333333333334, 26.0, 25.18534690642576, 0.4426012744189542, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.315, 0.7705340699815838, 0.6666666666666666, 0.5987789088688134, 0.6475337581396514, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16520366], dtype=float32), -2.0596993]. 
=============================================
[2019-04-04 07:26:38,647] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.23923805e-11 8.22338475e-10 1.27431081e-29 7.57370314e-28
 2.45359117e-22 1.00000000e+00 3.04479660e-22], sum to 1.0000
[2019-04-04 07:26:38,648] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2291
[2019-04-04 07:26:38,659] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.0, 20.0, 114.5, 839.5, 26.0, 27.52139503633424, 0.9527854378444746, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5061600.0000, 
sim time next is 5062200.0000, 
raw observation next is [11.16666666666667, 19.83333333333334, 113.3333333333333, 832.6666666666667, 26.0, 27.89056090850255, 0.9952645795974627, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7719298245614037, 0.1983333333333334, 0.37777777777777766, 0.9200736648250462, 0.6666666666666666, 0.8242134090418792, 0.8317548598658209, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4952687], dtype=float32), -1.1580111]. 
=============================================
[2019-04-04 07:26:40,367] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0166103e-11 4.5758858e-10 1.1025937e-27 2.5270621e-26 1.4198137e-21
 1.0000000e+00 9.4292192e-21], sum to 1.0000
[2019-04-04 07:26:40,368] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2967
[2019-04-04 07:26:40,383] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:26:40,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:26:40,385] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.466666666666667, 45.0, 104.1666666666667, 146.6666666666667, 26.0, 27.08955463472303, 0.8244785127342104, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4639200.0000, 
sim time next is 4639800.0000, 
raw observation next is [5.333333333333334, 45.5, 91.33333333333334, 146.3333333333333, 26.0, 27.32512478768017, 0.8476509127366104, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6103416435826409, 0.455, 0.30444444444444446, 0.1616942909760589, 0.6666666666666666, 0.7770937323066809, 0.7825503042455368, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.564206], dtype=float32), 0.7055675]. 
=============================================
[2019-04-04 07:26:40,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run13
[2019-04-04 07:26:41,197] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:26:41,197] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:26:41,201] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run13
[2019-04-04 07:26:44,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:26:44,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:26:44,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run13
[2019-04-04 07:26:45,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:26:45,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:26:45,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run13
[2019-04-04 07:26:49,927] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.8341825e-12 2.0270901e-09 1.1449684e-28 1.9792357e-26 3.5079501e-22
 1.0000000e+00 4.5321004e-21], sum to 1.0000
[2019-04-04 07:26:49,927] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4698
[2019-04-04 07:26:50,011] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 67.0, 167.0, 524.0, 26.0, 25.61314759672197, 0.4665184886527061, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4787400.0000, 
sim time next is 4788000.0000, 
raw observation next is [-3.0, 65.0, 163.5, 575.5, 26.0, 25.57698934639584, 0.4615382050525825, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3795013850415513, 0.65, 0.545, 0.6359116022099448, 0.6666666666666666, 0.63141577886632, 0.6538460683508608, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29291192], dtype=float32), 0.14717333]. 
=============================================
[2019-04-04 07:26:50,015] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[89.07753]
 [89.41867]
 [89.7153 ]
 [89.87526]
 [89.87636]], R is [[88.8639679 ]
 [88.97532654]
 [89.08557129]
 [89.19471741]
 [89.30277252]].
[2019-04-04 07:26:51,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:26:51,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:26:51,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run13
[2019-04-04 07:26:51,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:26:51,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:26:51,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run13
[2019-04-04 07:26:53,106] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:26:53,107] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:26:53,110] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run13
[2019-04-04 07:26:54,455] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8890246e-11 2.1771949e-09 3.3029804e-28 2.5312685e-26 9.4113908e-22
 1.0000000e+00 6.8130973e-21], sum to 1.0000
[2019-04-04 07:26:54,455] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7433
[2019-04-04 07:26:54,473] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.433333333333333, 88.33333333333334, 0.0, 0.0, 26.0, 24.19240022771296, 0.09729269250240746, 0.0, 1.0, 42594.51462153073], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 96000.0000, 
sim time next is 96600.0000, 
raw observation next is [-2.616666666666667, 87.66666666666666, 0.0, 0.0, 26.0, 24.11331844208428, 0.09571525056384377, 0.0, 1.0, 42778.78708577724], 
processed observation next is [1.0, 0.08695652173913043, 0.3901200369344414, 0.8766666666666666, 0.0, 0.0, 0.6666666666666666, 0.5094432035070234, 0.5319050835212812, 0.0, 1.0, 0.20370850993227257], 
reward next is 0.7963, 
noisyNet noise sample is [array([1.6211973], dtype=float32), -1.3898662]. 
=============================================
[2019-04-04 07:27:01,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:27:01,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:27:01,530] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run13
[2019-04-04 07:27:01,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:27:01,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:27:02,003] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run13
[2019-04-04 07:27:03,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:27:03,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:27:03,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run13
[2019-04-04 07:27:04,397] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:27:04,397] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:27:04,400] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run13
[2019-04-04 07:27:06,232] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:27:06,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:27:06,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run13
[2019-04-04 07:27:06,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:27:06,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:27:06,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run13
[2019-04-04 07:27:07,013] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:27:07,013] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:27:07,016] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run13
[2019-04-04 07:27:08,424] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.0477330e-10 9.0566044e-09 1.8842103e-26 6.5351791e-25 4.3782938e-20
 1.0000000e+00 9.9284822e-20], sum to 1.0000
[2019-04-04 07:27:08,424] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4495
[2019-04-04 07:27:08,470] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.75, 19.0, 0.0, 0.0, 26.0, 26.82739328840258, 0.791588087235005, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5089800.0000, 
sim time next is 5090400.0000, 
raw observation next is [8.7, 19.0, 0.0, 0.0, 26.0, 26.77361193079548, 0.7795689769627919, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.703601108033241, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7311343275662899, 0.759856325654264, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9352378], dtype=float32), -2.335072]. 
=============================================
[2019-04-04 07:27:10,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:27:10,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:27:10,052] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run13
[2019-04-04 07:27:10,384] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.3569761e-12 2.7230747e-09 4.3869699e-28 2.4085631e-26 4.5710970e-21
 1.0000000e+00 5.4626289e-21], sum to 1.0000
[2019-04-04 07:27:10,385] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1668
[2019-04-04 07:27:10,440] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 61.0, 157.0, 308.0, 26.0, 25.57265863610166, 0.3914542556862253, 1.0, 1.0, 47602.41413190322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 129600.0000, 
sim time next is 130200.0000, 
raw observation next is [-8.3, 61.0, 148.0, 406.3333333333334, 26.0, 25.71184389530071, 0.4038812196402607, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.23268698060941828, 0.61, 0.49333333333333335, 0.44898710865561703, 0.6666666666666666, 0.6426536579417258, 0.6346270732134203, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3987585], dtype=float32), -0.21604757]. 
=============================================
[2019-04-04 07:27:10,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:27:10,714] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:27:10,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run13
[2019-04-04 07:27:20,803] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.6893707e-11 6.0294791e-09 3.5101080e-28 4.1826003e-26 1.3537109e-21
 1.0000000e+00 5.0906837e-20], sum to 1.0000
[2019-04-04 07:27:20,803] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9942
[2019-04-04 07:27:20,887] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 86.0, 0.0, 0.0, 26.0, 24.54594287459142, 0.1944084624649519, 0.0, 1.0, 42208.38880693354], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 61200.0000, 
sim time next is 61800.0000, 
raw observation next is [5.316666666666667, 86.5, 0.0, 0.0, 26.0, 24.59527120127176, 0.1945476055171012, 0.0, 1.0, 18737.71282386491], 
processed observation next is [0.0, 0.7391304347826086, 0.6098799630655587, 0.865, 0.0, 0.0, 0.6666666666666666, 0.5496059334393134, 0.5648492018390338, 0.0, 1.0, 0.08922720392316623], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.30724388], dtype=float32), -0.4454857]. 
=============================================
[2019-04-04 07:27:56,800] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5013363e-10 2.5697993e-09 7.8176100e-27 4.6635568e-25 5.3529273e-21
 1.0000000e+00 1.4931000e-18], sum to 1.0000
[2019-04-04 07:27:56,801] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4721
[2019-04-04 07:27:56,874] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.93652700834915, 0.279929529823899, 0.0, 1.0, 55423.96114403652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 586800.0000, 
sim time next is 587400.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.94051795388465, 0.2840133886212677, 0.0, 1.0, 45983.22836862898], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5783764961570542, 0.5946711295404226, 0.0, 1.0, 0.2189677541363285], 
reward next is 0.7810, 
noisyNet noise sample is [array([-0.9472975], dtype=float32), 0.86813796]. 
=============================================
[2019-04-04 07:27:57,887] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.01206155e-08 3.23117426e-08 5.69476605e-23 8.24044909e-22
 2.14635287e-17 1.00000000e+00 7.95792351e-17], sum to 1.0000
[2019-04-04 07:27:57,887] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8499
[2019-04-04 07:27:57,954] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.666666666666668, 40.66666666666667, 0.0, 0.0, 26.0, 25.11197160025682, 0.2896876005572346, 1.0, 1.0, 95384.57959184551], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 415200.0000, 
sim time next is 415800.0000, 
raw observation next is [-9.75, 41.0, 0.0, 0.0, 26.0, 25.08810655526753, 0.2949771493438629, 1.0, 1.0, 87545.82455675269], 
processed observation next is [1.0, 0.8260869565217391, 0.19252077562326872, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5906755462722941, 0.5983257164479543, 1.0, 1.0, 0.41688487884167946], 
reward next is 0.5831, 
noisyNet noise sample is [array([-0.70810574], dtype=float32), 0.40738007]. 
=============================================
[2019-04-04 07:28:08,927] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8386011e-11 5.7975420e-09 1.5209505e-28 6.1850910e-26 1.8503386e-21
 1.0000000e+00 1.4858994e-20], sum to 1.0000
[2019-04-04 07:28:08,927] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0933
[2019-04-04 07:28:08,994] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.966666666666667, 84.0, 0.0, 0.0, 26.0, 24.61228698035394, 0.1916546671522021, 0.0, 1.0, 40381.5434141131], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 535200.0000, 
sim time next is 535800.0000, 
raw observation next is [1.783333333333333, 84.5, 0.0, 0.0, 26.0, 24.58432731840342, 0.1867008508719897, 0.0, 1.0, 40464.9196910838], 
processed observation next is [0.0, 0.17391304347826086, 0.5120036934441367, 0.845, 0.0, 0.0, 0.6666666666666666, 0.5486939432002851, 0.5622336169573299, 0.0, 1.0, 0.19269009376706572], 
reward next is 0.8073, 
noisyNet noise sample is [array([0.7787756], dtype=float32), -0.6728747]. 
=============================================
[2019-04-04 07:28:12,123] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.0513458e-11 2.5383493e-09 1.8199427e-29 1.1064828e-27 5.6274782e-22
 1.0000000e+00 4.4679850e-21], sum to 1.0000
[2019-04-04 07:28:12,130] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7418
[2019-04-04 07:28:12,147] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.3, 88.0, 0.0, 0.0, 26.0, 24.927642114463, 0.2437813532758226, 0.0, 1.0, 39626.52366343861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 525600.0000, 
sim time next is 526200.0000, 
raw observation next is [4.216666666666667, 87.66666666666667, 0.0, 0.0, 26.0, 24.89696569251651, 0.2399983508712857, 0.0, 1.0, 39653.4434080095], 
processed observation next is [0.0, 0.08695652173913043, 0.5794090489381348, 0.8766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5747471410430425, 0.5799994502904285, 0.0, 1.0, 0.18882592099052142], 
reward next is 0.8112, 
noisyNet noise sample is [array([-0.03047851], dtype=float32), -0.8605014]. 
=============================================
[2019-04-04 07:28:20,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.0007899e-13 2.3824659e-10 1.5339358e-31 4.6447077e-30 1.7355263e-24
 1.0000000e+00 3.3426356e-24], sum to 1.0000
[2019-04-04 07:28:20,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8717
[2019-04-04 07:28:20,066] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.05, 89.5, 96.0, 0.0, 26.0, 26.60617084062624, 0.6549872715172586, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 988200.0000, 
sim time next is 988800.0000, 
raw observation next is [11.23333333333333, 88.33333333333333, 100.0, 0.0, 26.0, 26.62828950360224, 0.66160723288213, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7737765466297323, 0.8833333333333333, 0.3333333333333333, 0.0, 0.6666666666666666, 0.7190241253001867, 0.7205357442940433, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37989935], dtype=float32), -2.0823314]. 
=============================================
[2019-04-04 07:28:23,920] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0816286e-11 4.9612279e-09 1.4249314e-27 5.9192166e-26 2.2346690e-21
 1.0000000e+00 3.1512098e-20], sum to 1.0000
[2019-04-04 07:28:23,923] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2075
[2019-04-04 07:28:23,968] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.4164097221238, 0.1437870085213516, 0.0, 1.0, 38639.96947217781], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 887400.0000, 
sim time next is 888000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.43784658548977, 0.156063090250566, 0.0, 1.0, 38573.82863322292], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5364872154574808, 0.552021030083522, 0.0, 1.0, 0.18368489825344247], 
reward next is 0.8163, 
noisyNet noise sample is [array([-0.24914967], dtype=float32), 0.3529191]. 
=============================================
[2019-04-04 07:28:23,985] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.94627]
 [84.00358]
 [84.04404]
 [84.09516]
 [84.12352]], R is [[83.88359833]
 [83.86076355]
 [83.83795166]
 [83.81508636]
 [83.79208374]].
[2019-04-04 07:28:25,341] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.4977919e-12 2.3150362e-09 8.0532074e-31 1.8074613e-29 5.2772150e-23
 1.0000000e+00 1.0678338e-22], sum to 1.0000
[2019-04-04 07:28:25,344] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6315
[2019-04-04 07:28:25,352] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.0, 77.0, 0.0, 0.0, 26.0, 25.9156054019178, 0.591101125226258, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1042800.0000, 
sim time next is 1043400.0000, 
raw observation next is [13.9, 77.5, 0.0, 0.0, 26.0, 25.81389063268284, 0.579897273662448, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.847645429362881, 0.775, 0.0, 0.0, 0.6666666666666666, 0.6511575527235699, 0.693299091220816, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-3.2568479], dtype=float32), -0.07239207]. 
=============================================
[2019-04-04 07:28:26,015] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4168114e-09 3.6796095e-08 1.4164460e-25 1.8453741e-23 1.5247159e-19
 1.0000000e+00 7.5775235e-19], sum to 1.0000
[2019-04-04 07:28:26,020] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5338
[2019-04-04 07:28:26,053] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 54.0, 34.0, 2.5, 26.0, 25.97622768856376, 0.4398636406175685, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 752400.0000, 
sim time next is 753000.0000, 
raw observation next is [-2.983333333333333, 54.33333333333333, 0.0, 0.0, 26.0, 25.98305632285437, 0.4267984245394964, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.37996306555863346, 0.5433333333333333, 0.0, 0.0, 0.6666666666666666, 0.6652546935711975, 0.6422661415131655, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.752095], dtype=float32), -0.47877768]. 
=============================================
[2019-04-04 07:28:26,062] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.47203 ]
 [77.465195]
 [77.425964]
 [77.52619 ]
 [77.45155 ]], R is [[77.62633514]
 [77.85007477]
 [78.07157135]
 [78.29085541]
 [78.50794983]].
[2019-04-04 07:28:26,741] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9714908e-11 1.9859224e-10 4.9555953e-30 2.6706447e-28 2.4688951e-22
 1.0000000e+00 3.4332846e-21], sum to 1.0000
[2019-04-04 07:28:26,743] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2516
[2019-04-04 07:28:26,798] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.9, 92.16666666666667, 12.0, 0.0, 26.0, 24.51019020644011, 0.308786619859154, 1.0, 1.0, 196475.4379351738], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 924600.0000, 
sim time next is 925200.0000, 
raw observation next is [5.0, 92.0, 9.0, 0.0, 26.0, 24.98663989512217, 0.3530730837438517, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6011080332409973, 0.92, 0.03, 0.0, 0.6666666666666666, 0.5822199912601809, 0.6176910279146172, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06798209], dtype=float32), 1.2333976]. 
=============================================
[2019-04-04 07:28:36,281] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.4685249e-10 1.5606352e-08 2.5320714e-25 4.3546992e-24 3.8446163e-19
 1.0000000e+00 1.1781423e-18], sum to 1.0000
[2019-04-04 07:28:36,281] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0406
[2019-04-04 07:28:36,311] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.73333333333334, 54.33333333333334, 0.0, 0.0, 26.0, 26.49482871840005, 0.7746503049422818, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1102800.0000, 
sim time next is 1103400.0000, 
raw observation next is [15.55, 55.0, 0.0, 0.0, 26.0, 26.47225811067944, 0.7600300936241794, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8933518005540168, 0.55, 0.0, 0.0, 0.6666666666666666, 0.7060215092232868, 0.7533433645413932, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8047051], dtype=float32), 0.23114638]. 
=============================================
[2019-04-04 07:28:41,186] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1938830e-08 4.4545450e-07 1.1554523e-24 8.8054295e-23 2.0424913e-18
 9.9999952e-01 9.6071742e-18], sum to 1.0000
[2019-04-04 07:28:41,188] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2480
[2019-04-04 07:28:41,192] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.08333333333333, 95.5, 0.0, 0.0, 26.0, 23.69219307880338, 0.1869189160835803, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1227000.0000, 
sim time next is 1227600.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.66923626586728, 0.1823068121841675, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.47243635548894, 0.5607689373947226, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6537439], dtype=float32), 1.1207532]. 
=============================================
[2019-04-04 07:28:41,811] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.9802704e-11 7.7141415e-10 5.6705131e-29 4.6274188e-27 2.5632815e-22
 1.0000000e+00 3.7538333e-21], sum to 1.0000
[2019-04-04 07:28:41,813] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7427
[2019-04-04 07:28:41,863] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 98.0, 95.0, 0.0, 26.0, 25.00590812527077, 0.47951702973714, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1251000.0000, 
sim time next is 1251600.0000, 
raw observation next is [14.4, 97.33333333333334, 96.0, 0.0, 26.0, 25.04385335880163, 0.4825128444538711, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 0.9733333333333334, 0.32, 0.0, 0.6666666666666666, 0.5869877799001358, 0.660837614817957, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01098534], dtype=float32), 0.37064505]. 
=============================================
[2019-04-04 07:28:50,568] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.9432420e-12 1.7554541e-10 4.0549721e-31 1.9437749e-28 1.4983741e-23
 1.0000000e+00 1.3788256e-22], sum to 1.0000
[2019-04-04 07:28:50,571] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2755
[2019-04-04 07:28:50,582] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.73333333333333, 100.0, 15.83333333333333, 0.0, 26.0, 24.59638454213935, 0.4222132050827792, 0.0, 1.0, 27591.97117206172], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1269600.0000, 
sim time next is 1270200.0000, 
raw observation next is [12.46666666666667, 100.0, 12.66666666666667, 0.0, 26.0, 24.59365702715257, 0.4231401653433213, 0.0, 1.0, 27421.75056704666], 
processed observation next is [0.0, 0.6956521739130435, 0.8079409048938138, 1.0, 0.04222222222222223, 0.0, 0.6666666666666666, 0.5494714189293809, 0.6410467217811071, 0.0, 1.0, 0.1305797646049841], 
reward next is 0.8694, 
noisyNet noise sample is [array([-0.21892272], dtype=float32), -1.3062048]. 
=============================================
[2019-04-04 07:28:51,130] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0718147e-11 3.6496774e-09 4.7961336e-30 1.0795738e-27 1.9884725e-22
 1.0000000e+00 1.3592580e-21], sum to 1.0000
[2019-04-04 07:28:51,134] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8754
[2019-04-04 07:28:51,162] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 49.0, 171.5, 20.66666666666666, 26.0, 26.88419818524511, 0.7823470058273713, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1603200.0000, 
sim time next is 1603800.0000, 
raw observation next is [13.8, 49.0, 176.0, 0.0, 26.0, 26.92385349646387, 0.8084297584184026, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.844875346260388, 0.49, 0.5866666666666667, 0.0, 0.6666666666666666, 0.7436544580386558, 0.7694765861394676, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.4177444], dtype=float32), -1.3411328]. 
=============================================
[2019-04-04 07:28:52,755] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.1216139e-13 2.3251465e-10 8.8463469e-29 2.2791804e-27 2.7259355e-22
 1.0000000e+00 1.2335501e-22], sum to 1.0000
[2019-04-04 07:28:52,756] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3987
[2019-04-04 07:28:52,782] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 41.33333333333334, 0.0, 26.0, 25.91991875428627, 0.4057185545253616, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1438800.0000, 
sim time next is 1439400.0000, 
raw observation next is [1.1, 92.0, 36.66666666666667, 0.0, 26.0, 25.39024309344087, 0.4281410665955603, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.12222222222222223, 0.0, 0.6666666666666666, 0.6158535911200724, 0.6427136888651868, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.50695956], dtype=float32), -1.6963202]. 
=============================================
[2019-04-04 07:28:52,858] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1767359e-11 3.0974352e-09 4.0553960e-28 5.4994109e-27 5.6692356e-22
 1.0000000e+00 4.4091670e-20], sum to 1.0000
[2019-04-04 07:28:52,858] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9956
[2019-04-04 07:28:52,872] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.3, 62.0, 0.0, 0.0, 26.0, 25.75102116934796, 0.7034232290404389, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1112400.0000, 
sim time next is 1113000.0000, 
raw observation next is [13.2, 62.33333333333334, 0.0, 0.0, 26.0, 25.80543684575658, 0.7033146419125189, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8282548476454294, 0.6233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6504530704797151, 0.7344382139708396, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8969053], dtype=float32), 0.96499085]. 
=============================================
[2019-04-04 07:28:52,889] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[88.23749 ]
 [87.68282 ]
 [86.80676 ]
 [86.038956]
 [84.73009 ]], R is [[88.96591949]
 [89.07626343]
 [89.09634399]
 [88.87162018]
 [88.27529907]].
[2019-04-04 07:28:58,110] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4944109e-12 4.4257376e-10 7.6923764e-30 8.2622641e-29 7.2292677e-23
 1.0000000e+00 1.8674127e-22], sum to 1.0000
[2019-04-04 07:28:58,110] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3924
[2019-04-04 07:28:58,123] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 100.1666666666667, 0.0, 26.0, 25.69187157384454, 0.534121363092705, 1.0, 1.0, 26890.26780051531], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1345200.0000, 
sim time next is 1345800.0000, 
raw observation next is [1.1, 92.0, 94.33333333333333, 0.0, 26.0, 25.73437045236918, 0.5418331281569696, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.3144444444444444, 0.0, 0.6666666666666666, 0.644530871030765, 0.6806110427189899, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2270187], dtype=float32), 0.24742417]. 
=============================================
[2019-04-04 07:28:58,947] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.2093399e-12 3.6226208e-10 1.3032568e-29 2.8621356e-28 4.2304432e-22
 1.0000000e+00 7.6889462e-22], sum to 1.0000
[2019-04-04 07:28:58,947] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5067
[2019-04-04 07:28:58,969] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 84.66666666666667, 99.0, 0.0, 26.0, 25.85256138246571, 0.521292264311667, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1685400.0000, 
sim time next is 1686000.0000, 
raw observation next is [1.1, 85.33333333333334, 103.0, 0.0, 26.0, 25.85107333081093, 0.4987437310684121, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.8533333333333334, 0.3433333333333333, 0.0, 0.6666666666666666, 0.6542561109009108, 0.6662479103561374, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3866731], dtype=float32), 0.0030719317]. 
=============================================
[2019-04-04 07:28:58,988] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[91.19504 ]
 [91.327866]
 [91.440765]
 [91.56134 ]
 [91.678764]], R is [[91.18498993]
 [91.27313995]
 [91.3604126 ]
 [91.44680786]
 [91.532341  ]].
[2019-04-04 07:29:09,353] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.43484521e-11 9.43639722e-10 1.52779388e-29 1.91024170e-27
 1.04627764e-22 1.00000000e+00 1.48298802e-21], sum to 1.0000
[2019-04-04 07:29:09,353] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3395
[2019-04-04 07:29:09,373] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 49.0, 160.5, 0.0, 26.0, 27.30239239255057, 0.8474264890798135, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1605600.0000, 
sim time next is 1606200.0000, 
raw observation next is [13.8, 49.0, 155.3333333333333, 0.0, 26.0, 27.31025844841956, 0.8572711979612752, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.844875346260388, 0.49, 0.5177777777777777, 0.0, 0.6666666666666666, 0.7758548707016301, 0.7857570659870917, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.77859515], dtype=float32), -1.0001171]. 
=============================================
[2019-04-04 07:29:09,403] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9934616e-10 1.7921451e-09 4.8531918e-28 6.1852562e-26 2.8789765e-21
 1.0000000e+00 6.5219967e-20], sum to 1.0000
[2019-04-04 07:29:09,406] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3956
[2019-04-04 07:29:09,422] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.433333333333333, 92.0, 0.0, 0.0, 26.0, 25.3106081488815, 0.483493318366705, 0.0, 1.0, 60988.41839000902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1464000.0000, 
sim time next is 1464600.0000, 
raw observation next is [1.516666666666667, 92.0, 0.0, 0.0, 26.0, 25.31672357607782, 0.4875627871732768, 0.0, 1.0, 46622.74637416694], 
processed observation next is [1.0, 0.9565217391304348, 0.5046168051708219, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6097269646731517, 0.662520929057759, 0.0, 1.0, 0.22201307797222353], 
reward next is 0.7780, 
noisyNet noise sample is [array([-0.16368453], dtype=float32), 0.5538002]. 
=============================================
[2019-04-04 07:29:14,334] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7858642e-09 1.1956239e-07 7.1060842e-25 3.2908270e-23 1.5553880e-18
 9.9999988e-01 2.5525725e-18], sum to 1.0000
[2019-04-04 07:29:14,339] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8753
[2019-04-04 07:29:14,359] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.91090293483037, -0.03593548115421654, 0.0, 1.0, 44743.60724809644], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1913400.0000, 
sim time next is 1914000.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.84220468040011, -0.04368505932205657, 0.0, 1.0, 44849.5148479788], 
processed observation next is [1.0, 0.13043478260869565, 0.2299168975069252, 0.78, 0.0, 0.0, 0.6666666666666666, 0.48685039003334235, 0.4854383135593145, 0.0, 1.0, 0.21356911832370856], 
reward next is 0.7864, 
noisyNet noise sample is [array([-0.5863614], dtype=float32), 1.1027048]. 
=============================================
[2019-04-04 07:29:14,382] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[75.50643 ]
 [75.63287 ]
 [75.75989 ]
 [75.845955]
 [75.902016]], R is [[75.42086029]
 [75.45358276]
 [75.48643494]
 [75.51812744]
 [75.54873657]].
[2019-04-04 07:29:15,099] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0057405e-09 7.0245221e-08 4.0415044e-24 6.9428563e-23 1.0528137e-18
 9.9999988e-01 8.5513882e-18], sum to 1.0000
[2019-04-04 07:29:15,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9072
[2019-04-04 07:29:15,211] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.082246176207, -0.0971919606688255, 1.0, 1.0, 202359.7642434866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1927200.0000, 
sim time next is 1927800.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.62482582125511, 0.009840905635030853, 1.0, 1.0, 203280.4378791368], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4687354851045926, 0.5032803018783436, 1.0, 1.0, 0.9680020851387466], 
reward next is 0.0320, 
noisyNet noise sample is [array([-1.0039232], dtype=float32), -0.41022393]. 
=============================================
[2019-04-04 07:29:33,490] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.06573694e-09 1.21699895e-08 5.19186734e-25 8.82059141e-24
 2.01361184e-19 1.00000000e+00 1.56891533e-18], sum to 1.0000
[2019-04-04 07:29:33,491] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5945
[2019-04-04 07:29:33,525] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.4, 78.33333333333334, 0.0, 0.0, 26.0, 24.40964691229294, 0.1645686071385101, 0.0, 1.0, 45781.04206257057], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1816800.0000, 
sim time next is 1817400.0000, 
raw observation next is [-5.5, 78.16666666666666, 0.0, 0.0, 26.0, 24.37724949106302, 0.1595942677116169, 0.0, 1.0, 45854.25793800203], 
processed observation next is [0.0, 0.0, 0.3102493074792244, 0.7816666666666666, 0.0, 0.0, 0.6666666666666666, 0.5314374575885848, 0.5531980892372056, 0.0, 1.0, 0.2183536092285811], 
reward next is 0.7816, 
noisyNet noise sample is [array([0.07706439], dtype=float32), -1.8325701]. 
=============================================
[2019-04-04 07:29:34,243] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0940463e-09 1.7652468e-08 3.5768462e-25 2.4505779e-23 1.2300472e-18
 1.0000000e+00 6.9795201e-18], sum to 1.0000
[2019-04-04 07:29:34,244] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8835
[2019-04-04 07:29:34,258] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 26.0, 24.64900015062855, 0.1569397455878737, 0.0, 1.0, 44814.0864979174], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1893600.0000, 
sim time next is 1894200.0000, 
raw observation next is [-6.383333333333334, 75.66666666666667, 0.0, 0.0, 26.0, 24.61099300354116, 0.1488877435063332, 0.0, 1.0, 44841.1875464525], 
processed observation next is [0.0, 0.9565217391304348, 0.28578024007386893, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.55091608362843, 0.5496292478354444, 0.0, 1.0, 0.21352946450691668], 
reward next is 0.7865, 
noisyNet noise sample is [array([0.824064], dtype=float32), -0.40339464]. 
=============================================
[2019-04-04 07:29:35,576] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4741298e-10 1.0411847e-08 9.0557611e-27 6.7240576e-25 1.2488351e-20
 1.0000000e+00 2.3783508e-19], sum to 1.0000
[2019-04-04 07:29:35,590] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6535
[2019-04-04 07:29:35,604] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666667, 86.0, 0.0, 0.0, 26.0, 24.63242051747679, 0.2098084017840577, 0.0, 1.0, 42742.73339635407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2082000.0000, 
sim time next is 2082600.0000, 
raw observation next is [-4.75, 86.0, 0.0, 0.0, 26.0, 24.62384574435463, 0.2016989458205753, 0.0, 1.0, 42796.58379403744], 
processed observation next is [1.0, 0.08695652173913043, 0.3310249307479225, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5519871453628857, 0.5672329819401918, 0.0, 1.0, 0.20379325616208305], 
reward next is 0.7962, 
noisyNet noise sample is [array([-0.35846442], dtype=float32), 1.3361331]. 
=============================================
[2019-04-04 07:29:48,919] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.5617910e-10 1.2420038e-08 1.8243453e-26 1.0053963e-24 2.9488365e-20
 1.0000000e+00 3.8396918e-19], sum to 1.0000
[2019-04-04 07:29:48,919] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0255
[2019-04-04 07:29:49,025] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.1, 86.5, 29.0, 0.0, 26.0, 25.46904718861826, 0.2988364197561647, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2017800.0000, 
sim time next is 2018400.0000, 
raw observation next is [-6.066666666666666, 86.33333333333333, 35.66666666666666, 0.0, 26.0, 25.63111456143667, 0.3137377552148049, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2945521698984303, 0.8633333333333333, 0.11888888888888886, 0.0, 0.6666666666666666, 0.635926213453056, 0.6045792517382683, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1992915], dtype=float32), 0.430229]. 
=============================================
[2019-04-04 07:30:31,508] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3318219e-09 1.3769405e-08 3.2556748e-25 1.4260709e-23 5.5136329e-19
 1.0000000e+00 1.6797496e-18], sum to 1.0000
[2019-04-04 07:30:31,509] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6797
[2019-04-04 07:30:31,546] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.1, 58.0, 0.0, 0.0, 26.0, 25.26992018373328, 0.3552117471373228, 0.0, 1.0, 39136.10283157401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2328000.0000, 
sim time next is 2328600.0000, 
raw observation next is [-2.2, 58.5, 0.0, 0.0, 26.0, 25.18886297456008, 0.3423647455157446, 0.0, 1.0, 38787.40772606265], 
processed observation next is [1.0, 0.9565217391304348, 0.4016620498614959, 0.585, 0.0, 0.0, 0.6666666666666666, 0.5990719145466734, 0.6141215818385816, 0.0, 1.0, 0.1847019415526793], 
reward next is 0.8153, 
noisyNet noise sample is [array([-1.5376478], dtype=float32), -0.0036630179]. 
=============================================
[2019-04-04 07:30:31,968] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.1265804e-09 1.4136114e-07 1.2346557e-23 2.1196912e-22 1.7390912e-17
 9.9999988e-01 3.3865549e-17], sum to 1.0000
[2019-04-04 07:30:31,968] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0736
[2019-04-04 07:30:32,061] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8666666666666667, 29.0, 89.5, 842.8333333333334, 26.0, 24.9122262737366, 0.2626160083225564, 0.0, 1.0, 26143.37100430447], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2464800.0000, 
sim time next is 2465400.0000, 
raw observation next is [1.233333333333333, 28.5, 89.0, 840.6666666666666, 26.0, 24.91859079848704, 0.2696375241517449, 0.0, 1.0, 18719.87687657799], 
processed observation next is [0.0, 0.5217391304347826, 0.49676823638042483, 0.285, 0.2966666666666667, 0.9289134438305708, 0.6666666666666666, 0.5765492332072532, 0.5898791747172484, 0.0, 1.0, 0.08914227084084757], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.32503906], dtype=float32), 1.7101378]. 
=============================================
[2019-04-04 07:30:53,855] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2990206e-09 1.2822556e-08 5.1380230e-24 6.7330513e-22 6.8041995e-18
 1.0000000e+00 3.7141574e-17], sum to 1.0000
[2019-04-04 07:30:53,856] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1668
[2019-04-04 07:30:53,892] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 38.0, 0.0, 0.0, 26.0, 25.08002711008614, 0.2199969999297073, 0.0, 1.0, 39481.49782496073], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2505600.0000, 
sim time next is 2506200.0000, 
raw observation next is [-1.7, 38.33333333333334, 0.0, 0.0, 26.0, 25.05171598808378, 0.2141952259963923, 0.0, 1.0, 39430.52558757641], 
processed observation next is [1.0, 0.0, 0.4155124653739613, 0.3833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5876429990069818, 0.5713984086654641, 0.0, 1.0, 0.18776440755988769], 
reward next is 0.8122, 
noisyNet noise sample is [array([-0.37882015], dtype=float32), -0.7209118]. 
=============================================
[2019-04-04 07:30:57,569] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 07:30:57,569] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:30:57,570] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:30:57,572] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run18
[2019-04-04 07:30:57,638] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:30:57,638] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:30:57,640] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run18
[2019-04-04 07:30:57,705] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:30:57,705] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:30:57,707] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run18
[2019-04-04 07:31:32,990] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.24660403], dtype=float32), 0.24633567]
[2019-04-04 07:31:32,990] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-12.2, 64.0, 0.0, 0.0, 26.0, 24.18705793291628, 0.0760274983507616, 0.0, 1.0, 43910.98420150529]
[2019-04-04 07:31:32,990] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 07:31:32,991] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.1763024e-09 5.5927408e-08 6.9020312e-23 3.1542505e-21 2.7534702e-17
 9.9999988e-01 1.5622578e-16], sampled 0.6392067129241602
[2019-04-04 07:33:12,768] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.24660403], dtype=float32), 0.24633567]
[2019-04-04 07:33:12,768] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.302993013, 79.80014609333334, 0.0, 0.0, 26.0, 24.42362403139343, 0.1375087214442574, 0.0, 1.0, 41496.82838340002]
[2019-04-04 07:33:12,768] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 07:33:12,769] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.1707778e-09 4.2555286e-08 7.4671399e-25 3.8064476e-23 9.5361207e-19
 1.0000000e+00 3.3276335e-18], sampled 0.3571495466268424
[2019-04-04 07:34:09,182] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 07:34:38,540] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.8201 263377775.6296 1552.0399
[2019-04-04 07:34:46,129] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 07:34:47,176] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1700000, evaluation results [1700000.0, 7241.820116049566, 263377775.62958866, 1552.0399168798824, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 07:34:56,070] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0074635e-09 2.0427054e-08 8.1302155e-25 2.3816052e-23 1.1951625e-18
 1.0000000e+00 5.1754536e-18], sum to 1.0000
[2019-04-04 07:34:56,070] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7029
[2019-04-04 07:34:56,119] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 25.17282556512783, 0.355670113010083, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2571000.0000, 
sim time next is 2571600.0000, 
raw observation next is [0.1333333333333334, 35.33333333333334, 0.0, 0.0, 26.0, 25.22870236011997, 0.3476266848150986, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.46629732225300097, 0.35333333333333344, 0.0, 0.0, 0.6666666666666666, 0.602391863343331, 0.6158755616050329, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.63319373], dtype=float32), 1.4988757]. 
=============================================
[2019-04-04 07:34:57,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3023524e-10 6.1931722e-09 1.4613753e-25 2.9630383e-24 1.2436396e-19
 1.0000000e+00 3.0103501e-19], sum to 1.0000
[2019-04-04 07:34:57,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0176
[2019-04-04 07:34:57,359] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8, 56.00000000000001, 0.0, 0.0, 26.0, 25.37573464568249, 0.3632743357899826, 1.0, 1.0, 18689.24543405809], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2658000.0000, 
sim time next is 2658600.0000, 
raw observation next is [-0.8999999999999999, 57.0, 0.0, 0.0, 26.0, 25.16842581132667, 0.3464219517566686, 1.0, 1.0, 47712.58838862893], 
processed observation next is [1.0, 0.782608695652174, 0.43767313019390586, 0.57, 0.0, 0.0, 0.6666666666666666, 0.5973688176105559, 0.6154739839188895, 1.0, 1.0, 0.22720280185061395], 
reward next is 0.7728, 
noisyNet noise sample is [array([1.148104], dtype=float32), 0.45325094]. 
=============================================
[2019-04-04 07:35:12,028] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.7811225e-10 3.3322422e-08 7.2399958e-25 8.0179546e-24 1.1269521e-18
 1.0000000e+00 1.8863180e-18], sum to 1.0000
[2019-04-04 07:35:12,037] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1600
[2019-04-04 07:35:12,080] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.72982884361428, 0.2520264755064147, 0.0, 1.0, 42375.7080492326], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2769000.0000, 
sim time next is 2769600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.78589496416924, 0.2505675977867731, 0.0, 1.0, 42218.72170452648], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5654912470141035, 0.5835225325955911, 0.0, 1.0, 0.20104153192631655], 
reward next is 0.7990, 
noisyNet noise sample is [array([1.5661753], dtype=float32), 1.6261512]. 
=============================================
[2019-04-04 07:35:22,896] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0464117e-10 4.9455822e-09 2.5511885e-27 3.1692245e-25 4.2231755e-20
 1.0000000e+00 2.6784416e-19], sum to 1.0000
[2019-04-04 07:35:22,896] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6980
[2019-04-04 07:35:22,974] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 119.0, 90.5, 26.0, 25.22410845245643, 0.3572380776155487, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2970000.0000, 
sim time next is 2970600.0000, 
raw observation next is [-4.0, 71.0, 130.6666666666667, 104.3333333333333, 26.0, 25.29266064542553, 0.3539504858172739, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.4355555555555557, 0.11528545119705337, 0.6666666666666666, 0.6077217204521276, 0.6179834952724247, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10650924], dtype=float32), 0.18077323]. 
=============================================
[2019-04-04 07:35:23,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8036980e-10 1.1264834e-07 2.7906706e-25 8.4306125e-24 1.6696037e-19
 9.9999988e-01 1.5537216e-18], sum to 1.0000
[2019-04-04 07:35:23,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4332
[2019-04-04 07:35:23,852] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.083333333333334, 76.83333333333334, 0.0, 0.0, 26.0, 24.66028106469301, 0.2671806405980895, 0.0, 1.0, 43998.25219366558], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3298200.0000, 
sim time next is 3298800.0000, 
raw observation next is [-9.266666666666667, 76.66666666666667, 0.0, 0.0, 26.0, 24.65952195806871, 0.2604934506617724, 0.0, 1.0, 43749.08787626091], 
processed observation next is [1.0, 0.17391304347826086, 0.20590951061865187, 0.7666666666666667, 0.0, 0.0, 0.6666666666666666, 0.5549601631723924, 0.5868311502205908, 0.0, 1.0, 0.20832898988695672], 
reward next is 0.7917, 
noisyNet noise sample is [array([-2.430542], dtype=float32), -0.37816617]. 
=============================================
[2019-04-04 07:35:28,117] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7697657e-11 8.3066193e-10 2.0302773e-29 7.5535958e-27 3.9784871e-22
 1.0000000e+00 8.1941191e-22], sum to 1.0000
[2019-04-04 07:35:28,127] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6004
[2019-04-04 07:35:28,144] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.833333333333333, 94.16666666666666, 0.0, 0.0, 26.0, 25.5068481515744, 0.5894849303285792, 0.0, 1.0, 57106.68160286587], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3197400.0000, 
sim time next is 3198000.0000, 
raw observation next is [1.666666666666667, 95.33333333333334, 0.0, 0.0, 26.0, 25.47356727516603, 0.5882747036919711, 0.0, 1.0, 61749.93797262498], 
processed observation next is [1.0, 0.0, 0.5087719298245615, 0.9533333333333335, 0.0, 0.0, 0.6666666666666666, 0.6227972729305025, 0.6960915678973237, 0.0, 1.0, 0.2940473236791666], 
reward next is 0.7060, 
noisyNet noise sample is [array([-0.81588894], dtype=float32), -0.864382]. 
=============================================
[2019-04-04 07:35:28,150] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[89.53406 ]
 [89.498856]
 [89.88245 ]
 [90.152214]
 [90.49849 ]], R is [[89.41241455]
 [89.24635315]
 [89.16860962]
 [89.18769073]
 [89.29581451]].
[2019-04-04 07:35:31,537] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.3097709e-09 3.7772939e-08 1.2512986e-23 2.5099853e-22 9.8514159e-18
 1.0000000e+00 1.0946159e-17], sum to 1.0000
[2019-04-04 07:35:31,538] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4865
[2019-04-04 07:35:31,546] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8333333333333334, 39.83333333333334, 75.0, 610.6666666666667, 26.0, 25.20595805853744, 0.3725544014672276, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3081000.0000, 
sim time next is 3081600.0000, 
raw observation next is [1.0, 40.0, 70.5, 579.5, 26.0, 25.20794857754612, 0.3650245197997903, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.4903047091412743, 0.4, 0.235, 0.6403314917127072, 0.6666666666666666, 0.6006623814621767, 0.6216748399332634, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.73152643], dtype=float32), -1.1388589]. 
=============================================
[2019-04-04 07:35:36,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1723325e-10 2.0467135e-09 1.9337473e-27 8.2116883e-26 3.3484477e-21
 1.0000000e+00 2.5944690e-20], sum to 1.0000
[2019-04-04 07:35:36,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1429
[2019-04-04 07:35:37,006] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23974281289097, 0.3792054194474568, 0.0, 1.0, 41599.79775912599], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3480000.0000, 
sim time next is 3480600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.25551578893704, 0.3641487800766691, 0.0, 1.0, 45836.85980933642], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6046263157447532, 0.6213829266922231, 0.0, 1.0, 0.2182707609968401], 
reward next is 0.7817, 
noisyNet noise sample is [array([0.14910339], dtype=float32), -0.5744433]. 
=============================================
[2019-04-04 07:35:41,009] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.0782673e-10 1.5182986e-09 1.0214661e-25 8.7829150e-25 1.1196029e-19
 1.0000000e+00 3.1163171e-19], sum to 1.0000
[2019-04-04 07:35:41,010] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3231
[2019-04-04 07:35:41,034] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.74906857921675, 0.58790739915641, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3261600.0000, 
sim time next is 3262200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.72216670017934, 0.4708388163795121, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6435138916816117, 0.656946272126504, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0254463], dtype=float32), -0.061981235]. 
=============================================
[2019-04-04 07:35:44,382] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.51547653e-12 9.96276811e-11 1.29105320e-31 7.93350549e-29
 1.30581865e-23 1.00000000e+00 9.23699164e-24], sum to 1.0000
[2019-04-04 07:35:44,388] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2915
[2019-04-04 07:35:44,427] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 110.1666666666667, 798.8333333333334, 26.0, 27.46541616237553, 0.8999449068088539, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3159600.0000, 
sim time next is 3160200.0000, 
raw observation next is [7.0, 100.0, 108.3333333333333, 791.6666666666666, 26.0, 27.53708494384357, 0.9156349512322698, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.361111111111111, 0.8747697974217311, 0.6666666666666666, 0.7947570786536309, 0.8052116504107566, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.722429], dtype=float32), 0.6527055]. 
=============================================
[2019-04-04 07:35:49,323] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4747887e-09 2.4857245e-08 2.7902131e-25 6.2652193e-24 1.9100673e-19
 1.0000000e+00 6.3038804e-19], sum to 1.0000
[2019-04-04 07:35:49,324] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8367
[2019-04-04 07:35:49,338] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.97571162147957, 0.3503904900156018, 0.0, 1.0, 41454.70592634379], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3370800.0000, 
sim time next is 3371400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.96371026169886, 0.3429652785585509, 0.0, 1.0, 41423.37609965451], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5803091884749051, 0.614321759519517, 0.0, 1.0, 0.19725417190311673], 
reward next is 0.8027, 
noisyNet noise sample is [array([0.06922803], dtype=float32), 0.70283616]. 
=============================================
[2019-04-04 07:35:52,623] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3291827e-09 5.3865228e-09 1.8345342e-26 3.7783348e-25 6.0760623e-20
 1.0000000e+00 1.4355051e-19], sum to 1.0000
[2019-04-04 07:35:52,623] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2885
[2019-04-04 07:35:52,650] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.28347715669839, 0.4300466041573595, 0.0, 1.0, 43705.85957382304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3800400.0000, 
sim time next is 3801000.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.29101660599099, 0.4254303490786949, 0.0, 1.0, 43692.16324521752], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6075847171659158, 0.641810116359565, 0.0, 1.0, 0.20805792021532152], 
reward next is 0.7919, 
noisyNet noise sample is [array([-1.2961698], dtype=float32), 0.70799905]. 
=============================================
[2019-04-04 07:35:52,665] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.67959 ]
 [80.727684]
 [80.754   ]
 [80.623116]
 [80.56228 ]], R is [[80.62955475]
 [80.61513519]
 [80.60051727]
 [80.58480072]
 [80.56507874]].
[2019-04-04 07:35:56,741] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2467835e-10 3.5412908e-09 1.4431209e-26 1.4474503e-24 1.6975469e-19
 1.0000000e+00 1.5509970e-19], sum to 1.0000
[2019-04-04 07:35:56,744] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8896
[2019-04-04 07:35:56,760] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.166666666666667, 51.5, 0.0, 0.0, 26.0, 25.91242723504627, 0.588421972439448, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3520200.0000, 
sim time next is 3520800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 26.04723469976449, 0.5910601754436543, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6706028916470409, 0.6970200584812182, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6654656], dtype=float32), -0.12725416]. 
=============================================
[2019-04-04 07:36:00,319] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4528542e-10 1.0657918e-09 8.4863884e-27 6.2499835e-26 1.4930529e-20
 1.0000000e+00 3.5297936e-20], sum to 1.0000
[2019-04-04 07:36:00,322] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4963
[2019-04-04 07:36:00,360] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 44.33333333333334, 71.66666666666666, 606.3333333333333, 26.0, 26.94161402846371, 0.7489456389658123, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3859800.0000, 
sim time next is 3860400.0000, 
raw observation next is [3.0, 43.66666666666667, 67.83333333333333, 578.6666666666666, 26.0, 26.92828268201058, 0.6413542332125896, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.4366666666666667, 0.2261111111111111, 0.6394106813996316, 0.6666666666666666, 0.744023556834215, 0.7137847444041965, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07283118], dtype=float32), 0.39380997]. 
=============================================
[2019-04-04 07:36:01,429] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.4701955e-12 9.1158703e-10 1.2226455e-27 2.4060837e-26 1.1131972e-20
 1.0000000e+00 6.6987764e-21], sum to 1.0000
[2019-04-04 07:36:01,430] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3030
[2019-04-04 07:36:01,479] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 95.0, 734.0, 26.0, 26.75388588351309, 0.7315936191667488, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3510000.0000, 
sim time next is 3510600.0000, 
raw observation next is [3.0, 49.0, 92.66666666666666, 718.3333333333333, 26.0, 26.81047075311992, 0.6299239843344419, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.3088888888888889, 0.7937384898710865, 0.6666666666666666, 0.7342058960933265, 0.7099746614448139, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11362357], dtype=float32), 0.19503754]. 
=============================================
[2019-04-04 07:36:05,131] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2292736e-10 1.8554461e-09 4.2600842e-27 3.8931006e-25 1.5486441e-19
 1.0000000e+00 8.1466905e-20], sum to 1.0000
[2019-04-04 07:36:05,131] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8631
[2019-04-04 07:36:05,136] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 45.0, 116.5, 822.5, 26.0, 25.52372932112246, 0.4643987175481723, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3672000.0000, 
sim time next is 3672600.0000, 
raw observation next is [4.166666666666667, 44.5, 116.6666666666667, 824.6666666666667, 26.0, 25.44377919642034, 0.457000917492458, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5780240073868884, 0.445, 0.388888888888889, 0.9112338858195212, 0.6666666666666666, 0.6203149330350284, 0.6523336391641527, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12487558], dtype=float32), 1.0721972]. 
=============================================
[2019-04-04 07:36:08,432] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.9252374e-09 7.3158780e-08 3.6478279e-23 5.8420378e-22 1.6704147e-17
 9.9999988e-01 1.3798083e-16], sum to 1.0000
[2019-04-04 07:36:08,442] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8227
[2019-04-04 07:36:08,471] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666666, 41.0, 11.66666666666667, 118.3333333333333, 26.0, 25.10267815529062, 0.376395816357677, 0.0, 1.0, 39654.14530611029], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3606000.0000, 
sim time next is 3606600.0000, 
raw observation next is [-0.8333333333333334, 41.5, 0.0, 0.0, 26.0, 25.10543104893027, 0.364707408097858, 0.0, 1.0, 32226.83312432631], 
processed observation next is [0.0, 0.7391304347826086, 0.43951985226223456, 0.415, 0.0, 0.0, 0.6666666666666666, 0.5921192540775225, 0.6215691360326193, 0.0, 1.0, 0.15346111011583957], 
reward next is 0.8465, 
noisyNet noise sample is [array([0.4517411], dtype=float32), -0.37954587]. 
=============================================
[2019-04-04 07:36:09,729] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5904585e-10 1.4029169e-09 3.0536756e-27 2.0473996e-25 1.0768507e-20
 1.0000000e+00 1.3610331e-20], sum to 1.0000
[2019-04-04 07:36:09,743] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7372
[2019-04-04 07:36:09,764] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 60.0, 48.33333333333334, 408.3333333333334, 26.0, 26.35184621972768, 0.4730157164653492, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3775800.0000, 
sim time next is 3776400.0000, 
raw observation next is [0.0, 60.0, 40.5, 343.0, 26.0, 26.43603858000904, 0.6156048997169882, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.6, 0.135, 0.37900552486187844, 0.6666666666666666, 0.7030032150007534, 0.7052016332389961, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.208871], dtype=float32), 0.5551203]. 
=============================================
[2019-04-04 07:36:12,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4245336e-10 1.9676154e-08 9.5740628e-27 4.3135126e-25 8.7494299e-20
 1.0000000e+00 3.0822440e-19], sum to 1.0000
[2019-04-04 07:36:12,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5993
[2019-04-04 07:36:12,690] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 72.0, 32.99999999999999, 233.6666666666666, 26.0, 25.25818021808772, 0.3102440716261076, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3743400.0000, 
sim time next is 3744000.0000, 
raw observation next is [-4.0, 71.0, 47.0, 282.5, 26.0, 25.2529568748157, 0.3018097731761751, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.71, 0.15666666666666668, 0.31215469613259667, 0.6666666666666666, 0.6044130729013082, 0.6006032577253917, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18493642], dtype=float32), 1.2909257]. 
=============================================
[2019-04-04 07:36:12,710] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[83.209785]
 [82.52386 ]
 [81.8894  ]
 [81.4226  ]
 [81.24919 ]], R is [[83.9335022 ]
 [84.09416962]
 [84.25322723]
 [84.41069794]
 [84.56658936]].
[2019-04-04 07:36:16,826] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.7858752e-10 1.5314221e-08 6.8090788e-26 2.7945392e-24 6.1757537e-19
 1.0000000e+00 2.0489608e-19], sum to 1.0000
[2019-04-04 07:36:16,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0220
[2019-04-04 07:36:16,882] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 58.0, 93.0, 444.0, 26.0, 25.82387071258097, 0.4417990618195773, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4005000.0000, 
sim time next is 4005600.0000, 
raw observation next is [-11.66666666666667, 56.33333333333333, 94.33333333333333, 486.3333333333333, 26.0, 26.11420655380419, 0.4727424296826579, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.13942751615881802, 0.5633333333333332, 0.3144444444444444, 0.5373848987108656, 0.6666666666666666, 0.6761838794836826, 0.6575808098942193, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.87854886], dtype=float32), -0.4440021]. 
=============================================
[2019-04-04 07:36:23,726] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.64557118e-10 3.13788995e-08 1.00249664e-25 1.10583892e-24
 2.06411629e-20 1.00000000e+00 7.66619747e-20], sum to 1.0000
[2019-04-04 07:36:23,726] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5653
[2019-04-04 07:36:23,743] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.5, 61.5, 0.0, 0.0, 26.0, 24.91666195230227, 0.2834750567061717, 0.0, 1.0, 42173.18363782619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3911400.0000, 
sim time next is 3912000.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 26.0, 24.88507640823134, 0.2680609096972446, 0.0, 1.0, 42333.43865486327], 
processed observation next is [1.0, 0.2608695652173913, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5737563673526115, 0.5893536365657482, 0.0, 1.0, 0.20158780311839652], 
reward next is 0.7984, 
noisyNet noise sample is [array([0.02363507], dtype=float32), 0.5725007]. 
=============================================
[2019-04-04 07:36:23,751] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[82.00869 ]
 [82.313324]
 [82.62141 ]
 [82.96625 ]
 [83.29379 ]], R is [[81.68353271]
 [81.6658783 ]
 [81.64910126]
 [81.63307953]
 [81.61791992]].
[2019-04-04 07:36:24,269] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.7924408e-11 2.3505565e-08 9.4235499e-26 3.8020630e-24 1.2898107e-19
 1.0000000e+00 1.1658077e-18], sum to 1.0000
[2019-04-04 07:36:24,272] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0727
[2019-04-04 07:36:24,284] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.4132140626375, 0.3219714102994023, 0.0, 1.0, 54281.35811103373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4249200.0000, 
sim time next is 4249800.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.36043118715667, 0.3203615385194852, 0.0, 1.0, 69382.40589490208], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6133692655963893, 0.6067871795064951, 0.0, 1.0, 0.33039240902334327], 
reward next is 0.6696, 
noisyNet noise sample is [array([-0.19867006], dtype=float32), 3.0673618]. 
=============================================
[2019-04-04 07:36:29,008] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.0368596e-10 7.8010887e-09 1.6253710e-24 1.1450592e-22 3.7362528e-18
 1.0000000e+00 1.0017591e-17], sum to 1.0000
[2019-04-04 07:36:29,008] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6133
[2019-04-04 07:36:29,032] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 42.0, 187.0, 89.0, 26.0, 25.0895011040126, 0.3568637773884319, 0.0, 1.0, 35132.51445404698], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4199400.0000, 
sim time next is 4200000.0000, 
raw observation next is [2.0, 42.66666666666667, 182.5, 163.8333333333333, 26.0, 25.02836998915939, 0.3631574102317432, 0.0, 1.0, 55018.30535231438], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.4266666666666667, 0.6083333333333333, 0.1810313075506445, 0.6666666666666666, 0.5856974990966158, 0.6210524700772477, 0.0, 1.0, 0.2619919302491161], 
reward next is 0.7380, 
noisyNet noise sample is [array([1.0414268], dtype=float32), -0.23680511]. 
=============================================
[2019-04-04 07:36:29,038] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[75.438  ]
 [76.03808]
 [76.68088]
 [77.21686]
 [77.57244]], R is [[75.19132996]
 [75.27211761]
 [75.51939392]
 [75.7641983 ]
 [76.00655365]].
[2019-04-04 07:36:32,790] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5563506e-09 1.2584076e-07 4.0836418e-24 1.4571079e-22 2.1624265e-18
 9.9999988e-01 8.3096587e-18], sum to 1.0000
[2019-04-04 07:36:32,790] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2096
[2019-04-04 07:36:32,803] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 38.0, 0.0, 0.0, 26.0, 24.71024340140844, 0.1949178310500141, 0.0, 1.0, 40164.89355677031], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4082400.0000, 
sim time next is 4083000.0000, 
raw observation next is [-4.166666666666667, 38.5, 0.0, 0.0, 26.0, 24.6851568521959, 0.1920669973210452, 0.0, 1.0, 40143.74606340181], 
processed observation next is [1.0, 0.2608695652173913, 0.3471837488457987, 0.385, 0.0, 0.0, 0.6666666666666666, 0.5570964043496582, 0.5640223324403484, 0.0, 1.0, 0.19116069554000864], 
reward next is 0.8088, 
noisyNet noise sample is [array([-0.5135259], dtype=float32), 2.0689828]. 
=============================================
[2019-04-04 07:36:32,820] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.98445 ]
 [75.002075]
 [75.03789 ]
 [75.086334]
 [75.13647 ]], R is [[75.00162506]
 [75.06034851]
 [75.11847687]
 [75.17597961]
 [75.23283386]].
[2019-04-04 07:36:37,489] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3303994e-10 5.8573404e-09 3.2363053e-27 3.5378997e-25 1.3602753e-20
 1.0000000e+00 2.3424414e-20], sum to 1.0000
[2019-04-04 07:36:37,491] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3820
[2019-04-04 07:36:37,503] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 50.0, 109.0, 68.0, 26.0, 25.14848188646396, 0.4467539974466325, 1.0, 1.0, 81532.12329189994], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4552200.0000, 
sim time next is 4552800.0000, 
raw observation next is [2.0, 50.66666666666666, 95.50000000000001, 61.33333333333334, 26.0, 25.43501695888741, 0.4896097480084016, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.5066666666666666, 0.31833333333333336, 0.06777163904235728, 0.6666666666666666, 0.6195847465739508, 0.6632032493361338, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0957063], dtype=float32), -1.0137388]. 
=============================================
[2019-04-04 07:36:38,835] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0065113e-10 1.2838224e-07 3.3889188e-25 9.8923576e-24 9.5189649e-19
 9.9999988e-01 7.0211204e-19], sum to 1.0000
[2019-04-04 07:36:38,837] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1590
[2019-04-04 07:36:38,856] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.166666666666667, 40.16666666666666, 0.0, 0.0, 26.0, 25.39557946140402, 0.4162167090627606, 0.0, 1.0, 49787.36483397098], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4151400.0000, 
sim time next is 4152000.0000, 
raw observation next is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 26.0, 25.36561998477268, 0.412082038879743, 0.0, 1.0, 56803.63093087377], 
processed observation next is [0.0, 0.043478260869565216, 0.42566943674976926, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.6138016653977235, 0.637360679626581, 0.0, 1.0, 0.27049348062320844], 
reward next is 0.7295, 
noisyNet noise sample is [array([-0.46800455], dtype=float32), 0.12099577]. 
=============================================
[2019-04-04 07:36:38,860] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[79.66498 ]
 [79.78064 ]
 [80.173584]
 [80.19574 ]
 [80.26785 ]], R is [[79.70081329]
 [79.66672516]
 [79.72323608]
 [79.83666229]
 [79.94894409]].
[2019-04-04 07:36:40,956] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2661703e-12 1.1157543e-09 2.7034438e-29 1.8084977e-27 2.1072595e-22
 1.0000000e+00 1.5875876e-22], sum to 1.0000
[2019-04-04 07:36:40,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0955
[2019-04-04 07:36:40,983] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333333, 61.83333333333333, 152.3333333333333, 595.0, 26.0, 26.36444083032271, 0.6084260032854474, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4614600.0000, 
sim time next is 4615200.0000, 
raw observation next is [0.0, 60.0, 146.5, 638.0, 26.0, 26.44656609887163, 0.6264508900377345, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.6, 0.48833333333333334, 0.7049723756906078, 0.6666666666666666, 0.7038805082393026, 0.7088169633459115, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39563808], dtype=float32), -0.1760028]. 
=============================================
[2019-04-04 07:36:43,236] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.5824001e-11 3.7085377e-09 3.8441735e-28 7.3658254e-26 1.3223622e-21
 1.0000000e+00 3.5702420e-21], sum to 1.0000
[2019-04-04 07:36:43,236] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4527
[2019-04-04 07:36:43,251] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.166666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.54467112456315, 0.3698939641756421, 0.0, 1.0, 18745.2356612352], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4342800.0000, 
sim time next is 4343400.0000, 
raw observation next is [3.1, 73.0, 0.0, 0.0, 26.0, 25.50169170808444, 0.3564580509204561, 0.0, 1.0, 41800.18459727104], 
processed observation next is [1.0, 0.2608695652173913, 0.5484764542936289, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6251409756737033, 0.6188193503068188, 0.0, 1.0, 0.19904849808224306], 
reward next is 0.8010, 
noisyNet noise sample is [array([-1.3201691], dtype=float32), -0.6666726]. 
=============================================
[2019-04-04 07:36:46,600] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3246813e-11 9.2585767e-10 1.6686921e-28 2.3468604e-27 4.3224307e-22
 1.0000000e+00 3.2139854e-21], sum to 1.0000
[2019-04-04 07:36:46,603] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0799
[2019-04-04 07:36:46,635] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 50.5, 247.0, 48.0, 26.0, 25.24785960351334, 0.4870230263700575, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4541400.0000, 
sim time next is 4542000.0000, 
raw observation next is [2.666666666666667, 50.0, 249.8333333333333, 58.83333333333333, 26.0, 25.70309468866133, 0.536801064178151, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5364727608494922, 0.5, 0.8327777777777776, 0.06500920810313075, 0.6666666666666666, 0.6419245573884442, 0.6789336880593836, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44952425], dtype=float32), -0.33021465]. 
=============================================
[2019-04-04 07:36:46,640] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[87.77973 ]
 [87.053375]
 [85.88139 ]
 [85.45233 ]
 [85.227585]], R is [[88.38460541]
 [88.50076294]
 [87.6812973 ]
 [87.64265442]
 [87.76622772]].
[2019-04-04 07:36:50,178] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5203266e-12 4.6136753e-10 4.4394901e-30 1.1855984e-27 9.6318143e-23
 1.0000000e+00 3.8000369e-22], sum to 1.0000
[2019-04-04 07:36:50,179] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9515
[2019-04-04 07:36:50,233] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 180.3333333333333, 5.0, 26.0, 25.78555447282891, 0.4265586071346394, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4454400.0000, 
sim time next is 4455000.0000, 
raw observation next is [0.0, 92.0, 196.0, 6.0, 26.0, 25.37469156010963, 0.471657935093155, 1.0, 1.0, 117225.3075612451], 
processed observation next is [1.0, 0.5652173913043478, 0.46260387811634357, 0.92, 0.6533333333333333, 0.0066298342541436465, 0.6666666666666666, 0.6145576300091357, 0.6572193116977183, 1.0, 1.0, 0.5582157502916434], 
reward next is 0.4418, 
noisyNet noise sample is [array([0.09696374], dtype=float32), 0.8616162]. 
=============================================
[2019-04-04 07:36:50,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[90.91102 ]
 [90.50426 ]
 [90.18125 ]
 [89.961205]
 [89.925545]], R is [[91.37173462]
 [91.45801544]
 [91.54343414]
 [91.62799835]
 [91.7117157 ]].
[2019-04-04 07:36:52,111] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3583805e-11 4.0864316e-09 1.9082442e-29 1.5420287e-28 9.4223061e-23
 1.0000000e+00 5.3370411e-22], sum to 1.0000
[2019-04-04 07:36:52,116] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7033
[2019-04-04 07:36:52,133] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.533333333333333, 60.66666666666666, 0.0, 0.0, 26.0, 26.57976484409846, 0.7885736668496693, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4398600.0000, 
sim time next is 4399200.0000, 
raw observation next is [9.4, 61.0, 0.0, 0.0, 26.0, 26.52167266204998, 0.777263726241541, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.7229916897506927, 0.61, 0.0, 0.0, 0.6666666666666666, 0.710139388504165, 0.7590879087471804, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20450427], dtype=float32), -0.62819004]. 
=============================================
[2019-04-04 07:36:54,225] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.0027439e-12 5.4715021e-10 2.0165774e-28 1.4044891e-27 3.9218297e-22
 1.0000000e+00 3.6622210e-21], sum to 1.0000
[2019-04-04 07:36:54,227] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8324
[2019-04-04 07:36:54,249] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 47.66666666666667, 261.1666666666666, 102.1666666666667, 26.0, 26.25926326193117, 0.4930044176514095, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4544400.0000, 
sim time next is 4545000.0000, 
raw observation next is [3.0, 47.0, 264.0, 113.0, 26.0, 25.85814968332861, 0.5500367451053122, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.47, 0.88, 0.12486187845303867, 0.6666666666666666, 0.6548458069440507, 0.6833455817017707, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.38796625], dtype=float32), -2.282162]. 
=============================================
[2019-04-04 07:36:54,265] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[88.16312 ]
 [88.188354]
 [88.01055 ]
 [87.72385 ]
 [87.26797 ]], R is [[88.1962738 ]
 [88.31430817]
 [88.4311676 ]
 [88.54685974]
 [88.66139221]].
[2019-04-04 07:36:55,566] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4568841e-10 3.9106589e-09 1.6394300e-26 5.2885124e-26 2.0158320e-20
 1.0000000e+00 1.6718338e-20], sum to 1.0000
[2019-04-04 07:36:55,571] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9145
[2019-04-04 07:36:55,607] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 55.33333333333334, 0.0, 0.0, 26.0, 25.09939761747727, 0.497018982602681, 0.0, 1.0, 102765.455883789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4567200.0000, 
sim time next is 4567800.0000, 
raw observation next is [2.0, 56.16666666666666, 0.0, 0.0, 26.0, 25.24314325793836, 0.5237845939428795, 0.0, 1.0, 60830.26161647401], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.5616666666666665, 0.0, 0.0, 0.6666666666666666, 0.6035952714948634, 0.6745948646476264, 0.0, 1.0, 0.28966791245940005], 
reward next is 0.7103, 
noisyNet noise sample is [array([-3.0012465], dtype=float32), -0.1404963]. 
=============================================
[2019-04-04 07:37:07,624] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.6442038e-12 2.0465341e-09 8.6721043e-28 6.1939928e-26 1.2186826e-20
 1.0000000e+00 2.1040807e-20], sum to 1.0000
[2019-04-04 07:37:07,624] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8575
[2019-04-04 07:37:07,658] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.5751770160396, 0.5446336946083494, 0.0, 1.0, 25842.15867692713], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4651800.0000, 
sim time next is 4652400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.49723619256704, 0.5483028865525935, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6247696827139201, 0.6827676288508645, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([1.9501983], dtype=float32), 0.02219445]. 
=============================================
[2019-04-04 07:37:07,659] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:07,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:07,672] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run14
[2019-04-04 07:37:08,778] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:08,779] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:08,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run14
[2019-04-04 07:37:11,715] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:11,716] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:11,719] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run14
[2019-04-04 07:37:11,994] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:11,995] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:12,016] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run14
[2019-04-04 07:37:20,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:20,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:20,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run14
[2019-04-04 07:37:20,528] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:20,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:20,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run14
[2019-04-04 07:37:21,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:21,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:21,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run14
[2019-04-04 07:37:27,682] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7554650e-11 5.9112457e-09 1.4210711e-28 1.1056859e-26 3.3412386e-21
 1.0000000e+00 5.0243916e-21], sum to 1.0000
[2019-04-04 07:37:27,682] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4312
[2019-04-04 07:37:27,699] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 29.33333333333334, 123.1666666666667, 848.3333333333334, 26.0, 27.03410943091249, 0.810403249332526, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5053200.0000, 
sim time next is 5053800.0000, 
raw observation next is [7.5, 27.66666666666666, 123.3333333333333, 851.6666666666666, 26.0, 27.25880030579111, 0.6377317788382427, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6703601108033241, 0.2766666666666666, 0.411111111111111, 0.9410681399631675, 0.6666666666666666, 0.7715666921492591, 0.7125772596127476, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60549575], dtype=float32), 0.17014164]. 
=============================================
[2019-04-04 07:37:27,962] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:27,962] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:27,980] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run14
[2019-04-04 07:37:28,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:28,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:28,518] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run14
[2019-04-04 07:37:31,112] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1961119e-10 7.2670883e-09 3.4398440e-26 3.9323252e-25 2.4637230e-20
 1.0000000e+00 1.7502974e-19], sum to 1.0000
[2019-04-04 07:37:31,112] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4428
[2019-04-04 07:37:31,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:31,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:31,124] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run14
[2019-04-04 07:37:31,171] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.49027235446491, 0.4124890396392109, 0.0, 1.0, 27925.26104757822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5022600.0000, 
sim time next is 5023200.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.46022779703505, 0.4114293615651376, 0.0, 1.0, 43198.39326073141], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6216856497529207, 0.6371431205217125, 0.0, 1.0, 0.20570663457491148], 
reward next is 0.7943, 
noisyNet noise sample is [array([-0.37687105], dtype=float32), -0.12633894]. 
=============================================
[2019-04-04 07:37:31,384] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.3295034e-10 9.4835677e-09 5.9655018e-27 7.8986739e-25 2.8816808e-20
 1.0000000e+00 1.4640581e-19], sum to 1.0000
[2019-04-04 07:37:31,384] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9700
[2019-04-04 07:37:31,419] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.25, 27.5, 0.0, 0.0, 26.0, 25.93534602878092, 0.5558817985628172, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5095800.0000, 
sim time next is 5096400.0000, 
raw observation next is [8.2, 30.0, 0.0, 0.0, 26.0, 25.8586162867003, 0.544308307600441, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6897506925207757, 0.3, 0.0, 0.0, 0.6666666666666666, 0.6548846905583584, 0.6814361025334804, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1628118], dtype=float32), 0.18629777]. 
=============================================
[2019-04-04 07:37:31,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:31,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:31,436] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run14
[2019-04-04 07:37:32,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:32,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:32,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run14
[2019-04-04 07:37:33,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:33,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:33,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run14
[2019-04-04 07:37:34,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:34,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:34,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run14
[2019-04-04 07:37:37,254] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:37,254] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:37,272] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run14
[2019-04-04 07:37:39,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:37:39,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:37:39,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run14
[2019-04-04 07:37:57,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5859331e-09 2.7911954e-08 1.4057509e-24 5.4866816e-23 3.2775791e-19
 1.0000000e+00 3.7042227e-18], sum to 1.0000
[2019-04-04 07:37:57,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0100
[2019-04-04 07:37:57,541] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 68.5, 0.0, 0.0, 26.0, 24.6076225599856, 0.2051156515121354, 0.0, 1.0, 45837.2702444938], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 162600.0000, 
sim time next is 163200.0000, 
raw observation next is [-8.4, 69.0, 0.0, 0.0, 26.0, 24.53585903636052, 0.1906968843945994, 0.0, 1.0, 45855.36768567257], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.69, 0.0, 0.0, 0.6666666666666666, 0.54465491969671, 0.5635656281315331, 0.0, 1.0, 0.21835889374129797], 
reward next is 0.7816, 
noisyNet noise sample is [array([0.40077958], dtype=float32), -0.1352516]. 
=============================================
[2019-04-04 07:37:58,069] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0176088e-09 9.4237578e-09 3.7629174e-24 6.4482904e-23 5.1490437e-19
 1.0000000e+00 7.6754817e-18], sum to 1.0000
[2019-04-04 07:37:58,069] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2920
[2019-04-04 07:37:58,094] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.95604265903334, 0.05980824400081478, 0.0, 1.0, 44755.96856224185], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 171000.0000, 
sim time next is 171600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.91256948743874, 0.05038536637614859, 0.0, 1.0, 44700.74668771146], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.49271412395322844, 0.5167951221253829, 0.0, 1.0, 0.2128606985129117], 
reward next is 0.7871, 
noisyNet noise sample is [array([0.20246527], dtype=float32), 0.3912473]. 
=============================================
[2019-04-04 07:38:10,734] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1732203e-09 1.7404068e-08 3.3663437e-26 1.9057974e-24 3.4141027e-19
 1.0000000e+00 1.7218023e-18], sum to 1.0000
[2019-04-04 07:38:10,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9853
[2019-04-04 07:38:10,757] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.34700776804339, 0.1251458140245719, 0.0, 1.0, 42150.16864584575], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 611400.0000, 
sim time next is 612000.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.31142078317975, 0.1182345848200558, 0.0, 1.0, 42196.57045761545], 
processed observation next is [0.0, 0.08695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5259517319316457, 0.5394115282733519, 0.0, 1.0, 0.20093604979816881], 
reward next is 0.7991, 
noisyNet noise sample is [array([-0.59833735], dtype=float32), -0.42180854]. 
=============================================
[2019-04-04 07:38:10,802] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.753395]
 [81.75309 ]
 [81.629944]
 [81.454124]
 [81.40976 ]], R is [[81.62175751]
 [81.60482788]
 [81.58824158]
 [81.57187653]
 [81.55558777]].
[2019-04-04 07:38:21,031] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.3456317e-10 9.5427382e-09 7.5387172e-26 1.2642049e-23 9.1140505e-20
 1.0000000e+00 2.6982236e-19], sum to 1.0000
[2019-04-04 07:38:21,031] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7221
[2019-04-04 07:38:21,088] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.383333333333333, 59.83333333333333, 148.3333333333333, 80.66666666666667, 26.0, 24.93134039623359, 0.2372703239281244, 0.0, 1.0, 26322.47873404474], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 654600.0000, 
sim time next is 655200.0000, 
raw observation next is [-1.2, 60.0, 131.5, 74.5, 26.0, 24.91552576091288, 0.2350488496345698, 0.0, 1.0, 36940.1447262215], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.6, 0.43833333333333335, 0.08232044198895028, 0.6666666666666666, 0.5762938134094068, 0.5783496165448566, 0.0, 1.0, 0.17590545107724523], 
reward next is 0.8241, 
noisyNet noise sample is [array([-0.8603992], dtype=float32), -0.117405474]. 
=============================================
[2019-04-04 07:38:22,686] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7393928e-09 1.4009389e-08 7.1354762e-25 9.2655066e-24 6.0239077e-19
 1.0000000e+00 4.5895029e-18], sum to 1.0000
[2019-04-04 07:38:22,688] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7748
[2019-04-04 07:38:22,735] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 57.00000000000001, 0.0, 0.0, 26.0, 24.92469601497108, 0.2141380510020773, 0.0, 1.0, 18736.21245953262], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 667200.0000, 
sim time next is 667800.0000, 
raw observation next is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.92310895247499, 0.2064432019208399, 0.0, 1.0, 28454.38386302943], 
processed observation next is [0.0, 0.7391304347826086, 0.42936288088642666, 0.57, 0.0, 0.0, 0.6666666666666666, 0.5769257460395826, 0.56881440064028, 0.0, 1.0, 0.13549706601442585], 
reward next is 0.8645, 
noisyNet noise sample is [array([-1.0388539], dtype=float32), 0.38523966]. 
=============================================
[2019-04-04 07:38:29,945] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9671820e-10 6.8035266e-10 1.4608992e-28 2.8639734e-26 9.0528486e-22
 1.0000000e+00 1.5612351e-21], sum to 1.0000
[2019-04-04 07:38:29,947] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0407
[2019-04-04 07:38:29,974] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.516666666666667, 92.66666666666667, 0.0, 0.0, 26.0, 24.8632109173549, 0.2350073751799262, 0.0, 1.0, 41214.39888348656], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 510600.0000, 
sim time next is 511200.0000, 
raw observation next is [2.7, 92.0, 0.0, 0.0, 26.0, 24.85962909012352, 0.2342003861466514, 0.0, 1.0, 41107.86920675982], 
processed observation next is [1.0, 0.9565217391304348, 0.5373961218836566, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5716357575102933, 0.5780667953822172, 0.0, 1.0, 0.1957517581274277], 
reward next is 0.8042, 
noisyNet noise sample is [array([0.75411147], dtype=float32), -0.2967455]. 
=============================================
[2019-04-04 07:38:30,735] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9757437e-10 1.2209608e-08 4.4592218e-26 1.3369485e-23 2.2640166e-19
 1.0000000e+00 5.1664166e-19], sum to 1.0000
[2019-04-04 07:38:30,737] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4555
[2019-04-04 07:38:30,781] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 72.5, 0.0, 0.0, 26.0, 24.40841205883969, 0.09371328009453572, 0.0, 1.0, 40981.59000275359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 695400.0000, 
sim time next is 696000.0000, 
raw observation next is [-3.4, 73.0, 0.0, 0.0, 26.0, 24.43411566605558, 0.0881304878774506, 0.0, 1.0, 41006.26333166781], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5361763055046316, 0.5293768292924835, 0.0, 1.0, 0.19526792062698958], 
reward next is 0.8047, 
noisyNet noise sample is [array([0.86007226], dtype=float32), -1.069529]. 
=============================================
[2019-04-04 07:38:30,795] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[79.28474]
 [78.82556]
 [78.49491]
 [78.3363 ]
 [78.06828]], R is [[79.26428986]
 [79.27649689]
 [79.28871155]
 [79.30088806]
 [79.31288147]].
[2019-04-04 07:38:30,961] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4029330e-09 1.0257585e-08 5.6794218e-25 5.7694819e-23 8.8979715e-19
 1.0000000e+00 2.5488071e-18], sum to 1.0000
[2019-04-04 07:38:30,961] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0235
[2019-04-04 07:38:30,971] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 26.0, 24.3979390401772, 0.1668660294525439, 0.0, 1.0, 41965.66166907362], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 777000.0000, 
sim time next is 777600.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.36194400541292, 0.1583034681269115, 0.0, 1.0, 41881.33942706395], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5301620004510766, 0.5527678227089705, 0.0, 1.0, 0.19943494965268546], 
reward next is 0.8006, 
noisyNet noise sample is [array([-1.2963347], dtype=float32), 0.47965068]. 
=============================================
[2019-04-04 07:38:33,156] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.5139238e-10 1.3559628e-08 5.1022971e-25 4.1664730e-24 3.6138880e-19
 1.0000000e+00 9.5627183e-18], sum to 1.0000
[2019-04-04 07:38:33,156] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3552
[2019-04-04 07:38:33,194] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.60917868355652, 0.1329699045459239, 0.0, 1.0, 41513.82729833216], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 688200.0000, 
sim time next is 688800.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.56899243712342, 0.1261612259412028, 0.0, 1.0, 41448.69008922127], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5474160364269517, 0.5420537419804009, 0.0, 1.0, 0.1973747147105775], 
reward next is 0.8026, 
noisyNet noise sample is [array([0.06978212], dtype=float32), -0.10382216]. 
=============================================
[2019-04-04 07:38:33,705] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5035650e-11 2.6665374e-09 3.7234853e-28 2.2047313e-26 3.6215712e-21
 1.0000000e+00 3.0448460e-20], sum to 1.0000
[2019-04-04 07:38:33,705] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4041
[2019-04-04 07:38:33,750] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.133333333333333, 80.16666666666667, 132.6666666666667, 462.3333333333334, 26.0, 24.97541261543923, 0.3244839735495748, 0.0, 1.0, 18727.44136028136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 564600.0000, 
sim time next is 565200.0000, 
raw observation next is [-1.2, 80.0, 134.0, 495.5, 26.0, 24.95918454519931, 0.3232094471013158, 0.0, 1.0, 35238.0654484776], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.44666666666666666, 0.5475138121546961, 0.6666666666666666, 0.5799320454332758, 0.6077364823671053, 0.0, 1.0, 0.16780031165941717], 
reward next is 0.8322, 
noisyNet noise sample is [array([-0.6141645], dtype=float32), 0.37184182]. 
=============================================
[2019-04-04 07:38:47,825] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7034915e-12 1.7122864e-09 9.4562691e-29 3.7128351e-27 6.6666527e-22
 1.0000000e+00 4.2170699e-22], sum to 1.0000
[2019-04-04 07:38:47,825] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8359
[2019-04-04 07:38:47,838] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 58.5, 99.66666666666666, 669.0, 26.0, 25.86301252543569, 0.3895616733027364, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 733800.0000, 
sim time next is 734400.0000, 
raw observation next is [-0.6, 57.0, 107.5, 614.0, 26.0, 25.84235977200898, 0.3889771677855018, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.44598337950138506, 0.57, 0.35833333333333334, 0.6784530386740332, 0.6666666666666666, 0.6535299810007483, 0.6296590559285006, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.57133174], dtype=float32), 0.021258451]. 
=============================================
[2019-04-04 07:38:53,908] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8676512e-11 9.6568220e-10 7.8681904e-30 6.6277345e-28 2.7303258e-23
 1.0000000e+00 1.5330954e-21], sum to 1.0000
[2019-04-04 07:38:53,908] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8327
[2019-04-04 07:38:53,962] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.20718304945527, 0.4033736009755641, 0.0, 1.0, 39254.3961789595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 945000.0000, 
sim time next is 945600.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.21440912858466, 0.4061675166149429, 0.0, 1.0, 39108.15402485767], 
processed observation next is [1.0, 0.9565217391304348, 0.6011080332409973, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6012007607153883, 0.6353891722049809, 0.0, 1.0, 0.18622930488027462], 
reward next is 0.8138, 
noisyNet noise sample is [array([0.63606006], dtype=float32), -1.178743]. 
=============================================
[2019-04-04 07:39:02,267] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.6783083e-13 1.0846377e-10 2.2727152e-32 2.8709245e-30 2.3323611e-24
 1.0000000e+00 6.4257943e-24], sum to 1.0000
[2019-04-04 07:39:02,267] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1487
[2019-04-04 07:39:02,282] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 25.77510989330037, 0.5307559013723149, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1017000.0000, 
sim time next is 1017600.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.6585361654449, 0.5225276015551693, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6382113471204084, 0.6741758671850565, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5859746], dtype=float32), -1.2934613]. 
=============================================
[2019-04-04 07:39:05,161] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7739014e-10 3.5862753e-09 2.8873388e-26 7.0682356e-25 4.0169341e-21
 1.0000000e+00 7.4797600e-20], sum to 1.0000
[2019-04-04 07:39:05,161] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2417
[2019-04-04 07:39:05,166] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.7, 50.0, 18.0, 1.5, 26.0, 27.96416548877713, 1.027869892466817, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1098000.0000, 
sim time next is 1098600.0000, 
raw observation next is [17.43333333333333, 50.5, 0.0, 0.0, 26.0, 27.96385902878735, 1.028371394882608, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.9455216989843028, 0.505, 0.0, 0.0, 0.6666666666666666, 0.8303215857322792, 0.8427904649608694, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6048082], dtype=float32), -0.7150998]. 
=============================================
[2019-04-04 07:39:08,280] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2343724e-12 9.8748587e-10 3.9169695e-30 1.3117122e-27 2.0860556e-23
 1.0000000e+00 1.3569001e-22], sum to 1.0000
[2019-04-04 07:39:08,280] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5502
[2019-04-04 07:39:08,309] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.43333333333333, 77.0, 0.0, 0.0, 26.0, 25.64298024103188, 0.615965463534635, 0.0, 1.0, 27562.53789054022], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1140000.0000, 
sim time next is 1140600.0000, 
raw observation next is [11.51666666666667, 77.0, 0.0, 0.0, 26.0, 25.63861426107606, 0.6169288377562352, 0.0, 1.0, 27973.9696980074], 
processed observation next is [0.0, 0.17391304347826086, 0.7816251154201295, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6365511884230051, 0.7056429459187451, 0.0, 1.0, 0.13320937951432096], 
reward next is 0.8668, 
noisyNet noise sample is [array([0.4719307], dtype=float32), -0.50603986]. 
=============================================
[2019-04-04 07:39:08,583] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3609303e-11 1.9545593e-10 6.3791991e-30 9.8033012e-29 1.4555684e-23
 1.0000000e+00 2.0783124e-22], sum to 1.0000
[2019-04-04 07:39:08,584] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5616
[2019-04-04 07:39:08,635] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 98.66666666666666, 0.0, 0.0, 26.0, 24.61689946009365, 0.2948205607211813, 1.0, 1.0, 129113.7269729315], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 931200.0000, 
sim time next is 931800.0000, 
raw observation next is [4.4, 99.33333333333334, 0.0, 0.0, 26.0, 24.67524095302724, 0.32134401555946, 1.0, 1.0, 40425.59544365747], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.9933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5562700794189368, 0.6071146718531534, 1.0, 1.0, 0.19250283544598795], 
reward next is 0.8075, 
noisyNet noise sample is [array([1.2527885], dtype=float32), -0.11525263]. 
=============================================
[2019-04-04 07:39:11,709] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.6714761e-13 3.6480610e-10 2.0904194e-32 6.6357976e-30 1.0758220e-24
 1.0000000e+00 1.1602495e-24], sum to 1.0000
[2019-04-04 07:39:11,709] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2780
[2019-04-04 07:39:11,737] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.36666666666667, 86.0, 126.6666666666667, 0.0, 26.0, 26.77678412591492, 0.5768948313239153, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 994800.0000, 
sim time next is 995400.0000, 
raw observation next is [12.45, 86.0, 128.0, 0.0, 26.0, 25.73372131643142, 0.5533700844303221, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8074792243767314, 0.86, 0.4266666666666667, 0.0, 0.6666666666666666, 0.644476776369285, 0.6844566948101073, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8123357], dtype=float32), -0.20649222]. 
=============================================
[2019-04-04 07:39:12,482] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.1857880e-12 3.0147898e-09 2.4818416e-29 5.0920615e-28 1.4616954e-22
 1.0000000e+00 1.4988329e-21], sum to 1.0000
[2019-04-04 07:39:12,500] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4238
[2019-04-04 07:39:12,516] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.433333333333334, 83.0, 0.0, 0.0, 26.0, 25.37725346011483, 0.4497889614758182, 0.0, 1.0, 52714.65082210901], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 967200.0000, 
sim time next is 967800.0000, 
raw observation next is [8.616666666666667, 83.0, 0.0, 0.0, 26.0, 25.42711406599805, 0.4534099452560036, 0.0, 1.0, 18763.63343352981], 
processed observation next is [1.0, 0.17391304347826086, 0.7012927054478302, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6189261721665043, 0.6511366484186679, 0.0, 1.0, 0.08935063539776099], 
reward next is 0.9106, 
noisyNet noise sample is [array([-0.46404025], dtype=float32), 1.6613572]. 
=============================================
[2019-04-04 07:39:19,166] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9120051e-11 1.9769062e-09 2.1660720e-29 1.3345000e-27 1.7906948e-22
 1.0000000e+00 1.4907700e-21], sum to 1.0000
[2019-04-04 07:39:19,167] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0202
[2019-04-04 07:39:19,197] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.283333333333333, 92.0, 0.0, 0.0, 26.0, 25.43767057768596, 0.5481027439598934, 0.0, 1.0, 42227.59538314333], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1309800.0000, 
sim time next is 1310400.0000, 
raw observation next is [2.2, 92.0, 0.0, 0.0, 26.0, 25.40704384514646, 0.5491325839974299, 0.0, 1.0, 55386.6395864658], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6172536537622051, 0.68304419466581, 0.0, 1.0, 0.2637459027926943], 
reward next is 0.7363, 
noisyNet noise sample is [array([0.28627977], dtype=float32), -0.7167612]. 
=============================================
[2019-04-04 07:39:21,484] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3090019e-12 6.3844496e-10 1.6446131e-30 1.7544540e-28 3.5987085e-22
 1.0000000e+00 9.9357084e-22], sum to 1.0000
[2019-04-04 07:39:21,484] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7346
[2019-04-04 07:39:21,494] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.7000000000000001, 92.0, 92.5, 0.0, 26.0, 26.09858196388575, 0.5901684356917419, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1333200.0000, 
sim time next is 1333800.0000, 
raw observation next is [0.8, 92.0, 102.0, 0.0, 26.0, 26.10111074711984, 0.5895895519912868, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4847645429362882, 0.92, 0.34, 0.0, 0.6666666666666666, 0.6750925622599867, 0.6965298506637622, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.94534785], dtype=float32), -0.20747639]. 
=============================================
[2019-04-04 07:39:21,949] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 07:39:21,954] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:39:21,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:39:21,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run19
[2019-04-04 07:39:21,996] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:39:22,001] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:39:21,997] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:39:22,002] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:39:22,004] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run19
[2019-04-04 07:39:22,026] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run19
[2019-04-04 07:39:34,417] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.24636225], dtype=float32), 0.24974751]
[2019-04-04 07:39:34,417] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.258567319166667, 74.25038115166666, 46.807507186, 18.80134392999999, 26.0, 25.03137624252928, 0.230504827743894, 1.0, 1.0, 75038.93439483382]
[2019-04-04 07:39:34,417] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 07:39:34,418] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.0785802e-10 5.7276059e-09 6.3298828e-26 2.5287425e-24 1.5392323e-19
 1.0000000e+00 6.3639223e-19], sampled 0.9164329468533217
[2019-04-04 07:40:37,474] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.24636225], dtype=float32), 0.24974751]
[2019-04-04 07:40:37,475] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.1, 92.0, 0.0, 0.0, 26.0, 25.24407662002876, 0.4377978329677077, 0.0, 1.0, 0.0]
[2019-04-04 07:40:37,475] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 07:40:37,476] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.5392612e-11 8.4978657e-10 5.5829832e-29 4.3739048e-27 7.0240311e-22
 1.0000000e+00 3.5847213e-21], sampled 0.8245283734232917
[2019-04-04 07:42:15,908] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.24636225], dtype=float32), 0.24974751]
[2019-04-04 07:42:15,908] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.166666666666667, 55.83333333333333, 112.0, 777.3333333333333, 26.0, 25.43067850607588, 0.4628231785046248, 0.0, 1.0, 0.0]
[2019-04-04 07:42:15,908] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 07:42:15,909] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.90140681e-10 7.82023246e-09 2.28663613e-26 1.13474377e-24
 1.02569795e-19 1.00000000e+00 2.03933832e-19], sampled 0.42029014914865115
[2019-04-04 07:42:21,246] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.24636225], dtype=float32), 0.24974751]
[2019-04-04 07:42:21,246] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [17.5, 62.0, 0.0, 0.0, 26.0, 28.50737443653083, 1.382202680187931, 0.0, 0.0, 0.0]
[2019-04-04 07:42:21,247] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 07:42:21,247] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.1037815e-10 3.4382992e-09 1.3963538e-26 4.2406160e-25 3.9303555e-20
 1.0000000e+00 1.4187702e-19], sampled 0.4665251535857101
[2019-04-04 07:42:30,610] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 07:43:04,671] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 07:43:10,766] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 07:43:11,811] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1800000, evaluation results [1800000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 07:43:15,580] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3539280e-12 3.6306794e-10 3.1524786e-31 4.2226269e-28 6.8912976e-23
 1.0000000e+00 1.3437597e-22], sum to 1.0000
[2019-04-04 07:43:15,580] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7395
[2019-04-04 07:43:15,617] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.8947498897097, 0.6299980311467492, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1029600.0000, 
sim time next is 1030200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.97011176552465, 0.6272501117463283, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6641759804603874, 0.7090833705821095, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1315215], dtype=float32), -0.2526929]. 
=============================================
[2019-04-04 07:43:18,367] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.3390235e-09 3.5258516e-08 6.9819223e-24 1.3939935e-22 9.8712353e-19
 1.0000000e+00 2.3397302e-17], sum to 1.0000
[2019-04-04 07:43:18,367] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3519
[2019-04-04 07:43:18,393] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.35590633610353, 0.3284045748231311, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1204800.0000, 
sim time next is 1205400.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 26.0, 24.35457843481877, 0.324250079432387, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5295482029015641, 0.6080833598107956, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.76326585], dtype=float32), 1.4135982]. 
=============================================
[2019-04-04 07:43:23,211] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1045320e-09 4.4105061e-08 2.1739167e-24 1.5772946e-23 3.5271722e-18
 1.0000000e+00 4.2394987e-18], sum to 1.0000
[2019-04-04 07:43:23,211] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5882
[2019-04-04 07:43:23,262] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.3, 65.0, 120.0, 0.0, 26.0, 25.06659139296303, 0.4975187687246472, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1176000.0000, 
sim time next is 1176600.0000, 
raw observation next is [18.3, 65.0, 112.0, 0.0, 26.0, 25.05496988730549, 0.4945159971504398, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.37333333333333335, 0.0, 0.6666666666666666, 0.5879141572754575, 0.6648386657168133, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4215246], dtype=float32), 0.723798]. 
=============================================
[2019-04-04 07:43:25,029] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.9096360e-12 1.1666265e-10 1.1418877e-30 1.1922785e-28 1.5586569e-23
 1.0000000e+00 6.9214902e-23], sum to 1.0000
[2019-04-04 07:43:25,038] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8487
[2019-04-04 07:43:25,071] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.09999999999999998, 95.83333333333334, 54.66666666666666, 0.0, 26.0, 26.00010903553061, 0.5182693333617501, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1417800.0000, 
sim time next is 1418400.0000, 
raw observation next is [0.0, 95.0, 59.0, 0.0, 26.0, 26.01572939083412, 0.5191180797065776, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.19666666666666666, 0.0, 0.6666666666666666, 0.6679774492361767, 0.6730393599021925, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.133122], dtype=float32), -1.469268]. 
=============================================
[2019-04-04 07:43:26,776] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4565243e-11 4.4999623e-10 1.0938114e-30 5.3862934e-29 2.4058762e-23
 1.0000000e+00 6.4241343e-22], sum to 1.0000
[2019-04-04 07:43:26,776] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9176
[2019-04-04 07:43:26,787] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 100.0, 89.0, 0.0, 26.0, 24.79277955711379, 0.451174436002287, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1259400.0000, 
sim time next is 1260000.0000, 
raw observation next is [13.8, 100.0, 86.0, 0.0, 26.0, 24.77696480203304, 0.4488755681696356, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.2866666666666667, 0.0, 0.6666666666666666, 0.5647470668360866, 0.6496251893898785, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4812433], dtype=float32), -1.1764865]. 
=============================================
[2019-04-04 07:43:26,794] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[96.51548 ]
 [96.52478 ]
 [96.526276]
 [96.48876 ]
 [95.90373 ]], R is [[96.52088928]
 [96.55567932]
 [96.59012604]
 [96.62422943]
 [96.6579895 ]].
[2019-04-04 07:43:27,784] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.1322562e-12 8.4758667e-10 3.5688249e-29 2.4885752e-27 3.9828146e-22
 1.0000000e+00 1.1496861e-21], sum to 1.0000
[2019-04-04 07:43:27,789] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2363
[2019-04-04 07:43:27,804] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.466666666666667, 98.66666666666666, 0.0, 0.0, 26.0, 25.5164521085709, 0.4679383311754643, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1492800.0000, 
sim time next is 1493400.0000, 
raw observation next is [1.283333333333333, 99.33333333333334, 0.0, 0.0, 26.0, 25.57648028398304, 0.4357567977620329, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.4981532779316713, 0.9933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6313733569985868, 0.6452522659206776, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40238616], dtype=float32), -0.12644629]. 
=============================================
[2019-04-04 07:43:28,787] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0975338e-11 2.0599893e-10 1.2418223e-29 5.0911319e-27 2.0926560e-22
 1.0000000e+00 6.1327349e-22], sum to 1.0000
[2019-04-04 07:43:28,788] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4760
[2019-04-04 07:43:28,835] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.06337977307381, 0.4626635419458065, 0.0, 1.0, 18704.39638861737], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1366800.0000, 
sim time next is 1367400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.96706534765517, 0.4576913461369636, 1.0, 1.0, 67410.58191113203], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.580588778971264, 0.6525637820456546, 1.0, 1.0, 0.3210027710053906], 
reward next is 0.6790, 
noisyNet noise sample is [array([0.41710755], dtype=float32), 1.4834856]. 
=============================================
[2019-04-04 07:43:31,252] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5860806e-11 6.5670924e-10 2.1182208e-29 7.8510827e-27 3.5139636e-22
 1.0000000e+00 1.2196400e-21], sum to 1.0000
[2019-04-04 07:43:31,253] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2357
[2019-04-04 07:43:31,326] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.9919376567396, 0.4351606810403794, 1.0, 1.0, 115350.6804688235], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1363200.0000, 
sim time next is 1363800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.82650109826463, 0.4746019686469252, 1.0, 1.0, 173374.3180651607], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5688750915220524, 0.6582006562156417, 1.0, 1.0, 0.8255919907864796], 
reward next is 0.1744, 
noisyNet noise sample is [array([0.5701821], dtype=float32), -0.80040234]. 
=============================================
[2019-04-04 07:43:38,913] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.7510257e-11 2.0246596e-09 5.8026854e-28 4.8925461e-27 2.9386163e-21
 1.0000000e+00 2.0490603e-21], sum to 1.0000
[2019-04-04 07:43:38,914] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9003
[2019-04-04 07:43:38,923] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.1, 92.0, 0.0, 0.0, 26.0, 25.39093515015161, 0.4632602595449458, 0.0, 1.0, 66580.01180554692], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1475400.0000, 
sim time next is 1476000.0000, 
raw observation next is [2.2, 92.0, 0.0, 0.0, 26.0, 25.34732627950481, 0.4679777470488888, 0.0, 1.0, 58844.20187082824], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6122771899587341, 0.6559925823496296, 0.0, 1.0, 0.2802104850991821], 
reward next is 0.7198, 
noisyNet noise sample is [array([0.06919841], dtype=float32), 0.11193912]. 
=============================================
[2019-04-04 07:43:38,929] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[87.371666]
 [87.139435]
 [86.80205 ]
 [86.506966]
 [86.521706]], R is [[87.32189941]
 [87.13163757]
 [86.93816376]
 [86.8014679 ]
 [86.8441925 ]].
[2019-04-04 07:43:43,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3843243e-12 1.1249343e-09 1.2310819e-30 1.7135572e-28 5.0142959e-23
 1.0000000e+00 4.9607575e-23], sum to 1.0000
[2019-04-04 07:43:43,647] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5953
[2019-04-04 07:43:43,655] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.266666666666667, 65.66666666666667, 185.8333333333333, 96.0, 26.0, 26.72715417435157, 0.7213878221643358, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1592400.0000, 
sim time next is 1593000.0000, 
raw observation next is [8.55, 64.5, 200.0, 88.0, 26.0, 26.78202466763614, 0.7272374585834583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6994459833795015, 0.645, 0.6666666666666666, 0.09723756906077348, 0.6666666666666666, 0.7318353889696784, 0.7424124861944862, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.408609], dtype=float32), 0.414426]. 
=============================================
[2019-04-04 07:43:43,660] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[92.56226]
 [92.48235]
 [92.46516]
 [92.46279]
 [92.45547]], R is [[92.65823364]
 [92.73165131]
 [92.80433655]
 [92.876297  ]
 [92.94753265]].
[2019-04-04 07:43:44,892] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1332378e-10 3.5565753e-09 1.5520154e-26 3.9293880e-24 3.3214158e-20
 1.0000000e+00 6.9497305e-20], sum to 1.0000
[2019-04-04 07:43:44,899] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3507
[2019-04-04 07:43:44,949] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 83.0, 122.5, 0.0, 26.0, 25.00128072285539, 0.3504538243078549, 0.0, 1.0, 32786.27399759259], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1774800.0000, 
sim time next is 1775400.0000, 
raw observation next is [-2.8, 83.0, 121.3333333333333, 0.0, 26.0, 25.0001381362959, 0.3467115395963287, 0.0, 1.0, 38611.20704254595], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.40444444444444433, 0.0, 0.6666666666666666, 0.5833448446913249, 0.6155705131987762, 0.0, 1.0, 0.18386289067879025], 
reward next is 0.8161, 
noisyNet noise sample is [array([0.12598613], dtype=float32), 0.16840218]. 
=============================================
[2019-04-04 07:43:46,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1862414e-11 2.9775174e-10 3.2265760e-29 5.0047497e-27 1.1554678e-21
 1.0000000e+00 8.8720967e-22], sum to 1.0000
[2019-04-04 07:43:46,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4186
[2019-04-04 07:43:46,548] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 66.5, 0.0, 26.0, 25.47373680942638, 0.5259537708683637, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1695600.0000, 
sim time next is 1696200.0000, 
raw observation next is [1.183333333333333, 86.83333333333334, 62.0, 0.0, 26.0, 25.78764630151948, 0.5501785346181678, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49538319482917825, 0.8683333333333334, 0.20666666666666667, 0.0, 0.6666666666666666, 0.6489705251266233, 0.6833928448727226, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.303217], dtype=float32), -0.8839524]. 
=============================================
[2019-04-04 07:43:48,895] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.1843533e-10 6.7292185e-09 2.4655865e-26 1.3132462e-24 7.3232373e-19
 1.0000000e+00 2.0234757e-19], sum to 1.0000
[2019-04-04 07:43:48,895] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7547
[2019-04-04 07:43:48,939] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.833333333333334, 76.33333333333334, 0.0, 0.0, 26.0, 25.31884693430874, 0.3106445787580351, 1.0, 1.0, 29428.62226029583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1966800.0000, 
sim time next is 1967400.0000, 
raw observation next is [-4.75, 75.0, 0.0, 0.0, 26.0, 25.13656910292825, 0.3018486939378899, 1.0, 1.0, 82046.40752468245], 
processed observation next is [1.0, 0.782608695652174, 0.3310249307479225, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5947140919106874, 0.60061623131263, 1.0, 1.0, 0.39069717868896403], 
reward next is 0.6093, 
noisyNet noise sample is [array([0.99437046], dtype=float32), -1.3966751]. 
=============================================
[2019-04-04 07:43:55,094] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7682272e-10 7.7602476e-09 1.8400712e-26 2.4055905e-24 1.1925796e-19
 1.0000000e+00 3.3653287e-19], sum to 1.0000
[2019-04-04 07:43:55,094] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8160
[2019-04-04 07:43:55,145] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 83.0, 119.0, 0.0, 26.0, 24.96439456273545, 0.3463579267881324, 0.0, 1.0, 54828.90168155248], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1776600.0000, 
sim time next is 1777200.0000, 
raw observation next is [-2.8, 83.0, 115.6666666666667, 0.0, 26.0, 24.96167245328473, 0.3575402757344004, 0.0, 1.0, 49751.14215381105], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.38555555555555565, 0.0, 0.6666666666666666, 0.5801393711070609, 0.6191800919114668, 0.0, 1.0, 0.23691020073243357], 
reward next is 0.7631, 
noisyNet noise sample is [array([-0.7687431], dtype=float32), -0.7952944]. 
=============================================
[2019-04-04 07:43:55,732] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7817030e-08 4.3593548e-08 8.1521399e-25 3.6682849e-23 3.4561014e-18
 9.9999988e-01 2.1202707e-17], sum to 1.0000
[2019-04-04 07:43:55,732] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5601
[2019-04-04 07:43:55,752] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.69257264824572, 0.2316150998966837, 0.0, 1.0, 43000.31830027781], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1983600.0000, 
sim time next is 1984200.0000, 
raw observation next is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.64378953849547, 0.2224093403647981, 0.0, 1.0, 42988.65764516341], 
processed observation next is [1.0, 1.0, 0.3074792243767313, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5536491282079558, 0.574136446788266, 0.0, 1.0, 0.2047078935483972], 
reward next is 0.7953, 
noisyNet noise sample is [array([0.22902781], dtype=float32), 1.8027154]. 
=============================================
[2019-04-04 07:43:55,963] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.2958377e-11 1.4201230e-09 3.7081664e-26 1.1391627e-25 6.2739105e-21
 1.0000000e+00 1.4169791e-20], sum to 1.0000
[2019-04-04 07:43:55,963] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7985
[2019-04-04 07:43:56,024] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 82.66666666666667, 58.0, 0.0, 26.0, 25.24048933272555, 0.382860979283547, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2044200.0000, 
sim time next is 2044800.0000, 
raw observation next is [-3.9, 82.0, 51.5, 0.0, 26.0, 25.65918023910019, 0.4100935222599352, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.17166666666666666, 0.0, 0.6666666666666666, 0.6382650199250159, 0.6366978407533117, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2976558], dtype=float32), 0.63113487]. 
=============================================
[2019-04-04 07:44:03,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.3579811e-10 9.8722079e-09 3.3211919e-25 1.7054100e-23 2.6953636e-19
 1.0000000e+00 3.8496816e-18], sum to 1.0000
[2019-04-04 07:44:03,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6091
[2019-04-04 07:44:03,646] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.7, 67.33333333333334, 141.0, 0.0, 26.0, 25.93600357184424, 0.3566254965786185, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2123400.0000, 
sim time next is 2124000.0000, 
raw observation next is [-5.6, 68.0, 137.0, 0.0, 26.0, 25.54703328039283, 0.3979210990928978, 1.0, 1.0, 53824.00838728814], 
processed observation next is [1.0, 0.6086956521739131, 0.30747922437673136, 0.68, 0.45666666666666667, 0.0, 0.6666666666666666, 0.6289194400327359, 0.6326403663642993, 1.0, 1.0, 0.2563048018442292], 
reward next is 0.7437, 
noisyNet noise sample is [array([1.7423271], dtype=float32), -0.14275548]. 
=============================================
[2019-04-04 07:44:03,670] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.116585]
 [75.36628 ]
 [75.61663 ]
 [75.8432  ]
 [76.1964  ]], R is [[75.16477966]
 [75.41313171]
 [75.65900421]
 [75.7628479 ]
 [75.85612488]].
[2019-04-04 07:44:11,738] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.7273297e-10 3.1019640e-09 1.9756687e-25 8.5636042e-24 7.0054976e-19
 1.0000000e+00 4.6911558e-19], sum to 1.0000
[2019-04-04 07:44:11,738] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6113
[2019-04-04 07:44:11,767] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.966666666666667, 83.0, 0.0, 0.0, 26.0, 25.27757834205332, 0.413672457834399, 0.0, 1.0, 43086.33272279808], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2150400.0000, 
sim time next is 2151000.0000, 
raw observation next is [-6.15, 83.0, 0.0, 0.0, 26.0, 25.26787101905561, 0.4091321879713989, 0.0, 1.0, 42958.98619060393], 
processed observation next is [1.0, 0.9130434782608695, 0.29224376731301943, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6056559182546343, 0.6363773959904663, 0.0, 1.0, 0.20456660090763776], 
reward next is 0.7954, 
noisyNet noise sample is [array([-0.37432486], dtype=float32), -0.2798995]. 
=============================================
[2019-04-04 07:44:11,789] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[76.77174 ]
 [76.770256]
 [76.79515 ]
 [76.79908 ]
 [76.91672 ]], R is [[76.72974396]
 [76.75727844]
 [76.78343201]
 [76.80626678]
 [76.81856537]].
[2019-04-04 07:44:22,384] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9805516e-10 5.2216333e-09 6.5845696e-27 7.2963066e-25 8.2038545e-20
 1.0000000e+00 9.4107899e-20], sum to 1.0000
[2019-04-04 07:44:22,384] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2167
[2019-04-04 07:44:22,424] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 45.0, 171.0, 64.5, 26.0, 25.7804699135174, 0.4593240235849318, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2296800.0000, 
sim time next is 2297400.0000, 
raw observation next is [-0.3166666666666667, 44.66666666666666, 154.3333333333333, 63.0, 26.0, 26.05202319657091, 0.4833436670673492, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.45383194829178214, 0.44666666666666655, 0.5144444444444443, 0.06961325966850829, 0.6666666666666666, 0.6710019330475759, 0.6611145556891164, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1256104], dtype=float32), -0.29358712]. 
=============================================
[2019-04-04 07:44:24,837] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6428480e-09 4.0505206e-08 3.1958846e-25 3.0357255e-23 1.6191874e-18
 1.0000000e+00 8.2997395e-19], sum to 1.0000
[2019-04-04 07:44:24,837] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2405
[2019-04-04 07:44:24,890] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 55.0, 0.0, 0.0, 26.0, 25.43301926057366, 0.4261490673601802, 0.0, 1.0, 32336.74252528141], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2323800.0000, 
sim time next is 2324400.0000, 
raw observation next is [-1.7, 55.33333333333333, 0.0, 0.0, 26.0, 25.40218378438302, 0.4230242712954465, 0.0, 1.0, 51810.36194285156], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5533333333333332, 0.0, 0.0, 0.6666666666666666, 0.616848648698585, 0.6410080904318155, 0.0, 1.0, 0.24671600925167408], 
reward next is 0.7533, 
noisyNet noise sample is [array([0.7910708], dtype=float32), 0.57812715]. 
=============================================
[2019-04-04 07:44:26,542] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8083063e-10 3.2365326e-09 7.2052999e-27 7.6436347e-25 7.5757522e-21
 1.0000000e+00 8.8005109e-20], sum to 1.0000
[2019-04-04 07:44:26,542] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7856
[2019-04-04 07:44:26,577] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3064901e-10 2.0973931e-09 1.1521550e-26 4.3239907e-25 6.5491469e-20
 1.0000000e+00 1.4848462e-19], sum to 1.0000
[2019-04-04 07:44:26,577] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7754
[2019-04-04 07:44:26,582] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 83.33333333333334, 0.0, 0.0, 26.0, 25.70976880397636, 0.4179460351947294, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2056800.0000, 
sim time next is 2057400.0000, 
raw observation next is [-3.9, 84.0, 0.0, 0.0, 26.0, 25.52679598965621, 0.380167539066483, 0.0, 1.0, 9355.89811702314], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.84, 0.0, 0.0, 0.6666666666666666, 0.6272329991380176, 0.626722513022161, 0.0, 1.0, 0.04455189579534829], 
reward next is 0.9554, 
noisyNet noise sample is [array([0.44435686], dtype=float32), 1.5941741]. 
=============================================
[2019-04-04 07:44:26,630] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.716666666666667, 81.5, 106.0, 63.99999999999999, 26.0, 25.45550190737459, 0.3225826986265306, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2105400.0000, 
sim time next is 2106000.0000, 
raw observation next is [-7.8, 82.0, 123.0, 77.5, 26.0, 25.48065807651072, 0.3381847230361474, 1.0, 1.0, 18741.48153585846], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.82, 0.41, 0.0856353591160221, 0.6666666666666666, 0.6233881730425601, 0.6127282410120491, 1.0, 1.0, 0.08924515017075457], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.63603944], dtype=float32), 1.1712798]. 
=============================================
[2019-04-04 07:44:26,637] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.00234]
 [80.74649]
 [80.43409]
 [80.09576]
 [79.70474]], R is [[81.42140198]
 [81.60718536]
 [81.79111481]
 [81.97320557]
 [82.1534729 ]].
[2019-04-04 07:44:28,570] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9666123e-09 2.3214886e-08 2.8804536e-25 1.6315901e-23 2.1153178e-18
 1.0000000e+00 6.9460275e-18], sum to 1.0000
[2019-04-04 07:44:28,570] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0012
[2019-04-04 07:44:28,583] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 62.5, 0.0, 0.0, 26.0, 24.81828029479663, 0.2649517954661158, 0.0, 1.0, 38511.03404110295], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2335800.0000, 
sim time next is 2336400.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.79001587154515, 0.2688315479867441, 0.0, 1.0, 38522.95655818343], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5658346559620959, 0.5896105159955813, 0.0, 1.0, 0.18344265027706397], 
reward next is 0.8166, 
noisyNet noise sample is [array([-0.2284952], dtype=float32), -1.2125261]. 
=============================================
[2019-04-04 07:44:37,073] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4948821e-09 3.1613791e-08 9.9737814e-25 1.8259146e-23 1.2867055e-18
 1.0000000e+00 4.4969777e-17], sum to 1.0000
[2019-04-04 07:44:37,074] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7981
[2019-04-04 07:44:37,088] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.27348727963535, 0.1288614339286308, 0.0, 1.0, 40866.90425966296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2350800.0000, 
sim time next is 2351400.0000, 
raw observation next is [-3.3, 68.33333333333333, 0.0, 0.0, 26.0, 24.23965585296918, 0.1224874438856606, 0.0, 1.0, 40959.05553368173], 
processed observation next is [0.0, 0.21739130434782608, 0.37119113573407203, 0.6833333333333332, 0.0, 0.0, 0.6666666666666666, 0.5199713210807649, 0.5408291479618869, 0.0, 1.0, 0.19504312158896062], 
reward next is 0.8050, 
noisyNet noise sample is [array([0.27235407], dtype=float32), -0.30238086]. 
=============================================
[2019-04-04 07:44:37,371] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.4293587e-10 1.8351111e-08 2.5362090e-25 6.7541908e-24 4.1582426e-19
 1.0000000e+00 1.2259952e-18], sum to 1.0000
[2019-04-04 07:44:37,371] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9170
[2019-04-04 07:44:37,439] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8999999999999999, 50.5, 0.0, 0.0, 26.0, 24.89548908208593, 0.3161692003754803, 1.0, 1.0, 135165.5567412156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2309400.0000, 
sim time next is 2310000.0000, 
raw observation next is [-1.0, 51.0, 0.0, 0.0, 26.0, 25.14712304854552, 0.3949133395806414, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4349030470914128, 0.51, 0.0, 0.0, 0.6666666666666666, 0.5955935873787933, 0.6316377798602139, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27473903], dtype=float32), -0.6397161]. 
=============================================
[2019-04-04 07:44:37,466] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.76864]
 [76.14236]
 [75.42332]
 [75.17139]
 [75.18662]], R is [[77.06900024]
 [76.65466309]
 [75.95017242]
 [75.82054901]
 [76.06234741]].
[2019-04-04 07:44:37,709] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3977773e-10 4.6134399e-09 1.9194976e-26 1.2939915e-24 4.5136587e-20
 1.0000000e+00 6.9493330e-20], sum to 1.0000
[2019-04-04 07:44:37,714] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7969
[2019-04-04 07:44:37,793] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 69.0, 0.0, 0.0, 26.0, 24.81157685581888, 0.4004334896178374, 1.0, 1.0, 191048.2190190194], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2223600.0000, 
sim time next is 2224200.0000, 
raw observation next is [-4.5, 68.5, 0.0, 0.0, 26.0, 25.10354775028267, 0.4387595376696549, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.685, 0.0, 0.0, 0.6666666666666666, 0.5919623125235557, 0.6462531792232183, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00350243], dtype=float32), 0.3013564]. 
=============================================
[2019-04-04 07:44:45,436] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0294340e-09 2.4309492e-07 2.1757413e-24 4.2673298e-23 1.1746507e-18
 9.9999976e-01 2.7200819e-18], sum to 1.0000
[2019-04-04 07:44:45,437] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0616
[2019-04-04 07:44:45,459] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.78279101768214, 0.2492490475083805, 0.0, 1.0, 38610.86455626436], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2338200.0000, 
sim time next is 2338800.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.72399014774924, 0.2395209467274793, 0.0, 1.0, 38721.5800340112], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5603325123124367, 0.5798403155758264, 0.0, 1.0, 0.18438847635243427], 
reward next is 0.8156, 
noisyNet noise sample is [array([0.05472514], dtype=float32), -1.0577767]. 
=============================================
[2019-04-04 07:44:53,161] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2288680e-09 4.0336289e-08 8.8758795e-24 2.3328263e-22 5.5757565e-18
 1.0000000e+00 2.3131509e-17], sum to 1.0000
[2019-04-04 07:44:53,161] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5911
[2019-04-04 07:44:53,179] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 87.0, 0.0, 0.0, 26.0, 23.40160163833825, -0.03562714537551242, 0.0, 1.0, 44318.26056192593], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2694600.0000, 
sim time next is 2695200.0000, 
raw observation next is [-15.0, 85.66666666666666, 0.0, 0.0, 26.0, 23.35208161083843, -0.04195298078915421, 0.0, 1.0, 44216.51554865587], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.8566666666666666, 0.0, 0.0, 0.6666666666666666, 0.4460068009032024, 0.486015673070282, 0.0, 1.0, 0.21055483594598035], 
reward next is 0.7894, 
noisyNet noise sample is [array([-1.7480555], dtype=float32), -0.114903204]. 
=============================================
[2019-04-04 07:44:53,457] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4937566e-09 9.2970204e-08 6.1440880e-23 1.7639154e-21 2.9250988e-18
 9.9999988e-01 2.2395390e-16], sum to 1.0000
[2019-04-04 07:44:53,458] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7686
[2019-04-04 07:44:53,476] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.566666666666666, 60.66666666666667, 0.0, 0.0, 26.0, 23.31678559223303, -0.1279216809487761, 0.0, 1.0, 44295.88722860877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2438400.0000, 
sim time next is 2439000.0000, 
raw observation next is [-8.65, 60.5, 0.0, 0.0, 26.0, 23.29353983916405, -0.1357208393790042, 0.0, 1.0, 44255.1847029104], 
processed observation next is [0.0, 0.21739130434782608, 0.22299168975069253, 0.605, 0.0, 0.0, 0.6666666666666666, 0.4411283199303376, 0.4547597202069986, 0.0, 1.0, 0.2107389747757638], 
reward next is 0.7893, 
noisyNet noise sample is [array([-0.43109024], dtype=float32), -0.58047885]. 
=============================================
[2019-04-04 07:44:53,479] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[70.11121 ]
 [70.193436]
 [70.28473 ]
 [70.34111 ]
 [70.41565 ]], R is [[70.10115051]
 [70.18920898]
 [70.27619171]
 [70.36209869]
 [70.44701385]].
[2019-04-04 07:44:54,214] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5760248e-09 7.6279676e-09 1.0937229e-24 1.8669204e-23 1.4759131e-18
 1.0000000e+00 1.7998549e-18], sum to 1.0000
[2019-04-04 07:44:54,214] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2468
[2019-04-04 07:44:54,227] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 59.0, 0.0, 0.0, 26.0, 25.08125234469609, 0.3091760744266629, 0.0, 1.0, 42696.59854506038], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2588400.0000, 
sim time next is 2589000.0000, 
raw observation next is [-4.0, 59.5, 0.0, 0.0, 26.0, 25.02023677306313, 0.2983480504908543, 0.0, 1.0, 42118.69311684432], 
processed observation next is [1.0, 1.0, 0.3518005540166205, 0.595, 0.0, 0.0, 0.6666666666666666, 0.5850197310885941, 0.5994493501636181, 0.0, 1.0, 0.20056520531830627], 
reward next is 0.7994, 
noisyNet noise sample is [array([0.1273558], dtype=float32), -0.568908]. 
=============================================
[2019-04-04 07:44:54,230] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.472694]
 [77.48172 ]
 [77.475464]
 [77.46716 ]
 [77.31454 ]], R is [[77.48545837]
 [77.50728607]
 [77.52052307]
 [77.51291656]
 [77.48174286]].
[2019-04-04 07:44:55,787] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7510378e-11 1.3739963e-09 5.2347213e-26 1.1855317e-25 1.9423152e-20
 1.0000000e+00 9.5111211e-20], sum to 1.0000
[2019-04-04 07:44:55,787] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1531
[2019-04-04 07:44:55,818] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.333333333333334, 62.33333333333334, 112.8333333333333, 796.0, 26.0, 25.9437117050253, 0.4666115158403307, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2722800.0000, 
sim time next is 2723400.0000, 
raw observation next is [-7.0, 61.5, 113.0, 799.0, 26.0, 25.86483538246843, 0.3619502188077265, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2686980609418283, 0.615, 0.37666666666666665, 0.8828729281767956, 0.6666666666666666, 0.6554029485390359, 0.6206500729359088, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1111703], dtype=float32), -1.4469739]. 
=============================================
[2019-04-04 07:44:56,292] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.8475847e-09 2.1056191e-08 1.4239996e-23 3.7421789e-22 3.3510357e-17
 1.0000000e+00 3.6534794e-17], sum to 1.0000
[2019-04-04 07:44:56,300] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5999
[2019-04-04 07:44:56,326] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666667, 43.0, 0.0, 0.0, 26.0, 24.93861783447537, 0.2229316747108338, 0.0, 1.0, 42972.89626739178], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2413200.0000, 
sim time next is 2413800.0000, 
raw observation next is [-4.75, 42.5, 0.0, 0.0, 26.0, 24.89207616204424, 0.2125057676489288, 0.0, 1.0, 42991.61512267384], 
processed observation next is [0.0, 0.9565217391304348, 0.3310249307479225, 0.425, 0.0, 0.0, 0.6666666666666666, 0.5743396801703534, 0.5708352558829762, 0.0, 1.0, 0.20472197677463733], 
reward next is 0.7953, 
noisyNet noise sample is [array([-0.4139605], dtype=float32), 0.63234]. 
=============================================
[2019-04-04 07:44:59,321] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1438541e-09 1.5110565e-08 5.1790545e-23 2.9435864e-21 7.2190070e-18
 1.0000000e+00 5.8318331e-17], sum to 1.0000
[2019-04-04 07:44:59,322] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5500
[2019-04-04 07:44:59,368] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 26.5, 0.0, 0.0, 26.0, 24.85893252401149, 0.2064678281656796, 0.0, 1.0, 90358.52771250856], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2482200.0000, 
sim time next is 2482800.0000, 
raw observation next is [1.833333333333333, 27.0, 0.0, 0.0, 26.0, 24.8501720086123, 0.2152601539275842, 0.0, 1.0, 69708.12922578372], 
processed observation next is [0.0, 0.7391304347826086, 0.5133887349953832, 0.27, 0.0, 0.0, 0.6666666666666666, 0.5708476673843584, 0.5717533846425281, 0.0, 1.0, 0.331943472503732], 
reward next is 0.6681, 
noisyNet noise sample is [array([0.17672454], dtype=float32), 0.90580875]. 
=============================================
[2019-04-04 07:44:59,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.1924604e-10 3.8547267e-08 9.4567935e-25 8.6236941e-24 7.8664333e-19
 1.0000000e+00 1.9543128e-18], sum to 1.0000
[2019-04-04 07:44:59,682] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1474
[2019-04-04 07:44:59,700] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.70003962233048, 0.2111893491351483, 0.0, 1.0, 41495.89234606855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2773200.0000, 
sim time next is 2773800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.63578765697242, 0.2053459388168005, 0.0, 1.0, 41387.14850355319], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5529823047477015, 0.5684486462722668, 0.0, 1.0, 0.19708165954072945], 
reward next is 0.8029, 
noisyNet noise sample is [array([-0.3348289], dtype=float32), 0.15476923]. 
=============================================
[2019-04-04 07:45:14,830] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.87284996e-11 4.59012550e-10 2.82089063e-29 1.04115534e-26
 3.75338757e-22 1.00000000e+00 1.45678088e-21], sum to 1.0000
[2019-04-04 07:45:14,830] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8827
[2019-04-04 07:45:14,893] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 24.91726486211086, 0.4026277437424834, 0.0, 1.0, 49856.07582795578], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2923200.0000, 
sim time next is 2923800.0000, 
raw observation next is [-1.0, 79.16666666666667, 0.0, 0.0, 26.0, 24.95747041193793, 0.4062932285490475, 1.0, 1.0, 19495.81979073956], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.7916666666666667, 0.0, 0.0, 0.6666666666666666, 0.5797892009948274, 0.6354310761830159, 1.0, 1.0, 0.09283723709875981], 
reward next is 0.9072, 
noisyNet noise sample is [array([-1.2855835], dtype=float32), -0.82724196]. 
=============================================
[2019-04-04 07:45:24,150] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8728664e-11 3.7324860e-10 3.9262147e-30 2.1570276e-28 6.8446892e-23
 1.0000000e+00 2.6495967e-22], sum to 1.0000
[2019-04-04 07:45:24,150] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8159
[2019-04-04 07:45:24,208] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.08978207492596, 0.4132519307649558, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2919600.0000, 
sim time next is 2920200.0000, 
raw observation next is [-1.0, 89.66666666666667, 0.0, 0.0, 26.0, 25.15431098505591, 0.41295332524116, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.8966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5961925820879926, 0.6376511084137201, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8924467], dtype=float32), 0.86038435]. 
=============================================
[2019-04-04 07:45:36,559] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6810448e-10 8.3441819e-08 1.2958141e-24 4.7687298e-23 1.2861264e-18
 9.9999988e-01 8.8565546e-19], sum to 1.0000
[2019-04-04 07:45:36,560] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6213
[2019-04-04 07:45:36,579] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.0, 76.0, 0.0, 0.0, 26.0, 24.48140527495197, 0.2209205188449593, 0.0, 1.0, 43706.5326040172], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3301200.0000, 
sim time next is 3301800.0000, 
raw observation next is [-10.16666666666667, 76.0, 0.0, 0.0, 26.0, 24.47132094795427, 0.2167825673098577, 0.0, 1.0, 43648.18802809566], 
processed observation next is [1.0, 0.21739130434782608, 0.18097876269621416, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5392767456628557, 0.5722608557699526, 0.0, 1.0, 0.20784851441950314], 
reward next is 0.7922, 
noisyNet noise sample is [array([1.5039252], dtype=float32), 0.9881085]. 
=============================================
[2019-04-04 07:45:37,342] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3573290e-10 4.9582103e-09 8.2202087e-27 2.5416242e-24 1.5620403e-20
 1.0000000e+00 3.3125476e-19], sum to 1.0000
[2019-04-04 07:45:37,342] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2428
[2019-04-04 07:45:37,382] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 217.0, 154.0, 26.0, 24.97850470787592, 0.3390372724516872, 0.0, 1.0, 18743.28119188454], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2977200.0000, 
sim time next is 2977800.0000, 
raw observation next is [-3.0, 65.0, 230.0, 197.3333333333333, 26.0, 24.9963368883564, 0.3424755225193072, 0.0, 1.0, 18739.89428127718], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.7666666666666667, 0.21804788213627987, 0.6666666666666666, 0.5830280740296999, 0.6141585075064357, 0.0, 1.0, 0.08923759181560562], 
reward next is 0.9108, 
noisyNet noise sample is [array([1.2058618], dtype=float32), -0.7167581]. 
=============================================
[2019-04-04 07:45:37,916] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.1159884e-11 2.7584282e-09 2.8668796e-27 3.2783319e-25 5.9148527e-21
 1.0000000e+00 1.8899390e-20], sum to 1.0000
[2019-04-04 07:45:37,916] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5905
[2019-04-04 07:45:37,959] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.833333333333334, 75.83333333333334, 98.0, 542.0, 26.0, 26.12435615181302, 0.5233322289776516, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3316200.0000, 
sim time next is 3316800.0000, 
raw observation next is [-8.666666666666668, 74.66666666666667, 101.0, 578.5, 26.0, 26.19778104253987, 0.5294674312593952, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.22253000923361033, 0.7466666666666667, 0.33666666666666667, 0.6392265193370166, 0.6666666666666666, 0.6831484202116558, 0.6764891437531317, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07578024], dtype=float32), 0.18089995]. 
=============================================
[2019-04-04 07:45:40,106] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5257943e-09 3.6423565e-08 1.3556052e-24 3.7202292e-23 9.1971079e-19
 1.0000000e+00 5.0040093e-18], sum to 1.0000
[2019-04-04 07:45:40,106] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4850
[2019-04-04 07:45:40,133] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.816666666666666, 76.16666666666667, 0.0, 0.0, 26.0, 24.49023517303901, 0.2247646916343202, 0.0, 1.0, 43761.61456668792], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3300600.0000, 
sim time next is 3301200.0000, 
raw observation next is [-10.0, 76.0, 0.0, 0.0, 26.0, 24.48197698787257, 0.2210465167498432, 0.0, 1.0, 43706.19970792518], 
processed observation next is [1.0, 0.21739130434782608, 0.18559556786703602, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5401647489893809, 0.5736821722499478, 0.0, 1.0, 0.20812476051392945], 
reward next is 0.7919, 
noisyNet noise sample is [array([-0.53746384], dtype=float32), -1.7359519]. 
=============================================
[2019-04-04 07:45:40,368] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5798299e-11 1.9513122e-09 2.3130608e-26 1.1393111e-24 5.0375787e-20
 1.0000000e+00 5.8289996e-20], sum to 1.0000
[2019-04-04 07:45:40,368] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7959
[2019-04-04 07:45:40,393] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 98.0, 737.0, 26.0, 25.17617565376543, 0.4081533761388808, 0.0, 1.0, 18706.76182187096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2990400.0000, 
sim time next is 2991000.0000, 
raw observation next is [-2.0, 60.0, 95.0, 723.0, 26.0, 25.1452760988774, 0.4046696643474286, 0.0, 1.0, 26529.37968332747], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6, 0.31666666666666665, 0.7988950276243094, 0.6666666666666666, 0.59543967490645, 0.6348898881158095, 0.0, 1.0, 0.12633037944441652], 
reward next is 0.8737, 
noisyNet noise sample is [array([0.60449314], dtype=float32), 0.7074361]. 
=============================================
[2019-04-04 07:45:40,406] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.97403]
 [77.19602]
 [77.51534]
 [77.80855]
 [78.16917]], R is [[76.95818329]
 [77.09951782]
 [77.32852173]
 [77.46613312]
 [77.60236359]].
[2019-04-04 07:45:44,425] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.7623967e-11 3.1878988e-09 1.4008980e-28 1.7612552e-26 6.0501522e-22
 1.0000000e+00 7.7373804e-21], sum to 1.0000
[2019-04-04 07:45:44,430] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6799
[2019-04-04 07:45:44,445] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 100.0, 0.0, 0.0, 26.0, 25.3199409378909, 0.4942767284584277, 0.0, 1.0, 40578.87633492117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3216600.0000, 
sim time next is 3217200.0000, 
raw observation next is [-2.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.32460250173402, 0.4890967261507077, 0.0, 1.0, 40585.42880629021], 
processed observation next is [1.0, 0.21739130434782608, 0.38873499538319484, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6103835418111684, 0.6630322420502359, 0.0, 1.0, 0.19326394669662006], 
reward next is 0.8067, 
noisyNet noise sample is [array([0.95116436], dtype=float32), -0.19456407]. 
=============================================
[2019-04-04 07:45:48,401] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0604732e-13 6.8404504e-11 7.9259874e-33 8.9565297e-30 2.8228554e-25
 1.0000000e+00 2.5078772e-24], sum to 1.0000
[2019-04-04 07:45:48,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4597
[2019-04-04 07:45:48,415] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 103.5, 696.5, 26.0, 26.64464233686327, 0.6339443979195666, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3146400.0000, 
sim time next is 3147000.0000, 
raw observation next is [7.0, 100.0, 105.0, 713.0, 26.0, 26.70806740674579, 0.6552142855542773, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.35, 0.7878453038674034, 0.6666666666666666, 0.7256722838954826, 0.7184047618514257, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5915314], dtype=float32), -0.8649855]. 
=============================================
[2019-04-04 07:45:48,475] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[100.10256 ]
 [100.290886]
 [100.36204 ]
 [100.3183  ]
 [100.128456]], R is [[99.93880463]
 [99.93941498]
 [99.94002533]
 [99.94062805]
 [99.94122314]].
[2019-04-04 07:45:53,469] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7402429e-12 4.5644207e-11 6.0902473e-33 1.7716154e-30 6.0269693e-25
 1.0000000e+00 5.3161680e-24], sum to 1.0000
[2019-04-04 07:45:53,469] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4540
[2019-04-04 07:45:53,483] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 112.3333333333333, 811.6666666666666, 26.0, 27.3159103812561, 0.856794857753988, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3157800.0000, 
sim time next is 3158400.0000, 
raw observation next is [7.0, 100.0, 112.1666666666667, 808.8333333333334, 26.0, 27.33089177976377, 0.8690691687207721, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.373888888888889, 0.8937384898710866, 0.6666666666666666, 0.7775743149803143, 0.7896897229069241, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8051123], dtype=float32), -0.48903203]. 
=============================================
[2019-04-04 07:45:55,581] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.08862336e-10 5.40275957e-09 1.21127712e-26 1.72167887e-24
 3.55325998e-20 1.00000000e+00 8.53049588e-20], sum to 1.0000
[2019-04-04 07:45:55,583] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5000
[2019-04-04 07:45:55,622] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 70.0, 0.0, 0.0, 26.0, 25.28242855469399, 0.4221989613031789, 0.0, 1.0, 40966.93618843595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3550200.0000, 
sim time next is 3550800.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 26.0, 25.26733661682589, 0.4137452721930978, 0.0, 1.0, 40859.29947193808], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6056113847354908, 0.6379150907310326, 0.0, 1.0, 0.19456809272351466], 
reward next is 0.8054, 
noisyNet noise sample is [array([-0.15351337], dtype=float32), -1.8698373]. 
=============================================
[2019-04-04 07:45:56,316] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8936158e-09 3.2945451e-08 1.0936478e-24 3.3800661e-23 1.0471268e-17
 1.0000000e+00 3.1418523e-17], sum to 1.0000
[2019-04-04 07:45:56,316] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2620
[2019-04-04 07:45:56,330] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 46.0, 0.0, 0.0, 26.0, 25.41756115933284, 0.3914973348570298, 0.0, 1.0, 58996.64690590644], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3619800.0000, 
sim time next is 3620400.0000, 
raw observation next is [-1.666666666666667, 47.33333333333333, 0.0, 0.0, 26.0, 25.39991949426165, 0.3907859619436702, 0.0, 1.0, 54934.03280625104], 
processed observation next is [0.0, 0.9130434782608695, 0.4164358264081256, 0.4733333333333333, 0.0, 0.0, 0.6666666666666666, 0.6166599578551374, 0.6302619873145567, 0.0, 1.0, 0.2615906324107192], 
reward next is 0.7384, 
noisyNet noise sample is [array([-0.81425506], dtype=float32), -1.1325231]. 
=============================================
[2019-04-04 07:45:56,733] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8139136e-12 1.8570703e-10 1.5931088e-29 1.9365672e-27 3.1736450e-22
 1.0000000e+00 8.5697244e-22], sum to 1.0000
[2019-04-04 07:45:56,734] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2601
[2019-04-04 07:45:56,748] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 71.0, 76.33333333333333, 627.6666666666666, 26.0, 26.58954287743919, 0.8037845349363048, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3253800.0000, 
sim time next is 3254400.0000, 
raw observation next is [-3.0, 71.0, 72.0, 598.5, 26.0, 26.78275278271611, 0.8227565158837623, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3795013850415513, 0.71, 0.24, 0.6613259668508288, 0.6666666666666666, 0.7318960652263424, 0.7742521719612542, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.599779], dtype=float32), -0.9363124]. 
=============================================
[2019-04-04 07:46:00,342] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2683579e-11 2.0549751e-09 2.9471856e-26 6.8236961e-25 3.1009350e-20
 1.0000000e+00 9.5778904e-21], sum to 1.0000
[2019-04-04 07:46:00,342] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0641
[2019-04-04 07:46:00,377] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.166666666666667, 46.66666666666667, 78.0, 616.3333333333334, 26.0, 25.50190748045217, 0.6000308716960121, 1.0, 1.0, 62378.03064504134], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3340200.0000, 
sim time next is 3340800.0000, 
raw observation next is [-2.0, 46.0, 73.5, 587.5, 26.0, 26.04204494118304, 0.6483160012345127, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.46, 0.245, 0.649171270718232, 0.6666666666666666, 0.6701704117652533, 0.7161053337448376, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.5148919], dtype=float32), 0.9413645]. 
=============================================
[2019-04-04 07:46:02,784] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.3782065e-10 2.6088754e-08 5.0008592e-26 8.4242480e-24 1.7045160e-19
 1.0000000e+00 2.2047812e-19], sum to 1.0000
[2019-04-04 07:46:02,792] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2277
[2019-04-04 07:46:02,804] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.85077324029807, 0.274203041431712, 0.0, 1.0, 41187.59130488877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3382800.0000, 
sim time next is 3383400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.85188682913365, 0.2726618762287066, 0.0, 1.0, 41268.76897226414], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5709905690944709, 0.5908872920762355, 0.0, 1.0, 0.19651794748697207], 
reward next is 0.8035, 
noisyNet noise sample is [array([-0.70230347], dtype=float32), 0.77692115]. 
=============================================
[2019-04-04 07:46:03,785] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2068939e-11 1.6936745e-09 2.6277487e-26 3.9491316e-25 1.3673455e-20
 1.0000000e+00 3.4418362e-20], sum to 1.0000
[2019-04-04 07:46:03,786] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4758
[2019-04-04 07:46:03,798] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 52.0, 114.0, 800.0, 26.0, 26.00731702655712, 0.5800951871968825, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3331800.0000, 
sim time next is 3332400.0000, 
raw observation next is [-4.333333333333334, 51.33333333333333, 112.6666666666667, 792.0, 26.0, 26.08313118745254, 0.5901560593011399, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3425669436749769, 0.5133333333333333, 0.37555555555555564, 0.8751381215469614, 0.6666666666666666, 0.6735942656210451, 0.6967186864337133, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.69899833], dtype=float32), 0.69924974]. 
=============================================
[2019-04-04 07:46:06,865] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.6735263e-12 4.5655113e-10 1.5689484e-29 1.8581206e-27 7.8664816e-22
 1.0000000e+00 7.7831728e-22], sum to 1.0000
[2019-04-04 07:46:06,870] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8188
[2019-04-04 07:46:06,902] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 77.0, 100.0, 675.0, 26.0, 26.12957238157847, 0.4860441888535057, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3749400.0000, 
sim time next is 3750000.0000, 
raw observation next is [-3.333333333333333, 77.0, 101.8333333333333, 690.6666666666666, 26.0, 26.15378665138049, 0.4914535485779877, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.37026777469990774, 0.77, 0.3394444444444443, 0.7631675874769797, 0.6666666666666666, 0.6794822209483741, 0.6638178495259959, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3134801], dtype=float32), -1.0217367]. 
=============================================
[2019-04-04 07:46:06,913] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[88.347626]
 [88.14162 ]
 [87.816696]
 [87.419136]
 [86.78041 ]], R is [[88.52548218]
 [88.64022827]
 [88.75382996]
 [88.86629486]
 [88.88666534]].
[2019-04-04 07:46:08,292] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8065644e-12 8.6655699e-10 4.8302342e-28 2.4470606e-27 2.9931367e-21
 1.0000000e+00 7.8558999e-21], sum to 1.0000
[2019-04-04 07:46:08,292] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9527
[2019-04-04 07:46:08,311] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 50.5, 106.3333333333333, 785.3333333333334, 26.0, 26.03706773890016, 0.5783663949245811, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3420600.0000, 
sim time next is 3421200.0000, 
raw observation next is [3.0, 52.0, 104.6666666666667, 780.1666666666667, 26.0, 25.43781218918868, 0.5193509242097284, 1.0, 1.0, 63199.78331019129], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.52, 0.348888888888889, 0.8620626151012892, 0.6666666666666666, 0.61981768243239, 0.6731169747365762, 1.0, 1.0, 0.300951349096149], 
reward next is 0.6990, 
noisyNet noise sample is [array([-0.4352097], dtype=float32), -0.4492905]. 
=============================================
[2019-04-04 07:46:08,875] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.9797785e-10 2.8877274e-09 3.6241188e-26 8.0887623e-25 8.9081851e-21
 1.0000000e+00 5.8925627e-19], sum to 1.0000
[2019-04-04 07:46:08,879] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4282
[2019-04-04 07:46:08,891] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.42332000466836, 0.4558003444788273, 0.0, 1.0, 28013.63339986143], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3544800.0000, 
sim time next is 3545400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.38672767257577, 0.4502700853867707, 0.0, 1.0, 54610.03971627458], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6155606393813141, 0.6500900284622569, 0.0, 1.0, 0.2600478081727361], 
reward next is 0.7400, 
noisyNet noise sample is [array([0.2273486], dtype=float32), -0.81131816]. 
=============================================
[2019-04-04 07:46:11,152] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5311987e-10 1.1188065e-08 1.2086093e-26 2.0331269e-24 6.8292600e-20
 1.0000000e+00 1.8989734e-19], sum to 1.0000
[2019-04-04 07:46:11,154] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9860
[2019-04-04 07:46:11,170] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.833333333333333, 76.0, 0.0, 0.0, 26.0, 25.27405391919069, 0.3587280713739254, 0.0, 1.0, 41154.42403866138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3905400.0000, 
sim time next is 3906000.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.23160500375077, 0.3542446613606353, 0.0, 1.0, 41006.7035448197], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.602633750312564, 0.6180815537868785, 0.0, 1.0, 0.1952700168800938], 
reward next is 0.8047, 
noisyNet noise sample is [array([2.265523], dtype=float32), -0.40660205]. 
=============================================
[2019-04-04 07:46:11,176] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.17968]
 [81.17335]
 [81.14643]
 [81.12589]
 [81.08828]], R is [[81.20517731]
 [81.19715118]
 [81.18663025]
 [81.17325592]
 [81.18145752]].
[2019-04-04 07:46:13,336] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.7504379e-11 2.1664990e-09 3.5564885e-27 6.8208556e-26 4.5857181e-20
 1.0000000e+00 1.1683932e-20], sum to 1.0000
[2019-04-04 07:46:13,336] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4410
[2019-04-04 07:46:13,349] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.82693672916823, 0.5365625830050281, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3783000.0000, 
sim time next is 3783600.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.74081729582459, 0.5021190938485348, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.40720221606648205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6450681079853826, 0.6673730312828449, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7445106], dtype=float32), 0.14545046]. 
=============================================
[2019-04-04 07:46:16,678] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.0074807e-10 2.2954918e-09 1.1403230e-26 1.0173324e-25 6.2764274e-20
 1.0000000e+00 1.6151459e-19], sum to 1.0000
[2019-04-04 07:46:16,678] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7504
[2019-04-04 07:46:16,695] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.73974107034144, 0.5121538041352853, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3523200.0000, 
sim time next is 3523800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.59697913538164, 0.4972772817451599, 1.0, 1.0, 90390.50116795329], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6330815946151368, 0.6657590939150533, 1.0, 1.0, 0.4304309579426347], 
reward next is 0.5696, 
noisyNet noise sample is [array([1.9387649], dtype=float32), 0.865237]. 
=============================================
[2019-04-04 07:46:17,171] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9928641e-10 2.2129658e-09 4.4201144e-27 2.6924370e-25 7.7936343e-21
 1.0000000e+00 1.1529550e-19], sum to 1.0000
[2019-04-04 07:46:17,171] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8203
[2019-04-04 07:46:17,212] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.13580113539919, 0.4486824243770218, 1.0, 1.0, 64228.65932282841], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3787200.0000, 
sim time next is 3787800.0000, 
raw observation next is [-2.166666666666667, 66.0, 0.0, 0.0, 26.0, 25.11974814560102, 0.4516521385827377, 0.0, 1.0, 59707.61209856893], 
processed observation next is [1.0, 0.8695652173913043, 0.4025854108956602, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5933123454667516, 0.6505507128609126, 0.0, 1.0, 0.28432196237413776], 
reward next is 0.7157, 
noisyNet noise sample is [array([-1.3301257], dtype=float32), 0.62584436]. 
=============================================
[2019-04-04 07:46:21,292] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1850034e-09 1.4327915e-08 4.5837450e-26 2.2622161e-23 1.4454849e-18
 1.0000000e+00 8.2226571e-19], sum to 1.0000
[2019-04-04 07:46:21,294] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6655
[2019-04-04 07:46:21,317] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 42.0, 94.0, 743.5, 26.0, 25.20321709672947, 0.4507287499785559, 0.0, 1.0, 18693.52366712677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3596400.0000, 
sim time next is 3597000.0000, 
raw observation next is [-0.8333333333333334, 42.16666666666666, 91.33333333333334, 728.6666666666667, 26.0, 25.19363794287896, 0.4566077149319352, 0.0, 1.0, 18693.66010805489], 
processed observation next is [0.0, 0.6521739130434783, 0.43951985226223456, 0.4216666666666666, 0.30444444444444446, 0.8051565377532229, 0.6666666666666666, 0.5994698285732468, 0.6522025716439784, 0.0, 1.0, 0.08901742908597567], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.9300557], dtype=float32), -0.27602917]. 
=============================================
[2019-04-04 07:46:21,340] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.6554  ]
 [77.776566]
 [77.98295 ]
 [78.16665 ]
 [78.26419 ]], R is [[77.65769958]
 [77.79210663]
 [78.01418304]
 [78.23403931]
 [78.36266327]].
[2019-04-04 07:46:21,804] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.87271851e-09 6.49049028e-08 6.88278142e-24 2.19132004e-22
 1.13056085e-17 9.99999881e-01 1.37537752e-17], sum to 1.0000
[2019-04-04 07:46:21,808] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0325
[2019-04-04 07:46:21,824] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.07851493431246, 0.3319604784455698, 0.0, 1.0, 40815.57278108032], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4065600.0000, 
sim time next is 4066200.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.12342938662488, 0.3374268230215609, 0.0, 1.0, 40818.54556162639], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5936191155520735, 0.6124756076738537, 0.0, 1.0, 0.1943740264839352], 
reward next is 0.8056, 
noisyNet noise sample is [array([0.98918945], dtype=float32), -1.5563676]. 
=============================================
[2019-04-04 07:46:30,937] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.8952633e-09 1.0115080e-07 1.5057999e-23 5.1206135e-22 5.8871954e-18
 9.9999988e-01 1.0079535e-17], sum to 1.0000
[2019-04-04 07:46:30,938] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5568
[2019-04-04 07:46:30,949] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.17020243552392, 0.3197072112809527, 0.0, 1.0, 40862.98082500348], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4068000.0000, 
sim time next is 4068600.0000, 
raw observation next is [-5.833333333333333, 40.50000000000001, 0.0, 0.0, 26.0, 25.13964613143821, 0.3220907483969913, 0.0, 1.0, 40871.3771475985], 
processed observation next is [1.0, 0.08695652173913043, 0.30101569713758086, 0.4050000000000001, 0.0, 0.0, 0.6666666666666666, 0.5949705109531841, 0.607363582798997, 0.0, 1.0, 0.19462560546475474], 
reward next is 0.8054, 
noisyNet noise sample is [array([-0.63362336], dtype=float32), -0.59039336]. 
=============================================
[2019-04-04 07:46:31,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.0665071e-11 4.4847959e-10 2.9678287e-28 3.8990584e-26 2.2502378e-21
 1.0000000e+00 9.4230347e-21], sum to 1.0000
[2019-04-04 07:46:31,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3102
[2019-04-04 07:46:31,326] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.833333333333333, 42.16666666666667, 33.33333333333333, 303.0, 26.0, 26.72670127059125, 0.7062496009509904, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3863400.0000, 
sim time next is 3864000.0000, 
raw observation next is [2.666666666666667, 43.33333333333334, 25.66666666666666, 241.0, 26.0, 26.89348595561444, 0.6769456528649425, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5364727608494922, 0.4333333333333334, 0.08555555555555554, 0.2662983425414365, 0.6666666666666666, 0.7411238296345367, 0.7256485509549808, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8979652], dtype=float32), -0.05848348]. 
=============================================
[2019-04-04 07:46:31,336] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[83.85532]
 [84.24981]
 [84.60591]
 [84.99737]
 [85.42427]], R is [[83.54725647]
 [83.71178436]
 [83.87466431]
 [84.03591919]
 [84.19556427]].
[2019-04-04 07:46:32,179] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8024227e-11 7.0877026e-10 3.5143568e-27 8.2681097e-26 8.2929905e-21
 1.0000000e+00 4.2760879e-20], sum to 1.0000
[2019-04-04 07:46:32,180] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0505
[2019-04-04 07:46:32,204] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 44.5, 18.0, 179.0, 26.0, 26.75058066333533, 0.6358550435011839, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3864600.0000, 
sim time next is 3865200.0000, 
raw observation next is [2.333333333333333, 45.66666666666667, 15.0, 149.1666666666667, 26.0, 26.45038937634845, 0.649926154480902, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5272391505078486, 0.4566666666666667, 0.05, 0.1648250460405157, 0.6666666666666666, 0.7041991146957042, 0.716642051493634, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7664021], dtype=float32), 0.43262726]. 
=============================================
[2019-04-04 07:46:33,263] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4569768e-11 2.3130091e-10 2.6538775e-28 1.6024304e-26 5.1903110e-22
 1.0000000e+00 1.7583529e-21], sum to 1.0000
[2019-04-04 07:46:33,265] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5287
[2019-04-04 07:46:33,293] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 75.5, 634.0, 26.0, 26.88208176940148, 0.7516313569092299, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3859200.0000, 
sim time next is 3859800.0000, 
raw observation next is [3.0, 44.33333333333334, 71.66666666666666, 606.3333333333333, 26.0, 26.94172605571995, 0.7490076859636132, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.4433333333333334, 0.23888888888888885, 0.6699815837937384, 0.6666666666666666, 0.7451438379766625, 0.7496692286545378, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2306341], dtype=float32), 1.6141189]. 
=============================================
[2019-04-04 07:46:35,980] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0358887e-10 2.0333284e-09 1.0580555e-26 1.6605041e-24 1.3848309e-20
 1.0000000e+00 3.8429741e-19], sum to 1.0000
[2019-04-04 07:46:35,981] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0700
[2019-04-04 07:46:35,984] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5288764e-09 3.6740971e-08 1.5076912e-23 1.3196051e-22 7.2206870e-18
 1.0000000e+00 9.0306889e-18], sum to 1.0000
[2019-04-04 07:46:35,987] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0776
[2019-04-04 07:46:36,012] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.46381760879428, 0.2127120877238552, 0.0, 1.0, 43793.76130420015], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3981600.0000, 
sim time next is 3982200.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.45815643089603, 0.1998393479972278, 0.0, 1.0, 43781.47396214983], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5381797025746691, 0.5666131159990759, 0.0, 1.0, 0.20848320934357062], 
reward next is 0.7915, 
noisyNet noise sample is [array([0.29966438], dtype=float32), -0.72166395]. 
=============================================
[2019-04-04 07:46:36,022] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0218636e-11 4.6489319e-09 8.2579157e-29 1.5941751e-26 9.1436464e-21
 1.0000000e+00 1.2906271e-20], sum to 1.0000
[2019-04-04 07:46:36,023] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5228
[2019-04-04 07:46:36,032] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.966666666666667, 70.66666666666667, 0.0, 0.0, 26.0, 25.62632669184796, 0.4134332472546856, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4332000.0000, 
sim time next is 4332600.0000, 
raw observation next is [3.95, 70.5, 0.0, 0.0, 26.0, 25.66631272736394, 0.4025558168258401, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.57202216066482, 0.705, 0.0, 0.0, 0.6666666666666666, 0.6388593939469951, 0.63418527227528, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.71522367], dtype=float32), 0.9168078]. 
=============================================
[2019-04-04 07:46:36,062] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.333333333333334, 62.0, 5.0, 135.8333333333333, 26.0, 24.64175813170727, 0.3513895677110341, 1.0, 1.0, 202408.6187601095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3914400.0000, 
sim time next is 3915000.0000, 
raw observation next is [-7.5, 61.0, 6.0, 163.0, 26.0, 25.15332162876777, 0.3759824116115754, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.2548476454293629, 0.61, 0.02, 0.18011049723756906, 0.6666666666666666, 0.5961101357306475, 0.6253274705371918, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9814232], dtype=float32), -1.1839151]. 
=============================================
[2019-04-04 07:46:36,067] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[80.064926]
 [78.86984 ]
 [79.17809 ]
 [79.45311 ]
 [79.746445]], R is [[80.86042786]
 [80.08797455]
 [80.08351135]
 [80.07965851]
 [80.07653809]].
[2019-04-04 07:46:42,138] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.5379126e-13 8.9294606e-11 2.3803862e-32 1.0536270e-30 2.2693197e-24
 1.0000000e+00 5.4639752e-24], sum to 1.0000
[2019-04-04 07:46:42,139] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8141
[2019-04-04 07:46:42,165] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.8, 28.5, 118.0, 853.0, 26.0, 28.29509646518122, 0.9129994932836368, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4365000.0000, 
sim time next is 4365600.0000, 
raw observation next is [14.73333333333333, 28.66666666666666, 117.5, 851.1666666666667, 26.0, 27.53326319815614, 0.9393590298643448, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8707294552169899, 0.2866666666666666, 0.39166666666666666, 0.9405156537753223, 0.6666666666666666, 0.7944385998463449, 0.8131196766214482, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6782921], dtype=float32), -0.13493848]. 
=============================================
[2019-04-04 07:46:46,682] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.0195411e-10 3.9480041e-09 6.2648067e-26 2.1418140e-24 2.3046666e-19
 1.0000000e+00 8.8284573e-19], sum to 1.0000
[2019-04-04 07:46:46,682] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2677
[2019-04-04 07:46:46,698] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 42.5, 0.0, 0.0, 26.0, 25.48801611301933, 0.4576954072606491, 0.0, 1.0, 18756.55756789297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4145400.0000, 
sim time next is 4146000.0000, 
raw observation next is [-0.6666666666666666, 42.33333333333334, 0.0, 0.0, 26.0, 25.47172337873763, 0.4479873272926981, 0.0, 1.0, 22897.53815801508], 
processed observation next is [1.0, 1.0, 0.44413665743305636, 0.42333333333333345, 0.0, 0.0, 0.6666666666666666, 0.6226436148948024, 0.649329109097566, 0.0, 1.0, 0.109035895990548], 
reward next is 0.8910, 
noisyNet noise sample is [array([-1.42897], dtype=float32), -0.34792063]. 
=============================================
[2019-04-04 07:46:46,718] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.03223]
 [80.02027]
 [80.18384]
 [80.05603]
 [79.89772]], R is [[79.97215271]
 [80.08311462]
 [80.19295502]
 [80.2682724 ]
 [80.14543152]].
[2019-04-04 07:46:53,901] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.52244298e-12 1.15519226e-10 3.42847880e-30 2.66377682e-28
 1.45569134e-22 1.00000000e+00 2.32110015e-22], sum to 1.0000
[2019-04-04 07:46:53,905] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1639
[2019-04-04 07:46:53,945] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.95, 49.83333333333334, 200.3333333333333, 442.3333333333334, 26.0, 27.20459239108473, 0.8552530069985483, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4632600.0000, 
sim time next is 4633200.0000, 
raw observation next is [5.0, 50.0, 199.0, 364.0, 26.0, 27.33526657649114, 0.8744631594101889, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6011080332409973, 0.5, 0.6633333333333333, 0.4022099447513812, 0.6666666666666666, 0.7779388813742617, 0.7914877198033964, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4329487], dtype=float32), -0.85946846]. 
=============================================
[2019-04-04 07:46:57,682] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2417834e-12 5.6731425e-10 1.6007218e-31 2.5373857e-28 2.5751881e-23
 1.0000000e+00 4.1639329e-23], sum to 1.0000
[2019-04-04 07:46:57,682] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8616
[2019-04-04 07:46:57,705] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 196.5, 73.0, 26.0, 26.50146284245206, 0.6608231045004946, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4446000.0000, 
sim time next is 4446600.0000, 
raw observation next is [1.0, 86.0, 178.3333333333333, 48.66666666666666, 26.0, 26.51203243987723, 0.6588315521053606, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.5944444444444443, 0.05377532228360957, 0.6666666666666666, 0.709336036656436, 0.7196105173684536, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.55914026], dtype=float32), 0.63058597]. 
=============================================
[2019-04-04 07:47:10,190] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.06736451e-11 1.72321546e-10 3.48740353e-29 1.21140655e-26
 1.92390821e-21 1.00000000e+00 3.40748935e-21], sum to 1.0000
[2019-04-04 07:47:10,191] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0147
[2019-04-04 07:47:10,243] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 71.0, 123.0, 171.0, 26.0, 25.63566599506286, 0.4738534092556028, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4609800.0000, 
sim time next is 4610400.0000, 
raw observation next is [-2.0, 71.0, 129.8333333333333, 227.3333333333333, 26.0, 25.91954378591563, 0.5038773388002669, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.4327777777777776, 0.2511970534069981, 0.6666666666666666, 0.6599619821596358, 0.6679591129334224, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3389736], dtype=float32), -1.1111634]. 
=============================================
[2019-04-04 07:47:16,168] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9662636e-09 1.3596072e-07 5.5875324e-25 1.6869049e-23 7.9894878e-19
 9.9999988e-01 3.3977205e-18], sum to 1.0000
[2019-04-04 07:47:16,168] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9519
[2019-04-04 07:47:16,230] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.166666666666667, 50.0, 0.0, 0.0, 26.0, 25.24036965685116, 0.293629291770586, 0.0, 1.0, 43873.87404650166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4936200.0000, 
sim time next is 4936800.0000, 
raw observation next is [-1.333333333333333, 50.0, 0.0, 0.0, 26.0, 25.26633927973789, 0.2929019126263157, 0.0, 1.0, 39996.58443744598], 
processed observation next is [1.0, 0.13043478260869565, 0.42566943674976926, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6055282733114907, 0.5976339708754386, 0.0, 1.0, 0.19045992589259989], 
reward next is 0.8095, 
noisyNet noise sample is [array([0.86442524], dtype=float32), 0.6432761]. 
=============================================
[2019-04-04 07:47:25,558] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:47:25,558] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:47:25,562] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run15
[2019-04-04 07:47:27,330] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:47:27,330] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:47:27,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run15
[2019-04-04 07:47:30,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:47:30,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:47:30,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run15
[2019-04-04 07:47:32,735] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2831766e-10 8.6276875e-10 3.2319117e-28 3.3258052e-26 4.8558943e-21
 1.0000000e+00 1.0207598e-20], sum to 1.0000
[2019-04-04 07:47:32,735] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1421
[2019-04-04 07:47:32,819] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 25.0, 17.0, 152.0, 26.0, 27.21228054926695, 0.6695804694695662, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4989600.0000, 
sim time next is 4990200.0000, 
raw observation next is [6.0, 24.66666666666667, 0.0, 0.0, 26.0, 26.45034619267885, 0.700055566451771, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2466666666666667, 0.0, 0.0, 0.6666666666666666, 0.7041955160565708, 0.7333518554839237, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3785352], dtype=float32), -1.3688493]. 
=============================================
[2019-04-04 07:47:33,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:47:33,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:47:33,714] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run15
[2019-04-04 07:47:38,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:47:38,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:47:38,940] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run15
[2019-04-04 07:47:41,594] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 07:47:41,595] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:47:41,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:47:41,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run20
[2019-04-04 07:47:41,620] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:47:41,621] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:47:41,624] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:47:41,627] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:47:41,632] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run20
[2019-04-04 07:47:41,667] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run20
[2019-04-04 07:48:07,979] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2477151], dtype=float32), 0.2530043]
[2019-04-04 07:48:07,980] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-5.6, 72.5, 84.0, 384.0, 26.0, 25.4225504285933, 0.3984190796382263, 1.0, 1.0, 54983.47963294305]
[2019-04-04 07:48:07,980] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 07:48:07,981] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.0564119e-11 1.6136023e-09 3.7727716e-27 1.5724029e-25 1.8937061e-20
 1.0000000e+00 4.1130735e-20], sampled 0.35678352897107957
[2019-04-04 07:48:21,739] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.2477151], dtype=float32), 0.2530043]
[2019-04-04 07:48:21,739] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.90231075, 90.05188184666666, 0.0, 0.0, 26.0, 24.37694161396282, 0.1288297479923901, 0.0, 1.0, 40428.04194646154]
[2019-04-04 07:48:21,739] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 07:48:21,740] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.0239064e-10 4.6933541e-09 1.1751520e-27 1.0014341e-25 7.3944753e-21
 1.0000000e+00 4.7916227e-20], sampled 0.15981765533972836
[2019-04-04 07:49:02,171] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.2477151], dtype=float32), 0.2530043]
[2019-04-04 07:49:02,171] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [12.11813438, 49.452331715, 0.0, 0.0, 26.0, 26.16373024624182, 0.612581383940192, 1.0, 1.0, 0.0]
[2019-04-04 07:49:02,171] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 07:49:02,172] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.6289452e-12 4.0936535e-10 6.7504349e-30 5.9913379e-28 1.6281527e-22
 1.0000000e+00 7.2938968e-22], sampled 0.6653079774648255
[2019-04-04 07:50:12,903] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2477151], dtype=float32), 0.2530043]
[2019-04-04 07:50:12,903] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.833333333333333, 47.66666666666667, 0.0, 0.0, 26.0, 24.99872752173492, 0.4289660914645113, 1.0, 1.0, 78529.56371914683]
[2019-04-04 07:50:12,903] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 07:50:12,904] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.4581956e-10 1.7651852e-09 9.4959694e-27 3.5994170e-25 4.0073246e-20
 1.0000000e+00 1.0741393e-19], sampled 0.35184122254835637
[2019-04-04 07:50:47,135] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 07:51:21,392] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6673 263409857.2690 1551.6144
[2019-04-04 07:51:29,603] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 07:51:30,637] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1900000, evaluation results [1900000.0, 7241.667346338161, 263409857.26898408, 1551.6144136320988, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 07:51:32,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:51:32,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:51:32,496] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run15
[2019-04-04 07:51:36,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:51:36,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:51:36,035] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run15
[2019-04-04 07:51:39,248] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4566778e-10 1.4261516e-09 3.2209332e-28 1.4351716e-26 6.0350108e-21
 1.0000000e+00 3.6148513e-20], sum to 1.0000
[2019-04-04 07:51:39,248] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8270
[2019-04-04 07:51:39,314] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 89.0, 0.0, 0.0, 26.0, 24.57956286372576, 0.2024170388879722, 0.0, 1.0, 40448.73673452801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 64800.0000, 
sim time next is 65400.0000, 
raw observation next is [4.300000000000001, 88.5, 0.0, 0.0, 26.0, 24.59067359118214, 0.2037488803032968, 0.0, 1.0, 36801.61455681515], 
processed observation next is [0.0, 0.782608695652174, 0.5817174515235458, 0.885, 0.0, 0.0, 0.6666666666666666, 0.5492227992651783, 0.5679162934344323, 0.0, 1.0, 0.17524578360388166], 
reward next is 0.8248, 
noisyNet noise sample is [array([-0.03555528], dtype=float32), -1.7810324]. 
=============================================
[2019-04-04 07:51:45,816] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.895989e-11 9.759292e-10 5.799231e-29 8.275773e-27 9.594707e-22
 1.000000e+00 5.762977e-21], sum to 1.0000
[2019-04-04 07:51:45,816] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6922
[2019-04-04 07:51:45,867] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.9, 90.66666666666667, 92.5, 0.0, 26.0, 24.3092211554365, 0.1074310963721308, 0.0, 1.0, 37464.90274477972], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 44400.0000, 
sim time next is 45000.0000, 
raw observation next is [8.0, 89.5, 96.0, 0.0, 26.0, 24.33186481939253, 0.1138029923021768, 0.0, 1.0, 26303.01114069356], 
processed observation next is [0.0, 0.5217391304347826, 0.6842105263157896, 0.895, 0.32, 0.0, 0.6666666666666666, 0.5276554016160441, 0.5379343307673923, 0.0, 1.0, 0.12525243400330266], 
reward next is 0.8747, 
noisyNet noise sample is [array([0.85853845], dtype=float32), 1.4918969]. 
=============================================
[2019-04-04 07:51:45,869] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[90.60645 ]
 [90.46616 ]
 [90.322495]
 [90.16711 ]
 [90.05313 ]], R is [[90.63748932]
 [90.55271149]
 [90.45818329]
 [90.39460754]
 [90.39233398]].
[2019-04-04 07:51:48,254] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.4945218e-11 1.2929799e-09 4.7501606e-29 1.1020345e-27 4.8313729e-22
 1.0000000e+00 1.4889172e-21], sum to 1.0000
[2019-04-04 07:51:48,254] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1608
[2019-04-04 07:51:48,367] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.3, 86.0, 91.5, 0.0, 26.0, 24.38909694269353, 0.125530322252895, 0.0, 1.0, 18748.83488959337], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 46800.0000, 
sim time next is 47400.0000, 
raw observation next is [8.200000000000001, 86.0, 90.0, 0.0, 26.0, 24.38765698843311, 0.125306758211099, 0.0, 1.0, 28087.117814352], 
processed observation next is [0.0, 0.5652173913043478, 0.6897506925207757, 0.86, 0.3, 0.0, 0.6666666666666666, 0.5323047490360926, 0.5417689194036996, 0.0, 1.0, 0.13374818006834285], 
reward next is 0.8663, 
noisyNet noise sample is [array([0.69416696], dtype=float32), 0.67509985]. 
=============================================
[2019-04-04 07:51:48,815] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2936416e-10 8.8685352e-09 6.9422903e-26 2.1982670e-24 2.6995207e-19
 1.0000000e+00 1.7776979e-19], sum to 1.0000
[2019-04-04 07:51:48,815] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5813
[2019-04-04 07:51:48,834] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.166666666666667, 35.66666666666667, 0.0, 0.0, 26.0, 25.78689456630298, 0.5740986143883977, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5003400.0000, 
sim time next is 5004000.0000, 
raw observation next is [3.0, 37.0, 0.0, 0.0, 26.0, 25.8000079586964, 0.5671334871444121, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.37, 0.0, 0.0, 0.6666666666666666, 0.6500006632247001, 0.689044495714804, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7453255], dtype=float32), 0.049867522]. 
=============================================
[2019-04-04 07:51:48,890] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[80.60406 ]
 [80.48187 ]
 [80.39536 ]
 [80.472374]
 [80.213715]], R is [[80.87966919]
 [81.07087708]
 [81.26016998]
 [81.4475708 ]
 [81.39739227]].
[2019-04-04 07:51:49,259] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:51:49,259] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:51:49,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run15
[2019-04-04 07:51:49,364] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:51:49,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:51:49,368] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run15
[2019-04-04 07:51:52,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:51:52,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:51:52,242] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run15
[2019-04-04 07:51:52,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:51:52,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:51:52,744] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run15
[2019-04-04 07:51:54,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:51:54,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:51:54,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run15
[2019-04-04 07:51:57,896] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4774563e-09 6.0559906e-08 1.0015457e-23 1.5410401e-21 1.3643099e-17
 9.9999988e-01 4.5039812e-17], sum to 1.0000
[2019-04-04 07:51:57,896] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5192
[2019-04-04 07:51:57,922] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.1, 68.0, 0.0, 0.0, 26.0, 22.54336135718775, -0.2725876896612096, 0.0, 1.0, 47882.79125333548], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 283200.0000, 
sim time next is 283800.0000, 
raw observation next is [-12.2, 67.5, 0.0, 0.0, 26.0, 22.49436981691802, -0.271494243364201, 0.0, 1.0, 47923.38660380243], 
processed observation next is [1.0, 0.2608695652173913, 0.12465373961218838, 0.675, 0.0, 0.0, 0.6666666666666666, 0.3745308180765017, 0.4095019188785997, 0.0, 1.0, 0.22820660287524966], 
reward next is 0.7718, 
noisyNet noise sample is [array([1.7830418], dtype=float32), 0.91159487]. 
=============================================
[2019-04-04 07:51:58,161] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:51:58,161] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:51:58,164] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run15
[2019-04-04 07:51:58,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:51:58,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:51:58,316] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run15
[2019-04-04 07:51:59,196] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:51:59,197] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:51:59,210] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run15
[2019-04-04 07:52:00,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:52:00,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:52:00,900] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run15
[2019-04-04 07:52:14,435] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1352567e-07 5.3598889e-08 5.3931792e-22 6.7349550e-21 3.4754056e-17
 9.9999988e-01 1.1414776e-16], sum to 1.0000
[2019-04-04 07:52:14,435] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6709
[2019-04-04 07:52:14,475] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 47.0, 0.0, 0.0, 26.0, 24.74623798126638, 0.1975773531596808, 0.0, 1.0, 45900.58205660735], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 421200.0000, 
sim time next is 421800.0000, 
raw observation next is [-10.6, 47.33333333333334, 0.0, 0.0, 26.0, 24.64889701898008, 0.1816494770082815, 0.0, 1.0, 45863.91739028784], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.47333333333333344, 0.0, 0.0, 0.6666666666666666, 0.5540747515816733, 0.5605498256694271, 0.0, 1.0, 0.21839960662041827], 
reward next is 0.7816, 
noisyNet noise sample is [array([-0.10268126], dtype=float32), -1.1049068]. 
=============================================
[2019-04-04 07:52:21,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.5092794e-11 1.5478663e-09 8.9505999e-27 3.5873405e-25 1.1488143e-20
 1.0000000e+00 5.1114660e-20], sum to 1.0000
[2019-04-04 07:52:21,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0986
[2019-04-04 07:52:21,897] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.800000000000001, 69.66666666666667, 148.1666666666667, 0.0, 26.0, 25.30769052402218, 0.2243182312713306, 1.0, 1.0, 25955.21375903906], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 213600.0000, 
sim time next is 214200.0000, 
raw observation next is [-5.6, 68.5, 153.0, 0.0, 26.0, 25.27870619666147, 0.2277688850628579, 1.0, 1.0, 25380.31838272041], 
processed observation next is [1.0, 0.4782608695652174, 0.30747922437673136, 0.685, 0.51, 0.0, 0.6666666666666666, 0.6065588497217892, 0.5759229616876193, 1.0, 1.0, 0.12085865896533528], 
reward next is 0.8791, 
noisyNet noise sample is [array([0.16518967], dtype=float32), 0.19426009]. 
=============================================
[2019-04-04 07:52:28,023] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5244321e-10 5.5079328e-09 1.6841185e-26 6.4700409e-25 3.7424255e-20
 1.0000000e+00 9.9300732e-20], sum to 1.0000
[2019-04-04 07:52:28,023] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2943
[2019-04-04 07:52:28,075] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.41666666666667, 48.16666666666667, 90.66666666666667, 724.6666666666666, 26.0, 26.00716694159344, 0.4439157613672793, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 303000.0000, 
sim time next is 303600.0000, 
raw observation next is [-10.23333333333333, 47.33333333333334, 86.83333333333334, 741.3333333333334, 26.0, 26.0275522430264, 0.3349511420292609, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.1791320406278856, 0.47333333333333344, 0.28944444444444445, 0.8191528545119706, 0.6666666666666666, 0.6689626869188666, 0.6116503806764203, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06861763], dtype=float32), -1.1463217]. 
=============================================
[2019-04-04 07:52:44,030] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1323312e-08 2.4707552e-07 1.3437089e-21 1.8517231e-20 2.4585343e-17
 9.9999976e-01 2.4719082e-16], sum to 1.0000
[2019-04-04 07:52:44,030] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3077
[2019-04-04 07:52:44,050] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.9, 50.5, 0.0, 0.0, 26.0, 23.11715966782695, -0.1991127916065451, 0.0, 1.0, 46083.0397167549], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 444600.0000, 
sim time next is 445200.0000, 
raw observation next is [-11.0, 51.0, 0.0, 0.0, 26.0, 23.02708203501658, -0.210894639354243, 0.0, 1.0, 46193.30568350979], 
processed observation next is [1.0, 0.13043478260869565, 0.15789473684210528, 0.51, 0.0, 0.0, 0.6666666666666666, 0.4189235029180483, 0.429701786881919, 0.0, 1.0, 0.21996812230242757], 
reward next is 0.7800, 
noisyNet noise sample is [array([-0.11980911], dtype=float32), -0.5124921]. 
=============================================
[2019-04-04 07:52:48,232] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0976895e-09 3.4790810e-09 2.8782693e-24 9.6191099e-23 1.9058686e-18
 1.0000000e+00 2.7378632e-18], sum to 1.0000
[2019-04-04 07:52:48,232] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2991
[2019-04-04 07:52:48,337] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 38.0, 29.0, 555.0, 26.0, 25.76063225726051, 0.3996183246728098, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 403200.0000, 
sim time next is 403800.0000, 
raw observation next is [-8.9, 37.66666666666667, 26.33333333333333, 504.3333333333333, 26.0, 25.48937205171736, 0.390125224903618, 1.0, 1.0, 188965.6461299974], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.3766666666666667, 0.08777777777777776, 0.5572744014732964, 0.6666666666666666, 0.6241143376431134, 0.6300417416345393, 1.0, 1.0, 0.8998364101428447], 
reward next is 0.1002, 
noisyNet noise sample is [array([0.6382735], dtype=float32), -0.7485328]. 
=============================================
[2019-04-04 07:52:48,559] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.2433659e-10 4.7216466e-09 7.7606803e-27 8.6925816e-26 5.5703086e-20
 1.0000000e+00 4.9335142e-20], sum to 1.0000
[2019-04-04 07:52:48,564] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3451
[2019-04-04 07:52:48,636] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6000000000000001, 86.0, 0.0, 0.0, 26.0, 24.41752529925012, 0.1852445817434039, 1.0, 1.0, 199595.7220258428], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 497400.0000, 
sim time next is 498000.0000, 
raw observation next is [0.7000000000000001, 88.0, 0.0, 0.0, 26.0, 24.64570379148203, 0.2111113328416357, 1.0, 1.0, 99130.22521133015], 
processed observation next is [1.0, 0.782608695652174, 0.4819944598337951, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5538086492901693, 0.5703704442805452, 1.0, 1.0, 0.47204869148252454], 
reward next is 0.5280, 
noisyNet noise sample is [array([1.1458031], dtype=float32), -0.071097516]. 
=============================================
[2019-04-04 07:52:48,699] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.456856]
 [79.99944 ]
 [78.72256 ]
 [78.104675]
 [77.78048 ]], R is [[82.15723419]
 [81.38520813]
 [80.7039566 ]
 [80.67086792]
 [80.86415863]].
[2019-04-04 07:52:49,792] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1305564e-10 1.8359013e-09 3.0293570e-27 3.8006127e-24 4.3163383e-20
 1.0000000e+00 6.6644505e-19], sum to 1.0000
[2019-04-04 07:52:49,792] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0173
[2019-04-04 07:52:49,859] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.483333333333333, 65.0, 96.33333333333333, 12.66666666666666, 26.0, 24.8733482155206, 0.2107165854792935, 0.0, 1.0, 23409.64282720065], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 643800.0000, 
sim time next is 644400.0000, 
raw observation next is [-3.4, 65.0, 94.5, 19.0, 26.0, 24.89401267808919, 0.2093521702173358, 0.0, 1.0, 21059.36725124832], 
processed observation next is [0.0, 0.4782608695652174, 0.368421052631579, 0.65, 0.315, 0.020994475138121547, 0.6666666666666666, 0.5745010565074326, 0.5697840567391119, 0.0, 1.0, 0.10028270119642058], 
reward next is 0.8997, 
noisyNet noise sample is [array([0.16572013], dtype=float32), 1.113606]. 
=============================================
[2019-04-04 07:52:52,420] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8818613e-11 1.1093246e-09 3.3849752e-27 1.8452667e-25 7.2335781e-21
 1.0000000e+00 1.6377349e-20], sum to 1.0000
[2019-04-04 07:52:52,420] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2362
[2019-04-04 07:52:52,489] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.783333333333333, 71.66666666666667, 94.66666666666666, 0.0, 26.0, 25.571792270669, 0.2966686033994522, 1.0, 1.0, 18714.79128171975], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 816600.0000, 
sim time next is 817200.0000, 
raw observation next is [-4.5, 71.0, 98.5, 0.0, 26.0, 25.59600278783704, 0.2991288959471972, 1.0, 1.0, 18713.59845215913], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.3283333333333333, 0.0, 0.6666666666666666, 0.6330002323197533, 0.599709631982399, 1.0, 1.0, 0.08911237358171015], 
reward next is 0.9109, 
noisyNet noise sample is [array([-2.0554116], dtype=float32), 0.8082857]. 
=============================================
[2019-04-04 07:52:55,745] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.7604377e-10 3.1243452e-08 7.8784777e-26 8.5405726e-24 6.9354064e-19
 1.0000000e+00 5.3814944e-19], sum to 1.0000
[2019-04-04 07:52:55,747] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1969
[2019-04-04 07:52:55,798] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8999999999999999, 31.5, 119.0, 0.0, 26.0, 24.95089509436038, 0.2028666009498425, 1.0, 1.0, 46392.28670483363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 480600.0000, 
sim time next is 481200.0000, 
raw observation next is [-0.8, 32.66666666666666, 114.8333333333333, 0.0, 26.0, 25.27990577364826, 0.2358166122246518, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4404432132963989, 0.32666666666666655, 0.38277777777777766, 0.0, 0.6666666666666666, 0.6066588144706883, 0.5786055374082173, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.62870145], dtype=float32), -1.5696875]. 
=============================================
[2019-04-04 07:53:04,892] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.76640125e-12 5.20218646e-10 1.00056906e-29 4.45153525e-28
 1.58661370e-21 1.00000000e+00 5.84016675e-22], sum to 1.0000
[2019-04-04 07:53:04,893] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2673
[2019-04-04 07:53:04,948] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.8, 93.0, 98.66666666666666, 0.0, 26.0, 25.12937993465778, 0.2450991145452246, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 911400.0000, 
sim time next is 912000.0000, 
raw observation next is [3.8, 93.0, 97.33333333333333, 0.0, 26.0, 25.07515304699709, 0.2471599848050535, 1.0, 1.0, 43331.13372484838], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.3244444444444444, 0.0, 0.6666666666666666, 0.5895960872497575, 0.5823866616016845, 1.0, 1.0, 0.20633873202308753], 
reward next is 0.7937, 
noisyNet noise sample is [array([1.4048483], dtype=float32), -1.38879]. 
=============================================
[2019-04-04 07:53:04,997] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[90.48077 ]
 [90.361725]
 [90.2472  ]
 [90.06528 ]
 [89.95269 ]], R is [[90.61392212]
 [90.61883545]
 [90.71264648]
 [90.71656799]
 [90.80940247]].
[2019-04-04 07:53:05,983] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7729570e-12 3.6913661e-10 9.7133138e-32 1.5048932e-30 6.1555574e-24
 1.0000000e+00 8.9126578e-24], sum to 1.0000
[2019-04-04 07:53:05,995] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4245
[2019-04-04 07:53:06,016] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.68333333333333, 91.83333333333333, 84.0, 0.0, 26.0, 26.52269950750698, 0.639751271942474, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 987000.0000, 
sim time next is 987600.0000, 
raw observation next is [10.86666666666667, 90.66666666666667, 90.0, 0.0, 26.0, 26.56876687938409, 0.6489280693504703, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7636195752539245, 0.9066666666666667, 0.3, 0.0, 0.6666666666666666, 0.7140639066153408, 0.7163093564501568, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24601318], dtype=float32), 0.81395745]. 
=============================================
[2019-04-04 07:53:11,893] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.7362804e-12 1.7063291e-10 2.5357176e-31 1.3999838e-29 2.4772902e-23
 1.0000000e+00 7.9148485e-23], sum to 1.0000
[2019-04-04 07:53:11,898] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5939
[2019-04-04 07:53:11,917] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.03387725505881, 0.412729189198129, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1022400.0000, 
sim time next is 1023000.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 24.95350508454042, 0.402919547548357, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.579458757045035, 0.6343065158494524, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27236044], dtype=float32), -0.11952693]. 
=============================================
[2019-04-04 07:53:11,951] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[96.772285]
 [97.39915 ]
 [97.35717 ]
 [97.34279 ]
 [97.99651 ]], R is [[96.57156372]
 [96.60585022]
 [96.6397934 ]
 [96.67339325]
 [96.70665741]].
[2019-04-04 07:53:15,262] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3187463e-12 9.6274048e-12 3.9265232e-33 4.7560463e-32 5.1112384e-25
 1.0000000e+00 1.6442488e-23], sum to 1.0000
[2019-04-04 07:53:15,262] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9991
[2019-04-04 07:53:15,270] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.76666666666667, 73.33333333333334, 137.3333333333333, 35.83333333333333, 26.0, 27.21921020064749, 0.881819348756815, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1075200.0000, 
sim time next is 1075800.0000, 
raw observation next is [15.13333333333333, 71.66666666666667, 160.6666666666667, 71.66666666666666, 26.0, 27.2610506516841, 0.8921491855795421, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8818097876269622, 0.7166666666666667, 0.5355555555555557, 0.07918968692449355, 0.6666666666666666, 0.7717542209736751, 0.7973830618598474, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.96303463], dtype=float32), -0.74666816]. 
=============================================
[2019-04-04 07:53:15,790] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0395004e-11 1.3384320e-10 4.7628727e-32 1.0343172e-29 9.2149554e-25
 1.0000000e+00 2.3847963e-23], sum to 1.0000
[2019-04-04 07:53:15,791] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7342
[2019-04-04 07:53:15,812] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.95, 79.5, 0.0, 0.0, 26.0, 26.55733375789599, 0.6505303681620519, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1013400.0000, 
sim time next is 1014000.0000, 
raw observation next is [14.76666666666667, 80.0, 0.0, 0.0, 26.0, 25.86680499199058, 0.5874933422432767, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8716528162511544, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6555670826658817, 0.6958311140810922, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.81962574], dtype=float32), 0.08206056]. 
=============================================
[2019-04-04 07:53:15,826] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[98.48468]
 [98.64661]
 [98.78287]
 [98.86299]
 [98.89216]], R is [[98.36930847]
 [98.38562012]
 [98.40176392]
 [98.4177475 ]
 [98.43357086]].
[2019-04-04 07:53:36,780] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8402619e-12 2.7575114e-10 3.3215729e-32 4.3175351e-30 8.1086579e-25
 1.0000000e+00 4.3709592e-24], sum to 1.0000
[2019-04-04 07:53:36,784] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2251
[2019-04-04 07:53:36,835] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 81.0, 81.66666666666667, 0.0, 26.0, 25.75015858897729, 0.5693208139485835, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1003800.0000, 
sim time next is 1004400.0000, 
raw observation next is [14.4, 81.0, 75.5, 0.0, 26.0, 24.30480128871091, 0.482859866546478, 1.0, 1.0, 195414.3100041063], 
processed observation next is [1.0, 0.6521739130434783, 0.8614958448753465, 0.81, 0.25166666666666665, 0.0, 0.6666666666666666, 0.5254001073925757, 0.660953288848826, 1.0, 1.0, 0.9305443333528871], 
reward next is 0.0695, 
noisyNet noise sample is [array([0.52877086], dtype=float32), 1.2048818]. 
=============================================
[2019-04-04 07:53:42,928] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.17812355e-08 4.75508841e-08 4.85872211e-24 5.09444479e-23
 1.98219048e-18 1.00000000e+00 7.32413508e-18], sum to 1.0000
[2019-04-04 07:53:42,934] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3456
[2019-04-04 07:53:42,937] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.3, 63.66666666666667, 0.0, 0.0, 26.0, 24.85237249198265, 0.4380180393866784, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1186800.0000, 
sim time next is 1187400.0000, 
raw observation next is [18.3, 63.33333333333333, 0.0, 0.0, 26.0, 24.8328980928949, 0.4338926036565564, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.6333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5694081744079084, 0.6446308678855188, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38527247], dtype=float32), 1.8574187]. 
=============================================
[2019-04-04 07:53:48,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9276309e-11 4.3492370e-09 6.4425453e-28 1.7623307e-26 7.4131973e-22
 1.0000000e+00 8.3056543e-21], sum to 1.0000
[2019-04-04 07:53:48,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4117
[2019-04-04 07:53:48,763] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.56069704653373, 0.5620170903168232, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1314600.0000, 
sim time next is 1315200.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.60778177257971, 0.5477148217391307, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6339818143816425, 0.6825716072463769, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06670897], dtype=float32), -1.3708878]. 
=============================================
[2019-04-04 07:53:51,299] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.7421235e-11 2.4221467e-09 4.3714840e-28 2.9349792e-26 7.5636531e-21
 1.0000000e+00 1.4602924e-20], sum to 1.0000
[2019-04-04 07:53:51,301] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8454
[2019-04-04 07:53:51,338] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 85.66666666666667, 0.0, 0.0, 26.0, 25.05453695302055, 0.3846521139155192, 0.0, 1.0, 43325.28735856305], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1743600.0000, 
sim time next is 1744200.0000, 
raw observation next is [-0.6, 85.0, 0.0, 0.0, 26.0, 25.03365835084416, 0.3800382463680859, 0.0, 1.0, 43371.82641775], 
processed observation next is [0.0, 0.17391304347826086, 0.44598337950138506, 0.85, 0.0, 0.0, 0.6666666666666666, 0.58613819590368, 0.6266794154560286, 0.0, 1.0, 0.20653250675119048], 
reward next is 0.7935, 
noisyNet noise sample is [array([-0.44957876], dtype=float32), 0.9663166]. 
=============================================
[2019-04-04 07:53:55,573] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.8421956e-12 1.0784903e-10 1.4599348e-29 4.7326729e-28 2.9823645e-22
 1.0000000e+00 2.1653858e-22], sum to 1.0000
[2019-04-04 07:53:55,573] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2440
[2019-04-04 07:53:55,593] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.2, 50.0, 82.0, 253.0, 26.0, 26.981535817959, 0.794201173544045, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1526400.0000, 
sim time next is 1527000.0000, 
raw observation next is [11.91666666666667, 51.33333333333334, 83.66666666666667, 177.9999999999999, 26.0, 27.08150668276578, 0.8075722044848616, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7927054478301017, 0.5133333333333334, 0.2788888888888889, 0.19668508287292805, 0.6666666666666666, 0.756792223563815, 0.7691907348282871, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.85098875], dtype=float32), -0.8081576]. 
=============================================
[2019-04-04 07:53:55,626] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[89.18416 ]
 [89.346664]
 [89.61929 ]
 [89.82923 ]
 [90.03543 ]], R is [[89.08538818]
 [89.1945343 ]
 [89.30258942]
 [89.40956116]
 [89.51546478]].
[2019-04-04 07:53:55,638] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2266070e-12 1.8733069e-10 3.5749683e-30 3.5323635e-29 8.7943176e-24
 1.0000000e+00 1.5356700e-22], sum to 1.0000
[2019-04-04 07:53:55,639] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7259
[2019-04-04 07:53:55,646] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.133333333333333, 69.66666666666667, 87.5, 700.8333333333334, 26.0, 25.96050469659746, 0.6304922057379434, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1516800.0000, 
sim time next is 1517400.0000, 
raw observation next is [8.6, 68.0, 85.0, 701.0, 26.0, 26.15142134647602, 0.6600371844355313, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.700831024930748, 0.68, 0.2833333333333333, 0.7745856353591161, 0.6666666666666666, 0.679285112206335, 0.7200123948118438, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7207704], dtype=float32), 0.20462883]. 
=============================================
[2019-04-04 07:53:56,105] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.1154783e-12 7.1043232e-10 3.8638867e-29 5.9662756e-27 6.6655812e-23
 1.0000000e+00 1.7699280e-21], sum to 1.0000
[2019-04-04 07:53:56,105] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6076
[2019-04-04 07:53:56,142] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.433333333333333, 100.0, 22.83333333333333, 0.0, 26.0, 25.72630294878039, 0.482434904288007, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1500000.0000, 
sim time next is 1500600.0000, 
raw observation next is [1.516666666666667, 100.0, 27.66666666666666, 0.0, 26.0, 25.76735504842308, 0.4889379451954774, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5046168051708219, 1.0, 0.0922222222222222, 0.0, 0.6666666666666666, 0.64727958736859, 0.6629793150651592, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13814409], dtype=float32), -0.04423374]. 
=============================================
[2019-04-04 07:53:58,924] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4191719e-10 1.7737780e-09 6.4956513e-28 3.9114827e-26 8.7798170e-21
 1.0000000e+00 4.1160378e-21], sum to 1.0000
[2019-04-04 07:53:58,924] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2472
[2019-04-04 07:53:58,936] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 93.33333333333334, 0.0, 0.0, 26.0, 25.48070551338752, 0.4676443777678759, 0.0, 1.0, 18756.85714869923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1478400.0000, 
sim time next is 1479000.0000, 
raw observation next is [2.2, 93.66666666666667, 0.0, 0.0, 26.0, 25.43935510775873, 0.4597108029793159, 0.0, 1.0, 38942.76562124836], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9366666666666668, 0.0, 0.0, 0.6666666666666666, 0.6199462589798941, 0.6532369343264386, 0.0, 1.0, 0.18544174105356365], 
reward next is 0.8146, 
noisyNet noise sample is [array([1.4402112], dtype=float32), -0.95038235]. 
=============================================
[2019-04-04 07:53:58,947] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[85.881454]
 [85.92293 ]
 [85.85307 ]
 [85.96393 ]
 [85.67685 ]], R is [[85.81548309]
 [85.86801147]
 [86.00933075]
 [86.05988312]
 [85.98306274]].
[2019-04-04 07:54:06,275] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.0558471e-13 5.0093749e-11 7.3033611e-30 1.7550430e-28 1.1157634e-23
 1.0000000e+00 7.8450507e-23], sum to 1.0000
[2019-04-04 07:54:06,278] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1067
[2019-04-04 07:54:06,284] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 49.0, 160.5, 0.0, 26.0, 27.30239239255057, 0.8474264890798135, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1605600.0000, 
sim time next is 1606200.0000, 
raw observation next is [13.8, 49.0, 155.3333333333333, 0.0, 26.0, 27.31025844841956, 0.8572711979612752, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.844875346260388, 0.49, 0.5177777777777777, 0.0, 0.6666666666666666, 0.7758548707016301, 0.7857570659870917, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06661519], dtype=float32), -1.9599972]. 
=============================================
[2019-04-04 07:54:20,392] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.4126667e-10 3.4493608e-09 8.4258742e-27 1.2165945e-24 3.8896517e-20
 1.0000000e+00 1.3688180e-19], sum to 1.0000
[2019-04-04 07:54:20,394] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8369
[2019-04-04 07:54:20,454] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.8, 83.66666666666667, 52.0, 0.0, 26.0, 25.12097042545417, 0.3442666230397556, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1761000.0000, 
sim time next is 1761600.0000, 
raw observation next is [-1.9, 84.33333333333334, 58.5, 0.0, 26.0, 25.06563497236721, 0.3261598345726832, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.4099722991689751, 0.8433333333333334, 0.195, 0.0, 0.6666666666666666, 0.5888029143639342, 0.6087199448575611, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.61710685], dtype=float32), -2.1272242]. 
=============================================
[2019-04-04 07:54:20,782] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1934230e-10 1.4423106e-09 6.1169285e-26 2.3131161e-24 4.2626840e-20
 1.0000000e+00 1.5520810e-18], sum to 1.0000
[2019-04-04 07:54:20,784] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8767
[2019-04-04 07:54:20,840] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.100000000000001, 75.5, 0.0, 0.0, 26.0, 25.52683794522214, 0.4221529197857141, 1.0, 1.0, 25738.81854558135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2142600.0000, 
sim time next is 2143200.0000, 
raw observation next is [-5.2, 77.0, 0.0, 0.0, 26.0, 25.49245182559808, 0.4043834773180189, 0.0, 1.0, 31743.99861523229], 
processed observation next is [1.0, 0.8260869565217391, 0.31855955678670367, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6243709854665068, 0.6347944924393396, 0.0, 1.0, 0.1511618981677728], 
reward next is 0.8488, 
noisyNet noise sample is [array([1.5585238], dtype=float32), 0.6024442]. 
=============================================
[2019-04-04 07:54:28,048] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3290914e-09 3.9069832e-09 2.9494479e-25 8.4263692e-24 5.7073911e-19
 1.0000000e+00 7.7181979e-19], sum to 1.0000
[2019-04-04 07:54:28,048] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2759
[2019-04-04 07:54:28,094] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.2, 77.0, 0.0, 0.0, 26.0, 25.49245181944331, 0.404383476659922, 0.0, 1.0, 31743.99859561905], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2143200.0000, 
sim time next is 2143800.0000, 
raw observation next is [-5.3, 78.5, 0.0, 0.0, 26.0, 25.38064546020705, 0.3814161323422002, 0.0, 1.0, 34459.72555200889], 
processed observation next is [1.0, 0.8260869565217391, 0.31578947368421056, 0.785, 0.0, 0.0, 0.6666666666666666, 0.6150537883505874, 0.6271387107807334, 0.0, 1.0, 0.16409393120004231], 
reward next is 0.8359, 
noisyNet noise sample is [array([0.30689865], dtype=float32), 0.07888283]. 
=============================================
[2019-04-04 07:54:32,295] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3653643e-10 2.6329894e-09 2.6259851e-26 1.7014000e-24 9.3394733e-21
 1.0000000e+00 2.1828720e-19], sum to 1.0000
[2019-04-04 07:54:32,295] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5436
[2019-04-04 07:54:32,396] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.633333333333333, 84.83333333333334, 67.66666666666667, 373.0000000000001, 26.0, 25.5206622419184, 0.2643638785131077, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1933800.0000, 
sim time next is 1934400.0000, 
raw observation next is [-8.366666666666667, 83.66666666666667, 76.33333333333334, 461.0, 26.0, 25.56360220879624, 0.2870698454262784, 1.0, 1.0, 18744.42608818979], 
processed observation next is [1.0, 0.391304347826087, 0.23084025854108958, 0.8366666666666667, 0.2544444444444445, 0.5093922651933702, 0.6666666666666666, 0.6303001840663534, 0.5956899484754261, 1.0, 1.0, 0.0892591718485228], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.6749655], dtype=float32), 0.22192116]. 
=============================================
[2019-04-04 07:54:35,586] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0109770e-08 1.9982032e-07 1.0084502e-23 4.1730454e-22 1.0708141e-17
 9.9999976e-01 8.4253325e-18], sum to 1.0000
[2019-04-04 07:54:35,588] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5122
[2019-04-04 07:54:35,648] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.100000000000001, 78.33333333333334, 0.0, 0.0, 26.0, 23.75849940015071, -0.000321348009796001, 0.0, 1.0, 41891.66792812503], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2182200.0000, 
sim time next is 2182800.0000, 
raw observation next is [-6.0, 77.66666666666667, 0.0, 0.0, 26.0, 23.71107453428481, -0.00740201426041844, 0.0, 1.0, 41891.52558355702], 
processed observation next is [1.0, 0.2608695652173913, 0.296398891966759, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.4759228778570674, 0.49753266191319384, 0.0, 1.0, 0.19948345515979535], 
reward next is 0.8005, 
noisyNet noise sample is [array([0.05309292], dtype=float32), -0.43799776]. 
=============================================
[2019-04-04 07:54:48,200] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.1600763e-10 2.7172133e-08 1.7302776e-24 1.3018695e-23 2.1492276e-18
 1.0000000e+00 2.5301976e-18], sum to 1.0000
[2019-04-04 07:54:48,200] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5139
[2019-04-04 07:54:48,237] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.50037241068129, 0.1581035272567075, 0.0, 1.0, 42501.328628356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2169000.0000, 
sim time next is 2169600.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.44755562560933, 0.1608057641810113, 0.0, 1.0, 42498.48819551616], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5372963021341107, 0.5536019213936704, 0.0, 1.0, 0.20237375331198174], 
reward next is 0.7976, 
noisyNet noise sample is [array([-0.6487772], dtype=float32), 1.1553937]. 
=============================================
[2019-04-04 07:54:50,980] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.0062292e-10 7.2146351e-09 1.6590037e-24 1.3351952e-23 8.5759511e-19
 1.0000000e+00 3.9097531e-18], sum to 1.0000
[2019-04-04 07:54:50,980] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0190
[2019-04-04 07:54:50,997] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 66.33333333333333, 0.0, 0.0, 26.0, 24.17204015050078, 0.1080277332075461, 0.0, 1.0, 41138.57092747241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2353200.0000, 
sim time next is 2353800.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.14957927682763, 0.1030538797251529, 0.0, 1.0, 41174.56090618784], 
processed observation next is [0.0, 0.21739130434782608, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5124649397356359, 0.5343512932417176, 0.0, 1.0, 0.19606933764851353], 
reward next is 0.8039, 
noisyNet noise sample is [array([-1.3560144], dtype=float32), -0.5995096]. 
=============================================
[2019-04-04 07:54:55,808] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.18311214e-09 1.79501978e-08 1.09394876e-23 1.24121584e-22
 6.62124869e-18 1.00000000e+00 1.13335482e-17], sum to 1.0000
[2019-04-04 07:54:55,808] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8785
[2019-04-04 07:54:55,826] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.7, 29.0, 0.0, 0.0, 26.0, 25.2399406698794, 0.2843621973500608, 0.0, 1.0, 42259.38276239834], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2491200.0000, 
sim time next is 2491800.0000, 
raw observation next is [-0.7833333333333333, 30.33333333333334, 0.0, 0.0, 26.0, 25.28946653986403, 0.2866569717933605, 0.0, 1.0, 40744.81278979175], 
processed observation next is [0.0, 0.8695652173913043, 0.44090489381348114, 0.3033333333333334, 0.0, 0.0, 0.6666666666666666, 0.6074555449886692, 0.5955523239311201, 0.0, 1.0, 0.19402291804662739], 
reward next is 0.8060, 
noisyNet noise sample is [array([0.51060754], dtype=float32), -0.6606436]. 
=============================================
[2019-04-04 07:55:06,002] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.9853159e-10 1.5009132e-07 1.6625141e-23 1.5668732e-21 2.1200271e-18
 9.9999988e-01 1.7032241e-17], sum to 1.0000
[2019-04-04 07:55:06,002] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8892
[2019-04-04 07:55:06,040] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.33333333333334, 83.0, 0.0, 0.0, 26.0, 23.23840890537035, -0.07448604775231732, 0.0, 1.0, 43693.18876091257], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2697600.0000, 
sim time next is 2698200.0000, 
raw observation next is [-15.5, 83.0, 0.0, 0.0, 26.0, 23.23151621535589, -0.08624045843470984, 0.0, 1.0, 43575.31323283043], 
processed observation next is [1.0, 0.21739130434782608, 0.033240997229916885, 0.83, 0.0, 0.0, 0.6666666666666666, 0.4359596846129907, 0.4712531805217634, 0.0, 1.0, 0.2075014915849068], 
reward next is 0.7925, 
noisyNet noise sample is [array([0.52993107], dtype=float32), -0.53488106]. 
=============================================
[2019-04-04 07:55:09,065] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1919847e-10 1.7795387e-09 4.7989980e-26 5.0856462e-24 7.3862757e-20
 1.0000000e+00 1.9253639e-19], sum to 1.0000
[2019-04-04 07:55:09,066] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6316
[2019-04-04 07:55:09,081] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.09999999999999999, 44.83333333333334, 73.33333333333333, 14.0, 26.0, 25.8816253378543, 0.4143582437151611, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2304600.0000, 
sim time next is 2305200.0000, 
raw observation next is [-0.2, 45.66666666666667, 57.16666666666667, 6.999999999999998, 26.0, 25.89612092352017, 0.4087784644035344, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4570637119113574, 0.4566666666666667, 0.19055555555555556, 0.007734806629834252, 0.6666666666666666, 0.658010076960014, 0.6362594881345115, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27303797], dtype=float32), 1.3836125]. 
=============================================
[2019-04-04 07:55:19,845] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8014449e-09 5.1312261e-09 4.1921385e-25 4.6550654e-23 2.9896479e-19
 1.0000000e+00 5.0731225e-19], sum to 1.0000
[2019-04-04 07:55:19,845] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6680
[2019-04-04 07:55:19,870] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.43259928798409, 0.4181680225181905, 0.0, 1.0, 24190.7679453485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2582400.0000, 
sim time next is 2583000.0000, 
raw observation next is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.41433630297388, 0.4157892446376665, 0.0, 1.0, 42287.60704113411], 
processed observation next is [1.0, 0.9130434782608695, 0.38504155124653744, 0.56, 0.0, 0.0, 0.6666666666666666, 0.6178613585811566, 0.6385964148792221, 0.0, 1.0, 0.20136955733873388], 
reward next is 0.7986, 
noisyNet noise sample is [array([0.283816], dtype=float32), 1.0225484]. 
=============================================
[2019-04-04 07:55:19,874] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.701035]
 [76.583275]
 [76.517166]
 [76.48552 ]
 [76.50379 ]], R is [[76.92695618]
 [77.0424881 ]
 [77.1595993 ]
 [77.23154449]
 [77.2275238 ]].
[2019-04-04 07:55:22,524] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.4706589e-11 4.1585415e-09 1.3394835e-26 1.7797221e-24 2.7570295e-20
 1.0000000e+00 1.2729666e-19], sum to 1.0000
[2019-04-04 07:55:22,524] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0760
[2019-04-04 07:55:22,592] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 54.33333333333333, 36.00000000000001, 12.0, 26.0, 25.08031843178941, 0.1785612000990034, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2535000.0000, 
sim time next is 2535600.0000, 
raw observation next is [-2.8, 54.66666666666667, 43.5, 15.0, 26.0, 25.04161822967978, 0.2173226906420027, 1.0, 1.0, 12489.04494948355], 
processed observation next is [1.0, 0.34782608695652173, 0.38504155124653744, 0.5466666666666667, 0.145, 0.016574585635359115, 0.6666666666666666, 0.5868015191399817, 0.5724408968806676, 1.0, 1.0, 0.059471642616588334], 
reward next is 0.9405, 
noisyNet noise sample is [array([-1.153656], dtype=float32), 1.4061718]. 
=============================================
[2019-04-04 07:55:24,662] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.94149960e-10 1.41032785e-08 6.04160649e-26 3.22809870e-24
 3.14668132e-20 1.00000000e+00 5.41784722e-20], sum to 1.0000
[2019-04-04 07:55:24,664] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3201
[2019-04-04 07:55:24,743] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.333333333333334, 64.0, 18.0, 34.49999999999999, 26.0, 24.35986424628376, 0.2150949877501997, 1.0, 1.0, 192370.0678741435], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2792400.0000, 
sim time next is 2793000.0000, 
raw observation next is [-6.166666666666666, 64.0, 35.99999999999999, 68.99999999999999, 26.0, 24.92668168044824, 0.2997196982934003, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.29178208679593726, 0.64, 0.11999999999999998, 0.07624309392265191, 0.6666666666666666, 0.5772234733706867, 0.5999065660978001, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.67267907], dtype=float32), -1.1339729]. 
=============================================
[2019-04-04 07:55:24,757] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[80.10035 ]
 [78.42847 ]
 [76.939316]
 [75.66032 ]
 [75.68561 ]], R is [[81.2392807 ]
 [80.51084137]
 [79.73918152]
 [78.97996521]
 [78.88848114]].
[2019-04-04 07:55:34,120] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.4843686e-11 1.4882137e-09 1.5820640e-27 4.9894072e-26 1.5184051e-21
 1.0000000e+00 1.3128132e-20], sum to 1.0000
[2019-04-04 07:55:34,121] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4299
[2019-04-04 07:55:34,150] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.333333333333333, 80.33333333333334, 0.0, 0.0, 26.0, 25.36602417841518, 0.4243569641603782, 0.0, 1.0, 46357.15036113304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2931600.0000, 
sim time next is 2932200.0000, 
raw observation next is [-1.5, 81.5, 0.0, 0.0, 26.0, 25.27687315762742, 0.412729115354552, 0.0, 1.0, 44772.63159015116], 
processed observation next is [1.0, 0.9565217391304348, 0.4210526315789474, 0.815, 0.0, 0.0, 0.6666666666666666, 0.6064060964689517, 0.6375763717848507, 0.0, 1.0, 0.21320300757214838], 
reward next is 0.7868, 
noisyNet noise sample is [array([0.31671605], dtype=float32), 0.14279452]. 
=============================================
[2019-04-04 07:55:37,627] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.5809856e-10 9.9262278e-09 3.6572695e-26 2.3545668e-24 2.0105033e-19
 1.0000000e+00 2.7246611e-19], sum to 1.0000
[2019-04-04 07:55:37,627] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9831
[2019-04-04 07:55:37,706] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.833333333333333, 70.0, 170.0, 59.99999999999999, 26.0, 24.88821704620688, 0.2888051392374154, 0.0, 1.0, 104632.2287988356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2974200.0000, 
sim time next is 2974800.0000, 
raw observation next is [-3.666666666666667, 69.0, 174.0, 42.0, 26.0, 24.84692965704457, 0.3061786248087006, 0.0, 1.0, 97430.21683276162], 
processed observation next is [0.0, 0.43478260869565216, 0.3610341643582641, 0.69, 0.58, 0.04640883977900553, 0.6666666666666666, 0.5705774714203807, 0.6020595416029002, 0.0, 1.0, 0.46395341348934105], 
reward next is 0.5360, 
noisyNet noise sample is [array([-0.95045453], dtype=float32), -0.9577599]. 
=============================================
[2019-04-04 07:55:38,518] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2343880e-12 4.5390289e-10 1.7577750e-30 6.3253835e-29 2.1614727e-23
 1.0000000e+00 3.6592584e-22], sum to 1.0000
[2019-04-04 07:55:38,519] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7482
[2019-04-04 07:55:38,635] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 100.0, 28.33333333333333, 185.3333333333333, 26.0, 25.46780815855699, 0.3296990852095083, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3138600.0000, 
sim time next is 3139200.0000, 
raw observation next is [6.0, 100.0, 42.0, 237.0, 26.0, 25.37600581538433, 0.3723072969966096, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6288088642659281, 1.0, 0.14, 0.261878453038674, 0.6666666666666666, 0.6146671512820276, 0.6241024323322032, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7079588], dtype=float32), 0.2771145]. 
=============================================
[2019-04-04 07:55:41,247] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8946020e-10 5.8481223e-08 2.6884959e-25 5.9945505e-24 7.3684091e-19
 1.0000000e+00 1.0917670e-18], sum to 1.0000
[2019-04-04 07:55:41,251] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5446
[2019-04-04 07:55:41,267] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.1825283135273, 0.0905249650244585, 0.0, 1.0, 39745.01635142711], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3036600.0000, 
sim time next is 3037200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.14882955647676, 0.08254441811063189, 0.0, 1.0, 39861.80822207215], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5124024630397299, 0.5275148060368773, 0.0, 1.0, 0.18981813439081976], 
reward next is 0.8102, 
noisyNet noise sample is [array([-0.76592815], dtype=float32), -0.50081855]. 
=============================================
[2019-04-04 07:55:42,598] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6470948e-12 3.9645959e-10 2.8987540e-31 6.5509658e-29 7.7280222e-23
 1.0000000e+00 9.3832095e-23], sum to 1.0000
[2019-04-04 07:55:42,598] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8071
[2019-04-04 07:55:42,636] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 90.33333333333333, 464.3333333333333, 26.0, 25.90785620746978, 0.5948292413311294, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3228600.0000, 
sim time next is 3229200.0000, 
raw observation next is [-3.0, 92.0, 93.0, 511.5, 26.0, 26.00474802424819, 0.606518018572629, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.31, 0.5651933701657459, 0.6666666666666666, 0.6670623353540158, 0.702172672857543, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.013394], dtype=float32), -0.101288326]. 
=============================================
[2019-04-04 07:55:45,876] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0363731e-10 3.1038163e-09 2.1879140e-28 7.7539074e-26 1.2504409e-21
 1.0000000e+00 1.3406890e-20], sum to 1.0000
[2019-04-04 07:55:45,880] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4605
[2019-04-04 07:55:45,917] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 84.83333333333334, 0.0, 0.0, 26.0, 25.05711674841823, 0.2934851166068765, 0.0, 1.0, 54752.01118959405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2861400.0000, 
sim time next is 2862000.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.07543929589623, 0.2961217797604782, 0.0, 1.0, 53651.30025832925], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.589619941324686, 0.5987072599201594, 0.0, 1.0, 0.25548238218252023], 
reward next is 0.7445, 
noisyNet noise sample is [array([0.7187283], dtype=float32), -1.0235122]. 
=============================================
[2019-04-04 07:55:45,936] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[84.98397 ]
 [84.95782 ]
 [84.85367 ]
 [84.58106 ]
 [84.577965]], R is [[85.00299835]
 [84.89224243]
 [84.77809143]
 [84.66274261]
 [84.54838562]].
[2019-04-04 07:55:57,386] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3726393e-09 5.2359748e-09 1.7197153e-25 5.5802080e-24 1.1621972e-19
 1.0000000e+00 1.7894078e-18], sum to 1.0000
[2019-04-04 07:55:57,388] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4686
[2019-04-04 07:55:57,419] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.96371026169886, 0.3429652785585509, 0.0, 1.0, 41423.37609965451], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3371400.0000, 
sim time next is 3372000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.93181866707538, 0.3565012904215064, 0.0, 1.0, 41701.04912511476], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.577651555589615, 0.6188337634738355, 0.0, 1.0, 0.1985764244053084], 
reward next is 0.8014, 
noisyNet noise sample is [array([-0.39634097], dtype=float32), 1.4001825]. 
=============================================
[2019-04-04 07:55:57,430] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[78.02028]
 [78.03703]
 [77.93444]
 [77.80372]
 [77.90728]], R is [[78.21173096]
 [78.23236084]
 [78.25263214]
 [78.27255249]
 [78.29209137]].
[2019-04-04 07:56:07,405] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.2480425e-11 4.5122377e-09 5.7749473e-29 2.9885539e-26 4.5405945e-22
 1.0000000e+00 2.4939714e-21], sum to 1.0000
[2019-04-04 07:56:07,437] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3714
[2019-04-04 07:56:07,453] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.35622834776907, 0.3107643894317293, 0.0, 1.0, 63508.21569445484], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3133200.0000, 
sim time next is 3133800.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 25.35531556543287, 0.315116941076453, 0.0, 1.0, 57222.26729956547], 
processed observation next is [1.0, 0.2608695652173913, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6129429637860726, 0.6050389803588176, 0.0, 1.0, 0.27248698714078795], 
reward next is 0.7275, 
noisyNet noise sample is [array([-0.285451], dtype=float32), 1.0090638]. 
=============================================
[2019-04-04 07:56:10,941] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.90638233e-11 3.69214048e-10 7.81325367e-28 1.11407744e-26
 1.04026474e-21 1.00000000e+00 3.80563828e-21], sum to 1.0000
[2019-04-04 07:56:10,943] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7489
[2019-04-04 07:56:11,032] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 24.98567933089518, 0.4260420333138212, 0.0, 1.0, 94791.15190999027], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3440400.0000, 
sim time next is 3441000.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 24.98247260818918, 0.4360422307385555, 1.0, 1.0, 59119.45094789597], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5818727173490984, 0.6453474102461851, 1.0, 1.0, 0.28152119498998085], 
reward next is 0.7185, 
noisyNet noise sample is [array([0.34879473], dtype=float32), 0.7907103]. 
=============================================
[2019-04-04 07:56:11,047] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[84.2156  ]
 [84.287254]
 [83.650795]
 [83.93316 ]
 [83.713806]], R is [[84.65338135]
 [84.35546112]
 [84.09906769]
 [84.07205963]
 [84.2313385 ]].
[2019-04-04 07:56:19,029] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6321836e-09 1.0607167e-07 2.3505462e-24 2.2960982e-22 9.5296108e-19
 9.9999988e-01 1.1975772e-17], sum to 1.0000
[2019-04-04 07:56:19,031] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5895
[2019-04-04 07:56:19,050] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.33333333333333, 76.0, 0.0, 0.0, 26.0, 24.47039625723653, 0.2024481616530373, 0.0, 1.0, 43616.18397801429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3302400.0000, 
sim time next is 3303000.0000, 
raw observation next is [-10.5, 76.0, 0.0, 0.0, 26.0, 24.39886148542573, 0.1883777948123616, 0.0, 1.0, 43627.97321766606], 
processed observation next is [1.0, 0.21739130434782608, 0.17174515235457063, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5332384571188108, 0.5627925982707872, 0.0, 1.0, 0.20775225341745743], 
reward next is 0.7922, 
noisyNet noise sample is [array([-0.4177358], dtype=float32), 0.93266284]. 
=============================================
[2019-04-04 07:56:19,059] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.773605]
 [74.94696 ]
 [75.1205  ]
 [75.26439 ]
 [75.4186  ]], R is [[74.61869812]
 [74.66481781]
 [74.71031952]
 [74.75508881]
 [74.79914856]].
[2019-04-04 07:56:21,233] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6066346e-10 3.3979179e-09 9.1804558e-27 5.7529930e-25 3.6871605e-20
 1.0000000e+00 3.3084165e-20], sum to 1.0000
[2019-04-04 07:56:21,233] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8446
[2019-04-04 07:56:21,270] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.333333333333333, 54.0, 117.3333333333333, 809.1666666666667, 26.0, 26.11297024880354, 0.5635323766638758, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3328800.0000, 
sim time next is 3329400.0000, 
raw observation next is [-5.166666666666667, 54.0, 116.6666666666667, 807.3333333333334, 26.0, 26.01435438354135, 0.5536299608360165, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.31948291782086796, 0.54, 0.388888888888889, 0.8920810313075507, 0.6666666666666666, 0.6678628652951124, 0.6845433202786722, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08407494], dtype=float32), 1.0367043]. 
=============================================
[2019-04-04 07:56:24,047] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.3673207e-12 4.1449585e-10 2.3731198e-28 3.7032166e-27 6.8227431e-22
 1.0000000e+00 3.4229652e-22], sum to 1.0000
[2019-04-04 07:56:24,047] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1006
[2019-04-04 07:56:24,056] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 111.3333333333333, 800.8333333333334, 26.0, 26.29847878225935, 0.6139510776248985, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3418800.0000, 
sim time next is 3419400.0000, 
raw observation next is [3.0, 49.0, 109.6666666666667, 795.6666666666666, 26.0, 26.40693778187495, 0.6293754430979085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.3655555555555557, 0.8791896869244935, 0.6666666666666666, 0.7005781484895793, 0.7097918143659695, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1065012], dtype=float32), -0.4476645]. 
=============================================
[2019-04-04 07:56:26,381] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.2070976e-11 2.6396216e-09 1.1700399e-26 2.2468280e-25 5.2778647e-21
 1.0000000e+00 4.3622895e-20], sum to 1.0000
[2019-04-04 07:56:26,382] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6425
[2019-04-04 07:56:26,450] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666666, 71.33333333333333, 17.16666666666666, 155.6666666666667, 26.0, 25.49589557310507, 0.3939210791841697, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3483600.0000, 
sim time next is 3484200.0000, 
raw observation next is [-0.8333333333333334, 71.16666666666667, 31.33333333333333, 204.3333333333333, 26.0, 25.39923988147002, 0.3932774904484142, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.43951985226223456, 0.7116666666666667, 0.10444444444444442, 0.22578268876611413, 0.6666666666666666, 0.616603323455835, 0.6310924968161381, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9276373], dtype=float32), -0.041906215]. 
=============================================
[2019-04-04 07:56:32,304] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-04 07:56:32,315] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:56:32,316] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:56:32,318] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:56:32,318] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:56:32,324] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:56:32,391] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run21
[2019-04-04 07:56:32,398] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:56:32,609] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run21
[2019-04-04 07:56:32,742] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run21
[2019-04-04 07:59:41,286] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 08:00:09,661] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 08:00:14,060] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 08:00:15,083] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 2000000, evaluation results [2000000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 08:00:16,861] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1330049e-09 6.2910122e-09 1.4538244e-25 1.7061184e-24 5.7593756e-20
 1.0000000e+00 3.4780417e-19], sum to 1.0000
[2019-04-04 08:00:16,861] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3717
[2019-04-04 08:00:16,878] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 59.16666666666666, 0.0, 0.0, 26.0, 25.63943777345906, 0.5748000150630587, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3877800.0000, 
sim time next is 3878400.0000, 
raw observation next is [-1.0, 58.33333333333334, 0.0, 0.0, 26.0, 25.71761895734873, 0.5732305329232106, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.5833333333333335, 0.0, 0.0, 0.6666666666666666, 0.6431349131123941, 0.6910768443077369, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3832724], dtype=float32), 1.4361678]. 
=============================================
[2019-04-04 08:00:23,419] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2183494e-10 3.2893948e-09 1.7889091e-26 1.1743517e-24 7.4079465e-20
 1.0000000e+00 1.3576729e-20], sum to 1.0000
[2019-04-04 08:00:23,422] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4308
[2019-04-04 08:00:23,432] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 26.0, 25.37376678057673, 0.4710047995548871, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3872400.0000, 
sim time next is 3873000.0000, 
raw observation next is [1.0, 51.0, 0.0, 0.0, 26.0, 25.281546521174, 0.4552570888625946, 0.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.51, 0.0, 0.0, 0.6666666666666666, 0.6067955434311667, 0.6517523629541982, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.4659265], dtype=float32), -0.7477299]. 
=============================================
[2019-04-04 08:00:23,448] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.21446 ]
 [80.81322 ]
 [81.38208 ]
 [81.24411 ]
 [81.092926]], R is [[79.75780487]
 [79.87127686]
 [79.98361206]
 [80.09482574]
 [80.29387665]].
[2019-04-04 08:00:28,966] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4637086e-09 1.2303563e-08 9.1177135e-25 3.1790110e-23 9.1633838e-19
 1.0000000e+00 2.3187684e-18], sum to 1.0000
[2019-04-04 08:00:28,968] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4771
[2019-04-04 08:00:28,983] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.333333333333333, 44.66666666666667, 0.0, 0.0, 26.0, 25.44728279247686, 0.3927212886119885, 0.0, 1.0, 57161.85389414127], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3619200.0000, 
sim time next is 3619800.0000, 
raw observation next is [-1.5, 46.0, 0.0, 0.0, 26.0, 25.41831003948413, 0.391386290525206, 0.0, 1.0, 59811.64953315182], 
processed observation next is [0.0, 0.9130434782608695, 0.4210526315789474, 0.46, 0.0, 0.0, 0.6666666666666666, 0.6181925032903441, 0.6304620968417353, 0.0, 1.0, 0.2848173787292944], 
reward next is 0.7152, 
noisyNet noise sample is [array([-1.277782], dtype=float32), -1.0513206]. 
=============================================
[2019-04-04 08:00:29,302] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.1092745e-09 3.6954063e-08 2.5867271e-23 1.9235771e-22 7.9195504e-18
 1.0000000e+00 9.9423635e-18], sum to 1.0000
[2019-04-04 08:00:29,308] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8386
[2019-04-04 08:00:29,321] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 40.33333333333334, 0.0, 0.0, 26.0, 25.04164398116169, 0.3322170530138444, 0.0, 1.0, 40714.13537736724], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4063800.0000, 
sim time next is 4064400.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.05680472244678, 0.3277396060960582, 0.0, 1.0, 40752.43876395369], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5880670602038984, 0.6092465353653528, 0.0, 1.0, 0.19405923220930327], 
reward next is 0.8059, 
noisyNet noise sample is [array([0.9741533], dtype=float32), 1.2006905]. 
=============================================
[2019-04-04 08:00:36,443] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7844701e-09 2.0666074e-07 4.6006707e-24 4.3193943e-23 5.4750489e-18
 9.9999976e-01 6.0354104e-18], sum to 1.0000
[2019-04-04 08:00:36,444] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9493
[2019-04-04 08:00:36,461] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 25.0010500037704, 0.3060315683380984, 0.0, 1.0, 39467.05405201349], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4164000.0000, 
sim time next is 4164600.0000, 
raw observation next is [-3.833333333333333, 53.33333333333333, 0.0, 0.0, 26.0, 24.96059705733531, 0.2993216399080283, 0.0, 1.0, 39461.20940865902], 
processed observation next is [0.0, 0.17391304347826086, 0.3564173591874424, 0.5333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5800497547779425, 0.5997738799693427, 0.0, 1.0, 0.18791052099361438], 
reward next is 0.8121, 
noisyNet noise sample is [array([-0.9737365], dtype=float32), -0.22940734]. 
=============================================
[2019-04-04 08:00:44,790] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.6222877e-11 4.0833386e-09 2.1369151e-26 9.9607021e-25 3.3369096e-20
 1.0000000e+00 8.0613365e-20], sum to 1.0000
[2019-04-04 08:00:44,802] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4160
[2019-04-04 08:00:44,828] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.666666666666667, 39.5, 116.6666666666667, 804.3333333333334, 26.0, 26.49550303229624, 0.5812792328548869, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4014600.0000, 
sim time next is 4015200.0000, 
raw observation next is [-7.333333333333334, 39.0, 117.8333333333333, 810.1666666666666, 26.0, 26.48160869593675, 0.5820356741515926, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.2594644506001847, 0.39, 0.39277777777777767, 0.8952117863720074, 0.6666666666666666, 0.7068007246613958, 0.6940118913838642, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01993637], dtype=float32), 0.46497402]. 
=============================================
[2019-04-04 08:00:46,363] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7197381e-12 1.2622713e-10 5.0871610e-31 9.9242333e-29 3.2029302e-23
 1.0000000e+00 5.1936160e-23], sum to 1.0000
[2019-04-04 08:00:46,364] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0834
[2019-04-04 08:00:46,399] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 159.0, 4.0, 26.0, 26.10860628595847, 0.6137264877498715, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4456200.0000, 
sim time next is 4456800.0000, 
raw observation next is [0.0, 92.0, 140.5, 3.0, 26.0, 26.17012067416086, 0.6218168057309964, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.92, 0.4683333333333333, 0.0033149171270718232, 0.6666666666666666, 0.680843389513405, 0.7072722685769989, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3587737], dtype=float32), -0.8439323]. 
=============================================
[2019-04-04 08:00:47,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.3923272e-11 5.6270294e-10 5.1672999e-28 2.8289880e-26 1.7569785e-21
 1.0000000e+00 6.2957754e-21], sum to 1.0000
[2019-04-04 08:00:47,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5413
[2019-04-04 08:00:47,897] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.166666666666667, 31.66666666666667, 109.3333333333333, 806.0, 26.0, 26.81500240004313, 0.7125103767470069, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4111800.0000, 
sim time next is 4112400.0000, 
raw observation next is [3.333333333333333, 32.33333333333334, 107.6666666666667, 800.0, 26.0, 26.92459112165371, 0.7310893603168104, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5549399815327793, 0.3233333333333334, 0.358888888888889, 0.8839779005524862, 0.6666666666666666, 0.7437159268044757, 0.7436964534389369, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48740172], dtype=float32), -0.3162927]. 
=============================================
[2019-04-04 08:00:48,071] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4965038e-11 1.1970022e-09 4.2500974e-29 1.6234979e-26 9.5802611e-22
 1.0000000e+00 4.2810309e-21], sum to 1.0000
[2019-04-04 08:00:48,072] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8500
[2019-04-04 08:00:48,118] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.06249717222639, 0.4575665358630409, 1.0, 1.0, 23591.84888744531], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4478400.0000, 
sim time next is 4479000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.03110691416811, 0.452150011995568, 0.0, 1.0, 43980.62172635389], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5859255761806758, 0.6507166706651893, 0.0, 1.0, 0.20943153203025663], 
reward next is 0.7906, 
noisyNet noise sample is [array([1.186441], dtype=float32), -0.40606862]. 
=============================================
[2019-04-04 08:00:48,135] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.000015]
 [85.882324]
 [86.51153 ]
 [86.4694  ]
 [86.536095]], R is [[85.46033478]
 [85.49339294]
 [85.54937744]
 [85.6938858 ]
 [85.74784088]].
[2019-04-04 08:00:51,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5132962e-10 6.9630777e-09 2.7761851e-27 2.8641047e-26 1.1651435e-21
 1.0000000e+00 2.4297493e-20], sum to 1.0000
[2019-04-04 08:00:51,479] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4135
[2019-04-04 08:00:51,498] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.6, 69.0, 0.0, 0.0, 26.0, 25.43873011796288, 0.3691159726983123, 0.0, 1.0, 70132.2863804242], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4338000.0000, 
sim time next is 4338600.0000, 
raw observation next is [3.55, 69.33333333333333, 0.0, 0.0, 26.0, 25.40684927504778, 0.3747806900894587, 0.0, 1.0, 66397.8831103594], 
processed observation next is [1.0, 0.21739130434782608, 0.5609418282548477, 0.6933333333333332, 0.0, 0.0, 0.6666666666666666, 0.6172374395873149, 0.6249268966964863, 0.0, 1.0, 0.3161803957636162], 
reward next is 0.6838, 
noisyNet noise sample is [array([1.2436799], dtype=float32), -0.28075475]. 
=============================================
[2019-04-04 08:00:52,651] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1710324e-09 2.7376526e-08 3.1230806e-24 2.2141925e-22 2.9248538e-18
 1.0000000e+00 7.7250058e-18], sum to 1.0000
[2019-04-04 08:00:52,651] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6219
[2019-04-04 08:00:52,684] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666666, 30.33333333333333, 0.0, 0.0, 26.0, 25.52968419161521, 0.4820227524835435, 0.0, 1.0, 69938.71938201391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4052400.0000, 
sim time next is 4053000.0000, 
raw observation next is [-4.833333333333334, 30.66666666666667, 0.0, 0.0, 26.0, 25.4833671865053, 0.4828031997091805, 0.0, 1.0, 74903.6534743932], 
processed observation next is [1.0, 0.9130434782608695, 0.32871652816251157, 0.3066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6236139322087751, 0.6609343999030601, 0.0, 1.0, 0.3566840641637771], 
reward next is 0.6433, 
noisyNet noise sample is [array([0.24672003], dtype=float32), -1.289243]. 
=============================================
[2019-04-04 08:00:52,710] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.55034 ]
 [72.51195 ]
 [72.688866]
 [73.023026]
 [73.33251 ]], R is [[72.6568222 ]
 [72.59721375]
 [72.67558289]
 [72.94882965]
 [73.21934509]].
[2019-04-04 08:00:58,682] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2569640e-11 3.9828789e-09 1.8134381e-26 1.5445751e-24 6.8803552e-20
 1.0000000e+00 2.7460308e-19], sum to 1.0000
[2019-04-04 08:00:58,682] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3924
[2019-04-04 08:00:58,696] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.9, 69.66666666666667, 0.0, 0.0, 26.0, 24.92371655845263, 0.288846891237495, 0.0, 1.0, 56695.86860771634], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4302600.0000, 
sim time next is 4303200.0000, 
raw observation next is [5.8, 70.33333333333334, 0.0, 0.0, 26.0, 24.88493893743714, 0.2965466297898283, 0.0, 1.0, 196798.518188599], 
processed observation next is [0.0, 0.8260869565217391, 0.6232686980609419, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.5737449114530951, 0.5988488765966095, 0.0, 1.0, 0.9371358008980905], 
reward next is 0.0629, 
noisyNet noise sample is [array([1.0243329], dtype=float32), -1.5798985]. 
=============================================
[2019-04-04 08:01:02,017] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0633027e-11 6.7791739e-10 3.3304103e-28 3.2378629e-26 3.3667505e-21
 1.0000000e+00 1.7597295e-20], sum to 1.0000
[2019-04-04 08:01:02,017] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6139
[2019-04-04 08:01:02,030] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.866666666666667, 58.66666666666667, 134.8333333333333, 610.0, 26.0, 25.47509786831943, 0.4610052605392794, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4288800.0000, 
sim time next is 4289400.0000, 
raw observation next is [6.9, 58.0, 116.0, 655.0, 26.0, 25.47654040037177, 0.4641081440772945, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6537396121883658, 0.58, 0.38666666666666666, 0.7237569060773481, 0.6666666666666666, 0.6230450333643143, 0.6547027146924315, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5076771], dtype=float32), -0.014312251]. 
=============================================
[2019-04-04 08:01:13,128] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1206548e-10 1.6806578e-09 1.0399560e-27 1.9519429e-26 2.7416841e-21
 1.0000000e+00 1.9636294e-20], sum to 1.0000
[2019-04-04 08:01:13,132] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9052
[2019-04-04 08:01:13,189] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.00732256960551, 0.4629914090524419, 1.0, 1.0, 45838.84824888209], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4478400.0000, 
sim time next is 4479000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.04368468893399, 0.4639388700060298, 0.0, 1.0, 18709.4175032591], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5869737240778324, 0.6546462900020099, 0.0, 1.0, 0.08909246430123381], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.0118668], dtype=float32), -0.31746063]. 
=============================================
[2019-04-04 08:01:13,197] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.16903 ]
 [83.885925]
 [84.04451 ]
 [83.600716]
 [83.609695]], R is [[83.60144806]
 [83.54715729]
 [83.19000244]
 [82.90838623]
 [82.96541595]].
[2019-04-04 08:01:13,542] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.7282855e-12 9.2693375e-10 1.1213673e-28 5.7027105e-27 1.5980873e-22
 1.0000000e+00 1.0835544e-21], sum to 1.0000
[2019-04-04 08:01:13,543] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0087
[2019-04-04 08:01:13,552] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 208.5, 62.5, 26.0, 26.09374122446678, 0.5745268200569723, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4546800.0000, 
sim time next is 4547400.0000, 
raw observation next is [2.833333333333333, 45.5, 190.0, 45.66666666666666, 26.0, 26.27172900311142, 0.5899486937475796, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.541089566020314, 0.455, 0.6333333333333333, 0.05046040515653774, 0.6666666666666666, 0.6893107502592851, 0.6966495645825265, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.79230493], dtype=float32), 1.0476407]. 
=============================================
[2019-04-04 08:01:23,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:23,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:23,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run16
[2019-04-04 08:01:24,143] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:24,146] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:24,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run16
[2019-04-04 08:01:26,683] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.6562385e-12 2.4783273e-10 3.9774997e-30 6.2890318e-28 2.0333160e-21
 1.0000000e+00 3.1762612e-22], sum to 1.0000
[2019-04-04 08:01:26,683] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8137
[2019-04-04 08:01:26,697] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 121.0, 862.5, 26.0, 27.58013598296083, 0.9039112420502441, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5058000.0000, 
sim time next is 5058600.0000, 
raw observation next is [9.333333333333334, 24.16666666666667, 120.0, 861.6666666666666, 26.0, 27.69521429723304, 0.9322104603356666, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.7211449676823639, 0.24166666666666672, 0.4, 0.9521178637200737, 0.6666666666666666, 0.80793452476942, 0.8107368201118889, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.58288026], dtype=float32), 0.20042707]. 
=============================================
[2019-04-04 08:01:28,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:28,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:28,372] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run16
[2019-04-04 08:01:29,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:29,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:29,650] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run16
[2019-04-04 08:01:30,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:30,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:30,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run16
[2019-04-04 08:01:32,554] A3C_AGENT_WORKER-Thread-2 INFO:Local step 127500, global step 2035070: loss 0.3862
[2019-04-04 08:01:32,554] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 127500, global step 2035070: learning rate 0.0000
[2019-04-04 08:01:32,836] A3C_AGENT_WORKER-Thread-3 INFO:Local step 127500, global step 2035153: loss 0.3846
[2019-04-04 08:01:32,837] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 127500, global step 2035153: learning rate 0.0000
[2019-04-04 08:01:36,014] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:36,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:36,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run16
[2019-04-04 08:01:36,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:36,410] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:36,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run16
[2019-04-04 08:01:37,103] A3C_AGENT_WORKER-Thread-16 INFO:Local step 127500, global step 2036494: loss 0.3866
[2019-04-04 08:01:37,104] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 127500, global step 2036494: learning rate 0.0000
[2019-04-04 08:01:38,542] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7618174e-10 7.9010132e-09 2.1981905e-25 4.4177276e-24 3.9622417e-19
 1.0000000e+00 6.9308047e-19], sum to 1.0000
[2019-04-04 08:01:38,542] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0867
[2019-04-04 08:01:38,562] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.416666666666667, 74.83333333333333, 0.0, 0.0, 26.0, 23.37269961048132, -0.06395484839273313, 0.0, 1.0, 44885.9793918526], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 107400.0000, 
sim time next is 108000.0000, 
raw observation next is [-6.7, 75.0, 0.0, 0.0, 26.0, 23.31655173008694, -0.06945395021849109, 0.0, 1.0, 45096.50871361812], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.75, 0.0, 0.0, 0.6666666666666666, 0.4430459775072449, 0.476848683260503, 0.0, 1.0, 0.2147452795886577], 
reward next is 0.7853, 
noisyNet noise sample is [array([1.520937], dtype=float32), -0.2902548]. 
=============================================
[2019-04-04 08:01:38,588] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.08662 ]
 [78.46082 ]
 [78.844826]
 [79.23621 ]
 [79.61643 ]], R is [[77.73858643]
 [77.74745941]
 [77.75717163]
 [77.76758575]
 [77.77866364]].
[2019-04-04 08:01:39,024] A3C_AGENT_WORKER-Thread-18 INFO:Local step 127500, global step 2037181: loss 0.3889
[2019-04-04 08:01:39,025] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 127500, global step 2037181: learning rate 0.0000
[2019-04-04 08:01:39,154] A3C_AGENT_WORKER-Thread-6 INFO:Local step 127500, global step 2037215: loss 0.3993
[2019-04-04 08:01:39,155] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 127500, global step 2037215: learning rate 0.0000
[2019-04-04 08:01:41,013] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.62653048e-12 7.36279093e-11 7.22056354e-29 1.58241405e-27
 1.52484233e-21 1.00000000e+00 7.57863897e-22], sum to 1.0000
[2019-04-04 08:01:41,014] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0284
[2019-04-04 08:01:41,109] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.666666666666667, 25.33333333333333, 28.33333333333334, 253.3333333333333, 26.0, 27.33721509660292, 0.8476868517210754, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4988400.0000, 
sim time next is 4989000.0000, 
raw observation next is [6.333333333333333, 25.16666666666667, 22.66666666666667, 202.6666666666667, 26.0, 27.23548611409671, 0.8395088197655118, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6380424746075716, 0.2516666666666667, 0.07555555555555557, 0.22394106813996323, 0.6666666666666666, 0.7696238428413924, 0.7798362732551706, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11293942], dtype=float32), 0.41144225]. 
=============================================
[2019-04-04 08:01:41,159] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[86.46856]
 [87.05809]
 [87.61582]
 [88.06539]
 [88.53077]], R is [[86.03395081]
 [86.1736145 ]
 [86.31188202]
 [86.44876099]
 [86.58427429]].
[2019-04-04 08:01:44,874] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:44,874] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:44,878] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run16
[2019-04-04 08:01:44,932] A3C_AGENT_WORKER-Thread-4 INFO:Local step 127500, global step 2039225: loss 0.3760
[2019-04-04 08:01:44,933] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 127500, global step 2039226: learning rate 0.0000
[2019-04-04 08:01:45,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:45,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:45,109] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run16
[2019-04-04 08:01:45,230] A3C_AGENT_WORKER-Thread-15 INFO:Local step 127500, global step 2039314: loss 0.3816
[2019-04-04 08:01:45,240] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 127500, global step 2039314: learning rate 0.0000
[2019-04-04 08:01:47,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:47,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:47,598] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run16
[2019-04-04 08:01:48,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:48,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:48,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run16
[2019-04-04 08:01:50,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:50,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:50,108] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run16
[2019-04-04 08:01:51,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:51,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:51,944] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run16
[2019-04-04 08:01:53,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:53,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:53,800] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run16
[2019-04-04 08:01:54,503] A3C_AGENT_WORKER-Thread-12 INFO:Local step 127500, global step 2041482: loss 0.3774
[2019-04-04 08:01:54,503] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 127500, global step 2041482: learning rate 0.0000
[2019-04-04 08:01:54,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:54,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:54,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run16
[2019-04-04 08:01:55,148] A3C_AGENT_WORKER-Thread-11 INFO:Local step 127500, global step 2041598: loss 0.3628
[2019-04-04 08:01:55,148] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 127500, global step 2041598: learning rate 0.0000
[2019-04-04 08:01:56,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:01:56,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:01:56,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run16
[2019-04-04 08:01:58,195] A3C_AGENT_WORKER-Thread-10 INFO:Local step 127500, global step 2042061: loss 0.3694
[2019-04-04 08:01:58,195] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 127500, global step 2042061: learning rate 0.0000
[2019-04-04 08:01:59,691] A3C_AGENT_WORKER-Thread-19 INFO:Local step 127500, global step 2042326: loss 0.3690
[2019-04-04 08:01:59,692] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 127500, global step 2042326: learning rate 0.0000
[2019-04-04 08:02:00,146] A3C_AGENT_WORKER-Thread-13 INFO:Local step 127500, global step 2042420: loss 0.3726
[2019-04-04 08:02:00,147] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 127500, global step 2042420: learning rate 0.0000
[2019-04-04 08:02:02,599] A3C_AGENT_WORKER-Thread-14 INFO:Local step 127500, global step 2042985: loss 0.3744
[2019-04-04 08:02:02,599] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 127500, global step 2042985: learning rate 0.0000
[2019-04-04 08:02:02,671] A3C_AGENT_WORKER-Thread-3 INFO:Local step 128000, global step 2043002: loss 1.0709
[2019-04-04 08:02:02,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 128000, global step 2043002: learning rate 0.0000
[2019-04-04 08:02:02,823] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.8751780e-10 5.2896878e-09 1.4108047e-26 1.1210702e-24 7.8391955e-21
 1.0000000e+00 2.4920527e-19], sum to 1.0000
[2019-04-04 08:02:02,823] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3342
[2019-04-04 08:02:02,851] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.2560216395749, -0.5648554714690567, 0.0, 1.0, 40289.55994337339], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 23400.0000, 
sim time next is 24000.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 26.0, 21.27657085770218, -0.5596152463626756, 0.0, 1.0, 40276.90643655739], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.2730475714751816, 0.31346158454577483, 0.0, 1.0, 0.19179479255503518], 
reward next is 0.8082, 
noisyNet noise sample is [array([-0.46312428], dtype=float32), -0.621545]. 
=============================================
[2019-04-04 08:02:02,857] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.1077 ]
 [82.04262]
 [81.9656 ]
 [81.90524]
 [81.81751]], R is [[82.15174103]
 [82.1383667 ]
 [82.12503815]
 [82.11174011]
 [82.09846497]].
[2019-04-04 08:02:03,563] A3C_AGENT_WORKER-Thread-2 INFO:Local step 128000, global step 2043226: loss 1.0314
[2019-04-04 08:02:03,563] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 128000, global step 2043226: learning rate 0.0000
[2019-04-04 08:02:04,267] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9195426e-10 7.5352746e-09 8.9600332e-26 1.5463142e-24 1.8523885e-19
 1.0000000e+00 1.8929199e-19], sum to 1.0000
[2019-04-04 08:02:04,268] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3061
[2019-04-04 08:02:04,314] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.15, 61.5, 87.0, 512.0, 26.0, 25.82912897032808, 0.3618578635121333, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 297000.0000, 
sim time next is 297600.0000, 
raw observation next is [-10.96666666666667, 61.0, 90.16666666666666, 536.3333333333334, 26.0, 25.82335594581672, 0.3602859564727177, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.15881809787626952, 0.61, 0.3005555555555555, 0.592633517495396, 0.6666666666666666, 0.6519463288180599, 0.6200953188242392, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1897461], dtype=float32), 0.9575981]. 
=============================================
[2019-04-04 08:02:04,331] A3C_AGENT_WORKER-Thread-5 INFO:Local step 127500, global step 2043409: loss 0.3570
[2019-04-04 08:02:04,331] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 127500, global step 2043409: learning rate 0.0000
[2019-04-04 08:02:05,442] A3C_AGENT_WORKER-Thread-17 INFO:Local step 127500, global step 2043740: loss 0.3706
[2019-04-04 08:02:05,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 127500, global step 2043740: learning rate 0.0000
[2019-04-04 08:02:06,935] A3C_AGENT_WORKER-Thread-20 INFO:Local step 127500, global step 2044076: loss 0.3741
[2019-04-04 08:02:06,937] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 127500, global step 2044076: learning rate 0.0000
[2019-04-04 08:02:09,379] A3C_AGENT_WORKER-Thread-16 INFO:Local step 128000, global step 2044633: loss 1.0410
[2019-04-04 08:02:09,379] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 128000, global step 2044633: learning rate 0.0000
[2019-04-04 08:02:10,243] A3C_AGENT_WORKER-Thread-18 INFO:Local step 128000, global step 2044897: loss 1.0227
[2019-04-04 08:02:10,245] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 128000, global step 2044897: learning rate 0.0000
[2019-04-04 08:02:10,853] A3C_AGENT_WORKER-Thread-6 INFO:Local step 128000, global step 2045083: loss 0.9810
[2019-04-04 08:02:10,854] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 128000, global step 2045083: learning rate 0.0000
[2019-04-04 08:02:12,852] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.2875305e-10 4.7180815e-09 4.3737900e-26 4.4671260e-24 2.3472997e-19
 1.0000000e+00 1.2861454e-19], sum to 1.0000
[2019-04-04 08:02:12,854] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9675
[2019-04-04 08:02:12,947] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 75.0, 101.5, 0.0, 26.0, 25.33843366372377, 0.2130454596066233, 1.0, 1.0, 18735.06468850206], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 208800.0000, 
sim time next is 209400.0000, 
raw observation next is [-7.116666666666666, 74.5, 109.0, 0.0, 26.0, 25.33701357318431, 0.2172063717961486, 1.0, 1.0, 19020.37479803658], 
processed observation next is [1.0, 0.43478260869565216, 0.265466297322253, 0.745, 0.36333333333333334, 0.0, 0.6666666666666666, 0.6114177977653593, 0.5724021239320495, 1.0, 1.0, 0.09057321332398371], 
reward next is 0.9094, 
noisyNet noise sample is [array([-1.8774214], dtype=float32), -1.6669859]. 
=============================================
[2019-04-04 08:02:16,950] A3C_AGENT_WORKER-Thread-4 INFO:Local step 128000, global step 2046574: loss 0.9341
[2019-04-04 08:02:16,951] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 128000, global step 2046574: learning rate 0.0000
[2019-04-04 08:02:17,401] A3C_AGENT_WORKER-Thread-15 INFO:Local step 128000, global step 2046676: loss 0.9969
[2019-04-04 08:02:17,421] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 128000, global step 2046676: learning rate 0.0000
[2019-04-04 08:02:21,312] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3158730e-11 3.2225620e-09 5.7426349e-28 4.4383817e-26 2.1988240e-21
 1.0000000e+00 5.3463155e-21], sum to 1.0000
[2019-04-04 08:02:21,319] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3400
[2019-04-04 08:02:21,334] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.9, 93.33333333333334, 0.0, 0.0, 26.0, 24.84885717990676, 0.2331518028813951, 0.0, 1.0, 40907.63460564463], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 512400.0000, 
sim time next is 513000.0000, 
raw observation next is [3.0, 94.0, 0.0, 0.0, 26.0, 24.84442048157373, 0.2328825112181385, 0.0, 1.0, 40793.74227061921], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.94, 0.0, 0.0, 0.6666666666666666, 0.5703683734644777, 0.5776275037393795, 0.0, 1.0, 0.19425591557437719], 
reward next is 0.8057, 
noisyNet noise sample is [array([-0.62602866], dtype=float32), 0.13782728]. 
=============================================
[2019-04-04 08:02:21,341] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[85.30748 ]
 [85.37413 ]
 [85.51239 ]
 [85.50136 ]
 [85.636734]], R is [[85.49523163]
 [85.44548035]
 [85.39573669]
 [85.34603119]
 [85.29631042]].
[2019-04-04 08:02:25,419] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6798425e-11 7.1524542e-10 1.7381597e-28 2.8585053e-26 1.9972997e-21
 1.0000000e+00 9.5182300e-21], sum to 1.0000
[2019-04-04 08:02:25,419] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2577
[2019-04-04 08:02:25,482] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 80.0, 134.0, 495.5, 26.0, 24.95914801577921, 0.3232041106560725, 0.0, 1.0, 35220.4881852539], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 565200.0000, 
sim time next is 565800.0000, 
raw observation next is [-1.2, 80.0, 135.3333333333333, 528.6666666666666, 26.0, 24.93676420521925, 0.3278864464451518, 0.0, 1.0, 47463.65194559583], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.45111111111111096, 0.5841620626151013, 0.6666666666666666, 0.578063683768271, 0.6092954821483839, 0.0, 1.0, 0.22601739021712303], 
reward next is 0.7740, 
noisyNet noise sample is [array([-0.52012074], dtype=float32), -0.8254438]. 
=============================================
[2019-04-04 08:02:27,301] A3C_AGENT_WORKER-Thread-12 INFO:Local step 128000, global step 2049206: loss 1.0180
[2019-04-04 08:02:27,302] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 128000, global step 2049206: learning rate 0.0000
[2019-04-04 08:02:28,040] A3C_AGENT_WORKER-Thread-11 INFO:Local step 128000, global step 2049422: loss 1.0410
[2019-04-04 08:02:28,040] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 128000, global step 2049422: learning rate 0.0000
[2019-04-04 08:02:30,978] A3C_AGENT_WORKER-Thread-10 INFO:Local step 128000, global step 2050312: loss 0.9645
[2019-04-04 08:02:30,979] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 128000, global step 2050312: learning rate 0.0000
[2019-04-04 08:02:31,708] A3C_AGENT_WORKER-Thread-13 INFO:Local step 128000, global step 2050494: loss 1.0475
[2019-04-04 08:02:31,709] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 128000, global step 2050494: learning rate 0.0000
[2019-04-04 08:02:31,768] A3C_AGENT_WORKER-Thread-3 INFO:Local step 128500, global step 2050505: loss 0.8605
[2019-04-04 08:02:31,768] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 128500, global step 2050505: learning rate 0.0000
[2019-04-04 08:02:31,788] A3C_AGENT_WORKER-Thread-19 INFO:Local step 128000, global step 2050510: loss 1.0557
[2019-04-04 08:02:31,790] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 128000, global step 2050510: learning rate 0.0000
[2019-04-04 08:02:32,240] A3C_AGENT_WORKER-Thread-2 INFO:Local step 128500, global step 2050619: loss 0.8587
[2019-04-04 08:02:32,246] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 128500, global step 2050620: learning rate 0.0000
[2019-04-04 08:02:33,311] A3C_AGENT_WORKER-Thread-14 INFO:Local step 128000, global step 2050899: loss 1.0361
[2019-04-04 08:02:33,311] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 128000, global step 2050899: learning rate 0.0000
[2019-04-04 08:02:34,303] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4877681e-09 6.0158780e-09 1.3609356e-26 7.2713274e-25 1.7683236e-19
 1.0000000e+00 1.4109718e-18], sum to 1.0000
[2019-04-04 08:02:34,304] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8229
[2019-04-04 08:02:34,371] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.633333333333333, 87.0, 0.0, 0.0, 26.0, 24.96217262825176, 0.278284999005222, 0.0, 1.0, 50628.45749733503], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 585600.0000, 
sim time next is 586200.0000, 
raw observation next is [-2.716666666666667, 87.0, 0.0, 0.0, 26.0, 24.94367306464763, 0.2783374406580816, 0.0, 1.0, 57972.92754029929], 
processed observation next is [0.0, 0.782608695652174, 0.3873499538319483, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5786394220539691, 0.5927791468860272, 0.0, 1.0, 0.27606155971571095], 
reward next is 0.7239, 
noisyNet noise sample is [array([1.0464728], dtype=float32), 0.37940675]. 
=============================================
[2019-04-04 08:02:35,461] A3C_AGENT_WORKER-Thread-5 INFO:Local step 128000, global step 2051403: loss 1.0075
[2019-04-04 08:02:35,462] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 128000, global step 2051403: learning rate 0.0000
[2019-04-04 08:02:37,377] A3C_AGENT_WORKER-Thread-17 INFO:Local step 128000, global step 2051984: loss 1.0395
[2019-04-04 08:02:37,378] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 128000, global step 2051984: learning rate 0.0000
[2019-04-04 08:02:38,050] A3C_AGENT_WORKER-Thread-18 INFO:Local step 128500, global step 2052210: loss 0.8684
[2019-04-04 08:02:38,062] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 128500, global step 2052212: learning rate 0.0000
[2019-04-04 08:02:38,375] A3C_AGENT_WORKER-Thread-16 INFO:Local step 128500, global step 2052303: loss 0.8670
[2019-04-04 08:02:38,378] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 128500, global step 2052305: learning rate 0.0000
[2019-04-04 08:02:38,816] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0902817e-10 9.9013056e-09 7.5215114e-26 1.9492960e-24 1.1021146e-19
 1.0000000e+00 9.1227165e-19], sum to 1.0000
[2019-04-04 08:02:38,832] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7245
[2019-04-04 08:02:38,915] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 68.0, 135.0, 51.0, 26.0, 25.18753304071021, 0.2427406720562342, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 639000.0000, 
sim time next is 639600.0000, 
raw observation next is [-3.899999999999999, 67.0, 129.1666666666667, 42.5, 26.0, 25.12818477152884, 0.2261491716400879, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.35457063711911363, 0.67, 0.4305555555555557, 0.04696132596685083, 0.6666666666666666, 0.5940153976274033, 0.5753830572133626, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24794002], dtype=float32), 0.32607245]. 
=============================================
[2019-04-04 08:02:39,184] A3C_AGENT_WORKER-Thread-6 INFO:Local step 128500, global step 2052508: loss 0.8701
[2019-04-04 08:02:39,185] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 128500, global step 2052508: learning rate 0.0000
[2019-04-04 08:02:39,996] A3C_AGENT_WORKER-Thread-20 INFO:Local step 128000, global step 2052686: loss 0.9967
[2019-04-04 08:02:39,997] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 128000, global step 2052686: learning rate 0.0000
[2019-04-04 08:02:42,395] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.9204579e-10 8.4098311e-09 2.6271989e-25 1.1049534e-24 4.4720117e-20
 1.0000000e+00 2.7653519e-19], sum to 1.0000
[2019-04-04 08:02:42,395] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9001
[2019-04-04 08:02:42,442] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 54.0, 55.0, 26.5, 26.0, 24.87719425562774, 0.2227243867193659, 0.0, 1.0, 42015.02229329164], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 662400.0000, 
sim time next is 663000.0000, 
raw observation next is [-0.7, 54.5, 45.66666666666666, 22.66666666666666, 26.0, 24.89981619937474, 0.2202679908268631, 0.0, 1.0, 31231.28286150737], 
processed observation next is [0.0, 0.6956521739130435, 0.443213296398892, 0.545, 0.1522222222222222, 0.02504604051565377, 0.6666666666666666, 0.5749846832812283, 0.5734226636089543, 0.0, 1.0, 0.14872039457860653], 
reward next is 0.8513, 
noisyNet noise sample is [array([-0.17491005], dtype=float32), -0.9301206]. 
=============================================
[2019-04-04 08:02:42,452] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[77.01017]
 [77.08707]
 [77.148  ]
 [77.16171]
 [77.19562]], R is [[76.93916321]
 [76.96969604]
 [76.98162079]
 [76.99072266]
 [77.02278137]].
[2019-04-04 08:02:42,993] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1373304e-10 3.7385939e-09 2.2659167e-27 6.1117768e-25 3.8137498e-20
 1.0000000e+00 1.3369818e-19], sum to 1.0000
[2019-04-04 08:02:42,994] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2763
[2019-04-04 08:02:43,076] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.97591074184835, 0.3012214966125514, 0.0, 1.0, 79431.64285071363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 588600.0000, 
sim time next is 589200.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 25.02208609115603, 0.3059082074010565, 0.0, 1.0, 55786.72100675124], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5851738409296692, 0.6019694024670188, 0.0, 1.0, 0.26565105241310116], 
reward next is 0.7343, 
noisyNet noise sample is [array([-0.0518291], dtype=float32), 1.4722406]. 
=============================================
[2019-04-04 08:02:45,461] A3C_AGENT_WORKER-Thread-4 INFO:Local step 128500, global step 2054295: loss 0.8722
[2019-04-04 08:02:45,465] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 128500, global step 2054297: learning rate 0.0000
[2019-04-04 08:02:46,191] A3C_AGENT_WORKER-Thread-15 INFO:Local step 128500, global step 2054528: loss 0.8684
[2019-04-04 08:02:46,194] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 128500, global step 2054528: learning rate 0.0000
[2019-04-04 08:02:51,516] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.6671079e-10 3.4980197e-08 4.4735528e-25 3.8072297e-24 1.5712324e-19
 1.0000000e+00 8.3756917e-19], sum to 1.0000
[2019-04-04 08:02:51,516] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7904
[2019-04-04 08:02:51,563] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.27173470705799, 0.05338054274654772, 0.0, 1.0, 41433.56783361636], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 702000.0000, 
sim time next is 702600.0000, 
raw observation next is [-3.3, 75.0, 0.0, 0.0, 26.0, 24.2711637327798, 0.05503330739065138, 0.0, 1.0, 41505.69802623042], 
processed observation next is [1.0, 0.13043478260869565, 0.37119113573407203, 0.75, 0.0, 0.0, 0.6666666666666666, 0.52259697773165, 0.5183444357968838, 0.0, 1.0, 0.1976461810772877], 
reward next is 0.8024, 
noisyNet noise sample is [array([-1.5244486], dtype=float32), -0.17613281]. 
=============================================
[2019-04-04 08:02:54,597] A3C_AGENT_WORKER-Thread-12 INFO:Local step 128500, global step 2057042: loss 0.8737
[2019-04-04 08:02:54,602] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 128500, global step 2057042: learning rate 0.0000
[2019-04-04 08:02:55,391] A3C_AGENT_WORKER-Thread-11 INFO:Local step 128500, global step 2057246: loss 0.8656
[2019-04-04 08:02:55,394] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 128500, global step 2057246: learning rate 0.0000
[2019-04-04 08:02:58,473] A3C_AGENT_WORKER-Thread-10 INFO:Local step 128500, global step 2058158: loss 0.8771
[2019-04-04 08:02:58,475] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 128500, global step 2058158: learning rate 0.0000
[2019-04-04 08:02:58,582] A3C_AGENT_WORKER-Thread-3 INFO:Local step 129000, global step 2058194: loss 1.8419
[2019-04-04 08:02:58,582] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 129000, global step 2058194: learning rate 0.0000
[2019-04-04 08:02:59,042] A3C_AGENT_WORKER-Thread-13 INFO:Local step 128500, global step 2058350: loss 0.8759
[2019-04-04 08:02:59,043] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 128500, global step 2058350: learning rate 0.0000
[2019-04-04 08:02:59,117] A3C_AGENT_WORKER-Thread-2 INFO:Local step 129000, global step 2058378: loss 1.8454
[2019-04-04 08:02:59,129] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 129000, global step 2058378: learning rate 0.0000
[2019-04-04 08:02:59,681] A3C_AGENT_WORKER-Thread-19 INFO:Local step 128500, global step 2058562: loss 0.8727
[2019-04-04 08:02:59,682] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 128500, global step 2058562: learning rate 0.0000
[2019-04-04 08:03:00,968] A3C_AGENT_WORKER-Thread-14 INFO:Local step 128500, global step 2059020: loss 0.8833
[2019-04-04 08:03:00,969] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 128500, global step 2059020: learning rate 0.0000
[2019-04-04 08:03:02,927] A3C_AGENT_WORKER-Thread-5 INFO:Local step 128500, global step 2059627: loss 0.8767
[2019-04-04 08:03:02,927] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 128500, global step 2059627: learning rate 0.0000
[2019-04-04 08:03:03,704] A3C_AGENT_WORKER-Thread-18 INFO:Local step 129000, global step 2059878: loss 1.8752
[2019-04-04 08:03:03,704] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 129000, global step 2059878: learning rate 0.0000
[2019-04-04 08:03:04,623] A3C_AGENT_WORKER-Thread-16 INFO:Local step 129000, global step 2060240: loss 1.8039
[2019-04-04 08:03:04,624] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 129000, global step 2060240: learning rate 0.0000
[2019-04-04 08:03:04,794] A3C_AGENT_WORKER-Thread-17 INFO:Local step 128500, global step 2060312: loss 0.8793
[2019-04-04 08:03:04,795] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 128500, global step 2060312: learning rate 0.0000
[2019-04-04 08:03:04,840] A3C_AGENT_WORKER-Thread-6 INFO:Local step 129000, global step 2060328: loss 1.8561
[2019-04-04 08:03:04,840] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 129000, global step 2060328: learning rate 0.0000
[2019-04-04 08:03:06,664] A3C_AGENT_WORKER-Thread-20 INFO:Local step 128500, global step 2061051: loss 0.8865
[2019-04-04 08:03:06,665] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 128500, global step 2061051: learning rate 0.0000
[2019-04-04 08:03:08,296] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6217533e-10 1.5001584e-08 3.7527787e-26 5.3421868e-25 6.8756325e-20
 1.0000000e+00 4.3349537e-19], sum to 1.0000
[2019-04-04 08:03:08,297] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9653
[2019-04-04 08:03:08,364] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.99289710570721, 0.2482116744536555, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 891000.0000, 
sim time next is 891600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.13529534109222, 0.2494025292238976, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5946079450910183, 0.5831341764079658, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4130539], dtype=float32), -0.47956577]. 
=============================================
[2019-04-04 08:03:10,154] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8332560e-09 2.1255001e-08 6.7247247e-25 3.7331517e-23 2.1005291e-19
 1.0000000e+00 5.0547738e-18], sum to 1.0000
[2019-04-04 08:03:10,156] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4922
[2019-04-04 08:03:10,199] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.87827903437, 0.2013358294776576, 0.0, 1.0, 56899.82649067248], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 669000.0000, 
sim time next is 669600.0000, 
raw observation next is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.86734112436041, 0.2034636400330516, 0.0, 1.0, 55076.37726380645], 
processed observation next is [0.0, 0.782608695652174, 0.42936288088642666, 0.57, 0.0, 0.0, 0.6666666666666666, 0.5722784270300343, 0.5678212133443505, 0.0, 1.0, 0.26226846316098307], 
reward next is 0.7377, 
noisyNet noise sample is [array([3.1086411], dtype=float32), -1.6483119]. 
=============================================
[2019-04-04 08:03:10,288] A3C_AGENT_WORKER-Thread-4 INFO:Local step 129000, global step 2062601: loss 1.8214
[2019-04-04 08:03:10,295] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 129000, global step 2062601: learning rate 0.0000
[2019-04-04 08:03:10,552] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2808037e-11 2.5863003e-10 9.5364622e-31 8.0817136e-29 9.3358665e-23
 1.0000000e+00 8.9567324e-23], sum to 1.0000
[2019-04-04 08:03:10,554] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0844
[2019-04-04 08:03:10,571] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.3, 75.5, 0.0, 0.0, 26.0, 26.04438386451297, 0.6293779662178652, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1041000.0000, 
sim time next is 1041600.0000, 
raw observation next is [14.2, 76.0, 0.0, 0.0, 26.0, 26.02685737638963, 0.6200976258975024, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.8559556786703602, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6689047813658023, 0.7066992086325007, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.78223145], dtype=float32), -0.85145646]. 
=============================================
[2019-04-04 08:03:11,157] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.3799788e-13 3.2532291e-10 7.2487852e-32 1.9567486e-29 4.8296570e-23
 1.0000000e+00 3.4137457e-22], sum to 1.0000
[2019-04-04 08:03:11,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7632
[2019-04-04 08:03:11,182] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.11666666666667, 80.5, 0.0, 0.0, 26.0, 25.62216666231978, 0.5970340078864028, 0.0, 1.0, 24752.07907255277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1062600.0000, 
sim time next is 1063200.0000, 
raw observation next is [12.93333333333334, 81.0, 0.0, 0.0, 26.0, 25.68458653687527, 0.622005800978403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.8208679593721148, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6403822114062724, 0.7073352669928009, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28174722], dtype=float32), 0.08251757]. 
=============================================
[2019-04-04 08:03:11,711] A3C_AGENT_WORKER-Thread-15 INFO:Local step 129000, global step 2063264: loss 1.8189
[2019-04-04 08:03:11,712] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 129000, global step 2063264: learning rate 0.0000
[2019-04-04 08:03:12,759] A3C_AGENT_WORKER-Thread-3 INFO:Local step 129500, global step 2063776: loss 0.4817
[2019-04-04 08:03:12,764] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 129500, global step 2063777: learning rate 0.0000
[2019-04-04 08:03:12,846] A3C_AGENT_WORKER-Thread-2 INFO:Local step 129500, global step 2063818: loss 0.4713
[2019-04-04 08:03:12,847] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 129500, global step 2063818: learning rate 0.0000
[2019-04-04 08:03:17,343] A3C_AGENT_WORKER-Thread-18 INFO:Local step 129500, global step 2065839: loss 0.4611
[2019-04-04 08:03:17,344] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 129500, global step 2065839: learning rate 0.0000
[2019-04-04 08:03:17,662] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3544859e-10 2.7983170e-08 2.1326652e-25 1.3073781e-24 3.7597416e-19
 1.0000000e+00 2.6708518e-18], sum to 1.0000
[2019-04-04 08:03:17,668] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3561
[2019-04-04 08:03:17,737] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.616666666666667, 75.0, 21.33333333333334, 0.0, 26.0, 25.41330475271119, 0.2662927514863024, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 807000.0000, 
sim time next is 807600.0000, 
raw observation next is [-6.533333333333334, 75.0, 26.66666666666667, 0.0, 26.0, 25.41764477140353, 0.2933917509010219, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.28162511542012925, 0.75, 0.0888888888888889, 0.0, 0.6666666666666666, 0.6181370642836276, 0.5977972503003407, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03234223], dtype=float32), -0.7643637]. 
=============================================
[2019-04-04 08:03:18,102] A3C_AGENT_WORKER-Thread-6 INFO:Local step 129500, global step 2066193: loss 0.4570
[2019-04-04 08:03:18,104] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 129500, global step 2066193: learning rate 0.0000
[2019-04-04 08:03:18,330] A3C_AGENT_WORKER-Thread-16 INFO:Local step 129500, global step 2066322: loss 0.4573
[2019-04-04 08:03:18,331] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 129500, global step 2066322: learning rate 0.0000
[2019-04-04 08:03:18,788] A3C_AGENT_WORKER-Thread-12 INFO:Local step 129000, global step 2066534: loss 1.9245
[2019-04-04 08:03:18,790] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 129000, global step 2066534: learning rate 0.0000
[2019-04-04 08:03:19,179] A3C_AGENT_WORKER-Thread-11 INFO:Local step 129000, global step 2066683: loss 1.9246
[2019-04-04 08:03:19,180] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 129000, global step 2066683: learning rate 0.0000
[2019-04-04 08:03:19,187] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6848679e-10 3.2784101e-08 1.1447735e-25 3.1944358e-24 4.4873572e-20
 1.0000000e+00 6.7186939e-19], sum to 1.0000
[2019-04-04 08:03:19,188] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4558
[2019-04-04 08:03:19,210] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.2, 81.66666666666667, 0.0, 0.0, 26.0, 24.81106195830476, 0.2513174132819223, 0.0, 1.0, 41484.45330957807], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 858000.0000, 
sim time next is 858600.0000, 
raw observation next is [-3.1, 81.0, 0.0, 0.0, 26.0, 24.78779943392512, 0.2468039214060481, 0.0, 1.0, 41401.01788892916], 
processed observation next is [1.0, 0.9565217391304348, 0.37673130193905824, 0.81, 0.0, 0.0, 0.6666666666666666, 0.5656499528270933, 0.582267973802016, 0.0, 1.0, 0.197147704232996], 
reward next is 0.8029, 
noisyNet noise sample is [array([1.4668024], dtype=float32), -0.8540796]. 
=============================================
[2019-04-04 08:03:20,985] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.0384302e-12 2.2135358e-09 3.7972101e-30 3.6245990e-28 1.6764613e-23
 1.0000000e+00 2.2591560e-22], sum to 1.0000
[2019-04-04 08:03:20,985] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8419
[2019-04-04 08:03:20,997] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.200000000000001, 83.0, 0.0, 0.0, 26.0, 25.70443283018223, 0.4640435899644521, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 973200.0000, 
sim time next is 973800.0000, 
raw observation next is [9.4, 83.0, 0.0, 0.0, 26.0, 25.62576375874747, 0.449001770363239, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7229916897506927, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6354803132289559, 0.6496672567877463, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.85893315], dtype=float32), -0.05401246]. 
=============================================
[2019-04-04 08:03:21,798] A3C_AGENT_WORKER-Thread-13 INFO:Local step 129000, global step 2067908: loss 1.8973
[2019-04-04 08:03:21,799] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 129000, global step 2067908: learning rate 0.0000
[2019-04-04 08:03:21,978] A3C_AGENT_WORKER-Thread-19 INFO:Local step 129000, global step 2068008: loss 1.8414
[2019-04-04 08:03:21,981] A3C_AGENT_WORKER-Thread-10 INFO:Local step 129000, global step 2068010: loss 1.8978
[2019-04-04 08:03:21,981] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 129000, global step 2068009: learning rate 0.0000
[2019-04-04 08:03:21,981] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 129000, global step 2068010: learning rate 0.0000
[2019-04-04 08:03:23,203] A3C_AGENT_WORKER-Thread-4 INFO:Local step 129500, global step 2068656: loss 0.4562
[2019-04-04 08:03:23,204] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 129500, global step 2068656: learning rate 0.0000
[2019-04-04 08:03:23,745] A3C_AGENT_WORKER-Thread-15 INFO:Local step 129500, global step 2068951: loss 0.4360
[2019-04-04 08:03:23,746] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 129500, global step 2068951: learning rate 0.0000
[2019-04-04 08:03:24,293] A3C_AGENT_WORKER-Thread-14 INFO:Local step 129000, global step 2069205: loss 1.8535
[2019-04-04 08:03:24,294] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 129000, global step 2069205: learning rate 0.0000
[2019-04-04 08:03:26,050] A3C_AGENT_WORKER-Thread-5 INFO:Local step 129000, global step 2070093: loss 1.8276
[2019-04-04 08:03:26,053] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 129000, global step 2070094: learning rate 0.0000
[2019-04-04 08:03:27,058] A3C_AGENT_WORKER-Thread-17 INFO:Local step 129000, global step 2070669: loss 1.8354
[2019-04-04 08:03:27,060] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 129000, global step 2070669: learning rate 0.0000
[2019-04-04 08:03:29,138] A3C_AGENT_WORKER-Thread-20 INFO:Local step 129000, global step 2071824: loss 1.8584
[2019-04-04 08:03:29,139] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 129000, global step 2071824: learning rate 0.0000
[2019-04-04 08:03:29,462] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0110400e-11 2.5967992e-10 5.6668123e-30 4.0346265e-28 1.0333158e-21
 1.0000000e+00 9.5375974e-22], sum to 1.0000
[2019-04-04 08:03:29,465] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7578
[2019-04-04 08:03:29,489] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.7000000000000001, 95.0, 15.0, 0.0, 26.0, 25.55343850316529, 0.4887714564547792, 1.0, 1.0, 36300.10573948561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1356000.0000, 
sim time next is 1356600.0000, 
raw observation next is [0.6, 95.5, 12.0, 0.0, 26.0, 25.50891904966633, 0.3838459288369021, 1.0, 1.0, 28026.49050922374], 
processed observation next is [1.0, 0.6956521739130435, 0.479224376731302, 0.955, 0.04, 0.0, 0.6666666666666666, 0.6257432541388607, 0.627948642945634, 1.0, 1.0, 0.13345947861535112], 
reward next is 0.8665, 
noisyNet noise sample is [array([-0.8128904], dtype=float32), -0.7406358]. 
=============================================
[2019-04-04 08:03:29,585] A3C_AGENT_WORKER-Thread-2 INFO:Local step 130000, global step 2072082: loss 1.1121
[2019-04-04 08:03:29,586] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 130000, global step 2072082: learning rate 0.0000
[2019-04-04 08:03:29,634] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8904747e-12 7.0215289e-10 4.1430331e-31 1.1277505e-28 4.5297943e-24
 1.0000000e+00 1.4508682e-22], sum to 1.0000
[2019-04-04 08:03:29,639] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5507
[2019-04-04 08:03:29,683] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.3, 77.16666666666667, 0.0, 0.0, 26.0, 25.5399269807661, 0.5751727998837609, 0.0, 1.0, 128030.3641787439], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1047000.0000, 
sim time next is 1047600.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 25.56507704402874, 0.6138239130630473, 0.0, 1.0, 55958.9905167527], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.630423087002395, 0.7046079710210158, 0.0, 1.0, 0.2664713834131081], 
reward next is 0.7335, 
noisyNet noise sample is [array([-1.6144956], dtype=float32), 0.4715183]. 
=============================================
[2019-04-04 08:03:29,747] A3C_AGENT_WORKER-Thread-3 INFO:Local step 130000, global step 2072188: loss 1.1122
[2019-04-04 08:03:29,750] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 130000, global step 2072189: learning rate 0.0000
[2019-04-04 08:03:30,759] A3C_AGENT_WORKER-Thread-11 INFO:Local step 129500, global step 2072805: loss 0.4742
[2019-04-04 08:03:30,762] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 129500, global step 2072807: learning rate 0.0000
[2019-04-04 08:03:31,061] A3C_AGENT_WORKER-Thread-12 INFO:Local step 129500, global step 2073009: loss 0.4855
[2019-04-04 08:03:31,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 129500, global step 2073011: learning rate 0.0000
[2019-04-04 08:03:33,014] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5609831e-12 4.6908349e-11 2.6747481e-32 1.6062530e-29 1.0395531e-24
 1.0000000e+00 1.6685937e-24], sum to 1.0000
[2019-04-04 08:03:33,019] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0000
[2019-04-04 08:03:33,028] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.76666666666667, 80.0, 0.0, 0.0, 26.0, 25.86486321200488, 0.5870051415170545, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1014000.0000, 
sim time next is 1014600.0000, 
raw observation next is [14.58333333333333, 80.5, 0.0, 0.0, 26.0, 25.99987632360288, 0.5776844736780745, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8665743305632503, 0.805, 0.0, 0.0, 0.6666666666666666, 0.66665636030024, 0.6925614912260248, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.77831405], dtype=float32), 1.3276981]. 
=============================================
[2019-04-04 08:03:33,219] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6210729e-10 4.2782982e-09 2.8632742e-26 1.2029478e-25 7.2558571e-20
 1.0000000e+00 1.3319974e-20], sum to 1.0000
[2019-04-04 08:03:33,221] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0153
[2019-04-04 08:03:33,231] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.0, 59.5, 0.0, 0.0, 26.0, 25.84252355976564, 0.6696302113395323, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1108200.0000, 
sim time next is 1108800.0000, 
raw observation next is [13.8, 60.0, 0.0, 0.0, 26.0, 25.78807563138178, 0.6564299987168375, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.844875346260388, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6490063026151484, 0.7188099995722791, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.55377036], dtype=float32), -0.5904395]. 
=============================================
[2019-04-04 08:03:33,255] A3C_AGENT_WORKER-Thread-13 INFO:Local step 129500, global step 2074368: loss 0.4547
[2019-04-04 08:03:33,256] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 129500, global step 2074368: learning rate 0.0000
[2019-04-04 08:03:33,460] A3C_AGENT_WORKER-Thread-19 INFO:Local step 129500, global step 2074482: loss 0.4776
[2019-04-04 08:03:33,461] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 129500, global step 2074483: learning rate 0.0000
[2019-04-04 08:03:33,683] A3C_AGENT_WORKER-Thread-18 INFO:Local step 130000, global step 2074609: loss 1.1266
[2019-04-04 08:03:33,684] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 130000, global step 2074610: learning rate 0.0000
[2019-04-04 08:03:33,881] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.1029336e-11 8.9580215e-10 4.1472434e-29 3.7098056e-27 2.8042276e-22
 1.0000000e+00 5.3743290e-21], sum to 1.0000
[2019-04-04 08:03:33,882] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1565
[2019-04-04 08:03:33,906] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.9, 84.0, 0.0, 0.0, 26.0, 25.82975373054571, 0.6095682961377173, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1636200.0000, 
sim time next is 1636800.0000, 
raw observation next is [7.0, 83.33333333333334, 0.0, 0.0, 26.0, 25.71671463364167, 0.5972397830102607, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6565096952908588, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6430595528034724, 0.6990799276700869, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1739877], dtype=float32), 1.585358]. 
=============================================
[2019-04-04 08:03:33,932] A3C_AGENT_WORKER-Thread-10 INFO:Local step 129500, global step 2074751: loss 0.4476
[2019-04-04 08:03:33,934] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 129500, global step 2074751: learning rate 0.0000
[2019-04-04 08:03:34,908] A3C_AGENT_WORKER-Thread-6 INFO:Local step 130000, global step 2075305: loss 1.1434
[2019-04-04 08:03:34,909] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 130000, global step 2075305: learning rate 0.0000
[2019-04-04 08:03:35,087] A3C_AGENT_WORKER-Thread-16 INFO:Local step 130000, global step 2075403: loss 1.1492
[2019-04-04 08:03:35,088] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 130000, global step 2075404: learning rate 0.0000
[2019-04-04 08:03:36,397] A3C_AGENT_WORKER-Thread-14 INFO:Local step 129500, global step 2076090: loss 0.4684
[2019-04-04 08:03:36,401] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 129500, global step 2076090: learning rate 0.0000
[2019-04-04 08:03:37,783] A3C_AGENT_WORKER-Thread-5 INFO:Local step 129500, global step 2076906: loss 0.4841
[2019-04-04 08:03:37,787] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 129500, global step 2076909: learning rate 0.0000
[2019-04-04 08:03:39,347] A3C_AGENT_WORKER-Thread-17 INFO:Local step 129500, global step 2077766: loss 0.4571
[2019-04-04 08:03:39,350] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 129500, global step 2077766: learning rate 0.0000
[2019-04-04 08:03:39,800] A3C_AGENT_WORKER-Thread-4 INFO:Local step 130000, global step 2078016: loss 1.2080
[2019-04-04 08:03:39,811] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 130000, global step 2078023: learning rate 0.0000
[2019-04-04 08:03:39,861] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8538963e-11 1.7099031e-10 1.7810477e-30 1.6457264e-27 5.1713553e-23
 1.0000000e+00 1.0487196e-21], sum to 1.0000
[2019-04-04 08:03:39,861] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4755
[2019-04-04 08:03:39,875] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.0, 50.66666666666666, 78.66666666666667, 403.0000000000001, 26.0, 26.82906390138291, 0.7680219110046509, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1525200.0000, 
sim time next is 1525800.0000, 
raw observation next is [12.1, 50.33333333333334, 80.33333333333333, 328.0, 26.0, 26.90601400343636, 0.7795155220265023, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7977839335180056, 0.5033333333333334, 0.2677777777777778, 0.3624309392265193, 0.6666666666666666, 0.7421678336196967, 0.7598385073421675, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5074865], dtype=float32), -1.9730091]. 
=============================================
[2019-04-04 08:03:40,300] A3C_AGENT_WORKER-Thread-15 INFO:Local step 130000, global step 2078273: loss 1.2302
[2019-04-04 08:03:40,303] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 130000, global step 2078274: learning rate 0.0000
[2019-04-04 08:03:41,341] A3C_AGENT_WORKER-Thread-20 INFO:Local step 129500, global step 2078806: loss 0.4987
[2019-04-04 08:03:41,341] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 129500, global step 2078806: learning rate 0.0000
[2019-04-04 08:03:41,723] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7145443e-11 1.3392857e-09 1.4146348e-28 3.7207913e-26 2.3614055e-22
 1.0000000e+00 1.1225954e-21], sum to 1.0000
[2019-04-04 08:03:41,725] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4432
[2019-04-04 08:03:41,735] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.60170639255526, 0.4960277364727466, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1705200.0000, 
sim time next is 1705800.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.42191303710545, 0.471303858052252, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6184927530921209, 0.6571012860174174, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20985278], dtype=float32), -2.220564]. 
=============================================
[2019-04-04 08:03:43,311] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.9591541e-11 2.7509606e-10 4.7231656e-29 1.1909109e-27 5.4237764e-22
 1.0000000e+00 1.9342253e-21], sum to 1.0000
[2019-04-04 08:03:43,311] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2687
[2019-04-04 08:03:43,319] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 87.0, 0.0, 26.0, 25.75344642659768, 0.4639896318801581, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1423200.0000, 
sim time next is 1423800.0000, 
raw observation next is [0.0, 95.0, 90.0, 0.0, 26.0, 25.66673584671005, 0.4652817314133817, 1.0, 1.0, 31445.55695523261], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.3, 0.0, 0.6666666666666666, 0.6388946538925042, 0.6550939104711272, 1.0, 1.0, 0.14974074740586957], 
reward next is 0.8503, 
noisyNet noise sample is [array([-0.6526031], dtype=float32), -0.98075235]. 
=============================================
[2019-04-04 08:03:46,884] A3C_AGENT_WORKER-Thread-2 INFO:Local step 130500, global step 2081266: loss 0.1191
[2019-04-04 08:03:46,886] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 130500, global step 2081267: learning rate 0.0000
[2019-04-04 08:03:47,137] A3C_AGENT_WORKER-Thread-3 INFO:Local step 130500, global step 2081382: loss 0.1144
[2019-04-04 08:03:47,146] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 130500, global step 2081382: learning rate 0.0000
[2019-04-04 08:03:47,532] A3C_AGENT_WORKER-Thread-11 INFO:Local step 130000, global step 2081558: loss 1.4185
[2019-04-04 08:03:47,532] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 130000, global step 2081558: learning rate 0.0000
[2019-04-04 08:03:47,698] A3C_AGENT_WORKER-Thread-12 INFO:Local step 130000, global step 2081627: loss 1.4502
[2019-04-04 08:03:47,699] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 130000, global step 2081627: learning rate 0.0000
[2019-04-04 08:03:49,879] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0758620e-11 2.1607298e-09 2.0987494e-28 4.4837652e-27 3.2516955e-22
 1.0000000e+00 1.4959426e-21], sum to 1.0000
[2019-04-04 08:03:49,880] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0935
[2019-04-04 08:03:49,916] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 82.0, 19.0, 20.0, 26.0, 25.40751925710425, 0.4583653709218081, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1584000.0000, 
sim time next is 1584600.0000, 
raw observation next is [5.266666666666667, 81.00000000000001, 25.0, 25.0, 26.0, 25.33033801193147, 0.4803577614237481, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6084949215143122, 0.8100000000000002, 0.08333333333333333, 0.027624309392265192, 0.6666666666666666, 0.6108615009942892, 0.660119253807916, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10442788], dtype=float32), 0.5671293]. 
=============================================
[2019-04-04 08:03:49,932] A3C_AGENT_WORKER-Thread-13 INFO:Local step 130000, global step 2082499: loss 1.4720
[2019-04-04 08:03:49,935] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 130000, global step 2082499: learning rate 0.0000
[2019-04-04 08:03:50,146] A3C_AGENT_WORKER-Thread-19 INFO:Local step 130000, global step 2082610: loss 1.4843
[2019-04-04 08:03:50,147] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 130000, global step 2082610: learning rate 0.0000
[2019-04-04 08:03:50,716] A3C_AGENT_WORKER-Thread-10 INFO:Local step 130000, global step 2082902: loss 1.4743
[2019-04-04 08:03:50,719] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 130000, global step 2082904: learning rate 0.0000
[2019-04-04 08:03:51,023] A3C_AGENT_WORKER-Thread-18 INFO:Local step 130500, global step 2083055: loss 0.1089
[2019-04-04 08:03:51,025] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 130500, global step 2083056: learning rate 0.0000
[2019-04-04 08:03:51,039] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3310428e-12 5.3563276e-10 2.0748660e-30 7.0954018e-28 2.9018318e-22
 1.0000000e+00 5.2696938e-22], sum to 1.0000
[2019-04-04 08:03:51,040] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3260
[2019-04-04 08:03:51,046] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.63333333333333, 49.66666666666667, 89.16666666666666, 0.0, 26.0, 26.8036327018704, 0.793219971533099, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1610400.0000, 
sim time next is 1611000.0000, 
raw observation next is [13.55, 50.0, 78.0, 0.0, 26.0, 27.04341700353508, 0.8135681954375874, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8379501385041552, 0.5, 0.26, 0.0, 0.6666666666666666, 0.7536180836279233, 0.7711893984791959, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6217988], dtype=float32), -0.5819342]. 
=============================================
[2019-04-04 08:03:51,058] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[90.37092 ]
 [90.55656 ]
 [90.746315]
 [90.897545]
 [90.90479 ]], R is [[90.28071594]
 [90.3779068 ]
 [90.47412872]
 [90.56938934]
 [90.66369629]].
[2019-04-04 08:03:52,604] A3C_AGENT_WORKER-Thread-14 INFO:Local step 130000, global step 2083871: loss 1.5671
[2019-04-04 08:03:52,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 130000, global step 2083871: learning rate 0.0000
[2019-04-04 08:03:52,673] A3C_AGENT_WORKER-Thread-16 INFO:Local step 130500, global step 2083907: loss 0.0938
[2019-04-04 08:03:52,679] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 130500, global step 2083908: learning rate 0.0000
[2019-04-04 08:03:52,783] A3C_AGENT_WORKER-Thread-6 INFO:Local step 130500, global step 2083970: loss 0.1001
[2019-04-04 08:03:52,783] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 130500, global step 2083970: learning rate 0.0000
[2019-04-04 08:03:55,060] A3C_AGENT_WORKER-Thread-5 INFO:Local step 130000, global step 2084653: loss 1.5555
[2019-04-04 08:03:55,061] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 130000, global step 2084653: learning rate 0.0000
[2019-04-04 08:03:57,840] A3C_AGENT_WORKER-Thread-17 INFO:Local step 130000, global step 2085436: loss 1.5990
[2019-04-04 08:03:57,846] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 130000, global step 2085436: learning rate 0.0000
[2019-04-04 08:03:58,330] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9641448e-10 3.1305116e-09 6.0036328e-25 7.3838693e-23 2.5106020e-19
 1.0000000e+00 4.3869058e-19], sum to 1.0000
[2019-04-04 08:03:58,331] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8826
[2019-04-04 08:03:58,397] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 62.0, 80.33333333333334, 0.0, 26.0, 25.61369509099515, 0.3267668062394954, 1.0, 1.0, 53521.89651961753], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1956000.0000, 
sim time next is 1956600.0000, 
raw observation next is [-2.8, 62.0, 74.0, 0.0, 26.0, 25.63432904141165, 0.3237720360911077, 1.0, 1.0, 36291.59539592021], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.24666666666666667, 0.0, 0.6666666666666666, 0.6361940867843042, 0.6079240120303693, 1.0, 1.0, 0.17281712093295337], 
reward next is 0.8272, 
noisyNet noise sample is [array([0.596113], dtype=float32), -0.3101909]. 
=============================================
[2019-04-04 08:03:58,730] A3C_AGENT_WORKER-Thread-4 INFO:Local step 130500, global step 2085730: loss 0.0954
[2019-04-04 08:03:58,732] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 130500, global step 2085730: learning rate 0.0000
[2019-04-04 08:03:59,189] A3C_AGENT_WORKER-Thread-15 INFO:Local step 130500, global step 2085865: loss 0.0951
[2019-04-04 08:03:59,193] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 130500, global step 2085866: learning rate 0.0000
[2019-04-04 08:04:01,501] A3C_AGENT_WORKER-Thread-20 INFO:Local step 130000, global step 2086509: loss 1.6179
[2019-04-04 08:04:01,501] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 130000, global step 2086509: learning rate 0.0000
[2019-04-04 08:04:02,658] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.0368860e-09 5.8902131e-08 4.3041803e-23 2.2544133e-21 2.1540627e-17
 9.9999988e-01 1.5053344e-16], sum to 1.0000
[2019-04-04 08:04:02,658] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2464
[2019-04-04 08:04:02,691] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.0, 83.50000000000001, 0.0, 0.0, 26.0, 23.28357726944897, -0.1585070296034638, 0.0, 1.0, 44772.84372460052], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1923000.0000, 
sim time next is 1923600.0000, 
raw observation next is [-9.100000000000001, 85.0, 0.0, 0.0, 26.0, 23.25883988788627, -0.1686694702427577, 0.0, 1.0, 44687.21542111969], 
processed observation next is [1.0, 0.2608695652173913, 0.21052631578947364, 0.85, 0.0, 0.0, 0.6666666666666666, 0.4382366573238559, 0.4437768432524141, 0.0, 1.0, 0.21279626391009376], 
reward next is 0.7872, 
noisyNet noise sample is [array([-0.6702294], dtype=float32), 0.3127478]. 
=============================================
[2019-04-04 08:04:09,697] A3C_AGENT_WORKER-Thread-11 INFO:Local step 130500, global step 2088607: loss 0.0938
[2019-04-04 08:04:09,698] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 130500, global step 2088607: learning rate 0.0000
[2019-04-04 08:04:10,778] A3C_AGENT_WORKER-Thread-12 INFO:Local step 130500, global step 2088861: loss 0.0836
[2019-04-04 08:04:10,779] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 130500, global step 2088861: learning rate 0.0000
[2019-04-04 08:04:15,186] A3C_AGENT_WORKER-Thread-13 INFO:Local step 130500, global step 2089855: loss 0.0825
[2019-04-04 08:04:15,187] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 130500, global step 2089855: learning rate 0.0000
[2019-04-04 08:04:16,039] A3C_AGENT_WORKER-Thread-10 INFO:Local step 130500, global step 2090048: loss 0.0877
[2019-04-04 08:04:16,045] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 130500, global step 2090052: learning rate 0.0000
[2019-04-04 08:04:16,080] A3C_AGENT_WORKER-Thread-19 INFO:Local step 130500, global step 2090057: loss 0.0863
[2019-04-04 08:04:16,081] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 130500, global step 2090057: learning rate 0.0000
[2019-04-04 08:04:17,947] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1240672e-10 5.5453642e-09 6.6609196e-26 3.2203746e-24 8.2737506e-20
 1.0000000e+00 9.6921644e-20], sum to 1.0000
[2019-04-04 08:04:17,947] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5054
[2019-04-04 08:04:18,091] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 65.0, 56.0, 0.0, 26.0, 24.85987120212622, 0.3745868945247761, 1.0, 1.0, 198437.4866391685], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2131200.0000, 
sim time next is 2131800.0000, 
raw observation next is [-4.5, 65.5, 46.0, 0.0, 26.0, 25.38491741090162, 0.4326796529372829, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.655, 0.15333333333333332, 0.0, 0.6666666666666666, 0.6154097842418015, 0.6442265509790943, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03836465], dtype=float32), -0.22584356]. 
=============================================
[2019-04-04 08:04:18,557] A3C_AGENT_WORKER-Thread-2 INFO:Local step 131000, global step 2090604: loss 0.2820
[2019-04-04 08:04:18,589] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 131000, global step 2090604: learning rate 0.0000
[2019-04-04 08:04:20,036] A3C_AGENT_WORKER-Thread-3 INFO:Local step 131000, global step 2090891: loss 0.2878
[2019-04-04 08:04:20,037] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 131000, global step 2090891: learning rate 0.0000
[2019-04-04 08:04:20,544] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.9534281e-10 1.1543344e-08 5.1743103e-25 1.7291370e-23 7.3103386e-20
 1.0000000e+00 1.1843020e-18], sum to 1.0000
[2019-04-04 08:04:20,544] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2002
[2019-04-04 08:04:20,608] A3C_AGENT_WORKER-Thread-14 INFO:Local step 130500, global step 2091040: loss 0.1016
[2019-04-04 08:04:20,609] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 130500, global step 2091040: learning rate 0.0000
[2019-04-04 08:04:20,611] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.566666666666667, 85.33333333333334, 34.33333333333334, 0.0, 26.0, 24.97640432872877, 0.3331087779962963, 0.0, 1.0, 56937.79337887621], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1786800.0000, 
sim time next is 1787400.0000, 
raw observation next is [-3.65, 84.5, 28.0, 0.0, 26.0, 24.98691943447408, 0.3343324673243779, 0.0, 1.0, 46340.06372100889], 
processed observation next is [0.0, 0.6956521739130435, 0.3614958448753463, 0.845, 0.09333333333333334, 0.0, 0.6666666666666666, 0.5822432862061732, 0.6114441557747926, 0.0, 1.0, 0.22066697010004233], 
reward next is 0.7793, 
noisyNet noise sample is [array([0.62471443], dtype=float32), -0.09700406]. 
=============================================
[2019-04-04 08:04:21,464] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.55281643e-09 1.96410865e-09 9.26372981e-26 1.29069406e-23
 1.22218014e-18 1.00000000e+00 4.21045602e-18], sum to 1.0000
[2019-04-04 08:04:21,464] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6140
[2019-04-04 08:04:21,526] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 86.0, 0.0, 0.0, 26.0, 24.98085207243692, 0.3061601400114238, 0.0, 1.0, 42287.44611533136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2070000.0000, 
sim time next is 2070600.0000, 
raw observation next is [-4.5, 86.83333333333334, 0.0, 0.0, 26.0, 24.92562742282109, 0.295955352872405, 0.0, 1.0, 42398.85750750983], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.8683333333333334, 0.0, 0.0, 0.6666666666666666, 0.5771356185684242, 0.5986517842908017, 0.0, 1.0, 0.2018993214643325], 
reward next is 0.7981, 
noisyNet noise sample is [array([-1.2929757], dtype=float32), 0.19036151]. 
=============================================
[2019-04-04 08:04:22,582] A3C_AGENT_WORKER-Thread-5 INFO:Local step 130500, global step 2091497: loss 0.0948
[2019-04-04 08:04:22,606] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 130500, global step 2091497: learning rate 0.0000
[2019-04-04 08:04:24,068] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.8278086e-10 2.5748443e-08 1.8256421e-24 8.0109576e-23 4.5507977e-19
 1.0000000e+00 5.1891732e-18], sum to 1.0000
[2019-04-04 08:04:24,097] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4274
[2019-04-04 08:04:24,108] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.616666666666667, 77.5, 0.0, 0.0, 26.0, 24.17331251950632, 0.09195740478491284, 0.0, 1.0, 42113.57451120663], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2175000.0000, 
sim time next is 2175600.0000, 
raw observation next is [-6.533333333333334, 77.0, 0.0, 0.0, 26.0, 24.14886341213963, 0.08516802762487825, 0.0, 1.0, 42074.23045600582], 
processed observation next is [1.0, 0.17391304347826086, 0.28162511542012925, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5124052843449691, 0.5283893425416261, 0.0, 1.0, 0.20035347836193246], 
reward next is 0.7996, 
noisyNet noise sample is [array([1.1607417], dtype=float32), 1.7221028]. 
=============================================
[2019-04-04 08:04:25,089] A3C_AGENT_WORKER-Thread-17 INFO:Local step 130500, global step 2092103: loss 0.0927
[2019-04-04 08:04:25,090] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 130500, global step 2092103: learning rate 0.0000
[2019-04-04 08:04:25,659] A3C_AGENT_WORKER-Thread-18 INFO:Local step 131000, global step 2092238: loss 0.2858
[2019-04-04 08:04:25,660] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 131000, global step 2092238: learning rate 0.0000
[2019-04-04 08:04:27,285] A3C_AGENT_WORKER-Thread-16 INFO:Local step 131000, global step 2092568: loss 0.2902
[2019-04-04 08:04:27,285] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 131000, global step 2092568: learning rate 0.0000
[2019-04-04 08:04:29,626] A3C_AGENT_WORKER-Thread-6 INFO:Local step 131000, global step 2093119: loss 0.2751
[2019-04-04 08:04:29,626] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 131000, global step 2093119: learning rate 0.0000
[2019-04-04 08:04:32,238] A3C_AGENT_WORKER-Thread-20 INFO:Local step 130500, global step 2093620: loss 0.0912
[2019-04-04 08:04:32,239] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 130500, global step 2093620: learning rate 0.0000
[2019-04-04 08:04:34,185] A3C_AGENT_WORKER-Thread-4 INFO:Local step 131000, global step 2093964: loss 0.2384
[2019-04-04 08:04:34,188] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 131000, global step 2093964: learning rate 0.0000
[2019-04-04 08:04:35,197] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.8304176e-09 6.2169512e-08 1.3245309e-24 2.7782457e-23 1.0039620e-18
 9.9999988e-01 7.1329695e-18], sum to 1.0000
[2019-04-04 08:04:35,198] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0147
[2019-04-04 08:04:35,284] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.100000000000001, 83.0, 0.0, 0.0, 26.0, 25.05763274819361, 0.298910180342712, 0.0, 1.0, 43298.47359999262], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1980600.0000, 
sim time next is 1981200.0000, 
raw observation next is [-6.0, 83.0, 0.0, 0.0, 26.0, 25.03801453332805, 0.2826745506747796, 0.0, 1.0, 43492.41783559304], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5865012111106708, 0.5942248502249265, 0.0, 1.0, 0.2071067515980621], 
reward next is 0.7929, 
noisyNet noise sample is [array([0.05461872], dtype=float32), 2.68077]. 
=============================================
[2019-04-04 08:04:36,704] A3C_AGENT_WORKER-Thread-15 INFO:Local step 131000, global step 2094383: loss 0.2640
[2019-04-04 08:04:36,705] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 131000, global step 2094383: learning rate 0.0000
[2019-04-04 08:04:49,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5842638e-10 6.8350321e-09 2.0531707e-26 7.9503665e-25 3.9957386e-20
 1.0000000e+00 4.3767076e-20], sum to 1.0000
[2019-04-04 08:04:49,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0494
[2019-04-04 08:04:50,079] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3166666666666667, 44.66666666666666, 154.3333333333333, 63.0, 26.0, 26.05202319657091, 0.4833436670673492, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2297400.0000, 
sim time next is 2298000.0000, 
raw observation next is [-0.03333333333333333, 44.33333333333334, 137.6666666666667, 61.5, 26.0, 26.24207935206509, 0.4944745245145388, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46168051708217916, 0.4433333333333334, 0.45888888888888907, 0.06795580110497237, 0.6666666666666666, 0.6868399460054242, 0.6648248415048462, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1176627], dtype=float32), -1.0346214]. 
=============================================
[2019-04-04 08:04:50,130] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.2110505e-11 2.1164088e-09 3.3307294e-27 2.8077556e-25 2.3586351e-20
 1.0000000e+00 3.7313920e-20], sum to 1.0000
[2019-04-04 08:04:50,131] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1626
[2019-04-04 08:04:50,153] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.56971]
 [80.1439 ]
 [80.75779]
 [81.15133]
 [80.77904]], R is [[79.22177124]
 [79.4295578 ]
 [79.63526154]
 [79.83891296]
 [79.09513855]].
[2019-04-04 08:04:50,237] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.65, 63.5, 137.0, 0.0, 26.0, 25.54633656131591, 0.3306396502598146, 1.0, 1.0, 23294.03547501322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1949400.0000, 
sim time next is 1950000.0000, 
raw observation next is [-3.566666666666666, 63.0, 132.8333333333333, 0.0, 26.0, 25.66003501195884, 0.3377456549579564, 1.0, 1.0, 23106.17931630252], 
processed observation next is [1.0, 0.5652173913043478, 0.3638042474607572, 0.63, 0.4427777777777776, 0.0, 0.6666666666666666, 0.63833625099657, 0.6125818849859854, 1.0, 1.0, 0.11002942531572629], 
reward next is 0.8900, 
noisyNet noise sample is [array([-0.31059912], dtype=float32), 1.5639716]. 
=============================================
[2019-04-04 08:04:50,286] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[79.12942 ]
 [79.509415]
 [79.82787 ]
 [80.14894 ]
 [80.40648 ]], R is [[78.91099548]
 [79.01096344]
 [79.11118317]
 [79.21401978]
 [79.32060242]].
[2019-04-04 08:04:51,262] A3C_AGENT_WORKER-Thread-11 INFO:Local step 131000, global step 2096836: loss 0.2068
[2019-04-04 08:04:51,282] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 131000, global step 2096838: learning rate 0.0000
[2019-04-04 08:04:51,949] A3C_AGENT_WORKER-Thread-12 INFO:Local step 131000, global step 2096954: loss 0.2139
[2019-04-04 08:04:51,950] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 131000, global step 2096954: learning rate 0.0000
[2019-04-04 08:04:56,018] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2788837e-10 2.4608298e-09 1.5968044e-26 3.7612794e-25 8.5646305e-21
 1.0000000e+00 2.0569399e-20], sum to 1.0000
[2019-04-04 08:04:56,018] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2055
[2019-04-04 08:04:56,039] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.533333333333334, 64.0, 174.6666666666667, 128.5, 26.0, 25.87215863575882, 0.4060181565658607, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2118000.0000, 
sim time next is 2118600.0000, 
raw observation next is [-6.45, 64.0, 151.0, 134.0, 26.0, 25.74971406205852, 0.3664353086785084, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.28393351800554023, 0.64, 0.5033333333333333, 0.14806629834254142, 0.6666666666666666, 0.6458095051715432, 0.6221451028928361, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9117648], dtype=float32), 0.52889633]. 
=============================================
[2019-04-04 08:04:58,187] A3C_AGENT_WORKER-Thread-13 INFO:Local step 131000, global step 2098104: loss 0.2187
[2019-04-04 08:04:58,253] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 131000, global step 2098104: learning rate 0.0000
[2019-04-04 08:04:58,902] A3C_AGENT_WORKER-Thread-2 INFO:Local step 131500, global step 2098211: loss 0.0033
[2019-04-04 08:04:58,970] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 131500, global step 2098211: learning rate 0.0000
[2019-04-04 08:04:59,037] A3C_AGENT_WORKER-Thread-10 INFO:Local step 131000, global step 2098233: loss 0.1872
[2019-04-04 08:04:59,069] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 131000, global step 2098233: learning rate 0.0000
[2019-04-04 08:04:59,702] A3C_AGENT_WORKER-Thread-19 INFO:Local step 131000, global step 2098363: loss 0.1785
[2019-04-04 08:04:59,706] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 131000, global step 2098363: learning rate 0.0000
[2019-04-04 08:05:02,231] A3C_AGENT_WORKER-Thread-3 INFO:Local step 131500, global step 2098892: loss 0.0023
[2019-04-04 08:05:02,232] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 131500, global step 2098892: learning rate 0.0000
[2019-04-04 08:05:04,635] A3C_AGENT_WORKER-Thread-14 INFO:Local step 131000, global step 2099282: loss 0.1880
[2019-04-04 08:05:04,636] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 131000, global step 2099282: learning rate 0.0000
[2019-04-04 08:05:07,170] A3C_AGENT_WORKER-Thread-5 INFO:Local step 131000, global step 2099602: loss 0.1890
[2019-04-04 08:05:07,174] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 131000, global step 2099602: learning rate 0.0000
[2019-04-04 08:05:08,088] A3C_AGENT_WORKER-Thread-18 INFO:Local step 131500, global step 2099738: loss 0.0029
[2019-04-04 08:05:08,112] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 131500, global step 2099738: learning rate 0.0000
[2019-04-04 08:05:09,876] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 08:05:09,876] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:05:09,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:05:09,879] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run22
[2019-04-04 08:05:09,904] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:05:09,905] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:05:09,907] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run22
[2019-04-04 08:05:09,946] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:05:09,962] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:05:09,964] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run22
[2019-04-04 08:07:25,795] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.24619567], dtype=float32), 0.25732014]
[2019-04-04 08:07:25,795] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.5, 43.66666666666667, 198.6666666666667, 157.0, 26.0, 26.47387584717731, 0.5462755814741546, 1.0, 1.0, 0.0]
[2019-04-04 08:07:25,795] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 08:07:25,796] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.7002248e-11 2.0657585e-09 5.0596891e-27 3.0516759e-25 2.1512842e-20
 1.0000000e+00 4.4219179e-20], sampled 0.9266083201329725
[2019-04-04 08:07:25,932] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.24619567], dtype=float32), 0.25732014]
[2019-04-04 08:07:25,932] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [6.033333333333334, 40.66666666666667, 165.3333333333333, 496.8333333333333, 26.0, 24.89692057285992, 0.275681651045915, 0.0, 1.0, 0.0]
[2019-04-04 08:07:25,932] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:07:25,933] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.7776380e-10 6.6561507e-09 2.2644835e-26 1.4800413e-24 9.8286964e-20
 1.0000000e+00 2.3758207e-19], sampled 0.7972353255365882
[2019-04-04 08:07:26,194] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.24619567], dtype=float32), 0.25732014]
[2019-04-04 08:07:26,196] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.788084481, 46.75614401, 177.32798935, 168.3905163, 26.0, 25.426929113447, 0.3262634214508113, 1.0, 1.0, 0.0]
[2019-04-04 08:07:26,196] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 08:07:26,197] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.13216617e-10 3.72318754e-09 3.12777494e-26 1.36745232e-24
 1.07804724e-19 1.00000000e+00 1.95125018e-19], sampled 0.8001317425822327
[2019-04-04 08:07:54,642] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.24619567], dtype=float32), 0.25732014]
[2019-04-04 08:07:54,642] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.333333333333333, 73.0, 63.33333333333334, 540.1666666666667, 26.0, 27.08709816110531, 0.5242184521403552, 1.0, 1.0, 0.0]
[2019-04-04 08:07:54,642] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 08:07:54,643] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.1051381e-11 2.7901970e-10 8.6821957e-29 6.9203670e-27 1.4096247e-21
 1.0000000e+00 3.2272048e-21], sampled 0.0771620260142728
[2019-04-04 08:08:23,693] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 08:08:56,954] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 08:09:00,096] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6651 275800324.5281 1233.1507
[2019-04-04 08:09:01,130] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2100000, evaluation results [2100000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.665121294629, 275800324.5281271, 1233.1506611100626]
[2019-04-04 08:09:02,114] A3C_AGENT_WORKER-Thread-16 INFO:Local step 131500, global step 2100221: loss 0.0013
[2019-04-04 08:09:02,115] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 131500, global step 2100221: learning rate 0.0000
[2019-04-04 08:09:03,050] A3C_AGENT_WORKER-Thread-17 INFO:Local step 131000, global step 2100443: loss 0.1748
[2019-04-04 08:09:03,050] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 131000, global step 2100443: learning rate 0.0000
[2019-04-04 08:09:05,490] A3C_AGENT_WORKER-Thread-6 INFO:Local step 131500, global step 2100988: loss 0.0007
[2019-04-04 08:09:05,490] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 131500, global step 2100988: learning rate 0.0000
[2019-04-04 08:09:06,082] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1343358e-11 3.4103647e-09 4.4215646e-27 2.9025678e-26 1.2678676e-20
 1.0000000e+00 2.6011284e-20], sum to 1.0000
[2019-04-04 08:09:06,083] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6527
[2019-04-04 08:09:06,218] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.15, 48.0, 221.0, 69.0, 26.0, 25.02726676135889, 0.3269678040444552, 1.0, 1.0, 119782.7184146446], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2295000.0000, 
sim time next is 2295600.0000, 
raw observation next is [-0.9666666666666668, 47.0, 204.3333333333333, 67.5, 26.0, 24.69597617636885, 0.3475088337999195, 1.0, 1.0, 198531.0323868092], 
processed observation next is [1.0, 0.5652173913043478, 0.43582640812557716, 0.47, 0.681111111111111, 0.07458563535911603, 0.6666666666666666, 0.5579980146974041, 0.6158362779333065, 1.0, 1.0, 0.9453858685086153], 
reward next is 0.0546, 
noisyNet noise sample is [array([-0.5254683], dtype=float32), -1.0839702]. 
=============================================
[2019-04-04 08:09:09,013] A3C_AGENT_WORKER-Thread-4 INFO:Local step 131500, global step 2101775: loss 0.0007
[2019-04-04 08:09:09,050] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 131500, global step 2101775: learning rate 0.0000
[2019-04-04 08:09:09,101] A3C_AGENT_WORKER-Thread-20 INFO:Local step 131000, global step 2101794: loss 0.1649
[2019-04-04 08:09:09,108] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 131000, global step 2101796: learning rate 0.0000
[2019-04-04 08:09:11,716] A3C_AGENT_WORKER-Thread-15 INFO:Local step 131500, global step 2102365: loss 0.0002
[2019-04-04 08:09:11,718] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 131500, global step 2102365: learning rate 0.0000
[2019-04-04 08:09:18,241] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2436089e-09 4.9573714e-08 4.6003915e-24 9.2851484e-23 1.9405026e-18
 1.0000000e+00 5.6614454e-18], sum to 1.0000
[2019-04-04 08:09:18,241] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5473
[2019-04-04 08:09:18,280] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.64195419534897, -0.02871173717167374, 0.0, 1.0, 43241.56119282253], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2266800.0000, 
sim time next is 2267400.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.58457900610213, -0.03019884183710195, 0.0, 1.0, 43230.28110860802], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.465381583841844, 0.4899337193876327, 0.0, 1.0, 0.20585848146956198], 
reward next is 0.7941, 
noisyNet noise sample is [array([-1.06265], dtype=float32), 0.9790693]. 
=============================================
[2019-04-04 08:09:20,516] A3C_AGENT_WORKER-Thread-12 INFO:Local step 131500, global step 2104863: loss 0.0001
[2019-04-04 08:09:20,519] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 131500, global step 2104865: learning rate 0.0000
[2019-04-04 08:09:20,828] A3C_AGENT_WORKER-Thread-11 INFO:Local step 131500, global step 2104967: loss 0.0001
[2019-04-04 08:09:20,835] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 131500, global step 2104967: learning rate 0.0000
[2019-04-04 08:09:20,908] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9179652e-10 2.7540281e-09 3.3241181e-26 6.0057174e-25 4.9680749e-20
 1.0000000e+00 1.3391665e-19], sum to 1.0000
[2019-04-04 08:09:20,908] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5552
[2019-04-04 08:09:20,925] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.9524248e-11 2.0463622e-09 7.2993338e-27 1.8784627e-25 1.7764835e-20
 1.0000000e+00 1.8189723e-20], sum to 1.0000
[2019-04-04 08:09:20,928] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.683333333333334, 69.0, 0.0, 0.0, 26.0, 25.04123623029878, 0.3326039349587325, 0.0, 1.0, 45142.65919293525], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2674200.0000, 
sim time next is 2674800.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 26.0, 24.98012076868509, 0.3210031375292484, 0.0, 1.0, 44648.11770223535], 
processed observation next is [1.0, 1.0, 0.32409972299168976, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5816767307237575, 0.6070010458430828, 0.0, 1.0, 0.2126100842963588], 
reward next is 0.7874, 
noisyNet noise sample is [array([0.82887125], dtype=float32), -1.1932603]. 
=============================================
[2019-04-04 08:09:20,928] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3370
[2019-04-04 08:09:20,982] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 55.33333333333333, 65.16666666666666, 20.5, 26.0, 25.5244578045705, 0.256093151469647, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2536800.0000, 
sim time next is 2537400.0000, 
raw observation next is [-2.8, 55.66666666666667, 79.33333333333333, 23.0, 26.0, 25.60794135593115, 0.260165389044632, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.38504155124653744, 0.5566666666666668, 0.2644444444444444, 0.02541436464088398, 0.6666666666666666, 0.6339951129942625, 0.5867217963482106, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2085232], dtype=float32), 1.9022737]. 
=============================================
[2019-04-04 08:09:24,265] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132000, global step 2106021: loss 0.0244
[2019-04-04 08:09:24,266] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132000, global step 2106021: learning rate 0.0000
[2019-04-04 08:09:24,365] A3C_AGENT_WORKER-Thread-10 INFO:Local step 131500, global step 2106055: loss 0.0001
[2019-04-04 08:09:24,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 131500, global step 2106055: learning rate 0.0000
[2019-04-04 08:09:24,676] A3C_AGENT_WORKER-Thread-13 INFO:Local step 131500, global step 2106170: loss 0.0001
[2019-04-04 08:09:24,683] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 131500, global step 2106170: learning rate 0.0000
[2019-04-04 08:09:26,254] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132000, global step 2106682: loss 0.0192
[2019-04-04 08:09:26,256] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132000, global step 2106682: learning rate 0.0000
[2019-04-04 08:09:26,616] A3C_AGENT_WORKER-Thread-19 INFO:Local step 131500, global step 2106799: loss 0.0001
[2019-04-04 08:09:26,618] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 131500, global step 2106799: learning rate 0.0000
[2019-04-04 08:09:28,939] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132000, global step 2107554: loss 0.0187
[2019-04-04 08:09:28,940] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132000, global step 2107554: learning rate 0.0000
[2019-04-04 08:09:28,963] A3C_AGENT_WORKER-Thread-14 INFO:Local step 131500, global step 2107559: loss 0.0001
[2019-04-04 08:09:28,965] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 131500, global step 2107563: learning rate 0.0000
[2019-04-04 08:09:29,034] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1725071e-09 1.6761648e-07 4.3528903e-23 2.2513195e-21 6.4628144e-17
 9.9999988e-01 1.0088387e-16], sum to 1.0000
[2019-04-04 08:09:29,034] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2619
[2019-04-04 08:09:29,056] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.35788633067902, -0.1141751885564759, 0.0, 1.0, 44379.89674003916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2437200.0000, 
sim time next is 2437800.0000, 
raw observation next is [-8.483333333333334, 60.83333333333334, 0.0, 0.0, 26.0, 23.34730426499294, -0.1210125403191497, 0.0, 1.0, 44337.34294783675], 
processed observation next is [0.0, 0.21739130434782608, 0.2276084949215143, 0.6083333333333334, 0.0, 0.0, 0.6666666666666666, 0.44560868874941156, 0.4596624865602834, 0.0, 1.0, 0.2111302045135083], 
reward next is 0.7889, 
noisyNet noise sample is [array([0.56135726], dtype=float32), -0.5921435]. 
=============================================
[2019-04-04 08:09:30,059] A3C_AGENT_WORKER-Thread-5 INFO:Local step 131500, global step 2107984: loss 0.0002
[2019-04-04 08:09:30,063] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 131500, global step 2107985: learning rate 0.0000
[2019-04-04 08:09:30,397] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5115862e-09 3.2096022e-08 2.0409586e-25 3.2532681e-24 3.6507311e-19
 1.0000000e+00 9.6097036e-19], sum to 1.0000
[2019-04-04 08:09:30,400] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9774
[2019-04-04 08:09:30,482] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 69.0, 51.0, 59.99999999999999, 26.0, 24.87427359912463, 0.2868960890975108, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2364000.0000, 
sim time next is 2364600.0000, 
raw observation next is [-3.4, 69.0, 64.99999999999999, 120.0, 26.0, 25.20015590421547, 0.314459814883175, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.21666666666666662, 0.13259668508287292, 0.6666666666666666, 0.6000129920179559, 0.6048199382943916, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7619824], dtype=float32), -0.49505815]. 
=============================================
[2019-04-04 08:09:30,660] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132000, global step 2108205: loss 0.0157
[2019-04-04 08:09:30,660] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132000, global step 2108205: learning rate 0.0000
[2019-04-04 08:09:31,792] A3C_AGENT_WORKER-Thread-6 INFO:Local step 132000, global step 2108586: loss 0.0179
[2019-04-04 08:09:31,793] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 132000, global step 2108586: learning rate 0.0000
[2019-04-04 08:09:32,562] A3C_AGENT_WORKER-Thread-17 INFO:Local step 131500, global step 2108871: loss 0.0001
[2019-04-04 08:09:32,564] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 131500, global step 2108871: learning rate 0.0000
[2019-04-04 08:09:34,439] A3C_AGENT_WORKER-Thread-4 INFO:Local step 132000, global step 2109546: loss 0.0102
[2019-04-04 08:09:34,440] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 132000, global step 2109546: learning rate 0.0000
[2019-04-04 08:09:36,429] A3C_AGENT_WORKER-Thread-20 INFO:Local step 131500, global step 2110256: loss 0.0006
[2019-04-04 08:09:36,430] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 131500, global step 2110256: learning rate 0.0000
[2019-04-04 08:09:37,000] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132000, global step 2110476: loss 0.0106
[2019-04-04 08:09:37,001] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132000, global step 2110476: learning rate 0.0000
[2019-04-04 08:09:43,936] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132000, global step 2112978: loss 0.0080
[2019-04-04 08:09:43,941] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132000, global step 2112978: learning rate 0.0000
[2019-04-04 08:09:44,293] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132000, global step 2113097: loss 0.0056
[2019-04-04 08:09:44,294] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132000, global step 2113098: learning rate 0.0000
[2019-04-04 08:09:46,529] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2113810: loss 0.0261
[2019-04-04 08:09:46,559] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2113811: learning rate 0.0000
[2019-04-04 08:09:46,927] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7747245e-10 2.8433802e-09 4.2735122e-27 1.2239161e-25 3.2881092e-20
 1.0000000e+00 6.9593342e-20], sum to 1.0000
[2019-04-04 08:09:46,927] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4750
[2019-04-04 08:09:46,986] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 75.0, 85.0, 45.5, 26.0, 25.89373479293604, 0.3563303658571443, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2624400.0000, 
sim time next is 2625000.0000, 
raw observation next is [-6.416666666666667, 73.33333333333333, 87.66666666666667, 60.66666666666667, 26.0, 25.8365895151169, 0.3531739161155074, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2848568790397045, 0.7333333333333333, 0.2922222222222222, 0.06703499079189687, 0.6666666666666666, 0.6530491262597415, 0.6177246387051691, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38983744], dtype=float32), -1.1406583]. 
=============================================
[2019-04-04 08:09:46,996] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[84.62093]
 [84.47983]
 [84.26234]
 [83.8946 ]
 [83.38128]], R is [[84.77072144]
 [84.92301178]
 [85.07378387]
 [85.22304535]
 [85.37081909]].
[2019-04-04 08:09:47,133] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132000, global step 2114022: loss 0.0098
[2019-04-04 08:09:47,133] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132000, global step 2114022: learning rate 0.0000
[2019-04-04 08:09:47,736] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132000, global step 2114246: loss 0.0100
[2019-04-04 08:09:47,737] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132000, global step 2114246: learning rate 0.0000
[2019-04-04 08:09:48,398] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132000, global step 2114466: loss 0.0151
[2019-04-04 08:09:48,398] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132000, global step 2114466: learning rate 0.0000
[2019-04-04 08:09:48,524] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5093260e-11 9.2646535e-10 5.6359222e-29 1.1508718e-27 3.0915774e-22
 1.0000000e+00 1.5127855e-21], sum to 1.0000
[2019-04-04 08:09:48,524] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7654
[2019-04-04 08:09:48,548] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 100.0, 63.5, 0.0, 26.0, 25.43294473398656, 0.3046065288976483, 1.0, 1.0, 18680.86315755842], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2887200.0000, 
sim time next is 2887800.0000, 
raw observation next is [0.1666666666666667, 98.83333333333334, 68.33333333333334, 0.0, 26.0, 25.41595527947072, 0.3061076997887729, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.4672206832871654, 0.9883333333333334, 0.2277777777777778, 0.0, 0.6666666666666666, 0.6179962732892266, 0.602035899929591, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.17469501], dtype=float32), -1.064337]. 
=============================================
[2019-04-04 08:09:48,766] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2114589: loss 0.0238
[2019-04-04 08:09:48,768] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2114589: learning rate 0.0000
[2019-04-04 08:09:51,311] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2115425: loss 0.0188
[2019-04-04 08:09:51,312] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2115425: learning rate 0.0000
[2019-04-04 08:09:52,428] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132000, global step 2115792: loss 0.0138
[2019-04-04 08:09:52,448] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132000, global step 2115792: learning rate 0.0000
[2019-04-04 08:09:52,452] A3C_AGENT_WORKER-Thread-5 INFO:Local step 132000, global step 2115802: loss 0.0130
[2019-04-04 08:09:52,461] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 132000, global step 2115802: learning rate 0.0000
[2019-04-04 08:09:53,983] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2116307: loss 0.0183
[2019-04-04 08:09:53,983] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2116307: learning rate 0.0000
[2019-04-04 08:09:54,978] A3C_AGENT_WORKER-Thread-6 INFO:Local step 132500, global step 2116685: loss 0.0182
[2019-04-04 08:09:54,981] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 132500, global step 2116686: learning rate 0.0000
[2019-04-04 08:09:55,689] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132000, global step 2116975: loss 0.0123
[2019-04-04 08:09:55,690] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132000, global step 2116975: learning rate 0.0000
[2019-04-04 08:09:57,239] A3C_AGENT_WORKER-Thread-4 INFO:Local step 132500, global step 2117519: loss 0.0155
[2019-04-04 08:09:57,242] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 132500, global step 2117519: learning rate 0.0000
[2019-04-04 08:09:59,504] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1545166e-09 4.3856424e-08 1.5102922e-24 2.2296855e-23 1.7163593e-18
 1.0000000e+00 2.0098295e-18], sum to 1.0000
[2019-04-04 08:09:59,504] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8050
[2019-04-04 08:09:59,520] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.09896658900318, 0.02217592947156554, 0.0, 1.0, 56215.32361038332], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2788200.0000, 
sim time next is 2788800.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 23.93934202041516, 0.005971386234299866, 0.0, 1.0, 60583.22641948125], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.49494516836793007, 0.5019904620780999, 0.0, 1.0, 0.2884915543784822], 
reward next is 0.7115, 
noisyNet noise sample is [array([-0.52544343], dtype=float32), 0.26533362]. 
=============================================
[2019-04-04 08:09:59,657] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132000, global step 2118477: loss 0.0173
[2019-04-04 08:09:59,672] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132000, global step 2118479: learning rate 0.0000
[2019-04-04 08:09:59,900] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2118570: loss 0.0125
[2019-04-04 08:09:59,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2118570: learning rate 0.0000
[2019-04-04 08:10:00,502] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4918701e-10 1.3338963e-09 3.2274657e-27 3.2731694e-26 2.7754735e-21
 1.0000000e+00 9.9368133e-21], sum to 1.0000
[2019-04-04 08:10:00,502] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5690
[2019-04-04 08:10:00,579] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 0.0, 0.0, 26.0, 25.08054898332288, 0.4437333789208893, 0.0, 1.0, 41116.9311549214], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3222600.0000, 
sim time next is 3223200.0000, 
raw observation next is [-3.0, 92.0, 0.0, 0.0, 26.0, 25.13807498244271, 0.5060226536471876, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5948395818702258, 0.6686742178823959, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.57846284], dtype=float32), 0.7167676]. 
=============================================
[2019-04-04 08:10:02,599] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.3975359e-10 2.8099391e-08 1.6933386e-24 1.2370703e-22 5.4094477e-18
 1.0000000e+00 1.6082915e-18], sum to 1.0000
[2019-04-04 08:10:02,600] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6679
[2019-04-04 08:10:02,616] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.46090830261587, 0.1527962030780163, 0.0, 1.0, 40753.45255464116], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2779200.0000, 
sim time next is 2779800.0000, 
raw observation next is [-6.166666666666666, 59.83333333333334, 0.0, 0.0, 26.0, 24.44305567702954, 0.1547926448357638, 0.0, 1.0, 40765.96127458049], 
processed observation next is [1.0, 0.17391304347826086, 0.29178208679593726, 0.5983333333333334, 0.0, 0.0, 0.6666666666666666, 0.5369213064191284, 0.5515975482785879, 0.0, 1.0, 0.19412362511704995], 
reward next is 0.8059, 
noisyNet noise sample is [array([0.956818], dtype=float32), -0.3642917]. 
=============================================
[2019-04-04 08:10:03,674] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9825777e-09 7.2620723e-09 9.2736651e-26 1.4861352e-23 6.5434065e-19
 1.0000000e+00 3.9039986e-19], sum to 1.0000
[2019-04-04 08:10:03,675] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9575
[2019-04-04 08:10:03,698] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.89583828553497, 0.3187500087092634, 0.0, 1.0, 43869.76460736858], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3293400.0000, 
sim time next is 3294000.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.83977598718884, 0.3093039836941421, 0.0, 1.0, 43908.25010039363], 
processed observation next is [1.0, 0.13043478260869565, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5699813322657367, 0.6031013278980474, 0.0, 1.0, 0.20908690523996964], 
reward next is 0.7909, 
noisyNet noise sample is [array([-0.4410407], dtype=float32), -1.0617114]. 
=============================================
[2019-04-04 08:10:03,707] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.22557]
 [78.47838]
 [78.69157]
 [78.77964]
 [79.14529]], R is [[78.09264374]
 [78.10282135]
 [78.11306763]
 [78.12321472]
 [78.133255  ]].
[2019-04-04 08:10:05,801] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132500, global step 2120902: loss 0.0054
[2019-04-04 08:10:05,810] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132500, global step 2120904: learning rate 0.0000
[2019-04-04 08:10:05,825] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6804123e-09 8.9234087e-09 1.2256875e-24 4.3435524e-23 5.8601936e-19
 1.0000000e+00 7.9474835e-18], sum to 1.0000
[2019-04-04 08:10:05,825] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0078
[2019-04-04 08:10:05,843] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.03708630966322, 0.291468683116379, 0.0, 1.0, 38372.66261427897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3019800.0000, 
sim time next is 3020400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.99787872516501, 0.2838739160015498, 0.0, 1.0, 38296.97878564578], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5831565604304174, 0.5946246386671833, 0.0, 1.0, 0.18236656564593226], 
reward next is 0.8176, 
noisyNet noise sample is [array([-0.6229645], dtype=float32), -0.060653526]. 
=============================================
[2019-04-04 08:10:06,603] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6014479e-11 4.3836077e-09 5.3144361e-27 7.6903718e-25 4.8355242e-21
 1.0000000e+00 7.1945869e-20], sum to 1.0000
[2019-04-04 08:10:06,605] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3240
[2019-04-04 08:10:06,639] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 104.0, 767.3333333333334, 26.0, 25.11105607057395, 0.4129695866293375, 0.0, 1.0, 18712.95613905228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2988600.0000, 
sim time next is 2989200.0000, 
raw observation next is [-2.0, 60.00000000000001, 102.5, 759.1666666666667, 26.0, 25.1486207520684, 0.4187886686817798, 0.0, 1.0, 18711.19869482028], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6000000000000001, 0.3416666666666667, 0.8388581952117865, 0.6666666666666666, 0.5957183960057, 0.6395962228939266, 0.0, 1.0, 0.08910094616581085], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.4150471], dtype=float32), -0.07848621]. 
=============================================
[2019-04-04 08:10:06,697] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132500, global step 2121225: loss 0.0044
[2019-04-04 08:10:06,697] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132500, global step 2121225: learning rate 0.0000
[2019-04-04 08:10:06,940] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2121305: loss 0.0527
[2019-04-04 08:10:06,943] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2121305: learning rate 0.0000
[2019-04-04 08:10:07,304] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.5569246e-10 1.7839363e-09 3.6154608e-26 7.4673925e-26 5.2299069e-20
 1.0000000e+00 1.0255884e-19], sum to 1.0000
[2019-04-04 08:10:07,304] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6323
[2019-04-04 08:10:07,324] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 48.0, 60.0, 501.0, 26.0, 26.65521768856553, 0.6754748042776946, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3342600.0000, 
sim time next is 3343200.0000, 
raw observation next is [-2.0, 48.66666666666666, 51.83333333333334, 439.6666666666667, 26.0, 26.6025763045729, 0.5200568680441534, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.4866666666666666, 0.1727777777777778, 0.48581952117863725, 0.6666666666666666, 0.7168813587144083, 0.6733522893480511, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2082174], dtype=float32), -0.7817238]. 
=============================================
[2019-04-04 08:10:08,669] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2122000: loss 0.0599
[2019-04-04 08:10:08,671] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2122000: learning rate 0.0000
[2019-04-04 08:10:09,848] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2122431: loss 0.0021
[2019-04-04 08:10:09,848] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2122431: learning rate 0.0000
[2019-04-04 08:10:10,353] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132500, global step 2122659: loss 0.0025
[2019-04-04 08:10:10,355] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132500, global step 2122660: learning rate 0.0000
[2019-04-04 08:10:10,693] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2122819: loss 0.0570
[2019-04-04 08:10:10,694] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2122819: learning rate 0.0000
[2019-04-04 08:10:10,831] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2122882: loss 0.0029
[2019-04-04 08:10:10,833] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2122882: learning rate 0.0000
[2019-04-04 08:10:12,990] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3631127e-11 9.0971464e-10 2.2013254e-28 1.2492077e-26 4.5996589e-22
 1.0000000e+00 1.3844347e-20], sum to 1.0000
[2019-04-04 08:10:12,990] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9912
[2019-04-04 08:10:13,058] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 80.33333333333333, 0.0, 0.0, 26.0, 24.88654047116556, 0.3927068952797815, 1.0, 1.0, 101905.7813019534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2922600.0000, 
sim time next is 2923200.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 24.91735521974622, 0.4025858968592665, 0.0, 1.0, 49588.62481319209], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.576446268312185, 0.6341952989530889, 0.0, 1.0, 0.23613630863424803], 
reward next is 0.7639, 
noisyNet noise sample is [array([-0.24494214], dtype=float32), 0.29009518]. 
=============================================
[2019-04-04 08:10:13,534] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2123802: loss 0.0564
[2019-04-04 08:10:13,535] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2123802: learning rate 0.0000
[2019-04-04 08:10:14,065] A3C_AGENT_WORKER-Thread-6 INFO:Local step 133000, global step 2124038: loss 0.0596
[2019-04-04 08:10:14,067] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 133000, global step 2124038: learning rate 0.0000
[2019-04-04 08:10:14,830] A3C_AGENT_WORKER-Thread-5 INFO:Local step 132500, global step 2124361: loss 0.0031
[2019-04-04 08:10:14,830] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 132500, global step 2124361: learning rate 0.0000
[2019-04-04 08:10:14,951] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2124411: loss 0.0028
[2019-04-04 08:10:14,952] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2124411: learning rate 0.0000
[2019-04-04 08:10:16,144] A3C_AGENT_WORKER-Thread-4 INFO:Local step 133000, global step 2124954: loss 0.0620
[2019-04-04 08:10:16,145] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 133000, global step 2124954: learning rate 0.0000
[2019-04-04 08:10:17,180] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.6307794e-09 5.3714132e-08 5.4231915e-24 2.1419060e-22 9.9872952e-18
 1.0000000e+00 5.8852424e-18], sum to 1.0000
[2019-04-04 08:10:17,180] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8457
[2019-04-04 08:10:17,196] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.84008398536729, 0.246376479061709, 0.0, 1.0, 37931.16879212891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3024000.0000, 
sim time next is 3024600.0000, 
raw observation next is [-4.166666666666667, 66.0, 0.0, 0.0, 26.0, 24.80523955170657, 0.2405401219831908, 0.0, 1.0, 37891.24811552476], 
processed observation next is [0.0, 0.0, 0.3471837488457987, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5671032959755475, 0.5801800406610637, 0.0, 1.0, 0.18043451483583217], 
reward next is 0.8196, 
noisyNet noise sample is [array([-0.73215854], dtype=float32), -0.8085856]. 
=============================================
[2019-04-04 08:10:17,710] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2125584: loss 0.0022
[2019-04-04 08:10:17,713] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2125584: learning rate 0.0000
[2019-04-04 08:10:18,824] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2126068: loss 0.0565
[2019-04-04 08:10:18,827] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2126069: learning rate 0.0000
[2019-04-04 08:10:18,904] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.63621512e-14 1.25243566e-11 3.09863250e-33 1.30605251e-31
 2.05626776e-25 1.00000000e+00 6.07145958e-25], sum to 1.0000
[2019-04-04 08:10:18,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2841
[2019-04-04 08:10:18,926] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 102.8333333333333, 770.1666666666667, 26.0, 27.66588305701444, 0.9324860875931615, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3162000.0000, 
sim time next is 3162600.0000, 
raw observation next is [7.0, 100.0, 101.0, 763.0, 26.0, 27.61473878401948, 0.9382280805794202, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6565096952908588, 1.0, 0.33666666666666667, 0.8430939226519337, 0.6666666666666666, 0.8012282320016233, 0.8127426935264733, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31111625], dtype=float32), 1.259669]. 
=============================================
[2019-04-04 08:10:21,179] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2127174: loss 0.0018
[2019-04-04 08:10:21,180] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2127174: learning rate 0.0000
[2019-04-04 08:10:23,292] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5769974e-11 1.8915471e-09 1.8615686e-27 5.2435751e-26 5.3875687e-21
 1.0000000e+00 1.3042326e-20], sum to 1.0000
[2019-04-04 08:10:23,296] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6052
[2019-04-04 08:10:23,337] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.666666666666668, 74.66666666666667, 101.0, 578.5, 26.0, 26.19777913589209, 0.5294671723399546, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3316800.0000, 
sim time next is 3317400.0000, 
raw observation next is [-8.5, 73.5, 104.0, 615.0, 26.0, 26.20990925717398, 0.534446124346477, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.22714681440443216, 0.735, 0.3466666666666667, 0.6795580110497238, 0.6666666666666666, 0.6841591047644983, 0.6781487081154923, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9562489], dtype=float32), -0.028385872]. 
=============================================
[2019-04-04 08:10:24,381] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133000, global step 2128584: loss 0.0671
[2019-04-04 08:10:24,385] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133000, global step 2128584: learning rate 0.0000
[2019-04-04 08:10:25,370] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2129043: loss 1.2763
[2019-04-04 08:10:25,373] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2129043: learning rate 0.0000
[2019-04-04 08:10:26,050] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133000, global step 2129348: loss 0.0663
[2019-04-04 08:10:26,052] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133000, global step 2129348: learning rate 0.0000
[2019-04-04 08:10:27,041] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2129830: loss 1.3208
[2019-04-04 08:10:27,043] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2129831: learning rate 0.0000
[2019-04-04 08:10:28,426] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2130413: loss 0.0659
[2019-04-04 08:10:28,429] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2130414: learning rate 0.0000
[2019-04-04 08:10:28,995] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133000, global step 2130681: loss 0.0654
[2019-04-04 08:10:29,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133000, global step 2130681: learning rate 0.0000
[2019-04-04 08:10:29,239] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2130804: loss 1.2846
[2019-04-04 08:10:29,250] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2130809: learning rate 0.0000
[2019-04-04 08:10:29,423] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2130899: loss 0.0635
[2019-04-04 08:10:29,424] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2130899: learning rate 0.0000
[2019-04-04 08:10:29,913] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9015174e-10 4.2494435e-09 7.6614381e-26 1.9211331e-24 1.6290063e-19
 1.0000000e+00 2.1063706e-19], sum to 1.0000
[2019-04-04 08:10:29,916] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2584
[2019-04-04 08:10:29,951] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.02973056791074, 0.467006693162936, 0.0, 1.0, 64979.19150273594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3528600.0000, 
sim time next is 3529200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.0581050288389, 0.4840054750723544, 0.0, 1.0, 198624.8002762148], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5881754190699082, 0.6613351583574515, 0.0, 1.0, 0.9458323822676896], 
reward next is 0.0542, 
noisyNet noise sample is [array([-1.4528843], dtype=float32), -1.1362612]. 
=============================================
[2019-04-04 08:10:31,556] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2131885: loss 1.2969
[2019-04-04 08:10:31,560] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2131886: learning rate 0.0000
[2019-04-04 08:10:32,065] A3C_AGENT_WORKER-Thread-6 INFO:Local step 133500, global step 2132119: loss 1.2985
[2019-04-04 08:10:32,070] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 133500, global step 2132123: learning rate 0.0000
[2019-04-04 08:10:32,729] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2132425: loss 0.0679
[2019-04-04 08:10:32,731] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2132425: learning rate 0.0000
[2019-04-04 08:10:33,207] A3C_AGENT_WORKER-Thread-5 INFO:Local step 133000, global step 2132645: loss 0.0536
[2019-04-04 08:10:33,208] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 133000, global step 2132646: learning rate 0.0000
[2019-04-04 08:10:33,465] A3C_AGENT_WORKER-Thread-4 INFO:Local step 133500, global step 2132764: loss 1.2649
[2019-04-04 08:10:33,477] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 133500, global step 2132765: learning rate 0.0000
[2019-04-04 08:10:36,201] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2134080: loss 0.0551
[2019-04-04 08:10:36,205] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2134080: learning rate 0.0000
[2019-04-04 08:10:36,396] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2134180: loss 1.2682
[2019-04-04 08:10:36,396] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2134180: learning rate 0.0000
[2019-04-04 08:10:39,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3051168e-10 7.4222406e-10 8.6862416e-28 1.5034962e-26 5.3919893e-21
 1.0000000e+00 1.1039782e-20], sum to 1.0000
[2019-04-04 08:10:39,315] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6594
[2019-04-04 08:10:39,331] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 70.66666666666666, 579.0, 26.0, 26.82087341356267, 0.7548478174317914, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3514200.0000, 
sim time next is 3514800.0000, 
raw observation next is [3.0, 49.0, 66.33333333333334, 552.0, 26.0, 26.97145212100335, 0.518959959777768, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.22111111111111115, 0.6099447513812155, 0.6666666666666666, 0.7476210100836124, 0.672986653259256, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27117166], dtype=float32), 1.5409582]. 
=============================================
[2019-04-04 08:10:39,573] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2135653: loss 0.0504
[2019-04-04 08:10:39,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2135653: learning rate 0.0000
[2019-04-04 08:10:41,210] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7773903e-10 1.0050530e-09 9.7075228e-27 3.2519990e-26 2.0401776e-20
 1.0000000e+00 2.9915514e-20], sum to 1.0000
[2019-04-04 08:10:41,215] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8677
[2019-04-04 08:10:41,230] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.666666666666667, 43.33333333333334, 25.66666666666666, 241.0, 26.0, 26.89343687639829, 0.6769305889434117, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3864000.0000, 
sim time next is 3864600.0000, 
raw observation next is [2.5, 44.5, 18.0, 179.0, 26.0, 26.75058066333533, 0.6358550435011839, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5318559556786704, 0.445, 0.06, 0.19779005524861878, 0.6666666666666666, 0.7292150552779443, 0.7119516811670613, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06571662], dtype=float32), -1.0547882]. 
=============================================
[2019-04-04 08:10:42,017] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133500, global step 2136771: loss 1.2540
[2019-04-04 08:10:42,018] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133500, global step 2136771: learning rate 0.0000
[2019-04-04 08:10:42,272] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2136869: loss 0.1326
[2019-04-04 08:10:42,273] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2136869: learning rate 0.0000
[2019-04-04 08:10:43,170] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0480452e-11 2.4241711e-09 3.0770491e-28 4.3598116e-27 3.8517467e-21
 1.0000000e+00 5.7227554e-21], sum to 1.0000
[2019-04-04 08:10:43,170] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5478
[2019-04-04 08:10:43,190] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.0, 27.0, 101.0, 663.0, 26.0, 25.71873995584046, 0.4641196685059435, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3663000.0000, 
sim time next is 3663600.0000, 
raw observation next is [11.0, 27.33333333333333, 102.6666666666667, 679.6666666666666, 26.0, 25.69879198922166, 0.4653711337522601, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.27333333333333326, 0.3422222222222223, 0.751012891344383, 0.6666666666666666, 0.6415659991018051, 0.6551237112507534, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4165335], dtype=float32), -0.30871794]. 
=============================================
[2019-04-04 08:10:43,287] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2137341: loss 0.1231
[2019-04-04 08:10:43,289] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2137341: learning rate 0.0000
[2019-04-04 08:10:43,907] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133500, global step 2137606: loss 1.2264
[2019-04-04 08:10:43,909] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133500, global step 2137607: learning rate 0.0000
[2019-04-04 08:10:44,773] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2995672e-09 7.5035214e-09 2.6593657e-24 9.1023624e-23 1.7569275e-18
 1.0000000e+00 6.8111330e-18], sum to 1.0000
[2019-04-04 08:10:44,773] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4653
[2019-04-04 08:10:44,790] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333333, 40.0, 22.16666666666666, 204.1666666666667, 26.0, 25.1255396549195, 0.3852713893430373, 0.0, 1.0, 51990.35264702993], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3604800.0000, 
sim time next is 3605400.0000, 
raw observation next is [-0.5, 40.5, 14.0, 142.0, 26.0, 25.10690667678549, 0.3782934355155576, 0.0, 1.0, 48755.42195978988], 
processed observation next is [0.0, 0.7391304347826086, 0.44875346260387816, 0.405, 0.04666666666666667, 0.1569060773480663, 0.6666666666666666, 0.5922422230654574, 0.6260978118385192, 0.0, 1.0, 0.2321686759989994], 
reward next is 0.7678, 
noisyNet noise sample is [array([-0.6346956], dtype=float32), 1.0874763]. 
=============================================
[2019-04-04 08:10:44,831] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.02239403e-09 1.05679465e-08 4.22793392e-26 2.30682453e-24
 1.33650247e-19 1.00000000e+00 8.28939302e-19], sum to 1.0000
[2019-04-04 08:10:44,834] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1660
[2019-04-04 08:10:44,847] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.5, 29.5, 4.0, 121.0, 26.0, 25.45632849017798, 0.3605036363095248, 0.0, 1.0, 44497.7843921971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3655800.0000, 
sim time next is 3656400.0000, 
raw observation next is [8.333333333333334, 30.33333333333334, 18.16666666666666, 168.0, 26.0, 25.45176232461094, 0.3721604142791353, 0.0, 1.0, 39637.5989764605], 
processed observation next is [0.0, 0.30434782608695654, 0.6934441366574331, 0.3033333333333334, 0.060555555555555536, 0.1856353591160221, 0.6666666666666666, 0.6209801937175783, 0.6240534714263785, 0.0, 1.0, 0.18875047131647857], 
reward next is 0.8112, 
noisyNet noise sample is [array([-1.0793954], dtype=float32), 0.114726424]. 
=============================================
[2019-04-04 08:10:45,680] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2138465: loss 0.1279
[2019-04-04 08:10:45,683] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2138466: learning rate 0.0000
[2019-04-04 08:10:45,703] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2138475: loss 1.2275
[2019-04-04 08:10:45,706] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2138477: learning rate 0.0000
[2019-04-04 08:10:46,476] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133500, global step 2138835: loss 1.2348
[2019-04-04 08:10:46,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133500, global step 2138835: learning rate 0.0000
[2019-04-04 08:10:47,336] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2139209: loss 1.2116
[2019-04-04 08:10:47,337] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2139209: learning rate 0.0000
[2019-04-04 08:10:48,050] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2139493: loss 0.1212
[2019-04-04 08:10:48,051] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2139493: learning rate 0.0000
[2019-04-04 08:10:48,859] A3C_AGENT_WORKER-Thread-6 INFO:Local step 134000, global step 2139903: loss 0.0843
[2019-04-04 08:10:48,861] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 134000, global step 2139904: learning rate 0.0000
[2019-04-04 08:10:50,214] A3C_AGENT_WORKER-Thread-4 INFO:Local step 134000, global step 2140523: loss 0.1238
[2019-04-04 08:10:50,215] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 134000, global step 2140524: learning rate 0.0000
[2019-04-04 08:10:50,725] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2140755: loss 1.1818
[2019-04-04 08:10:50,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2140755: learning rate 0.0000
[2019-04-04 08:10:51,021] A3C_AGENT_WORKER-Thread-5 INFO:Local step 133500, global step 2140881: loss 1.1870
[2019-04-04 08:10:51,022] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 133500, global step 2140881: learning rate 0.0000
[2019-04-04 08:10:51,911] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.1381088e-10 2.6920905e-08 1.2614337e-25 1.2485181e-24 1.7684046e-19
 1.0000000e+00 8.4193540e-19], sum to 1.0000
[2019-04-04 08:10:51,916] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2022
[2019-04-04 08:10:51,943] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.166666666666666, 26.66666666666666, 0.0, 0.0, 26.0, 25.48963163864869, 0.3558270079193124, 0.0, 1.0, 46426.62359010256], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3653400.0000, 
sim time next is 3654000.0000, 
raw observation next is [9.0, 27.0, 0.0, 0.0, 26.0, 25.50392633255898, 0.3554702153134661, 0.0, 1.0, 30363.04211555087], 
processed observation next is [0.0, 0.30434782608695654, 0.7119113573407203, 0.27, 0.0, 0.0, 0.6666666666666666, 0.6253271943799149, 0.6184900717711553, 0.0, 1.0, 0.14458591483595654], 
reward next is 0.8554, 
noisyNet noise sample is [array([-1.9438374], dtype=float32), 0.47862488]. 
=============================================
[2019-04-04 08:10:51,959] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[81.111984]
 [81.0219  ]
 [80.98919 ]
 [81.01179 ]
 [81.0229  ]], R is [[81.18497467]
 [81.1520462 ]
 [81.17794037]
 [81.26907349]
 [81.36711884]].
[2019-04-04 08:10:52,164] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2141401: loss 0.0878
[2019-04-04 08:10:52,164] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2141401: learning rate 0.0000
[2019-04-04 08:10:52,519] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.90830528e-11 3.79309562e-09 1.36140733e-27 3.98563344e-26
 1.20729975e-20 1.00000000e+00 2.70822487e-20], sum to 1.0000
[2019-04-04 08:10:52,523] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2417
[2019-04-04 08:10:52,564] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666667, 75.0, 76.66666666666667, 397.3333333333333, 26.0, 25.36013461873507, 0.4162422408080828, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3831600.0000, 
sim time next is 3832200.0000, 
raw observation next is [-4.5, 74.0, 91.0, 447.0, 26.0, 25.55966734364329, 0.4589944323882118, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3379501385041552, 0.74, 0.30333333333333334, 0.49392265193370166, 0.6666666666666666, 0.6299722786369409, 0.6529981441294039, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5955816], dtype=float32), 0.4057928]. 
=============================================
[2019-04-04 08:10:54,021] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2142213: loss 1.1488
[2019-04-04 08:10:54,025] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2142213: learning rate 0.0000
[2019-04-04 08:10:57,257] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2143707: loss 1.1324
[2019-04-04 08:10:57,259] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2143707: learning rate 0.0000
[2019-04-04 08:10:58,135] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4212172e-09 4.6459469e-08 1.4470031e-24 6.1587551e-23 4.0653297e-18
 1.0000000e+00 3.4595313e-18], sum to 1.0000
[2019-04-04 08:10:58,137] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4709
[2019-04-04 08:10:58,151] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.166666666666667, 49.83333333333334, 0.0, 0.0, 26.0, 24.78182683347869, 0.2512509759561343, 0.0, 1.0, 39678.36219543718], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4169400.0000, 
sim time next is 4170000.0000, 
raw observation next is [-4.333333333333334, 49.66666666666667, 0.0, 0.0, 26.0, 24.77356968530453, 0.2447861568828066, 0.0, 1.0, 39750.67718149976], 
processed observation next is [0.0, 0.2608695652173913, 0.3425669436749769, 0.4966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5644641404420442, 0.5815953856276023, 0.0, 1.0, 0.18928893895952267], 
reward next is 0.8107, 
noisyNet noise sample is [array([0.7997697], dtype=float32), -0.6918411]. 
=============================================
[2019-04-04 08:10:58,166] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[75.85017]
 [75.98851]
 [76.08919]
 [76.19739]
 [76.30499]], R is [[75.77884674]
 [75.83211517]
 [75.88518524]
 [75.93802643]
 [75.99058533]].
[2019-04-04 08:10:58,318] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134000, global step 2144183: loss 0.0715
[2019-04-04 08:10:58,323] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134000, global step 2144183: learning rate 0.0000
[2019-04-04 08:10:59,643] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1061216e-09 2.4616418e-09 3.0152353e-26 2.1365994e-24 1.2681827e-19
 1.0000000e+00 1.7955952e-19], sum to 1.0000
[2019-04-04 08:10:59,643] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4921
[2019-04-04 08:10:59,655] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 26.0, 25.28307305797101, 0.455524717061257, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3873000.0000, 
sim time next is 3873600.0000, 
raw observation next is [1.0, 51.0, 0.0, 0.0, 26.0, 25.19790103862235, 0.4429995387981707, 0.0, 1.0, 23991.73931460749], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.51, 0.0, 0.0, 0.6666666666666666, 0.5998250865518626, 0.6476665129327236, 0.0, 1.0, 0.1142463776886071], 
reward next is 0.8858, 
noisyNet noise sample is [array([-0.42234075], dtype=float32), -0.40355656]. 
=============================================
[2019-04-04 08:11:00,463] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134000, global step 2145160: loss 0.1054
[2019-04-04 08:11:00,479] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134000, global step 2145167: learning rate 0.0000
[2019-04-04 08:11:00,526] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2145199: loss 0.4686
[2019-04-04 08:11:00,528] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2145200: learning rate 0.0000
[2019-04-04 08:11:01,652] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.1709695e-12 7.4894768e-10 1.8882450e-28 3.6751150e-26 2.5168034e-21
 1.0000000e+00 2.9452713e-21], sum to 1.0000
[2019-04-04 08:11:01,653] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9187
[2019-04-04 08:11:01,687] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 53.83333333333334, 94.0, 541.3333333333333, 26.0, 25.84275455845108, 0.4847927350080858, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3919800.0000, 
sim time next is 3920400.0000, 
raw observation next is [-8.0, 53.0, 95.5, 579.0, 26.0, 25.90889042083212, 0.4881493706242583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24099722991689754, 0.53, 0.31833333333333336, 0.6397790055248619, 0.6666666666666666, 0.65907420173601, 0.6627164568747528, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.47569272], dtype=float32), 0.15941976]. 
=============================================
[2019-04-04 08:11:02,059] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4301139e-10 6.3251102e-09 3.2730211e-25 5.9425030e-24 3.3704935e-19
 1.0000000e+00 6.9626568e-19], sum to 1.0000
[2019-04-04 08:11:02,060] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2829
[2019-04-04 08:11:02,116] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.833333333333334, 44.33333333333333, 0.0, 0.0, 26.0, 25.56894520502246, 0.5356341692579053, 0.0, 1.0, 57114.12798528703], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3959400.0000, 
sim time next is 3960000.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.555611343825, 0.5346547533716187, 0.0, 1.0, 48030.32016127172], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6296342786520833, 0.6782182511238729, 0.0, 1.0, 0.22871581029177007], 
reward next is 0.7713, 
noisyNet noise sample is [array([1.1866847], dtype=float32), 1.0778841]. 
=============================================
[2019-04-04 08:11:02,124] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.46407]
 [77.06849]
 [76.63854]
 [76.26697]
 [76.26207]], R is [[75.74599457]
 [75.71656036]
 [75.58030701]
 [75.31616211]
 [75.56300354]].
[2019-04-04 08:11:02,184] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2145928: loss 0.4578
[2019-04-04 08:11:02,184] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2145928: learning rate 0.0000
[2019-04-04 08:11:02,225] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0491672e-10 6.4541936e-09 5.9205066e-25 1.7960504e-23 3.2698238e-19
 1.0000000e+00 1.4683096e-18], sum to 1.0000
[2019-04-04 08:11:02,227] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8548
[2019-04-04 08:11:02,261] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.55196496960624, 0.5322216576527047, 0.0, 1.0, 41406.73541906241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3960600.0000, 
sim time next is 3961200.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.55207971186449, 0.5298367263659456, 0.0, 1.0, 68501.133082357], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6293399759887075, 0.6766122421219819, 0.0, 1.0, 0.3261958718207476], 
reward next is 0.6738, 
noisyNet noise sample is [array([0.634988], dtype=float32), -0.1956277]. 
=============================================
[2019-04-04 08:11:02,741] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2146147: loss 0.0943
[2019-04-04 08:11:02,742] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2146147: learning rate 0.0000
[2019-04-04 08:11:03,637] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134000, global step 2146549: loss 0.0634
[2019-04-04 08:11:03,638] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134000, global step 2146549: learning rate 0.0000
[2019-04-04 08:11:04,086] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2146759: loss 0.0972
[2019-04-04 08:11:04,087] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2146759: learning rate 0.0000
[2019-04-04 08:11:04,579] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2146981: loss 0.4378
[2019-04-04 08:11:04,592] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2146987: learning rate 0.0000
[2019-04-04 08:11:06,481] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2147855: loss 0.4395
[2019-04-04 08:11:06,483] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2147856: learning rate 0.0000
[2019-04-04 08:11:07,500] A3C_AGENT_WORKER-Thread-6 INFO:Local step 134500, global step 2148335: loss 0.4217
[2019-04-04 08:11:07,504] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 134500, global step 2148336: learning rate 0.0000
[2019-04-04 08:11:07,509] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2148340: loss 0.0970
[2019-04-04 08:11:07,510] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2148340: learning rate 0.0000
[2019-04-04 08:11:08,406] A3C_AGENT_WORKER-Thread-5 INFO:Local step 134000, global step 2148701: loss 0.1042
[2019-04-04 08:11:08,407] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 134000, global step 2148701: learning rate 0.0000
[2019-04-04 08:11:08,614] A3C_AGENT_WORKER-Thread-4 INFO:Local step 134500, global step 2148781: loss 0.4147
[2019-04-04 08:11:08,615] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 134500, global step 2148781: learning rate 0.0000
[2019-04-04 08:11:10,468] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2149645: loss 0.4003
[2019-04-04 08:11:10,488] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2149645: learning rate 0.0000
[2019-04-04 08:11:10,607] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2149708: loss 0.0918
[2019-04-04 08:11:10,612] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2149709: learning rate 0.0000
[2019-04-04 08:11:10,931] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5904977e-09 6.9190868e-08 4.6420842e-23 3.4690652e-22 1.6058461e-17
 9.9999988e-01 1.4349699e-17], sum to 1.0000
[2019-04-04 08:11:10,935] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4737
[2019-04-04 08:11:10,950] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.66666666666667, 67.0, 0.0, 0.0, 26.0, 23.98346868243574, 0.06946195842066245, 0.0, 1.0, 43753.16992236014], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3991200.0000, 
sim time next is 3991800.0000, 
raw observation next is [-12.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.92049102266168, 0.05623561432546614, 0.0, 1.0, 43786.77862788732], 
processed observation next is [1.0, 0.17391304347826086, 0.10710987996306563, 0.68, 0.0, 0.0, 0.6666666666666666, 0.4933742518884732, 0.5187452047751554, 0.0, 1.0, 0.20850846965660627], 
reward next is 0.7915, 
noisyNet noise sample is [array([-0.488269], dtype=float32), 0.8790347]. 
=============================================
[2019-04-04 08:11:14,433] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2151530: loss 0.0941
[2019-04-04 08:11:14,435] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2151530: learning rate 0.0000
[2019-04-04 08:11:16,103] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134500, global step 2152314: loss 0.4085
[2019-04-04 08:11:16,104] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134500, global step 2152314: learning rate 0.0000
[2019-04-04 08:11:16,668] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2152608: loss 1.7395
[2019-04-04 08:11:16,670] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2152609: learning rate 0.0000
[2019-04-04 08:11:18,660] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2153552: loss 1.6942
[2019-04-04 08:11:18,663] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2153553: learning rate 0.0000
[2019-04-04 08:11:18,726] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134500, global step 2153588: loss 0.4171
[2019-04-04 08:11:18,728] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134500, global step 2153589: learning rate 0.0000
[2019-04-04 08:11:20,311] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2154328: loss 1.7476
[2019-04-04 08:11:20,311] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2154328: learning rate 0.0000
[2019-04-04 08:11:20,993] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2154643: loss 0.4064
[2019-04-04 08:11:20,994] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2154643: learning rate 0.0000
[2019-04-04 08:11:21,385] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3021552e-11 3.7411403e-09 1.8988777e-28 1.7930494e-26 7.0013993e-22
 1.0000000e+00 2.6122342e-21], sum to 1.0000
[2019-04-04 08:11:21,385] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1805
[2019-04-04 08:11:21,397] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.5, 69.66666666666667, 0.0, 0.0, 26.0, 25.42536083764848, 0.3825379762499379, 0.0, 1.0, 40339.03531122617], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4339200.0000, 
sim time next is 4339800.0000, 
raw observation next is [3.45, 70.0, 0.0, 0.0, 26.0, 25.48837646148949, 0.3829832592040628, 0.0, 1.0, 18756.2594473145], 
processed observation next is [1.0, 0.21739130434782608, 0.5581717451523546, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6240313717907909, 0.6276610864013542, 0.0, 1.0, 0.0893155211776881], 
reward next is 0.9107, 
noisyNet noise sample is [array([1.2572353], dtype=float32), -0.7034785]. 
=============================================
[2019-04-04 08:11:21,548] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134500, global step 2154926: loss 0.3756
[2019-04-04 08:11:21,550] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134500, global step 2154927: learning rate 0.0000
[2019-04-04 08:11:21,886] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2155100: loss 0.4099
[2019-04-04 08:11:21,888] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2155100: learning rate 0.0000
[2019-04-04 08:11:22,545] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2155387: loss 1.6925
[2019-04-04 08:11:22,546] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2155387: learning rate 0.0000
[2019-04-04 08:11:23,369] A3C_AGENT_WORKER-Thread-6 INFO:Local step 135000, global step 2155758: loss 1.7122
[2019-04-04 08:11:23,372] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 135000, global step 2155759: learning rate 0.0000
[2019-04-04 08:11:24,577] A3C_AGENT_WORKER-Thread-4 INFO:Local step 135000, global step 2156386: loss 1.7805
[2019-04-04 08:11:24,581] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 135000, global step 2156387: learning rate 0.0000
[2019-04-04 08:11:25,262] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.2899144e-11 1.9927229e-09 3.8649355e-28 2.6382750e-26 7.4195379e-21
 1.0000000e+00 6.0942848e-20], sum to 1.0000
[2019-04-04 08:11:25,264] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2777
[2019-04-04 08:11:25,281] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.1, 73.0, 0.0, 0.0, 26.0, 25.78267629394275, 0.4501126757278497, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4309200.0000, 
sim time next is 4309800.0000, 
raw observation next is [5.05, 73.5, 0.0, 0.0, 26.0, 25.74073401435937, 0.438096201981269, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.6024930747922439, 0.735, 0.0, 0.0, 0.6666666666666666, 0.6450611678632807, 0.6460320673270896, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41386294], dtype=float32), 0.015278228]. 
=============================================
[2019-04-04 08:11:25,594] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2156893: loss 0.4156
[2019-04-04 08:11:25,600] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2156894: learning rate 0.0000
[2019-04-04 08:11:25,835] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2157024: loss 1.7155
[2019-04-04 08:11:25,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2157024: learning rate 0.0000
[2019-04-04 08:11:26,673] A3C_AGENT_WORKER-Thread-5 INFO:Local step 134500, global step 2157455: loss 0.4121
[2019-04-04 08:11:26,675] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 134500, global step 2157457: learning rate 0.0000
[2019-04-04 08:11:28,066] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7351588e-11 3.5585818e-10 3.4829363e-29 9.0898177e-27 2.1904009e-22
 1.0000000e+00 1.3917148e-21], sum to 1.0000
[2019-04-04 08:11:28,067] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1760
[2019-04-04 08:11:28,148] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 74.0, 20.83333333333334, 45.83333333333334, 26.0, 24.84499392762313, 0.4687498239212073, 1.0, 1.0, 177613.3436684992], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4470000.0000, 
sim time next is 4470600.0000, 
raw observation next is [0.0, 73.0, 16.66666666666667, 36.66666666666667, 26.0, 24.93248620160991, 0.5522312483382483, 1.0, 1.0, 51422.29471801704], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.73, 0.05555555555555557, 0.04051565377532229, 0.6666666666666666, 0.5777071834674924, 0.6840770827794161, 1.0, 1.0, 0.24486807008579542], 
reward next is 0.7551, 
noisyNet noise sample is [array([1.1072445], dtype=float32), 0.26245487]. 
=============================================
[2019-04-04 08:11:29,058] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2158628: loss 0.3893
[2019-04-04 08:11:29,059] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2158628: learning rate 0.0000
[2019-04-04 08:11:32,040] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135000, global step 2160100: loss 1.6907
[2019-04-04 08:11:32,042] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135000, global step 2160102: learning rate 0.0000
[2019-04-04 08:11:33,104] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5443916e-10 5.2351861e-09 4.7447326e-27 2.1073125e-25 1.8832747e-20
 1.0000000e+00 7.7579848e-20], sum to 1.0000
[2019-04-04 08:11:33,114] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5851
[2019-04-04 08:11:33,170] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2160624: loss 0.4474
[2019-04-04 08:11:33,187] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2160624: learning rate 0.0000
[2019-04-04 08:11:33,192] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.9666666666666667, 71.33333333333334, 0.0, 0.0, 26.0, 25.19723456878289, 0.3835606140144486, 0.0, 1.0, 40429.94648901503], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4518600.0000, 
sim time next is 4519200.0000, 
raw observation next is [-0.9333333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.21055773401685, 0.4304753313418897, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4367497691597415, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6008798111680708, 0.6434917771139632, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40700245], dtype=float32), -0.7202259]. 
=============================================
[2019-04-04 08:11:33,915] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2160970: loss 0.0004
[2019-04-04 08:11:33,917] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2160971: learning rate 0.0000
[2019-04-04 08:11:34,259] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.1457951e-11 2.0001405e-09 8.0945218e-28 3.1751886e-26 1.2754345e-21
 1.0000000e+00 7.5117241e-21], sum to 1.0000
[2019-04-04 08:11:34,265] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7534
[2019-04-04 08:11:34,286] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.43083703066279, 0.4324649000142586, 0.0, 1.0, 30449.21346916046], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4690200.0000, 
sim time next is 4690800.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.44892643278416, 0.4211828789196022, 0.0, 1.0, 19807.86338389092], 
processed observation next is [1.0, 0.30434782608695654, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6207438693986799, 0.6403942929732007, 0.0, 1.0, 0.09432315897090914], 
reward next is 0.9057, 
noisyNet noise sample is [array([-0.1517209], dtype=float32), -0.013882699]. 
=============================================
[2019-04-04 08:11:34,368] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135000, global step 2161200: loss 1.6634
[2019-04-04 08:11:34,377] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135000, global step 2161200: learning rate 0.0000
[2019-04-04 08:11:35,697] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2161837: loss 0.0021
[2019-04-04 08:11:35,698] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2161839: learning rate 0.0000
[2019-04-04 08:11:36,672] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2162281: loss 1.6005
[2019-04-04 08:11:36,678] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2162282: learning rate 0.0000
[2019-04-04 08:11:36,944] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2162396: loss 1.6093
[2019-04-04 08:11:36,944] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2162396: learning rate 0.0000
[2019-04-04 08:11:36,997] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.70367070e-11 2.80227863e-09 1.01991099e-28 1.30213079e-26
 1.20156062e-21 1.00000000e+00 1.06467855e-20], sum to 1.0000
[2019-04-04 08:11:37,000] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3412
[2019-04-04 08:11:37,066] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 174.0, 421.0, 26.0, 25.61872780141827, 0.4614678493296047, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4786200.0000, 
sim time next is 4786800.0000, 
raw observation next is [-3.666666666666667, 69.0, 170.5, 472.5, 26.0, 25.58715159341561, 0.4669730604549489, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3610341643582641, 0.69, 0.5683333333333334, 0.5220994475138122, 0.6666666666666666, 0.632262632784634, 0.6556576868183163, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07811079], dtype=float32), 1.5927019]. 
=============================================
[2019-04-04 08:11:37,706] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135000, global step 2162761: loss 1.6402
[2019-04-04 08:11:37,708] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135000, global step 2162761: learning rate 0.0000
[2019-04-04 08:11:37,894] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2162853: loss 0.0019
[2019-04-04 08:11:37,897] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2162853: learning rate 0.0000
[2019-04-04 08:11:38,479] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4633947e-10 4.7469304e-09 1.1162134e-25 2.3174704e-24 1.0025303e-19
 1.0000000e+00 5.3300706e-19], sum to 1.0000
[2019-04-04 08:11:38,479] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4690
[2019-04-04 08:11:38,492] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 35.0, 73.83333333333334, 488.3333333333333, 26.0, 25.21070579468701, 0.4205323611842324, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4812000.0000, 
sim time next is 4812600.0000, 
raw observation next is [3.0, 34.5, 65.66666666666667, 427.6666666666667, 26.0, 25.20335768509565, 0.4078852072919708, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.345, 0.2188888888888889, 0.47255985267034994, 0.6666666666666666, 0.6002798070913041, 0.6359617357639903, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6579848], dtype=float32), -1.4594938]. 
=============================================
[2019-04-04 08:11:39,670] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2163718: loss 0.0026
[2019-04-04 08:11:39,671] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2163718: learning rate 0.0000
[2019-04-04 08:11:40,124] A3C_AGENT_WORKER-Thread-6 INFO:Local step 135500, global step 2163933: loss 0.0027
[2019-04-04 08:11:40,125] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 135500, global step 2163933: learning rate 0.0000
[2019-04-04 08:11:40,745] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.4956082e-11 4.1355790e-09 1.1155355e-27 3.7519933e-25 1.8869638e-20
 1.0000000e+00 1.7719758e-20], sum to 1.0000
[2019-04-04 08:11:40,746] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5629
[2019-04-04 08:11:40,825] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.1666666666666667, 51.16666666666667, 288.6666666666666, 260.0, 26.0, 24.97898150457865, 0.3351604756876691, 0.0, 1.0, 32013.84086552718], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4878600.0000, 
sim time next is 4879200.0000, 
raw observation next is [0.0666666666666666, 50.33333333333334, 285.8333333333333, 284.0, 26.0, 24.99449074311573, 0.3450167918472194, 0.0, 1.0, 18716.84457968229], 
processed observation next is [0.0, 0.4782608695652174, 0.46445060018467227, 0.5033333333333334, 0.9527777777777777, 0.3138121546961326, 0.6666666666666666, 0.5828742285929774, 0.6150055972824064, 0.0, 1.0, 0.08912783133182042], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.88108635], dtype=float32), 1.0396742]. 
=============================================
[2019-04-04 08:11:41,164] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2164365: loss 1.6175
[2019-04-04 08:11:41,170] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2164366: learning rate 0.0000
[2019-04-04 08:11:41,798] A3C_AGENT_WORKER-Thread-4 INFO:Local step 135500, global step 2164676: loss 0.0014
[2019-04-04 08:11:41,800] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 135500, global step 2164676: learning rate 0.0000
[2019-04-04 08:11:42,147] A3C_AGENT_WORKER-Thread-5 INFO:Local step 135000, global step 2164834: loss 1.6359
[2019-04-04 08:11:42,149] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 135000, global step 2164834: learning rate 0.0000
[2019-04-04 08:11:43,271] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2165373: loss 0.0017
[2019-04-04 08:11:43,272] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2165373: learning rate 0.0000
[2019-04-04 08:11:44,508] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.1495245e-10 1.2776002e-08 3.3279135e-25 1.8640027e-23 7.6869931e-19
 1.0000000e+00 5.8820780e-18], sum to 1.0000
[2019-04-04 08:11:44,508] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6303
[2019-04-04 08:11:44,526] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 63.33333333333333, 0.0, 0.0, 26.0, 24.56121173522393, 0.171160821016163, 0.0, 1.0, 39478.78686073156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4862400.0000, 
sim time next is 4863000.0000, 
raw observation next is [-3.833333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 24.53398000591719, 0.1640394569892031, 0.0, 1.0, 39492.02505330533], 
processed observation next is [0.0, 0.2608695652173913, 0.3564173591874424, 0.6416666666666667, 0.0, 0.0, 0.6666666666666666, 0.5444983338264325, 0.5546798189964011, 0.0, 1.0, 0.18805726215859683], 
reward next is 0.8119, 
noisyNet noise sample is [array([0.5670475], dtype=float32), 2.1592388]. 
=============================================
[2019-04-04 08:11:44,545] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[76.93985 ]
 [77.01826 ]
 [77.09178 ]
 [77.165405]
 [77.27064 ]], R is [[76.91607666]
 [76.95892334]
 [77.00132751]
 [77.04325867]
 [77.08467102]].
[2019-04-04 08:11:45,341] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2166378: loss 1.6009
[2019-04-04 08:11:45,342] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2166378: learning rate 0.0000
[2019-04-04 08:11:49,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:11:49,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:11:49,403] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135500, global step 2168260: loss 0.0027
[2019-04-04 08:11:49,405] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135500, global step 2168260: learning rate 0.0000
[2019-04-04 08:11:49,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run17
[2019-04-04 08:11:49,443] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2168269: loss 1.6316
[2019-04-04 08:11:49,444] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2168269: learning rate 0.0000
[2019-04-04 08:11:50,877] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6375910e-09 1.1257462e-08 3.6401930e-25 3.6478261e-24 3.7203787e-19
 1.0000000e+00 1.0023128e-18], sum to 1.0000
[2019-04-04 08:11:50,877] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1912
[2019-04-04 08:11:50,886] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 25.80009934946773, 0.5672273438275154, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5004000.0000, 
sim time next is 5004600.0000, 
raw observation next is [3.0, 36.5, 0.0, 0.0, 26.0, 25.79380222322046, 0.5085262268064571, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.365, 0.0, 0.0, 0.6666666666666666, 0.6494835186017051, 0.669508742268819, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01510325], dtype=float32), 0.6643003]. 
=============================================
[2019-04-04 08:11:51,183] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135500, global step 2169030: loss 0.0008
[2019-04-04 08:11:51,186] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135500, global step 2169031: learning rate 0.0000
[2019-04-04 08:11:51,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:11:51,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:11:51,414] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run17
[2019-04-04 08:11:51,771] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.1991399e-11 8.2906010e-10 9.6824067e-28 1.4811194e-25 2.5816740e-20
 1.0000000e+00 5.5982356e-20], sum to 1.0000
[2019-04-04 08:11:51,771] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2339
[2019-04-04 08:11:51,821] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 45.0, 127.1666666666667, 820.8333333333334, 26.0, 25.14283897251713, 0.4151217758077963, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4792800.0000, 
sim time next is 4793400.0000, 
raw observation next is [-0.5, 44.5, 122.0, 839.0, 26.0, 25.07506216708574, 0.4086450877313589, 0.0, 1.0, 18707.90543533873], 
processed observation next is [0.0, 0.4782608695652174, 0.44875346260387816, 0.445, 0.4066666666666667, 0.9270718232044199, 0.6666666666666666, 0.5895885139238116, 0.6362150292437864, 0.0, 1.0, 0.08908526397780347], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.13868861], dtype=float32), -0.29219493]. 
=============================================
[2019-04-04 08:11:53,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:11:53,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:11:53,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run17
[2019-04-04 08:11:53,893] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2170055: loss 0.0003
[2019-04-04 08:11:53,894] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2170055: learning rate 0.0000
[2019-04-04 08:11:54,078] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2170123: loss 0.0009
[2019-04-04 08:11:54,079] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2170123: learning rate 0.0000
[2019-04-04 08:11:54,959] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135500, global step 2170482: loss 0.0003
[2019-04-04 08:11:54,960] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135500, global step 2170482: learning rate 0.0000
[2019-04-04 08:11:55,434] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:11:55,434] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:11:55,439] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run17
[2019-04-04 08:11:56,038] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:11:56,039] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:11:56,052] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run17
[2019-04-04 08:11:57,008] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:11:57,008] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:11:57,047] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run17
[2019-04-04 08:11:58,930] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2171607: loss 0.0010
[2019-04-04 08:11:58,934] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2171607: learning rate 0.0000
[2019-04-04 08:11:59,269] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:11:59,269] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:11:59,272] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run17
[2019-04-04 08:11:59,747] A3C_AGENT_WORKER-Thread-5 INFO:Local step 135500, global step 2171837: loss 0.0002
[2019-04-04 08:11:59,759] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 135500, global step 2171837: learning rate 0.0000
[2019-04-04 08:12:03,416] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1128987e-11 3.3544881e-10 1.9663114e-29 6.8437324e-28 1.8234237e-22
 1.0000000e+00 2.1089643e-22], sum to 1.0000
[2019-04-04 08:12:03,416] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5427
[2019-04-04 08:12:03,427] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.5, 25.5, 92.0, 774.0, 26.0, 26.79393048728001, 0.811827658382259, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4980600.0000, 
sim time next is 4981200.0000, 
raw observation next is [8.666666666666668, 25.33333333333333, 88.66666666666667, 751.8333333333333, 26.0, 27.24008110892287, 0.8550570845494335, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7026777469990768, 0.2533333333333333, 0.29555555555555557, 0.8307550644567219, 0.6666666666666666, 0.7700067590769057, 0.7850190281831445, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10529502], dtype=float32), -0.14903684]. 
=============================================
[2019-04-04 08:12:03,714] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2172955: loss 0.0003
[2019-04-04 08:12:03,715] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2172955: learning rate 0.0000
[2019-04-04 08:12:06,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:12:06,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:12:06,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run17
[2019-04-04 08:12:08,299] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2174627: loss 0.0001
[2019-04-04 08:12:08,306] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2174630: learning rate 0.0000
[2019-04-04 08:12:08,311] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:12:08,311] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:12:08,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run17
[2019-04-04 08:12:09,608] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.6763979e-11 8.7743466e-09 5.8723687e-26 1.0528106e-24 6.1426923e-20
 1.0000000e+00 4.5590685e-19], sum to 1.0000
[2019-04-04 08:12:09,608] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0245
[2019-04-04 08:12:09,655] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 63.16666666666667, 42.33333333333334, 2.999999999999999, 26.0, 25.31498845580347, 0.2533325595360833, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 119400.0000, 
sim time next is 120000.0000, 
raw observation next is [-7.8, 65.33333333333334, 43.66666666666666, 1.5, 26.0, 25.31741570690654, 0.2552981293621413, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.6533333333333334, 0.14555555555555552, 0.0016574585635359116, 0.6666666666666666, 0.6097846422422117, 0.5850993764540471, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8592414], dtype=float32), -0.7591012]. 
=============================================
[2019-04-04 08:12:09,662] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[79.93027 ]
 [80.280495]
 [80.538895]
 [80.68736 ]
 [80.61041 ]], R is [[79.74401855]
 [79.94657898]
 [80.14711761]
 [80.34564972]
 [80.54219055]].
[2019-04-04 08:12:10,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:12:10,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:12:10,688] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run17
[2019-04-04 08:12:10,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:12:10,858] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:12:10,872] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run17
[2019-04-04 08:12:11,649] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:12:11,649] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:12:11,652] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run17
[2019-04-04 08:12:14,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6772912e-10 6.7879498e-09 1.6115153e-26 9.3414857e-25 3.3387685e-20
 1.0000000e+00 1.3898753e-19], sum to 1.0000
[2019-04-04 08:12:14,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1380
[2019-04-04 08:12:15,019] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.9, 19.0, 0.0, 0.0, 26.0, 26.99981218531288, 0.828417662514584, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5088000.0000, 
sim time next is 5088600.0000, 
raw observation next is [8.85, 19.0, 0.0, 0.0, 26.0, 26.93990876742996, 0.8161215831986356, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7077562326869806, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7449923972858299, 0.7720405277328785, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19211228], dtype=float32), 0.031181749]. 
=============================================
[2019-04-04 08:12:16,947] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:12:16,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:12:16,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run17
[2019-04-04 08:12:17,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:12:17,587] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:12:17,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run17
[2019-04-04 08:12:20,076] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.5768878e-09 7.5524582e-08 1.8866945e-23 9.7600133e-22 1.3836765e-17
 9.9999988e-01 2.4642341e-17], sum to 1.0000
[2019-04-04 08:12:20,077] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9513
[2019-04-04 08:12:20,095] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.58697322399401, -0.2746984774354094, 0.0, 1.0, 44980.21943306887], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 197400.0000, 
sim time next is 198000.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.59963194229614, -0.2846053581967187, 0.0, 1.0, 44983.22906741899], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.38330266185801154, 0.4051315472677604, 0.0, 1.0, 0.21420585270199521], 
reward next is 0.7858, 
noisyNet noise sample is [array([-0.38002545], dtype=float32), 0.09157306]. 
=============================================
[2019-04-04 08:12:20,184] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[71.32788 ]
 [71.415695]
 [71.51787 ]
 [71.6248  ]
 [71.721   ]], R is [[71.3278656 ]
 [71.40039062]
 [71.47221375]
 [71.54338074]
 [71.61386871]].
[2019-04-04 08:12:21,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:12:21,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:12:21,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run17
[2019-04-04 08:12:26,887] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:12:26,887] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:12:26,891] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run17
[2019-04-04 08:12:36,490] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.2446381e-10 6.4739201e-09 3.0808945e-25 4.6620473e-24 4.9924899e-19
 1.0000000e+00 3.5139286e-19], sum to 1.0000
[2019-04-04 08:12:36,490] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8412
[2019-04-04 08:12:36,532] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 42.0, 70.0, 477.0, 26.0, 26.26698164258647, 0.5286488384571079, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 315000.0000, 
sim time next is 315600.0000, 
raw observation next is [-9.5, 42.0, 62.33333333333334, 437.5, 26.0, 26.31216862155506, 0.524594860000917, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.2077777777777778, 0.48342541436464087, 0.6666666666666666, 0.6926807184629217, 0.674864953333639, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.89795774], dtype=float32), 0.10793851]. 
=============================================
[2019-04-04 08:12:48,934] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.8052572e-08 2.6963536e-07 1.9324315e-22 8.8349142e-21 6.1435664e-17
 9.9999964e-01 1.7589137e-16], sum to 1.0000
[2019-04-04 08:12:48,936] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8848
[2019-04-04 08:12:48,975] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-14.66666666666667, 69.0, 0.0, 0.0, 26.0, 23.25581194638117, -0.1130215196485651, 0.0, 1.0, 47800.39444627356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 350400.0000, 
sim time next is 351000.0000, 
raw observation next is [-14.75, 69.0, 0.0, 0.0, 26.0, 23.29819291892085, -0.1212209936813704, 0.0, 1.0, 47895.43855705271], 
processed observation next is [1.0, 0.043478260869565216, 0.05401662049861495, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4415160765767376, 0.45959300210620985, 0.0, 1.0, 0.22807351693834624], 
reward next is 0.7719, 
noisyNet noise sample is [array([-0.73603046], dtype=float32), 1.5968275]. 
=============================================
[2019-04-04 08:12:48,985] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[68.63047 ]
 [68.889465]
 [68.99327 ]
 [69.245735]
 [69.36415 ]], R is [[68.59130096]
 [68.67776489]
 [68.76371765]
 [68.84905243]
 [68.93374634]].
[2019-04-04 08:12:49,662] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5347834e-10 1.3281005e-09 2.6906916e-25 1.5036418e-23 3.6206631e-19
 1.0000000e+00 7.9930251e-19], sum to 1.0000
[2019-04-04 08:12:49,663] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0159
[2019-04-04 08:12:49,721] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.1, 61.0, 0.0, 0.0, 26.0, 25.32286418168126, 0.3422585518887196, 1.0, 1.0, 82596.34471488331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 326400.0000, 
sim time next is 327000.0000, 
raw observation next is [-12.2, 62.0, 0.0, 0.0, 26.0, 25.22380161669614, 0.3387076214628316, 1.0, 1.0, 76568.10625246541], 
processed observation next is [1.0, 0.782608695652174, 0.12465373961218838, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6019834680580116, 0.6129025404876106, 1.0, 1.0, 0.3646100297736448], 
reward next is 0.6354, 
noisyNet noise sample is [array([0.10874981], dtype=float32), -2.1636398]. 
=============================================
[2019-04-04 08:12:49,743] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.904755]
 [74.48343 ]
 [74.03073 ]
 [73.8862  ]
 [73.54025 ]], R is [[75.04007721]
 [74.8963623 ]
 [74.64807129]
 [74.90158844]
 [74.74570465]].
[2019-04-04 08:12:53,138] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.2591766e-08 3.2586485e-07 6.6589267e-21 6.8108636e-20 5.2179591e-16
 9.9999964e-01 1.4419344e-15], sum to 1.0000
[2019-04-04 08:12:53,138] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4741
[2019-04-04 08:12:53,152] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.76560268382346, -0.4610421458101261, 0.0, 1.0, 48956.14621793653], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 368400.0000, 
sim time next is 369000.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.7826081811242, -0.4751985559176858, 0.0, 1.0, 49027.46503632655], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.6666666666666666, 0.31521734842701665, 0.34160048136077137, 0.0, 1.0, 0.2334641192206026], 
reward next is 0.7665, 
noisyNet noise sample is [array([-0.7098427], dtype=float32), 0.99000233]. 
=============================================
[2019-04-04 08:12:53,157] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[64.0018  ]
 [64.09273 ]
 [64.21449 ]
 [64.32404 ]
 [64.436966]], R is [[64.02739716]
 [64.15399933]
 [64.2796936 ]
 [64.40467072]
 [64.52890778]].
[2019-04-04 08:13:04,765] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1712635e-11 3.4577461e-09 2.3452293e-29 9.4847104e-27 8.1834668e-22
 1.0000000e+00 7.4659020e-21], sum to 1.0000
[2019-04-04 08:13:04,765] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4657
[2019-04-04 08:13:04,783] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.8, 86.0, 0.0, 0.0, 26.0, 24.78561903256266, 0.2244046202673026, 0.0, 1.0, 39818.90805066546], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 529200.0000, 
sim time next is 529800.0000, 
raw observation next is [3.616666666666667, 85.33333333333334, 0.0, 0.0, 26.0, 24.76743881023197, 0.221296153010156, 0.0, 1.0, 39866.15943425304], 
processed observation next is [0.0, 0.13043478260869565, 0.5627885503231764, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5639532341859974, 0.5737653843367186, 0.0, 1.0, 0.18983885444882398], 
reward next is 0.8102, 
noisyNet noise sample is [array([0.7020754], dtype=float32), -0.72746176]. 
=============================================
[2019-04-04 08:13:05,253] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.4676350e-11 6.6070487e-09 4.1701694e-28 5.7948607e-26 1.2463413e-20
 1.0000000e+00 5.8730812e-20], sum to 1.0000
[2019-04-04 08:13:05,253] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9959
[2019-04-04 08:13:05,265] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.783333333333333, 84.5, 0.0, 0.0, 26.0, 24.58400008269679, 0.1866279134998324, 0.0, 1.0, 40465.11093470645], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 535800.0000, 
sim time next is 536400.0000, 
raw observation next is [1.6, 85.0, 0.0, 0.0, 26.0, 24.55543412441036, 0.1816735689282826, 0.0, 1.0, 40547.69598100542], 
processed observation next is [0.0, 0.21739130434782608, 0.5069252077562327, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5462861770341968, 0.5605578563094276, 0.0, 1.0, 0.19308426657621627], 
reward next is 0.8069, 
noisyNet noise sample is [array([-0.43139258], dtype=float32), 0.31020913]. 
=============================================
[2019-04-04 08:13:10,520] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7212338e-11 1.2741803e-09 9.4022926e-26 1.5945591e-25 2.3944715e-20
 1.0000000e+00 2.7906886e-20], sum to 1.0000
[2019-04-04 08:13:10,520] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2602
[2019-04-04 08:13:10,577] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.2, 57.00000000000001, 60.16666666666666, 758.1666666666667, 26.0, 25.8029755751181, 0.3302344506606982, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 386400.0000, 
sim time next is 387000.0000, 
raw observation next is [-13.1, 55.5, 58.0, 764.0, 26.0, 25.72599114830975, 0.3223382364971495, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.0997229916897507, 0.555, 0.19333333333333333, 0.8441988950276244, 0.6666666666666666, 0.6438325956924791, 0.6074460788323831, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35458928], dtype=float32), -0.323218]. 
=============================================
[2019-04-04 08:13:10,604] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[78.02687 ]
 [78.41708 ]
 [78.62511 ]
 [78.959625]
 [79.22049 ]], R is [[77.9797821 ]
 [78.19998169]
 [78.41798401]
 [78.63380432]
 [78.84746552]].
[2019-04-04 08:13:20,524] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.23332353e-09 1.44503467e-08 1.16062517e-25 1.09574886e-23
 9.94304184e-19 1.00000000e+00 1.08108298e-18], sum to 1.0000
[2019-04-04 08:13:20,524] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4934
[2019-04-04 08:13:20,545] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.93170094363385, 0.1982951552063301, 0.0, 1.0, 42324.18104342073], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 681000.0000, 
sim time next is 681600.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.89289340311029, 0.1914518352003117, 0.0, 1.0, 42266.57408705408], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5744077835925241, 0.5638172784001039, 0.0, 1.0, 0.20126940041454325], 
reward next is 0.7987, 
noisyNet noise sample is [array([-0.78527564], dtype=float32), 0.0001928187]. 
=============================================
[2019-04-04 08:13:21,351] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4767111e-10 2.0668856e-09 1.3361919e-26 1.4309854e-24 3.4184796e-20
 1.0000000e+00 7.7972668e-20], sum to 1.0000
[2019-04-04 08:13:21,351] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0001
[2019-04-04 08:13:21,367] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.83406318772936, 0.2600412321948312, 0.0, 1.0, 41629.49310382274], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 856200.0000, 
sim time next is 856800.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.81203111527758, 0.2606556366044068, 0.0, 1.0, 41620.79765729181], 
processed observation next is [1.0, 0.9565217391304348, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5676692596064651, 0.5868852122014689, 0.0, 1.0, 0.19819427455853245], 
reward next is 0.8018, 
noisyNet noise sample is [array([1.0236927], dtype=float32), -0.11267409]. 
=============================================
[2019-04-04 08:13:27,849] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8433336e-12 2.2505509e-10 1.0575339e-28 2.9894531e-27 8.8954460e-22
 1.0000000e+00 1.1755502e-21], sum to 1.0000
[2019-04-04 08:13:27,878] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6003
[2019-04-04 08:13:27,908] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 48.33333333333334, 96.0, 719.0, 26.0, 25.31673349680495, 0.3605708548683313, 1.0, 1.0, 18680.50499731712], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 739200.0000, 
sim time next is 739800.0000, 
raw observation next is [0.5, 47.5, 89.0, 773.0, 26.0, 25.35882971036455, 0.382069213817876, 1.0, 1.0, 18680.87732155754], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.475, 0.2966666666666667, 0.8541436464088398, 0.6666666666666666, 0.6132358091970458, 0.6273564046059587, 1.0, 1.0, 0.08895655867408352], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.16973576], dtype=float32), 0.8951583]. 
=============================================
[2019-04-04 08:13:32,223] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5228378e-12 7.8772650e-10 3.4929918e-31 1.9710436e-28 5.8912025e-23
 1.0000000e+00 4.1684941e-23], sum to 1.0000
[2019-04-04 08:13:32,223] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0082
[2019-04-04 08:13:32,250] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.8, 86.33333333333334, 0.0, 0.0, 26.0, 25.48746379996748, 0.4579219987709814, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 976800.0000, 
sim time next is 977400.0000, 
raw observation next is [9.7, 88.0, 0.0, 0.0, 26.0, 25.48273706008815, 0.452817280662542, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.7313019390581719, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6235614216740126, 0.6509390935541807, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.80996513], dtype=float32), -0.49112982]. 
=============================================
[2019-04-04 08:13:33,759] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5899569e-11 2.7055383e-10 2.1231465e-30 3.0264894e-29 2.1518882e-23
 1.0000000e+00 1.5631771e-22], sum to 1.0000
[2019-04-04 08:13:33,760] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2931
[2019-04-04 08:13:33,795] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 76.66666666666667, 0.0, 0.0, 26.0, 25.50938651676638, 0.6034302538747894, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1026600.0000, 
sim time next is 1027200.0000, 
raw observation next is [14.4, 76.33333333333334, 0.0, 0.0, 26.0, 25.73072201518756, 0.619269484806908, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7633333333333334, 0.0, 0.0, 0.6666666666666666, 0.6442268345989633, 0.7064231616023027, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2735522], dtype=float32), 0.63539964]. 
=============================================
[2019-04-04 08:13:42,396] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-04 08:13:42,397] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:13:42,397] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:13:42,398] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:13:42,398] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:13:42,399] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:13:42,399] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:13:42,404] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run23
[2019-04-04 08:13:42,438] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run23
[2019-04-04 08:13:42,471] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run23
[2019-04-04 08:14:40,530] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.25058016], dtype=float32), 0.2638788]
[2019-04-04 08:14:40,530] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.267930045833333, 93.87033211833332, 0.0, 0.0, 26.0, 25.2680804800457, 0.3820414003181773, 0.0, 1.0, 38505.87876302595]
[2019-04-04 08:14:40,531] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 08:14:40,532] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.9237787e-11 4.5346332e-10 2.2374237e-29 3.4568112e-27 3.7717020e-22
 1.0000000e+00 3.1714319e-21], sampled 0.17141287229156532
[2019-04-04 08:16:33,520] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.25058016], dtype=float32), 0.2638788]
[2019-04-04 08:16:33,521] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.130891233, 57.08662486, 17.59145249, 583.61143275, 26.0, 26.43871839204137, 0.6274109041512449, 1.0, 1.0, 0.0]
[2019-04-04 08:16:33,521] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 08:16:33,522] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.17593347e-11 6.50139997e-10 8.31618187e-28 4.95484921e-26
 6.93674873e-21 1.00000000e+00 1.25079935e-20], sampled 0.7419206776620166
[2019-04-04 08:16:49,394] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.25058016], dtype=float32), 0.2638788]
[2019-04-04 08:16:49,394] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.2, 44.0, 206.5, 584.0, 26.0, 26.82716294933962, 0.6910590249395238, 1.0, 1.0, 0.0]
[2019-04-04 08:16:49,394] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:16:49,394] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.0646995e-12 2.5843080e-10 2.2320093e-30 2.2559195e-28 1.0273175e-22
 1.0000000e+00 1.6579804e-22], sampled 0.0670334119717142
[2019-04-04 08:16:51,476] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 08:17:20,679] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6417 263415234.9355 1551.9605
[2019-04-04 08:17:27,790] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 08:17:28,845] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 2200000, evaluation results [2200000.0, 7241.641738402564, 263415234.93545747, 1551.9605289518147, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 08:17:37,584] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1239080e-11 1.0528166e-09 1.2156231e-27 3.4378500e-26 3.2717654e-22
 1.0000000e+00 1.3345200e-20], sum to 1.0000
[2019-04-04 08:17:37,584] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2341
[2019-04-04 08:17:37,588] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [19.3, 49.83333333333334, 124.0, 0.0, 26.0, 27.74882959348164, 1.0030840572918, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1090200.0000, 
sim time next is 1090800.0000, 
raw observation next is [19.4, 49.0, 116.0, 0.0, 26.0, 27.80621957410134, 1.001892969901353, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.38666666666666666, 0.0, 0.6666666666666666, 0.8171849645084451, 0.8339643233004509, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47669926], dtype=float32), 0.20201182]. 
=============================================
[2019-04-04 08:17:47,650] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0922129e-11 3.4787076e-10 7.8984169e-29 5.7558878e-26 1.0074364e-21
 1.0000000e+00 2.0414388e-20], sum to 1.0000
[2019-04-04 08:17:47,651] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7807
[2019-04-04 08:17:47,700] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 83.33333333333334, 32.33333333333333, 0.0, 26.0, 25.90329205638411, 0.3995420555451148, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 836400.0000, 
sim time next is 837000.0000, 
raw observation next is [-3.9, 84.0, 29.0, 0.0, 26.0, 25.89962546925489, 0.2951061031947936, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.84, 0.09666666666666666, 0.0, 0.6666666666666666, 0.6583021224379074, 0.5983687010649312, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6346763], dtype=float32), 0.14169826]. 
=============================================
[2019-04-04 08:17:47,749] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[85.89333]
 [85.89828]
 [85.90735]
 [85.92828]
 [85.97117]], R is [[85.93598938]
 [86.07662964]
 [86.21586609]
 [86.35370636]
 [86.49017334]].
[2019-04-04 08:17:49,663] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3026290e-12 5.8650956e-10 3.5013698e-31 3.8141541e-29 8.0647956e-24
 1.0000000e+00 2.4126509e-22], sum to 1.0000
[2019-04-04 08:17:49,668] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1260
[2019-04-04 08:17:49,678] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.7, 82.0, 0.0, 0.0, 26.0, 25.67807029932417, 0.6162648320547074, 0.0, 1.0, 18724.41652088541], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1150200.0000, 
sim time next is 1150800.0000, 
raw observation next is [12.7, 82.66666666666667, 0.0, 0.0, 26.0, 25.70565306531518, 0.6152337300261607, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6421377554429316, 0.7050779100087202, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.59350455], dtype=float32), 0.6209769]. 
=============================================
[2019-04-04 08:17:52,761] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4815783e-11 8.1207613e-10 9.4384657e-29 5.2799427e-27 4.0972686e-22
 1.0000000e+00 1.7219968e-21], sum to 1.0000
[2019-04-04 08:17:52,766] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9165
[2019-04-04 08:17:52,784] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.24128194542043, 0.4589012404121981, 0.0, 1.0, 45629.28417774481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1396800.0000, 
sim time next is 1397400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.27506326244353, 0.4608830434588271, 0.0, 1.0, 41067.74435156401], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6062552718702943, 0.6536276811529423, 0.0, 1.0, 0.19556068738840005], 
reward next is 0.8044, 
noisyNet noise sample is [array([1.1157185], dtype=float32), 1.1109028]. 
=============================================
[2019-04-04 08:17:55,697] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8011838e-10 4.5334074e-09 1.2285404e-28 2.4658310e-26 1.6061615e-21
 1.0000000e+00 1.3404793e-20], sum to 1.0000
[2019-04-04 08:17:55,730] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8375
[2019-04-04 08:17:55,744] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.24129424529551, 0.4589079373845548, 0.0, 1.0, 45630.27990173167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1396800.0000, 
sim time next is 1397400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.27507672706881, 0.4608899176000655, 0.0, 1.0, 41068.09431772098], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6062563939224009, 0.6536299725333552, 0.0, 1.0, 0.1955623538939094], 
reward next is 0.8044, 
noisyNet noise sample is [array([-2.3042896], dtype=float32), 0.43065763]. 
=============================================
[2019-04-04 08:18:06,317] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2793772e-11 2.3892555e-10 3.3179434e-29 2.9954119e-27 1.3282369e-21
 1.0000000e+00 6.7446966e-22], sum to 1.0000
[2019-04-04 08:18:06,318] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9640
[2019-04-04 08:18:06,329] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.533333333333333, 74.66666666666667, 0.0, 0.0, 26.0, 25.39618207292975, 0.5592340403848649, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1628400.0000, 
sim time next is 1629000.0000, 
raw observation next is [7.45, 75.0, 0.0, 0.0, 26.0, 25.32673612871723, 0.5762518170695801, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.6689750692520776, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6105613440597691, 0.6920839390231933, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([-0.9060703], dtype=float32), 0.14921446]. 
=============================================
[2019-04-04 08:18:06,332] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.63068 ]
 [90.241234]
 [90.03408 ]
 [89.84073 ]
 [89.6549  ]], R is [[90.17206573]
 [89.33597565]
 [89.44261932]
 [89.54819489]
 [89.65271759]].
[2019-04-04 08:18:19,863] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.4248035e-10 3.3288421e-09 7.4835650e-25 8.4977075e-24 9.0580743e-20
 1.0000000e+00 1.4575501e-18], sum to 1.0000
[2019-04-04 08:18:19,865] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6769
[2019-04-04 08:18:19,889] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 84.0, 0.0, 0.0, 26.0, 24.81704498981393, 0.2515388164804643, 0.0, 1.0, 45746.00818535707], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1809000.0000, 
sim time next is 1809600.0000, 
raw observation next is [-5.0, 83.33333333333334, 0.0, 0.0, 26.0, 24.77856812176908, 0.2454401926755521, 0.0, 1.0, 45698.64618959244], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.56488067681409, 0.5818133975585174, 0.0, 1.0, 0.21761260090282114], 
reward next is 0.7824, 
noisyNet noise sample is [array([1.6765032], dtype=float32), 1.3710257]. 
=============================================
[2019-04-04 08:18:20,365] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7327491e-10 1.7252956e-08 1.6282310e-25 2.1614233e-23 5.8707546e-19
 1.0000000e+00 2.1524520e-18], sum to 1.0000
[2019-04-04 08:18:20,367] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5572
[2019-04-04 08:18:20,421] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 75.0, 152.0, 66.0, 26.0, 24.97494860511811, 0.2485409067456974, 0.0, 1.0, 27542.61801436358], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1854000.0000, 
sim time next is 1854600.0000, 
raw observation next is [-5.5, 74.33333333333333, 162.6666666666667, 71.0, 26.0, 24.97006703325474, 0.2491906919688205, 0.0, 1.0, 40513.2663979592], 
processed observation next is [0.0, 0.4782608695652174, 0.3102493074792244, 0.7433333333333333, 0.5422222222222224, 0.07845303867403315, 0.6666666666666666, 0.580838919437895, 0.5830635639896068, 0.0, 1.0, 0.1929203161807581], 
reward next is 0.8071, 
noisyNet noise sample is [array([0.39922807], dtype=float32), 0.49177313]. 
=============================================
[2019-04-04 08:18:24,905] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5529282e-10 2.6605225e-09 3.7864545e-27 3.3331735e-25 2.6391086e-20
 1.0000000e+00 7.0164859e-20], sum to 1.0000
[2019-04-04 08:18:24,905] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9742
[2019-04-04 08:18:24,928] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 86.33333333333333, 0.0, 0.0, 26.0, 25.07678731633122, 0.3891389999760537, 0.0, 1.0, 43279.59381557771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1743000.0000, 
sim time next is 1743600.0000, 
raw observation next is [-0.6, 85.66666666666667, 0.0, 0.0, 26.0, 25.05453702888274, 0.3846517031207497, 0.0, 1.0, 43325.28732294629], 
processed observation next is [0.0, 0.17391304347826086, 0.44598337950138506, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5878780857402285, 0.6282172343735832, 0.0, 1.0, 0.20631089201402997], 
reward next is 0.7937, 
noisyNet noise sample is [array([-1.5973551], dtype=float32), 1.3614638]. 
=============================================
[2019-04-04 08:18:39,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9719905e-10 6.8670731e-09 3.9612774e-25 1.2094921e-23 8.7400937e-19
 1.0000000e+00 1.2284435e-18], sum to 1.0000
[2019-04-04 08:18:39,842] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7699
[2019-04-04 08:18:39,886] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.01102720880633, 0.332258723262512, 0.0, 1.0, 79134.87633459154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1798200.0000, 
sim time next is 1798800.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.08115871112037, 0.3342703463961108, 0.0, 1.0, 57553.53650542837], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5900965592600308, 0.6114234487987036, 0.0, 1.0, 0.2740644595496589], 
reward next is 0.7259, 
noisyNet noise sample is [array([0.9579277], dtype=float32), -0.0826671]. 
=============================================
[2019-04-04 08:18:55,486] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3163910e-09 2.7067113e-09 6.3896846e-26 1.7440945e-24 5.6068060e-20
 1.0000000e+00 3.6638874e-19], sum to 1.0000
[2019-04-04 08:18:55,486] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9879
[2019-04-04 08:18:55,501] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 84.0, 0.0, 0.0, 26.0, 25.38110569888996, 0.4269763119006735, 0.0, 1.0, 47841.38395617245], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2064600.0000, 
sim time next is 2065200.0000, 
raw observation next is [-3.899999999999999, 83.33333333333334, 0.0, 0.0, 26.0, 25.36300401581893, 0.4251211244327135, 0.0, 1.0, 50063.96016326156], 
processed observation next is [1.0, 0.9130434782608695, 0.35457063711911363, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6135836679849108, 0.6417070414775712, 0.0, 1.0, 0.23839981030124552], 
reward next is 0.7616, 
noisyNet noise sample is [array([0.2544126], dtype=float32), 1.2080941]. 
=============================================
[2019-04-04 08:19:00,302] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4210911e-11 5.6728289e-10 6.9611944e-27 1.7191774e-25 1.1863401e-20
 1.0000000e+00 4.8734335e-20], sum to 1.0000
[2019-04-04 08:19:00,303] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5457
[2019-04-04 08:19:00,385] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 64.0, 150.0, 67.0, 26.0, 25.57202650785641, 0.2698663998387741, 1.0, 1.0, 53129.45100666853], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2120400.0000, 
sim time next is 2121000.0000, 
raw observation next is [-6.100000000000001, 64.66666666666667, 149.6666666666667, 44.66666666666666, 26.0, 25.55238431688923, 0.3902472389619956, 1.0, 1.0, 36962.03266160019], 
processed observation next is [1.0, 0.5652173913043478, 0.2936288088642659, 0.6466666666666667, 0.49888888888888905, 0.049355432780847135, 0.6666666666666666, 0.6293653597407692, 0.6300824129873318, 1.0, 1.0, 0.1760096793409533], 
reward next is 0.8240, 
noisyNet noise sample is [array([0.77624184], dtype=float32), -0.639571]. 
=============================================
[2019-04-04 08:19:00,420] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[79.089005]
 [79.492615]
 [79.676544]
 [79.872116]
 [80.79524 ]], R is [[78.72459412]
 [78.68434906]
 [78.44706726]
 [77.93714905]
 [78.15777588]].
[2019-04-04 08:19:03,947] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.7806928e-11 6.9371314e-10 7.2897854e-28 9.3763613e-26 7.5683294e-21
 1.0000000e+00 8.3330102e-21], sum to 1.0000
[2019-04-04 08:19:03,947] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1509
[2019-04-04 08:19:04,000] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.550000000000001, 82.5, 101.0, 39.0, 26.0, 25.59322600214106, 0.3054496590856162, 1.0, 1.0, 18737.60918249515], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2280600.0000, 
sim time next is 2281200.0000, 
raw observation next is [-7.266666666666667, 81.0, 113.8333333333333, 40.83333333333333, 26.0, 25.60375487123953, 0.3160806935487487, 1.0, 1.0, 18734.99220253694], 
processed observation next is [1.0, 0.391304347826087, 0.26131117266851345, 0.81, 0.3794444444444443, 0.04511970534069981, 0.6666666666666666, 0.6336462392699609, 0.6053602311829162, 1.0, 1.0, 0.08921424858350925], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.66729873], dtype=float32), -0.35429254]. 
=============================================
[2019-04-04 08:19:08,837] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.5902605e-11 2.8369826e-09 3.7265423e-27 4.0949341e-25 5.2881426e-21
 1.0000000e+00 6.2492397e-20], sum to 1.0000
[2019-04-04 08:19:08,838] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5838
[2019-04-04 08:19:08,879] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.583333333333333, 69.66666666666667, 0.0, 0.0, 26.0, 25.68372433882221, 0.4336369338036349, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2227800.0000, 
sim time next is 2228400.0000, 
raw observation next is [-4.6, 70.0, 0.0, 0.0, 26.0, 25.61017796806739, 0.3982682270720534, 0.0, 1.0, 19372.80898856057], 
processed observation next is [1.0, 0.8260869565217391, 0.33518005540166207, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6341814973389491, 0.6327560756906845, 0.0, 1.0, 0.09225147137409795], 
reward next is 0.9077, 
noisyNet noise sample is [array([-0.87961316], dtype=float32), 0.51543236]. 
=============================================
[2019-04-04 08:19:13,121] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5923415e-09 6.9640500e-08 1.8642647e-24 1.1762179e-22 1.6082599e-19
 9.9999988e-01 4.4154661e-18], sum to 1.0000
[2019-04-04 08:19:13,127] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7315
[2019-04-04 08:19:13,293] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 75.0, 0.0, 0.0, 26.0, 23.65786992108811, 0.04094790434246442, 1.0, 1.0, 202416.1885477095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2186400.0000, 
sim time next is 2187000.0000, 
raw observation next is [-5.6, 75.0, 0.0, 0.0, 26.0, 23.92722970640725, 0.1457399139942662, 1.0, 1.0, 202712.3337479954], 
processed observation next is [1.0, 0.30434782608695654, 0.30747922437673136, 0.75, 0.0, 0.0, 0.6666666666666666, 0.4939358088672708, 0.548579971331422, 1.0, 1.0, 0.9652968273714067], 
reward next is 0.0347, 
noisyNet noise sample is [array([0.05843023], dtype=float32), -1.5988312]. 
=============================================
[2019-04-04 08:19:13,301] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[75.69203 ]
 [74.094986]
 [74.175385]
 [74.22741 ]
 [74.29274 ]], R is [[76.70249939]
 [75.97158813]
 [76.01216888]
 [76.05239105]
 [76.09233093]].
[2019-04-04 08:19:22,430] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0384512e-10 1.6194142e-09 9.5714380e-26 4.0900633e-25 7.5644943e-20
 1.0000000e+00 2.2230655e-19], sum to 1.0000
[2019-04-04 08:19:22,432] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0780
[2019-04-04 08:19:22,498] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.9666666666666667, 38.66666666666667, 0.0, 0.0, 26.0, 25.1389980211511, 0.3447832313828448, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2575200.0000, 
sim time next is 2575800.0000, 
raw observation next is [-1.15, 40.0, 0.0, 0.0, 26.0, 25.10267519125579, 0.3278620009727205, 0.0, 1.0, 18709.45943026739], 
processed observation next is [1.0, 0.8260869565217391, 0.4307479224376732, 0.4, 0.0, 0.0, 0.6666666666666666, 0.5918895992713159, 0.6092873336575735, 0.0, 1.0, 0.08909266395365424], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.15052082], dtype=float32), -1.423585]. 
=============================================
[2019-04-04 08:19:27,451] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.0417249e-10 1.7885981e-08 3.6055598e-25 1.8829845e-23 4.7779172e-19
 1.0000000e+00 2.5922457e-18], sum to 1.0000
[2019-04-04 08:19:27,452] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8114
[2019-04-04 08:19:27,496] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.41089788569676, 0.4135741988862, 0.0, 1.0, 43958.37408230102], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2583600.0000, 
sim time next is 2584200.0000, 
raw observation next is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.3910486563515, 0.4117030541116891, 0.0, 1.0, 52161.68657750445], 
processed observation next is [1.0, 0.9130434782608695, 0.38504155124653744, 0.56, 0.0, 0.0, 0.6666666666666666, 0.6159207213626251, 0.6372343513705631, 0.0, 1.0, 0.24838898370240214], 
reward next is 0.7516, 
noisyNet noise sample is [array([-0.85008144], dtype=float32), -0.6405524]. 
=============================================
[2019-04-04 08:19:34,429] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2175520e-11 6.9344197e-10 1.2893923e-27 5.8508359e-26 1.0041139e-21
 1.0000000e+00 6.3657368e-21], sum to 1.0000
[2019-04-04 08:19:34,429] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6968
[2019-04-04 08:19:34,454] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 26.0, 25.30316416706269, 0.3148677602674409, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2553600.0000, 
sim time next is 2554200.0000, 
raw observation next is [3.0, 28.0, 171.0, 482.0, 26.0, 25.41841486330128, 0.3428289143208301, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.28, 0.57, 0.532596685082873, 0.6666666666666666, 0.61820123860844, 0.6142763047736101, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5936421], dtype=float32), -1.0844537]. 
=============================================
[2019-04-04 08:19:34,838] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.4721603e-11 2.2847899e-09 2.9759134e-27 2.3415867e-25 7.7427547e-21
 1.0000000e+00 1.7863778e-20], sum to 1.0000
[2019-04-04 08:19:34,845] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0608
[2019-04-04 08:19:34,886] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0406595e-10 4.1054391e-09 1.8924366e-26 2.5723383e-25 2.1786614e-20
 1.0000000e+00 1.8751957e-20], sum to 1.0000
[2019-04-04 08:19:34,886] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0578
[2019-04-04 08:19:34,913] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.3, 29.0, 92.0, 256.5, 26.0, 25.92266348156503, 0.3076393803705602, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2563200.0000, 
sim time next is 2563800.0000, 
raw observation next is [3.2, 29.0, 84.66666666666666, 225.0, 26.0, 25.48441536161936, 0.3352690249673107, 1.0, 1.0, 27885.25744827399], 
processed observation next is [1.0, 0.6956521739130435, 0.551246537396122, 0.29, 0.2822222222222222, 0.24861878453038674, 0.6666666666666666, 0.6237012801349465, 0.6117563416557702, 1.0, 1.0, 0.13278694022987614], 
reward next is 0.8672, 
noisyNet noise sample is [array([0.8801736], dtype=float32), -0.53054625]. 
=============================================
[2019-04-04 08:19:34,914] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.0, 68.0, 116.1666666666667, 691.8333333333334, 26.0, 26.08157625717502, 0.4731931763094314, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2716800.0000, 
sim time next is 2717400.0000, 
raw observation next is [-9.5, 66.0, 115.3333333333333, 709.6666666666666, 26.0, 26.07939679726842, 0.478966604797809, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.1994459833795014, 0.66, 0.3844444444444443, 0.7841620626151012, 0.6666666666666666, 0.6732830664390349, 0.659655534932603, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.95380634], dtype=float32), 1.7632191]. 
=============================================
[2019-04-04 08:19:39,887] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.8707440e-12 3.2455332e-09 1.6579636e-27 4.2813306e-26 9.5707325e-21
 1.0000000e+00 1.2035527e-19], sum to 1.0000
[2019-04-04 08:19:39,888] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4810
[2019-04-04 08:19:39,931] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.9, 76.33333333333334, 79.66666666666667, 15.16666666666666, 26.0, 25.70691299273188, 0.3579626825851049, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2623200.0000, 
sim time next is 2623800.0000, 
raw observation next is [-6.800000000000001, 75.66666666666666, 82.33333333333333, 30.33333333333333, 26.0, 25.86143795045314, 0.3669347187715784, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2742382271468144, 0.7566666666666666, 0.27444444444444444, 0.03351749539594843, 0.6666666666666666, 0.6551198292044283, 0.6223115729238594, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39356807], dtype=float32), -0.17343582]. 
=============================================
[2019-04-04 08:19:52,775] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.3257680e-12 1.3512470e-10 8.9309020e-31 2.7189379e-28 2.1516913e-23
 1.0000000e+00 1.2773962e-22], sum to 1.0000
[2019-04-04 08:19:52,776] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9489
[2019-04-04 08:19:52,816] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 100.0, 78.0, 27.0, 26.0, 25.92084114481721, 0.3538867630415987, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2908800.0000, 
sim time next is 2909400.0000, 
raw observation next is [2.0, 98.83333333333334, 75.66666666666666, 36.00000000000001, 26.0, 25.44042317157564, 0.3764351671068822, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.9883333333333334, 0.2522222222222222, 0.03977900552486189, 0.6666666666666666, 0.6200352642979702, 0.6254783890356274, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.75126195], dtype=float32), -0.20745619]. 
=============================================
[2019-04-04 08:19:53,399] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0261533e-09 6.9363941e-09 6.9861014e-25 4.5644166e-24 1.7522481e-19
 1.0000000e+00 5.2226530e-19], sum to 1.0000
[2019-04-04 08:19:53,399] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6502
[2019-04-04 08:19:53,412] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.9379890662936, 0.2699417999050932, 0.0, 1.0, 38076.03076267635], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3022200.0000, 
sim time next is 3022800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.91045394927001, 0.2615421604029159, 0.0, 1.0, 38017.40150498323], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5758711624391676, 0.5871807201343052, 0.0, 1.0, 0.1810352452618249], 
reward next is 0.8190, 
noisyNet noise sample is [array([0.27522933], dtype=float32), 0.3281402]. 
=============================================
[2019-04-04 08:20:04,193] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6497186e-11 3.0527744e-11 1.4956561e-30 3.2616365e-28 1.3417664e-23
 1.0000000e+00 3.5036553e-23], sum to 1.0000
[2019-04-04 08:20:04,193] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5098
[2019-04-04 08:20:04,223] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.833333333333333, 100.0, 0.0, 0.0, 26.0, 26.42567799738815, 0.7717000753151345, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3179400.0000, 
sim time next is 3180000.0000, 
raw observation next is [3.666666666666667, 100.0, 0.0, 0.0, 26.0, 26.31069454542537, 0.7485683200687863, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.564173591874423, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6925578787854475, 0.749522773356262, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3587389], dtype=float32), 0.8152849]. 
=============================================
[2019-04-04 08:20:04,233] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[95.075516]
 [94.799576]
 [94.50467 ]
 [94.23483 ]
 [94.957985]], R is [[95.42540741]
 [95.47115326]
 [95.51644135]
 [95.5612793 ]
 [95.60566711]].
[2019-04-04 08:20:10,937] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4318716e-11 7.7464024e-10 1.2042964e-27 3.9765064e-26 1.0116192e-20
 1.0000000e+00 3.1932729e-20], sum to 1.0000
[2019-04-04 08:20:10,937] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0791
[2019-04-04 08:20:10,958] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 83.83333333333334, 0.0, 0.0, 26.0, 25.41388097072504, 0.4859555137164617, 0.0, 1.0, 25628.82130828057], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2927400.0000, 
sim time next is 2928000.0000, 
raw observation next is [-1.0, 82.66666666666667, 0.0, 0.0, 26.0, 25.40926560482288, 0.4829261795215841, 0.0, 1.0, 36503.76861304296], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6174388004019068, 0.6609753931738614, 0.0, 1.0, 0.17382746958591885], 
reward next is 0.8262, 
noisyNet noise sample is [array([-0.7484828], dtype=float32), -1.7811979]. 
=============================================
[2019-04-04 08:20:10,967] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[83.16571 ]
 [83.767006]
 [84.39004 ]
 [84.95101 ]
 [85.78502 ]], R is [[82.73671722]
 [82.78730774]
 [82.77231598]
 [82.70189667]
 [82.56704712]].
[2019-04-04 08:20:11,279] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1373868e-10 3.3656650e-10 2.2717923e-28 6.7700182e-27 1.0002640e-21
 1.0000000e+00 6.5334978e-21], sum to 1.0000
[2019-04-04 08:20:11,279] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6746
[2019-04-04 08:20:11,329] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 79.16666666666667, 0.0, 0.0, 26.0, 24.95814174938439, 0.4062593651049748, 1.0, 1.0, 18966.80501071285], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2923800.0000, 
sim time next is 2924400.0000, 
raw observation next is [-1.0, 80.33333333333334, 0.0, 0.0, 26.0, 24.98391823548731, 0.4203640499073423, 0.0, 1.0, 171112.8289800145], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.8033333333333335, 0.0, 0.0, 0.6666666666666666, 0.581993186290609, 0.6401213499691141, 0.0, 1.0, 0.8148229951429262], 
reward next is 0.1852, 
noisyNet noise sample is [array([0.7071668], dtype=float32), 1.2966372]. 
=============================================
[2019-04-04 08:20:12,569] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.0079824e-10 1.1997090e-08 4.4641599e-25 4.8370848e-24 1.4350179e-19
 1.0000000e+00 4.1067103e-19], sum to 1.0000
[2019-04-04 08:20:12,570] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8402
[2019-04-04 08:20:12,640] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 61.5, 83.0, 359.0, 26.0, 24.05625922925497, 0.2164023290185836, 0.0, 1.0, 201798.0560552196], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3054600.0000, 
sim time next is 3055200.0000, 
raw observation next is [-6.0, 60.66666666666666, 85.66666666666667, 405.0, 26.0, 24.93344841003985, 0.2892589713573689, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.6066666666666666, 0.28555555555555556, 0.44751381215469616, 0.6666666666666666, 0.5777873675033209, 0.596419657119123, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5298805], dtype=float32), 0.2083965]. 
=============================================
[2019-04-04 08:20:14,097] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1477914e-09 1.1511639e-08 9.7827319e-26 3.4313939e-24 9.4373211e-20
 1.0000000e+00 2.9127715e-19], sum to 1.0000
[2019-04-04 08:20:14,097] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8238
[2019-04-04 08:20:14,158] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 77.0, 0.0, 0.0, 26.0, 25.04900055916687, 0.3998018811910963, 0.0, 1.0, 43632.00378346306], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3285000.0000, 
sim time next is 3285600.0000, 
raw observation next is [-7.0, 74.66666666666667, 0.0, 0.0, 26.0, 25.04732999048822, 0.4023332053568185, 0.0, 1.0, 43670.50567074386], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.5872774992073516, 0.6341110684522728, 0.0, 1.0, 0.2079547889083041], 
reward next is 0.7920, 
noisyNet noise sample is [array([-0.5790194], dtype=float32), 0.24681701]. 
=============================================
[2019-04-04 08:20:19,580] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8541916e-11 5.1253551e-10 1.3899643e-28 4.1172191e-27 1.3804412e-22
 1.0000000e+00 1.7950659e-21], sum to 1.0000
[2019-04-04 08:20:19,580] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7035
[2019-04-04 08:20:19,633] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 93.0, 0.0, 0.0, 26.0, 25.42314425096293, 0.392462385308785, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2914800.0000, 
sim time next is 2915400.0000, 
raw observation next is [1.166666666666667, 93.0, 0.0, 0.0, 26.0, 24.93144509750729, 0.3857143277759649, 1.0, 1.0, 150609.2410922645], 
processed observation next is [1.0, 0.7391304347826086, 0.49492151431209613, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5776204247922742, 0.6285714425919883, 1.0, 1.0, 0.7171868623441167], 
reward next is 0.2828, 
noisyNet noise sample is [array([-0.46491286], dtype=float32), -0.13277583]. 
=============================================
[2019-04-04 08:20:20,494] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.1937859e-12 7.3811585e-10 7.9020073e-28 4.6307408e-26 1.1811960e-21
 1.0000000e+00 2.4056125e-21], sum to 1.0000
[2019-04-04 08:20:20,504] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0988
[2019-04-04 08:20:20,539] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.166666666666666, 71.16666666666667, 106.3333333333333, 656.6666666666666, 26.0, 26.2633166147755, 0.5585577664561125, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3318600.0000, 
sim time next is 3319200.0000, 
raw observation next is [-8.0, 70.0, 107.5, 677.5, 26.0, 26.31585418423318, 0.5685221034737732, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.24099722991689754, 0.7, 0.35833333333333334, 0.7486187845303868, 0.6666666666666666, 0.6929878486860984, 0.6895073678245911, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7204051], dtype=float32), -1.4993321]. 
=============================================
[2019-04-04 08:20:28,884] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5944306e-10 5.1985589e-09 8.1072622e-26 2.2760039e-24 6.1111393e-20
 1.0000000e+00 1.6648404e-19], sum to 1.0000
[2019-04-04 08:20:28,887] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4396
[2019-04-04 08:20:28,915] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.36805573151265, 0.5902443239171239, 0.0, 1.0, 62022.36599482335], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3531000.0000, 
sim time next is 3531600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.66868748311508, 0.610649281789963, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6390572902595899, 0.7035497605966543, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10316496], dtype=float32), -0.6039524]. 
=============================================
[2019-04-04 08:20:35,042] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0959854e-09 1.4730043e-08 4.5149191e-26 9.3452328e-24 4.4172499e-19
 1.0000000e+00 1.0678645e-18], sum to 1.0000
[2019-04-04 08:20:35,044] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6361
[2019-04-04 08:20:35,060] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.333333333333333, 62.33333333333333, 0.0, 0.0, 26.0, 25.75056745716137, 0.4580863608525614, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3703200.0000, 
sim time next is 3703800.0000, 
raw observation next is [2.166666666666667, 62.16666666666667, 0.0, 0.0, 26.0, 25.72266101161913, 0.4454409806525463, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.5226223453370269, 0.6216666666666667, 0.0, 0.0, 0.6666666666666666, 0.6435550843015942, 0.6484803268841821, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.70772403], dtype=float32), -1.1384887]. 
=============================================
[2019-04-04 08:20:43,668] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.5239254e-10 1.6840784e-09 5.5041358e-26 6.2015378e-25 1.4668308e-19
 1.0000000e+00 1.3430033e-19], sum to 1.0000
[2019-04-04 08:20:43,670] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4039
[2019-04-04 08:20:43,696] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333333, 68.66666666666666, 0.0, 0.0, 26.0, 25.10038256035005, 0.4461173675141495, 0.0, 1.0, 91031.95761190004], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3527400.0000, 
sim time next is 3528000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.03506243903738, 0.456141600769205, 0.0, 1.0, 103012.3548216757], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5862552032531149, 0.6520472002564016, 0.0, 1.0, 0.4905350229603605], 
reward next is 0.5095, 
noisyNet noise sample is [array([1.8199632], dtype=float32), -0.9547157]. 
=============================================
[2019-04-04 08:20:43,701] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[79.3547 ]
 [79.53218]
 [79.92931]
 [80.6365 ]
 [81.28499]], R is [[78.93775177]
 [78.71488953]
 [78.6418457 ]
 [78.71424103]
 [78.78423309]].
[2019-04-04 08:20:43,768] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2631645e-09 2.6981615e-08 5.5781622e-25 2.9309669e-23 9.7904953e-19
 1.0000000e+00 1.8396796e-17], sum to 1.0000
[2019-04-04 08:20:43,768] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7096
[2019-04-04 08:20:43,781] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.666666666666666, 70.0, 17.16666666666666, 171.6666666666667, 26.0, 24.24885440202508, 0.1925034391985604, 0.0, 1.0, 41553.57934043268], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3570000.0000, 
sim time next is 3570600.0000, 
raw observation next is [-6.833333333333334, 70.0, 31.33333333333333, 222.3333333333333, 26.0, 24.22195260215564, 0.1953863411925358, 0.0, 1.0, 41508.15817161274], 
processed observation next is [0.0, 0.30434782608695654, 0.27331486611265005, 0.7, 0.10444444444444442, 0.24567219152854508, 0.6666666666666666, 0.5184960501796366, 0.5651287803975119, 0.0, 1.0, 0.19765789605529877], 
reward next is 0.8023, 
noisyNet noise sample is [array([0.4861622], dtype=float32), -0.9016093]. 
=============================================
[2019-04-04 08:20:44,457] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8713441e-09 4.5508379e-09 1.8815896e-25 2.4867854e-24 9.5480185e-20
 1.0000000e+00 6.2191114e-19], sum to 1.0000
[2019-04-04 08:20:44,460] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8310
[2019-04-04 08:20:44,493] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.45998991698313, 0.5496201965105914, 0.0, 1.0, 18758.38164189698], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3790200.0000, 
sim time next is 3790800.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.5900430683939, 0.5529338201839545, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.632503589032825, 0.6843112733946515, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31224772], dtype=float32), -0.4951464]. 
=============================================
[2019-04-04 08:20:49,931] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.01625611e-10 8.50958060e-09 7.41600318e-28 1.14106770e-25
 2.14290524e-20 1.00000000e+00 1.01120684e-19], sum to 1.0000
[2019-04-04 08:20:49,935] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4015
[2019-04-04 08:20:49,956] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.666666666666666, 43.0, 116.3333333333333, 827.1666666666667, 26.0, 25.33525357544618, 0.4550740718852497, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3674400.0000, 
sim time next is 3675000.0000, 
raw observation next is [4.833333333333334, 42.5, 115.6666666666667, 825.3333333333334, 26.0, 25.30729201044385, 0.4530633314294641, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5964912280701755, 0.425, 0.38555555555555565, 0.9119705340699816, 0.6666666666666666, 0.6089410008703208, 0.651021110476488, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4769926], dtype=float32), -0.2974593]. 
=============================================
[2019-04-04 08:20:49,964] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[84.98814]
 [85.20333]
 [85.4253 ]
 [85.63424]
 [85.84773]], R is [[84.92310333]
 [85.07387543]
 [85.2231369 ]
 [85.37090302]
 [85.51719666]].
[2019-04-04 08:20:50,323] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1318288e-11 9.5912711e-10 1.9889974e-28 5.4066377e-27 8.8954850e-21
 1.0000000e+00 1.2585572e-21], sum to 1.0000
[2019-04-04 08:20:50,324] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0017
[2019-04-04 08:20:50,353] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.0, 27.66666666666667, 104.3333333333333, 696.3333333333334, 26.0, 25.68254718263945, 0.4661755660913031, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3664200.0000, 
sim time next is 3664800.0000, 
raw observation next is [11.0, 28.0, 106.0, 713.0, 26.0, 25.66735606404694, 0.4686034195709179, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.7673130193905818, 0.28, 0.35333333333333333, 0.7878453038674034, 0.6666666666666666, 0.6389463386705782, 0.6562011398569726, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4374492], dtype=float32), -0.040877447]. 
=============================================
[2019-04-04 08:20:52,820] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3565983e-11 1.8621220e-09 2.7537191e-27 4.0152539e-26 7.0233111e-21
 1.0000000e+00 2.4858611e-20], sum to 1.0000
[2019-04-04 08:20:52,820] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9334
[2019-04-04 08:20:52,877] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 41.5, 116.0, 824.0, 26.0, 25.01521995749058, 0.5615334987313676, 1.0, 1.0, 197455.1691209885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3936600.0000, 
sim time next is 3937200.0000, 
raw observation next is [-5.333333333333333, 40.33333333333334, 114.1666666666667, 818.0, 26.0, 25.7304286968874, 0.6525017061289485, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3148661126500462, 0.40333333333333343, 0.38055555555555565, 0.9038674033149171, 0.6666666666666666, 0.6442023914072834, 0.7175005687096495, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4531157], dtype=float32), 0.1767914]. 
=============================================
[2019-04-04 08:20:56,822] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2508285e-10 6.5703283e-09 1.2320575e-25 2.4413766e-24 8.0832923e-20
 1.0000000e+00 4.9111719e-19], sum to 1.0000
[2019-04-04 08:20:56,822] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2024
[2019-04-04 08:20:56,870] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 74.0, 0.0, 0.0, 26.0, 25.22037803129468, 0.4140022456962431, 0.0, 1.0, 43839.87055760241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3803400.0000, 
sim time next is 3804000.0000, 
raw observation next is [-3.666666666666667, 75.0, 0.0, 0.0, 26.0, 25.28845096176005, 0.4103251677056098, 0.0, 1.0, 43879.0863943887], 
processed observation next is [1.0, 0.0, 0.3610341643582641, 0.75, 0.0, 0.0, 0.6666666666666666, 0.607370913480004, 0.6367750559018699, 0.0, 1.0, 0.20894803044947], 
reward next is 0.7911, 
noisyNet noise sample is [array([1.0518255], dtype=float32), 2.336991]. 
=============================================
[2019-04-04 08:20:56,890] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.072945]
 [80.13388 ]
 [80.071724]
 [79.97686 ]
 [80.09756 ]], R is [[80.15225983]
 [80.1419754 ]
 [80.13202667]
 [80.12241364]
 [80.11306763]].
[2019-04-04 08:20:57,038] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1403103e-11 4.1157175e-09 2.6184629e-26 1.9253444e-24 1.2950960e-19
 1.0000000e+00 2.6607410e-20], sum to 1.0000
[2019-04-04 08:20:57,038] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9450
[2019-04-04 08:20:57,075] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.166666666666666, 40.66666666666667, 114.3333333333333, 792.6666666666667, 26.0, 26.56654887755948, 0.5868880215437026, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4013400.0000, 
sim time next is 4014000.0000, 
raw observation next is [-8.0, 40.0, 115.5, 798.5, 26.0, 26.56591163649881, 0.5814053621988381, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.24099722991689754, 0.4, 0.385, 0.8823204419889503, 0.6666666666666666, 0.7138259697082342, 0.6938017873996127, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.62218624], dtype=float32), -0.6690293]. 
=============================================
[2019-04-04 08:20:57,079] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.88634]
 [78.95967]
 [78.96257]
 [79.08551]
 [79.22575]], R is [[79.05021667]
 [79.25971222]
 [79.46711731]
 [79.6724472 ]
 [79.87572479]].
[2019-04-04 08:21:10,366] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.7436852e-10 2.8090374e-09 2.0928285e-25 3.3856149e-23 1.2095644e-18
 1.0000000e+00 1.0446208e-18], sum to 1.0000
[2019-04-04 08:21:10,367] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6465
[2019-04-04 08:21:10,375] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 25.66666666666667, 13.33333333333334, 128.6666666666667, 26.0, 25.70267603473182, 0.5128236285708914, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4038600.0000, 
sim time next is 4039200.0000, 
raw observation next is [-3.0, 26.0, 0.0, 0.0, 26.0, 25.73403952434404, 0.4934035773660245, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.26, 0.0, 0.0, 0.6666666666666666, 0.6445032936953368, 0.6644678591220082, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23838584], dtype=float32), -0.9071139]. 
=============================================
[2019-04-04 08:21:13,605] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8894565e-12 3.7580793e-11 4.4990043e-31 1.4729079e-28 5.2699699e-24
 1.0000000e+00 1.3279575e-22], sum to 1.0000
[2019-04-04 08:21:13,606] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7683
[2019-04-04 08:21:13,629] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.46666666666667, 58.66666666666666, 0.0, 0.0, 26.0, 26.96380224841894, 0.8686060575517048, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4394400.0000, 
sim time next is 4395000.0000, 
raw observation next is [10.33333333333333, 58.83333333333334, 0.0, 0.0, 26.0, 26.92722722489847, 0.8566673429806034, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7488457987072946, 0.5883333333333334, 0.0, 0.0, 0.6666666666666666, 0.7439356020748725, 0.7855557809935344, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0145444], dtype=float32), 0.21540934]. 
=============================================
[2019-04-04 08:21:13,646] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[92.95318]
 [93.80951]
 [94.69036]
 [95.63123]
 [95.6701 ]], R is [[92.44046783]
 [92.5160675 ]
 [92.59090424]
 [92.66499329]
 [92.73834229]].
[2019-04-04 08:21:21,013] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.64652933e-11 4.89242036e-10 1.14253755e-29 1.37723982e-27
 1.18455323e-21 1.00000000e+00 1.26680338e-21], sum to 1.0000
[2019-04-04 08:21:21,015] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5401
[2019-04-04 08:21:21,028] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.05, 62.5, 0.0, 0.0, 26.0, 25.64159423997291, 0.5986162173265589, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4404600.0000, 
sim time next is 4405200.0000, 
raw observation next is [7.9, 62.66666666666667, 0.0, 0.0, 26.0, 25.62174092010765, 0.621365800175056, 0.0, 1.0, 143190.9370672884], 
processed observation next is [1.0, 1.0, 0.6814404432132966, 0.6266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6351450766756376, 0.7071219333916853, 0.0, 1.0, 0.6818616050823257], 
reward next is 0.3181, 
noisyNet noise sample is [array([-0.710589], dtype=float32), -0.49519315]. 
=============================================
[2019-04-04 08:21:31,839] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7266843e-12 6.0962894e-11 3.1767224e-29 2.7657733e-27 4.7996120e-22
 1.0000000e+00 6.3094524e-22], sum to 1.0000
[2019-04-04 08:21:31,840] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9592
[2019-04-04 08:21:31,867] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.6, 47.5, 40.0, 145.0, 26.0, 27.31500589531953, 0.77448369262021, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4642200.0000, 
sim time next is 4642800.0000, 
raw observation next is [4.4, 48.0, 33.33333333333334, 120.8333333333333, 26.0, 26.96406914680888, 0.629585531576426, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5844875346260389, 0.48, 0.11111111111111115, 0.1335174953959484, 0.6666666666666666, 0.7470057622340732, 0.7098618438588087, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19152878], dtype=float32), 1.5918075]. 
=============================================
[2019-04-04 08:21:35,963] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7317435e-09 3.8754717e-08 6.0990603e-25 1.2391572e-23 2.7702299e-19
 1.0000000e+00 3.1088738e-18], sum to 1.0000
[2019-04-04 08:21:35,963] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4401
[2019-04-04 08:21:36,008] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 49.5, 0.0, 0.0, 26.0, 24.7448990014138, 0.2359197752901885, 0.0, 1.0, 39838.11522436859], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4170600.0000, 
sim time next is 4171200.0000, 
raw observation next is [-4.666666666666666, 49.33333333333333, 0.0, 0.0, 26.0, 24.70766354644067, 0.227071722446303, 0.0, 1.0, 39941.03219577109], 
processed observation next is [0.0, 0.2608695652173913, 0.33333333333333337, 0.4933333333333333, 0.0, 0.0, 0.6666666666666666, 0.5589719622033892, 0.5756905741487677, 0.0, 1.0, 0.19019539140843378], 
reward next is 0.8098, 
noisyNet noise sample is [array([-2.1416113], dtype=float32), 0.32210362]. 
=============================================
[2019-04-04 08:21:57,308] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1300249e-12 1.5067582e-10 4.0363023e-29 1.0845482e-26 1.2317962e-22
 1.0000000e+00 1.4477011e-21], sum to 1.0000
[2019-04-04 08:21:57,308] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2736
[2019-04-04 08:21:57,378] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 78.0, 56.33333333333333, 0.0, 26.0, 26.30516934017589, 0.6084701117733079, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4464600.0000, 
sim time next is 4465200.0000, 
raw observation next is [0.0, 78.0, 52.66666666666666, 0.0, 26.0, 26.28517520741781, 0.6023588582923595, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.17555555555555552, 0.0, 0.6666666666666666, 0.6904312672848176, 0.7007862860974532, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([3.0693514], dtype=float32), -0.88030374]. 
=============================================
[2019-04-04 08:21:58,290] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-04 08:21:58,301] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:21:58,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:21:58,303] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run24
[2019-04-04 08:21:58,394] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:21:58,394] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:21:58,397] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:21:58,397] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:21:58,399] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run24
[2019-04-04 08:21:58,520] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run24
[2019-04-04 08:25:04,285] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 08:25:36,502] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 08:25:41,427] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 08:25:42,464] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 2300000, evaluation results [2300000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 08:25:52,824] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1365004e-09 6.2203416e-09 6.2206583e-25 3.5610027e-23 2.0163329e-19
 1.0000000e+00 4.5218469e-19], sum to 1.0000
[2019-04-04 08:25:52,824] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2897
[2019-04-04 08:25:52,939] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 48.0, 0.0, 0.0, 26.0, 25.15291666753146, 0.2653484409803434, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4951800.0000, 
sim time next is 4952400.0000, 
raw observation next is [-2.333333333333333, 47.33333333333333, 15.5, 93.33333333333331, 26.0, 25.35134039786504, 0.2674025039626997, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3979686057248385, 0.4733333333333333, 0.051666666666666666, 0.1031307550644567, 0.6666666666666666, 0.6126116998220867, 0.5891341679875666, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17669877], dtype=float32), -1.657838]. 
=============================================
[2019-04-04 08:26:04,054] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:04,054] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:04,070] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run18
[2019-04-04 08:26:05,752] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8130586e-10 8.7038057e-09 5.1928156e-26 4.0607759e-24 1.4733360e-19
 1.0000000e+00 1.7806637e-19], sum to 1.0000
[2019-04-04 08:26:05,752] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9814
[2019-04-04 08:26:05,823] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 53.33333333333334, 0.0, 0.0, 26.0, 25.33496479171718, 0.3964864014362783, 0.0, 1.0, 48472.66366321658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5026800.0000, 
sim time next is 5027400.0000, 
raw observation next is [-1.0, 52.5, 0.0, 0.0, 26.0, 25.40495229256869, 0.3955630203523081, 0.0, 1.0, 24020.93411730944], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.525, 0.0, 0.0, 0.6666666666666666, 0.6170793577140575, 0.631854340117436, 0.0, 1.0, 0.11438540055861637], 
reward next is 0.8856, 
noisyNet noise sample is [array([-0.35431132], dtype=float32), -0.74681437]. 
=============================================
[2019-04-04 08:26:06,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:06,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:06,566] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run18
[2019-04-04 08:26:11,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:11,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:11,322] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run18
[2019-04-04 08:26:12,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:12,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:12,724] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run18
[2019-04-04 08:26:15,382] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:15,382] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:15,387] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run18
[2019-04-04 08:26:15,511] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:15,511] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:15,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run18
[2019-04-04 08:26:15,930] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0488410e-11 8.1589541e-10 4.8197981e-28 4.4330493e-27 8.1501322e-22
 1.0000000e+00 1.2885603e-21], sum to 1.0000
[2019-04-04 08:26:15,930] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1620
[2019-04-04 08:26:15,978] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666667, 31.5, 111.0, 745.6666666666667, 26.0, 26.33186565815473, 0.5173229189890091, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4960200.0000, 
sim time next is 4960800.0000, 
raw observation next is [1.0, 30.0, 112.5, 760.0, 26.0, 26.43286376580593, 0.5375894342753825, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.3, 0.375, 0.8397790055248618, 0.6666666666666666, 0.7027386471504942, 0.6791964780917942, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32794005], dtype=float32), -1.3473878]. 
=============================================
[2019-04-04 08:26:16,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:16,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:16,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run18
[2019-04-04 08:26:26,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:26,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:26,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run18
[2019-04-04 08:26:26,535] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:26,535] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:26,539] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run18
[2019-04-04 08:26:26,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:26,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:26,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run18
[2019-04-04 08:26:28,060] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:28,060] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:28,086] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run18
[2019-04-04 08:26:30,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:30,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:30,488] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run18
[2019-04-04 08:26:35,559] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:35,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:35,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run18
[2019-04-04 08:26:39,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:39,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:39,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run18
[2019-04-04 08:26:40,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:40,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:40,324] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run18
[2019-04-04 08:26:42,613] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7991264e-11 1.2119254e-09 1.3745420e-27 3.5933393e-25 2.6361089e-21
 1.0000000e+00 1.2052252e-19], sum to 1.0000
[2019-04-04 08:26:42,613] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5071
[2019-04-04 08:26:42,628] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 91.66666666666667, 0.0, 0.0, 26.0, 24.35419017580823, 0.1528319096119378, 0.0, 1.0, 40551.69227925447], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 89400.0000, 
sim time next is 90000.0000, 
raw observation next is [-0.6, 91.0, 0.0, 0.0, 26.0, 24.3714303086828, 0.1490428931985083, 0.0, 1.0, 40767.50113871512], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5309525257235667, 0.5496809643995028, 0.0, 1.0, 0.1941309578034053], 
reward next is 0.8059, 
noisyNet noise sample is [array([1.0078236], dtype=float32), 1.2172369]. 
=============================================
[2019-04-04 08:26:42,654] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[84.79137]
 [84.86117]
 [84.74751]
 [84.77571]
 [84.77255]], R is [[84.81139374]
 [84.77017975]
 [84.73021698]
 [84.69114685]
 [84.65286255]].
[2019-04-04 08:26:42,988] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6234848e-09 1.2047263e-08 3.2897193e-25 1.3175673e-23 2.3932858e-19
 1.0000000e+00 5.5957833e-19], sum to 1.0000
[2019-04-04 08:26:42,988] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2763
[2019-04-04 08:26:43,031] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.85, 74.5, 0.0, 0.0, 26.0, 23.47807391872512, -0.04035781000599455, 0.0, 1.0, 44525.69669653519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 106200.0000, 
sim time next is 106800.0000, 
raw observation next is [-6.133333333333334, 74.66666666666667, 0.0, 0.0, 26.0, 23.43859308736714, -0.05244801096806027, 0.0, 1.0, 44693.63480630574], 
processed observation next is [1.0, 0.21739130434782608, 0.2927054478301016, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.4532160906139282, 0.48251732967731326, 0.0, 1.0, 0.21282683241097972], 
reward next is 0.7872, 
noisyNet noise sample is [array([0.5895251], dtype=float32), 0.1268266]. 
=============================================
[2019-04-04 08:26:45,306] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.7890145e-10 6.5337868e-09 2.7436231e-26 1.2054428e-24 1.1850290e-19
 1.0000000e+00 3.6385659e-19], sum to 1.0000
[2019-04-04 08:26:45,307] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9083
[2019-04-04 08:26:45,326] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 47.33333333333333, 0.0, 0.0, 26.0, 25.47871707650857, 0.3777997188633133, 0.0, 1.0, 29969.15941826757], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5031600.0000, 
sim time next is 5032200.0000, 
raw observation next is [-1.0, 46.66666666666667, 0.0, 0.0, 26.0, 25.46156254295531, 0.3657493256209017, 0.0, 1.0, 42170.72044389302], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 0.46666666666666673, 0.0, 0.0, 0.6666666666666666, 0.6217968785796092, 0.6219164418736339, 0.0, 1.0, 0.20081295449472866], 
reward next is 0.7992, 
noisyNet noise sample is [array([0.67730916], dtype=float32), 0.887161]. 
=============================================
[2019-04-04 08:26:50,978] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:26:50,979] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:50,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run18
[2019-04-04 08:26:55,291] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.3522799e-11 2.8582137e-09 1.8230791e-26 8.6894370e-25 2.7772117e-20
 1.0000000e+00 2.4173148e-20], sum to 1.0000
[2019-04-04 08:26:55,291] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3314
[2019-04-04 08:26:55,368] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 70.83333333333333, 143.3333333333333, 0.0, 26.0, 25.29802695630258, 0.2253997364919033, 1.0, 1.0, 26655.83791648593], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 213000.0000, 
sim time next is 213600.0000, 
raw observation next is [-5.800000000000001, 69.66666666666667, 148.1666666666667, 0.0, 26.0, 25.30769052402218, 0.2243182312713306, 1.0, 1.0, 25955.21375903906], 
processed observation next is [1.0, 0.4782608695652174, 0.30193905817174516, 0.6966666666666668, 0.49388888888888904, 0.0, 0.6666666666666666, 0.6089742103351817, 0.5747727437571102, 1.0, 1.0, 0.12359625599542409], 
reward next is 0.8764, 
noisyNet noise sample is [array([-0.57405543], dtype=float32), 0.18403865]. 
=============================================
[2019-04-04 08:27:05,730] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2552424e-10 1.4185455e-08 1.1406157e-24 1.7903798e-23 5.4895402e-19
 1.0000000e+00 1.1199580e-18], sum to 1.0000
[2019-04-04 08:27:05,730] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6408
[2019-04-04 08:27:05,794] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 43.0, 82.0, 623.0, 26.0, 26.3462166629834, 0.4094056677218972, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 311400.0000, 
sim time next is 312000.0000, 
raw observation next is [-9.5, 42.66666666666667, 80.0, 598.6666666666667, 26.0, 25.74376134984213, 0.4019591930904604, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4266666666666667, 0.26666666666666666, 0.661510128913444, 0.6666666666666666, 0.6453134458201776, 0.6339863976968202, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2837527], dtype=float32), 0.49806282]. 
=============================================
[2019-04-04 08:27:05,801] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.46762 ]
 [76.8434  ]
 [77.174545]
 [77.61373 ]
 [77.963745]], R is [[76.34184265]
 [76.57842255]
 [76.81263733]
 [77.04450989]
 [77.27406311]].
[2019-04-04 08:27:11,023] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.3345345e-11 5.8375960e-10 1.3104212e-28 1.7570678e-26 1.3611674e-21
 1.0000000e+00 2.3509535e-21], sum to 1.0000
[2019-04-04 08:27:11,027] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7773
[2019-04-04 08:27:11,074] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.966666666666667, 86.66666666666666, 0.0, 0.0, 26.0, 24.82516076309591, 0.2303674896883714, 0.0, 1.0, 39739.29136822684], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 528000.0000, 
sim time next is 528600.0000, 
raw observation next is [3.883333333333333, 86.33333333333334, 0.0, 0.0, 26.0, 24.80507767676671, 0.2273329332063967, 0.0, 1.0, 39775.55235472529], 
processed observation next is [0.0, 0.08695652173913043, 0.5701754385964913, 0.8633333333333334, 0.0, 0.0, 0.6666666666666666, 0.567089806397226, 0.5757776444021322, 0.0, 1.0, 0.1894073921653585], 
reward next is 0.8106, 
noisyNet noise sample is [array([-0.96348596], dtype=float32), -0.66729623]. 
=============================================
[2019-04-04 08:27:15,021] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0859729e-10 5.2465916e-09 1.6255500e-25 5.4258997e-25 1.3272506e-19
 1.0000000e+00 5.3507661e-20], sum to 1.0000
[2019-04-04 08:27:15,025] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5389
[2019-04-04 08:27:15,083] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.9, 52.5, 58.0, 811.0, 26.0, 25.69511346431823, 0.3989267782083425, 1.0, 1.0, 145733.7396287627], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 388200.0000, 
sim time next is 388800.0000, 
raw observation next is [-12.8, 51.0, 58.0, 834.5, 26.0, 25.88701736614201, 0.2803974094616202, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.1080332409972299, 0.51, 0.19333333333333333, 0.9220994475138121, 0.6666666666666666, 0.6572514471785009, 0.5934658031538734, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.83836585], dtype=float32), 2.299893]. 
=============================================
[2019-04-04 08:27:15,670] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0319635e-08 9.5074412e-08 3.1956940e-22 6.9493818e-21 5.4951600e-17
 9.9999988e-01 1.5794315e-16], sum to 1.0000
[2019-04-04 08:27:15,688] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7097
[2019-04-04 08:27:15,711] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.1, 68.0, 0.0, 0.0, 26.0, 22.54336135718775, -0.2725876896612096, 0.0, 1.0, 47882.79125333548], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 283200.0000, 
sim time next is 283800.0000, 
raw observation next is [-12.2, 67.5, 0.0, 0.0, 26.0, 22.49436981691802, -0.271494243364201, 0.0, 1.0, 47923.38660380243], 
processed observation next is [1.0, 0.2608695652173913, 0.12465373961218838, 0.675, 0.0, 0.0, 0.6666666666666666, 0.3745308180765017, 0.4095019188785997, 0.0, 1.0, 0.22820660287524966], 
reward next is 0.7718, 
noisyNet noise sample is [array([0.57249105], dtype=float32), 0.6353578]. 
=============================================
[2019-04-04 08:27:18,442] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.7873639e-10 1.2666861e-08 5.0757425e-24 8.5669417e-23 6.8127407e-19
 1.0000000e+00 3.0673960e-18], sum to 1.0000
[2019-04-04 08:27:18,444] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2697
[2019-04-04 08:27:18,501] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.916666666666668, 41.66666666666666, 0.0, 0.0, 26.0, 25.10305260667086, 0.2989271332309922, 1.0, 1.0, 56136.2461883214], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 417000.0000, 
sim time next is 417600.0000, 
raw observation next is [-10.0, 42.0, 0.0, 0.0, 26.0, 25.12013806097653, 0.296165886812644, 0.0, 1.0, 48726.39333218463], 
processed observation next is [1.0, 0.8695652173913043, 0.18559556786703602, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5933448384147108, 0.5987219622708814, 0.0, 1.0, 0.23203044443897441], 
reward next is 0.7680, 
noisyNet noise sample is [array([1.4050431], dtype=float32), -0.032102562]. 
=============================================
[2019-04-04 08:27:20,159] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6797735e-09 5.5167988e-08 3.8341933e-24 6.6844236e-23 3.7784349e-18
 1.0000000e+00 2.2432831e-17], sum to 1.0000
[2019-04-04 08:27:20,159] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5786
[2019-04-04 08:27:20,179] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 67.5, 0.0, 0.0, 26.0, 23.63980803546939, -0.04189697248292201, 0.0, 1.0, 43757.3155629233], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 629400.0000, 
sim time next is 630000.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 26.0, 23.60817816609236, -0.048570189593965, 0.0, 1.0, 43801.75141282743], 
processed observation next is [0.0, 0.30434782608695654, 0.3379501385041552, 0.68, 0.0, 0.0, 0.6666666666666666, 0.46734818050769683, 0.4838099368020117, 0.0, 1.0, 0.20857976863251157], 
reward next is 0.7914, 
noisyNet noise sample is [array([-0.45849735], dtype=float32), -1.7850518]. 
=============================================
[2019-04-04 08:27:20,203] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[75.1078 ]
 [75.24148]
 [75.39083]
 [75.55241]
 [75.70931]], R is [[75.0329361 ]
 [75.07424164]
 [75.11523438]
 [75.15576172]
 [75.19573212]].
[2019-04-04 08:27:45,625] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0992100e-10 2.2003310e-09 3.2717912e-29 3.1945408e-27 4.0683465e-22
 1.0000000e+00 1.7048263e-21], sum to 1.0000
[2019-04-04 08:27:45,639] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9315
[2019-04-04 08:27:45,701] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 60.0, 91.83333333333333, 724.0, 26.0, 25.85742940678581, 0.391733178831867, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 733200.0000, 
sim time next is 733800.0000, 
raw observation next is [-0.6, 58.5, 99.66666666666666, 669.0, 26.0, 25.86301252543569, 0.3895616733027364, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.585, 0.3322222222222222, 0.7392265193370166, 0.6666666666666666, 0.6552510437863074, 0.6298538911009121, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14533262], dtype=float32), 0.13151744]. 
=============================================
[2019-04-04 08:27:49,812] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.13146505e-13 3.76279875e-11 4.62975283e-33 2.70051372e-31
 3.53304447e-25 1.00000000e+00 6.17370877e-25], sum to 1.0000
[2019-04-04 08:27:49,812] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4951
[2019-04-04 08:27:49,827] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.86666666666667, 90.66666666666667, 90.0, 0.0, 26.0, 26.56876671610129, 0.6489280183254277, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 987600.0000, 
sim time next is 988200.0000, 
raw observation next is [11.05, 89.5, 96.0, 0.0, 26.0, 26.60601027181912, 0.6549575184709174, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7686980609418284, 0.895, 0.32, 0.0, 0.6666666666666666, 0.7171675226515933, 0.7183191728236391, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0177778], dtype=float32), -1.248493]. 
=============================================
[2019-04-04 08:27:55,822] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3602712e-11 1.9752591e-09 1.1957219e-28 2.4061925e-27 3.2191987e-22
 1.0000000e+00 1.9874953e-21], sum to 1.0000
[2019-04-04 08:27:55,822] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6013
[2019-04-04 08:27:55,850] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.15, 67.0, 139.0, 68.0, 26.0, 25.87350514096686, 0.3330410024407361, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 729000.0000, 
sim time next is 729600.0000, 
raw observation next is [-0.9666666666666668, 66.66666666666667, 129.8333333333333, 186.5, 26.0, 25.86998799532421, 0.3441106034221828, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.43582640812557716, 0.6666666666666667, 0.4327777777777776, 0.20607734806629835, 0.6666666666666666, 0.655832332943684, 0.6147035344740609, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5736436], dtype=float32), -0.68460995]. 
=============================================
[2019-04-04 08:28:02,684] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5181285e-09 3.3395914e-08 1.3749269e-24 4.4226029e-23 3.1113874e-19
 1.0000000e+00 7.8931295e-18], sum to 1.0000
[2019-04-04 08:28:02,686] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6561
[2019-04-04 08:28:02,702] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.45096527030076, 0.1378829645540063, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1236600.0000, 
sim time next is 1237200.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.43140988882749, 0.1402564567215762, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.452617490735624, 0.5467521522405254, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04458007], dtype=float32), -0.34320843]. 
=============================================
[2019-04-04 08:28:04,672] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.0799130e-12 1.4299016e-10 1.0281053e-30 2.5873338e-27 7.4959728e-23
 1.0000000e+00 1.7734017e-21], sum to 1.0000
[2019-04-04 08:28:04,674] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5176
[2019-04-04 08:28:04,695] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.1, 98.0, 101.0, 0.0, 26.0, 25.01299339909514, 0.4778035493872634, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1254600.0000, 
sim time next is 1255200.0000, 
raw observation next is [14.0, 98.66666666666666, 100.0, 0.0, 26.0, 24.98216828858361, 0.4739813699777751, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8504155124653741, 0.9866666666666666, 0.3333333333333333, 0.0, 0.6666666666666666, 0.5818473573819674, 0.6579937899925917, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02687368], dtype=float32), 0.1273158]. 
=============================================
[2019-04-04 08:28:06,200] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.7846779e-09 5.4618933e-08 1.6644042e-24 1.2788392e-22 1.0620915e-18
 9.9999988e-01 2.5282196e-18], sum to 1.0000
[2019-04-04 08:28:06,208] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0473
[2019-04-04 08:28:06,215] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.1, 78.33333333333333, 0.0, 0.0, 26.0, 24.16406590526762, 0.285260985663863, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1210200.0000, 
sim time next is 1210800.0000, 
raw observation next is [16.1, 78.66666666666667, 0.0, 0.0, 26.0, 24.13840717412264, 0.2807690298581762, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5115339311768867, 0.5935896766193921, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26773673], dtype=float32), -1.3302525]. 
=============================================
[2019-04-04 08:28:06,281] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4481779e-12 3.0034816e-10 8.0505047e-31 2.9571378e-28 5.3095230e-23
 1.0000000e+00 1.4374654e-21], sum to 1.0000
[2019-04-04 08:28:06,282] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9394
[2019-04-04 08:28:06,294] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 83.0, 0.0, 0.0, 26.0, 25.5394859831149, 0.4476349475205048, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 964800.0000, 
sim time next is 965400.0000, 
raw observation next is [7.883333333333334, 83.0, 0.0, 0.0, 26.0, 25.50033489218492, 0.4322385419938407, 0.0, 1.0, 21544.93334996061], 
processed observation next is [1.0, 0.17391304347826086, 0.6809787626962143, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6250279076820767, 0.6440795139979469, 0.0, 1.0, 0.10259492071409815], 
reward next is 0.8974, 
noisyNet noise sample is [array([0.81686634], dtype=float32), -0.041004036]. 
=============================================
[2019-04-04 08:28:06,697] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8185389e-11 5.3501017e-09 2.1145012e-27 1.6304064e-25 1.7688354e-20
 1.0000000e+00 6.3843196e-20], sum to 1.0000
[2019-04-04 08:28:06,701] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7458
[2019-04-04 08:28:06,760] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.88515099502428, 0.3042608046724737, 0.0, 1.0, 33446.91716482838], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 849600.0000, 
sim time next is 850200.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.99096813221182, 0.3065506465522951, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5825806776843182, 0.6021835488507651, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0348493], dtype=float32), -0.35281497]. 
=============================================
[2019-04-04 08:28:08,638] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.10983454e-13 1.18238769e-11 1.34206172e-33 1.03991824e-31
 2.12030281e-25 1.00000000e+00 2.83247864e-24], sum to 1.0000
[2019-04-04 08:28:08,641] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0166
[2019-04-04 08:28:08,652] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.38333333333333, 82.5, 7.999999999999999, 27.66666666666666, 26.0, 25.89792850279543, 0.6249464238013606, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1065000.0000, 
sim time next is 1065600.0000, 
raw observation next is [12.2, 83.0, 11.5, 38.0, 26.0, 25.92345413178455, 0.6239938193512852, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.03833333333333333, 0.041988950276243095, 0.6666666666666666, 0.6602878443153791, 0.7079979397837617, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5689653], dtype=float32), 0.38622123]. 
=============================================
[2019-04-04 08:28:10,233] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2013942e-12 7.3694123e-10 8.5679899e-30 1.3870465e-27 2.6714475e-22
 1.0000000e+00 3.6257993e-22], sum to 1.0000
[2019-04-04 08:28:10,233] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0984
[2019-04-04 08:28:10,242] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.433333333333334, 94.83333333333334, 99.33333333333333, 0.0, 26.0, 25.32479868658363, 0.2756041551652125, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 906600.0000, 
sim time next is 907200.0000, 
raw observation next is [2.7, 97.0, 100.5, 0.0, 26.0, 25.31768205835842, 0.2736813353251277, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5373961218836566, 0.97, 0.335, 0.0, 0.6666666666666666, 0.6098068381965351, 0.5912271117750426, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0180268], dtype=float32), 0.1414051]. 
=============================================
[2019-04-04 08:28:12,471] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2632117e-12 1.9980119e-10 7.4456701e-31 2.3626675e-29 1.3571590e-22
 1.0000000e+00 5.7525820e-23], sum to 1.0000
[2019-04-04 08:28:12,474] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7937
[2019-04-04 08:28:12,486] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.35, 71.0, 83.0, 0.0, 26.0, 25.72488111296596, 0.5971124605221844, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1157400.0000, 
sim time next is 1158000.0000, 
raw observation next is [16.63333333333333, 69.66666666666667, 90.83333333333333, 0.0, 26.0, 25.67842884149631, 0.5778787489717162, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.9233610341643583, 0.6966666666666668, 0.30277777777777776, 0.0, 0.6666666666666666, 0.6398690701246924, 0.6926262496572387, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2238257], dtype=float32), 0.8445913]. 
=============================================
[2019-04-04 08:28:12,499] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[96.16182]
 [96.75941]
 [97.34768]
 [97.15398]
 [96.92203]], R is [[95.50158691]
 [95.54656982]
 [95.5911026 ]
 [95.63519287]
 [95.67884064]].
[2019-04-04 08:28:26,884] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.7429016e-11 1.9169229e-10 3.7974584e-29 1.6629493e-26 2.2303163e-22
 1.0000000e+00 2.4892379e-21], sum to 1.0000
[2019-04-04 08:28:26,884] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4402
[2019-04-04 08:28:26,906] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 82.0, 0.0, 0.0, 26.0, 25.5898401825248, 0.583613518286071, 0.0, 1.0, 93418.84051648348], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1638000.0000, 
sim time next is 1638600.0000, 
raw observation next is [7.2, 82.66666666666667, 0.0, 0.0, 26.0, 25.53142472217041, 0.6088248626438326, 0.0, 1.0, 91388.5079616094], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6276187268475342, 0.7029416208812775, 0.0, 1.0, 0.43518337124575907], 
reward next is 0.5648, 
noisyNet noise sample is [array([-0.08653168], dtype=float32), -1.6739463]. 
=============================================
[2019-04-04 08:28:27,420] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6368619e-10 3.2658265e-09 4.6116292e-27 1.6953156e-25 2.8893861e-20
 1.0000000e+00 9.1632044e-20], sum to 1.0000
[2019-04-04 08:28:27,421] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6191
[2019-04-04 08:28:27,432] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [19.11666666666667, 49.16666666666667, 54.0, 0.0, 26.0, 27.72198181166084, 0.9972518787158734, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1095000.0000, 
sim time next is 1095600.0000, 
raw observation next is [18.83333333333333, 49.33333333333334, 44.5, 0.0, 26.0, 27.80945155858674, 1.007436372604826, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.9843028624192061, 0.4933333333333334, 0.14833333333333334, 0.0, 0.6666666666666666, 0.817454296548895, 0.8358121242016088, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42808506], dtype=float32), -0.092943646]. 
=============================================
[2019-04-04 08:28:37,268] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.7908500e-12 5.0299809e-10 1.0281602e-30 4.3196808e-28 2.5414039e-22
 1.0000000e+00 6.0835912e-23], sum to 1.0000
[2019-04-04 08:28:37,269] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8396
[2019-04-04 08:28:37,306] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.38333333333333, 50.66666666666666, 68.66666666666667, 12.33333333333333, 26.0, 27.28981520328937, 0.8451840738133966, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1612200.0000, 
sim time next is 1612800.0000, 
raw observation next is [13.3, 51.0, 64.0, 18.5, 26.0, 27.35813844660763, 0.8535208123190294, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8310249307479226, 0.51, 0.21333333333333335, 0.020441988950276244, 0.6666666666666666, 0.7798448705506358, 0.7845069374396765, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.74030554], dtype=float32), 1.5801481]. 
=============================================
[2019-04-04 08:28:41,836] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.31782415e-11 2.42752901e-10 5.67286840e-30 4.15919363e-27
 8.17875437e-22 1.00000000e+00 6.01207582e-22], sum to 1.0000
[2019-04-04 08:28:41,836] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2073
[2019-04-04 08:28:41,864] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 100.0, 60.0, 0.0, 26.0, 26.129759101085, 0.5233572421458085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1504800.0000, 
sim time next is 1505400.0000, 
raw observation next is [2.383333333333333, 99.33333333333334, 64.33333333333333, 0.0, 26.0, 26.07092028861077, 0.5225919103896128, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5286241920590952, 0.9933333333333334, 0.21444444444444444, 0.0, 0.6666666666666666, 0.672576690717564, 0.6741973034632043, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0048075], dtype=float32), 1.1030179]. 
=============================================
[2019-04-04 08:28:42,622] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2543432e-11 1.0786816e-10 8.9525373e-29 5.9721732e-27 6.4852227e-22
 1.0000000e+00 2.5861279e-21], sum to 1.0000
[2019-04-04 08:28:42,625] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5134
[2019-04-04 08:28:42,641] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.82404975304923, 0.4750353990776266, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1704600.0000, 
sim time next is 1705200.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.60170640371813, 0.4960285434968383, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6334755336431774, 0.6653428478322795, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22295813], dtype=float32), 0.6413125]. 
=============================================
[2019-04-04 08:29:10,522] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.33920496e-10 1.05343778e-08 4.76381662e-25 1.29428346e-23
 6.06418670e-19 1.00000000e+00 2.15136835e-18], sum to 1.0000
[2019-04-04 08:29:10,522] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2280
[2019-04-04 08:29:10,551] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 85.0, 0.0, 0.0, 26.0, 24.9559572664103, 0.2298938944509678, 0.0, 1.0, 44893.05127757676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1887600.0000, 
sim time next is 1888200.0000, 
raw observation next is [-5.6, 84.5, 0.0, 0.0, 26.0, 24.96498264706086, 0.2249413182165426, 0.0, 1.0, 44594.76036366628], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.845, 0.0, 0.0, 0.6666666666666666, 0.5804152205884051, 0.5749804394055141, 0.0, 1.0, 0.2123560017317442], 
reward next is 0.7876, 
noisyNet noise sample is [array([0.30755624], dtype=float32), -0.94254255]. 
=============================================
[2019-04-04 08:29:24,501] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1782826e-09 1.9033862e-08 1.2249636e-23 9.5353373e-23 1.0760305e-18
 1.0000000e+00 3.4219668e-18], sum to 1.0000
[2019-04-04 08:29:24,502] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0851
[2019-04-04 08:29:24,546] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.37081712083968, 0.1707564950661233, 0.0, 1.0, 42569.45129801419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2167800.0000, 
sim time next is 2168400.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.49007990360808, 0.169993000209038, 0.0, 1.0, 42515.78503846233], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.54083999196734, 0.5566643334030127, 0.0, 1.0, 0.202456119230773], 
reward next is 0.7975, 
noisyNet noise sample is [array([0.9428125], dtype=float32), -0.028138822]. 
=============================================
[2019-04-04 08:29:31,929] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9634436e-11 1.8836732e-09 5.6869577e-28 7.4783675e-26 6.0847111e-21
 1.0000000e+00 4.4093186e-20], sum to 1.0000
[2019-04-04 08:29:31,929] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4262
[2019-04-04 08:29:32,007] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.05, 79.0, 147.0, 0.0, 26.0, 25.61352629786406, 0.3350824457549352, 1.0, 1.0, 24356.03132114273], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2028600.0000, 
sim time next is 2029200.0000, 
raw observation next is [-4.866666666666667, 77.66666666666667, 148.5, 0.0, 26.0, 25.64371457393278, 0.3401588399050302, 1.0, 1.0, 23531.22640552824], 
processed observation next is [1.0, 0.4782608695652174, 0.3277931671283472, 0.7766666666666667, 0.495, 0.0, 0.6666666666666666, 0.6369762144943983, 0.6133862799683434, 1.0, 1.0, 0.112053459073944], 
reward next is 0.8879, 
noisyNet noise sample is [array([1.4939519], dtype=float32), 2.3354638]. 
=============================================
[2019-04-04 08:29:35,571] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.20290691e-10 1.23908634e-08 1.36534132e-24 5.23656400e-24
 2.92660456e-19 1.00000000e+00 5.36335689e-19], sum to 1.0000
[2019-04-04 08:29:35,571] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6159
[2019-04-04 08:29:35,619] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.49425636089506, 0.161493453998913, 0.0, 1.0, 42439.06314931958], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2170200.0000, 
sim time next is 2170800.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.49888018357656, 0.1600654390824777, 0.0, 1.0, 42362.12942320872], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.54157334863138, 0.5533551463608258, 0.0, 1.0, 0.2017244258248034], 
reward next is 0.7983, 
noisyNet noise sample is [array([-0.11744159], dtype=float32), 1.0401601]. 
=============================================
[2019-04-04 08:29:41,582] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9195719e-10 2.8448666e-09 4.8485709e-26 4.3210740e-24 6.6831844e-20
 1.0000000e+00 8.9127150e-19], sum to 1.0000
[2019-04-04 08:29:41,582] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9487
[2019-04-04 08:29:41,643] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 69.5, 0.0, 0.0, 26.0, 25.13777204102743, 0.4022660292589341, 0.0, 1.0, 91065.4550027276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2233800.0000, 
sim time next is 2234400.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 26.0, 25.17153057555723, 0.4084202566873782, 0.0, 1.0, 61583.06842277938], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5976275479631026, 0.6361400855624594, 0.0, 1.0, 0.2932527067751399], 
reward next is 0.7067, 
noisyNet noise sample is [array([-1.4655026], dtype=float32), 1.4553047]. 
=============================================
[2019-04-04 08:29:43,602] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6296077e-09 7.9222637e-08 2.7034724e-23 2.7706163e-22 6.1994573e-17
 9.9999988e-01 6.4431460e-17], sum to 1.0000
[2019-04-04 08:29:43,603] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3887
[2019-04-04 08:29:43,628] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 57.0, 0.0, 0.0, 26.0, 23.77428931568539, -0.02954008641207329, 0.0, 1.0, 44044.33087886962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2431200.0000, 
sim time next is 2431800.0000, 
raw observation next is [-8.1, 58.0, 0.0, 0.0, 26.0, 23.72711781425372, -0.03950678126089894, 0.0, 1.0, 44128.52039879106], 
processed observation next is [0.0, 0.13043478260869565, 0.23822714681440446, 0.58, 0.0, 0.0, 0.6666666666666666, 0.4772598178544767, 0.4868310729130337, 0.0, 1.0, 0.21013581142281457], 
reward next is 0.7899, 
noisyNet noise sample is [array([-0.65974003], dtype=float32), 1.3898108]. 
=============================================
[2019-04-04 08:29:53,151] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9420965e-09 1.3867266e-08 3.1257368e-25 7.1857127e-24 3.1394169e-19
 1.0000000e+00 2.6915839e-18], sum to 1.0000
[2019-04-04 08:29:53,151] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6725
[2019-04-04 08:29:53,219] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 47.83333333333334, 135.3333333333333, 113.6666666666666, 26.0, 24.93904919763082, 0.2967338361799097, 0.0, 1.0, 43348.21827574441], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2386200.0000, 
sim time next is 2386800.0000, 
raw observation next is [0.0, 47.0, 123.0, 170.5, 26.0, 24.9370314296676, 0.3037809761694005, 0.0, 1.0, 38340.35633285508], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.41, 0.18839779005524862, 0.6666666666666666, 0.5780859524722999, 0.6012603253898002, 0.0, 1.0, 0.182573125394548], 
reward next is 0.8174, 
noisyNet noise sample is [array([-1.1363468], dtype=float32), 0.7223214]. 
=============================================
[2019-04-04 08:30:00,061] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2783385e-11 7.0790157e-10 3.7323752e-27 2.5419222e-26 9.5336742e-21
 1.0000000e+00 4.4850961e-21], sum to 1.0000
[2019-04-04 08:30:00,061] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6111
[2019-04-04 08:30:00,119] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.283333333333333, 66.66666666666667, 112.6666666666667, 152.3333333333333, 26.0, 25.8691651547346, 0.3641132432598973, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2627400.0000, 
sim time next is 2628000.0000, 
raw observation next is [-5.0, 65.0, 122.5, 183.0, 26.0, 25.84888873731731, 0.3632664000308268, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.32409972299168976, 0.65, 0.4083333333333333, 0.2022099447513812, 0.6666666666666666, 0.6540740614431092, 0.6210888000102756, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17905796], dtype=float32), -0.90158063]. 
=============================================
[2019-04-04 08:30:00,123] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[84.83407]
 [84.8514 ]
 [84.96094]
 [85.17877]
 [85.32599]], R is [[84.9934082 ]
 [85.14347839]
 [85.29204559]
 [85.43912506]
 [85.58473206]].
[2019-04-04 08:30:02,800] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4664215e-09 3.4881459e-08 1.3303798e-24 3.2250846e-23 1.3908041e-18
 1.0000000e+00 4.2110665e-18], sum to 1.0000
[2019-04-04 08:30:02,801] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9842
[2019-04-04 08:30:02,825] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.383333333333333, 56.5, 0.0, 0.0, 26.0, 24.73906990825132, 0.1256096568207206, 0.0, 1.0, 38627.51128689079], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2527800.0000, 
sim time next is 2528400.0000, 
raw observation next is [-2.466666666666667, 56.00000000000001, 0.0, 0.0, 26.0, 24.72175207042626, 0.1221395393964346, 0.0, 1.0, 38693.8301045395], 
processed observation next is [1.0, 0.2608695652173913, 0.39427516158818104, 0.56, 0.0, 0.0, 0.6666666666666666, 0.5601460058688549, 0.5407131797988115, 0.0, 1.0, 0.18425633383114046], 
reward next is 0.8157, 
noisyNet noise sample is [array([-0.7788723], dtype=float32), 1.1244072]. 
=============================================
[2019-04-04 08:30:10,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3265635e-09 4.3345644e-08 1.8353551e-24 1.1957027e-22 5.7988872e-18
 1.0000000e+00 3.7175718e-18], sum to 1.0000
[2019-04-04 08:30:10,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7814
[2019-04-04 08:30:10,955] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.93579880262054, 0.03574675067953326, 0.0, 1.0, 40210.89315107832], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3041400.0000, 
sim time next is 3042000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.90986916661879, 0.02962442599609564, 0.0, 1.0, 40198.07540357955], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.49248909721823253, 0.5098748086653652, 0.0, 1.0, 0.19141940668371213], 
reward next is 0.8086, 
noisyNet noise sample is [array([0.08090337], dtype=float32), 1.0025192]. 
=============================================
[2019-04-04 08:30:10,999] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.517006]
 [76.592804]
 [76.67716 ]
 [76.75283 ]
 [76.82084 ]], R is [[76.49342346]
 [76.53701019]
 [76.58016968]
 [76.62297058]
 [76.66549683]].
[2019-04-04 08:30:15,595] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1352275e-11 3.4172584e-10 2.9973321e-27 4.2684638e-26 7.8654650e-21
 1.0000000e+00 1.3677315e-20], sum to 1.0000
[2019-04-04 08:30:15,605] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4976
[2019-04-04 08:30:15,633] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 51.33333333333333, 80.66666666666667, 587.3333333333334, 26.0, 26.56466115468712, 0.6480115876086808, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2734800.0000, 
sim time next is 2735400.0000, 
raw observation next is [-3.166666666666667, 50.66666666666667, 75.33333333333334, 560.6666666666666, 26.0, 26.61334817519431, 0.6235030298324594, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3748845798707295, 0.5066666666666667, 0.2511111111111111, 0.6195211786372007, 0.6666666666666666, 0.7177790145995259, 0.7078343432774865, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09602419], dtype=float32), 2.8702767]. 
=============================================
[2019-04-04 08:30:30,793] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.4753010e-10 9.0910799e-09 1.6493159e-26 2.1749110e-25 3.0860309e-20
 1.0000000e+00 5.7741588e-20], sum to 1.0000
[2019-04-04 08:30:30,793] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2142
[2019-04-04 08:30:30,851] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.83170397725888, 0.3347308395078181, 0.0, 1.0, 43323.48322472895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2940600.0000, 
sim time next is 2941200.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.88031317309304, 0.3310345491023737, 0.0, 1.0, 43297.67905520149], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5733594310910867, 0.6103448497007912, 0.0, 1.0, 0.20617942407238807], 
reward next is 0.7938, 
noisyNet noise sample is [array([-0.02394736], dtype=float32), -0.8450477]. 
=============================================
[2019-04-04 08:30:41,569] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5705184e-09 9.3613597e-09 3.3446223e-26 4.9707565e-24 5.0272506e-20
 1.0000000e+00 3.5337833e-19], sum to 1.0000
[2019-04-04 08:30:41,569] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8829
[2019-04-04 08:30:41,607] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 79.33333333333334, 0.0, 0.0, 26.0, 25.08990263122907, 0.4038318425217877, 0.0, 1.0, 43564.2098731316], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3284400.0000, 
sim time next is 3285000.0000, 
raw observation next is [-7.0, 77.0, 0.0, 0.0, 26.0, 25.04956081066516, 0.3999226907855473, 0.0, 1.0, 43631.52581988611], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5874634008887633, 0.6333075635951825, 0.0, 1.0, 0.20776917057088626], 
reward next is 0.7922, 
noisyNet noise sample is [array([-1.652881], dtype=float32), -1.4138963]. 
=============================================
[2019-04-04 08:30:41,615] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.63875]
 [80.75187]
 [80.79151]
 [81.00596]
 [81.18354]], R is [[80.36584473]
 [80.35473633]
 [80.34409332]
 [80.33391571]
 [80.32420349]].
[2019-04-04 08:30:42,412] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.6407143e-10 7.1938859e-09 1.7620425e-25 5.1529408e-24 1.9122909e-19
 1.0000000e+00 3.8044376e-19], sum to 1.0000
[2019-04-04 08:30:42,412] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0703
[2019-04-04 08:30:42,468] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 53.33333333333334, 113.5, 815.0, 26.0, 25.05886375617982, 0.352712464711925, 0.0, 1.0, 18727.83645492481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3068400.0000, 
sim time next is 3069000.0000, 
raw observation next is [-2.5, 52.5, 114.0, 817.0, 26.0, 25.08209129188666, 0.3566447614904044, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.525, 0.38, 0.9027624309392265, 0.6666666666666666, 0.5901742743238882, 0.6188815871634682, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.102802], dtype=float32), 0.25693882]. 
=============================================
[2019-04-04 08:30:42,477] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[78.556564]
 [78.55826 ]
 [78.52176 ]
 [78.368225]
 [78.264114]], R is [[78.684021  ]
 [78.80799866]
 [78.88405609]
 [78.87953949]
 [78.89792633]].
[2019-04-04 08:30:43,605] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.8489290e-12 3.7114846e-11 2.0479256e-30 5.4578394e-28 6.7062864e-23
 1.0000000e+00 1.4725360e-22], sum to 1.0000
[2019-04-04 08:30:43,605] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7905
[2019-04-04 08:30:43,613] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.333333333333333, 75.83333333333334, 95.66666666666667, 741.3333333333334, 26.0, 26.9355726073875, 0.8402657399791904, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3250200.0000, 
sim time next is 3250800.0000, 
raw observation next is [-2.0, 71.0, 93.0, 727.5, 26.0, 26.97678990958632, 0.8500607014287945, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.40720221606648205, 0.71, 0.31, 0.8038674033149171, 0.6666666666666666, 0.7480658257988599, 0.7833535671429316, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2011153], dtype=float32), -1.2016698]. 
=============================================
[2019-04-04 08:30:44,623] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6801194e-09 4.9646074e-09 3.7948595e-25 9.2240736e-23 6.0150672e-19
 1.0000000e+00 1.2123136e-17], sum to 1.0000
[2019-04-04 08:30:44,623] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7453
[2019-04-04 08:30:44,719] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8, 87.0, 0.0, 0.0, 26.0, 25.00165594983886, 0.2827512597343558, 0.0, 1.0, 30831.4222338875], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3090600.0000, 
sim time next is 3091200.0000, 
raw observation next is [-0.8666666666666667, 88.66666666666666, 0.0, 0.0, 26.0, 25.00399545867737, 0.2799968040310092, 0.0, 1.0, 31573.23371954219], 
processed observation next is [0.0, 0.782608695652174, 0.4385964912280702, 0.8866666666666666, 0.0, 0.0, 0.6666666666666666, 0.5836662882231142, 0.5933322680103364, 0.0, 1.0, 0.15034873199781995], 
reward next is 0.8497, 
noisyNet noise sample is [array([1.0763999], dtype=float32), 0.030970542]. 
=============================================
[2019-04-04 08:30:47,582] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1315234e-09 1.6210251e-08 3.3638552e-25 1.3636191e-23 3.5727660e-19
 1.0000000e+00 2.3527426e-18], sum to 1.0000
[2019-04-04 08:30:47,586] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5413
[2019-04-04 08:30:47,609] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 65.0, 0.0, 0.0, 26.0, 25.34081732087324, 0.364394144025678, 0.0, 1.0, 40590.77773862823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3012000.0000, 
sim time next is 3012600.0000, 
raw observation next is [-3.416666666666667, 65.0, 0.0, 0.0, 26.0, 25.30424348613305, 0.3596199422679962, 0.0, 1.0, 40219.52650276611], 
processed observation next is [0.0, 0.8695652173913043, 0.36795937211449675, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6086869571777541, 0.6198733140893321, 0.0, 1.0, 0.1915215547750767], 
reward next is 0.8085, 
noisyNet noise sample is [array([-0.18282092], dtype=float32), -0.21019904]. 
=============================================
[2019-04-04 08:31:01,030] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 08:31:01,032] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:31:01,037] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:31:01,040] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:31:01,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:31:01,038] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:31:01,046] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:31:01,046] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run25
[2019-04-04 08:31:01,073] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run25
[2019-04-04 08:31:01,102] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run25
[2019-04-04 08:33:47,064] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.24898864], dtype=float32), 0.2664866]
[2019-04-04 08:33:47,064] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.5, 52.0, 114.0, 800.0, 26.0, 26.00440816398061, 0.5800061309911424, 1.0, 1.0, 0.0]
[2019-04-04 08:33:47,064] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 08:33:47,065] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.1965126e-11 1.0993029e-09 3.3934435e-27 1.6469026e-25 2.0740930e-20
 1.0000000e+00 3.2257294e-20], sampled 0.35519451655227907
[2019-04-04 08:34:10,698] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 08:34:39,918] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.4981 263445398.1310 1557.1271
[2019-04-04 08:34:43,131] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 08:34:44,154] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2400000, evaluation results [2400000.0, 7241.498104138273, 263445398.13096312, 1557.1271045198032, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 08:34:47,666] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9652067e-09 2.0980544e-08 6.8943150e-25 5.0451675e-23 4.1156811e-19
 1.0000000e+00 3.0202668e-18], sum to 1.0000
[2019-04-04 08:34:47,667] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6755
[2019-04-04 08:34:47,708] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.58250140008099, 0.4006176182424226, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3629400.0000, 
sim time next is 3630000.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.61685516382133, 0.3964889224617303, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.7119113573407203, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6347379303184443, 0.6321629741539101, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1562738], dtype=float32), -0.87080884]. 
=============================================
[2019-04-04 08:34:47,734] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.27536]
 [76.10368]
 [76.09223]
 [76.096  ]
 [76.0107 ]], R is [[76.659729  ]
 [76.89313507]
 [77.12420654]
 [77.35296631]
 [77.45530701]].
[2019-04-04 08:34:48,086] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7348238e-10 1.5377973e-08 2.9097181e-26 5.7833172e-24 1.2079085e-19
 1.0000000e+00 2.0864976e-19], sum to 1.0000
[2019-04-04 08:34:48,089] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8724
[2019-04-04 08:34:48,108] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.866666666666667, 25.33333333333334, 0.0, 0.0, 26.0, 25.51344138848265, 0.3670664117034373, 0.0, 1.0, 25407.70748844287], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3636600.0000, 
sim time next is 3637200.0000, 
raw observation next is [8.733333333333334, 25.66666666666667, 0.0, 0.0, 26.0, 25.51110741117948, 0.3646531615355149, 0.0, 1.0, 29103.6232168347], 
processed observation next is [0.0, 0.08695652173913043, 0.7045244690674055, 0.2566666666666667, 0.0, 0.0, 0.6666666666666666, 0.62592561759829, 0.6215510538451716, 0.0, 1.0, 0.13858868198492716], 
reward next is 0.8614, 
noisyNet noise sample is [array([0.3840574], dtype=float32), -0.694889]. 
=============================================
[2019-04-04 08:34:48,829] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1358475e-10 1.8331091e-09 2.0148935e-28 1.0950912e-26 2.0053005e-21
 1.0000000e+00 4.6530109e-21], sum to 1.0000
[2019-04-04 08:34:48,831] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6827
[2019-04-04 08:34:48,861] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.50037212914064, 0.5487866369355154, 0.0, 1.0, 36641.8767486308], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3208800.0000, 
sim time next is 3209400.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.54515375189463, 0.5496654494471679, 0.0, 1.0, 18745.81329052171], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6287628126578859, 0.6832218164823893, 0.0, 1.0, 0.08926577757391291], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.3561239], dtype=float32), 0.91831064]. 
=============================================
[2019-04-04 08:34:48,906] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.8899832e-11 3.7137049e-09 8.9659434e-27 5.9289819e-25 5.8015366e-20
 1.0000000e+00 2.4252411e-19], sum to 1.0000
[2019-04-04 08:34:48,907] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2061
[2019-04-04 08:34:48,958] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 54.33333333333334, 113.5, 806.3333333333333, 26.0, 25.24303657610155, 0.4357066146640496, 0.0, 1.0, 18709.55262829536], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3583200.0000, 
sim time next is 3583800.0000, 
raw observation next is [-3.5, 54.5, 114.0, 816.0, 26.0, 25.18565720887107, 0.4333192214975603, 0.0, 1.0, 34204.16843174552], 
processed observation next is [0.0, 0.4782608695652174, 0.36565096952908593, 0.545, 0.38, 0.901657458563536, 0.6666666666666666, 0.5988047674059226, 0.6444397404991867, 0.0, 1.0, 0.16287699253212154], 
reward next is 0.8371, 
noisyNet noise sample is [array([0.17867362], dtype=float32), -0.9274048]. 
=============================================
[2019-04-04 08:34:57,037] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5706643e-10 1.0125519e-08 2.5394508e-26 1.4577626e-24 6.0878483e-20
 1.0000000e+00 8.6421536e-20], sum to 1.0000
[2019-04-04 08:34:57,039] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9131
[2019-04-04 08:34:57,061] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.25552957260504, 0.3641498422410407, 0.0, 1.0, 45836.85368509504], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3480600.0000, 
sim time next is 3481200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.21990022232681, 0.3507113389682139, 0.0, 1.0, 51733.86591553489], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6016583518605675, 0.6169037796560713, 0.0, 1.0, 0.24635174245492802], 
reward next is 0.7536, 
noisyNet noise sample is [array([-0.32383054], dtype=float32), -1.8360976]. 
=============================================
[2019-04-04 08:35:03,643] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4427783e-09 1.2659760e-08 4.5205200e-26 5.9661452e-25 2.1625988e-19
 1.0000000e+00 1.4150198e-18], sum to 1.0000
[2019-04-04 08:35:03,644] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8333
[2019-04-04 08:35:03,657] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.50142325610013, 0.3705087786623166, 0.0, 1.0, 27975.74533634291], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3635400.0000, 
sim time next is 3636000.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.50920031367377, 0.369243914549107, 0.0, 1.0, 24864.82843023255], 
processed observation next is [0.0, 0.08695652173913043, 0.7119113573407203, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6257666928061475, 0.6230813048497024, 0.0, 1.0, 0.11840394490586928], 
reward next is 0.8816, 
noisyNet noise sample is [array([0.9622194], dtype=float32), 1.1789342]. 
=============================================
[2019-04-04 08:35:03,685] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[79.61504]
 [79.35943]
 [79.00665]
 [78.6143 ]
 [78.26732]], R is [[79.88907623]
 [79.95697021]
 [79.98963928]
 [79.97471619]
 [79.92136383]].
[2019-04-04 08:35:03,969] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9219382e-10 2.6756211e-09 7.9736581e-26 1.3634155e-24 6.7962025e-20
 1.0000000e+00 3.4193682e-19], sum to 1.0000
[2019-04-04 08:35:03,973] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9420
[2019-04-04 08:35:03,982] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 42.0, 96.66666666666667, 758.3333333333333, 26.0, 25.21557394635941, 0.4524046691135949, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3595800.0000, 
sim time next is 3596400.0000, 
raw observation next is [-1.0, 42.0, 94.0, 743.5, 26.0, 25.2040626297129, 0.4508786718740868, 0.0, 1.0, 18693.51984386185], 
processed observation next is [0.0, 0.6521739130434783, 0.4349030470914128, 0.42, 0.31333333333333335, 0.8215469613259668, 0.6666666666666666, 0.600338552476075, 0.6502928906246956, 0.0, 1.0, 0.0890167611612469], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.4114468], dtype=float32), -1.7500952]. 
=============================================
[2019-04-04 08:35:07,220] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2819855e-10 5.3567697e-09 1.7756852e-26 2.7168180e-24 1.6424335e-20
 1.0000000e+00 2.3135456e-19], sum to 1.0000
[2019-04-04 08:35:07,221] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5818
[2019-04-04 08:35:07,234] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.83333333333334, 0.0, 0.0, 26.0, 25.63690850855883, 0.4721409624621883, 0.0, 1.0, 22348.40164770632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3881400.0000, 
sim time next is 3882000.0000, 
raw observation next is [-1.0, 56.66666666666667, 0.0, 0.0, 26.0, 25.55360242866891, 0.4527624229746645, 0.0, 1.0, 72886.5083782507], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.5666666666666668, 0.0, 0.0, 0.6666666666666666, 0.6294668690557425, 0.6509208076582215, 0.0, 1.0, 0.34707861132500334], 
reward next is 0.6529, 
noisyNet noise sample is [array([1.2057543], dtype=float32), -0.92543745]. 
=============================================
[2019-04-04 08:35:07,236] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.88857 ]
 [78.887054]
 [78.89277 ]
 [78.88038 ]
 [78.88853 ]], R is [[78.9401474 ]
 [79.04432678]
 [79.25388336]
 [79.46134186]
 [79.66673279]].
[2019-04-04 08:35:07,879] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5321225e-09 1.2716705e-08 3.3987845e-25 1.3739472e-23 3.2212181e-19
 1.0000000e+00 2.3575040e-18], sum to 1.0000
[2019-04-04 08:35:07,879] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9569
[2019-04-04 08:35:07,895] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.33108362309497, 0.3520014068536717, 0.0, 1.0, 41486.70132221535], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3723600.0000, 
sim time next is 3724200.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.34626222189401, 0.3496198939241311, 0.0, 1.0, 41100.35627361521], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6121885184911674, 0.6165399646413771, 0.0, 1.0, 0.1957159822553105], 
reward next is 0.8043, 
noisyNet noise sample is [array([-0.04782772], dtype=float32), -0.46053034]. 
=============================================
[2019-04-04 08:35:08,883] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.8390939e-11 1.4577014e-09 5.4138611e-27 3.0339743e-25 2.2041242e-20
 1.0000000e+00 3.1575995e-20], sum to 1.0000
[2019-04-04 08:35:08,883] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6172
[2019-04-04 08:35:08,919] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 37.33333333333334, 93.66666666666666, 745.3333333333334, 26.0, 27.0662124429202, 0.6790642243323557, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3942600.0000, 
sim time next is 3943200.0000, 
raw observation next is [-4.0, 36.66666666666667, 90.83333333333334, 734.6666666666667, 26.0, 26.38088767406258, 0.7059202021054768, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.3666666666666667, 0.3027777777777778, 0.8117863720073666, 0.6666666666666666, 0.6984073061718817, 0.7353067340351589, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60571766], dtype=float32), -0.21582213]. 
=============================================
[2019-04-04 08:35:18,742] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4736625e-11 4.6332169e-10 1.4475254e-27 2.9063235e-26 1.1015425e-20
 1.0000000e+00 4.0410334e-21], sum to 1.0000
[2019-04-04 08:35:18,742] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9459
[2019-04-04 08:35:18,778] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 41.5, 116.0, 824.0, 26.0, 26.30913480052656, 0.6939995124403638, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3936600.0000, 
sim time next is 3937200.0000, 
raw observation next is [-5.333333333333333, 40.33333333333334, 114.1666666666667, 818.0, 26.0, 26.5681600492984, 0.7215807174674339, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3148661126500462, 0.40333333333333343, 0.38055555555555565, 0.9038674033149171, 0.6666666666666666, 0.7140133374415333, 0.7405269058224779, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5246129], dtype=float32), -0.026456686]. 
=============================================
[2019-04-04 08:35:19,305] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4808714e-10 3.9711820e-09 4.7175346e-27 1.2719530e-25 4.8361882e-21
 1.0000000e+00 1.4962751e-19], sum to 1.0000
[2019-04-04 08:35:19,307] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3173
[2019-04-04 08:35:19,315] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 42.0, 0.0, 0.0, 26.0, 25.53008891434226, 0.4933166659938813, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4132200.0000, 
sim time next is 4132800.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.47821166648921, 0.4801128622134125, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6231843055407674, 0.6600376207378041, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.58442354], dtype=float32), -0.37782726]. 
=============================================
[2019-04-04 08:35:31,809] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7732845e-10 1.0619532e-08 5.6108971e-26 4.0072226e-25 1.6390732e-19
 1.0000000e+00 1.7064742e-19], sum to 1.0000
[2019-04-04 08:35:31,811] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8797
[2019-04-04 08:35:31,831] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 75.0, 0.0, 0.0, 26.0, 25.45920824886801, 0.4585751767023487, 0.0, 1.0, 37372.55157415221], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3795600.0000, 
sim time next is 3796200.0000, 
raw observation next is [-3.0, 74.0, 0.0, 0.0, 26.0, 25.40338305717683, 0.4508823809587741, 0.0, 1.0, 66403.34550134093], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6169485880980691, 0.650294126986258, 0.0, 1.0, 0.31620640714924253], 
reward next is 0.6838, 
noisyNet noise sample is [array([1.5529156], dtype=float32), 0.07175432]. 
=============================================
[2019-04-04 08:35:42,996] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.05515804e-10 2.40038267e-10 2.72846567e-28 8.66123206e-27
 3.20337324e-21 1.00000000e+00 2.69474421e-21], sum to 1.0000
[2019-04-04 08:35:43,011] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4345
[2019-04-04 08:35:43,035] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.333333333333333, 36.33333333333333, 81.33333333333333, 373.6666666666667, 26.0, 27.33040898927599, 0.7739767051327252, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4120800.0000, 
sim time next is 4121400.0000, 
raw observation next is [3.166666666666667, 36.66666666666667, 69.66666666666667, 310.3333333333334, 26.0, 26.83194261901023, 0.7386249297211905, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5503231763619576, 0.3666666666666667, 0.23222222222222225, 0.3429097605893187, 0.6666666666666666, 0.7359952182508526, 0.7462083099070634, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5889654], dtype=float32), 0.35953802]. 
=============================================
[2019-04-04 08:35:50,888] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.8511277e-12 2.5634095e-10 5.3682600e-30 1.4165689e-27 1.5221160e-22
 1.0000000e+00 6.5709362e-22], sum to 1.0000
[2019-04-04 08:35:50,891] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0939
[2019-04-04 08:35:50,899] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.833333333333333, 49.0, 126.3333333333333, 843.6666666666667, 26.0, 26.30837149178197, 0.6885656873060323, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4625400.0000, 
sim time next is 4626000.0000, 
raw observation next is [4.0, 49.0, 129.5, 836.0, 26.0, 26.47772273239582, 0.7272027421557768, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5734072022160666, 0.49, 0.43166666666666664, 0.9237569060773481, 0.6666666666666666, 0.7064768943663182, 0.7424009140519257, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19503954], dtype=float32), -1.7785901]. 
=============================================
[2019-04-04 08:35:50,902] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[90.11978 ]
 [89.987114]
 [89.94583 ]
 [90.0403  ]
 [89.98314 ]], R is [[90.36016846]
 [90.45656586]
 [90.55200195]
 [90.64648438]
 [90.74002075]].
[2019-04-04 08:35:55,291] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2192374e-10 4.0804187e-09 8.4849648e-27 6.2566180e-25 3.3884756e-20
 1.0000000e+00 2.6818141e-20], sum to 1.0000
[2019-04-04 08:35:55,293] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5580
[2019-04-04 08:35:55,311] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 55.0, 41.33333333333333, 26.0, 26.02367656776397, 0.530022349507092, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4554600.0000, 
sim time next is 4555200.0000, 
raw observation next is [2.0, 52.0, 41.5, 34.66666666666666, 26.0, 26.07325012575709, 0.5075865406684029, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.13833333333333334, 0.03830570902394106, 0.6666666666666666, 0.6727708438130909, 0.6691955135561343, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6944367], dtype=float32), 1.5055845]. 
=============================================
[2019-04-04 08:35:59,572] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.00647376e-10 8.69103189e-09 1.01513049e-25 3.95981393e-24
 1.03109214e-19 1.00000000e+00 3.80294296e-19], sum to 1.0000
[2019-04-04 08:35:59,573] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0049
[2019-04-04 08:35:59,583] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 175.1666666666667, 414.0, 26.0, 25.08917002308239, 0.3743039142284843, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4893600.0000, 
sim time next is 4894200.0000, 
raw observation next is [3.0, 45.0, 163.0, 422.0, 26.0, 25.09898846539158, 0.3748864506133925, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.5433333333333333, 0.4662983425414365, 0.6666666666666666, 0.5915823721159651, 0.6249621502044641, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.93177396], dtype=float32), 0.9774504]. 
=============================================
[2019-04-04 08:36:01,126] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7073098e-09 3.8361264e-08 4.9034417e-26 2.5317154e-24 3.0809156e-19
 1.0000000e+00 4.2816724e-19], sum to 1.0000
[2019-04-04 08:36:01,126] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9088
[2019-04-04 08:36:01,147] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.30132830116793, 0.1963126293946739, 0.0, 1.0, 41313.0794746868], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4771200.0000, 
sim time next is 4771800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.31306197291286, 0.1882172752299994, 0.0, 1.0, 41334.45094117964], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5260884977427382, 0.5627390917433331, 0.0, 1.0, 0.1968307187675221], 
reward next is 0.8032, 
noisyNet noise sample is [array([-1.070471], dtype=float32), -1.1092093]. 
=============================================
[2019-04-04 08:36:08,928] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4018434e-10 7.4108710e-09 1.4306838e-26 2.6702324e-24 7.2228280e-20
 1.0000000e+00 6.4651263e-19], sum to 1.0000
[2019-04-04 08:36:08,928] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5386
[2019-04-04 08:36:08,944] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.933333333333333, 76.5, 0.0, 0.0, 26.0, 25.0336222240612, 0.3385896248645321, 0.0, 1.0, 36229.43477869129], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4603800.0000, 
sim time next is 4604400.0000, 
raw observation next is [-3.0, 77.0, 0.0, 0.0, 26.0, 25.03445980696389, 0.3307139802111642, 0.0, 1.0, 36228.2776025174], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5862049839136576, 0.6102379934037214, 0.0, 1.0, 0.17251560763103524], 
reward next is 0.8275, 
noisyNet noise sample is [array([0.95023143], dtype=float32), 1.0118359]. 
=============================================
[2019-04-04 08:36:09,728] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9283711e-10 1.0290840e-08 2.3165045e-26 2.7606636e-25 3.4615870e-20
 1.0000000e+00 1.7782812e-19], sum to 1.0000
[2019-04-04 08:36:09,729] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7594
[2019-04-04 08:36:09,741] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.833333333333333, 40.5, 186.6666666666667, 714.0, 26.0, 25.17905355910154, 0.4462823367848053, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4798200.0000, 
sim time next is 4798800.0000, 
raw observation next is [2.0, 40.0, 195.0, 677.5, 26.0, 25.18755905819972, 0.4429870725419322, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.4, 0.65, 0.7486187845303868, 0.6666666666666666, 0.5989632548499767, 0.6476623575139774, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32706097], dtype=float32), -0.8577421]. 
=============================================
[2019-04-04 08:36:11,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:11,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:11,631] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run19
[2019-04-04 08:36:12,067] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2246453e-09 1.4821099e-08 1.5746598e-25 1.6802259e-23 9.7958740e-19
 1.0000000e+00 1.3868677e-18], sum to 1.0000
[2019-04-04 08:36:12,081] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6152
[2019-04-04 08:36:12,092] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 26.0, 25.32995300197402, 0.4317764571821766, 0.0, 1.0, 45609.33839087042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4824000.0000, 
sim time next is 4824600.0000, 
raw observation next is [0.8333333333333334, 47.66666666666667, 0.0, 0.0, 26.0, 25.42935710284336, 0.4389146633345445, 0.0, 1.0, 18764.58889973066], 
processed observation next is [0.0, 0.8695652173913043, 0.4856879039704525, 0.47666666666666674, 0.0, 0.0, 0.6666666666666666, 0.6191130919036132, 0.6463048877781815, 0.0, 1.0, 0.08935518523681267], 
reward next is 0.9106, 
noisyNet noise sample is [array([0.24691279], dtype=float32), -1.03535]. 
=============================================
[2019-04-04 08:36:13,994] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:13,994] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:14,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run19
[2019-04-04 08:36:14,244] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:14,244] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:14,248] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run19
[2019-04-04 08:36:15,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:15,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:15,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run19
[2019-04-04 08:36:17,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:17,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:17,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run19
[2019-04-04 08:36:17,993] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5148886e-10 3.1504581e-09 2.1081557e-26 1.0414027e-24 5.0754817e-20
 1.0000000e+00 3.0816092e-19], sum to 1.0000
[2019-04-04 08:36:18,001] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3654
[2019-04-04 08:36:18,020] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.6, 74.0, 0.0, 0.0, 26.0, 25.20453734037777, 0.378068716001495, 0.0, 1.0, 36162.77163810201], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4600800.0000, 
sim time next is 4601400.0000, 
raw observation next is [-2.666666666666667, 74.5, 0.0, 0.0, 26.0, 25.23993857380433, 0.3682043111323028, 0.0, 1.0, 36155.11520625441], 
processed observation next is [1.0, 0.2608695652173913, 0.38873499538319484, 0.745, 0.0, 0.0, 0.6666666666666666, 0.6033282144836942, 0.6227347703774343, 0.0, 1.0, 0.17216721526787815], 
reward next is 0.8278, 
noisyNet noise sample is [array([-0.32054687], dtype=float32), 0.07169875]. 
=============================================
[2019-04-04 08:36:18,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:18,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:18,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run19
[2019-04-04 08:36:20,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:20,774] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:20,783] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run19
[2019-04-04 08:36:26,870] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.7267095e-11 5.4370286e-09 1.5163340e-27 6.9570833e-26 2.5335452e-21
 1.0000000e+00 1.3923375e-19], sum to 1.0000
[2019-04-04 08:36:26,870] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0673
[2019-04-04 08:36:26,950] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 38.0, 0.0, 26.0, 23.19193762870782, -0.144888404966822, 0.0, 1.0, 58605.64452457494], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 34200.0000, 
sim time next is 34800.0000, 
raw observation next is [7.699999999999999, 93.0, 41.66666666666666, 0.0, 26.0, 23.28598681914837, -0.1226141324581852, 0.0, 1.0, 58381.46038731085], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.13888888888888887, 0.0, 0.6666666666666666, 0.4404989015956975, 0.4591286225139382, 0.0, 1.0, 0.27800695422528976], 
reward next is 0.7220, 
noisyNet noise sample is [array([0.19419484], dtype=float32), -0.8688686]. 
=============================================
[2019-04-04 08:36:28,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:28,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:28,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run19
[2019-04-04 08:36:28,147] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8163184e-12 6.2332688e-11 4.8162644e-30 6.7056523e-29 5.9644221e-23
 1.0000000e+00 1.9919952e-22], sum to 1.0000
[2019-04-04 08:36:28,147] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2491
[2019-04-04 08:36:28,172] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.66666666666667, 17.0, 42.66666666666666, 340.8333333333333, 26.0, 29.05810252422319, 0.9868706874026586, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5073600.0000, 
sim time next is 5074200.0000, 
raw observation next is [11.5, 17.0, 36.0, 292.0, 26.0, 28.8947070158662, 1.126665032884505, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7811634349030472, 0.17, 0.12, 0.32265193370165746, 0.6666666666666666, 0.9078922513221833, 0.8755550109615017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16003844], dtype=float32), 2.7467854]. 
=============================================
[2019-04-04 08:36:28,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.7986085e-10 1.3197397e-09 1.4158132e-26 6.8034485e-25 9.4885080e-20
 1.0000000e+00 2.1157362e-19], sum to 1.0000
[2019-04-04 08:36:28,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4772
[2019-04-04 08:36:28,231] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 77.0, 0.0, 0.0, 26.0, 25.20477179017629, 0.4320013346806588, 0.0, 1.0, 42787.48458671392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4748400.0000, 
sim time next is 4749000.0000, 
raw observation next is [-3.166666666666667, 78.16666666666667, 0.0, 0.0, 26.0, 25.19171568670983, 0.4255716378786926, 0.0, 1.0, 42147.47001608426], 
processed observation next is [1.0, 1.0, 0.3748845798707295, 0.7816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5993096405591526, 0.6418572126262309, 0.0, 1.0, 0.20070223817182983], 
reward next is 0.7993, 
noisyNet noise sample is [array([-0.57607466], dtype=float32), 1.1495829]. 
=============================================
[2019-04-04 08:36:28,259] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.36591 ]
 [83.414406]
 [83.434166]
 [83.33364 ]
 [83.13903 ]], R is [[83.16282654]
 [83.12744904]
 [83.08414459]
 [83.02085876]
 [82.92967224]].
[2019-04-04 08:36:28,502] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1270674e-10 1.5787385e-09 5.0314342e-27 1.2298412e-25 1.9113126e-20
 1.0000000e+00 9.8118387e-20], sum to 1.0000
[2019-04-04 08:36:28,517] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0721
[2019-04-04 08:36:28,537] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 23.66666666666666, 0.0, 0.0, 26.0, 26.18170576787166, 0.6599614155225592, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4992000.0000, 
sim time next is 4992600.0000, 
raw observation next is [6.0, 23.33333333333334, 0.0, 0.0, 26.0, 26.30906828511218, 0.662132777389281, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6924223570926816, 0.720710925796427, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8206043], dtype=float32), 0.32630205]. 
=============================================
[2019-04-04 08:36:29,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:29,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:29,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run19
[2019-04-04 08:36:30,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:30,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:30,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run19
[2019-04-04 08:36:31,580] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:31,580] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:31,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run19
[2019-04-04 08:36:32,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:32,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:32,512] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run19
[2019-04-04 08:36:36,666] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:36,666] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:36,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run19
[2019-04-04 08:36:44,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:44,654] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:44,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run19
[2019-04-04 08:36:44,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:44,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:44,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run19
[2019-04-04 08:36:54,343] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.94789828e-08 5.05298665e-08 9.82924222e-23 6.84848275e-21
 1.80223266e-17 9.99999881e-01 1.10808976e-16], sum to 1.0000
[2019-04-04 08:36:54,343] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1632
[2019-04-04 08:36:54,442] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.54738901152137, -0.2980058871274366, 0.0, 1.0, 45023.61995836028], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 198600.0000, 
sim time next is 199200.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.4833243543006, -0.2223548512970458, 1.0, 1.0, 202343.6265148717], 
processed observation next is [1.0, 0.30434782608695654, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3736103628583833, 0.42588171623431803, 1.0, 1.0, 0.9635410786422461], 
reward next is 0.0365, 
noisyNet noise sample is [array([-1.3739003], dtype=float32), -0.6267192]. 
=============================================
[2019-04-04 08:36:55,262] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:36:55,262] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:36:55,272] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run19
[2019-04-04 08:37:05,873] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6103422e-09 6.1796754e-08 6.0676712e-22 1.9215610e-21 3.9725369e-17
 9.9999988e-01 2.3547738e-16], sum to 1.0000
[2019-04-04 08:37:05,873] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2464
[2019-04-04 08:37:05,983] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.1, 41.5, 0.0, 0.0, 26.0, 22.87396089058539, -0.1718418274959656, 1.0, 1.0, 203487.4415189996], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 459000.0000, 
sim time next is 459600.0000, 
raw observation next is [-8.0, 41.0, 0.0, 0.0, 26.0, 23.46338043244446, -0.05871066988511914, 1.0, 1.0, 159859.1910821923], 
processed observation next is [1.0, 0.30434782608695654, 0.24099722991689754, 0.41, 0.0, 0.0, 0.6666666666666666, 0.455281702703705, 0.4804297767049603, 1.0, 1.0, 0.7612342432485347], 
reward next is 0.2388, 
noisyNet noise sample is [array([1.887083], dtype=float32), -0.25963217]. 
=============================================
[2019-04-04 08:37:11,156] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1523170e-08 7.7213173e-08 2.3997281e-21 1.6464229e-20 9.7623836e-17
 9.9999988e-01 2.0429306e-16], sum to 1.0000
[2019-04-04 08:37:11,156] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6119
[2019-04-04 08:37:11,183] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 26.0, 22.84793708868429, -0.2343692503920415, 0.0, 1.0, 49193.78976218739], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 355800.0000, 
sim time next is 356400.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 22.74675652679871, -0.2489099481155672, 0.0, 1.0, 49272.9961415028], 
processed observation next is [1.0, 0.13043478260869565, 0.04709141274238226, 0.69, 0.0, 0.0, 0.6666666666666666, 0.39556304389989244, 0.41703001729481093, 0.0, 1.0, 0.23463331495953715], 
reward next is 0.7654, 
noisyNet noise sample is [array([-0.73690605], dtype=float32), 0.013546787]. 
=============================================
[2019-04-04 08:37:18,574] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.2932842e-11 2.3500131e-09 4.1518837e-28 5.3838161e-26 2.1607414e-21
 1.0000000e+00 1.9950087e-20], sum to 1.0000
[2019-04-04 08:37:18,574] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1420
[2019-04-04 08:37:18,589] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 17.5, 54.5, 26.0, 24.34837812765612, 0.1476811022201212, 0.0, 1.0, 41025.87807236706], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 547200.0000, 
sim time next is 547800.0000, 
raw observation next is [0.4166666666666667, 91.83333333333334, 23.0, 71.00000000000001, 26.0, 24.33096940491359, 0.1449987863523344, 0.0, 1.0, 40966.92989815716], 
processed observation next is [0.0, 0.34782608695652173, 0.47414589104339805, 0.9183333333333334, 0.07666666666666666, 0.07845303867403317, 0.6666666666666666, 0.527580783742799, 0.5483329287841114, 0.0, 1.0, 0.19508061856265316], 
reward next is 0.8049, 
noisyNet noise sample is [array([-1.317694], dtype=float32), 0.34401727]. 
=============================================
[2019-04-04 08:37:20,299] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4586941e-09 5.9982703e-08 5.7530862e-23 4.3005084e-21 1.2084441e-17
 9.9999988e-01 4.5739316e-17], sum to 1.0000
[2019-04-04 08:37:20,299] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8691
[2019-04-04 08:37:20,371] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.55, 68.5, 0.0, 0.0, 26.0, 22.70104735429482, -0.1460935594493507, 1.0, 1.0, 203423.3985671833], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 286200.0000, 
sim time next is 286800.0000, 
raw observation next is [-12.63333333333333, 69.0, 0.0, 0.0, 26.0, 23.29102667155507, -0.02507762302530738, 1.0, 1.0, 164688.3983496913], 
processed observation next is [1.0, 0.30434782608695654, 0.11265004616805181, 0.69, 0.0, 0.0, 0.6666666666666666, 0.44091888929625583, 0.49164079232489755, 1.0, 1.0, 0.7842304683318633], 
reward next is 0.2158, 
noisyNet noise sample is [array([-1.019006], dtype=float32), 0.24656862]. 
=============================================
[2019-04-04 08:37:20,567] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.3770919e-08 2.2664990e-07 5.1663904e-21 2.5857847e-19 5.1245562e-16
 9.9999976e-01 6.7866067e-15], sum to 1.0000
[2019-04-04 08:37:20,573] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9135
[2019-04-04 08:37:20,585] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.63918710968239, -0.5058561330646211, 0.0, 1.0, 49374.7972155149], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 370800.0000, 
sim time next is 371400.0000, 
raw observation next is [-16.28333333333333, 78.5, 0.0, 0.0, 26.0, 21.58261355467166, -0.5246877354090199, 0.0, 1.0, 49481.77494666538], 
processed observation next is [1.0, 0.30434782608695654, 0.011542012927054512, 0.785, 0.0, 0.0, 0.6666666666666666, 0.2985511295559717, 0.3251040881969934, 0.0, 1.0, 0.2356274997460256], 
reward next is 0.7644, 
noisyNet noise sample is [array([1.2778094], dtype=float32), 1.4102391]. 
=============================================
[2019-04-04 08:37:24,662] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8479721e-09 4.0182787e-08 5.7951544e-24 5.2033154e-22 2.8925321e-18
 1.0000000e+00 1.2406935e-17], sum to 1.0000
[2019-04-04 08:37:24,663] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7817
[2019-04-04 08:37:24,721] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 33.0, 42.5, 0.0, 26.0, 25.48337492945487, 0.2023709983942975, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 464400.0000, 
sim time next is 465000.0000, 
raw observation next is [-5.916666666666667, 32.83333333333334, 49.0, 0.0, 26.0, 25.55880040978671, 0.2006389632237813, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2987072945521699, 0.3283333333333334, 0.16333333333333333, 0.0, 0.6666666666666666, 0.6299000341488924, 0.5668796544079271, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05903145], dtype=float32), 0.61333627]. 
=============================================
[2019-04-04 08:37:24,725] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[72.98243 ]
 [72.93018 ]
 [72.935394]
 [72.72822 ]
 [72.484535]], R is [[73.24445343]
 [73.51200867]
 [73.77688599]
 [74.03911591]
 [74.29872894]].
[2019-04-04 08:37:37,585] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.8955579e-10 1.7952868e-08 2.4144898e-24 7.1265341e-23 1.1589261e-18
 1.0000000e+00 1.9667276e-18], sum to 1.0000
[2019-04-04 08:37:37,585] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3354
[2019-04-04 08:37:37,638] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.35, 32.5, 62.0, 0.0, 26.0, 25.54059419436192, 0.2006207042171787, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 466200.0000, 
sim time next is 466800.0000, 
raw observation next is [-5.066666666666666, 32.33333333333334, 68.0, 0.0, 26.0, 25.55585905932324, 0.2024909888042913, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.32225300092336107, 0.3233333333333334, 0.22666666666666666, 0.0, 0.6666666666666666, 0.6296549216102699, 0.5674969962680971, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0358386], dtype=float32), -0.252151]. 
=============================================
[2019-04-04 08:37:38,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5971755e-10 2.7323525e-08 3.7206371e-25 9.9571365e-24 2.6501986e-18
 1.0000000e+00 7.7039351e-18], sum to 1.0000
[2019-04-04 08:37:38,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8944
[2019-04-04 08:37:38,919] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.3, 75.0, 0.0, 0.0, 26.0, 24.13657741827112, 0.06734081222012471, 0.0, 1.0, 43531.62785442648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 618000.0000, 
sim time next is 618600.0000, 
raw observation next is [-4.399999999999999, 75.0, 0.0, 0.0, 26.0, 24.08503548858216, 0.05779724294182507, 0.0, 1.0, 43748.53409306304], 
processed observation next is [0.0, 0.13043478260869565, 0.3407202216066483, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5070862907151801, 0.519265747647275, 0.0, 1.0, 0.20832635282410972], 
reward next is 0.7917, 
noisyNet noise sample is [array([0.17755319], dtype=float32), 0.47177815]. 
=============================================
[2019-04-04 08:37:39,557] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.7920150e-10 6.8527588e-09 1.1603286e-25 5.3126607e-24 5.4725597e-20
 1.0000000e+00 5.7280029e-19], sum to 1.0000
[2019-04-04 08:37:39,557] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1863
[2019-04-04 08:37:39,612] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 69.0, 115.6666666666667, 42.5, 26.0, 25.23054592446461, 0.2512121994204277, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 638400.0000, 
sim time next is 639000.0000, 
raw observation next is [-3.9, 68.0, 135.0, 51.0, 26.0, 25.18752609135223, 0.2427398167666141, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.68, 0.45, 0.056353591160221, 0.6666666666666666, 0.5989605076126857, 0.580913272255538, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.95398307], dtype=float32), -2.1041303]. 
=============================================
[2019-04-04 08:37:39,615] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[79.76231 ]
 [79.313736]
 [78.907364]
 [78.56661 ]
 [78.25454 ]], R is [[80.37385559]
 [80.57011414]
 [80.76441193]
 [80.95677185]
 [81.14720154]].
[2019-04-04 08:37:40,378] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3679869e-10 1.5693296e-09 3.1853794e-26 1.0940400e-24 2.6046334e-20
 1.0000000e+00 5.3967073e-19], sum to 1.0000
[2019-04-04 08:37:40,380] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6545
[2019-04-04 08:37:40,419] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.63529496250853, 0.2140714654035001, 0.0, 1.0, 39912.65722973297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 865200.0000, 
sim time next is 865800.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.65257446765129, 0.2233866981183018, 0.0, 1.0, 39782.90920181813], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.6666666666666666, 0.5543812056376076, 0.5744622327061006, 0.0, 1.0, 0.18944242477056253], 
reward next is 0.8106, 
noisyNet noise sample is [array([2.140872], dtype=float32), 0.4803689]. 
=============================================
[2019-04-04 08:37:40,687] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7966997e-10 2.9582610e-09 1.9383345e-26 1.0037867e-24 1.9259579e-20
 1.0000000e+00 3.8175051e-20], sum to 1.0000
[2019-04-04 08:37:40,687] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5433
[2019-04-04 08:37:40,736] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.2, 59.0, 0.0, 0.0, 26.0, 25.01070949865711, 0.3235629941380637, 0.0, 1.0, 77195.01407838715], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 764400.0000, 
sim time next is 765000.0000, 
raw observation next is [-5.3, 59.5, 0.0, 0.0, 26.0, 25.02807846925322, 0.3206416653127399, 0.0, 1.0, 57310.32321455106], 
processed observation next is [1.0, 0.8695652173913043, 0.31578947368421056, 0.595, 0.0, 0.0, 0.6666666666666666, 0.5856732057711017, 0.6068805551042467, 0.0, 1.0, 0.27290630102167174], 
reward next is 0.7271, 
noisyNet noise sample is [array([-0.33282727], dtype=float32), -1.2911304]. 
=============================================
[2019-04-04 08:37:40,758] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.36569 ]
 [81.502365]
 [81.46039 ]
 [81.12602 ]
 [80.82985 ]], R is [[79.06079102]
 [78.90259552]
 [78.80915833]
 [78.49117279]
 [78.24497223]].
[2019-04-04 08:37:45,614] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.4749182e-10 6.8388140e-09 2.8800566e-26 2.3944290e-25 3.0892818e-20
 1.0000000e+00 6.3026994e-20], sum to 1.0000
[2019-04-04 08:37:45,615] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4393
[2019-04-04 08:37:45,647] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.0395531e-10 3.1169407e-08 1.1905714e-25 1.7126719e-24 2.8714991e-19
 1.0000000e+00 1.1958465e-18], sum to 1.0000
[2019-04-04 08:37:45,647] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8611
[2019-04-04 08:37:45,658] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 74.66666666666667, 0.0, 0.0, 26.0, 24.71410161706071, 0.1821757032946421, 0.0, 1.0, 39009.24921612224], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 879600.0000, 
sim time next is 880200.0000, 
raw observation next is [-0.8999999999999999, 74.0, 0.0, 0.0, 26.0, 24.64434960797146, 0.1757762266129836, 0.0, 1.0, 39031.10242158404], 
processed observation next is [1.0, 0.17391304347826086, 0.43767313019390586, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5536958006642884, 0.5585920755376612, 0.0, 1.0, 0.1858623924837335], 
reward next is 0.8141, 
noisyNet noise sample is [array([-0.29724947], dtype=float32), -1.9738951]. 
=============================================
[2019-04-04 08:37:45,659] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.3, 75.0, 0.0, 0.0, 26.0, 24.27116298436415, 0.05503309240614629, 0.0, 1.0, 41505.69803335999], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 702600.0000, 
sim time next is 703200.0000, 
raw observation next is [-3.2, 75.0, 0.0, 0.0, 26.0, 24.29300790817129, 0.06332658127738273, 0.0, 1.0, 41551.88538604445], 
processed observation next is [1.0, 0.13043478260869565, 0.37396121883656513, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5244173256809409, 0.5211088604257942, 0.0, 1.0, 0.19786612088592595], 
reward next is 0.8021, 
noisyNet noise sample is [array([1.9263484], dtype=float32), -1.432934]. 
=============================================
[2019-04-04 08:37:53,586] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0697014e-11 1.3980981e-09 9.4445979e-27 6.0465406e-26 2.4685949e-21
 1.0000000e+00 1.9127495e-20], sum to 1.0000
[2019-04-04 08:37:53,586] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4574
[2019-04-04 08:37:53,644] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 75.0, 69.66666666666666, 0.0, 26.0, 25.72535949682214, 0.3022074562259624, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 813000.0000, 
sim time next is 813600.0000, 
raw observation next is [-6.2, 75.0, 74.0, 0.0, 26.0, 25.71989125609372, 0.2992898062145439, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2908587257617729, 0.75, 0.24666666666666667, 0.0, 0.6666666666666666, 0.6433242713411434, 0.5997632687381813, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.07699123], dtype=float32), -0.46242183]. 
=============================================
[2019-04-04 08:37:58,523] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.93119756e-11 2.29134378e-10 1.22047685e-30 2.40412703e-28
 4.65360944e-23 1.00000000e+00 2.95322386e-22], sum to 1.0000
[2019-04-04 08:37:58,530] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5026
[2019-04-04 08:37:58,538] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.05, 74.0, 0.0, 0.0, 26.0, 25.67121097274541, 0.6402287445864445, 0.0, 1.0, 53247.10227817594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1125000.0000, 
sim time next is 1125600.0000, 
raw observation next is [10.86666666666667, 75.0, 0.0, 0.0, 26.0, 25.65507805034846, 0.6421215977372617, 0.0, 1.0, 43456.30485257973], 
processed observation next is [0.0, 0.0, 0.7636195752539245, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6379231708623717, 0.7140405325790873, 0.0, 1.0, 0.2069347850122844], 
reward next is 0.7931, 
noisyNet noise sample is [array([-0.6650216], dtype=float32), 0.18959598]. 
=============================================
[2019-04-04 08:38:05,268] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6661468e-12 4.6354774e-11 2.1222951e-31 1.3263031e-28 1.1315586e-23
 1.0000000e+00 6.2101611e-23], sum to 1.0000
[2019-04-04 08:38:05,269] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4380
[2019-04-04 08:38:05,287] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 45.0, 0.0, 26.0, 26.03730379980853, 0.588597221355959, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1330200.0000, 
sim time next is 1330800.0000, 
raw observation next is [0.5, 92.0, 54.5, 0.0, 26.0, 26.08792225195037, 0.5895144679483473, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.18166666666666667, 0.0, 0.6666666666666666, 0.6739935209958642, 0.696504822649449, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1495163], dtype=float32), -1.8155814]. 
=============================================
[2019-04-04 08:38:09,736] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6715226e-11 1.4411804e-09 7.4429272e-28 3.8028427e-26 1.7588495e-21
 1.0000000e+00 9.6254652e-22], sum to 1.0000
[2019-04-04 08:38:09,743] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6754
[2019-04-04 08:38:09,756] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.24129424529551, 0.4589079373845548, 0.0, 1.0, 45630.27990173167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1396800.0000, 
sim time next is 1397400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.27507672706881, 0.4608899176000655, 0.0, 1.0, 41068.09431772098], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6062563939224009, 0.6536299725333552, 0.0, 1.0, 0.1955623538939094], 
reward next is 0.8044, 
noisyNet noise sample is [array([-0.42722002], dtype=float32), -0.63157535]. 
=============================================
[2019-04-04 08:38:12,686] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0417195e-11 6.4566248e-11 8.9313497e-30 1.5641096e-27 1.1358840e-22
 1.0000000e+00 1.9629394e-21], sum to 1.0000
[2019-04-04 08:38:12,686] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1308
[2019-04-04 08:38:12,732] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.96706534765517, 0.4576913461369636, 1.0, 1.0, 67410.58191113203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1367400.0000, 
sim time next is 1368000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.90458596037231, 0.4627027194062487, 1.0, 1.0, 84083.71554617138], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5753821633643591, 0.654234239802083, 1.0, 1.0, 0.400398645457959], 
reward next is 0.5996, 
noisyNet noise sample is [array([0.3657849], dtype=float32), 2.2657244]. 
=============================================
[2019-04-04 08:38:12,740] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[87.94617 ]
 [87.60588 ]
 [88.755554]
 [90.00525 ]
 [91.29314 ]], R is [[88.19562531]
 [87.99266815]
 [88.02367401]
 [88.14344025]
 [88.26200867]].
[2019-04-04 08:38:22,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6425155e-11 1.0687599e-09 4.1915405e-28 3.9924660e-26 1.1780057e-21
 1.0000000e+00 3.5578037e-20], sum to 1.0000
[2019-04-04 08:38:22,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2950
[2019-04-04 08:38:22,451] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.06337973196759, 0.4626635307875394, 0.0, 1.0, 18704.3963878939], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1366800.0000, 
sim time next is 1367400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.9670653035517, 0.4576913349279345, 1.0, 1.0, 67410.6078861627], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.580588775295975, 0.6525637783093116, 1.0, 1.0, 0.3210028946960129], 
reward next is 0.6790, 
noisyNet noise sample is [array([-2.826146], dtype=float32), -2.1030796]. 
=============================================
[2019-04-04 08:38:26,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9179376e-09 6.6465120e-08 1.0286262e-24 6.0245610e-23 8.5014568e-19
 9.9999988e-01 6.7299059e-18], sum to 1.0000
[2019-04-04 08:38:26,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4353
[2019-04-04 08:38:26,411] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.16666666666667, 95.0, 0.0, 0.0, 26.0, 23.71721293629015, 0.1915391014589871, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1226400.0000, 
sim time next is 1227000.0000, 
raw observation next is [15.08333333333333, 95.5, 0.0, 0.0, 26.0, 23.69239478984344, 0.1869647932096251, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8804247460757156, 0.955, 0.0, 0.0, 0.6666666666666666, 0.47436623248695337, 0.5623215977365418, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.87721145], dtype=float32), 0.2018113]. 
=============================================
[2019-04-04 08:38:26,492] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.37023 ]
 [77.36659 ]
 [77.352615]
 [77.32921 ]
 [77.325294]], R is [[77.60648346]
 [77.83042145]
 [78.05211639]
 [78.27159882]
 [78.48888397]].
[2019-04-04 08:38:33,224] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5962161e-09 1.4872809e-08 8.5851229e-25 3.6111932e-23 3.9797833e-19
 1.0000000e+00 2.3945197e-18], sum to 1.0000
[2019-04-04 08:38:33,224] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2730
[2019-04-04 08:38:33,265] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 78.16666666666666, 0.0, 0.0, 26.0, 24.37724949317457, 0.1595942682828584, 0.0, 1.0, 45854.25793643053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1817400.0000, 
sim time next is 1818000.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 26.0, 24.36013967770334, 0.1528062961196614, 0.0, 1.0, 45927.68327435266], 
processed observation next is [0.0, 0.043478260869565216, 0.30747922437673136, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5300116398086118, 0.5509354320398872, 0.0, 1.0, 0.21870325368739363], 
reward next is 0.7813, 
noisyNet noise sample is [array([0.74685115], dtype=float32), -0.17300865]. 
=============================================
[2019-04-04 08:38:33,274] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.95742 ]
 [76.99955 ]
 [76.995285]
 [77.12401 ]
 [77.14131 ]], R is [[76.7996521 ]
 [76.81330109]
 [76.8271637 ]
 [76.84121704]
 [76.85543823]].
[2019-04-04 08:38:33,651] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.3145652e-12 2.7322924e-10 3.1593064e-30 8.1104978e-29 7.0667334e-23
 1.0000000e+00 2.2046602e-22], sum to 1.0000
[2019-04-04 08:38:33,652] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0047
[2019-04-04 08:38:33,698] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.516666666666667, 100.0, 27.66666666666666, 0.0, 26.0, 25.76735503647562, 0.4889379417918951, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1500600.0000, 
sim time next is 1501200.0000, 
raw observation next is [1.6, 100.0, 32.5, 0.0, 26.0, 25.82561487585496, 0.4933226061622971, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5069252077562327, 1.0, 0.10833333333333334, 0.0, 0.6666666666666666, 0.6521345729879133, 0.6644408687207657, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.58299613], dtype=float32), -0.7584801]. 
=============================================
[2019-04-04 08:38:51,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.21718666e-09 1.37918255e-08 1.45733375e-25 1.48818894e-23
 1.98642086e-19 1.00000000e+00 1.27995467e-18], sum to 1.0000
[2019-04-04 08:38:51,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5593
[2019-04-04 08:38:51,173] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 77.0, 186.0, 84.0, 26.0, 25.02389678478425, 0.2848436564665893, 0.0, 1.0, 45306.65095097603], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1866600.0000, 
sim time next is 1867200.0000, 
raw observation next is [-4.5, 79.0, 167.0, 70.0, 26.0, 25.02572517597086, 0.2847397046276891, 0.0, 1.0, 43144.55794133589], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.79, 0.5566666666666666, 0.07734806629834254, 0.6666666666666666, 0.5854770979975715, 0.5949132348758964, 0.0, 1.0, 0.20545027591112328], 
reward next is 0.7945, 
noisyNet noise sample is [array([-0.13630888], dtype=float32), 0.71259147]. 
=============================================
[2019-04-04 08:38:54,525] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.2264839e-11 2.9318362e-10 6.5276950e-28 3.0906513e-26 6.9124729e-21
 1.0000000e+00 4.6728798e-21], sum to 1.0000
[2019-04-04 08:38:54,525] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5941
[2019-04-04 08:38:54,580] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.00000000000001, 0.0, 0.0, 26.0, 25.22569947605835, 0.4614653420367969, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1707000.0000, 
sim time next is 1707600.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.2656161503121, 0.4539233792035079, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6054680125260082, 0.6513077930678359, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.2771938], dtype=float32), 1.6926293]. 
=============================================
[2019-04-04 08:39:01,952] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8275587e-09 1.4256085e-08 4.4439091e-24 5.7038752e-23 1.2294281e-18
 1.0000000e+00 3.4180658e-18], sum to 1.0000
[2019-04-04 08:39:01,960] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9241
[2019-04-04 08:39:02,045] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 78.16666666666666, 0.0, 0.0, 26.0, 24.37724949106302, 0.1595942677116169, 0.0, 1.0, 45854.25793800203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1817400.0000, 
sim time next is 1818000.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 26.0, 24.36013967557328, 0.1528062955496106, 0.0, 1.0, 45927.68327595118], 
processed observation next is [0.0, 0.043478260869565216, 0.30747922437673136, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5300116396311066, 0.5509354318498701, 0.0, 1.0, 0.21870325369500562], 
reward next is 0.7813, 
noisyNet noise sample is [array([0.746616], dtype=float32), 1.8621513]. 
=============================================
[2019-04-04 08:39:02,053] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.60521 ]
 [75.63612 ]
 [75.62211 ]
 [75.74103 ]
 [75.747574]], R is [[75.47259521]
 [75.49951935]
 [75.52651978]
 [75.55358124]
 [75.58068085]].
[2019-04-04 08:39:04,034] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5636051e-10 1.1300223e-09 1.8809356e-26 2.7252466e-24 3.9689108e-20
 1.0000000e+00 3.9446269e-19], sum to 1.0000
[2019-04-04 08:39:04,034] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1336
[2019-04-04 08:39:04,132] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.683333333333334, 73.0, 0.0, 0.0, 26.0, 25.29265682076706, 0.3643836424944691, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1969800.0000, 
sim time next is 1970400.0000, 
raw observation next is [-4.866666666666667, 75.0, 0.0, 0.0, 26.0, 25.33035619781897, 0.338640391533843, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3277931671283472, 0.75, 0.0, 0.0, 0.6666666666666666, 0.610863016484914, 0.612880130511281, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29045877], dtype=float32), 1.0244243]. 
=============================================
[2019-04-04 08:39:16,519] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-04 08:39:16,537] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:39:16,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:16,552] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:39:16,552] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:16,554] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run26
[2019-04-04 08:39:16,583] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:39:16,583] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:16,628] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run26
[2019-04-04 08:39:16,756] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run26
[2019-04-04 08:42:29,875] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.24836458], dtype=float32), 0.26883662]
[2019-04-04 08:42:29,876] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.533333333333333, 38.33333333333334, 0.0, 0.0, 26.0, 25.4011027390194, 0.3843544849784097, 0.0, 1.0, 28681.09349111491]
[2019-04-04 08:42:29,876] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:42:29,877] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.7000439e-10 2.3971275e-08 5.6250654e-25 2.0337251e-23 8.0139083e-19
 1.0000000e+00 2.5765508e-18], sampled 0.40926633568281334
[2019-04-04 08:42:33,793] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4829 239928582.5118 1604.8457
[2019-04-04 08:43:06,134] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 08:43:12,972] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 08:43:14,006] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2500000, evaluation results [2500000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.482940420242, 239928582.51175264, 1604.845674400583, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 08:43:49,469] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.6157179e-10 5.8096523e-09 2.6689990e-25 2.1033152e-23 1.1193637e-19
 1.0000000e+00 1.0164439e-18], sum to 1.0000
[2019-04-04 08:43:49,470] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3366
[2019-04-04 08:43:49,560] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.800000000000001, 78.16666666666667, 0.0, 0.0, 26.0, 23.77552772071866, 0.01076033114449041, 0.0, 1.0, 43331.41020077615], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2099400.0000, 
sim time next is 2100000.0000, 
raw observation next is [-6.9, 78.33333333333334, 0.0, 0.0, 26.0, 23.7338109049366, 0.09466537048040669, 1.0, 1.0, 202393.2886624862], 
processed observation next is [1.0, 0.30434782608695654, 0.27146814404432135, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.47781757541138337, 0.5315551234934689, 1.0, 1.0, 0.963777565059458], 
reward next is 0.0362, 
noisyNet noise sample is [array([0.88357997], dtype=float32), -0.1909998]. 
=============================================
[2019-04-04 08:43:49,563] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[75.634315]
 [75.88059 ]
 [76.09822 ]
 [76.31905 ]
 [76.53547 ]], R is [[76.59671021]
 [76.62440491]
 [76.65130615]
 [76.67751312]
 [76.70314026]].
[2019-04-04 08:43:54,677] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7057304e-10 2.6127585e-09 1.1985962e-25 4.7631259e-25 4.7066942e-20
 1.0000000e+00 9.7196003e-20], sum to 1.0000
[2019-04-04 08:43:54,677] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0874
[2019-04-04 08:43:54,704] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.2, 45.66666666666667, 57.16666666666667, 6.999999999999998, 26.0, 25.89612092352017, 0.4087784644035344, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2305200.0000, 
sim time next is 2305800.0000, 
raw observation next is [-0.3, 46.5, 41.0, 0.0, 26.0, 25.88769305485939, 0.3947280279389671, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4542936288088643, 0.465, 0.13666666666666666, 0.0, 0.6666666666666666, 0.6573077545716158, 0.6315760093129891, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3124911], dtype=float32), 0.41605935]. 
=============================================
[2019-04-04 08:43:55,314] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8317528e-10 4.6687387e-09 1.1995568e-25 2.3453336e-24 1.7365573e-19
 1.0000000e+00 1.3455577e-18], sum to 1.0000
[2019-04-04 08:43:55,326] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2055
[2019-04-04 08:43:55,342] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 26.0, 24.80076006596724, 0.2686743964070832, 0.0, 1.0, 42204.93641430655], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2156400.0000, 
sim time next is 2157000.0000, 
raw observation next is [-7.3, 82.00000000000001, 0.0, 0.0, 26.0, 24.73039569730869, 0.2553642701049253, 0.0, 1.0, 42245.82307659354], 
processed observation next is [1.0, 1.0, 0.26038781163434904, 0.8200000000000002, 0.0, 0.0, 0.6666666666666666, 0.5608663081090576, 0.5851214233683084, 0.0, 1.0, 0.20117058607901683], 
reward next is 0.7988, 
noisyNet noise sample is [array([-0.84125215], dtype=float32), -0.78035814]. 
=============================================
[2019-04-04 08:43:55,358] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.55981 ]
 [78.61449 ]
 [78.733536]
 [78.77701 ]
 [78.72352 ]], R is [[78.50393677]
 [78.51792145]
 [78.53198242]
 [78.54619598]
 [78.55945587]].
[2019-04-04 08:44:18,054] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3592197e-09 1.6693553e-08 7.9617172e-25 1.7417418e-23 2.4283329e-19
 1.0000000e+00 4.1438171e-19], sum to 1.0000
[2019-04-04 08:44:18,070] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4590
[2019-04-04 08:44:18,089] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.68407757142769, -0.01663578602112774, 0.0, 1.0, 43271.28039951844], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2266200.0000, 
sim time next is 2266800.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.64195419534897, -0.02871173717167374, 0.0, 1.0, 43241.56119282253], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4701628496124141, 0.4904294209427755, 0.0, 1.0, 0.20591219615629777], 
reward next is 0.7941, 
noisyNet noise sample is [array([0.8603993], dtype=float32), 1.430873]. 
=============================================
[2019-04-04 08:44:18,138] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4917236e-10 3.4509419e-08 7.0998532e-25 1.7912677e-23 4.2475419e-19
 1.0000000e+00 4.1198594e-19], sum to 1.0000
[2019-04-04 08:44:18,139] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0840
[2019-04-04 08:44:18,173] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 49.0, 0.0, 0.0, 26.0, 24.97477623761025, 0.1691429071736763, 0.0, 1.0, 38367.70237698045], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2520000.0000, 
sim time next is 2520600.0000, 
raw observation next is [-1.8, 50.33333333333334, 0.0, 0.0, 26.0, 24.89582860530006, 0.1571427908531271, 0.0, 1.0, 38390.8314039379], 
processed observation next is [1.0, 0.17391304347826086, 0.41274238227146814, 0.5033333333333334, 0.0, 0.0, 0.6666666666666666, 0.574652383775005, 0.5523809302843757, 0.0, 1.0, 0.18281348287589474], 
reward next is 0.8172, 
noisyNet noise sample is [array([-0.17000522], dtype=float32), -2.5898552]. 
=============================================
[2019-04-04 08:44:40,819] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4592272e-11 8.0870033e-10 6.9814816e-28 9.2903070e-26 6.2137222e-21
 1.0000000e+00 1.4802250e-20], sum to 1.0000
[2019-04-04 08:44:40,823] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3934
[2019-04-04 08:44:40,842] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 29.0, 5.0, 46.0, 26.0, 25.73447930322649, 0.3608104894687671, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2827800.0000, 
sim time next is 2828400.0000, 
raw observation next is [5.333333333333333, 29.33333333333334, 0.0, 0.0, 26.0, 25.64074102747557, 0.3688328191726291, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6103416435826409, 0.2933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6367284189562975, 0.6229442730575431, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6250448], dtype=float32), -2.3261344]. 
=============================================
[2019-04-04 08:44:51,137] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0721097e-10 9.2075236e-10 8.8161708e-27 1.8113579e-25 2.0511103e-20
 1.0000000e+00 7.8289745e-21], sum to 1.0000
[2019-04-04 08:44:51,137] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8573
[2019-04-04 08:44:51,178] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.166666666666667, 50.66666666666667, 111.3333333333333, 784.0, 26.0, 26.16211062447505, 0.4930549962950528, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3333000.0000, 
sim time next is 3333600.0000, 
raw observation next is [-4.0, 50.0, 110.0, 776.0, 26.0, 25.73191515685676, 0.5234503092038599, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3518005540166205, 0.5, 0.36666666666666664, 0.8574585635359117, 0.6666666666666666, 0.6443262630713967, 0.6744834364012866, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2975593], dtype=float32), -2.4188385]. 
=============================================
[2019-04-04 08:44:55,051] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.10393326e-10 1.74689108e-09 2.28911473e-26 8.88617810e-24
 8.48153260e-20 1.00000000e+00 9.78325225e-19], sum to 1.0000
[2019-04-04 08:44:55,053] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9333
[2019-04-04 08:44:55,085] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.166666666666667, 43.33333333333333, 105.6666666666667, 795.6666666666666, 26.0, 25.11203036722713, 0.3624247028188539, 0.0, 1.0, 18710.29953285033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3073800.0000, 
sim time next is 3074400.0000, 
raw observation next is [-1.0, 42.0, 104.0, 790.5, 26.0, 25.11654289567031, 0.3627777553716112, 0.0, 1.0, 18708.99029009449], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.3466666666666667, 0.8734806629834254, 0.6666666666666666, 0.5930452413058592, 0.6209259184572037, 0.0, 1.0, 0.08909042995283091], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.0948781], dtype=float32), -0.9053933]. 
=============================================
[2019-04-04 08:44:58,536] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1218625e-11 3.1847369e-10 1.2294083e-27 8.0838842e-26 3.3766543e-21
 1.0000000e+00 1.9873436e-21], sum to 1.0000
[2019-04-04 08:44:58,541] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4713
[2019-04-04 08:44:58,590] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 48.0, 87.0, 674.0, 26.0, 26.70901119396065, 0.6052728418123406, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3339000.0000, 
sim time next is 3339600.0000, 
raw observation next is [-2.333333333333333, 47.33333333333333, 82.5, 645.1666666666667, 26.0, 26.0506796920674, 0.6325985856478772, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3979686057248385, 0.4733333333333333, 0.275, 0.7128913443830571, 0.6666666666666666, 0.6708899743389501, 0.7108661952159591, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.87827486], dtype=float32), -0.15320133]. 
=============================================
[2019-04-04 08:44:59,301] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4571863e-10 4.5532844e-10 7.0557822e-27 2.0308468e-25 3.3730513e-20
 1.0000000e+00 1.6388160e-20], sum to 1.0000
[2019-04-04 08:44:59,302] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4337
[2019-04-04 08:44:59,335] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 48.66666666666666, 51.83333333333334, 439.6666666666667, 26.0, 26.59953386540966, 0.5194392023937865, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3343200.0000, 
sim time next is 3343800.0000, 
raw observation next is [-2.0, 49.33333333333334, 43.66666666666667, 378.3333333333334, 26.0, 26.53206680802653, 0.6496512455390538, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.4933333333333334, 0.14555555555555558, 0.41804788213628, 0.6666666666666666, 0.7110055673355443, 0.7165504151796847, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5247808], dtype=float32), 0.027679734]. 
=============================================
[2019-04-04 08:45:08,817] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3792766e-09 1.4481738e-09 7.8165069e-27 4.5929365e-25 8.8316143e-21
 1.0000000e+00 1.7504509e-19], sum to 1.0000
[2019-04-04 08:45:08,827] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4849
[2019-04-04 08:45:08,842] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.39567201773448, 0.3440401596855704, 0.0, 1.0, 34719.49482060198], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3103200.0000, 
sim time next is 3103800.0000, 
raw observation next is [-0.8333333333333334, 100.0, 0.0, 0.0, 26.0, 25.39780066659842, 0.3403134573270234, 0.0, 1.0, 35210.04288854329], 
processed observation next is [0.0, 0.9565217391304348, 0.43951985226223456, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6164833888832018, 0.6134378191090079, 0.0, 1.0, 0.1676668708978252], 
reward next is 0.8323, 
noisyNet noise sample is [array([0.85181886], dtype=float32), 0.8171066]. 
=============================================
[2019-04-04 08:45:13,493] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3625706e-09 1.5142678e-08 2.6806424e-24 1.4028113e-22 4.6223767e-18
 1.0000000e+00 3.9029880e-18], sum to 1.0000
[2019-04-04 08:45:13,494] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5698
[2019-04-04 08:45:13,508] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 42.33333333333334, 70.66666666666667, 576.3333333333333, 26.0, 25.32315696646668, 0.4531210238781937, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3600600.0000, 
sim time next is 3601200.0000, 
raw observation next is [0.0, 41.66666666666667, 66.83333333333333, 545.6666666666666, 26.0, 25.30576459706464, 0.4471428729929647, 0.0, 1.0, 18684.31268463445], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.41666666666666674, 0.22277777777777777, 0.6029465930018416, 0.6666666666666666, 0.6088137164220534, 0.6490476243309883, 0.0, 1.0, 0.08897291754587833], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.36826742], dtype=float32), -1.039103]. 
=============================================
[2019-04-04 08:45:18,003] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4481381e-09 1.7467712e-09 2.2809385e-26 5.6952409e-24 3.6182037e-20
 1.0000000e+00 7.5199565e-20], sum to 1.0000
[2019-04-04 08:45:18,004] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5352
[2019-04-04 08:45:18,020] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.58969351040121, 0.5529030067622883, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3790800.0000, 
sim time next is 3791400.0000, 
raw observation next is [-3.0, 72.0, 0.0, 0.0, 26.0, 25.63965091425008, 0.5474177350600579, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3795013850415513, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6366375761875066, 0.6824725783533526, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8355409], dtype=float32), 0.69284415]. 
=============================================
[2019-04-04 08:45:18,510] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.7284602e-12 5.5908267e-10 9.1245144e-29 2.7724387e-27 8.6962101e-23
 1.0000000e+00 6.2219497e-22], sum to 1.0000
[2019-04-04 08:45:18,511] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8172
[2019-04-04 08:45:18,522] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.333333333333333, 67.0, 115.6666666666667, 823.1666666666666, 26.0, 26.46200068963748, 0.5943524528362522, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3757200.0000, 
sim time next is 3757800.0000, 
raw observation next is [-2.166666666666667, 66.0, 116.3333333333333, 824.3333333333334, 26.0, 26.49326282704165, 0.5943016467144335, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4025854108956602, 0.66, 0.38777777777777767, 0.910865561694291, 0.6666666666666666, 0.7077719022534709, 0.6981005489048112, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1277387], dtype=float32), 1.8124732]. 
=============================================
[2019-04-04 08:45:21,625] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3216636e-10 1.2464529e-09 3.8956220e-27 2.1971918e-26 5.6259785e-21
 1.0000000e+00 4.7027613e-21], sum to 1.0000
[2019-04-04 08:45:21,626] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9777
[2019-04-04 08:45:21,650] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.166666666666667, 65.0, 112.6666666666667, 759.3333333333333, 26.0, 26.35332392442718, 0.5912010258487004, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3322200.0000, 
sim time next is 3322800.0000, 
raw observation next is [-7.0, 64.0, 113.5, 769.0, 26.0, 26.37756357866645, 0.595245898753449, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.2686980609418283, 0.64, 0.37833333333333335, 0.8497237569060774, 0.6666666666666666, 0.6981302982222042, 0.6984152995844829, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01753524], dtype=float32), -1.8788118]. 
=============================================
[2019-04-04 08:45:25,526] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2250143e-09 1.3258436e-08 1.8281849e-25 4.8101464e-24 1.9475310e-20
 1.0000000e+00 6.9205539e-19], sum to 1.0000
[2019-04-04 08:45:25,533] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7781
[2019-04-04 08:45:25,550] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.86919532449953, 0.2823700551186903, 0.0, 1.0, 43978.25098933714], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3818400.0000, 
sim time next is 3819000.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.82938412158337, 0.2788454864524402, 0.0, 1.0, 43989.55834713976], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5691153434652808, 0.5929484954841467, 0.0, 1.0, 0.2094740873673322], 
reward next is 0.7905, 
noisyNet noise sample is [array([0.05423119], dtype=float32), -1.5191991]. 
=============================================
[2019-04-04 08:45:25,553] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.448044]
 [78.55581 ]
 [78.66346 ]
 [78.753136]
 [78.85992 ]], R is [[78.35054779]
 [78.35762024]
 [78.36481476]
 [78.37220764]
 [78.37982178]].
[2019-04-04 08:45:26,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9376282e-10 6.7878720e-09 4.4726950e-27 2.8047062e-24 1.3219198e-19
 1.0000000e+00 2.2745515e-19], sum to 1.0000
[2019-04-04 08:45:26,102] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3217
[2019-04-04 08:45:26,115] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.49358161850459, 0.3703319787103798, 0.0, 1.0, 35230.71284634083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3634800.0000, 
sim time next is 3635400.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.50142325610013, 0.3705087786623166, 0.0, 1.0, 27975.74533634291], 
processed observation next is [0.0, 0.043478260869565216, 0.7119113573407203, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6251186046750107, 0.6235029262207722, 0.0, 1.0, 0.13321783493496622], 
reward next is 0.8668, 
noisyNet noise sample is [array([0.01277976], dtype=float32), 1.2981254]. 
=============================================
[2019-04-04 08:45:29,030] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3390416e-11 1.3933746e-10 6.2422331e-28 1.0244821e-26 1.2202589e-21
 1.0000000e+00 5.4685084e-21], sum to 1.0000
[2019-04-04 08:45:29,031] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9105
[2019-04-04 08:45:29,085] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.05334964605034, 0.4154309948528388, 1.0, 1.0, 86696.91098990476], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3439800.0000, 
sim time next is 3440400.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 24.98567922125209, 0.4260419940260311, 0.0, 1.0, 94791.08974674073], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5821399351043409, 0.6420139980086771, 0.0, 1.0, 0.4513861416511464], 
reward next is 0.5486, 
noisyNet noise sample is [array([0.40985823], dtype=float32), 0.20677656]. 
=============================================
[2019-04-04 08:45:36,695] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.1882279e-11 2.2930413e-09 1.1110556e-26 3.2545589e-24 1.5194314e-20
 1.0000000e+00 7.4214991e-19], sum to 1.0000
[2019-04-04 08:45:36,695] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0315
[2019-04-04 08:45:36,732] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.27952623808578, 0.4192900841519785, 0.0, 1.0, 43705.61699158676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3801600.0000, 
sim time next is 3802200.0000, 
raw observation next is [-3.166666666666667, 72.0, 0.0, 0.0, 26.0, 25.25938388240499, 0.412472435389551, 0.0, 1.0, 43740.93417869663], 
processed observation next is [1.0, 0.0, 0.3748845798707295, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6049486568670824, 0.637490811796517, 0.0, 1.0, 0.20829016275569825], 
reward next is 0.7917, 
noisyNet noise sample is [array([-0.88151646], dtype=float32), 0.7042675]. 
=============================================
[2019-04-04 08:45:48,180] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7145504e-10 6.3708168e-09 4.7752740e-27 6.7416250e-25 1.2251244e-20
 1.0000000e+00 3.5202615e-19], sum to 1.0000
[2019-04-04 08:45:48,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0579
[2019-04-04 08:45:48,230] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 59.5, 111.0, 758.0, 26.0, 25.57117083339504, 0.4816104547708062, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3580200.0000, 
sim time next is 3580800.0000, 
raw observation next is [-4.333333333333334, 57.66666666666667, 111.5, 767.6666666666667, 26.0, 25.50010391073022, 0.471965352702187, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3425669436749769, 0.5766666666666667, 0.37166666666666665, 0.8482504604051566, 0.6666666666666666, 0.6250086592275185, 0.6573217842340623, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28695124], dtype=float32), -0.19290426]. 
=============================================
[2019-04-04 08:45:55,959] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5154259e-10 1.5747897e-09 4.5367369e-28 1.0384151e-25 3.0943869e-21
 1.0000000e+00 6.1937281e-20], sum to 1.0000
[2019-04-04 08:45:55,960] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7657
[2019-04-04 08:45:55,979] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.35, 75.5, 0.0, 0.0, 26.0, 25.53340598579315, 0.403251406668364, 0.0, 1.0, 18745.13104393443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4321800.0000, 
sim time next is 4322400.0000, 
raw observation next is [4.3, 75.33333333333334, 0.0, 0.0, 26.0, 25.50772856899009, 0.4021014169000985, 0.0, 1.0, 37512.7629353864], 
processed observation next is [1.0, 0.0, 0.5817174515235458, 0.7533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6256440474158408, 0.6340338056333662, 0.0, 1.0, 0.17863220445422096], 
reward next is 0.8214, 
noisyNet noise sample is [array([-1.2135668], dtype=float32), -0.8386663]. 
=============================================
[2019-04-04 08:45:57,592] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7602668e-12 1.9694871e-10 1.1398646e-29 1.1529590e-27 1.4596894e-22
 1.0000000e+00 2.0495371e-21], sum to 1.0000
[2019-04-04 08:45:57,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2538
[2019-04-04 08:45:57,628] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 60.0, 96.5, 743.5, 26.0, 26.72314536089698, 0.6799097403920027, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3769200.0000, 
sim time next is 3769800.0000, 
raw observation next is [0.0, 60.0, 93.33333333333334, 732.6666666666667, 26.0, 26.76071980533167, 0.6902562905711424, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.3111111111111111, 0.8095764272559853, 0.6666666666666666, 0.7300599837776393, 0.7300854301903809, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0240419], dtype=float32), -0.24352558]. 
=============================================
[2019-04-04 08:45:59,612] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3706520e-09 5.5759308e-09 1.0868438e-24 6.2488720e-23 7.1494756e-19
 1.0000000e+00 3.4253493e-18], sum to 1.0000
[2019-04-04 08:45:59,619] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9787
[2019-04-04 08:45:59,670] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.4, 42.0, 0.0, 0.0, 26.0, 25.01469681776178, 0.3000753657060189, 0.0, 1.0, 22700.40930506846], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4215600.0000, 
sim time next is 4216200.0000, 
raw observation next is [1.333333333333333, 42.16666666666666, 0.0, 0.0, 26.0, 25.00313998094597, 0.2939366407841337, 0.0, 1.0, 29825.46038514846], 
processed observation next is [0.0, 0.8260869565217391, 0.4995383194829178, 0.4216666666666666, 0.0, 0.0, 0.6666666666666666, 0.5835949984121642, 0.5979788802613779, 0.0, 1.0, 0.1420260018340403], 
reward next is 0.8580, 
noisyNet noise sample is [array([0.6557518], dtype=float32), -0.46614164]. 
=============================================
[2019-04-04 08:46:00,679] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7283034e-11 6.4907074e-10 3.7085566e-29 6.3085700e-27 2.5323167e-22
 1.0000000e+00 2.7550304e-21], sum to 1.0000
[2019-04-04 08:46:00,680] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7987
[2019-04-04 08:46:00,726] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.2666666666666667, 72.33333333333334, 113.0, 55.0, 26.0, 25.84199619743781, 0.4857563334423485, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4524000.0000, 
sim time next is 4524600.0000, 
raw observation next is [-0.1333333333333333, 72.16666666666666, 115.0, 44.00000000000001, 26.0, 25.925798897685, 0.5087866390391557, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4589104339796861, 0.7216666666666666, 0.38333333333333336, 0.04861878453038675, 0.6666666666666666, 0.6604832414737499, 0.6695955463463852, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.92400634], dtype=float32), -0.04014355]. 
=============================================
[2019-04-04 08:46:05,048] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.34991784e-09 6.88539785e-08 3.47450662e-23 3.56126220e-22
 1.29747076e-17 9.99999881e-01 4.81407304e-17], sum to 1.0000
[2019-04-04 08:46:05,050] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5136
[2019-04-04 08:46:05,064] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.1496286089317, 0.3628116850914495, 0.0, 1.0, 40721.00678499127], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4060200.0000, 
sim time next is 4060800.0000, 
raw observation next is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.12617167407436, 0.3560757010546274, 0.0, 1.0, 40693.6913267208], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.37, 0.0, 0.0, 0.6666666666666666, 0.5938476395061967, 0.6186919003515424, 0.0, 1.0, 0.1937794825081943], 
reward next is 0.8062, 
noisyNet noise sample is [array([0.9889275], dtype=float32), -0.82484204]. 
=============================================
[2019-04-04 08:46:17,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.70924125e-11 7.60335683e-10 5.54119011e-27 1.98520273e-25
 1.78355791e-21 1.00000000e+00 1.12711865e-20], sum to 1.0000
[2019-04-04 08:46:17,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2683
[2019-04-04 08:46:17,795] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 18.66666666666667, 18.66666666666667, 26.0, 25.56865384433846, 0.345527051538691, 1.0, 1.0, 28310.95432949887], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4557000.0000, 
sim time next is 4557600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.53289610481542, 0.451113378670124, 1.0, 1.0, 19875.61658156659], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6277413420679517, 0.6503711262233747, 1.0, 1.0, 0.0946457932455552], 
reward next is 0.9054, 
noisyNet noise sample is [array([-1.093761], dtype=float32), -2.096237]. 
=============================================
[2019-04-04 08:46:28,535] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9813321e-09 1.6885211e-08 2.8142110e-25 3.5777351e-24 7.1368316e-19
 1.0000000e+00 9.1711820e-19], sum to 1.0000
[2019-04-04 08:46:28,536] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9016
[2019-04-04 08:46:28,585] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 26.0, 25.57353414119829, 0.4464752628098805, 0.0, 1.0, 24631.91198535369], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5015400.0000, 
sim time next is 5016000.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 26.0, 25.5573492862305, 0.4396019058086598, 0.0, 1.0, 37453.55629868677], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.4, 0.0, 0.0, 0.6666666666666666, 0.629779107185875, 0.6465339686028866, 0.0, 1.0, 0.17835026808898463], 
reward next is 0.8216, 
noisyNet noise sample is [array([-0.59042454], dtype=float32), 1.470306]. 
=============================================
[2019-04-04 08:46:28,603] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.18731 ]
 [79.25064 ]
 [79.527855]
 [79.59309 ]
 [79.651054]], R is [[79.09252167]
 [79.18430328]
 [79.39246368]
 [79.5092926 ]
 [79.62493896]].
[2019-04-04 08:46:29,592] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.4335361e-10 2.6783273e-09 6.2456219e-26 6.2051109e-25 1.3820392e-20
 1.0000000e+00 1.1208720e-19], sum to 1.0000
[2019-04-04 08:46:29,594] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2564
[2019-04-04 08:46:29,622] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 78.16666666666667, 0.0, 0.0, 26.0, 25.23702252154104, 0.4364077798963332, 0.0, 1.0, 44520.99690527959], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4747800.0000, 
sim time next is 4748400.0000, 
raw observation next is [-3.0, 77.0, 0.0, 0.0, 26.0, 25.20485588250902, 0.4320462410759723, 0.0, 1.0, 42786.29812309823], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6004046568757516, 0.6440154136919908, 0.0, 1.0, 0.20374427677665824], 
reward next is 0.7963, 
noisyNet noise sample is [array([-0.59566593], dtype=float32), -0.6504191]. 
=============================================
[2019-04-04 08:46:30,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:46:30,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:46:30,918] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run20
[2019-04-04 08:46:33,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:46:33,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:46:33,282] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run20
[2019-04-04 08:46:33,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:46:33,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:46:33,975] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run20
[2019-04-04 08:46:34,917] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:46:34,917] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:46:34,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run20
[2019-04-04 08:46:36,546] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5721670e-11 5.7822804e-09 4.4149724e-28 1.5479896e-25 1.9706683e-20
 1.0000000e+00 1.2529346e-19], sum to 1.0000
[2019-04-04 08:46:36,546] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7561
[2019-04-04 08:46:36,596] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.1666666666666667, 51.16666666666667, 288.6666666666666, 260.0, 26.0, 24.97898150457865, 0.3351604756876691, 0.0, 1.0, 32013.84086552718], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4878600.0000, 
sim time next is 4879200.0000, 
raw observation next is [0.0666666666666666, 50.33333333333334, 285.8333333333333, 284.0, 26.0, 24.99449074311573, 0.3450167918472194, 0.0, 1.0, 18716.84457968229], 
processed observation next is [0.0, 0.4782608695652174, 0.46445060018467227, 0.5033333333333334, 0.9527777777777777, 0.3138121546961326, 0.6666666666666666, 0.5828742285929774, 0.6150055972824064, 0.0, 1.0, 0.08912783133182042], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.01151137], dtype=float32), 2.4382846]. 
=============================================
[2019-04-04 08:46:39,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:46:39,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:46:39,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run20
[2019-04-04 08:46:39,916] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:46:39,916] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:46:39,920] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run20
[2019-04-04 08:46:40,219] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.0430396e-10 7.2712756e-09 7.2119309e-26 3.1016755e-23 3.6406207e-19
 1.0000000e+00 5.3899590e-19], sum to 1.0000
[2019-04-04 08:46:40,219] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9404
[2019-04-04 08:46:40,271] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 35.0, 73.83333333333334, 488.3333333333333, 26.0, 25.21070579468701, 0.4205323611842324, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4812000.0000, 
sim time next is 4812600.0000, 
raw observation next is [3.0, 34.5, 65.66666666666667, 427.6666666666667, 26.0, 25.20335768509565, 0.4078852072919708, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.345, 0.2188888888888889, 0.47255985267034994, 0.6666666666666666, 0.6002798070913041, 0.6359617357639903, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0970323], dtype=float32), 0.79422027]. 
=============================================
[2019-04-04 08:46:40,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:46:40,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:46:40,716] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run20
[2019-04-04 08:46:45,347] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7180171e-10 1.1290017e-08 4.2454534e-26 2.5516205e-24 3.2306182e-20
 1.0000000e+00 1.4128832e-19], sum to 1.0000
[2019-04-04 08:46:45,347] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2168
[2019-04-04 08:46:45,387] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.45343541489133, 0.4037385770425423, 0.0, 1.0, 44397.96654327291], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5024400.0000, 
sim time next is 5025000.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.45268755793769, 0.3951481697841717, 0.0, 1.0, 40045.65255519385], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6210572964948075, 0.6317160565947239, 0.0, 1.0, 0.19069358359616118], 
reward next is 0.8093, 
noisyNet noise sample is [array([-0.37903056], dtype=float32), 0.62115467]. 
=============================================
[2019-04-04 08:46:45,426] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[79.08872]
 [79.05117]
 [79.10705]
 [79.10641]
 [79.11588]], R is [[79.14330292]
 [79.14044952]
 [79.21779633]
 [79.21955872]
 [79.2940979 ]].
[2019-04-04 08:46:50,186] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:46:50,187] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:46:50,190] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run20
[2019-04-04 08:46:50,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:46:50,434] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:46:50,439] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run20
[2019-04-04 08:46:50,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:46:50,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:46:50,959] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run20
[2019-04-04 08:46:52,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:46:52,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:46:52,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run20
[2019-04-04 08:46:53,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:46:53,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:46:53,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run20
[2019-04-04 08:46:55,515] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0819055e-09 5.1407287e-09 6.3764179e-25 1.3416079e-23 7.7500909e-19
 1.0000000e+00 4.4304997e-18], sum to 1.0000
[2019-04-04 08:46:55,516] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6842
[2019-04-04 08:46:55,573] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 26.0, 25.39899672218026, 0.3469320563161076, 0.0, 1.0, 30634.72933255359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4924800.0000, 
sim time next is 4925400.0000, 
raw observation next is [0.8333333333333334, 40.5, 0.0, 0.0, 26.0, 25.39002040116719, 0.3417667450011272, 0.0, 1.0, 38552.3253743102], 
processed observation next is [1.0, 0.0, 0.4856879039704525, 0.405, 0.0, 0.0, 0.6666666666666666, 0.6158350334305993, 0.6139222483337091, 0.0, 1.0, 0.18358250178242955], 
reward next is 0.8164, 
noisyNet noise sample is [array([-0.55008656], dtype=float32), -0.90171915]. 
=============================================
[2019-04-04 08:46:58,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:46:58,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:46:58,831] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run20
[2019-04-04 08:47:01,983] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.1043968e-11 2.6600149e-09 1.7356173e-26 1.8579321e-24 6.0366950e-20
 1.0000000e+00 1.5863007e-19], sum to 1.0000
[2019-04-04 08:47:01,983] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5183
[2019-04-04 08:47:02,024] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 63.5, 18.0, 0.0, 26.0, 25.29292455704604, 0.273326681360082, 1.0, 1.0, 35723.38178545182], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 232200.0000, 
sim time next is 232800.0000, 
raw observation next is [-3.4, 64.0, 15.0, 0.0, 26.0, 25.33267042078365, 0.2774934036278924, 1.0, 1.0, 32759.70007622046], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.64, 0.05, 0.0, 0.6666666666666666, 0.6110558683986375, 0.5924978012092975, 1.0, 1.0, 0.155998571791526], 
reward next is 0.8440, 
noisyNet noise sample is [array([-0.7614983], dtype=float32), -0.27359003]. 
=============================================
[2019-04-04 08:47:05,334] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:47:05,334] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:47:05,339] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run20
[2019-04-04 08:47:05,867] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5670047e-10 3.5350820e-09 1.2647970e-25 6.3213231e-24 5.5775380e-20
 1.0000000e+00 6.1898814e-19], sum to 1.0000
[2019-04-04 08:47:05,867] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3929
[2019-04-04 08:47:05,939] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 26.0, 24.11255932050526, 0.1405152667276249, 0.0, 1.0, 158981.0772210949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 114000.0000, 
sim time next is 114600.0000, 
raw observation next is [-7.299999999999999, 68.0, 12.33333333333333, 2.999999999999999, 26.0, 24.51583230206155, 0.1988713074125084, 1.0, 1.0, 109960.6595101405], 
processed observation next is [1.0, 0.30434782608695654, 0.2603878116343491, 0.68, 0.0411111111111111, 0.0033149171270718224, 0.6666666666666666, 0.5429860251717958, 0.5662904358041695, 1.0, 1.0, 0.5236221881435261], 
reward next is 0.4764, 
noisyNet noise sample is [array([0.40325102], dtype=float32), 0.05979694]. 
=============================================
[2019-04-04 08:47:07,320] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:47:07,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:47:07,768] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run20
[2019-04-04 08:47:11,478] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6661024e-08 6.1975470e-08 3.2608248e-22 8.7053126e-21 4.4126731e-17
 9.9999988e-01 4.5394650e-16], sum to 1.0000
[2019-04-04 08:47:11,479] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5268
[2019-04-04 08:47:11,507] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.9, 77.83333333333334, 0.0, 0.0, 26.0, 24.61080935577683, 0.1887749438227365, 0.0, 1.0, 47313.11988472302], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 335400.0000, 
sim time next is 336000.0000, 
raw observation next is [-13.0, 78.66666666666667, 0.0, 0.0, 26.0, 24.53147391939792, 0.1716392515649951, 0.0, 1.0, 47496.42632474878], 
processed observation next is [1.0, 0.9130434782608695, 0.10249307479224376, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5442894932831601, 0.5572130838549983, 0.0, 1.0, 0.22617345868927993], 
reward next is 0.7738, 
noisyNet noise sample is [array([-0.9694273], dtype=float32), -0.7705987]. 
=============================================
[2019-04-04 08:47:11,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[68.397446]
 [68.579   ]
 [68.816475]
 [69.32107 ]
 [70.10471 ]], R is [[68.50997925]
 [68.59957886]
 [68.69141388]
 [68.79250336]
 [68.92214203]].
[2019-04-04 08:47:12,727] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8111196e-10 1.0886363e-09 1.2392647e-25 1.6608336e-24 6.0700161e-20
 1.0000000e+00 1.6415261e-19], sum to 1.0000
[2019-04-04 08:47:12,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9623
[2019-04-04 08:47:12,795] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.55, 62.5, 0.0, 0.0, 26.0, 25.20881440176757, 0.2981158608242291, 1.0, 1.0, 34737.39801336994], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 153000.0000, 
sim time next is 153600.0000, 
raw observation next is [-7.633333333333333, 63.0, 0.0, 0.0, 26.0, 25.03332121260992, 0.2897913463413817, 1.0, 1.0, 140734.2104842861], 
processed observation next is [1.0, 0.782608695652174, 0.2511542012927055, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5861101010508266, 0.5965971154471272, 1.0, 1.0, 0.6701629070680291], 
reward next is 0.3298, 
noisyNet noise sample is [array([-0.02100983], dtype=float32), 0.3786736]. 
=============================================
[2019-04-04 08:47:17,931] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1071456e-08 1.7397169e-07 8.3400001e-22 3.0716780e-20 2.6230045e-16
 9.9999976e-01 2.4247816e-16], sum to 1.0000
[2019-04-04 08:47:17,931] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0760
[2019-04-04 08:47:17,944] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.23333333333333, 49.33333333333334, 0.0, 0.0, 26.0, 22.68106745977548, -0.2889552476222776, 0.0, 1.0, 46731.74790774404], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 451200.0000, 
sim time next is 451800.0000, 
raw observation next is [-10.05, 48.0, 0.0, 0.0, 26.0, 22.6379304407027, -0.2998035893161972, 0.0, 1.0, 46718.2418982141], 
processed observation next is [1.0, 0.21739130434782608, 0.18421052631578946, 0.48, 0.0, 0.0, 0.6666666666666666, 0.38649420339189167, 0.40006547022793426, 0.0, 1.0, 0.2224678185629243], 
reward next is 0.7775, 
noisyNet noise sample is [array([-1.0720977], dtype=float32), 0.79196143]. 
=============================================
[2019-04-04 08:47:18,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:47:18,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:47:18,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run20
[2019-04-04 08:47:19,965] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3260002e-08 1.3730491e-07 2.1336145e-20 1.2668577e-19 5.6532064e-16
 9.9999988e-01 4.4803188e-16], sum to 1.0000
[2019-04-04 08:47:19,965] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5972
[2019-04-04 08:47:19,979] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.0, 76.33333333333334, 0.0, 0.0, 26.0, 22.08030275547717, -0.4147053331453077, 0.0, 1.0, 48610.71614369805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 366000.0000, 
sim time next is 366600.0000, 
raw observation next is [-16.1, 77.16666666666666, 0.0, 0.0, 26.0, 21.98770827583859, -0.4362840859319763, 0.0, 1.0, 48660.85589311972], 
processed observation next is [1.0, 0.21739130434782608, 0.01662049861495839, 0.7716666666666666, 0.0, 0.0, 0.6666666666666666, 0.3323090229865491, 0.3545719713560079, 0.0, 1.0, 0.23171836139580818], 
reward next is 0.7683, 
noisyNet noise sample is [array([0.8054552], dtype=float32), -0.24041486]. 
=============================================
[2019-04-04 08:47:32,929] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9937959e-08 2.3557899e-08 7.1955638e-23 1.9984352e-21 1.3811506e-17
 1.0000000e+00 3.6475333e-17], sum to 1.0000
[2019-04-04 08:47:32,929] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1758
[2019-04-04 08:47:32,960] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.9, 77.83333333333334, 0.0, 0.0, 26.0, 24.61069672049589, 0.1887249625967624, 0.0, 1.0, 47312.12690073678], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 335400.0000, 
sim time next is 336000.0000, 
raw observation next is [-13.0, 78.66666666666667, 0.0, 0.0, 26.0, 24.53132727049775, 0.1715796836777531, 0.0, 1.0, 47495.95764540059], 
processed observation next is [1.0, 0.9130434782608695, 0.10249307479224376, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5442772725414793, 0.5571932278925843, 0.0, 1.0, 0.22617122688285993], 
reward next is 0.7738, 
noisyNet noise sample is [array([0.6650107], dtype=float32), 1.5018135]. 
=============================================
[2019-04-04 08:47:32,993] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[68.97639]
 [69.15424]
 [69.39594]
 [69.90215]
 [70.68647]], R is [[69.08794403]
 [69.17176819]
 [69.25789642]
 [69.35335541]
 [69.47748566]].
[2019-04-04 08:47:43,817] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0064630e-09 1.6121509e-08 1.7321267e-24 2.8774819e-22 2.6729107e-18
 1.0000000e+00 1.1806579e-17], sum to 1.0000
[2019-04-04 08:47:43,817] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8449
[2019-04-04 08:47:43,902] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.633333333333334, 32.66666666666667, 55.50000000000001, 0.0, 26.0, 25.53916881604338, 0.1982131046353866, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 465600.0000, 
sim time next is 466200.0000, 
raw observation next is [-5.35, 32.5, 62.0, 0.0, 26.0, 25.54069307649266, 0.2006359691465908, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.31440443213296404, 0.325, 0.20666666666666667, 0.0, 0.6666666666666666, 0.6283910897077215, 0.5668786563821969, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3665365], dtype=float32), -1.0296143]. 
=============================================
[2019-04-04 08:47:51,328] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2855112e-11 1.3007966e-09 3.4571691e-26 6.0902256e-25 7.0052537e-20
 1.0000000e+00 5.0287851e-20], sum to 1.0000
[2019-04-04 08:47:51,328] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9803
[2019-04-04 08:47:51,416] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-14.5, 66.0, 55.0, 733.5, 26.0, 25.67647739772639, 0.3259876841708557, 1.0, 1.0, 53258.65443488345], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 381600.0000, 
sim time next is 382200.0000, 
raw observation next is [-14.31666666666667, 65.0, 60.33333333333333, 732.0, 26.0, 25.78945741644938, 0.3430329192410912, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.06602031394275151, 0.65, 0.2011111111111111, 0.8088397790055248, 0.6666666666666666, 0.6491214513707817, 0.6143443064136971, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39351138], dtype=float32), -0.82803035]. 
=============================================
[2019-04-04 08:47:55,725] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.6395210e-11 2.7530720e-09 5.7249621e-28 1.1708397e-25 8.4451707e-22
 1.0000000e+00 4.1027467e-20], sum to 1.0000
[2019-04-04 08:47:55,725] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5301
[2019-04-04 08:47:55,770] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8666666666666667, 80.83333333333333, 116.3333333333333, 309.0000000000001, 26.0, 24.91399977995081, 0.3129424517787465, 0.0, 1.0, 9369.41555671446], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 562200.0000, 
sim time next is 562800.0000, 
raw observation next is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 26.0, 24.97026228076015, 0.3169965935311123, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4367497691597415, 0.8066666666666668, 0.4105555555555557, 0.38950276243093923, 0.6666666666666666, 0.5808551900633457, 0.6056655311770375, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.78832066], dtype=float32), -0.9198131]. 
=============================================
[2019-04-04 08:48:00,321] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9454841e-10 2.5932665e-09 1.5450214e-26 1.2259212e-24 1.4767563e-20
 1.0000000e+00 4.9688898e-20], sum to 1.0000
[2019-04-04 08:48:00,321] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6851
[2019-04-04 08:48:00,429] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 54.5, 106.0, 658.0, 26.0, 25.60681617404731, 0.3510711724291722, 1.0, 1.0, 198871.370433564], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 300600.0000, 
sim time next is 301200.0000, 
raw observation next is [-10.6, 52.66666666666667, 102.1666666666667, 674.6666666666666, 26.0, 25.59536425324301, 0.3970723608109969, 1.0, 1.0, 137665.6398862013], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.5266666666666667, 0.34055555555555567, 0.74548802946593, 0.6666666666666666, 0.632947021103584, 0.6323574536036657, 1.0, 1.0, 0.6555506661247681], 
reward next is 0.3444, 
noisyNet noise sample is [array([-1.6365428], dtype=float32), -0.14775887]. 
=============================================
[2019-04-04 08:48:02,181] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-04 08:48:02,182] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:48:02,182] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:48:02,184] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run27
[2019-04-04 08:48:02,219] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:48:02,221] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:48:02,221] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:48:02,226] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run27
[2019-04-04 08:48:02,294] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:48:02,299] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run27
[2019-04-04 08:51:16,026] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4829 239928582.5118 1604.8457
[2019-04-04 08:51:40,569] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 08:51:47,895] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6651 275800324.5281 1233.1507
[2019-04-04 08:51:48,928] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2600000, evaluation results [2600000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.482940420242, 239928582.51175264, 1604.845674400583, 7182.665121294629, 275800324.5281271, 1233.1506611100626]
[2019-04-04 08:51:52,825] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7599767e-10 3.6431815e-09 1.5177419e-25 1.2144946e-23 1.5134507e-19
 1.0000000e+00 1.1185917e-18], sum to 1.0000
[2019-04-04 08:51:52,828] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5734
[2019-04-04 08:51:52,895] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.283333333333334, 64.33333333333334, 92.66666666666667, 25.33333333333334, 26.0, 24.88751849114681, 0.2052409245765505, 0.0, 1.0, 35950.46793970876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 645000.0000, 
sim time next is 645600.0000, 
raw observation next is [-3.166666666666667, 63.66666666666667, 90.83333333333334, 31.66666666666667, 26.0, 24.86615190673492, 0.2046242402807458, 0.0, 1.0, 52295.36203770612], 
processed observation next is [0.0, 0.4782608695652174, 0.3748845798707295, 0.6366666666666667, 0.3027777777777778, 0.03499079189686925, 0.6666666666666666, 0.5721793255612434, 0.5682080800935819, 0.0, 1.0, 0.2490255335128863], 
reward next is 0.7510, 
noisyNet noise sample is [array([-1.2345102], dtype=float32), 0.026980989]. 
=============================================
[2019-04-04 08:52:00,823] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5227188e-12 6.5327616e-10 1.6203709e-29 2.6823058e-27 8.1932750e-22
 1.0000000e+00 1.0676423e-22], sum to 1.0000
[2019-04-04 08:52:00,823] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5871
[2019-04-04 08:52:00,846] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4166666666666667, 55.83333333333333, 115.3333333333333, 559.0, 26.0, 25.84071161178181, 0.3824118822356947, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 735000.0000, 
sim time next is 735600.0000, 
raw observation next is [-0.2333333333333334, 54.66666666666667, 123.1666666666667, 504.0, 26.0, 25.79166309836008, 0.3739687285162085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.456140350877193, 0.5466666666666667, 0.4105555555555557, 0.5569060773480663, 0.6666666666666666, 0.6493052581966733, 0.6246562428387362, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2150431], dtype=float32), -0.23299144]. 
=============================================
[2019-04-04 08:52:05,897] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2833784e-09 1.2216830e-08 2.4512405e-24 9.1193528e-24 8.9321496e-19
 1.0000000e+00 4.9902245e-19], sum to 1.0000
[2019-04-04 08:52:05,942] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1198
[2019-04-04 08:52:05,971] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.633333333333333, 71.0, 0.0, 0.0, 26.0, 24.19540021573315, 0.1016159226079102, 0.0, 1.0, 41543.54125666724], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 783600.0000, 
sim time next is 784200.0000, 
raw observation next is [-7.716666666666667, 71.0, 0.0, 0.0, 26.0, 24.142100579442, 0.08899313567706861, 0.0, 1.0, 41541.13027446296], 
processed observation next is [1.0, 0.043478260869565216, 0.24884579870729456, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5118417149535001, 0.5296643785590228, 0.0, 1.0, 0.19781490606887123], 
reward next is 0.8022, 
noisyNet noise sample is [array([1.159484], dtype=float32), -2.2690752]. 
=============================================
[2019-04-04 08:52:15,862] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.7175935e-10 4.6484265e-09 1.4114398e-26 7.3001208e-25 2.3295648e-20
 1.0000000e+00 1.0444409e-19], sum to 1.0000
[2019-04-04 08:52:15,865] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1983
[2019-04-04 08:52:15,899] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 76.0, 0.0, 0.0, 26.0, 24.67167788872085, 0.1996558731765851, 0.0, 1.0, 39073.38173164096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 878400.0000, 
sim time next is 879000.0000, 
raw observation next is [-1.1, 75.33333333333334, 0.0, 0.0, 26.0, 24.74316450038519, 0.1952123480974977, 0.0, 1.0, 39017.25905726756], 
processed observation next is [1.0, 0.17391304347826086, 0.4321329639889197, 0.7533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5619303750320993, 0.5650707826991659, 0.0, 1.0, 0.1857964717012741], 
reward next is 0.8142, 
noisyNet noise sample is [array([1.4680003], dtype=float32), 0.5476933]. 
=============================================
[2019-04-04 08:52:15,915] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[82.042496]
 [82.04438 ]
 [82.04638 ]
 [82.05304 ]
 [82.06498 ]], R is [[82.01499939]
 [82.00878143]
 [82.00236511]
 [81.99580383]
 [81.98910522]].
[2019-04-04 08:52:18,186] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.58258684e-11 1.70966824e-10 5.77241115e-30 1.47591126e-28
 1.02838015e-22 1.00000000e+00 1.11385872e-22], sum to 1.0000
[2019-04-04 08:52:18,186] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9129
[2019-04-04 08:52:18,196] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 92.83333333333333, 30.0, 0.0, 26.0, 25.78394147640775, 0.4027152533576151, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 922200.0000, 
sim time next is 922800.0000, 
raw observation next is [4.600000000000001, 92.66666666666667, 24.0, 0.0, 26.0, 25.76920356991501, 0.3958484809577676, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5900277008310251, 0.9266666666666667, 0.08, 0.0, 0.6666666666666666, 0.6474336308262508, 0.6319494936525892, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36069998], dtype=float32), -1.4654866]. 
=============================================
[2019-04-04 08:52:18,682] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.4198655e-12 3.4066766e-10 1.3471571e-30 6.1981934e-28 1.5148687e-23
 1.0000000e+00 8.7044452e-22], sum to 1.0000
[2019-04-04 08:52:18,683] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7944
[2019-04-04 08:52:18,694] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.1, 98.0, 101.0, 0.0, 26.0, 25.01305596649955, 0.4777895648005617, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1254600.0000, 
sim time next is 1255200.0000, 
raw observation next is [14.0, 98.66666666666666, 100.0, 0.0, 26.0, 24.98218224636839, 0.4739522459771884, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8504155124653741, 0.9866666666666666, 0.3333333333333333, 0.0, 0.6666666666666666, 0.581848520530699, 0.6579840819923961, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1913302], dtype=float32), -1.1140349]. 
=============================================
[2019-04-04 08:52:29,246] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8820032e-10 7.4995976e-10 1.8774784e-28 2.7587147e-26 4.9948473e-21
 1.0000000e+00 6.8800893e-21], sum to 1.0000
[2019-04-04 08:52:29,246] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1658
[2019-04-04 08:52:29,268] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.44166776668114, 0.582471464724366, 0.0, 1.0, 18762.89318820951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1372200.0000, 
sim time next is 1372800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.47796857866389, 0.5815683143471668, 0.0, 1.0, 18758.56086958057], 
processed observation next is [1.0, 0.9130434782608695, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6231640482219909, 0.6938561047823889, 0.0, 1.0, 0.08932648033133606], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.5351129], dtype=float32), 1.1404718]. 
=============================================
[2019-04-04 08:52:30,585] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.5555688e-11 3.9173981e-10 3.4764853e-29 2.8105483e-28 3.1620198e-22
 1.0000000e+00 3.5288475e-22], sum to 1.0000
[2019-04-04 08:52:30,585] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6378
[2019-04-04 08:52:30,596] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.866666666666667, 86.66666666666667, 0.0, 0.0, 26.0, 25.35884742833654, 0.4346915389499116, 0.0, 1.0, 37969.23273607915], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 955200.0000, 
sim time next is 955800.0000, 
raw observation next is [6.05, 85.5, 0.0, 0.0, 26.0, 25.38992757893496, 0.4330294376323695, 0.0, 1.0, 28283.58813965587], 
processed observation next is [1.0, 0.043478260869565216, 0.6301939058171746, 0.855, 0.0, 0.0, 0.6666666666666666, 0.6158272982445799, 0.6443431458774566, 0.0, 1.0, 0.13468375304598032], 
reward next is 0.8653, 
noisyNet noise sample is [array([0.06667751], dtype=float32), 2.847621]. 
=============================================
[2019-04-04 08:52:45,066] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1864656e-10 5.3384586e-10 2.7858057e-28 4.9872164e-27 1.3929949e-21
 1.0000000e+00 6.3291022e-21], sum to 1.0000
[2019-04-04 08:52:45,068] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7899
[2019-04-04 08:52:45,084] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.433333333333333, 92.0, 0.0, 0.0, 26.0, 25.3106081505866, 0.4834933235260699, 0.0, 1.0, 60988.42207490419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1464000.0000, 
sim time next is 1464600.0000, 
raw observation next is [1.516666666666667, 92.0, 0.0, 0.0, 26.0, 25.31672357982757, 0.4875627932713815, 0.0, 1.0, 46622.74766997824], 
processed observation next is [1.0, 0.9565217391304348, 0.5046168051708219, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6097269649856308, 0.6625209310904605, 0.0, 1.0, 0.22201308414275353], 
reward next is 0.7780, 
noisyNet noise sample is [array([0.29921895], dtype=float32), 0.888722]. 
=============================================
[2019-04-04 08:52:47,881] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2479407e-11 1.0525028e-10 5.3472497e-30 3.6173334e-28 2.3261347e-22
 1.0000000e+00 1.8904789e-21], sum to 1.0000
[2019-04-04 08:52:47,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7017
[2019-04-04 08:52:47,922] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.74218074858102, 0.5726982653467085, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1034400.0000, 
sim time next is 1035000.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.67955679889442, 0.5683817210785916, 0.0, 1.0, 60769.52288731973], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.639963066574535, 0.6894605736928638, 0.0, 1.0, 0.2893786804158082], 
reward next is 0.7106, 
noisyNet noise sample is [array([0.45114195], dtype=float32), 0.64266044]. 
=============================================
[2019-04-04 08:52:47,927] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[92.46056 ]
 [92.572685]
 [92.488625]
 [92.45009 ]
 [92.52755 ]], R is [[92.59279633]
 [92.66687012]
 [92.74020386]
 [92.81280518]
 [92.8846817 ]].
[2019-04-04 08:52:55,383] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4596279e-11 1.7653830e-10 7.1854716e-30 3.1554088e-27 4.9590545e-23
 1.0000000e+00 8.0002426e-22], sum to 1.0000
[2019-04-04 08:52:55,394] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1831
[2019-04-04 08:52:55,409] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.8, 96.66666666666666, 0.0, 0.0, 26.0, 25.71459449412724, 0.5677350694210371, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1651200.0000, 
sim time next is 1651800.0000, 
raw observation next is [6.699999999999999, 96.83333333333334, 0.0, 0.0, 26.0, 25.72333243484259, 0.5419830966262749, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6481994459833795, 0.9683333333333334, 0.0, 0.0, 0.6666666666666666, 0.6436110362368824, 0.6806610322087584, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30795187], dtype=float32), -1.0883934]. 
=============================================
[2019-04-04 08:52:57,779] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.2181071e-11 3.7369094e-10 8.9850696e-30 1.1214956e-28 1.4606140e-22
 1.0000000e+00 5.7389734e-22], sum to 1.0000
[2019-04-04 08:52:57,780] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7581
[2019-04-04 08:52:57,800] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.05, 97.0, 0.0, 0.0, 26.0, 25.59157854302113, 0.5383241446633447, 0.0, 1.0, 34057.44870397709], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1661400.0000, 
sim time next is 1662000.0000, 
raw observation next is [5.866666666666667, 97.0, 0.0, 0.0, 26.0, 25.52834463209079, 0.5318598237784077, 0.0, 1.0, 65498.16380392401], 
processed observation next is [1.0, 0.21739130434782608, 0.6251154201292707, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6273620526742324, 0.6772866079261358, 0.0, 1.0, 0.31189601811392387], 
reward next is 0.6881, 
noisyNet noise sample is [array([-0.50649375], dtype=float32), -0.65274465]. 
=============================================
[2019-04-04 08:52:57,806] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[89.93587]
 [89.95459]
 [90.15787]
 [90.21038]
 [90.09592]], R is [[89.84838867]
 [89.78772736]
 [89.88985443]
 [89.83377075]
 [89.66702271]].
[2019-04-04 08:53:02,718] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6236716e-11 1.8386066e-09 1.0807569e-26 5.7787493e-25 5.8762633e-20
 1.0000000e+00 7.4195419e-20], sum to 1.0000
[2019-04-04 08:53:02,720] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0888
[2019-04-04 08:53:02,764] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 65.0, 182.0, 2.0, 26.0, 25.57564236215125, 0.3137924470787776, 1.0, 1.0, 21269.5004954347], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1947600.0000, 
sim time next is 1948200.0000, 
raw observation next is [-3.816666666666666, 64.5, 167.0, 1.333333333333333, 26.0, 25.50874423877059, 0.3024772172061871, 1.0, 1.0, 22270.88834766649], 
processed observation next is [1.0, 0.5652173913043478, 0.3568790397045245, 0.645, 0.5566666666666666, 0.00147329650092081, 0.6666666666666666, 0.6257286865642158, 0.600825739068729, 1.0, 1.0, 0.10605184927460233], 
reward next is 0.8939, 
noisyNet noise sample is [array([0.17758586], dtype=float32), 1.421012]. 
=============================================
[2019-04-04 08:53:07,762] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.8126328e-10 5.9720002e-09 8.7397673e-25 3.2163740e-23 1.0663055e-18
 1.0000000e+00 2.8298198e-18], sum to 1.0000
[2019-04-04 08:53:07,763] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9774
[2019-04-04 08:53:07,812] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 83.0, 0.0, 0.0, 26.0, 24.182929794707, 0.1113310575205853, 0.0, 1.0, 46376.36725734088], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1821600.0000, 
sim time next is 1822200.0000, 
raw observation next is [-6.033333333333333, 83.66666666666667, 0.0, 0.0, 26.0, 24.14645339153761, 0.1072433408938425, 0.0, 1.0, 46442.90404994266], 
processed observation next is [0.0, 0.08695652173913043, 0.29547553093259465, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5122044492948007, 0.5357477802979475, 0.0, 1.0, 0.2211566859521079], 
reward next is 0.7788, 
noisyNet noise sample is [array([0.3982061], dtype=float32), -0.12927711]. 
=============================================
[2019-04-04 08:53:14,154] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1362056e-09 7.2173325e-09 1.2082141e-24 2.5755614e-23 6.8466360e-19
 1.0000000e+00 2.9254340e-18], sum to 1.0000
[2019-04-04 08:53:14,157] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0139
[2019-04-04 08:53:14,171] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.33841238473986, 0.109205251045296, 0.0, 1.0, 41260.30711373613], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2000400.0000, 
sim time next is 2001000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.30740450772384, 0.1005486121797591, 0.0, 1.0, 41257.72252390526], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5256170423103201, 0.5335162040599197, 0.0, 1.0, 0.1964653453519298], 
reward next is 0.8035, 
noisyNet noise sample is [array([1.4696012], dtype=float32), -2.1417232]. 
=============================================
[2019-04-04 08:53:14,173] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[75.0909  ]
 [75.13305 ]
 [75.1763  ]
 [75.209885]
 [75.2251  ]], R is [[75.10675049]
 [75.15920258]
 [75.21102142]
 [75.26198578]
 [75.3121109 ]].
[2019-04-04 08:53:21,112] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.85314261e-11 3.76235754e-09 1.06424485e-26 1.77837068e-25
 1.21439921e-20 1.00000000e+00 5.28671324e-20], sum to 1.0000
[2019-04-04 08:53:21,113] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3979
[2019-04-04 08:53:21,179] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.566666666666666, 80.16666666666667, 113.6666666666667, 444.6666666666667, 26.0, 25.64999915667559, 0.3051917388737436, 1.0, 1.0, 21621.49488075589], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1936200.0000, 
sim time next is 1936800.0000, 
raw observation next is [-7.3, 79.0, 128.0, 392.5, 26.0, 25.64573485309915, 0.3195491971231706, 1.0, 1.0, 22671.47600813078], 
processed observation next is [1.0, 0.43478260869565216, 0.26038781163434904, 0.79, 0.4266666666666667, 0.43370165745856354, 0.6666666666666666, 0.6371445710915958, 0.6065163990410568, 1.0, 1.0, 0.10795940956252752], 
reward next is 0.8920, 
noisyNet noise sample is [array([0.2657571], dtype=float32), -0.2107585]. 
=============================================
[2019-04-04 08:53:23,672] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.9396468e-09 1.9488098e-08 1.4802494e-25 8.2646534e-23 1.3331374e-18
 1.0000000e+00 1.6695667e-18], sum to 1.0000
[2019-04-04 08:53:23,680] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6215
[2019-04-04 08:53:23,708] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.25813438357073, 0.1318255587536841, 0.0, 1.0, 41703.85576488706], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1995600.0000, 
sim time next is 1996200.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.33137021355545, 0.1340124439971207, 0.0, 1.0, 41640.45409929976], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.527614184462954, 0.5446708146657069, 0.0, 1.0, 0.19828787666333217], 
reward next is 0.8017, 
noisyNet noise sample is [array([1.7123], dtype=float32), -0.41915783]. 
=============================================
[2019-04-04 08:53:24,922] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.4087915e-10 4.0933119e-09 2.5144844e-26 4.5116331e-25 8.5792840e-20
 1.0000000e+00 9.0692761e-20], sum to 1.0000
[2019-04-04 08:53:24,923] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3121
[2019-04-04 08:53:24,985] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.633333333333333, 79.66666666666667, 202.3333333333333, 69.66666666666666, 26.0, 25.84980128394227, 0.3975033557110924, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2110800.0000, 
sim time next is 2111400.0000, 
raw observation next is [-7.55, 78.5, 208.0, 60.0, 26.0, 25.84745075076481, 0.3909340226945622, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.25346260387811637, 0.785, 0.6933333333333334, 0.06629834254143646, 0.6666666666666666, 0.6539542292304009, 0.6303113408981874, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3561087], dtype=float32), -0.21829815]. 
=============================================
[2019-04-04 08:53:32,963] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.35376044e-09 1.33700455e-08 4.07811585e-24 8.04390673e-23
 1.89188841e-18 1.00000000e+00 1.03366872e-17], sum to 1.0000
[2019-04-04 08:53:32,964] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6652
[2019-04-04 08:53:32,980] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.466666666666667, 76.0, 0.0, 0.0, 26.0, 24.08021276093042, 0.02575503073592455, 0.0, 1.0, 45202.72940003697], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1905600.0000, 
sim time next is 1906200.0000, 
raw observation next is [-7.55, 76.5, 0.0, 0.0, 26.0, 24.05540584620114, 0.02239092134065847, 0.0, 1.0, 45193.15189838954], 
processed observation next is [1.0, 0.043478260869565216, 0.25346260387811637, 0.765, 0.0, 0.0, 0.6666666666666666, 0.504617153850095, 0.5074636404468862, 0.0, 1.0, 0.21520548523042637], 
reward next is 0.7848, 
noisyNet noise sample is [array([-0.46960923], dtype=float32), -2.1772203]. 
=============================================
[2019-04-04 08:53:56,404] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.3243863e-09 7.6409270e-09 2.4634361e-24 2.1605330e-23 3.4422075e-18
 1.0000000e+00 7.6343948e-18], sum to 1.0000
[2019-04-04 08:53:56,405] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3838
[2019-04-04 08:53:56,425] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8666666666666667, 31.66666666666667, 0.0, 0.0, 26.0, 25.30695946473351, 0.2863798082047632, 0.0, 1.0, 40354.34948402391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2492400.0000, 
sim time next is 2493000.0000, 
raw observation next is [-0.95, 33.0, 0.0, 0.0, 26.0, 25.31341518937742, 0.2849892866605148, 0.0, 1.0, 40192.38084833667], 
processed observation next is [0.0, 0.8695652173913043, 0.43628808864265933, 0.33, 0.0, 0.0, 0.6666666666666666, 0.6094512657814516, 0.5949964288868382, 0.0, 1.0, 0.19139228975398412], 
reward next is 0.8086, 
noisyNet noise sample is [array([-0.26290718], dtype=float32), 0.07302827]. 
=============================================
[2019-04-04 08:53:56,449] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[74.94384]
 [74.99071]
 [75.01531]
 [74.95138]
 [74.8701 ]], R is [[74.90942383]
 [74.96817017]
 [75.02446747]
 [75.07299042]
 [75.09687042]].
[2019-04-04 08:54:01,758] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.0556595e-10 1.0599194e-08 4.7191232e-25 3.6925033e-23 2.7068127e-18
 1.0000000e+00 9.7319280e-18], sum to 1.0000
[2019-04-04 08:54:01,760] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1307
[2019-04-04 08:54:01,778] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.1096981319699, 0.2730478633123376, 0.0, 1.0, 43155.10214263996], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2407800.0000, 
sim time next is 2408400.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.10599215965886, 0.2782895728653361, 0.0, 1.0, 43114.7109835685], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.592166013304905, 0.592763190955112, 0.0, 1.0, 0.20530814754080237], 
reward next is 0.7947, 
noisyNet noise sample is [array([-0.82645535], dtype=float32), -1.734599]. 
=============================================
[2019-04-04 08:54:04,957] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5876437e-09 1.6323311e-08 1.3045427e-25 7.7497167e-24 1.1817432e-18
 1.0000000e+00 1.5485148e-18], sum to 1.0000
[2019-04-04 08:54:04,959] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0501
[2019-04-04 08:54:04,990] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.916666666666667, 67.0, 0.0, 0.0, 26.0, 24.72725056464309, 0.2405462773527001, 0.0, 1.0, 41838.80532650564], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2595000.0000, 
sim time next is 2595600.0000, 
raw observation next is [-5.0, 68.0, 0.0, 0.0, 26.0, 24.76136850277294, 0.240152773989124, 0.0, 1.0, 41810.51589318649], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5634473752310782, 0.5800509246630413, 0.0, 1.0, 0.19909769472945948], 
reward next is 0.8009, 
noisyNet noise sample is [array([0.25409177], dtype=float32), -1.7295144]. 
=============================================
[2019-04-04 08:54:15,645] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.7115382e-09 2.9814416e-08 2.9397123e-23 3.4258258e-22 1.0520595e-17
 1.0000000e+00 2.3177172e-17], sum to 1.0000
[2019-04-04 08:54:15,646] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7292
[2019-04-04 08:54:15,659] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.1, 47.16666666666666, 0.0, 0.0, 26.0, 24.45960990262665, 0.1121176777707523, 0.0, 1.0, 43253.57926021003], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2422200.0000, 
sim time next is 2422800.0000, 
raw observation next is [-6.2, 48.0, 0.0, 0.0, 26.0, 24.40066731169138, 0.1007959238211367, 0.0, 1.0, 43320.6013343999], 
processed observation next is [0.0, 0.043478260869565216, 0.2908587257617729, 0.48, 0.0, 0.0, 0.6666666666666666, 0.5333889426409483, 0.5335986412737123, 0.0, 1.0, 0.20628857778285667], 
reward next is 0.7937, 
noisyNet noise sample is [array([0.89368874], dtype=float32), -0.8212391]. 
=============================================
[2019-04-04 08:54:44,711] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1034128e-11 2.0858557e-09 7.0562635e-28 6.9344029e-26 3.5863490e-21
 1.0000000e+00 3.6169736e-21], sum to 1.0000
[2019-04-04 08:54:44,712] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7824
[2019-04-04 08:54:44,752] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4166666666666667, 46.33333333333334, 191.0, 189.6666666666667, 26.0, 25.46031412401203, 0.4455401773070036, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2639400.0000, 
sim time next is 2640000.0000, 
raw observation next is [-0.2333333333333334, 45.66666666666667, 177.5, 200.3333333333333, 26.0, 25.84649216946636, 0.4813745730554521, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.456140350877193, 0.4566666666666667, 0.5916666666666667, 0.2213627992633517, 0.6666666666666666, 0.6538743474555299, 0.660458191018484, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49809074], dtype=float32), 1.4298761]. 
=============================================
[2019-04-04 08:54:44,756] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.90129 ]
 [85.42821 ]
 [85.56467 ]
 [85.069885]
 [84.79672 ]], R is [[84.5881958 ]
 [84.7423172 ]
 [84.58258057]
 [83.79502869]
 [83.32523346]].
[2019-04-04 08:54:45,821] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.03574805e-11 3.04559933e-10 8.84176334e-28 2.42992874e-25
 8.45916819e-21 1.00000000e+00 1.28286181e-20], sum to 1.0000
[2019-04-04 08:54:45,823] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0438
[2019-04-04 08:54:45,840] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.666666666666666, 28.66666666666667, 16.0, 51.0, 26.0, 25.82055880284765, 0.3767410717329167, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2827200.0000, 
sim time next is 2827800.0000, 
raw observation next is [5.5, 29.0, 5.0, 46.0, 26.0, 25.73447966248034, 0.3608106677679869, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6149584487534627, 0.29, 0.016666666666666666, 0.05082872928176796, 0.6666666666666666, 0.6445399718733617, 0.620270222589329, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17305131], dtype=float32), -0.34525573]. 
=============================================
[2019-04-04 08:54:47,972] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1162518e-10 1.4467603e-09 1.0974330e-26 3.4511491e-25 2.7964857e-20
 1.0000000e+00 3.5004430e-19], sum to 1.0000
[2019-04-04 08:54:47,972] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0445
[2019-04-04 08:54:47,990] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 79.33333333333334, 0.0, 0.0, 26.0, 25.08990263122907, 0.4038318425217877, 0.0, 1.0, 43564.2098731316], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3284400.0000, 
sim time next is 3285000.0000, 
raw observation next is [-7.0, 77.0, 0.0, 0.0, 26.0, 25.04956081066516, 0.3999226907855473, 0.0, 1.0, 43631.52581988611], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5874634008887633, 0.6333075635951825, 0.0, 1.0, 0.20776917057088626], 
reward next is 0.7922, 
noisyNet noise sample is [array([-0.33236113], dtype=float32), -0.6884539]. 
=============================================
[2019-04-04 08:54:47,999] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[80.34686 ]
 [80.47128 ]
 [80.521034]
 [80.71289 ]
 [80.90728 ]], R is [[80.07828522]
 [80.0700531 ]
 [80.06225586]
 [80.05490112]
 [80.04798126]].
[2019-04-04 08:54:54,943] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.0225704e-10 2.9904516e-08 7.4196081e-25 6.1020888e-24 2.6231510e-19
 1.0000000e+00 2.7712989e-18], sum to 1.0000
[2019-04-04 08:54:54,944] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4483
[2019-04-04 08:54:54,975] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 61.83333333333333, 0.0, 0.0, 26.0, 24.82752876270231, 0.2324705621182172, 0.0, 1.0, 42628.1155029188], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3390600.0000, 
sim time next is 3391200.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.7654958854628, 0.2239321301829474, 0.0, 1.0, 42771.76924353687], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5637913237885668, 0.5746440433943157, 0.0, 1.0, 0.20367509163588987], 
reward next is 0.7963, 
noisyNet noise sample is [array([1.4860687], dtype=float32), -1.6935458]. 
=============================================
[2019-04-04 08:55:02,231] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2192322e-11 2.4974234e-09 5.8720487e-28 1.7304682e-25 5.9023206e-21
 1.0000000e+00 1.5497778e-20], sum to 1.0000
[2019-04-04 08:55:02,232] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9773
[2019-04-04 08:55:02,280] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.833333333333333, 60.0, 58.66666666666667, 317.0, 26.0, 25.55692882846579, 0.3807228681440273, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3399000.0000, 
sim time next is 3399600.0000, 
raw observation next is [-1.666666666666667, 60.00000000000001, 72.83333333333333, 369.5, 26.0, 25.49433370361218, 0.4223484903349826, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4164358264081256, 0.6000000000000001, 0.24277777777777776, 0.40828729281767956, 0.6666666666666666, 0.6245278086343484, 0.6407828301116608, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.90437496], dtype=float32), 0.4452353]. 
=============================================
[2019-04-04 08:55:14,308] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.7716642e-10 6.5204162e-09 2.5560896e-26 1.3368356e-24 4.0783595e-19
 1.0000000e+00 4.6796451e-19], sum to 1.0000
[2019-04-04 08:55:14,308] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3776
[2019-04-04 08:55:14,349] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 54.16666666666667, 116.6666666666667, 820.6666666666667, 26.0, 25.17564318361119, 0.4457551144265033, 0.0, 1.0, 18712.06844492387], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3586200.0000, 
sim time next is 3586800.0000, 
raw observation next is [-2.666666666666667, 53.33333333333334, 117.3333333333333, 821.8333333333334, 26.0, 25.17564651955401, 0.4478751700982562, 0.0, 1.0, 18711.49163426232], 
processed observation next is [0.0, 0.5217391304347826, 0.38873499538319484, 0.5333333333333334, 0.391111111111111, 0.9081031307550645, 0.6666666666666666, 0.5979705432961676, 0.6492917233660854, 0.0, 1.0, 0.08910234111553485], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.5373792], dtype=float32), -0.028586404]. 
=============================================
[2019-04-04 08:55:16,334] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.6552048e-10 4.7670365e-09 3.1837652e-25 1.5106502e-23 1.5624275e-20
 1.0000000e+00 1.9229501e-18], sum to 1.0000
[2019-04-04 08:55:16,335] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8488
[2019-04-04 08:55:16,361] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.19148635406023, 0.2939195913744179, 0.0, 1.0, 41991.12035032944], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3733800.0000, 
sim time next is 3734400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.14941033691812, 0.2870688184077063, 0.0, 1.0, 41622.93661162525], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5957841947431767, 0.5956896061359022, 0.0, 1.0, 0.19820446005535833], 
reward next is 0.8018, 
noisyNet noise sample is [array([0.11514039], dtype=float32), 0.3398934]. 
=============================================
[2019-04-04 08:55:16,394] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.6503864e-10 5.8929075e-09 9.9153229e-25 4.4362555e-23 8.7640985e-19
 1.0000000e+00 6.3492796e-18], sum to 1.0000
[2019-04-04 08:55:16,396] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0131
[2019-04-04 08:55:16,410] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 48.33333333333334, 0.0, 0.0, 26.0, 25.31639464522011, 0.3707691383639962, 0.0, 1.0, 38478.60943869619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3626400.0000, 
sim time next is 3627000.0000, 
raw observation next is [3.0, 42.5, 0.0, 0.0, 26.0, 25.35875459764781, 0.3803946983859938, 0.0, 1.0, 38349.54984108561], 
processed observation next is [0.0, 1.0, 0.5457063711911359, 0.425, 0.0, 0.0, 0.6666666666666666, 0.6132295498039841, 0.6267982327953313, 0.0, 1.0, 0.18261690400516956], 
reward next is 0.8174, 
noisyNet noise sample is [array([-1.8926183], dtype=float32), 1.1596401]. 
=============================================
[2019-04-04 08:55:16,429] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.28333 ]
 [76.16617 ]
 [76.07948 ]
 [76.023315]
 [75.98828 ]], R is [[76.47350311]
 [76.52553558]
 [76.57659149]
 [76.62526703]
 [76.66718292]].
[2019-04-04 08:55:21,114] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.5813926e-11 8.2782448e-10 8.6263789e-27 2.9838052e-25 6.8077457e-21
 1.0000000e+00 9.1642881e-20], sum to 1.0000
[2019-04-04 08:55:21,114] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4569
[2019-04-04 08:55:21,160] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.166666666666667, 46.83333333333333, 0.0, 0.0, 26.0, 26.25144063262297, 0.6231420985060876, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3865800.0000, 
sim time next is 3866400.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 26.17209945470763, 0.5040836421715508, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6810082878923026, 0.6680278807238502, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5664239], dtype=float32), -1.380217]. 
=============================================
[2019-04-04 08:55:21,539] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0241081e-09 2.4799281e-08 1.9562088e-24 2.0345010e-23 8.9040492e-19
 1.0000000e+00 1.5614997e-18], sum to 1.0000
[2019-04-04 08:55:21,544] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0271
[2019-04-04 08:55:21,564] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.5, 0.0, 0.0, 26.0, 24.65541026527691, 0.2429060271614952, 0.0, 1.0, 42892.47256205879], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3389400.0000, 
sim time next is 3390000.0000, 
raw observation next is [-3.666666666666667, 63.66666666666667, 0.0, 0.0, 26.0, 24.79544662768685, 0.2434180586302695, 0.0, 1.0, 42747.84840151163], 
processed observation next is [1.0, 0.21739130434782608, 0.3610341643582641, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5662872189739042, 0.5811393528767564, 0.0, 1.0, 0.20356118286434108], 
reward next is 0.7964, 
noisyNet noise sample is [array([-0.04421559], dtype=float32), 0.35199624]. 
=============================================
[2019-04-04 08:55:21,581] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[76.11773]
 [76.16608]
 [76.21254]
 [76.27249]
 [76.31347]], R is [[76.11553955]
 [76.15013885]
 [76.18547821]
 [76.22014618]
 [76.25658417]].
[2019-04-04 08:55:32,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.2395550e-11 5.6148206e-09 1.7971228e-27 5.6692919e-25 3.1616614e-20
 1.0000000e+00 5.1744836e-19], sum to 1.0000
[2019-04-04 08:55:32,804] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0764
[2019-04-04 08:55:32,834] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 59.5, 111.0, 758.0, 26.0, 25.57128753596053, 0.4816330142212932, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3580200.0000, 
sim time next is 3580800.0000, 
raw observation next is [-4.333333333333334, 57.66666666666667, 111.5, 767.6666666666667, 26.0, 25.50021290711952, 0.471987234637492, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3425669436749769, 0.5766666666666667, 0.37166666666666665, 0.8482504604051566, 0.6666666666666666, 0.6250177422599599, 0.6573290782124973, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5889797], dtype=float32), 2.2330928]. 
=============================================
[2019-04-04 08:55:35,783] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.0169371e-11 2.9634326e-10 2.9065201e-28 1.8090781e-26 1.0240510e-21
 1.0000000e+00 3.4208781e-21], sum to 1.0000
[2019-04-04 08:55:35,787] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7234
[2019-04-04 08:55:35,810] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.166666666666667, 31.66666666666667, 109.3333333333333, 806.0, 26.0, 26.63457215665083, 0.6937527007454812, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4111800.0000, 
sim time next is 4112400.0000, 
raw observation next is [3.333333333333333, 32.33333333333334, 107.6666666666667, 800.0, 26.0, 26.86202403625707, 0.7233813160644081, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5549399815327793, 0.3233333333333334, 0.358888888888889, 0.8839779005524862, 0.6666666666666666, 0.7385020030214223, 0.7411271053548028, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44416663], dtype=float32), 0.8056874]. 
=============================================
[2019-04-04 08:55:37,702] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5535947e-11 6.7120642e-10 2.4308082e-26 2.1160284e-25 5.0777474e-20
 1.0000000e+00 7.4664185e-20], sum to 1.0000
[2019-04-04 08:55:37,702] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1581
[2019-04-04 08:55:37,734] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.666666666666667, 23.33333333333334, 59.16666666666667, 488.8333333333334, 26.0, 26.31194668630805, 0.6763719325845875, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4034400.0000, 
sim time next is 4035000.0000, 
raw observation next is [-1.833333333333333, 23.66666666666666, 51.33333333333334, 429.6666666666667, 26.0, 26.62693941118911, 0.7014454184629756, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.41181902123730385, 0.2366666666666666, 0.17111111111111113, 0.47476979742173114, 0.6666666666666666, 0.7189116175990925, 0.7338151394876585, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9777531], dtype=float32), 1.0623726]. 
=============================================
[2019-04-04 08:55:37,765] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.69775 ]
 [78.94568 ]
 [79.186676]
 [79.3466  ]
 [79.5006  ]], R is [[78.56856537]
 [78.78288269]
 [78.99505615]
 [79.20510864]
 [79.41305542]].
[2019-04-04 08:55:38,030] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8714193e-10 5.8519021e-09 4.3650059e-26 2.4751824e-24 1.4056364e-19
 1.0000000e+00 6.0512925e-19], sum to 1.0000
[2019-04-04 08:55:38,032] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0556
[2019-04-04 08:55:38,054] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 73.0, 0.0, 0.0, 26.0, 25.37649298908075, 0.3798893752086203, 0.0, 1.0, 38131.00567564154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3903600.0000, 
sim time next is 3904200.0000, 
raw observation next is [-3.5, 74.0, 0.0, 0.0, 26.0, 25.32948240904327, 0.3773527763706078, 0.0, 1.0, 41850.22765625849], 
processed observation next is [1.0, 0.17391304347826086, 0.36565096952908593, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6107902007536058, 0.6257842587902026, 0.0, 1.0, 0.19928679836313565], 
reward next is 0.8007, 
noisyNet noise sample is [array([-0.6036816], dtype=float32), 1.1447812]. 
=============================================
[2019-04-04 08:55:46,803] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7612761e-09 7.1397817e-09 1.4725562e-25 9.3627881e-24 1.5300589e-19
 1.0000000e+00 2.2109223e-18], sum to 1.0000
[2019-04-04 08:55:46,803] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5650
[2019-04-04 08:55:46,818] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.61644578963597, 0.3964357899903391, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3630000.0000, 
sim time next is 3630600.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.62622374437056, 0.3908769386597886, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.7119113573407203, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6355186453642133, 0.6302923128865962, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8824449], dtype=float32), -0.59240484]. 
=============================================
[2019-04-04 08:55:50,326] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0944099e-12 1.5888696e-10 4.3483303e-29 2.9132152e-27 1.2538747e-22
 1.0000000e+00 5.9629463e-22], sum to 1.0000
[2019-04-04 08:55:50,327] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9973
[2019-04-04 08:55:50,343] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 32.0, 117.5, 792.5, 26.0, 26.68425530128665, 0.6305498161113834, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4100400.0000, 
sim time next is 4101000.0000, 
raw observation next is [-0.6666666666666667, 31.33333333333334, 118.6666666666667, 800.3333333333334, 26.0, 26.7121346590459, 0.6381722488613221, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44413665743305636, 0.3133333333333334, 0.39555555555555566, 0.8843462246777164, 0.6666666666666666, 0.7260112215871585, 0.7127240829537741, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.7146833], dtype=float32), -0.5455699]. 
=============================================
[2019-04-04 08:55:50,355] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[85.26328 ]
 [85.264046]
 [85.2766  ]
 [85.40041 ]
 [85.393265]], R is [[85.45632935]
 [85.60176849]
 [85.74575043]
 [85.88829041]
 [86.02941132]].
[2019-04-04 08:55:55,082] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7118121e-10 1.3724333e-08 2.0650719e-25 3.3629203e-24 3.2115008e-19
 1.0000000e+00 1.0498981e-18], sum to 1.0000
[2019-04-04 08:55:55,093] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8804
[2019-04-04 08:55:55,175] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.19654905970373, 0.3611528742591841, 0.0, 1.0, 39489.9949385464], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4159200.0000, 
sim time next is 4159800.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.22491560366252, 0.3595544997718135, 0.0, 1.0, 39485.69589934436], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.60207630030521, 0.6198514999239378, 0.0, 1.0, 0.18802712333021124], 
reward next is 0.8120, 
noisyNet noise sample is [array([-1.9182544], dtype=float32), -2.563251]. 
=============================================
[2019-04-04 08:56:00,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8022452e-11 1.9257584e-10 7.4694281e-30 2.6393394e-28 1.2907292e-22
 1.0000000e+00 4.0692621e-22], sum to 1.0000
[2019-04-04 08:56:00,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3333
[2019-04-04 08:56:00,830] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.33333333333333, 58.83333333333334, 0.0, 0.0, 26.0, 26.92705155307286, 0.8566239763717615, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4395000.0000, 
sim time next is 4395600.0000, 
raw observation next is [10.2, 59.0, 0.0, 0.0, 26.0, 26.8629855582455, 0.8471213551343898, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7451523545706372, 0.59, 0.0, 0.0, 0.6666666666666666, 0.7385821298537918, 0.7823737850447966, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54946274], dtype=float32), -1.4158039]. 
=============================================
[2019-04-04 08:56:05,515] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.39715892e-10 1.34385125e-08 2.90175951e-26 4.51641998e-25
 7.47362789e-20 1.00000000e+00 1.79152431e-19], sum to 1.0000
[2019-04-04 08:56:05,519] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4726
[2019-04-04 08:56:05,619] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.4132140626375, 0.3219714102994023, 0.0, 1.0, 54281.35811103373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4249200.0000, 
sim time next is 4249800.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.36043118715667, 0.3203615385194852, 0.0, 1.0, 69382.40589490208], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6133692655963893, 0.6067871795064951, 0.0, 1.0, 0.33039240902334327], 
reward next is 0.6696, 
noisyNet noise sample is [array([-0.43419304], dtype=float32), -0.6605092]. 
=============================================
[2019-04-04 08:56:09,594] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.1671041e-12 7.3517792e-10 5.4479147e-29 1.1106446e-26 4.4432695e-22
 1.0000000e+00 6.3751173e-21], sum to 1.0000
[2019-04-04 08:56:09,594] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6423
[2019-04-04 08:56:09,642] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.8, 60.0, 172.5, 520.0, 26.0, 25.48511401226866, 0.4564173359832055, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4287600.0000, 
sim time next is 4288200.0000, 
raw observation next is [6.833333333333334, 59.33333333333333, 153.6666666666667, 565.0, 26.0, 25.47852099197236, 0.4580357793389154, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.651892890120037, 0.5933333333333333, 0.5122222222222224, 0.6243093922651933, 0.6666666666666666, 0.6232100826643633, 0.6526785931129718, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4005003], dtype=float32), -0.3624815]. 
=============================================
[2019-04-04 08:56:15,163] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-04 08:56:15,167] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:56:15,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:56:15,183] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run28
[2019-04-04 08:56:15,208] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:56:15,208] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:56:15,215] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run28
[2019-04-04 08:56:15,249] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:56:15,249] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:56:15,252] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run28
[2019-04-04 08:57:16,375] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2591785], dtype=float32), 0.27816784]
[2019-04-04 08:57:16,375] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.3, 83.33333333333334, 0.0, 0.0, 26.0, 25.41919543238379, 0.5523472422448621, 0.0, 1.0, 49838.39397741516]
[2019-04-04 08:57:16,375] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:57:16,376] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.6710407e-11 8.1960605e-10 4.6097777e-29 4.3588471e-27 4.8886596e-22
 1.0000000e+00 3.4616248e-21], sampled 0.5702742054031906
[2019-04-04 08:57:38,427] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2591785], dtype=float32), 0.27816784]
[2019-04-04 08:57:38,427] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [5.0, 70.0, 32.66666666666666, 250.0, 26.0, 26.0368216050719, 0.6021846308397315, 1.0, 1.0, 0.0]
[2019-04-04 08:57:38,428] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:57:38,428] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.6894905e-12 1.4908892e-10 1.0364342e-29 1.0079600e-27 1.5591633e-22
 1.0000000e+00 6.9703265e-22], sampled 0.5163414313946749
[2019-04-04 08:58:04,130] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2591785], dtype=float32), 0.27816784]
[2019-04-04 08:58:04,130] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.133333333333333, 79.66666666666667, 14.5, 53.66666666666666, 26.0, 24.30794879486817, 0.1187090015803095, 0.0, 1.0, 40378.90653502474]
[2019-04-04 08:58:04,130] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:58:04,131] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.8248782e-10 8.1216172e-09 6.3817923e-26 4.0250113e-24 1.2601246e-19
 1.0000000e+00 7.4357814e-19], sampled 0.630713326115582
[2019-04-04 08:58:51,527] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2591785], dtype=float32), 0.27816784]
[2019-04-04 08:58:51,527] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.3, 67.0, 0.0, 0.0, 26.0, 24.77796739470372, 0.3416210757702523, 0.0, 1.0, 44476.71724581083]
[2019-04-04 08:58:51,528] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:58:51,528] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.7164940e-10 1.5047485e-09 3.4576817e-27 2.9226461e-25 1.5535720e-20
 1.0000000e+00 1.0671310e-19], sampled 0.21208732895394034
[2019-04-04 08:58:51,570] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2591785], dtype=float32), 0.27816784]
[2019-04-04 08:58:51,570] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.366666666666667, 68.0, 0.0, 0.0, 26.0, 24.75272975146245, 0.3398063106702645, 0.0, 1.0, 49244.31211685959]
[2019-04-04 08:58:51,570] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:58:51,570] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.6859383e-10 1.4644007e-09 3.1492639e-27 2.5183830e-25 1.3740908e-20
 1.0000000e+00 9.7582374e-20], sampled 0.02438398872432357
[2019-04-04 08:59:21,239] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 08:59:52,369] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6417 263415234.9355 1551.9605
[2019-04-04 08:59:59,954] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 09:00:00,993] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 2700000, evaluation results [2700000.0, 7241.641738402564, 263415234.93545747, 1551.9605289518147, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 09:00:17,890] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1612068e-10 6.9172108e-09 1.6364496e-26 3.1558814e-25 1.5709081e-20
 1.0000000e+00 2.5187846e-19], sum to 1.0000
[2019-04-04 09:00:17,890] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6237
[2019-04-04 09:00:17,930] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.23699333870501, 0.3939565407622307, 0.0, 1.0, 40532.21489129043], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4517400.0000, 
sim time next is 4518000.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.24842504328701, 0.3840320302067661, 0.0, 1.0, 40467.27889342064], 
processed observation next is [1.0, 0.30434782608695654, 0.4349030470914128, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6040354202739175, 0.6280106767355887, 0.0, 1.0, 0.1927013280639078], 
reward next is 0.8073, 
noisyNet noise sample is [array([0.37804404], dtype=float32), -0.25905478]. 
=============================================
[2019-04-04 09:00:18,016] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.71121]
 [81.76306]
 [81.81358]
 [81.87637]
 [81.92555]], R is [[81.65176392]
 [81.64224243]
 [81.63237   ]
 [81.6222229 ]
 [81.61187744]].
[2019-04-04 09:00:24,655] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2127527e-10 1.7713302e-09 1.0973415e-25 6.8191681e-25 5.8134551e-20
 1.0000000e+00 2.2790952e-18], sum to 1.0000
[2019-04-04 09:00:24,655] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6959
[2019-04-04 09:00:24,680] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 49.0, 0.0, 0.0, 26.0, 25.53019112753844, 0.4306057922326614, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4825800.0000, 
sim time next is 4826400.0000, 
raw observation next is [0.3333333333333334, 49.66666666666667, 0.0, 0.0, 26.0, 25.52879045003419, 0.4202535413601708, 0.0, 1.0, 18749.89870522999], 
processed observation next is [0.0, 0.8695652173913043, 0.4718374884579871, 0.4966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6273992041695159, 0.6400845137867236, 0.0, 1.0, 0.08928523192966661], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.7732], dtype=float32), -0.2679032]. 
=============================================
[2019-04-04 09:00:33,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0084419e-11 2.8776763e-09 8.5984860e-27 4.9025252e-26 1.6043805e-20
 1.0000000e+00 1.5879959e-19], sum to 1.0000
[2019-04-04 09:00:33,137] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4524
[2019-04-04 09:00:33,183] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 46.0, 137.5, 784.5, 26.0, 25.25271581866092, 0.4327443150388743, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4791600.0000, 
sim time next is 4792200.0000, 
raw observation next is [-1.5, 45.5, 132.3333333333333, 802.6666666666666, 26.0, 25.18824505436695, 0.4225779509047176, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4210526315789474, 0.455, 0.44111111111111095, 0.8869244935543278, 0.6666666666666666, 0.5990204211972457, 0.6408593169682392, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.87465245], dtype=float32), 0.11996478]. 
=============================================
[2019-04-04 09:00:37,151] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4614944e-13 9.6543121e-11 7.4191644e-30 7.9850341e-28 2.7689352e-23
 1.0000000e+00 1.3656760e-22], sum to 1.0000
[2019-04-04 09:00:37,151] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7381
[2019-04-04 09:00:37,160] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.833333333333333, 57.66666666666666, 186.6666666666667, 12.0, 26.0, 26.22685737483192, 0.5498708142300284, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4531800.0000, 
sim time next is 4532400.0000, 
raw observation next is [2.0, 57.0, 169.5, 9.0, 26.0, 26.24044061360816, 0.5487195238880601, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.518005540166205, 0.57, 0.565, 0.009944751381215469, 0.6666666666666666, 0.6867033844673468, 0.6829065079626867, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.90460765], dtype=float32), 1.5064833]. 
=============================================
[2019-04-04 09:00:38,554] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.16830495e-11 1.60319635e-09 2.13168595e-28 1.71198424e-26
 3.10777673e-22 1.00000000e+00 1.51439635e-21], sum to 1.0000
[2019-04-04 09:00:38,555] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5649
[2019-04-04 09:00:38,613] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333334, 94.66666666666666, 10.5, 0.0, 26.0, 25.80559374035487, 0.4845831049174071, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4693200.0000, 
sim time next is 4693800.0000, 
raw observation next is [-0.1666666666666666, 93.33333333333334, 21.0, 0.0, 26.0, 25.81263130902079, 0.4789809694332459, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4579870729455217, 0.9333333333333335, 0.07, 0.0, 0.6666666666666666, 0.6510526090850659, 0.6596603231444153, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.94385254], dtype=float32), 0.98509026]. 
=============================================
[2019-04-04 09:00:41,305] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4515034e-09 1.2489880e-08 2.7303960e-25 1.6689885e-24 4.9810167e-20
 1.0000000e+00 3.6650336e-19], sum to 1.0000
[2019-04-04 09:00:41,320] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1608
[2019-04-04 09:00:41,338] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333334, 45.0, 0.0, 0.0, 26.0, 25.46854022726957, 0.4451008858355106, 0.0, 1.0, 18756.26134784349], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5019600.0000, 
sim time next is 5020200.0000, 
raw observation next is [0.0, 47.5, 0.0, 0.0, 26.0, 25.55187557501937, 0.4442996934959288, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.475, 0.0, 0.0, 0.6666666666666666, 0.6293229645849475, 0.6480998978319762, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45810106], dtype=float32), -0.9323685]. 
=============================================
[2019-04-04 09:00:42,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:00:42,910] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:00:42,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run21
[2019-04-04 09:00:44,405] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.1663830e-10 3.5120371e-09 1.3813557e-25 1.0802516e-23 1.2152618e-19
 1.0000000e+00 9.3113447e-19], sum to 1.0000
[2019-04-04 09:00:44,406] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6373
[2019-04-04 09:00:44,421] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 26.0, 25.40676067366283, 0.3765715641736455, 0.0, 1.0, 40997.40730147939], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4837200.0000, 
sim time next is 4837800.0000, 
raw observation next is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 26.0, 25.38710764350263, 0.383683233206179, 0.0, 1.0, 50627.1191802789], 
processed observation next is [0.0, 1.0, 0.41181902123730385, 0.5916666666666666, 0.0, 0.0, 0.6666666666666666, 0.6155923036252192, 0.6278944110687263, 0.0, 1.0, 0.24108151990609003], 
reward next is 0.7589, 
noisyNet noise sample is [array([-2.1344702], dtype=float32), -0.24039577]. 
=============================================
[2019-04-04 09:00:44,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:00:44,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:00:44,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run21
[2019-04-04 09:00:45,183] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:00:45,184] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:00:45,188] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run21
[2019-04-04 09:00:46,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:00:46,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:00:46,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run21
[2019-04-04 09:00:51,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:00:51,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:00:51,095] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run21
[2019-04-04 09:00:51,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:00:51,714] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:00:51,728] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run21
[2019-04-04 09:00:53,348] A3C_AGENT_WORKER-Thread-2 INFO:Local step 170000, global step 2715012: loss 0.0418
[2019-04-04 09:00:53,348] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 170000, global step 2715012: learning rate 0.0000
[2019-04-04 09:00:54,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:00:54,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:00:54,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run21
[2019-04-04 09:00:55,243] A3C_AGENT_WORKER-Thread-6 INFO:Local step 170000, global step 2715556: loss 0.0463
[2019-04-04 09:00:55,245] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 170000, global step 2715556: learning rate 0.0000
[2019-04-04 09:00:55,901] A3C_AGENT_WORKER-Thread-16 INFO:Local step 170000, global step 2715747: loss 0.0491
[2019-04-04 09:00:55,903] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 170000, global step 2715747: learning rate 0.0000
[2019-04-04 09:00:57,581] A3C_AGENT_WORKER-Thread-3 INFO:Local step 170000, global step 2716233: loss 0.0442
[2019-04-04 09:00:57,582] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 170000, global step 2716233: learning rate 0.0000
[2019-04-04 09:01:01,291] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.5796522e-12 2.3463381e-11 5.1675089e-31 3.0698372e-29 1.3756674e-23
 1.0000000e+00 4.9504357e-23], sum to 1.0000
[2019-04-04 09:01:01,292] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3011
[2019-04-04 09:01:01,322] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.0, 17.33333333333334, 62.66666666666667, 487.3333333333334, 26.0, 29.14120984256841, 1.034469936569848, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5071800.0000, 
sim time next is 5072400.0000, 
raw observation next is [12.0, 17.0, 56.0, 438.5, 26.0, 29.16177144236566, 1.202290236998601, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7950138504155125, 0.17, 0.18666666666666668, 0.4845303867403315, 0.6666666666666666, 0.9301476201971383, 0.9007634123328669, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1636326], dtype=float32), 0.5831389]. 
=============================================
[2019-04-04 09:01:01,344] A3C_AGENT_WORKER-Thread-15 INFO:Local step 170000, global step 2717619: loss 0.0464
[2019-04-04 09:01:01,345] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 170000, global step 2717619: learning rate 0.0000
[2019-04-04 09:01:02,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:01:02,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:01:02,090] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run21
[2019-04-04 09:01:02,206] A3C_AGENT_WORKER-Thread-18 INFO:Local step 170000, global step 2717967: loss 0.0437
[2019-04-04 09:01:02,207] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 170000, global step 2717967: learning rate 0.0000
[2019-04-04 09:01:03,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:01:03,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:01:03,132] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run21
[2019-04-04 09:01:03,454] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:01:03,454] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:01:03,458] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run21
[2019-04-04 09:01:04,576] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:01:04,576] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:01:04,587] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run21
[2019-04-04 09:01:04,764] A3C_AGENT_WORKER-Thread-4 INFO:Local step 170000, global step 2718597: loss 0.0413
[2019-04-04 09:01:04,764] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 170000, global step 2718597: learning rate 0.0000
[2019-04-04 09:01:05,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:01:05,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:01:05,268] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run21
[2019-04-04 09:01:06,979] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3245355e-10 1.8746018e-09 1.2387307e-25 1.0275909e-24 2.8758608e-20
 1.0000000e+00 3.2056746e-19], sum to 1.0000
[2019-04-04 09:01:06,979] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1857
[2019-04-04 09:01:07,032] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 36.5, 0.0, 0.0, 26.0, 25.79359626437235, 0.5084243562207519, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5004600.0000, 
sim time next is 5005200.0000, 
raw observation next is [3.0, 36.0, 0.0, 0.0, 26.0, 25.75336504570898, 0.4815232703267263, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6461137538090815, 0.6605077567755754, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6727823], dtype=float32), -1.7114251]. 
=============================================
[2019-04-04 09:01:13,538] A3C_AGENT_WORKER-Thread-12 INFO:Local step 170000, global step 2720616: loss 0.0488
[2019-04-04 09:01:13,550] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 170000, global step 2720616: learning rate 0.0000
[2019-04-04 09:01:14,481] A3C_AGENT_WORKER-Thread-19 INFO:Local step 170000, global step 2720881: loss 0.0543
[2019-04-04 09:01:14,483] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 170000, global step 2720881: learning rate 0.0000
[2019-04-04 09:01:14,823] A3C_AGENT_WORKER-Thread-11 INFO:Local step 170000, global step 2720970: loss 0.0445
[2019-04-04 09:01:14,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 170000, global step 2720970: learning rate 0.0000
[2019-04-04 09:01:14,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:01:14,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:01:14,838] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run21
[2019-04-04 09:01:15,850] A3C_AGENT_WORKER-Thread-10 INFO:Local step 170000, global step 2721200: loss 0.0514
[2019-04-04 09:01:15,851] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 170000, global step 2721200: learning rate 0.0000
[2019-04-04 09:01:16,314] A3C_AGENT_WORKER-Thread-13 INFO:Local step 170000, global step 2721314: loss 0.0559
[2019-04-04 09:01:16,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 170000, global step 2721314: learning rate 0.0000
[2019-04-04 09:01:19,273] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.3362392e-10 1.4782611e-09 7.9781270e-27 2.4073439e-24 1.4173089e-20
 1.0000000e+00 1.7245886e-19], sum to 1.0000
[2019-04-04 09:01:19,273] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4516
[2019-04-04 09:01:19,299] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333333, 95.16666666666667, 0.0, 0.0, 26.0, 24.44366966321932, 0.1789108912398988, 0.0, 1.0, 40102.08782873357], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 82200.0000, 
sim time next is 82800.0000, 
raw observation next is [0.3, 95.0, 0.0, 0.0, 26.0, 24.41986300824521, 0.1744534435126807, 0.0, 1.0, 40084.88789819977], 
processed observation next is [0.0, 1.0, 0.47091412742382277, 0.95, 0.0, 0.0, 0.6666666666666666, 0.534988584020434, 0.5581511478375603, 0.0, 1.0, 0.19088041856285604], 
reward next is 0.8091, 
noisyNet noise sample is [array([-0.5675397], dtype=float32), -1.1217257]. 
=============================================
[2019-04-04 09:01:19,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:01:19,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:01:19,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run21
[2019-04-04 09:01:20,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:01:20,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:01:20,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run21
[2019-04-04 09:01:23,696] A3C_AGENT_WORKER-Thread-2 INFO:Local step 170500, global step 2723159: loss 0.1194
[2019-04-04 09:01:23,699] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 170500, global step 2723159: learning rate 0.0000
[2019-04-04 09:01:26,241] A3C_AGENT_WORKER-Thread-6 INFO:Local step 170500, global step 2723680: loss 0.1016
[2019-04-04 09:01:26,244] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 170500, global step 2723680: learning rate 0.0000
[2019-04-04 09:01:26,332] A3C_AGENT_WORKER-Thread-5 INFO:Local step 170000, global step 2723701: loss 0.0469
[2019-04-04 09:01:26,335] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 170000, global step 2723702: learning rate 0.0000
[2019-04-04 09:01:26,576] A3C_AGENT_WORKER-Thread-16 INFO:Local step 170500, global step 2723755: loss 0.1092
[2019-04-04 09:01:26,580] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 170500, global step 2723755: learning rate 0.0000
[2019-04-04 09:01:27,895] A3C_AGENT_WORKER-Thread-3 INFO:Local step 170500, global step 2724143: loss 0.1053
[2019-04-04 09:01:27,897] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 170500, global step 2724143: learning rate 0.0000
[2019-04-04 09:01:30,956] A3C_AGENT_WORKER-Thread-17 INFO:Local step 170000, global step 2724987: loss 0.0510
[2019-04-04 09:01:30,956] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 170000, global step 2724987: learning rate 0.0000
[2019-04-04 09:01:31,118] A3C_AGENT_WORKER-Thread-15 INFO:Local step 170500, global step 2725024: loss 0.1216
[2019-04-04 09:01:31,146] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 170500, global step 2725024: learning rate 0.0000
[2019-04-04 09:01:31,566] A3C_AGENT_WORKER-Thread-14 INFO:Local step 170000, global step 2725122: loss 0.0463
[2019-04-04 09:01:31,566] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 170000, global step 2725122: learning rate 0.0000
[2019-04-04 09:01:32,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:01:32,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:01:32,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run21
[2019-04-04 09:01:32,593] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.49004012e-10 2.97032643e-09 1.27367602e-26 2.71656934e-24
 1.07687994e-19 1.00000000e+00 1.95349184e-19], sum to 1.0000
[2019-04-04 09:01:32,593] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0151
[2019-04-04 09:01:32,647] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.5, 50.16666666666666, 56.0, 893.0, 26.0, 25.52219303993748, 0.4026265010714583, 1.0, 1.0, 96171.36172335374], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 393000.0000, 
sim time next is 393600.0000, 
raw observation next is [-11.3, 49.33333333333334, 55.5, 890.0, 26.0, 25.66773353306255, 0.4391123930782942, 1.0, 1.0, 67477.09107082966], 
processed observation next is [1.0, 0.5652173913043478, 0.14958448753462603, 0.4933333333333334, 0.185, 0.9834254143646409, 0.6666666666666666, 0.6389777944218791, 0.6463707976927647, 1.0, 1.0, 0.32131948128966503], 
reward next is 0.6787, 
noisyNet noise sample is [array([0.04437556], dtype=float32), -1.5712134]. 
=============================================
[2019-04-04 09:01:34,597] A3C_AGENT_WORKER-Thread-18 INFO:Local step 170500, global step 2725757: loss 0.1224
[2019-04-04 09:01:34,598] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 170500, global step 2725757: learning rate 0.0000
[2019-04-04 09:01:36,351] A3C_AGENT_WORKER-Thread-4 INFO:Local step 170500, global step 2726250: loss 0.1109
[2019-04-04 09:01:36,351] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 170500, global step 2726250: learning rate 0.0000
[2019-04-04 09:01:40,052] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9394499e-09 8.1036866e-08 5.7513084e-23 2.6251590e-22 2.7101082e-18
 9.9999988e-01 1.9571859e-17], sum to 1.0000
[2019-04-04 09:01:40,052] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2203
[2019-04-04 09:01:40,141] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.533333333333333, 38.83333333333334, 15.33333333333334, 0.0, 26.0, 24.66590119266972, 0.09935042691753011, 0.0, 1.0, 66216.58740973691], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 461400.0000, 
sim time next is 462000.0000, 
raw observation next is [-7.266666666666667, 37.66666666666667, 19.16666666666667, 0.0, 26.0, 24.8400144373114, 0.1410491630912226, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.26131117266851345, 0.3766666666666667, 0.0638888888888889, 0.0, 0.6666666666666666, 0.5700012031092833, 0.5470163876970742, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5249457], dtype=float32), 0.6224659]. 
=============================================
[2019-04-04 09:01:40,204] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.20769]
 [72.6115 ]
 [71.30313]
 [69.80495]
 [67.93678]], R is [[72.8894043 ]
 [72.84519196]
 [72.68349457]
 [72.43193817]
 [71.94638824]].
[2019-04-04 09:01:43,520] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.0780642e-10 3.5826857e-09 2.1803116e-24 1.6441736e-23 1.0096155e-18
 1.0000000e+00 5.4667617e-19], sum to 1.0000
[2019-04-04 09:01:43,520] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2566
[2019-04-04 09:01:43,595] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.15, 53.0, 0.0, 0.0, 26.0, 25.69567113570359, 0.4011654208686277, 1.0, 1.0, 106320.850756951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 322200.0000, 
sim time next is 322800.0000, 
raw observation next is [-11.33333333333333, 54.33333333333334, 0.0, 0.0, 26.0, 25.66045285677018, 0.4144253208309974, 1.0, 1.0, 78149.45755467913], 
processed observation next is [1.0, 0.7391304347826086, 0.14866112650046176, 0.5433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6383710713975151, 0.6381417736103324, 1.0, 1.0, 0.37214027406990063], 
reward next is 0.6279, 
noisyNet noise sample is [array([-0.22998457], dtype=float32), -0.9495043]. 
=============================================
[2019-04-04 09:01:43,657] A3C_AGENT_WORKER-Thread-20 INFO:Local step 170000, global step 2728068: loss 0.0492
[2019-04-04 09:01:43,658] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 170000, global step 2728068: learning rate 0.0000
[2019-04-04 09:01:44,798] A3C_AGENT_WORKER-Thread-19 INFO:Local step 170500, global step 2728395: loss 0.1361
[2019-04-04 09:01:44,806] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 170500, global step 2728396: learning rate 0.0000
[2019-04-04 09:01:45,806] A3C_AGENT_WORKER-Thread-12 INFO:Local step 170500, global step 2728710: loss 0.1344
[2019-04-04 09:01:45,806] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 170500, global step 2728710: learning rate 0.0000
[2019-04-04 09:01:46,051] A3C_AGENT_WORKER-Thread-10 INFO:Local step 170500, global step 2728789: loss 0.1232
[2019-04-04 09:01:46,052] A3C_AGENT_WORKER-Thread-11 INFO:Local step 170500, global step 2728789: loss 0.1234
[2019-04-04 09:01:46,055] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 170500, global step 2728790: learning rate 0.0000
[2019-04-04 09:01:46,059] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 170500, global step 2728790: learning rate 0.0000
[2019-04-04 09:01:46,672] A3C_AGENT_WORKER-Thread-13 INFO:Local step 170500, global step 2728974: loss 0.1329
[2019-04-04 09:01:46,684] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 170500, global step 2728974: learning rate 0.0000
[2019-04-04 09:01:53,238] A3C_AGENT_WORKER-Thread-2 INFO:Local step 171000, global step 2730681: loss 0.0017
[2019-04-04 09:01:53,238] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 171000, global step 2730681: learning rate 0.0000
[2019-04-04 09:01:54,998] A3C_AGENT_WORKER-Thread-16 INFO:Local step 171000, global step 2731192: loss 0.0018
[2019-04-04 09:01:55,001] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 171000, global step 2731194: learning rate 0.0000
[2019-04-04 09:01:55,815] A3C_AGENT_WORKER-Thread-6 INFO:Local step 171000, global step 2731448: loss 0.0016
[2019-04-04 09:01:55,816] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 171000, global step 2731448: learning rate 0.0000
[2019-04-04 09:01:56,199] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2358916e-08 1.7406545e-07 1.1460784e-20 1.6712795e-19 4.6249730e-16
 9.9999976e-01 4.1222071e-16], sum to 1.0000
[2019-04-04 09:01:56,199] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7897
[2019-04-04 09:01:56,303] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.3, 42.5, 0.0, 0.0, 26.0, 22.49669682434098, -0.3338514588320365, 0.0, 1.0, 46024.76147984101], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 457800.0000, 
sim time next is 458400.0000, 
raw observation next is [-8.2, 42.0, 0.0, 0.0, 26.0, 22.53857977743213, -0.2641167282620536, 1.0, 1.0, 202324.9125496788], 
processed observation next is [1.0, 0.30434782608695654, 0.23545706371191139, 0.42, 0.0, 0.0, 0.6666666666666666, 0.37821498145267746, 0.41196109057931546, 1.0, 1.0, 0.96345196452228], 
reward next is 0.0365, 
noisyNet noise sample is [array([0.48821163], dtype=float32), -1.2635181]. 
=============================================
[2019-04-04 09:01:57,148] A3C_AGENT_WORKER-Thread-5 INFO:Local step 170500, global step 2731831: loss 0.1322
[2019-04-04 09:01:57,148] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 170500, global step 2731831: learning rate 0.0000
[2019-04-04 09:01:57,773] A3C_AGENT_WORKER-Thread-3 INFO:Local step 171000, global step 2732014: loss 0.0016
[2019-04-04 09:01:57,774] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 171000, global step 2732014: learning rate 0.0000
[2019-04-04 09:01:57,818] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.6467153e-10 1.1066560e-08 3.4929376e-24 2.3263820e-23 8.2856940e-19
 1.0000000e+00 1.2244434e-18], sum to 1.0000
[2019-04-04 09:01:57,819] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4413
[2019-04-04 09:01:57,899] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 30.0, 98.0, 0.0, 26.0, 25.47250040165761, 0.1736187975351851, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 469800.0000, 
sim time next is 470400.0000, 
raw observation next is [-3.033333333333333, 29.33333333333333, 102.0, 0.0, 26.0, 25.35831530660532, 0.1671294565338375, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.37857802400738694, 0.2933333333333333, 0.34, 0.0, 0.6666666666666666, 0.61319294221711, 0.5557098188446125, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6842082], dtype=float32), -1.1018595]. 
=============================================
[2019-04-04 09:02:00,210] A3C_AGENT_WORKER-Thread-15 INFO:Local step 171000, global step 2732627: loss 0.0015
[2019-04-04 09:02:00,214] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 171000, global step 2732627: learning rate 0.0000
[2019-04-04 09:02:01,555] A3C_AGENT_WORKER-Thread-14 INFO:Local step 170500, global step 2732996: loss 0.1471
[2019-04-04 09:02:01,556] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 170500, global step 2732998: learning rate 0.0000
[2019-04-04 09:02:01,801] A3C_AGENT_WORKER-Thread-17 INFO:Local step 170500, global step 2733060: loss 0.1462
[2019-04-04 09:02:01,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 170500, global step 2733060: learning rate 0.0000
[2019-04-04 09:02:03,567] A3C_AGENT_WORKER-Thread-18 INFO:Local step 171000, global step 2733633: loss 0.0014
[2019-04-04 09:02:03,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 171000, global step 2733633: learning rate 0.0000
[2019-04-04 09:02:05,621] A3C_AGENT_WORKER-Thread-4 INFO:Local step 171000, global step 2734275: loss 0.0015
[2019-04-04 09:02:05,624] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 171000, global step 2734275: learning rate 0.0000
[2019-04-04 09:02:07,920] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.8598925e-11 2.1762980e-09 4.0380756e-28 1.1313762e-25 4.6815329e-21
 1.0000000e+00 1.5189621e-20], sum to 1.0000
[2019-04-04 09:02:07,921] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6410
[2019-04-04 09:02:07,999] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.7666666666666667, 81.33333333333333, 102.6666666666667, 222.0, 26.0, 24.82400684198906, 0.2864745626891941, 0.0, 1.0, 58377.82488479392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 561000.0000, 
sim time next is 561600.0000, 
raw observation next is [-0.8, 81.0, 109.5, 265.5, 26.0, 24.83448818837244, 0.3077090229455779, 0.0, 1.0, 44099.4407143953], 
processed observation next is [0.0, 0.5217391304347826, 0.4404432132963989, 0.81, 0.365, 0.29337016574585634, 0.6666666666666666, 0.5695406823643699, 0.6025696743151926, 0.0, 1.0, 0.2099973367352157], 
reward next is 0.7900, 
noisyNet noise sample is [array([1.3116124], dtype=float32), 0.57899946]. 
=============================================
[2019-04-04 09:02:13,268] A3C_AGENT_WORKER-Thread-19 INFO:Local step 171000, global step 2736500: loss 0.0017
[2019-04-04 09:02:13,272] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 171000, global step 2736500: learning rate 0.0000
[2019-04-04 09:02:13,650] A3C_AGENT_WORKER-Thread-20 INFO:Local step 170500, global step 2736605: loss 0.1588
[2019-04-04 09:02:13,652] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 170500, global step 2736605: learning rate 0.0000
[2019-04-04 09:02:13,875] A3C_AGENT_WORKER-Thread-12 INFO:Local step 171000, global step 2736677: loss 0.0019
[2019-04-04 09:02:13,875] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 171000, global step 2736677: learning rate 0.0000
[2019-04-04 09:02:14,551] A3C_AGENT_WORKER-Thread-10 INFO:Local step 171000, global step 2736854: loss 0.0018
[2019-04-04 09:02:14,563] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 171000, global step 2736854: learning rate 0.0000
[2019-04-04 09:02:14,741] A3C_AGENT_WORKER-Thread-11 INFO:Local step 171000, global step 2736905: loss 0.0023
[2019-04-04 09:02:14,742] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 171000, global step 2736905: learning rate 0.0000
[2019-04-04 09:02:16,060] A3C_AGENT_WORKER-Thread-13 INFO:Local step 171000, global step 2737250: loss 0.0021
[2019-04-04 09:02:16,061] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 171000, global step 2737250: learning rate 0.0000
[2019-04-04 09:02:17,262] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7657179e-10 3.8259236e-09 2.3615087e-26 4.4386063e-24 4.9713736e-20
 1.0000000e+00 9.1149244e-19], sum to 1.0000
[2019-04-04 09:02:17,262] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8572
[2019-04-04 09:02:17,290] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.72534146979959, 0.2306545002519585, 0.0, 1.0, 39648.97664565075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 866400.0000, 
sim time next is 867000.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.78194655720925, 0.2267974490107881, 0.0, 1.0, 39550.80090927865], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.6666666666666666, 0.5651622131007707, 0.5755991496702627, 0.0, 1.0, 0.1883371471870412], 
reward next is 0.8117, 
noisyNet noise sample is [array([1.1081129], dtype=float32), 0.62854093]. 
=============================================
[2019-04-04 09:02:17,311] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[80.9504 ]
 [80.81459]
 [80.81899]
 [80.73977]
 [80.62923]], R is [[81.05198669]
 [81.05266571]
 [81.05269623]
 [81.05210876]
 [81.05091858]].
[2019-04-04 09:02:18,185] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0253384e-10 1.1793663e-09 1.2295597e-25 2.6033778e-23 1.5894867e-19
 1.0000000e+00 1.8254458e-18], sum to 1.0000
[2019-04-04 09:02:18,185] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9252
[2019-04-04 09:02:18,199] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.65, 71.5, 0.0, 0.0, 26.0, 24.39741353656148, 0.09159105718631388, 0.0, 1.0, 40964.45704765277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 693000.0000, 
sim time next is 693600.0000, 
raw observation next is [-3.566666666666666, 71.66666666666667, 0.0, 0.0, 26.0, 24.36988640497635, 0.09521044285577081, 0.0, 1.0, 40950.16498190151], 
processed observation next is [1.0, 0.0, 0.3638042474607572, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5308238670813624, 0.5317368142852569, 0.0, 1.0, 0.1950007856281024], 
reward next is 0.8050, 
noisyNet noise sample is [array([0.43601975], dtype=float32), 1.4701535]. 
=============================================
[2019-04-04 09:02:18,662] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.75694445e-10 1.05793205e-08 4.72382433e-25 2.64114891e-23
 9.69712517e-19 1.00000000e+00 3.06834397e-18], sum to 1.0000
[2019-04-04 09:02:18,663] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9314
[2019-04-04 09:02:18,745] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.116666666666666, 61.16666666666667, 0.0, 0.0, 26.0, 24.90989498811163, 0.2084444658283471, 0.0, 1.0, 40587.31129400532], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 672600.0000, 
sim time next is 673200.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.91640982639554, 0.2060605583202335, 0.0, 1.0, 42814.41591019015], 
processed observation next is [0.0, 0.8260869565217391, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5763674855329617, 0.5686868527734111, 0.0, 1.0, 0.2038781710009055], 
reward next is 0.7961, 
noisyNet noise sample is [array([1.5496043], dtype=float32), -2.1816504]. 
=============================================
[2019-04-04 09:02:19,358] A3C_AGENT_WORKER-Thread-2 INFO:Local step 171500, global step 2738285: loss 0.0109
[2019-04-04 09:02:19,359] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 171500, global step 2738285: learning rate 0.0000
[2019-04-04 09:02:21,074] A3C_AGENT_WORKER-Thread-16 INFO:Local step 171500, global step 2738855: loss 0.0152
[2019-04-04 09:02:21,074] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 171500, global step 2738855: learning rate 0.0000
[2019-04-04 09:02:22,257] A3C_AGENT_WORKER-Thread-6 INFO:Local step 171500, global step 2739249: loss 0.0130
[2019-04-04 09:02:22,260] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 171500, global step 2739249: learning rate 0.0000
[2019-04-04 09:02:23,441] A3C_AGENT_WORKER-Thread-3 INFO:Local step 171500, global step 2739653: loss 0.0125
[2019-04-04 09:02:23,441] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 171500, global step 2739653: learning rate 0.0000
[2019-04-04 09:02:24,109] A3C_AGENT_WORKER-Thread-5 INFO:Local step 171000, global step 2739869: loss 0.0021
[2019-04-04 09:02:24,112] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 171000, global step 2739869: learning rate 0.0000
[2019-04-04 09:02:24,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7043597e-09 1.7865863e-08 2.0645768e-24 1.9519385e-23 2.6508659e-18
 1.0000000e+00 3.5794388e-18], sum to 1.0000
[2019-04-04 09:02:24,549] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6803
[2019-04-04 09:02:24,570] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.25277695427169, 0.1386421519142427, 0.0, 1.0, 41705.45994597278], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 779400.0000, 
sim time next is 780000.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.25357467374609, 0.1409503233721204, 0.0, 1.0, 41651.74287610393], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5211312228121742, 0.5469834411240401, 0.0, 1.0, 0.19834163274335204], 
reward next is 0.8017, 
noisyNet noise sample is [array([1.2406634], dtype=float32), -0.7303703]. 
=============================================
[2019-04-04 09:02:24,575] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.37523 ]
 [75.44938 ]
 [75.436   ]
 [75.387375]
 [75.474464]], R is [[75.47258759]
 [75.51926422]
 [75.56523895]
 [75.61048889]
 [75.654953  ]].
[2019-04-04 09:02:25,776] A3C_AGENT_WORKER-Thread-15 INFO:Local step 171500, global step 2740553: loss 0.0105
[2019-04-04 09:02:25,778] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 171500, global step 2740554: learning rate 0.0000
[2019-04-04 09:02:26,651] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5887678e-10 1.9380380e-08 2.6477525e-26 7.5272555e-25 5.3652983e-20
 1.0000000e+00 6.7550319e-19], sum to 1.0000
[2019-04-04 09:02:26,652] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6163
[2019-04-04 09:02:26,676] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.52042898947788, 0.1619080564814608, 0.0, 1.0, 38492.44752416784], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 888600.0000, 
sim time next is 889200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.56033862079106, 0.1637784104801988, 0.0, 1.0, 38441.3958789251], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5466948850659218, 0.5545928034933997, 0.0, 1.0, 0.18305426609011952], 
reward next is 0.8169, 
noisyNet noise sample is [array([-0.05172239], dtype=float32), -2.7281718]. 
=============================================
[2019-04-04 09:02:28,584] A3C_AGENT_WORKER-Thread-14 INFO:Local step 171000, global step 2741639: loss 0.0024
[2019-04-04 09:02:28,584] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 171000, global step 2741639: learning rate 0.0000
[2019-04-04 09:02:28,983] A3C_AGENT_WORKER-Thread-17 INFO:Local step 171000, global step 2741776: loss 0.0030
[2019-04-04 09:02:28,984] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 171000, global step 2741776: learning rate 0.0000
[2019-04-04 09:02:29,207] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4014845e-10 5.2658105e-09 1.4206126e-25 3.5650103e-24 1.0119362e-19
 1.0000000e+00 2.2189055e-19], sum to 1.0000
[2019-04-04 09:02:29,208] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3848
[2019-04-04 09:02:29,236] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.17087384925158, 0.005562917802995925, 0.0, 1.0, 42035.92877027448], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 716400.0000, 
sim time next is 717000.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.10621444700531, 0.002680207362833429, 0.0, 1.0, 42077.33304301019], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5088512039171093, 0.5008934024542778, 0.0, 1.0, 0.20036825258576282], 
reward next is 0.7996, 
noisyNet noise sample is [array([0.5080994], dtype=float32), -1.3642517]. 
=============================================
[2019-04-04 09:02:29,252] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[79.56081 ]
 [79.554886]
 [79.5437  ]
 [79.525894]
 [79.5018  ]], R is [[79.54118347]
 [79.54560089]
 [79.55023193]
 [79.55493927]
 [79.55970764]].
[2019-04-04 09:02:29,620] A3C_AGENT_WORKER-Thread-18 INFO:Local step 171500, global step 2742004: loss 0.0131
[2019-04-04 09:02:29,623] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 171500, global step 2742008: learning rate 0.0000
[2019-04-04 09:02:31,349] A3C_AGENT_WORKER-Thread-4 INFO:Local step 171500, global step 2742685: loss 0.0130
[2019-04-04 09:02:31,352] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 171500, global step 2742685: learning rate 0.0000
[2019-04-04 09:02:31,719] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.8996289e-12 3.8351275e-10 4.9020316e-29 1.1059427e-27 1.4876675e-22
 1.0000000e+00 1.2034459e-21], sum to 1.0000
[2019-04-04 09:02:31,724] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8712
[2019-04-04 09:02:31,752] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 89.0, 0.0, 0.0, 26.0, 25.30799560841691, 0.4250689738580312, 0.0, 1.0, 38027.42941762351], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 954000.0000, 
sim time next is 954600.0000, 
raw observation next is [5.683333333333334, 87.83333333333334, 0.0, 0.0, 26.0, 25.33163860924913, 0.4298770969945997, 0.0, 1.0, 38000.63507809236], 
processed observation next is [1.0, 0.043478260869565216, 0.6200369344413666, 0.8783333333333334, 0.0, 0.0, 0.6666666666666666, 0.6109698841040941, 0.6432923656648666, 0.0, 1.0, 0.18095540513377315], 
reward next is 0.8190, 
noisyNet noise sample is [array([-2.6073854], dtype=float32), 0.055993352]. 
=============================================
[2019-04-04 09:02:34,040] A3C_AGENT_WORKER-Thread-2 INFO:Local step 172000, global step 2744048: loss 12.8465
[2019-04-04 09:02:34,044] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 172000, global step 2744051: learning rate 0.0000
[2019-04-04 09:02:35,004] A3C_AGENT_WORKER-Thread-16 INFO:Local step 172000, global step 2744510: loss 12.8754
[2019-04-04 09:02:35,006] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 172000, global step 2744511: learning rate 0.0000
[2019-04-04 09:02:35,785] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9505932e-10 2.8793234e-09 1.1948796e-26 1.4823127e-24 2.5738087e-19
 1.0000000e+00 1.2594901e-19], sum to 1.0000
[2019-04-04 09:02:35,785] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2553
[2019-04-04 09:02:35,798] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 83.66666666666667, 0.0, 0.0, 26.0, 24.77971639348739, 0.2208407634932938, 0.0, 1.0, 42543.74052600544], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 601800.0000, 
sim time next is 602400.0000, 
raw observation next is [-3.4, 84.33333333333334, 0.0, 0.0, 26.0, 24.74754564228263, 0.213096245593367, 0.0, 1.0, 42475.90142797336], 
processed observation next is [0.0, 1.0, 0.368421052631579, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5622954701902193, 0.5710320818644556, 0.0, 1.0, 0.20226619727606363], 
reward next is 0.7977, 
noisyNet noise sample is [array([2.030128], dtype=float32), 1.0050272]. 
=============================================
[2019-04-04 09:02:36,433] A3C_AGENT_WORKER-Thread-6 INFO:Local step 172000, global step 2745242: loss 12.8198
[2019-04-04 09:02:36,433] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 172000, global step 2745242: learning rate 0.0000
[2019-04-04 09:02:37,154] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0820445e-10 1.6932870e-09 4.9477424e-28 1.0450155e-25 9.1922889e-22
 1.0000000e+00 1.1324844e-20], sum to 1.0000
[2019-04-04 09:02:37,155] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1554
[2019-04-04 09:02:37,159] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.73333333333334, 54.33333333333334, 0.0, 0.0, 26.0, 26.4948289712949, 0.7746503675350297, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1102800.0000, 
sim time next is 1103400.0000, 
raw observation next is [15.55, 55.0, 0.0, 0.0, 26.0, 26.47225836474515, 0.7600301564570654, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8933518005540168, 0.55, 0.0, 0.0, 0.6666666666666666, 0.7060215303954293, 0.7533433854856885, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09287814], dtype=float32), 0.4725193]. 
=============================================
[2019-04-04 09:02:37,175] A3C_AGENT_WORKER-Thread-19 INFO:Local step 171500, global step 2745566: loss 0.0168
[2019-04-04 09:02:37,176] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 171500, global step 2745567: learning rate 0.0000
[2019-04-04 09:02:37,199] A3C_AGENT_WORKER-Thread-3 INFO:Local step 172000, global step 2745574: loss 12.7749
[2019-04-04 09:02:37,200] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 172000, global step 2745574: learning rate 0.0000
[2019-04-04 09:02:37,728] A3C_AGENT_WORKER-Thread-12 INFO:Local step 171500, global step 2745808: loss 0.0152
[2019-04-04 09:02:37,732] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 171500, global step 2745809: learning rate 0.0000
[2019-04-04 09:02:37,919] A3C_AGENT_WORKER-Thread-11 INFO:Local step 171500, global step 2745897: loss 0.0138
[2019-04-04 09:02:37,919] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 171500, global step 2745897: learning rate 0.0000
[2019-04-04 09:02:38,092] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6769158e-11 1.1498511e-09 2.5793599e-28 1.1781998e-27 6.2741705e-22
 1.0000000e+00 1.2804948e-21], sum to 1.0000
[2019-04-04 09:02:38,097] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1245
[2019-04-04 09:02:38,112] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [19.4, 49.0, 100.0, 0.0, 26.0, 27.72148269306992, 0.8904083618793949, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1092000.0000, 
sim time next is 1092600.0000, 
raw observation next is [19.4, 49.0, 92.0, 0.0, 26.0, 27.32458291201171, 0.9397308704037978, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.30666666666666664, 0.0, 0.6666666666666666, 0.7770485760009759, 0.8132436234679327, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5184491], dtype=float32), -0.8549128]. 
=============================================
[2019-04-04 09:02:38,282] A3C_AGENT_WORKER-Thread-10 INFO:Local step 171500, global step 2746080: loss 0.0124
[2019-04-04 09:02:38,287] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 171500, global step 2746081: learning rate 0.0000
[2019-04-04 09:02:39,022] A3C_AGENT_WORKER-Thread-13 INFO:Local step 171500, global step 2746435: loss 0.0118
[2019-04-04 09:02:39,023] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 171500, global step 2746435: learning rate 0.0000
[2019-04-04 09:02:39,186] A3C_AGENT_WORKER-Thread-20 INFO:Local step 171000, global step 2746524: loss 0.0028
[2019-04-04 09:02:39,188] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 171000, global step 2746525: learning rate 0.0000
[2019-04-04 09:02:39,364] A3C_AGENT_WORKER-Thread-15 INFO:Local step 172000, global step 2746606: loss 12.6916
[2019-04-04 09:02:39,365] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 172000, global step 2746606: learning rate 0.0000
[2019-04-04 09:02:40,694] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2240096e-11 1.3056331e-10 3.6853717e-29 2.6207500e-27 1.0308663e-22
 1.0000000e+00 8.7398801e-22], sum to 1.0000
[2019-04-04 09:02:40,695] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8007
[2019-04-04 09:02:40,733] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.06393646718314, 0.5054252139967061, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1365000.0000, 
sim time next is 1365600.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.18471782953669, 0.4984032886901668, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5987264857947242, 0.666134429563389, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3081064], dtype=float32), 1.1700224]. 
=============================================
[2019-04-04 09:02:42,679] A3C_AGENT_WORKER-Thread-18 INFO:Local step 172000, global step 2748222: loss 12.5383
[2019-04-04 09:02:42,680] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 172000, global step 2748222: learning rate 0.0000
[2019-04-04 09:02:43,402] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.3552323e-12 1.0333255e-10 9.2802546e-32 6.6858026e-30 1.4311765e-24
 1.0000000e+00 2.7391097e-23], sum to 1.0000
[2019-04-04 09:02:43,418] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8413
[2019-04-04 09:02:43,433] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.3, 77.16666666666667, 0.0, 0.0, 26.0, 25.95898696840225, 0.6159443721084826, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1051800.0000, 
sim time next is 1052400.0000, 
raw observation next is [14.2, 77.33333333333334, 0.0, 0.0, 26.0, 25.90982365413307, 0.6021878127312844, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.8559556786703602, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.6591519711777559, 0.7007292709104281, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17528747], dtype=float32), -0.99920297]. 
=============================================
[2019-04-04 09:02:44,001] A3C_AGENT_WORKER-Thread-4 INFO:Local step 172000, global step 2748836: loss 12.5473
[2019-04-04 09:02:44,010] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 172000, global step 2748844: learning rate 0.0000
[2019-04-04 09:02:46,384] A3C_AGENT_WORKER-Thread-5 INFO:Local step 171500, global step 2750047: loss 0.0047
[2019-04-04 09:02:46,385] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 171500, global step 2750048: learning rate 0.0000
[2019-04-04 09:02:48,259] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7207874e-11 1.3793311e-10 1.1426334e-29 5.3660544e-28 5.3359189e-23
 1.0000000e+00 1.2511321e-22], sum to 1.0000
[2019-04-04 09:02:48,267] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1916
[2019-04-04 09:02:48,340] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.8, 93.0, 95.0, 0.0, 26.0, 24.75888959810454, 0.2824096606408779, 1.0, 1.0, 187256.1929940673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 913200.0000, 
sim time next is 913800.0000, 
raw observation next is [3.8, 93.0, 94.0, 0.0, 26.0, 24.69889554600977, 0.3302281208009413, 1.0, 1.0, 145975.1824610198], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.31333333333333335, 0.0, 0.6666666666666666, 0.5582412955008141, 0.6100760402669804, 1.0, 1.0, 0.6951199164810468], 
reward next is 0.3049, 
noisyNet noise sample is [array([0.3115054], dtype=float32), -1.5404134]. 
=============================================
[2019-04-04 09:02:48,687] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2752881e-10 1.1419080e-10 2.6688306e-29 1.1538352e-26 1.7416961e-22
 1.0000000e+00 3.3175266e-21], sum to 1.0000
[2019-04-04 09:02:48,690] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1476
[2019-04-04 09:02:48,700] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.9, 92.0, 0.0, 0.0, 26.0, 25.46828280023246, 0.465669243548722, 0.0, 1.0, 56136.43757299529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1474200.0000, 
sim time next is 1474800.0000, 
raw observation next is [2.0, 92.0, 0.0, 0.0, 26.0, 25.42452838978479, 0.4653074187391583, 0.0, 1.0, 67652.67395380404], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6187106991487324, 0.6551024729130528, 0.0, 1.0, 0.3221555902562097], 
reward next is 0.6778, 
noisyNet noise sample is [array([-0.07173126], dtype=float32), -1.0622858]. 
=============================================
[2019-04-04 09:02:48,747] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.01414536e-12 2.19602808e-10 2.62905838e-29 1.03491425e-27
 1.20392291e-22 1.00000000e+00 1.88279252e-21], sum to 1.0000
[2019-04-04 09:02:48,749] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4005
[2019-04-04 09:02:48,786] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.38783436414933, 0.4861293515624119, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1362000.0000, 
sim time next is 1362600.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.25149059737872, 0.4487308788741926, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6042908831148933, 0.6495769596247308, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08065742], dtype=float32), 1.71277]. 
=============================================
[2019-04-04 09:02:50,153] A3C_AGENT_WORKER-Thread-19 INFO:Local step 172000, global step 2752033: loss 12.3696
[2019-04-04 09:02:50,154] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 172000, global step 2752034: learning rate 0.0000
[2019-04-04 09:02:50,509] A3C_AGENT_WORKER-Thread-11 INFO:Local step 172000, global step 2752184: loss 12.3449
[2019-04-04 09:02:50,510] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 172000, global step 2752184: learning rate 0.0000
[2019-04-04 09:02:50,604] A3C_AGENT_WORKER-Thread-12 INFO:Local step 172000, global step 2752223: loss 12.3596
[2019-04-04 09:02:50,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 172000, global step 2752223: learning rate 0.0000
[2019-04-04 09:02:50,981] A3C_AGENT_WORKER-Thread-14 INFO:Local step 171500, global step 2752420: loss 0.0054
[2019-04-04 09:02:50,983] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 171500, global step 2752420: learning rate 0.0000
[2019-04-04 09:02:51,191] A3C_AGENT_WORKER-Thread-17 INFO:Local step 171500, global step 2752549: loss 0.0048
[2019-04-04 09:02:51,193] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 171500, global step 2752549: learning rate 0.0000
[2019-04-04 09:02:51,332] A3C_AGENT_WORKER-Thread-2 INFO:Local step 172500, global step 2752625: loss 0.0943
[2019-04-04 09:02:51,338] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 172500, global step 2752627: learning rate 0.0000
[2019-04-04 09:02:51,375] A3C_AGENT_WORKER-Thread-10 INFO:Local step 172000, global step 2752643: loss 12.3370
[2019-04-04 09:02:51,376] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 172000, global step 2752643: learning rate 0.0000
[2019-04-04 09:02:51,861] A3C_AGENT_WORKER-Thread-13 INFO:Local step 172000, global step 2752850: loss 12.2709
[2019-04-04 09:02:51,861] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 172000, global step 2752850: learning rate 0.0000
[2019-04-04 09:02:52,155] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0630264e-12 4.6080820e-10 8.0855023e-30 3.0738697e-28 1.1359452e-21
 1.0000000e+00 4.0577470e-21], sum to 1.0000
[2019-04-04 09:02:52,162] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3746
[2019-04-04 09:02:52,183] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.883333333333334, 83.0, 0.0, 0.0, 26.0, 25.50033500001283, 0.4322385853428669, 0.0, 1.0, 21544.90424357255], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 965400.0000, 
sim time next is 966000.0000, 
raw observation next is [8.066666666666666, 83.0, 0.0, 0.0, 26.0, 25.42227444252448, 0.425535941629112, 0.0, 1.0, 69439.4021557638], 
processed observation next is [1.0, 0.17391304347826086, 0.6860572483841183, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6185228702103732, 0.6418453138763707, 0.0, 1.0, 0.33066381978935144], 
reward next is 0.6693, 
noisyNet noise sample is [array([0.10912468], dtype=float32), -0.84746045]. 
=============================================
[2019-04-04 09:02:52,199] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[90.8929 ]
 [90.90482]
 [90.99055]
 [90.98749]
 [91.01406]], R is [[90.91802216]
 [90.90625   ]
 [90.99718475]
 [90.99789429]
 [90.99858093]].
[2019-04-04 09:02:52,872] A3C_AGENT_WORKER-Thread-16 INFO:Local step 172500, global step 2753355: loss 0.1069
[2019-04-04 09:02:52,874] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 172500, global step 2753355: learning rate 0.0000
[2019-04-04 09:02:53,573] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.5652131e-11 1.9854158e-09 1.5895340e-28 3.1006519e-27 1.9546592e-22
 1.0000000e+00 1.5143848e-21], sum to 1.0000
[2019-04-04 09:02:53,579] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8939
[2019-04-04 09:02:53,658] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 26.0, 25.49721344961542, 0.4669725934444801, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1495800.0000, 
sim time next is 1496400.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 26.0, 25.59916014823041, 0.4606866331560492, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6332633456858675, 0.6535622110520164, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8652489], dtype=float32), -0.8767431]. 
=============================================
[2019-04-04 09:02:54,247] A3C_AGENT_WORKER-Thread-6 INFO:Local step 172500, global step 2754023: loss 0.1175
[2019-04-04 09:02:54,250] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 172500, global step 2754024: learning rate 0.0000
[2019-04-04 09:02:54,391] A3C_AGENT_WORKER-Thread-3 INFO:Local step 172500, global step 2754092: loss 0.1202
[2019-04-04 09:02:54,392] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 172500, global step 2754093: learning rate 0.0000
[2019-04-04 09:02:56,166] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3465830e-12 5.3502803e-11 4.5302795e-31 7.0455007e-29 1.5831056e-23
 1.0000000e+00 6.3160237e-23], sum to 1.0000
[2019-04-04 09:02:56,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0358
[2019-04-04 09:02:56,186] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 26.0, 25.82268626879889, 0.5983144068625639, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1058400.0000, 
sim time next is 1059000.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 26.0, 25.77317704660407, 0.589839824591971, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.8310249307479226, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6477647538836724, 0.6966132748639904, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1116235], dtype=float32), 1.5227375]. 
=============================================
[2019-04-04 09:02:56,189] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[95.185135]
 [95.2842  ]
 [95.357666]
 [95.492516]
 [95.56792 ]], R is [[95.09001923]
 [95.13912201]
 [95.18772888]
 [95.2358551 ]
 [95.28350067]].
[2019-04-04 09:02:56,259] A3C_AGENT_WORKER-Thread-15 INFO:Local step 172500, global step 2755036: loss 0.1473
[2019-04-04 09:02:56,260] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 172500, global step 2755036: learning rate 0.0000
[2019-04-04 09:02:58,631] A3C_AGENT_WORKER-Thread-5 INFO:Local step 172000, global step 2756324: loss 12.0093
[2019-04-04 09:02:58,633] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 172000, global step 2756324: learning rate 0.0000
[2019-04-04 09:02:59,813] A3C_AGENT_WORKER-Thread-18 INFO:Local step 172500, global step 2756917: loss 0.1589
[2019-04-04 09:02:59,813] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 172500, global step 2756917: learning rate 0.0000
[2019-04-04 09:03:00,898] A3C_AGENT_WORKER-Thread-20 INFO:Local step 171500, global step 2757428: loss 0.0046
[2019-04-04 09:03:00,899] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 171500, global step 2757429: learning rate 0.0000
[2019-04-04 09:03:01,666] A3C_AGENT_WORKER-Thread-4 INFO:Local step 172500, global step 2757800: loss 0.1783
[2019-04-04 09:03:01,667] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 172500, global step 2757801: learning rate 0.0000
[2019-04-04 09:03:02,497] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8885884e-13 9.2654086e-11 4.3913667e-31 6.0614007e-30 4.7831757e-24
 1.0000000e+00 7.4965696e-24], sum to 1.0000
[2019-04-04 09:03:02,502] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7316
[2019-04-04 09:03:02,552] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.33333333333333, 54.33333333333334, 160.1666666666667, 144.8333333333333, 26.0, 27.27464460356155, 0.8381482690977778, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1599600.0000, 
sim time next is 1600200.0000, 
raw observation next is [12.7, 53.0, 149.0, 124.0, 26.0, 27.21921544777122, 0.7207909754409135, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8144044321329641, 0.53, 0.49666666666666665, 0.13701657458563535, 0.6666666666666666, 0.768267953980935, 0.7402636584803045, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2517567], dtype=float32), -0.54556924]. 
=============================================
[2019-04-04 09:03:03,168] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.5838194e-12 1.6237713e-10 1.4264466e-30 1.9086528e-28 1.8483402e-23
 1.0000000e+00 3.8929430e-22], sum to 1.0000
[2019-04-04 09:03:03,174] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2258
[2019-04-04 09:03:03,212] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 40.5, 0.0, 26.0, 26.00237409194655, 0.5814913001129361, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1329600.0000, 
sim time next is 1330200.0000, 
raw observation next is [0.5, 92.0, 45.0, 0.0, 26.0, 26.03730379980853, 0.588597221355959, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.15, 0.0, 0.6666666666666666, 0.6697753166507109, 0.6961990737853196, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.600356], dtype=float32), 0.6343337]. 
=============================================
[2019-04-04 09:03:03,502] A3C_AGENT_WORKER-Thread-14 INFO:Local step 172000, global step 2758626: loss 11.7446
[2019-04-04 09:03:03,504] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 172000, global step 2758627: learning rate 0.0000
[2019-04-04 09:03:04,096] A3C_AGENT_WORKER-Thread-17 INFO:Local step 172000, global step 2758893: loss 11.7897
[2019-04-04 09:03:04,097] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 172000, global step 2758894: learning rate 0.0000
[2019-04-04 09:03:07,921] A3C_AGENT_WORKER-Thread-19 INFO:Local step 172500, global step 2760571: loss 0.2194
[2019-04-04 09:03:07,923] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 172500, global step 2760571: learning rate 0.0000
[2019-04-04 09:03:08,062] A3C_AGENT_WORKER-Thread-11 INFO:Local step 172500, global step 2760633: loss 0.2260
[2019-04-04 09:03:08,063] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 172500, global step 2760633: learning rate 0.0000
[2019-04-04 09:03:08,121] A3C_AGENT_WORKER-Thread-12 INFO:Local step 172500, global step 2760657: loss 0.2380
[2019-04-04 09:03:08,129] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 172500, global step 2760658: learning rate 0.0000
[2019-04-04 09:03:08,979] A3C_AGENT_WORKER-Thread-10 INFO:Local step 172500, global step 2761029: loss 0.2353
[2019-04-04 09:03:08,982] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 172500, global step 2761029: learning rate 0.0000
[2019-04-04 09:03:09,092] A3C_AGENT_WORKER-Thread-13 INFO:Local step 172500, global step 2761081: loss 0.2308
[2019-04-04 09:03:09,093] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 172500, global step 2761081: learning rate 0.0000
[2019-04-04 09:03:10,416] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.09420112e-12 1.10927961e-10 8.30637222e-31 2.03287443e-28
 1.18843544e-23 1.00000000e+00 1.69303030e-22], sum to 1.0000
[2019-04-04 09:03:10,421] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6198
[2019-04-04 09:03:10,450] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.783333333333333, 74.66666666666667, 89.0, 102.3333333333333, 26.0, 26.15963127692202, 0.6083159633311666, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1588200.0000, 
sim time next is 1588800.0000, 
raw observation next is [6.966666666666667, 73.33333333333334, 102.0, 119.1666666666667, 26.0, 26.21822615295562, 0.6312192135537081, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6555863342566944, 0.7333333333333334, 0.34, 0.13167587476979745, 0.6666666666666666, 0.6848521794129683, 0.7104064045179027, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37391526], dtype=float32), 2.0231652]. 
=============================================
[2019-04-04 09:03:10,668] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7795956e-10 6.9066075e-10 2.2949630e-27 7.2085481e-26 5.8948053e-21
 1.0000000e+00 1.8897300e-20], sum to 1.0000
[2019-04-04 09:03:10,668] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9200
[2019-04-04 09:03:10,726] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.73918272396633, 0.5668769608155728, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1713000.0000, 
sim time next is 1713600.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.72034471263294, 0.5568623281640114, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6433620593860784, 0.6856207760546704, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.92934984], dtype=float32), -0.63844246]. 
=============================================
[2019-04-04 09:03:10,985] A3C_AGENT_WORKER-Thread-2 INFO:Local step 173000, global step 2761901: loss 5.1639
[2019-04-04 09:03:10,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 173000, global step 2761901: learning rate 0.0000
[2019-04-04 09:03:11,645] A3C_AGENT_WORKER-Thread-16 INFO:Local step 173000, global step 2762220: loss 5.1762
[2019-04-04 09:03:11,647] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 173000, global step 2762220: learning rate 0.0000
[2019-04-04 09:03:12,830] A3C_AGENT_WORKER-Thread-6 INFO:Local step 173000, global step 2762768: loss 5.1837
[2019-04-04 09:03:12,830] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 173000, global step 2762768: learning rate 0.0000
[2019-04-04 09:03:13,396] A3C_AGENT_WORKER-Thread-3 INFO:Local step 173000, global step 2763015: loss 5.2107
[2019-04-04 09:03:13,397] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 173000, global step 2763015: learning rate 0.0000
[2019-04-04 09:03:14,402] A3C_AGENT_WORKER-Thread-20 INFO:Local step 172000, global step 2763386: loss 11.4724
[2019-04-04 09:03:14,403] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 172000, global step 2763386: learning rate 0.0000
[2019-04-04 09:03:15,570] A3C_AGENT_WORKER-Thread-15 INFO:Local step 173000, global step 2763781: loss 5.2000
[2019-04-04 09:03:15,570] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 173000, global step 2763781: learning rate 0.0000
[2019-04-04 09:03:16,709] A3C_AGENT_WORKER-Thread-5 INFO:Local step 172500, global step 2764173: loss 0.2740
[2019-04-04 09:03:16,716] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 172500, global step 2764173: learning rate 0.0000
[2019-04-04 09:03:19,341] A3C_AGENT_WORKER-Thread-18 INFO:Local step 173000, global step 2765191: loss 5.2011
[2019-04-04 09:03:19,343] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 173000, global step 2765193: learning rate 0.0000
[2019-04-04 09:03:20,970] A3C_AGENT_WORKER-Thread-4 INFO:Local step 173000, global step 2765772: loss 5.2139
[2019-04-04 09:03:20,971] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 173000, global step 2765772: learning rate 0.0000
[2019-04-04 09:03:22,249] A3C_AGENT_WORKER-Thread-14 INFO:Local step 172500, global step 2766149: loss 0.2964
[2019-04-04 09:03:22,252] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 172500, global step 2766149: learning rate 0.0000
[2019-04-04 09:03:23,627] A3C_AGENT_WORKER-Thread-17 INFO:Local step 172500, global step 2766557: loss 0.2792
[2019-04-04 09:03:23,631] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 172500, global step 2766557: learning rate 0.0000
[2019-04-04 09:03:26,365] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1517793e-10 4.4230477e-09 3.2844637e-26 2.9254236e-25 1.0755944e-20
 1.0000000e+00 2.5235742e-19], sum to 1.0000
[2019-04-04 09:03:26,365] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4037
[2019-04-04 09:03:26,413] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.866666666666666, 85.0, 62.33333333333333, 0.0, 26.0, 25.54746188585964, 0.2797162903457909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2020800.0000, 
sim time next is 2021400.0000, 
raw observation next is [-5.8, 84.5, 69.0, 0.0, 26.0, 25.50788034628935, 0.2841911672042438, 1.0, 1.0, 18724.46385866225], 
processed observation next is [1.0, 0.391304347826087, 0.30193905817174516, 0.845, 0.23, 0.0, 0.6666666666666666, 0.6256566955241126, 0.5947303890680813, 1.0, 1.0, 0.08916411361267738], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.1810726], dtype=float32), -0.22189555]. 
=============================================
[2019-04-04 09:03:27,293] A3C_AGENT_WORKER-Thread-19 INFO:Local step 173000, global step 2767924: loss 5.2017
[2019-04-04 09:03:27,294] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 173000, global step 2767924: learning rate 0.0000
[2019-04-04 09:03:27,384] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.2837357e-12 3.3393777e-10 5.3180204e-29 5.0264083e-27 2.9535846e-22
 1.0000000e+00 5.6387622e-21], sum to 1.0000
[2019-04-04 09:03:27,389] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3816
[2019-04-04 09:03:27,417] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 59.0, 0.0, 26.0, 26.02054884074219, 0.5400607345899354, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1436400.0000, 
sim time next is 1437000.0000, 
raw observation next is [1.1, 92.0, 54.66666666666666, 0.0, 26.0, 26.05263595944482, 0.5168377024430953, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.1822222222222222, 0.0, 0.6666666666666666, 0.6710529966204017, 0.6722792341476985, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49548933], dtype=float32), -0.14515337]. 
=============================================
[2019-04-04 09:03:27,438] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[85.18327]
 [85.42776]
 [85.60541]
 [85.86776]
 [86.13888]], R is [[85.11898041]
 [85.26779175]
 [85.41511536]
 [85.56096649]
 [85.70536041]].
[2019-04-04 09:03:27,876] A3C_AGENT_WORKER-Thread-11 INFO:Local step 173000, global step 2768120: loss 5.2251
[2019-04-04 09:03:27,877] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 173000, global step 2768120: learning rate 0.0000
[2019-04-04 09:03:28,224] A3C_AGENT_WORKER-Thread-12 INFO:Local step 173000, global step 2768223: loss 5.2059
[2019-04-04 09:03:28,226] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 173000, global step 2768223: learning rate 0.0000
[2019-04-04 09:03:28,306] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.1058719e-09 3.5130103e-08 1.7686025e-24 7.9763474e-23 3.0923504e-18
 1.0000000e+00 1.8286667e-17], sum to 1.0000
[2019-04-04 09:03:28,307] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4929
[2019-04-04 09:03:28,329] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.3093753880428, 0.1329307245647686, 0.0, 1.0, 41728.57022781008], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1993800.0000, 
sim time next is 1994400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.28262016653897, 0.1261326481978182, 0.0, 1.0, 41726.50911319499], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5235516805449141, 0.5420442160659394, 0.0, 1.0, 0.19869766244378567], 
reward next is 0.8013, 
noisyNet noise sample is [array([-1.3511553], dtype=float32), -0.21779776]. 
=============================================
[2019-04-04 09:03:29,173] A3C_AGENT_WORKER-Thread-10 INFO:Local step 173000, global step 2768500: loss 5.1910
[2019-04-04 09:03:29,174] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 173000, global step 2768500: learning rate 0.0000
[2019-04-04 09:03:29,527] A3C_AGENT_WORKER-Thread-13 INFO:Local step 173000, global step 2768598: loss 5.2136
[2019-04-04 09:03:29,528] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 173000, global step 2768598: learning rate 0.0000
[2019-04-04 09:03:33,843] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.4015213e-10 3.7327510e-09 5.9786299e-25 1.9261240e-23 4.9492727e-20
 1.0000000e+00 1.1138355e-18], sum to 1.0000
[2019-04-04 09:03:33,845] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2818
[2019-04-04 09:03:33,860] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 83.0, 0.0, 0.0, 26.0, 24.0624125554591, 0.07261129822212341, 0.0, 1.0, 43563.55689626418], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2095200.0000, 
sim time next is 2095800.0000, 
raw observation next is [-6.700000000000001, 82.16666666666667, 0.0, 0.0, 26.0, 24.05790066767673, 0.06129834786961833, 0.0, 1.0, 43596.21433403719], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.8216666666666668, 0.0, 0.0, 0.6666666666666666, 0.5048250556397275, 0.5204327826232061, 0.0, 1.0, 0.20760102063827235], 
reward next is 0.7924, 
noisyNet noise sample is [array([0.30875194], dtype=float32), 1.546292]. 
=============================================
[2019-04-04 09:03:34,963] A3C_AGENT_WORKER-Thread-20 INFO:Local step 172500, global step 2770338: loss 0.2681
[2019-04-04 09:03:34,964] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 172500, global step 2770338: learning rate 0.0000
[2019-04-04 09:03:38,065] A3C_AGENT_WORKER-Thread-2 INFO:Local step 173500, global step 2771229: loss 3.3891
[2019-04-04 09:03:38,066] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 173500, global step 2771229: learning rate 0.0000
[2019-04-04 09:03:38,684] A3C_AGENT_WORKER-Thread-5 INFO:Local step 173000, global step 2771378: loss 5.1859
[2019-04-04 09:03:38,702] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 173000, global step 2771378: learning rate 0.0000
[2019-04-04 09:03:38,831] A3C_AGENT_WORKER-Thread-16 INFO:Local step 173500, global step 2771421: loss 3.4000
[2019-04-04 09:03:38,833] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 173500, global step 2771421: learning rate 0.0000
[2019-04-04 09:03:39,774] A3C_AGENT_WORKER-Thread-6 INFO:Local step 173500, global step 2771691: loss 3.4090
[2019-04-04 09:03:39,775] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 173500, global step 2771691: learning rate 0.0000
[2019-04-04 09:03:40,226] A3C_AGENT_WORKER-Thread-3 INFO:Local step 173500, global step 2771829: loss 3.3804
[2019-04-04 09:03:40,230] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 173500, global step 2771830: learning rate 0.0000
[2019-04-04 09:03:42,664] A3C_AGENT_WORKER-Thread-15 INFO:Local step 173500, global step 2772720: loss 3.4327
[2019-04-04 09:03:42,667] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 173500, global step 2772723: learning rate 0.0000
[2019-04-04 09:03:43,698] A3C_AGENT_WORKER-Thread-14 INFO:Local step 173000, global step 2773026: loss 5.1537
[2019-04-04 09:03:43,699] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 173000, global step 2773026: learning rate 0.0000
[2019-04-04 09:03:43,993] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.5691776e-11 1.0420226e-09 2.5153849e-27 2.0943059e-25 3.3731654e-21
 1.0000000e+00 3.0342946e-20], sum to 1.0000
[2019-04-04 09:03:43,994] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9562
[2019-04-04 09:03:44,038] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 75.66666666666667, 153.0, 0.0, 26.0, 25.71793717905372, 0.3553206074029818, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2031000.0000, 
sim time next is 2031600.0000, 
raw observation next is [-4.5, 76.33333333333334, 154.5, 0.0, 26.0, 25.73706864066055, 0.3185442115417977, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7633333333333334, 0.515, 0.0, 0.6666666666666666, 0.6447557200550458, 0.6061814038472658, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3907853], dtype=float32), 0.6428547]. 
=============================================
[2019-04-04 09:03:45,242] A3C_AGENT_WORKER-Thread-17 INFO:Local step 173000, global step 2773408: loss 5.1387
[2019-04-04 09:03:45,242] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 173000, global step 2773408: learning rate 0.0000
[2019-04-04 09:03:47,580] A3C_AGENT_WORKER-Thread-18 INFO:Local step 173500, global step 2773938: loss 3.5080
[2019-04-04 09:03:47,581] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 173500, global step 2773938: learning rate 0.0000
[2019-04-04 09:03:48,994] A3C_AGENT_WORKER-Thread-4 INFO:Local step 173500, global step 2774474: loss 3.4827
[2019-04-04 09:03:48,995] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 173500, global step 2774474: learning rate 0.0000
[2019-04-04 09:03:49,279] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6677150e-11 1.6342866e-09 3.5750030e-26 2.7408120e-24 3.6736542e-20
 1.0000000e+00 1.4817794e-19], sum to 1.0000
[2019-04-04 09:03:49,280] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7315
[2019-04-04 09:03:49,309] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 62.0, 52.0, 0.0, 26.0, 25.55532408247431, 0.3160022003522833, 1.0, 1.0, 26354.08982037136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1958400.0000, 
sim time next is 1959000.0000, 
raw observation next is [-2.983333333333333, 64.16666666666667, 44.66666666666666, 0.0, 26.0, 25.53946175851087, 0.3109184634532439, 1.0, 1.0, 30873.87871335168], 
processed observation next is [1.0, 0.6956521739130435, 0.37996306555863346, 0.6416666666666667, 0.14888888888888885, 0.0, 0.6666666666666666, 0.628288479875906, 0.6036394878177479, 1.0, 1.0, 0.14701847006357943], 
reward next is 0.8530, 
noisyNet noise sample is [array([-0.5740707], dtype=float32), 0.13498142]. 
=============================================
[2019-04-04 09:03:49,322] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[77.464325]
 [77.38464 ]
 [77.28469 ]
 [77.18849 ]
 [77.04112 ]], R is [[77.58972168]
 [77.6883316 ]
 [77.78270721]
 [77.8650589 ]
 [77.91359711]].
[2019-04-04 09:03:55,023] A3C_AGENT_WORKER-Thread-11 INFO:Local step 173500, global step 2776335: loss 3.5656
[2019-04-04 09:03:55,024] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 173500, global step 2776335: learning rate 0.0000
[2019-04-04 09:03:55,103] A3C_AGENT_WORKER-Thread-19 INFO:Local step 173500, global step 2776365: loss 3.6379
[2019-04-04 09:03:55,105] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 173500, global step 2776365: learning rate 0.0000
[2019-04-04 09:03:55,737] A3C_AGENT_WORKER-Thread-12 INFO:Local step 173500, global step 2776581: loss 3.6554
[2019-04-04 09:03:55,739] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 173500, global step 2776581: learning rate 0.0000
[2019-04-04 09:03:55,993] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4007852e-10 3.9022021e-09 7.7151420e-25 2.0101235e-23 5.1139454e-19
 1.0000000e+00 8.8207480e-19], sum to 1.0000
[2019-04-04 09:03:55,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2277
[2019-04-04 09:03:56,019] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.383333333333333, 82.66666666666667, 0.0, 0.0, 26.0, 24.15790721628644, 0.1202064784412935, 0.0, 1.0, 43989.10715412992], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2254200.0000, 
sim time next is 2254800.0000, 
raw observation next is [-7.466666666666667, 83.33333333333334, 0.0, 0.0, 26.0, 24.16857574253524, 0.1361153061887521, 0.0, 1.0, 44292.46315501451], 
processed observation next is [1.0, 0.08695652173913043, 0.25577100646352724, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.5140479785446033, 0.545371768729584, 0.0, 1.0, 0.21091649121435482], 
reward next is 0.7891, 
noisyNet noise sample is [array([-0.23720703], dtype=float32), -1.015758]. 
=============================================
[2019-04-04 09:03:56,089] A3C_AGENT_WORKER-Thread-10 INFO:Local step 173500, global step 2776722: loss 3.6507
[2019-04-04 09:03:56,091] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 173500, global step 2776722: learning rate 0.0000
[2019-04-04 09:03:56,608] A3C_AGENT_WORKER-Thread-20 INFO:Local step 173000, global step 2776930: loss 5.1479
[2019-04-04 09:03:56,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 173000, global step 2776930: learning rate 0.0000
[2019-04-04 09:03:56,744] A3C_AGENT_WORKER-Thread-13 INFO:Local step 173500, global step 2776979: loss 3.6354
[2019-04-04 09:03:56,746] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 173500, global step 2776979: learning rate 0.0000
[2019-04-04 09:04:00,655] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.4668938e-11 1.7119534e-09 6.2598855e-26 2.0476194e-24 3.0904486e-20
 1.0000000e+00 4.3226821e-20], sum to 1.0000
[2019-04-04 09:04:00,655] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7821
[2019-04-04 09:04:00,741] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.7, 49.5, 17.0, 0.0, 26.0, 25.19439754218729, 0.2937263098375626, 1.0, 1.0, 77725.69932268471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2308200.0000, 
sim time next is 2308800.0000, 
raw observation next is [-0.8, 50.0, 11.0, 0.0, 26.0, 24.50627878462837, 0.2626086404935054, 1.0, 1.0, 196968.1535768699], 
processed observation next is [1.0, 0.7391304347826086, 0.4404432132963989, 0.5, 0.03666666666666667, 0.0, 0.6666666666666666, 0.5421898987190309, 0.5875362134978351, 1.0, 1.0, 0.9379435884612852], 
reward next is 0.0621, 
noisyNet noise sample is [array([-0.11148948], dtype=float32), -0.297756]. 
=============================================
[2019-04-04 09:04:02,329] A3C_AGENT_WORKER-Thread-2 INFO:Local step 174000, global step 2778644: loss 1.6557
[2019-04-04 09:04:02,330] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 174000, global step 2778645: learning rate 0.0000
[2019-04-04 09:04:03,202] A3C_AGENT_WORKER-Thread-16 INFO:Local step 174000, global step 2778954: loss 1.6654
[2019-04-04 09:04:03,203] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 174000, global step 2778954: learning rate 0.0000
[2019-04-04 09:04:03,507] A3C_AGENT_WORKER-Thread-6 INFO:Local step 174000, global step 2779069: loss 1.6757
[2019-04-04 09:04:03,507] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 174000, global step 2779069: learning rate 0.0000
[2019-04-04 09:04:04,769] A3C_AGENT_WORKER-Thread-3 INFO:Local step 174000, global step 2779523: loss 1.6680
[2019-04-04 09:04:04,770] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 174000, global step 2779525: learning rate 0.0000
[2019-04-04 09:04:05,742] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.0012936e-09 3.1311224e-08 7.9186804e-24 1.4438239e-21 6.5062487e-18
 1.0000000e+00 4.0264846e-17], sum to 1.0000
[2019-04-04 09:04:05,753] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0277
[2019-04-04 09:04:05,802] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.466666666666667, 27.5, 0.0, 0.0, 26.0, 24.86582499911454, 0.2207888873563326, 0.0, 1.0, 47170.51238111177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2483400.0000, 
sim time next is 2484000.0000, 
raw observation next is [1.1, 28.0, 0.0, 0.0, 26.0, 24.90493339989402, 0.2225962914263923, 0.0, 1.0, 25416.2293456449], 
processed observation next is [0.0, 0.782608695652174, 0.49307479224376743, 0.28, 0.0, 0.0, 0.6666666666666666, 0.575411116657835, 0.5741987638087974, 0.0, 1.0, 0.12102966355069], 
reward next is 0.8790, 
noisyNet noise sample is [array([0.43257868], dtype=float32), 0.25197008]. 
=============================================
[2019-04-04 09:04:05,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[71.08923 ]
 [71.10063 ]
 [71.136154]
 [71.28237 ]
 [71.31926 ]], R is [[71.25437927]
 [71.31721497]
 [71.27210236]
 [71.12910461]
 [71.05665588]].
[2019-04-04 09:04:05,916] A3C_AGENT_WORKER-Thread-5 INFO:Local step 173500, global step 2779918: loss 3.6536
[2019-04-04 09:04:05,916] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 173500, global step 2779918: learning rate 0.0000
[2019-04-04 09:04:07,210] A3C_AGENT_WORKER-Thread-15 INFO:Local step 174000, global step 2780312: loss 1.6656
[2019-04-04 09:04:07,210] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 174000, global step 2780312: learning rate 0.0000
[2019-04-04 09:04:10,420] A3C_AGENT_WORKER-Thread-14 INFO:Local step 173500, global step 2781411: loss 3.8359
[2019-04-04 09:04:10,423] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 173500, global step 2781415: learning rate 0.0000
[2019-04-04 09:04:11,047] A3C_AGENT_WORKER-Thread-18 INFO:Local step 174000, global step 2781694: loss 1.6169
[2019-04-04 09:04:11,048] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 174000, global step 2781695: learning rate 0.0000
[2019-04-04 09:04:12,000] A3C_AGENT_WORKER-Thread-17 INFO:Local step 173500, global step 2782114: loss 3.8978
[2019-04-04 09:04:12,016] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 173500, global step 2782114: learning rate 0.0000
[2019-04-04 09:04:12,480] A3C_AGENT_WORKER-Thread-4 INFO:Local step 174000, global step 2782281: loss 1.6141
[2019-04-04 09:04:12,480] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 174000, global step 2782281: learning rate 0.0000
[2019-04-04 09:04:18,597] A3C_AGENT_WORKER-Thread-19 INFO:Local step 174000, global step 2784376: loss 1.5970
[2019-04-04 09:04:18,598] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 174000, global step 2784377: learning rate 0.0000
[2019-04-04 09:04:18,959] A3C_AGENT_WORKER-Thread-11 INFO:Local step 174000, global step 2784520: loss 1.6013
[2019-04-04 09:04:18,960] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 174000, global step 2784520: learning rate 0.0000
[2019-04-04 09:04:19,026] A3C_AGENT_WORKER-Thread-12 INFO:Local step 174000, global step 2784548: loss 1.6112
[2019-04-04 09:04:19,029] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 174000, global step 2784549: learning rate 0.0000
[2019-04-04 09:04:19,186] A3C_AGENT_WORKER-Thread-10 INFO:Local step 174000, global step 2784598: loss 1.5951
[2019-04-04 09:04:19,189] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 174000, global step 2784598: learning rate 0.0000
[2019-04-04 09:04:20,163] A3C_AGENT_WORKER-Thread-13 INFO:Local step 174000, global step 2784958: loss 1.5956
[2019-04-04 09:04:20,164] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 174000, global step 2784958: learning rate 0.0000
[2019-04-04 09:04:22,540] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9566753e-09 8.0602129e-09 3.4057949e-24 2.2136171e-23 1.3495056e-18
 1.0000000e+00 1.4969502e-17], sum to 1.0000
[2019-04-04 09:04:22,542] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5455
[2019-04-04 09:04:22,567] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 36.33333333333334, 0.0, 0.0, 26.0, 25.25568696401628, 0.2730744043571229, 0.0, 1.0, 40028.2375786293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2495400.0000, 
sim time next is 2496000.0000, 
raw observation next is [-1.2, 35.66666666666667, 0.0, 0.0, 26.0, 25.26894970255266, 0.2731017079695999, 0.0, 1.0, 40058.51177069263], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.3566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6057458085460551, 0.5910339026565333, 0.0, 1.0, 0.1907548179556792], 
reward next is 0.8092, 
noisyNet noise sample is [array([0.91817135], dtype=float32), 1.5344772]. 
=============================================
[2019-04-04 09:04:22,592] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.70201]
 [74.78528]
 [74.86304]
 [74.9879 ]
 [75.07033]], R is [[74.67928314]
 [74.74187469]
 [74.80394745]
 [74.86523438]
 [74.92579651]].
[2019-04-04 09:04:22,775] A3C_AGENT_WORKER-Thread-20 INFO:Local step 173500, global step 2785915: loss 4.0156
[2019-04-04 09:04:22,776] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 173500, global step 2785915: learning rate 0.0000
[2019-04-04 09:04:22,838] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.6849234e-11 1.2605925e-09 8.4289239e-28 1.5925043e-25 1.8444260e-21
 1.0000000e+00 3.7235127e-21], sum to 1.0000
[2019-04-04 09:04:22,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1997
[2019-04-04 09:04:22,877] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 68.0, 169.5, 80.0, 26.0, 25.74628130103325, 0.3495152671845472, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2286000.0000, 
sim time next is 2286600.0000, 
raw observation next is [-4.7, 66.33333333333334, 166.6666666666667, 90.0, 26.0, 25.74327375338383, 0.3502532203740817, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.332409972299169, 0.6633333333333334, 0.5555555555555557, 0.09944751381215469, 0.6666666666666666, 0.6452728127819857, 0.6167510734580272, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.95898616], dtype=float32), 0.9951964]. 
=============================================
[2019-04-04 09:04:24,358] A3C_AGENT_WORKER-Thread-2 INFO:Local step 174500, global step 2786573: loss 0.9602
[2019-04-04 09:04:24,359] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 174500, global step 2786573: learning rate 0.0000
[2019-04-04 09:04:24,656] A3C_AGENT_WORKER-Thread-16 INFO:Local step 174500, global step 2786695: loss 0.9653
[2019-04-04 09:04:24,659] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 174500, global step 2786696: learning rate 0.0000
[2019-04-04 09:04:25,296] A3C_AGENT_WORKER-Thread-6 INFO:Local step 174500, global step 2786928: loss 0.9669
[2019-04-04 09:04:25,298] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 174500, global step 2786929: learning rate 0.0000
[2019-04-04 09:04:26,082] A3C_AGENT_WORKER-Thread-3 INFO:Local step 174500, global step 2787283: loss 0.9927
[2019-04-04 09:04:26,083] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 174500, global step 2787283: learning rate 0.0000
[2019-04-04 09:04:26,390] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6165680e-10 7.8902760e-09 8.0059349e-26 3.6196595e-24 1.8497468e-20
 1.0000000e+00 2.0159637e-19], sum to 1.0000
[2019-04-04 09:04:26,392] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5975
[2019-04-04 09:04:26,410] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.1, 69.0, 0.0, 0.0, 26.0, 25.41424802191241, 0.4430248998357458, 0.0, 1.0, 42680.76285422606], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2671200.0000, 
sim time next is 2671800.0000, 
raw observation next is [-3.416666666666667, 69.0, 0.0, 0.0, 26.0, 25.39592877964042, 0.3896131967078166, 0.0, 1.0, 51332.42686488504], 
processed observation next is [1.0, 0.9565217391304348, 0.36795937211449675, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6163273983033685, 0.6298710655692722, 0.0, 1.0, 0.244440127928024], 
reward next is 0.7556, 
noisyNet noise sample is [array([1.0053699], dtype=float32), 0.7748146]. 
=============================================
[2019-04-04 09:04:28,852] A3C_AGENT_WORKER-Thread-5 INFO:Local step 174000, global step 2788075: loss 1.5835
[2019-04-04 09:04:28,865] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 174000, global step 2788075: learning rate 0.0000
[2019-04-04 09:04:30,540] A3C_AGENT_WORKER-Thread-15 INFO:Local step 174500, global step 2788410: loss 0.9953
[2019-04-04 09:04:30,551] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 174500, global step 2788410: learning rate 0.0000
[2019-04-04 09:04:37,491] A3C_AGENT_WORKER-Thread-18 INFO:Local step 174500, global step 2789686: loss 1.0085
[2019-04-04 09:04:37,494] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 174500, global step 2789686: learning rate 0.0000
[2019-04-04 09:04:37,565] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7381557e-10 2.6240852e-09 1.0301601e-27 1.6369055e-26 2.1003107e-21
 1.0000000e+00 2.1282254e-20], sum to 1.0000
[2019-04-04 09:04:37,565] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6008
[2019-04-04 09:04:37,583] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.166666666666667, 98.83333333333334, 0.0, 0.0, 26.0, 24.82343801853745, 0.2270150480086178, 0.0, 1.0, 55439.05687490878], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2873400.0000, 
sim time next is 2874000.0000, 
raw observation next is [1.333333333333333, 97.66666666666667, 0.0, 0.0, 26.0, 24.77493697744571, 0.2199079602462973, 0.0, 1.0, 55431.75990371163], 
processed observation next is [1.0, 0.2608695652173913, 0.4995383194829178, 0.9766666666666667, 0.0, 0.0, 0.6666666666666666, 0.564578081453809, 0.5733026534154324, 0.0, 1.0, 0.26396076144624586], 
reward next is 0.7360, 
noisyNet noise sample is [array([0.1377652], dtype=float32), -0.2879751]. 
=============================================
[2019-04-04 09:04:37,611] A3C_AGENT_WORKER-Thread-14 INFO:Local step 174000, global step 2789713: loss 1.5703
[2019-04-04 09:04:37,614] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 174000, global step 2789713: learning rate 0.0000
[2019-04-04 09:04:37,684] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[87.41105]
 [87.35114]
 [87.26132]
 [87.17271]
 [87.08848]], R is [[87.32741547]
 [87.1901474 ]
 [87.05437469]
 [86.91854858]
 [86.78305054]].
[2019-04-04 09:04:38,003] A3C_AGENT_WORKER-Thread-4 INFO:Local step 174500, global step 2789801: loss 1.0227
[2019-04-04 09:04:38,004] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 174500, global step 2789801: learning rate 0.0000
[2019-04-04 09:04:38,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4453704e-09 6.2288208e-08 1.0615338e-23 2.4124667e-22 4.4228669e-18
 9.9999988e-01 1.1163115e-17], sum to 1.0000
[2019-04-04 09:04:38,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4557
[2019-04-04 09:04:38,670] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 48.0, 0.0, 0.0, 26.0, 24.4006672738174, 0.1007959137186688, 0.0, 1.0, 43320.60136058913], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2422800.0000, 
sim time next is 2423400.0000, 
raw observation next is [-6.383333333333334, 48.83333333333333, 0.0, 0.0, 26.0, 24.34109444545035, 0.09033266471949557, 0.0, 1.0, 43385.70521556772], 
processed observation next is [0.0, 0.043478260869565216, 0.28578024007386893, 0.4883333333333333, 0.0, 0.0, 0.6666666666666666, 0.5284245371208623, 0.5301108882398319, 0.0, 1.0, 0.2065985962646082], 
reward next is 0.7934, 
noisyNet noise sample is [array([-2.5624986], dtype=float32), 0.8209265]. 
=============================================
[2019-04-04 09:04:41,464] A3C_AGENT_WORKER-Thread-17 INFO:Local step 174000, global step 2790505: loss 1.5727
[2019-04-04 09:04:41,465] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 174000, global step 2790505: learning rate 0.0000
[2019-04-04 09:04:46,959] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6957863e-12 2.6873681e-10 2.5024451e-27 3.4443343e-26 1.2393902e-21
 1.0000000e+00 4.3551540e-21], sum to 1.0000
[2019-04-04 09:04:46,959] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1001
[2019-04-04 09:04:47,155] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.333333333333334, 62.33333333333334, 112.8333333333333, 796.0, 26.0, 25.9437117050253, 0.4666115158403307, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2722800.0000, 
sim time next is 2723400.0000, 
raw observation next is [-7.0, 61.5, 113.0, 799.0, 26.0, 25.86483538246843, 0.3619502188077265, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2686980609418283, 0.615, 0.37666666666666665, 0.8828729281767956, 0.6666666666666666, 0.6554029485390359, 0.6206500729359088, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1060405], dtype=float32), -0.8879269]. 
=============================================
[2019-04-04 09:04:50,802] A3C_AGENT_WORKER-Thread-19 INFO:Local step 174500, global step 2792290: loss 1.0402
[2019-04-04 09:04:50,803] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 174500, global step 2792290: learning rate 0.0000
[2019-04-04 09:04:51,723] A3C_AGENT_WORKER-Thread-11 INFO:Local step 174500, global step 2792491: loss 1.0454
[2019-04-04 09:04:51,759] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 174500, global step 2792491: learning rate 0.0000
[2019-04-04 09:04:51,781] A3C_AGENT_WORKER-Thread-12 INFO:Local step 174500, global step 2792506: loss 1.0443
[2019-04-04 09:04:51,781] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 174500, global step 2792506: learning rate 0.0000
[2019-04-04 09:04:52,342] A3C_AGENT_WORKER-Thread-10 INFO:Local step 174500, global step 2792623: loss 1.0437
[2019-04-04 09:04:52,343] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 174500, global step 2792623: learning rate 0.0000
[2019-04-04 09:04:54,281] A3C_AGENT_WORKER-Thread-13 INFO:Local step 174500, global step 2792998: loss 1.0440
[2019-04-04 09:04:54,281] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 174500, global step 2792998: learning rate 0.0000
[2019-04-04 09:05:00,953] A3C_AGENT_WORKER-Thread-20 INFO:Local step 174000, global step 2794405: loss 1.6215
[2019-04-04 09:05:00,977] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 174000, global step 2794405: learning rate 0.0000
[2019-04-04 09:05:01,211] A3C_AGENT_WORKER-Thread-16 INFO:Local step 175000, global step 2794464: loss 0.0283
[2019-04-04 09:05:01,211] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 175000, global step 2794464: learning rate 0.0000
[2019-04-04 09:05:01,454] A3C_AGENT_WORKER-Thread-2 INFO:Local step 175000, global step 2794524: loss 0.0303
[2019-04-04 09:05:01,454] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 175000, global step 2794524: learning rate 0.0000
[2019-04-04 09:05:03,042] A3C_AGENT_WORKER-Thread-6 INFO:Local step 175000, global step 2794865: loss 0.0292
[2019-04-04 09:05:03,042] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 175000, global step 2794865: learning rate 0.0000
[2019-04-04 09:05:04,292] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.2715207e-10 7.5107671e-09 5.5894942e-25 7.2211368e-23 1.5616783e-18
 1.0000000e+00 2.9591164e-18], sum to 1.0000
[2019-04-04 09:05:04,292] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4051
[2019-04-04 09:05:04,355] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 72.33333333333334, 0.0, 0.0, 26.0, 23.73304636665446, -0.0154576877729091, 0.0, 1.0, 40257.19456582331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3046800.0000, 
sim time next is 3047400.0000, 
raw observation next is [-6.0, 73.5, 0.0, 0.0, 26.0, 23.70667593914112, -0.0200800353785007, 0.0, 1.0, 40309.24722992616], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.735, 0.0, 0.0, 0.6666666666666666, 0.4755563282617601, 0.49330665487383313, 0.0, 1.0, 0.1919487963329817], 
reward next is 0.8081, 
noisyNet noise sample is [array([1.3240939], dtype=float32), 0.42401797]. 
=============================================
[2019-04-04 09:05:04,577] A3C_AGENT_WORKER-Thread-3 INFO:Local step 175000, global step 2795185: loss 0.0316
[2019-04-04 09:05:04,610] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 175000, global step 2795185: learning rate 0.0000
[2019-04-04 09:05:08,049] A3C_AGENT_WORKER-Thread-5 INFO:Local step 174500, global step 2795881: loss 1.0347
[2019-04-04 09:05:08,050] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 174500, global step 2795881: learning rate 0.0000
[2019-04-04 09:05:11,022] A3C_AGENT_WORKER-Thread-15 INFO:Local step 175000, global step 2796468: loss 0.0326
[2019-04-04 09:05:11,025] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 175000, global step 2796468: learning rate 0.0000
[2019-04-04 09:05:12,676] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.4055883e-12 1.4489190e-10 5.5796801e-30 3.1479050e-28 3.8123328e-23
 1.0000000e+00 2.6186981e-22], sum to 1.0000
[2019-04-04 09:05:12,676] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8322
[2019-04-04 09:05:12,752] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 94.16666666666666, 49.33333333333334, 49.66666666666667, 26.0, 25.76378952438934, 0.4626420049928043, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2911800.0000, 
sim time next is 2912400.0000, 
raw observation next is [2.0, 93.0, 38.5, 47.5, 26.0, 25.87595714035552, 0.4713234805414965, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.93, 0.12833333333333333, 0.052486187845303865, 0.6666666666666666, 0.6563297616962934, 0.6571078268471655, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8519584], dtype=float32), 0.7285062]. 
=============================================
[2019-04-04 09:05:13,267] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6428009e-12 6.2347072e-11 3.5485282e-30 1.5082625e-28 9.3260020e-24
 1.0000000e+00 3.8966554e-23], sum to 1.0000
[2019-04-04 09:05:13,267] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4758
[2019-04-04 09:05:13,361] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 93.0, 38.5, 47.5, 26.0, 25.87600136980741, 0.4712169454921458, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2912400.0000, 
sim time next is 2913000.0000, 
raw observation next is [1.833333333333333, 93.0, 27.66666666666666, 45.33333333333333, 26.0, 25.92722421906914, 0.4725842151739492, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5133887349953832, 0.93, 0.0922222222222222, 0.050092081031307543, 0.6666666666666666, 0.6606020182557616, 0.6575280717246498, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5364045], dtype=float32), 0.13136947]. 
=============================================
[2019-04-04 09:05:13,420] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[91.55046 ]
 [91.83838 ]
 [91.983185]
 [92.03938 ]
 [91.99216 ]], R is [[91.29039001]
 [91.37748718]
 [91.4637146 ]
 [91.5490799 ]
 [91.6335907 ]].
[2019-04-04 09:05:14,879] A3C_AGENT_WORKER-Thread-4 INFO:Local step 175000, global step 2797384: loss 0.0319
[2019-04-04 09:05:14,879] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 175000, global step 2797384: learning rate 0.0000
[2019-04-04 09:05:14,936] A3C_AGENT_WORKER-Thread-18 INFO:Local step 175000, global step 2797393: loss 0.0287
[2019-04-04 09:05:14,937] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 175000, global step 2797393: learning rate 0.0000
[2019-04-04 09:05:16,981] A3C_AGENT_WORKER-Thread-14 INFO:Local step 174500, global step 2797868: loss 1.0031
[2019-04-04 09:05:16,981] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 174500, global step 2797868: learning rate 0.0000
[2019-04-04 09:05:17,473] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.0385268e-11 1.5473055e-09 5.0853475e-27 1.7418804e-24 6.0836465e-20
 1.0000000e+00 2.6309077e-19], sum to 1.0000
[2019-04-04 09:05:17,473] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7681
[2019-04-04 09:05:17,555] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.166666666666667, 54.83333333333334, 112.0, 809.0, 26.0, 25.06373884299798, 0.3319137486232231, 0.0, 1.0, 40476.4722793976], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3066600.0000, 
sim time next is 3067200.0000, 
raw observation next is [-3.0, 55.0, 112.5, 811.0, 26.0, 25.03642946898411, 0.3378709179309343, 0.0, 1.0, 45284.95275849717], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.55, 0.375, 0.8961325966850828, 0.6666666666666666, 0.5863691224153426, 0.6126236393103114, 0.0, 1.0, 0.21564263218331986], 
reward next is 0.7844, 
noisyNet noise sample is [array([-0.3074059], dtype=float32), 0.22233751]. 
=============================================
[2019-04-04 09:05:20,291] A3C_AGENT_WORKER-Thread-17 INFO:Local step 174500, global step 2798606: loss 1.0080
[2019-04-04 09:05:20,291] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 174500, global step 2798606: learning rate 0.0000
[2019-04-04 09:05:21,820] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.5185908e-12 6.8536704e-10 1.1732298e-28 2.0852795e-27 2.0700440e-22
 1.0000000e+00 3.8845044e-21], sum to 1.0000
[2019-04-04 09:05:21,820] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8305
[2019-04-04 09:05:21,901] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.30027865718768, 0.506841977966908, 0.0, 1.0, 42609.47676954906], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3213600.0000, 
sim time next is 3214200.0000, 
raw observation next is [-1.833333333333333, 100.0, 0.0, 0.0, 26.0, 25.30983595712523, 0.5023871371912098, 0.0, 1.0, 41276.86057133339], 
processed observation next is [1.0, 0.17391304347826086, 0.41181902123730385, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6091529964271025, 0.6674623790637365, 0.0, 1.0, 0.1965564789111114], 
reward next is 0.8034, 
noisyNet noise sample is [array([-1.2016809], dtype=float32), 0.4109255]. 
=============================================
[2019-04-04 09:05:25,013] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1943872e-09 2.9438979e-08 1.4699535e-23 3.1633469e-22 8.4862014e-18
 1.0000000e+00 3.5315633e-17], sum to 1.0000
[2019-04-04 09:05:25,081] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0415
[2019-04-04 09:05:25,156] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.04670453195752, 0.3205895278681672, 0.0, 1.0, 47609.16342866742], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3003000.0000, 
sim time next is 3003600.0000, 
raw observation next is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.01585084798057, 0.3201143538857718, 0.0, 1.0, 55982.0596810974], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6000000000000001, 0.0, 0.0, 0.6666666666666666, 0.5846542373317142, 0.6067047846285906, 0.0, 1.0, 0.2665812365766543], 
reward next is 0.7334, 
noisyNet noise sample is [array([0.83247226], dtype=float32), 0.42427197]. 
=============================================
[2019-04-04 09:05:26,442] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 09:05:26,443] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:05:26,443] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:05:26,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run29
[2019-04-04 09:05:26,494] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:05:26,496] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:05:26,499] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run29
[2019-04-04 09:05:26,549] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:05:26,554] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:05:26,556] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run29
[2019-04-04 09:05:50,327] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.25861123], dtype=float32), 0.28075585]
[2019-04-04 09:05:50,327] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-11.870367568, 49.23241559666666, 185.7707623, 681.3536427333333, 26.0, 25.6198542786791, 0.4381585382133604, 1.0, 1.0, 79123.78448596463]
[2019-04-04 09:05:50,327] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:05:50,328] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.2645529e-11 1.1280971e-09 4.1497639e-27 1.7291747e-25 1.6901605e-20
 1.0000000e+00 2.8367775e-20], sampled 0.9326799872418622
[2019-04-04 09:06:46,608] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.25861123], dtype=float32), 0.28075585]
[2019-04-04 09:06:46,609] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [12.01666666666667, 81.33333333333333, 0.0, 0.0, 26.0, 25.87653541473134, 0.7216224995724726, 0.0, 1.0, 0.0]
[2019-04-04 09:06:46,609] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 09:06:46,610] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.7584441e-12 1.9204581e-10 7.1810564e-31 1.0330047e-28 2.0052526e-23
 1.0000000e+00 2.2505199e-22], sampled 0.14853555485808778
[2019-04-04 09:07:29,840] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.25861123], dtype=float32), 0.28075585]
[2019-04-04 09:07:29,840] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.306227432166667, 74.12978397666667, 185.7141563, 429.3729530333334, 26.0, 25.20475268678234, 0.3301385054563706, 0.0, 1.0, 0.0]
[2019-04-04 09:07:29,841] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:07:29,842] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.7333241e-11 2.0955777e-09 1.8153542e-27 1.8732819e-25 6.9069904e-21
 1.0000000e+00 3.3919158e-20], sampled 0.48320502382831443
[2019-04-04 09:08:09,730] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.25861123], dtype=float32), 0.28075585]
[2019-04-04 09:08:09,730] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.0, 77.0, 0.0, 0.0, 26.0, 24.96886653784296, 0.3324934887340393, 0.0, 1.0, 43832.44963746542]
[2019-04-04 09:08:09,730] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 09:08:09,731] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.6930926e-10 7.8396623e-09 2.4920927e-25 1.4211724e-23 3.0454518e-19
 1.0000000e+00 1.3164705e-18], sampled 0.5137342639804944
[2019-04-04 09:08:11,502] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.25861123], dtype=float32), 0.28075585]
[2019-04-04 09:08:11,503] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.245762267, 79.03786768, 80.029673705, 831.4807169000001, 26.0, 26.14612774783399, 0.6183523606077387, 1.0, 1.0, 0.0]
[2019-04-04 09:08:11,503] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:08:11,503] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.9795669e-12 1.2974756e-10 2.8428076e-30 2.9498371e-28 5.3452495e-23
 1.0000000e+00 1.7720011e-22], sampled 0.6442709344021731
[2019-04-04 09:08:35,317] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 09:09:05,153] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6037 263423224.0345 1559.2181
[2019-04-04 09:09:10,115] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7881 275774498.6820 1233.0964
[2019-04-04 09:09:11,137] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2800000, evaluation results [2800000.0, 7241.603695074028, 263423224.0344576, 1559.2181108402822, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.788101514333, 275774498.68198794, 1233.096368464637]
[2019-04-04 09:09:11,532] A3C_AGENT_WORKER-Thread-19 INFO:Local step 175000, global step 2800182: loss 0.0219
[2019-04-04 09:09:11,533] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 175000, global step 2800182: learning rate 0.0000
[2019-04-04 09:09:12,503] A3C_AGENT_WORKER-Thread-12 INFO:Local step 175000, global step 2800570: loss 0.0226
[2019-04-04 09:09:12,504] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 175000, global step 2800570: learning rate 0.0000
[2019-04-04 09:09:12,769] A3C_AGENT_WORKER-Thread-10 INFO:Local step 175000, global step 2800685: loss 0.0234
[2019-04-04 09:09:12,771] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 175000, global step 2800685: learning rate 0.0000
[2019-04-04 09:09:12,936] A3C_AGENT_WORKER-Thread-13 INFO:Local step 175000, global step 2800754: loss 0.0198
[2019-04-04 09:09:12,942] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 175000, global step 2800758: learning rate 0.0000
[2019-04-04 09:09:13,127] A3C_AGENT_WORKER-Thread-11 INFO:Local step 175000, global step 2800837: loss 0.0204
[2019-04-04 09:09:13,133] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 175000, global step 2800838: learning rate 0.0000
[2019-04-04 09:09:16,571] A3C_AGENT_WORKER-Thread-16 INFO:Local step 175500, global step 2802039: loss 0.9212
[2019-04-04 09:09:16,590] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 175500, global step 2802039: learning rate 0.0000
[2019-04-04 09:09:17,335] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.0267329e-12 5.4583969e-11 1.0827649e-29 2.9068085e-28 7.0592430e-23
 1.0000000e+00 3.7989768e-23], sum to 1.0000
[2019-04-04 09:09:17,336] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1967
[2019-04-04 09:09:17,376] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 71.0, 93.0, 727.5, 26.0, 26.97715041906887, 0.8501511115998025, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3250800.0000, 
sim time next is 3251400.0000, 
raw observation next is [-2.166666666666667, 71.0, 90.33333333333333, 713.6666666666666, 26.0, 27.02194261313049, 0.7338405940265266, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4025854108956602, 0.71, 0.3011111111111111, 0.7885819521178636, 0.6666666666666666, 0.7518285510942077, 0.7446135313421754, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7788217], dtype=float32), 1.2399381]. 
=============================================
[2019-04-04 09:09:17,784] A3C_AGENT_WORKER-Thread-2 INFO:Local step 175500, global step 2802520: loss 0.9502
[2019-04-04 09:09:17,785] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 175500, global step 2802520: learning rate 0.0000
[2019-04-04 09:09:18,134] A3C_AGENT_WORKER-Thread-6 INFO:Local step 175500, global step 2802681: loss 0.9562
[2019-04-04 09:09:18,135] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 175500, global step 2802681: learning rate 0.0000
[2019-04-04 09:09:18,586] A3C_AGENT_WORKER-Thread-3 INFO:Local step 175500, global step 2802865: loss 0.9367
[2019-04-04 09:09:18,588] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 175500, global step 2802865: learning rate 0.0000
[2019-04-04 09:09:19,010] A3C_AGENT_WORKER-Thread-20 INFO:Local step 174500, global step 2803036: loss 0.9874
[2019-04-04 09:09:19,012] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 174500, global step 2803037: learning rate 0.0000
[2019-04-04 09:09:19,208] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.5298354e-10 2.8337754e-09 1.3553247e-26 1.1554882e-24 7.6559979e-20
 1.0000000e+00 8.8023236e-20], sum to 1.0000
[2019-04-04 09:09:19,211] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1396
[2019-04-04 09:09:19,271] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.385651199915, 0.4808072702988616, 0.0, 1.0, 65369.4049848126], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3277800.0000, 
sim time next is 3278400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.30896087478197, 0.4738176896603553, 0.0, 1.0, 61427.08579382037], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6090800728984975, 0.6579392298867851, 0.0, 1.0, 0.29250993235152556], 
reward next is 0.7075, 
noisyNet noise sample is [array([0.5510708], dtype=float32), 1.1076621]. 
=============================================
[2019-04-04 09:09:21,652] A3C_AGENT_WORKER-Thread-5 INFO:Local step 175000, global step 2804242: loss 0.0171
[2019-04-04 09:09:21,656] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 175000, global step 2804244: learning rate 0.0000
[2019-04-04 09:09:21,680] A3C_AGENT_WORKER-Thread-15 INFO:Local step 175500, global step 2804253: loss 0.8888
[2019-04-04 09:09:21,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 175500, global step 2804255: learning rate 0.0000
[2019-04-04 09:09:23,967] A3C_AGENT_WORKER-Thread-18 INFO:Local step 175500, global step 2805204: loss 0.8919
[2019-04-04 09:09:23,968] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 175500, global step 2805204: learning rate 0.0000
[2019-04-04 09:09:23,989] A3C_AGENT_WORKER-Thread-4 INFO:Local step 175500, global step 2805211: loss 0.9095
[2019-04-04 09:09:23,992] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 175500, global step 2805211: learning rate 0.0000
[2019-04-04 09:09:27,211] A3C_AGENT_WORKER-Thread-14 INFO:Local step 175000, global step 2806526: loss 0.0166
[2019-04-04 09:09:27,212] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 175000, global step 2806526: learning rate 0.0000
[2019-04-04 09:09:28,864] A3C_AGENT_WORKER-Thread-17 INFO:Local step 175000, global step 2807270: loss 0.0125
[2019-04-04 09:09:28,865] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 175000, global step 2807270: learning rate 0.0000
[2019-04-04 09:09:29,070] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.1130276e-12 1.6449248e-10 6.0373348e-29 5.3995268e-27 2.3828434e-21
 1.0000000e+00 9.0340815e-22], sum to 1.0000
[2019-04-04 09:09:29,071] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5968
[2019-04-04 09:09:29,092] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 67.0, 44.66666666666667, 382.3333333333334, 26.0, 26.4720084100133, 0.6191992723781538, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3430200.0000, 
sim time next is 3430800.0000, 
raw observation next is [2.0, 67.0, 36.5, 317.0, 26.0, 26.45304185157752, 0.4663993406253426, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.12166666666666667, 0.35027624309392263, 0.6666666666666666, 0.7044201542981267, 0.6554664468751142, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2945141], dtype=float32), 0.6440107]. 
=============================================
[2019-04-04 09:09:30,466] A3C_AGENT_WORKER-Thread-19 INFO:Local step 175500, global step 2807903: loss 0.8893
[2019-04-04 09:09:30,466] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 175500, global step 2807903: learning rate 0.0000
[2019-04-04 09:09:31,951] A3C_AGENT_WORKER-Thread-10 INFO:Local step 175500, global step 2808509: loss 0.9189
[2019-04-04 09:09:31,951] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 175500, global step 2808509: learning rate 0.0000
[2019-04-04 09:09:32,121] A3C_AGENT_WORKER-Thread-11 INFO:Local step 175500, global step 2808582: loss 0.9215
[2019-04-04 09:09:32,123] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 175500, global step 2808584: learning rate 0.0000
[2019-04-04 09:09:32,381] A3C_AGENT_WORKER-Thread-12 INFO:Local step 175500, global step 2808692: loss 0.9325
[2019-04-04 09:09:32,381] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 175500, global step 2808692: learning rate 0.0000
[2019-04-04 09:09:32,826] A3C_AGENT_WORKER-Thread-13 INFO:Local step 175500, global step 2808853: loss 0.9233
[2019-04-04 09:09:32,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 175500, global step 2808853: learning rate 0.0000
[2019-04-04 09:09:34,506] A3C_AGENT_WORKER-Thread-16 INFO:Local step 176000, global step 2809654: loss 0.0329
[2019-04-04 09:09:34,506] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 176000, global step 2809654: learning rate 0.0000
[2019-04-04 09:09:35,263] A3C_AGENT_WORKER-Thread-2 INFO:Local step 176000, global step 2809985: loss 0.0283
[2019-04-04 09:09:35,265] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 176000, global step 2809985: learning rate 0.0000
[2019-04-04 09:09:35,937] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2702064e-11 2.7948164e-09 3.5281384e-27 5.7052374e-26 8.6312436e-21
 1.0000000e+00 5.8591174e-20], sum to 1.0000
[2019-04-04 09:09:35,938] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7981
[2019-04-04 09:09:35,964] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 30.0, 74.83333333333334, 356.0000000000001, 26.0, 25.55889386973467, 0.4153065923730512, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3658800.0000, 
sim time next is 3659400.0000, 
raw observation next is [9.5, 29.0, 89.0, 403.0, 26.0, 25.57836968608911, 0.4251244586619747, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.7257617728531857, 0.29, 0.2966666666666667, 0.4453038674033149, 0.6666666666666666, 0.6315308071740926, 0.6417081528873249, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8889395], dtype=float32), -0.029099032]. 
=============================================
[2019-04-04 09:09:36,175] A3C_AGENT_WORKER-Thread-6 INFO:Local step 176000, global step 2810384: loss 0.0258
[2019-04-04 09:09:36,181] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 176000, global step 2810385: learning rate 0.0000
[2019-04-04 09:09:36,619] A3C_AGENT_WORKER-Thread-3 INFO:Local step 176000, global step 2810613: loss 0.0291
[2019-04-04 09:09:36,627] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 176000, global step 2810615: learning rate 0.0000
[2019-04-04 09:09:39,299] A3C_AGENT_WORKER-Thread-20 INFO:Local step 175000, global step 2811888: loss 0.0150
[2019-04-04 09:09:39,301] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 175000, global step 2811890: learning rate 0.0000
[2019-04-04 09:09:39,801] A3C_AGENT_WORKER-Thread-15 INFO:Local step 176000, global step 2812113: loss 0.0298
[2019-04-04 09:09:39,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 176000, global step 2812113: learning rate 0.0000
[2019-04-04 09:09:40,233] A3C_AGENT_WORKER-Thread-5 INFO:Local step 175500, global step 2812303: loss 0.9235
[2019-04-04 09:09:40,234] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 175500, global step 2812303: learning rate 0.0000
[2019-04-04 09:09:41,622] A3C_AGENT_WORKER-Thread-4 INFO:Local step 176000, global step 2812950: loss 0.0293
[2019-04-04 09:09:41,625] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 176000, global step 2812951: learning rate 0.0000
[2019-04-04 09:09:42,517] A3C_AGENT_WORKER-Thread-18 INFO:Local step 176000, global step 2813416: loss 0.0299
[2019-04-04 09:09:42,518] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 176000, global step 2813416: learning rate 0.0000
[2019-04-04 09:09:45,305] A3C_AGENT_WORKER-Thread-14 INFO:Local step 175500, global step 2814694: loss 0.9412
[2019-04-04 09:09:45,305] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 175500, global step 2814694: learning rate 0.0000
[2019-04-04 09:09:47,542] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.3244696e-12 3.6266790e-10 9.2106841e-29 2.8394305e-27 1.7971891e-22
 1.0000000e+00 2.4690564e-21], sum to 1.0000
[2019-04-04 09:09:47,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9213
[2019-04-04 09:09:47,552] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 48.0, 108.3333333333333, 796.0, 26.0, 26.50343369946635, 0.7001165219030187, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3852600.0000, 
sim time next is 3853200.0000, 
raw observation next is [2.0, 48.0, 107.1666666666667, 789.0, 26.0, 26.68250684877065, 0.7205668722640466, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.35722222222222233, 0.8718232044198895, 0.6666666666666666, 0.7235422373975542, 0.7401889574213488, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7694782], dtype=float32), 1.8511001]. 
=============================================
[2019-04-04 09:09:47,803] A3C_AGENT_WORKER-Thread-17 INFO:Local step 175500, global step 2815781: loss 0.9616
[2019-04-04 09:09:47,804] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 175500, global step 2815781: learning rate 0.0000
[2019-04-04 09:09:48,367] A3C_AGENT_WORKER-Thread-19 INFO:Local step 176000, global step 2816044: loss 0.0343
[2019-04-04 09:09:48,369] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 176000, global step 2816045: learning rate 0.0000
[2019-04-04 09:09:49,907] A3C_AGENT_WORKER-Thread-10 INFO:Local step 176000, global step 2816776: loss 0.0319
[2019-04-04 09:09:49,908] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 176000, global step 2816776: learning rate 0.0000
[2019-04-04 09:09:50,273] A3C_AGENT_WORKER-Thread-11 INFO:Local step 176000, global step 2816952: loss 0.0293
[2019-04-04 09:09:50,274] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 176000, global step 2816952: learning rate 0.0000
[2019-04-04 09:09:50,560] A3C_AGENT_WORKER-Thread-13 INFO:Local step 176000, global step 2817109: loss 0.0256
[2019-04-04 09:09:50,561] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 176000, global step 2817109: learning rate 0.0000
[2019-04-04 09:09:50,730] A3C_AGENT_WORKER-Thread-12 INFO:Local step 176000, global step 2817191: loss 0.0258
[2019-04-04 09:09:50,732] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 176000, global step 2817191: learning rate 0.0000
[2019-04-04 09:09:51,008] A3C_AGENT_WORKER-Thread-16 INFO:Local step 176500, global step 2817323: loss 0.1894
[2019-04-04 09:09:51,013] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 176500, global step 2817323: learning rate 0.0000
[2019-04-04 09:09:51,792] A3C_AGENT_WORKER-Thread-2 INFO:Local step 176500, global step 2817686: loss 0.1928
[2019-04-04 09:09:51,794] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 176500, global step 2817686: learning rate 0.0000
[2019-04-04 09:09:52,953] A3C_AGENT_WORKER-Thread-6 INFO:Local step 176500, global step 2818255: loss 0.2803
[2019-04-04 09:09:52,954] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 176500, global step 2818255: learning rate 0.0000
[2019-04-04 09:09:53,230] A3C_AGENT_WORKER-Thread-3 INFO:Local step 176500, global step 2818378: loss 0.1689
[2019-04-04 09:09:53,232] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 176500, global step 2818378: learning rate 0.0000
[2019-04-04 09:09:56,854] A3C_AGENT_WORKER-Thread-15 INFO:Local step 176500, global step 2819975: loss 0.1764
[2019-04-04 09:09:56,855] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 176500, global step 2819975: learning rate 0.0000
[2019-04-04 09:09:57,620] A3C_AGENT_WORKER-Thread-5 INFO:Local step 176000, global step 2820313: loss 0.0300
[2019-04-04 09:09:57,621] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 176000, global step 2820314: learning rate 0.0000
[2019-04-04 09:09:57,875] A3C_AGENT_WORKER-Thread-20 INFO:Local step 175500, global step 2820418: loss 0.9739
[2019-04-04 09:09:57,876] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 175500, global step 2820418: learning rate 0.0000
[2019-04-04 09:09:58,276] A3C_AGENT_WORKER-Thread-4 INFO:Local step 176500, global step 2820589: loss 0.1791
[2019-04-04 09:09:58,277] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 176500, global step 2820589: learning rate 0.0000
[2019-04-04 09:09:59,669] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.0253064e-10 1.6510622e-08 9.4221933e-24 1.8773122e-22 9.8553948e-19
 1.0000000e+00 5.6356316e-18], sum to 1.0000
[2019-04-04 09:09:59,669] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2873
[2019-04-04 09:09:59,738] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 37.5, 0.0, 0.0, 26.0, 24.91582747105262, 0.2795580706476145, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4087800.0000, 
sim time next is 4088400.0000, 
raw observation next is [-4.333333333333334, 36.33333333333333, 15.33333333333333, 78.16666666666664, 26.0, 25.26867050908424, 0.3119569093327456, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3425669436749769, 0.3633333333333333, 0.0511111111111111, 0.08637200736648248, 0.6666666666666666, 0.6057225424236868, 0.6039856364442485, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4525027], dtype=float32), -0.7520556]. 
=============================================
[2019-04-04 09:09:59,831] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1674174e-09 3.8885329e-08 2.9619534e-24 1.1636721e-23 3.0816694e-18
 1.0000000e+00 3.4649463e-18], sum to 1.0000
[2019-04-04 09:09:59,832] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9975
[2019-04-04 09:09:59,865] A3C_AGENT_WORKER-Thread-18 INFO:Local step 176500, global step 2821295: loss 0.1770
[2019-04-04 09:09:59,867] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 176500, global step 2821296: learning rate 0.0000
[2019-04-04 09:09:59,900] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.166666666666667, 35.16666666666667, 30.66666666666666, 156.3333333333333, 26.0, 25.35803340562426, 0.3253649882580697, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4089000.0000, 
sim time next is 4089600.0000, 
raw observation next is [-4.0, 34.0, 46.0, 234.5, 26.0, 25.38615515206241, 0.3344951207962724, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.34, 0.15333333333333332, 0.2591160220994475, 0.6666666666666666, 0.6155129293385343, 0.6114983735987575, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0012009], dtype=float32), 1.1397194]. 
=============================================
[2019-04-04 09:10:03,080] A3C_AGENT_WORKER-Thread-14 INFO:Local step 176000, global step 2822657: loss 0.0246
[2019-04-04 09:10:03,082] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 176000, global step 2822657: learning rate 0.0000
[2019-04-04 09:10:05,593] A3C_AGENT_WORKER-Thread-17 INFO:Local step 176000, global step 2823807: loss 0.0235
[2019-04-04 09:10:05,594] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 176000, global step 2823807: learning rate 0.0000
[2019-04-04 09:10:05,672] A3C_AGENT_WORKER-Thread-19 INFO:Local step 176500, global step 2823844: loss 0.2200
[2019-04-04 09:10:05,674] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 176500, global step 2823845: learning rate 0.0000
[2019-04-04 09:10:06,210] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7205511e-11 6.0039564e-09 1.4245415e-25 1.5236712e-24 6.0130454e-20
 1.0000000e+00 5.0910139e-20], sum to 1.0000
[2019-04-04 09:10:06,212] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1301
[2019-04-04 09:10:06,270] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.97928260234481, 0.309256991751385, 0.0, 1.0, 43818.65602690108], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3816600.0000, 
sim time next is 3817200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.96308828561965, 0.3005874384167624, 0.0, 1.0, 43882.08864293048], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.580257357134971, 0.6001958128055874, 0.0, 1.0, 0.20896232687109753], 
reward next is 0.7910, 
noisyNet noise sample is [array([-0.47249836], dtype=float32), 0.47124887]. 
=============================================
[2019-04-04 09:10:06,866] A3C_AGENT_WORKER-Thread-10 INFO:Local step 176500, global step 2824372: loss 0.2197
[2019-04-04 09:10:06,870] A3C_AGENT_WORKER-Thread-11 INFO:Local step 176500, global step 2824373: loss 0.2148
[2019-04-04 09:10:06,873] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 176500, global step 2824373: learning rate 0.0000
[2019-04-04 09:10:06,875] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 176500, global step 2824373: learning rate 0.0000
[2019-04-04 09:10:07,513] A3C_AGENT_WORKER-Thread-13 INFO:Local step 176500, global step 2824635: loss 0.2153
[2019-04-04 09:10:07,516] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 176500, global step 2824635: learning rate 0.0000
[2019-04-04 09:10:08,060] A3C_AGENT_WORKER-Thread-12 INFO:Local step 176500, global step 2824842: loss 0.3222
[2019-04-04 09:10:08,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 176500, global step 2824842: learning rate 0.0000
[2019-04-04 09:10:09,262] A3C_AGENT_WORKER-Thread-16 INFO:Local step 177000, global step 2825411: loss 0.0053
[2019-04-04 09:10:09,263] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 177000, global step 2825411: learning rate 0.0000
[2019-04-04 09:10:10,197] A3C_AGENT_WORKER-Thread-2 INFO:Local step 177000, global step 2825850: loss 0.0067
[2019-04-04 09:10:10,199] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 177000, global step 2825851: learning rate 0.0000
[2019-04-04 09:10:11,523] A3C_AGENT_WORKER-Thread-6 INFO:Local step 177000, global step 2826445: loss 0.0044
[2019-04-04 09:10:11,534] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 177000, global step 2826447: learning rate 0.0000
[2019-04-04 09:10:11,563] A3C_AGENT_WORKER-Thread-3 INFO:Local step 177000, global step 2826461: loss 0.0057
[2019-04-04 09:10:11,564] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 177000, global step 2826461: learning rate 0.0000
[2019-04-04 09:10:14,022] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5611113e-12 6.1923539e-10 3.2384031e-28 1.8156392e-26 1.2035836e-21
 1.0000000e+00 3.0084644e-21], sum to 1.0000
[2019-04-04 09:10:14,023] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5855
[2019-04-04 09:10:14,035] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.033333333333333, 73.66666666666667, 0.0, 0.0, 26.0, 25.41277053232727, 0.3583224584454687, 0.0, 1.0, 85519.45668198958], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4344000.0000, 
sim time next is 4344600.0000, 
raw observation next is [2.966666666666666, 74.33333333333333, 0.0, 0.0, 26.0, 25.40052083562912, 0.365163275971975, 0.0, 1.0, 67023.3162507906], 
processed observation next is [1.0, 0.2608695652173913, 0.5447830101569714, 0.7433333333333333, 0.0, 0.0, 0.6666666666666666, 0.61671006963576, 0.6217210919906583, 0.0, 1.0, 0.31915864881328854], 
reward next is 0.6808, 
noisyNet noise sample is [array([0.45434237], dtype=float32), 1.0854937]. 
=============================================
[2019-04-04 09:10:14,472] A3C_AGENT_WORKER-Thread-5 INFO:Local step 176500, global step 2827798: loss 0.2455
[2019-04-04 09:10:14,473] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 176500, global step 2827798: learning rate 0.0000
[2019-04-04 09:10:15,353] A3C_AGENT_WORKER-Thread-15 INFO:Local step 177000, global step 2828223: loss 0.0033
[2019-04-04 09:10:15,355] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 177000, global step 2828226: learning rate 0.0000
[2019-04-04 09:10:16,235] A3C_AGENT_WORKER-Thread-20 INFO:Local step 176000, global step 2828638: loss 0.0153
[2019-04-04 09:10:16,238] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 176000, global step 2828639: learning rate 0.0000
[2019-04-04 09:10:16,512] A3C_AGENT_WORKER-Thread-4 INFO:Local step 177000, global step 2828768: loss 0.0021
[2019-04-04 09:10:16,513] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 177000, global step 2828768: learning rate 0.0000
[2019-04-04 09:10:18,170] A3C_AGENT_WORKER-Thread-18 INFO:Local step 177000, global step 2829574: loss 0.0012
[2019-04-04 09:10:18,172] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 177000, global step 2829576: learning rate 0.0000
[2019-04-04 09:10:19,639] A3C_AGENT_WORKER-Thread-14 INFO:Local step 176500, global step 2830338: loss 0.3034
[2019-04-04 09:10:19,642] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 176500, global step 2830340: learning rate 0.0000
[2019-04-04 09:10:21,768] A3C_AGENT_WORKER-Thread-17 INFO:Local step 176500, global step 2831333: loss 0.2730
[2019-04-04 09:10:21,769] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 176500, global step 2831333: learning rate 0.0000
[2019-04-04 09:10:23,268] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8013464e-09 8.0331127e-08 8.2385225e-24 2.9041570e-22 1.5541849e-17
 9.9999988e-01 5.1873915e-18], sum to 1.0000
[2019-04-04 09:10:23,268] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4647
[2019-04-04 09:10:23,321] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 36.0, 0.0, 0.0, 26.0, 24.82899811774097, 0.2207144359255359, 0.0, 1.0, 40190.81714542563], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4080600.0000, 
sim time next is 4081200.0000, 
raw observation next is [-4.0, 36.66666666666666, 0.0, 0.0, 26.0, 24.81842458596958, 0.2128651160840288, 0.0, 1.0, 40176.57983001737], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.3666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5682020488307984, 0.5709550386946763, 0.0, 1.0, 0.1913170468096065], 
reward next is 0.8087, 
noisyNet noise sample is [array([-0.12397361], dtype=float32), -1.7986223]. 
=============================================
[2019-04-04 09:10:24,389] A3C_AGENT_WORKER-Thread-19 INFO:Local step 177000, global step 2832571: loss 0.0037
[2019-04-04 09:10:24,392] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 177000, global step 2832571: learning rate 0.0000
[2019-04-04 09:10:24,769] A3C_AGENT_WORKER-Thread-11 INFO:Local step 177000, global step 2832766: loss 0.0036
[2019-04-04 09:10:24,775] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 177000, global step 2832767: learning rate 0.0000
[2019-04-04 09:10:24,812] A3C_AGENT_WORKER-Thread-10 INFO:Local step 177000, global step 2832788: loss 0.0027
[2019-04-04 09:10:24,824] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 177000, global step 2832795: learning rate 0.0000
[2019-04-04 09:10:24,879] A3C_AGENT_WORKER-Thread-16 INFO:Local step 177500, global step 2832819: loss 6.1872
[2019-04-04 09:10:24,881] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 177500, global step 2832819: learning rate 0.0000
[2019-04-04 09:10:25,808] A3C_AGENT_WORKER-Thread-13 INFO:Local step 177000, global step 2833252: loss 0.0028
[2019-04-04 09:10:25,808] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 177000, global step 2833252: learning rate 0.0000
[2019-04-04 09:10:26,195] A3C_AGENT_WORKER-Thread-12 INFO:Local step 177000, global step 2833413: loss 0.0025
[2019-04-04 09:10:26,196] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 177000, global step 2833413: learning rate 0.0000
[2019-04-04 09:10:26,404] A3C_AGENT_WORKER-Thread-2 INFO:Local step 177500, global step 2833505: loss 6.1624
[2019-04-04 09:10:26,406] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 177500, global step 2833505: learning rate 0.0000
[2019-04-04 09:10:27,484] A3C_AGENT_WORKER-Thread-6 INFO:Local step 177500, global step 2834071: loss 7.7897
[2019-04-04 09:10:27,486] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 177500, global step 2834071: learning rate 0.0000
[2019-04-04 09:10:27,759] A3C_AGENT_WORKER-Thread-3 INFO:Local step 177500, global step 2834222: loss 6.1959
[2019-04-04 09:10:27,767] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 177500, global step 2834222: learning rate 0.0000
[2019-04-04 09:10:30,673] A3C_AGENT_WORKER-Thread-15 INFO:Local step 177500, global step 2835644: loss 6.1547
[2019-04-04 09:10:30,675] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 177500, global step 2835645: learning rate 0.0000
[2019-04-04 09:10:32,581] A3C_AGENT_WORKER-Thread-5 INFO:Local step 177000, global step 2836593: loss 0.0025
[2019-04-04 09:10:32,607] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 177000, global step 2836594: learning rate 0.0000
[2019-04-04 09:10:32,614] A3C_AGENT_WORKER-Thread-4 INFO:Local step 177500, global step 2836605: loss 6.2168
[2019-04-04 09:10:32,615] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 177500, global step 2836605: learning rate 0.0000
[2019-04-04 09:10:32,852] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.4746266e-11 1.7151695e-09 1.3336552e-27 1.7487170e-25 1.1896574e-20
 1.0000000e+00 3.6366214e-20], sum to 1.0000
[2019-04-04 09:10:32,858] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8996
[2019-04-04 09:10:32,906] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.4772184056705, 0.507690522076037, 0.0, 1.0, 45586.71376037322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4667400.0000, 
sim time next is 4668000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.48425309024827, 0.5123761932454156, 0.0, 1.0, 33612.84438551131], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6236877575206892, 0.6707920644151386, 0.0, 1.0, 0.16006116374053003], 
reward next is 0.8399, 
noisyNet noise sample is [array([-0.49468875], dtype=float32), 1.8031081]. 
=============================================
[2019-04-04 09:10:32,924] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.11491 ]
 [83.13412 ]
 [82.953125]
 [82.73862 ]
 [82.78707 ]], R is [[83.14917755]
 [83.10060883]
 [82.96891022]
 [82.84413147]
 [82.81007385]].
[2019-04-04 09:10:32,978] A3C_AGENT_WORKER-Thread-20 INFO:Local step 176500, global step 2836800: loss 0.2796
[2019-04-04 09:10:32,982] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 176500, global step 2836802: learning rate 0.0000
[2019-04-04 09:10:34,331] A3C_AGENT_WORKER-Thread-18 INFO:Local step 177500, global step 2837499: loss 6.1524
[2019-04-04 09:10:34,333] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 177500, global step 2837500: learning rate 0.0000
[2019-04-04 09:10:37,735] A3C_AGENT_WORKER-Thread-14 INFO:Local step 177000, global step 2839063: loss 0.0048
[2019-04-04 09:10:37,735] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 177000, global step 2839063: learning rate 0.0000
[2019-04-04 09:10:39,591] A3C_AGENT_WORKER-Thread-17 INFO:Local step 177000, global step 2839874: loss 0.0035
[2019-04-04 09:10:39,597] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 177000, global step 2839875: learning rate 0.0000
[2019-04-04 09:10:39,909] A3C_AGENT_WORKER-Thread-19 INFO:Local step 177500, global step 2840037: loss 6.0716
[2019-04-04 09:10:39,910] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 177500, global step 2840037: learning rate 0.0000
[2019-04-04 09:10:40,386] A3C_AGENT_WORKER-Thread-11 INFO:Local step 177500, global step 2840265: loss 6.0460
[2019-04-04 09:10:40,393] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 177500, global step 2840266: learning rate 0.0000
[2019-04-04 09:10:41,067] A3C_AGENT_WORKER-Thread-10 INFO:Local step 177500, global step 2840551: loss 5.9940
[2019-04-04 09:10:41,069] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 177500, global step 2840551: learning rate 0.0000
[2019-04-04 09:10:41,478] A3C_AGENT_WORKER-Thread-13 INFO:Local step 177500, global step 2840735: loss 6.1066
[2019-04-04 09:10:41,480] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 177500, global step 2840735: learning rate 0.0000
[2019-04-04 09:10:41,708] A3C_AGENT_WORKER-Thread-12 INFO:Local step 177500, global step 2840859: loss 7.5371
[2019-04-04 09:10:41,709] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 177500, global step 2840859: learning rate 0.0000
[2019-04-04 09:10:42,040] A3C_AGENT_WORKER-Thread-16 INFO:Local step 178000, global step 2841032: loss 0.0078
[2019-04-04 09:10:42,041] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 178000, global step 2841032: learning rate 0.0000
[2019-04-04 09:10:43,782] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.6477093e-10 4.3685837e-09 2.1745210e-25 7.8750903e-24 1.7146243e-19
 1.0000000e+00 9.9580355e-19], sum to 1.0000
[2019-04-04 09:10:43,783] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5026
[2019-04-04 09:10:43,803] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8333333333333334, 47.66666666666667, 0.0, 0.0, 26.0, 25.42949964052153, 0.4388856595778535, 0.0, 1.0, 18764.58776650493], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4824600.0000, 
sim time next is 4825200.0000, 
raw observation next is [0.6666666666666667, 48.33333333333334, 0.0, 0.0, 26.0, 25.50561980709081, 0.4389407700509442, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4810710987996307, 0.48333333333333345, 0.0, 0.0, 0.6666666666666666, 0.6254683172575675, 0.6463135900169814, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22117773], dtype=float32), -0.9207518]. 
=============================================
[2019-04-04 09:10:43,911] A3C_AGENT_WORKER-Thread-2 INFO:Local step 178000, global step 2841901: loss 0.0064
[2019-04-04 09:10:43,912] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 178000, global step 2841901: learning rate 0.0000
[2019-04-04 09:10:44,145] A3C_AGENT_WORKER-Thread-6 INFO:Local step 178000, global step 2842000: loss 0.0080
[2019-04-04 09:10:44,162] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 178000, global step 2842001: learning rate 0.0000
[2019-04-04 09:10:44,940] A3C_AGENT_WORKER-Thread-3 INFO:Local step 178000, global step 2842355: loss 0.0079
[2019-04-04 09:10:44,944] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 178000, global step 2842356: learning rate 0.0000
[2019-04-04 09:10:47,695] A3C_AGENT_WORKER-Thread-15 INFO:Local step 178000, global step 2843701: loss 0.0068
[2019-04-04 09:10:47,697] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 178000, global step 2843702: learning rate 0.0000
[2019-04-04 09:10:47,720] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.5740335e-11 1.2241128e-09 4.7190595e-29 9.9291287e-27 2.2825187e-22
 1.0000000e+00 1.6843797e-21], sum to 1.0000
[2019-04-04 09:10:47,720] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5490
[2019-04-04 09:10:47,734] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.8, 65.0, 0.0, 0.0, 26.0, 25.91182146870536, 0.6263789036828692, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4410000.0000, 
sim time next is 4410600.0000, 
raw observation next is [6.683333333333334, 65.16666666666667, 0.0, 0.0, 26.0, 25.91900605327369, 0.6125410570021578, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6477377654662975, 0.6516666666666667, 0.0, 0.0, 0.6666666666666666, 0.6599171711061409, 0.7041803523340526, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1561612], dtype=float32), -2.0766363]. 
=============================================
[2019-04-04 09:10:49,212] A3C_AGENT_WORKER-Thread-5 INFO:Local step 177500, global step 2844405: loss 6.1359
[2019-04-04 09:10:49,212] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 177500, global step 2844405: learning rate 0.0000
[2019-04-04 09:10:50,076] A3C_AGENT_WORKER-Thread-4 INFO:Local step 178000, global step 2844780: loss 0.0097
[2019-04-04 09:10:50,077] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 178000, global step 2844781: learning rate 0.0000
[2019-04-04 09:10:51,195] A3C_AGENT_WORKER-Thread-20 INFO:Local step 177000, global step 2845275: loss 0.0039
[2019-04-04 09:10:51,195] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 177000, global step 2845275: learning rate 0.0000
[2019-04-04 09:10:52,067] A3C_AGENT_WORKER-Thread-18 INFO:Local step 178000, global step 2845672: loss 0.0096
[2019-04-04 09:10:52,069] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 178000, global step 2845675: learning rate 0.0000
[2019-04-04 09:10:54,053] A3C_AGENT_WORKER-Thread-14 INFO:Local step 177500, global step 2846596: loss 6.1704
[2019-04-04 09:10:54,056] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 177500, global step 2846597: learning rate 0.0000
[2019-04-04 09:10:55,521] A3C_AGENT_WORKER-Thread-17 INFO:Local step 177500, global step 2847256: loss 6.1340
[2019-04-04 09:10:55,523] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 177500, global step 2847258: learning rate 0.0000
[2019-04-04 09:10:57,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:10:57,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:10:57,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run22
[2019-04-04 09:10:57,394] A3C_AGENT_WORKER-Thread-19 INFO:Local step 178000, global step 2848161: loss 0.0075
[2019-04-04 09:10:57,396] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 178000, global step 2848161: learning rate 0.0000
[2019-04-04 09:10:57,749] A3C_AGENT_WORKER-Thread-11 INFO:Local step 178000, global step 2848340: loss 0.0090
[2019-04-04 09:10:57,751] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 178000, global step 2848340: learning rate 0.0000
[2019-04-04 09:10:58,391] A3C_AGENT_WORKER-Thread-10 INFO:Local step 178000, global step 2848638: loss 0.0104
[2019-04-04 09:10:58,413] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 178000, global step 2848639: learning rate 0.0000
[2019-04-04 09:10:58,832] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:10:58,832] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:10:58,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run22
[2019-04-04 09:10:58,875] A3C_AGENT_WORKER-Thread-13 INFO:Local step 178000, global step 2848826: loss 0.0090
[2019-04-04 09:10:58,885] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 178000, global step 2848832: learning rate 0.0000
[2019-04-04 09:10:58,992] A3C_AGENT_WORKER-Thread-12 INFO:Local step 178000, global step 2848870: loss 0.0095
[2019-04-04 09:10:58,994] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 178000, global step 2848871: learning rate 0.0000
[2019-04-04 09:10:59,267] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:10:59,267] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:10:59,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run22
[2019-04-04 09:10:59,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:10:59,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:10:59,367] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run22
[2019-04-04 09:11:02,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:11:02,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:11:03,000] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run22
[2019-04-04 09:11:06,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:11:06,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:11:06,156] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run22
[2019-04-04 09:11:07,414] A3C_AGENT_WORKER-Thread-5 INFO:Local step 178000, global step 2851512: loss 0.0116
[2019-04-04 09:11:07,415] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 178000, global step 2851512: learning rate 0.0000
[2019-04-04 09:11:07,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:11:07,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:11:07,860] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run22
[2019-04-04 09:11:08,406] A3C_AGENT_WORKER-Thread-20 INFO:Local step 177500, global step 2851806: loss 6.4945
[2019-04-04 09:11:08,406] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 177500, global step 2851806: learning rate 0.0000
[2019-04-04 09:11:10,434] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5461415e-11 5.7392082e-09 6.5966629e-27 2.1140274e-25 1.1648508e-20
 1.0000000e+00 4.1070374e-20], sum to 1.0000
[2019-04-04 09:11:10,434] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5734
[2019-04-04 09:11:10,460] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.2, 46.0, 281.0, 390.0, 26.0, 25.11479792016505, 0.3603881890241822, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4883400.0000, 
sim time next is 4884000.0000, 
raw observation next is [1.266666666666667, 45.66666666666667, 279.5, 389.6666666666666, 26.0, 25.09601903386104, 0.3545961203162108, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4976915974145891, 0.4566666666666667, 0.9316666666666666, 0.4305709023941067, 0.6666666666666666, 0.59133491948842, 0.6181987067720702, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42911246], dtype=float32), 0.18155049]. 
=============================================
[2019-04-04 09:11:10,512] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.22068 ]
 [82.36533 ]
 [82.51469 ]
 [82.69011 ]
 [82.830055]], R is [[82.21612549]
 [82.39396667]
 [82.57003021]
 [82.74433136]
 [82.91688538]].
[2019-04-04 09:11:11,743] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.9319320e-12 2.2542541e-10 4.9325317e-29 2.2488495e-27 1.0112714e-21
 1.0000000e+00 2.0773702e-21], sum to 1.0000
[2019-04-04 09:11:11,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9005
[2019-04-04 09:11:11,784] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 26.0, 28.455429099856, 1.075028519524866, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5077800.0000, 
sim time next is 5078400.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 26.0, 28.28859907957054, 1.050920043744063, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.0, 0.0, 0.6666666666666666, 0.8573832566308784, 0.850306681248021, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.52451193], dtype=float32), -0.23321015]. 
=============================================
[2019-04-04 09:11:12,695] A3C_AGENT_WORKER-Thread-14 INFO:Local step 178000, global step 2853072: loss 0.0081
[2019-04-04 09:11:12,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 178000, global step 2853072: learning rate 0.0000
[2019-04-04 09:11:13,932] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:11:13,932] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:11:13,946] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run22
[2019-04-04 09:11:13,984] A3C_AGENT_WORKER-Thread-17 INFO:Local step 178000, global step 2853647: loss 0.0101
[2019-04-04 09:11:13,987] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 178000, global step 2853647: learning rate 0.0000
[2019-04-04 09:11:14,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0565240e-09 5.9227911e-09 4.0830950e-25 9.6315523e-24 2.6064934e-19
 1.0000000e+00 1.5286886e-18], sum to 1.0000
[2019-04-04 09:11:14,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3595
[2019-04-04 09:11:14,232] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.566666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 23.56137770127332, -0.03120344680623312, 0.0, 1.0, 44362.30475851987], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 105600.0000, 
sim time next is 106200.0000, 
raw observation next is [-5.85, 74.5, 0.0, 0.0, 26.0, 23.48068237372269, -0.03973754094427333, 0.0, 1.0, 44523.55260481204], 
processed observation next is [1.0, 0.21739130434782608, 0.30055401662049863, 0.745, 0.0, 0.0, 0.6666666666666666, 0.4567235311435575, 0.48675415301857555, 0.0, 1.0, 0.21201691716577162], 
reward next is 0.7880, 
noisyNet noise sample is [array([1.0154029], dtype=float32), -0.016691985]. 
=============================================
[2019-04-04 09:11:14,593] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:11:14,593] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:11:14,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run22
[2019-04-04 09:11:14,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:11:14,623] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:11:14,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run22
[2019-04-04 09:11:15,591] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:11:15,591] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:11:15,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run22
[2019-04-04 09:11:16,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:11:16,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:11:16,184] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run22
[2019-04-04 09:11:23,899] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2483686e-12 7.3686640e-11 9.8075547e-31 1.6077926e-28 6.5830007e-23
 1.0000000e+00 5.2305176e-23], sum to 1.0000
[2019-04-04 09:11:23,900] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1982
[2019-04-04 09:11:23,969] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.0, 26.0, 100.5, 796.5, 26.0, 27.76036828963077, 0.7525818316127547, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4978800.0000, 
sim time next is 4979400.0000, 
raw observation next is [8.166666666666668, 25.83333333333334, 97.66666666666667, 789.0, 26.0, 27.07099334339733, 0.8037840228149921, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6888273314866113, 0.2583333333333334, 0.3255555555555556, 0.8718232044198895, 0.6666666666666666, 0.7559161119497775, 0.7679280076049974, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5318346], dtype=float32), -1.2758992]. 
=============================================
[2019-04-04 09:11:25,062] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:11:25,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:11:25,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run22
[2019-04-04 09:11:27,239] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1933542e-10 6.8882816e-10 1.4761928e-28 1.8562443e-26 6.7453691e-21
 1.0000000e+00 1.6709540e-20], sum to 1.0000
[2019-04-04 09:11:27,240] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4758
[2019-04-04 09:11:27,286] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.05, 84.0, 18.0, 0.0, 26.0, 24.50422910401001, 0.1860511567913048, 0.0, 1.0, 58935.20283097521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 59400.0000, 
sim time next is 60000.0000, 
raw observation next is [5.866666666666667, 84.66666666666666, 15.0, 0.0, 26.0, 24.53955699601114, 0.1896098215059848, 0.0, 1.0, 34419.24430637214], 
processed observation next is [0.0, 0.6956521739130435, 0.6251154201292707, 0.8466666666666666, 0.05, 0.0, 0.6666666666666666, 0.5449630830009283, 0.5632032738353282, 0.0, 1.0, 0.16390116336367683], 
reward next is 0.8361, 
noisyNet noise sample is [array([1.2096486], dtype=float32), 0.9985742]. 
=============================================
[2019-04-04 09:11:27,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[86.36312 ]
 [86.430855]
 [86.53963 ]
 [86.78757 ]
 [87.06323 ]], R is [[86.17442322]
 [86.03203583]
 [85.930336  ]
 [85.93476105]
 [85.98619843]].
[2019-04-04 09:11:29,162] A3C_AGENT_WORKER-Thread-20 INFO:Local step 178000, global step 2857304: loss 0.0085
[2019-04-04 09:11:29,163] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 178000, global step 2857304: learning rate 0.0000
[2019-04-04 09:11:31,038] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:11:31,038] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:11:31,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run22
[2019-04-04 09:11:32,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:11:32,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:11:32,736] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run22
[2019-04-04 09:11:45,431] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0557109e-10 1.8389293e-09 6.1330737e-26 3.8363292e-24 4.6517532e-20
 1.0000000e+00 1.8639141e-19], sum to 1.0000
[2019-04-04 09:11:45,431] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9139
[2019-04-04 09:11:45,487] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 64.0, 15.0, 0.0, 26.0, 25.33306431882052, 0.2775880722185964, 1.0, 1.0, 32756.95021756213], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 232800.0000, 
sim time next is 233400.0000, 
raw observation next is [-3.4, 64.5, 12.0, 0.0, 26.0, 25.39651209848178, 0.1835098400587817, 1.0, 1.0, 30583.87510876748], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.645, 0.04, 0.0, 0.6666666666666666, 0.6163760082068149, 0.5611699466862606, 1.0, 1.0, 0.1456375005179404], 
reward next is 0.8544, 
noisyNet noise sample is [array([0.52409744], dtype=float32), -0.15353587]. 
=============================================
[2019-04-04 09:11:47,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:11:47,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:11:47,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run22
[2019-04-04 09:11:55,335] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3742869e-10 3.4990455e-09 9.7694980e-25 1.5017845e-23 3.9398447e-19
 1.0000000e+00 2.4343351e-18], sum to 1.0000
[2019-04-04 09:11:55,335] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7404
[2019-04-04 09:11:55,392] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.46666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.25079576650921, 0.335741770448431, 1.0, 1.0, 56924.48000856584], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 328800.0000, 
sim time next is 329400.0000, 
raw observation next is [-12.55, 66.5, 0.0, 0.0, 26.0, 25.21809074262417, 0.3278303462845727, 0.0, 1.0, 69829.98205369519], 
processed observation next is [1.0, 0.8260869565217391, 0.11495844875346259, 0.665, 0.0, 0.0, 0.6666666666666666, 0.6015075618853475, 0.6092767820948576, 0.0, 1.0, 0.3325237240652152], 
reward next is 0.6675, 
noisyNet noise sample is [array([1.2680061], dtype=float32), -0.9084792]. 
=============================================
[2019-04-04 09:11:59,025] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.8235363e-11 7.1395601e-10 2.6950532e-28 5.3297267e-26 4.0920379e-21
 1.0000000e+00 9.5311699e-20], sum to 1.0000
[2019-04-04 09:11:59,026] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7960
[2019-04-04 09:11:59,080] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.416666666666667, 82.66666666666667, 28.66666666666666, 0.0, 26.0, 24.54627051744927, 0.1787405372246849, 0.0, 1.0, 28616.51865161897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 58200.0000, 
sim time next is 58800.0000, 
raw observation next is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 26.0, 24.5220054204006, 0.1773502594089305, 0.0, 1.0, 50689.92145719513], 
processed observation next is [0.0, 0.6956521739130435, 0.6352723915050786, 0.8333333333333335, 0.07777777777777777, 0.0, 0.6666666666666666, 0.54350045170005, 0.5591167531363102, 0.0, 1.0, 0.24138057836759585], 
reward next is 0.7586, 
noisyNet noise sample is [array([-0.02273877], dtype=float32), -1.0997019]. 
=============================================
[2019-04-04 09:12:05,783] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.80785803e-10 1.97417060e-09 1.16557497e-28 1.14769817e-26
 2.91650283e-22 1.00000000e+00 1.32624776e-20], sum to 1.0000
[2019-04-04 09:12:05,783] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3014
[2019-04-04 09:12:05,805] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.616666666666667, 85.33333333333334, 0.0, 0.0, 26.0, 24.76743881023197, 0.221296153010156, 0.0, 1.0, 39866.15943425304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 529800.0000, 
sim time next is 530400.0000, 
raw observation next is [3.433333333333334, 84.66666666666667, 0.0, 0.0, 26.0, 24.74891482709478, 0.2180113935537852, 0.0, 1.0, 39917.2265635859], 
processed observation next is [0.0, 0.13043478260869565, 0.5577100646352725, 0.8466666666666667, 0.0, 0.0, 0.6666666666666666, 0.562409568924565, 0.5726704645179285, 0.0, 1.0, 0.19008203125517095], 
reward next is 0.8099, 
noisyNet noise sample is [array([1.4033778], dtype=float32), -0.15810807]. 
=============================================
[2019-04-04 09:12:07,690] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9377822e-09 9.6033794e-08 1.4103130e-21 1.7288690e-20 6.4500070e-17
 9.9999988e-01 2.2679786e-16], sum to 1.0000
[2019-04-04 09:12:07,691] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2133
[2019-04-04 09:12:07,709] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.0, 51.0, 0.0, 0.0, 26.0, 23.02708203501658, -0.210894639354243, 0.0, 1.0, 46193.30568350979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 445200.0000, 
sim time next is 445800.0000, 
raw observation next is [-11.1, 51.5, 0.0, 0.0, 26.0, 22.97454222382104, -0.224726834044513, 0.0, 1.0, 46294.59259444171], 
processed observation next is [1.0, 0.13043478260869565, 0.1551246537396122, 0.515, 0.0, 0.0, 0.6666666666666666, 0.41454518531841994, 0.42509105531849567, 0.0, 1.0, 0.2204504409259129], 
reward next is 0.7795, 
noisyNet noise sample is [array([-0.2390336], dtype=float32), -0.7609112]. 
=============================================
[2019-04-04 09:12:23,954] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.7805408e-09 4.4616169e-09 1.5766381e-24 1.2918719e-22 4.5754131e-18
 1.0000000e+00 1.1276536e-17], sum to 1.0000
[2019-04-04 09:12:23,954] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8553
[2019-04-04 09:12:23,968] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 26.0, 24.55979689713424, 0.2067274320862237, 0.0, 1.0, 42624.62173195268], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 774000.0000, 
sim time next is 774600.0000, 
raw observation next is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 26.0, 24.52974427632785, 0.1992595160311072, 0.0, 1.0, 42464.16096084129], 
processed observation next is [1.0, 1.0, 0.2742382271468144, 0.6766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5441453563606542, 0.5664198386770357, 0.0, 1.0, 0.20221029028972043], 
reward next is 0.7978, 
noisyNet noise sample is [array([-0.15210687], dtype=float32), -0.17337812]. 
=============================================
[2019-04-04 09:12:25,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.8366579e-10 2.3526185e-09 1.6346973e-25 1.4042501e-24 3.1358485e-20
 1.0000000e+00 7.2990846e-19], sum to 1.0000
[2019-04-04 09:12:25,855] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2539
[2019-04-04 09:12:25,885] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.81203111527758, 0.2606556366044068, 0.0, 1.0, 41620.79765729181], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 856800.0000, 
sim time next is 857400.0000, 
raw observation next is [-3.3, 82.33333333333334, 0.0, 0.0, 26.0, 24.83370966923195, 0.2567677284626809, 0.0, 1.0, 41560.48373720986], 
processed observation next is [1.0, 0.9565217391304348, 0.37119113573407203, 0.8233333333333335, 0.0, 0.0, 0.6666666666666666, 0.5694758057693292, 0.5855892428208936, 0.0, 1.0, 0.19790706541528505], 
reward next is 0.8021, 
noisyNet noise sample is [array([-0.48460904], dtype=float32), -0.5259268]. 
=============================================
[2019-04-04 09:12:27,640] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7684413e-10 2.8286617e-09 5.4118403e-26 1.3260225e-24 1.6458965e-19
 1.0000000e+00 8.0933548e-19], sum to 1.0000
[2019-04-04 09:12:27,642] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6760
[2019-04-04 09:12:27,683] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.633333333333333, 75.33333333333334, 0.0, 0.0, 26.0, 24.27072406631899, 0.04904964650602528, 0.0, 1.0, 41592.18372134971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 706800.0000, 
sim time next is 707400.0000, 
raw observation next is [-2.55, 75.5, 0.0, 0.0, 26.0, 24.29857432278604, 0.04533756142600251, 0.0, 1.0, 41556.55022540701], 
processed observation next is [1.0, 0.17391304347826086, 0.3919667590027701, 0.755, 0.0, 0.0, 0.6666666666666666, 0.5248811935655032, 0.5151125204753342, 0.0, 1.0, 0.19788833440670003], 
reward next is 0.8021, 
noisyNet noise sample is [array([2.6048577], dtype=float32), 1.2020062]. 
=============================================
[2019-04-04 09:12:29,951] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5497222e-12 8.7396673e-10 2.7214684e-29 2.8913936e-27 7.6062947e-23
 1.0000000e+00 1.5183819e-21], sum to 1.0000
[2019-04-04 09:12:29,953] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4589
[2019-04-04 09:12:29,969] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.04999999999999999, 53.5, 131.0, 449.0, 26.0, 25.7484845934085, 0.3607472251572892, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 736200.0000, 
sim time next is 736800.0000, 
raw observation next is [0.1333333333333333, 52.33333333333333, 124.0, 503.0, 26.0, 25.65833607395623, 0.3574851238929495, 1.0, 1.0, 35379.1887096622], 
processed observation next is [1.0, 0.5217391304347826, 0.46629732225300097, 0.5233333333333333, 0.41333333333333333, 0.5558011049723757, 0.6666666666666666, 0.638194672829686, 0.6191617079643165, 1.0, 1.0, 0.16847232718886762], 
reward next is 0.8315, 
noisyNet noise sample is [array([-1.9004798], dtype=float32), -1.6939477]. 
=============================================
[2019-04-04 09:12:31,251] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0585152e-11 2.4981558e-10 1.3730170e-27 4.0058757e-26 2.8142690e-22
 1.0000000e+00 3.5429579e-21], sum to 1.0000
[2019-04-04 09:12:31,252] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1983
[2019-04-04 09:12:31,291] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 83.66666666666666, 57.33333333333334, 0.0, 26.0, 26.28186620210179, 0.4503129875735648, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 830400.0000, 
sim time next is 831000.0000, 
raw observation next is [-3.9, 84.83333333333334, 55.66666666666666, 0.0, 26.0, 26.26077279109061, 0.4445927523343778, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.8483333333333334, 0.18555555555555553, 0.0, 0.6666666666666666, 0.6883977325908841, 0.6481975841114592, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5098748], dtype=float32), -1.2816577]. 
=============================================
[2019-04-04 09:12:31,315] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[84.39721 ]
 [84.56767 ]
 [84.795906]
 [84.99772 ]
 [85.213524]], R is [[84.42881775]
 [84.58453369]
 [84.73868561]
 [84.89129639]
 [85.04238129]].
[2019-04-04 09:12:38,678] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.6094853e-11 7.1203854e-10 3.8220669e-27 3.1099288e-26 2.0807567e-21
 1.0000000e+00 5.2454515e-20], sum to 1.0000
[2019-04-04 09:12:38,679] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2065
[2019-04-04 09:12:38,717] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 106.1666666666667, 0.0, 26.0, 25.61172455923959, 0.3081974135182943, 1.0, 1.0, 23512.79406422996], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 818400.0000, 
sim time next is 819000.0000, 
raw observation next is [-4.5, 71.0, 110.0, 0.0, 26.0, 25.63207010912495, 0.3124027626679091, 1.0, 1.0, 26056.80041933106], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.36666666666666664, 0.0, 0.6666666666666666, 0.6360058424270791, 0.6041342542226363, 1.0, 1.0, 0.12408000199681457], 
reward next is 0.8759, 
noisyNet noise sample is [array([-1.294679], dtype=float32), -0.9271903]. 
=============================================
[2019-04-04 09:12:38,733] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[83.73733 ]
 [83.526535]
 [83.312355]
 [83.10413 ]
 [82.839325]], R is [[83.95993805]
 [84.00836945]
 [84.07824707]
 [84.14835358]
 [84.21775055]].
[2019-04-04 09:12:41,674] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2133194e-10 1.5885099e-09 6.1340064e-27 3.9192962e-25 9.5218618e-21
 1.0000000e+00 1.3325621e-19], sum to 1.0000
[2019-04-04 09:12:41,674] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3604
[2019-04-04 09:12:41,694] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 26.0, 24.77943892863297, 0.2107112364797742, 0.0, 1.0, 39351.1792246521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 872400.0000, 
sim time next is 873000.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 26.0, 24.73795844589004, 0.1989814588926267, 0.0, 1.0, 39356.50020428836], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5614965371575034, 0.5663271529642089, 0.0, 1.0, 0.18741190573470645], 
reward next is 0.8126, 
noisyNet noise sample is [array([0.99936956], dtype=float32), -0.9898182]. 
=============================================
[2019-04-04 09:12:41,723] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[81.50319]
 [81.54517]
 [81.37761]
 [81.36588]
 [81.32569]], R is [[81.62346649]
 [81.61985016]
 [81.61608124]
 [81.61222076]
 [81.60849762]].
[2019-04-04 09:12:43,246] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3574603e-13 6.0526098e-11 1.8557003e-32 1.2419402e-30 3.1309399e-25
 1.0000000e+00 4.6170377e-25], sum to 1.0000
[2019-04-04 09:12:43,250] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8149
[2019-04-04 09:12:43,258] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.73333333333333, 59.0, 179.3333333333333, 264.1666666666667, 26.0, 27.06157530267827, 0.860843552149818, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1082400.0000, 
sim time next is 1083000.0000, 
raw observation next is [18.01666666666667, 57.5, 177.6666666666667, 211.3333333333333, 26.0, 26.70058189723366, 0.8670106749837531, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.9616805170821794, 0.575, 0.5922222222222224, 0.23351749539594838, 0.6666666666666666, 0.7250484914361385, 0.7890035583279177, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6291029], dtype=float32), 0.5765035]. 
=============================================
[2019-04-04 09:12:43,281] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[100.67245]
 [102.09951]
 [103.33344]
 [104.61321]
 [105.86549]], R is [[99.30403137]
 [99.31098938]
 [99.31787872]
 [99.3246994 ]
 [99.33145142]].
[2019-04-04 09:12:44,117] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.9762100e-12 3.1957689e-10 4.8298795e-30 2.2754623e-27 1.8330051e-23
 1.0000000e+00 5.1980188e-22], sum to 1.0000
[2019-04-04 09:12:44,117] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5415
[2019-04-04 09:12:44,129] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 84.0, 87.0, 0.0, 26.0, 25.44417228833298, 0.2831390469846868, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 903600.0000, 
sim time next is 904200.0000, 
raw observation next is [1.366666666666667, 86.16666666666666, 90.33333333333333, 0.0, 26.0, 25.38690005160853, 0.2782604716454904, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5004616805170823, 0.8616666666666666, 0.3011111111111111, 0.0, 0.6666666666666666, 0.615575004300711, 0.5927534905484968, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6965846], dtype=float32), -0.05900955]. 
=============================================
[2019-04-04 09:13:01,533] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.00202266e-12 1.38952114e-10 7.51100581e-29 2.78785890e-27
 1.04694836e-22 1.00000000e+00 1.06945275e-21], sum to 1.0000
[2019-04-04 09:13:01,536] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1438
[2019-04-04 09:13:01,558] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.3658865339988, 0.4621588172209133, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1452000.0000, 
sim time next is 1452600.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.24407662002876, 0.4377978329677077, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6036730516690634, 0.6459326109892359, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13109706], dtype=float32), -0.30484143]. 
=============================================
[2019-04-04 09:13:02,817] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4623791e-10 1.0431541e-09 5.5778741e-29 6.0378478e-27 3.2387837e-22
 1.0000000e+00 1.8697313e-20], sum to 1.0000
[2019-04-04 09:13:02,818] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8268
[2019-04-04 09:13:02,837] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.25, 95.5, 0.0, 0.0, 26.0, 25.36973625436103, 0.514055944850092, 0.0, 1.0, 67083.72682445725], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1377000.0000, 
sim time next is 1377600.0000, 
raw observation next is [0.1666666666666667, 95.33333333333333, 0.0, 0.0, 26.0, 25.32473207092447, 0.5119309860490026, 0.0, 1.0, 53948.02395297067], 
processed observation next is [1.0, 0.9565217391304348, 0.4672206832871654, 0.9533333333333333, 0.0, 0.0, 0.6666666666666666, 0.6103943392437058, 0.6706436620163342, 0.0, 1.0, 0.25689535215700315], 
reward next is 0.7431, 
noisyNet noise sample is [array([1.1695849], dtype=float32), -1.2105169]. 
=============================================
[2019-04-04 09:13:04,541] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6326401e-11 4.4001330e-10 5.8906931e-28 8.9652940e-27 7.8768713e-22
 1.0000000e+00 3.1961350e-21], sum to 1.0000
[2019-04-04 09:13:04,555] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2988
[2019-04-04 09:13:04,580] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.63333333333333, 52.0, 0.0, 0.0, 26.0, 25.9110791344075, 0.7509257730530479, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1100400.0000, 
sim time next is 1101000.0000, 
raw observation next is [16.36666666666667, 52.5, 0.0, 0.0, 26.0, 26.20048767338358, 0.7719070387205882, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.9159741458910436, 0.525, 0.0, 0.0, 0.6666666666666666, 0.6833739727819651, 0.7573023462401961, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.305523], dtype=float32), -0.1957812]. 
=============================================
[2019-04-04 09:13:04,634] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.66015 ]
 [84.85667 ]
 [84.99142 ]
 [85.04269 ]
 [85.231476]], R is [[84.68874359]
 [84.84185791]
 [84.99343872]
 [85.14350891]
 [85.29207611]].
[2019-04-04 09:13:07,278] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6800658e-12 4.3560155e-11 8.3895871e-30 5.4495180e-28 6.8117394e-23
 1.0000000e+00 1.9729830e-22], sum to 1.0000
[2019-04-04 09:13:07,280] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9024
[2019-04-04 09:13:07,304] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.7000000000000001, 95.0, 15.0, 0.0, 26.0, 25.55343855304854, 0.4887714694659495, 1.0, 1.0, 36300.10571935276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1356000.0000, 
sim time next is 1356600.0000, 
raw observation next is [0.6, 95.5, 12.0, 0.0, 26.0, 25.50891909982351, 0.3838459414552404, 1.0, 1.0, 28026.49049191793], 
processed observation next is [1.0, 0.6956521739130435, 0.479224376731302, 0.955, 0.04, 0.0, 0.6666666666666666, 0.6257432583186257, 0.6279486471517468, 1.0, 1.0, 0.13345947853294254], 
reward next is 0.8665, 
noisyNet noise sample is [array([-1.020487], dtype=float32), 0.33650473]. 
=============================================
[2019-04-04 09:13:09,561] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1774292e-12 6.4641167e-11 6.4609693e-31 1.7040986e-28 8.4245690e-24
 1.0000000e+00 4.7574109e-23], sum to 1.0000
[2019-04-04 09:13:09,562] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7803
[2019-04-04 09:13:09,659] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.066666666666666, 78.0, 50.0, 51.83333333333333, 26.0, 26.05918291105849, 0.5836428691953205, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1586400.0000, 
sim time next is 1587000.0000, 
raw observation next is [6.333333333333333, 77.0, 62.99999999999999, 68.66666666666666, 26.0, 26.15609853380049, 0.592647211298473, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6380424746075716, 0.77, 0.20999999999999996, 0.07587476979742172, 0.6666666666666666, 0.6796748778167073, 0.6975490704328243, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37500402], dtype=float32), 0.34950083]. 
=============================================
[2019-04-04 09:13:09,680] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[94.84386 ]
 [94.67949 ]
 [94.485466]
 [94.10993 ]
 [93.54592 ]], R is [[95.00061035]
 [95.05060577]
 [95.10009766]
 [95.14910126]
 [95.19760895]].
[2019-04-04 09:13:17,975] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.3151984e-11 3.7885117e-09 1.3738815e-27 2.3010917e-26 8.6816634e-22
 1.0000000e+00 1.2477684e-20], sum to 1.0000
[2019-04-04 09:13:17,975] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2226
[2019-04-04 09:13:17,991] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.1, 92.0, 0.0, 0.0, 26.0, 25.39169335941566, 0.4632900603076979, 0.0, 1.0, 66611.05242834272], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1475400.0000, 
sim time next is 1476000.0000, 
raw observation next is [2.2, 92.0, 0.0, 0.0, 26.0, 25.34652133032327, 0.4679145593968999, 0.0, 1.0, 59257.99569614392], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6122101108602725, 0.6559715197989666, 0.0, 1.0, 0.2821809318863996], 
reward next is 0.7178, 
noisyNet noise sample is [array([1.6509254], dtype=float32), -0.7950523]. 
=============================================
[2019-04-04 09:13:18,014] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[85.95373 ]
 [85.72782 ]
 [85.44969 ]
 [85.2302  ]
 [85.308235]], R is [[85.98472595]
 [85.80768585]
 [85.62878418]
 [85.50812531]
 [85.56378174]].
[2019-04-04 09:13:19,905] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.6224079e-13 9.3749661e-11 3.5991057e-30 1.8844024e-28 4.5482025e-23
 1.0000000e+00 6.0234819e-23], sum to 1.0000
[2019-04-04 09:13:19,906] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8400
[2019-04-04 09:13:19,927] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 127.0, 0.0, 26.0, 26.12047777791783, 0.5919056846083466, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1337400.0000, 
sim time next is 1338000.0000, 
raw observation next is [1.1, 92.0, 124.6666666666667, 0.0, 26.0, 26.11833518701989, 0.5828239579084681, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.4155555555555557, 0.0, 0.6666666666666666, 0.6765279322516576, 0.694274652636156, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4041644], dtype=float32), -0.7748028]. 
=============================================
[2019-04-04 09:13:19,942] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[91.06707]
 [91.03028]
 [90.97473]
 [90.92928]
 [90.84554]], R is [[91.12197876]
 [91.21076202]
 [91.29865265]
 [91.38566589]
 [91.47180939]].
[2019-04-04 09:13:22,564] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0224961e-11 1.0591833e-09 2.4121362e-29 2.7545702e-27 4.6370077e-22
 1.0000000e+00 7.2327468e-22], sum to 1.0000
[2019-04-04 09:13:22,577] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4386
[2019-04-04 09:13:22,622] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 26.0, 24.69477037173798, 0.4478339799197172, 0.0, 1.0, 30755.79542912548], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1278000.0000, 
sim time next is 1278600.0000, 
raw observation next is [7.016666666666667, 96.0, 0.0, 0.0, 26.0, 24.70368509595668, 0.4487920090408796, 0.0, 1.0, 27467.23411872707], 
processed observation next is [0.0, 0.8260869565217391, 0.656971375807941, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5586404246630566, 0.6495973363469599, 0.0, 1.0, 0.13079635294631936], 
reward next is 0.8692, 
noisyNet noise sample is [array([0.0237342], dtype=float32), -0.14751406]. 
=============================================
[2019-04-04 09:13:35,653] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-04 09:13:35,654] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:13:35,654] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:13:35,655] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:13:35,655] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:13:35,657] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:13:35,657] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:13:35,659] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run30
[2019-04-04 09:13:35,708] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run30
[2019-04-04 09:13:35,709] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run30
[2019-04-04 09:16:51,862] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 09:17:23,721] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 09:17:28,398] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 09:17:29,438] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 2900000, evaluation results [2900000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 09:17:33,583] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.1978702e-11 3.3718994e-09 6.7690337e-28 3.6517209e-26 5.5156892e-21
 1.0000000e+00 2.9070642e-20], sum to 1.0000
[2019-04-04 09:17:33,583] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4289
[2019-04-04 09:17:33,597] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1, 91.0, 0.0, 0.0, 26.0, 25.26966805658574, 0.4377095588339251, 0.0, 1.0, 42942.19250791583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1737000.0000, 
sim time next is 1737600.0000, 
raw observation next is [0.06666666666666668, 91.0, 0.0, 0.0, 26.0, 25.27582855869893, 0.4335144297466214, 0.0, 1.0, 42954.52786016691], 
processed observation next is [0.0, 0.08695652173913043, 0.46445060018467227, 0.91, 0.0, 0.0, 0.6666666666666666, 0.6063190465582441, 0.6445048099155405, 0.0, 1.0, 0.20454537076269957], 
reward next is 0.7955, 
noisyNet noise sample is [array([0.76461166], dtype=float32), -0.70502007]. 
=============================================
[2019-04-04 09:17:36,474] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.1855688e-11 4.6978571e-10 3.5801442e-28 2.5211212e-26 7.3645719e-23
 1.0000000e+00 6.3326796e-20], sum to 1.0000
[2019-04-04 09:17:36,474] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7164
[2019-04-04 09:17:36,504] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 94.0, 0.0, 0.0, 26.0, 25.5197794881064, 0.5890640326965645, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1298400.0000, 
sim time next is 1299000.0000, 
raw observation next is [3.9, 93.5, 0.0, 0.0, 26.0, 25.52391706347978, 0.5859163759186031, 0.0, 1.0, 18748.8218485078], 
processed observation next is [1.0, 0.0, 0.5706371191135734, 0.935, 0.0, 0.0, 0.6666666666666666, 0.6269930886233149, 0.6953054586395343, 0.0, 1.0, 0.08928010404051333], 
reward next is 0.9107, 
noisyNet noise sample is [array([2.769195], dtype=float32), 0.2508799]. 
=============================================
[2019-04-04 09:17:36,552] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[88.088104]
 [88.19992 ]
 [88.40417 ]
 [88.3958  ]
 [88.106476]], R is [[87.92818451]
 [88.04890442]
 [88.16841888]
 [88.09397888]
 [88.00196838]].
[2019-04-04 09:17:36,763] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0267518e-10 3.8004608e-09 2.2407924e-26 2.9245659e-24 8.7683413e-20
 1.0000000e+00 5.9427674e-19], sum to 1.0000
[2019-04-04 09:17:36,763] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2281
[2019-04-04 09:17:36,809] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 87.0, 97.0, 0.0, 26.0, 24.96951567379322, 0.339984499941348, 0.0, 1.0, 18738.89367711615], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1765800.0000, 
sim time next is 1766400.0000, 
raw observation next is [-2.3, 87.0, 100.6666666666667, 0.0, 26.0, 24.95478300035971, 0.3332870416482461, 0.0, 1.0, 33951.6558728606], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.33555555555555566, 0.0, 0.6666666666666666, 0.5795652500299759, 0.6110956805494153, 0.0, 1.0, 0.16167455177552667], 
reward next is 0.8383, 
noisyNet noise sample is [array([0.8588998], dtype=float32), 0.51191574]. 
=============================================
[2019-04-04 09:17:44,140] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.4029141e-09 1.0856923e-08 3.4460466e-24 5.3141836e-23 2.2172894e-19
 1.0000000e+00 4.9901317e-18], sum to 1.0000
[2019-04-04 09:17:44,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9915
[2019-04-04 09:17:44,175] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 83.0, 0.0, 0.0, 26.0, 25.03801453415209, 0.2826745289176409, 0.0, 1.0, 43492.41783583057], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1981200.0000, 
sim time next is 1981800.0000, 
raw observation next is [-5.9, 83.0, 0.0, 0.0, 26.0, 24.92183346597277, 0.2669506350329666, 0.0, 1.0, 43102.7837243636], 
processed observation next is [1.0, 0.9565217391304348, 0.2991689750692521, 0.83, 0.0, 0.0, 0.6666666666666666, 0.576819455497731, 0.5889835450109889, 0.0, 1.0, 0.20525135106839812], 
reward next is 0.7947, 
noisyNet noise sample is [array([0.48572698], dtype=float32), -1.6901228]. 
=============================================
[2019-04-04 09:18:09,268] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2555605e-11 5.9202132e-10 7.6502483e-27 3.0496044e-25 4.1887922e-20
 1.0000000e+00 5.0684972e-20], sum to 1.0000
[2019-04-04 09:18:09,285] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8920
[2019-04-04 09:18:09,362] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 130.0, 0.0, 26.0, 25.74196719899886, 0.3400307316729342, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2199600.0000, 
sim time next is 2200200.0000, 
raw observation next is [-4.4, 70.5, 134.3333333333333, 0.0, 26.0, 25.62609016342897, 0.330406473753433, 1.0, 1.0, 75904.51236002636], 
processed observation next is [1.0, 0.4782608695652174, 0.3407202216066482, 0.705, 0.4477777777777776, 0.0, 0.6666666666666666, 0.6355075136190808, 0.6101354912511443, 1.0, 1.0, 0.3614500588572684], 
reward next is 0.6385, 
noisyNet noise sample is [array([1.8282394], dtype=float32), -0.42817184]. 
=============================================
[2019-04-04 09:18:19,246] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1933126e-11 1.7660902e-10 6.1461124e-28 2.4492005e-26 3.1066048e-21
 1.0000000e+00 1.5516827e-20], sum to 1.0000
[2019-04-04 09:18:19,246] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3717
[2019-04-04 09:18:19,294] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.9, 67.66666666666667, 269.3333333333334, 106.5, 26.0, 25.8821960738556, 0.4410250169004434, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2115600.0000, 
sim time next is 2116200.0000, 
raw observation next is [-6.800000000000001, 65.83333333333333, 245.6666666666667, 112.0, 26.0, 25.92188121155137, 0.4399438704559503, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.2742382271468144, 0.6583333333333333, 0.818888888888889, 0.12375690607734807, 0.6666666666666666, 0.6601567676292808, 0.6466479568186502, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12303419], dtype=float32), -1.361313]. 
=============================================
[2019-04-04 09:18:32,937] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4401655e-11 9.8884667e-10 9.7625167e-28 2.0883854e-26 7.4334159e-22
 1.0000000e+00 8.6761722e-21], sum to 1.0000
[2019-04-04 09:18:32,937] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2734
[2019-04-04 09:18:32,982] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.95, 52.16666666666666, 248.3333333333333, 72.33333333333333, 26.0, 25.32673803455797, 0.3167552644156448, 1.0, 1.0, 18697.59342053906], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2292600.0000, 
sim time next is 2293200.0000, 
raw observation next is [-1.7, 51.0, 241.5, 71.5, 26.0, 25.22074535228762, 0.3205781990161612, 1.0, 1.0, 18696.98945455885], 
processed observation next is [1.0, 0.5652173913043478, 0.4155124653739613, 0.51, 0.805, 0.07900552486187845, 0.6666666666666666, 0.6017287793573015, 0.6068593996720537, 1.0, 1.0, 0.0890332831169469], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.17883742], dtype=float32), 0.61557084]. 
=============================================
[2019-04-04 09:18:33,026] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.36512285e-11 1.11235865e-09 1.51446108e-27 3.28167092e-26
 1.05044132e-21 1.00000000e+00 1.30321805e-20], sum to 1.0000
[2019-04-04 09:18:33,026] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9450
[2019-04-04 09:18:33,085] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.516666666666667, 50.0, 234.6666666666667, 70.66666666666667, 26.0, 25.24706514382228, 0.3251476190431353, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2293800.0000, 
sim time next is 2294400.0000, 
raw observation next is [-1.333333333333333, 49.00000000000001, 227.8333333333333, 69.83333333333333, 26.0, 25.28085895697983, 0.2401469322942308, 1.0, 1.0, 7477.258160976296], 
processed observation next is [1.0, 0.5652173913043478, 0.42566943674976926, 0.49000000000000005, 0.7594444444444443, 0.07716390423572743, 0.6666666666666666, 0.6067382464149859, 0.5800489774314103, 1.0, 1.0, 0.035605991242744266], 
reward next is 0.9644, 
noisyNet noise sample is [array([0.17883742], dtype=float32), 0.61557084]. 
=============================================
[2019-04-04 09:18:39,591] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.2489956e-10 6.7031376e-09 4.6076401e-26 2.9933718e-24 9.7660575e-20
 1.0000000e+00 4.4779376e-19], sum to 1.0000
[2019-04-04 09:18:39,592] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2917
[2019-04-04 09:18:39,624] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.81583128467948, 0.2568578157225973, 0.0, 1.0, 38572.7311341381], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2337600.0000, 
sim time next is 2338200.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.78279101768214, 0.2492490475083805, 0.0, 1.0, 38610.86455626436], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.565232584806845, 0.5830830158361269, 0.0, 1.0, 0.18386125979173504], 
reward next is 0.8161, 
noisyNet noise sample is [array([-0.11036991], dtype=float32), 0.8961109]. 
=============================================
[2019-04-04 09:18:39,704] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.0636829e-10 3.4239933e-09 8.6017097e-25 9.3946297e-24 8.0561451e-19
 1.0000000e+00 1.1940552e-18], sum to 1.0000
[2019-04-04 09:18:39,705] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5705
[2019-04-04 09:18:39,756] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.433333333333333, 52.0, 0.0, 0.0, 26.0, 25.11085528170682, 0.4046251924126771, 0.0, 1.0, 61779.1973923629], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2580000.0000, 
sim time next is 2580600.0000, 
raw observation next is [-2.616666666666667, 54.0, 0.0, 0.0, 26.0, 25.28989237861587, 0.4157496883799736, 0.0, 1.0, 48658.19522119078], 
processed observation next is [1.0, 0.8695652173913043, 0.3901200369344414, 0.54, 0.0, 0.0, 0.6666666666666666, 0.6074910315513226, 0.6385832294599912, 0.0, 1.0, 0.2317056915294799], 
reward next is 0.7683, 
noisyNet noise sample is [array([-0.29942685], dtype=float32), 0.8306616]. 
=============================================
[2019-04-04 09:18:43,129] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5363701e-09 4.2947665e-08 7.1018648e-23 6.8973767e-22 7.6064303e-18
 1.0000000e+00 1.6868661e-16], sum to 1.0000
[2019-04-04 09:18:43,131] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8948
[2019-04-04 09:18:43,148] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.46585376741712, -0.0937878427990228, 0.0, 1.0, 44418.19698157634], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2435400.0000, 
sim time next is 2436000.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.42834672405547, -0.101872842755829, 0.0, 1.0, 44419.61312114086], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.4523622270046224, 0.46604238574805695, 0.0, 1.0, 0.2115219672435279], 
reward next is 0.7885, 
noisyNet noise sample is [array([-0.24109654], dtype=float32), 2.0021803]. 
=============================================
[2019-04-04 09:18:43,169] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[69.89486 ]
 [70.00862 ]
 [70.113235]
 [70.23479 ]
 [70.33937 ]], R is [[69.87524414]
 [69.96497345]
 [70.05388641]
 [70.14205933]
 [70.22956848]].
[2019-04-04 09:18:58,147] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5208678e-11 4.4452292e-10 3.9664861e-29 3.0889879e-27 1.5181843e-22
 1.0000000e+00 6.7876197e-22], sum to 1.0000
[2019-04-04 09:18:58,147] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5273
[2019-04-04 09:18:58,202] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 93.0, 17.5, 48.49999999999999, 26.0, 24.98138956067587, 0.2713296061265226, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2878800.0000, 
sim time next is 2879400.0000, 
raw observation next is [2.0, 93.0, 34.99999999999999, 69.99999999999999, 26.0, 25.00279007789892, 0.2654912370063399, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.11666666666666664, 0.07734806629834252, 0.6666666666666666, 0.5835658398249098, 0.5884970790021132, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7721275], dtype=float32), 0.5006721]. 
=============================================
[2019-04-04 09:18:58,292] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.26878316e-09 4.51856152e-08 1.12400404e-23 1.46836332e-21
 6.32631165e-18 1.00000000e+00 8.57539961e-18], sum to 1.0000
[2019-04-04 09:18:58,294] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4555
[2019-04-04 09:18:58,377] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 54.0, 40.0, 416.0, 26.0, 23.30715263994946, -0.005743958792985865, 0.0, 1.0, 203134.9941282829], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2449800.0000, 
sim time next is 2450400.0000, 
raw observation next is [-8.033333333333333, 52.66666666666666, 43.5, 457.5, 26.0, 24.00711317638646, 0.1052523159219054, 0.0, 1.0, 154200.2296520234], 
processed observation next is [0.0, 0.34782608695652173, 0.24007386888273316, 0.5266666666666666, 0.145, 0.505524861878453, 0.6666666666666666, 0.5005927646988718, 0.5350841053073018, 0.0, 1.0, 0.7342868078667781], 
reward next is 0.2657, 
noisyNet noise sample is [array([1.2074916], dtype=float32), -0.27192593]. 
=============================================
[2019-04-04 09:18:58,826] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.5588956e-11 7.6889345e-10 1.3399735e-27 9.1648547e-26 2.5224649e-21
 1.0000000e+00 5.4026752e-21], sum to 1.0000
[2019-04-04 09:18:58,826] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1005
[2019-04-04 09:18:58,848] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.166666666666666, 29.0, 161.6666666666667, 57.66666666666666, 26.0, 25.76749814078271, 0.4207623114110353, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2815800.0000, 
sim time next is 2816400.0000, 
raw observation next is [6.333333333333333, 28.0, 139.8333333333333, 28.83333333333333, 26.0, 25.87544715664286, 0.4280919352266327, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6380424746075716, 0.28, 0.466111111111111, 0.031860036832412515, 0.6666666666666666, 0.6562872630535717, 0.6426973117422109, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22312844], dtype=float32), -0.70210534]. 
=============================================
[2019-04-04 09:18:59,052] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.8241628e-11 3.0288301e-09 1.9173671e-27 6.5026741e-26 8.5951998e-21
 1.0000000e+00 1.5862757e-20], sum to 1.0000
[2019-04-04 09:18:59,056] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2897
[2019-04-04 09:18:59,083] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.9216746468656, 0.2578790663546431, 0.0, 1.0, 55707.94438553952], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2866200.0000, 
sim time next is 2866800.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.9091503965053, 0.2503120424096703, 0.0, 1.0, 55732.5156238617], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5757625330421083, 0.5834373474698901, 0.0, 1.0, 0.26539293154219856], 
reward next is 0.7346, 
noisyNet noise sample is [array([0.40607056], dtype=float32), -0.75879306]. 
=============================================
[2019-04-04 09:19:01,647] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.3988101e-11 3.7566568e-09 6.5682447e-25 4.6274458e-23 6.0032617e-19
 1.0000000e+00 1.0614678e-18], sum to 1.0000
[2019-04-04 09:19:01,647] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1322
[2019-04-04 09:19:01,664] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.66364262874959, 0.2486464830684262, 0.0, 1.0, 43363.88917734687], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2766000.0000, 
sim time next is 2766600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.64120014719525, 0.2461485832070079, 0.0, 1.0, 43140.07923676342], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5534333455996041, 0.5820495277356693, 0.0, 1.0, 0.20542894874649248], 
reward next is 0.7946, 
noisyNet noise sample is [array([-0.84549665], dtype=float32), -0.5745109]. 
=============================================
[2019-04-04 09:19:15,460] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2830910e-09 6.7839374e-09 6.6750877e-26 5.1947627e-24 1.4631488e-18
 1.0000000e+00 9.1809529e-18], sum to 1.0000
[2019-04-04 09:19:15,461] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4667
[2019-04-04 09:19:15,489] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.96197197134993, 0.2769486931268647, 0.0, 1.0, 38224.09640888419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3021000.0000, 
sim time next is 3021600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93079214708166, 0.2752204794139655, 0.0, 1.0, 38150.93497496381], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.577566012256805, 0.5917401598046551, 0.0, 1.0, 0.1816711189283991], 
reward next is 0.8183, 
noisyNet noise sample is [array([0.13890755], dtype=float32), 0.99818784]. 
=============================================
[2019-04-04 09:19:30,850] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.30338029e-10 5.68727607e-08 1.01150956e-23 4.77552310e-22
 1.09672660e-17 1.00000000e+00 3.64736622e-17], sum to 1.0000
[2019-04-04 09:19:30,851] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6748
[2019-04-04 09:19:30,910] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 71.16666666666667, 0.0, 0.0, 26.0, 23.76000552961344, -0.009181010160241948, 0.0, 1.0, 40215.71112227936], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3046200.0000, 
sim time next is 3046800.0000, 
raw observation next is [-6.0, 72.33333333333334, 0.0, 0.0, 26.0, 23.73304636665446, -0.0154576877729091, 0.0, 1.0, 40257.19456582331], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.4777538638878716, 0.4948474374090303, 0.0, 1.0, 0.1917009265039205], 
reward next is 0.8083, 
noisyNet noise sample is [array([-0.5288303], dtype=float32), -0.3899092]. 
=============================================
[2019-04-04 09:19:40,362] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.5719307e-10 3.0967322e-09 2.9234825e-25 6.9478375e-23 7.4876999e-19
 1.0000000e+00 3.7734367e-18], sum to 1.0000
[2019-04-04 09:19:40,362] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9435
[2019-04-04 09:19:40,405] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93079208640976, 0.2752204650914976, 0.0, 1.0, 38150.93490819046], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3021600.0000, 
sim time next is 3022200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93795231173934, 0.2698984712044595, 0.0, 1.0, 38075.94099414045], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5781626926449451, 0.5899661570681531, 0.0, 1.0, 0.18131400473400217], 
reward next is 0.8187, 
noisyNet noise sample is [array([2.1006508], dtype=float32), -0.4550828]. 
=============================================
[2019-04-04 09:19:40,950] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1086245e-09 2.0087285e-08 4.2284848e-25 2.3901344e-22 9.8300138e-19
 1.0000000e+00 1.1948121e-17], sum to 1.0000
[2019-04-04 09:19:40,954] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0005
[2019-04-04 09:19:40,976] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.46806262929402, 0.1602401886798432, 0.0, 1.0, 38373.37254575094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3030600.0000, 
sim time next is 3031200.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.42346484431128, 0.1540426015147497, 0.0, 1.0, 38521.25483205608], 
processed observation next is [0.0, 0.08695652173913043, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5352887370259399, 0.5513475338382499, 0.0, 1.0, 0.18343454681931468], 
reward next is 0.8166, 
noisyNet noise sample is [array([1.8136021], dtype=float32), 1.3087888]. 
=============================================
[2019-04-04 09:19:43,420] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2048703e-10 2.0384308e-09 8.0688804e-26 2.9846036e-24 7.0010861e-20
 1.0000000e+00 2.7486509e-19], sum to 1.0000
[2019-04-04 09:19:43,420] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5117
[2019-04-04 09:19:43,432] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.166666666666667, 82.83333333333333, 0.0, 0.0, 26.0, 25.72069251769284, 0.5849454617960814, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3273000.0000, 
sim time next is 3273600.0000, 
raw observation next is [-5.333333333333334, 84.66666666666667, 0.0, 0.0, 26.0, 25.6811876835281, 0.5682738102660726, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.31486611265004616, 0.8466666666666667, 0.0, 0.0, 0.6666666666666666, 0.6400989736273418, 0.6894246034220242, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35230488], dtype=float32), -0.20945582]. 
=============================================
[2019-04-04 09:19:44,323] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3552161e-11 1.5416869e-10 1.1493795e-28 1.4857581e-27 2.9780219e-22
 1.0000000e+00 1.8930767e-21], sum to 1.0000
[2019-04-04 09:19:44,326] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8481
[2019-04-04 09:19:44,359] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.166666666666667, 72.0, 67.66666666666666, 569.3333333333333, 26.0, 26.96684651946709, 0.8385254479914178, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3255000.0000, 
sim time next is 3255600.0000, 
raw observation next is [-3.333333333333333, 73.0, 63.33333333333334, 540.1666666666667, 26.0, 27.08704491956473, 0.5242006240024777, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.37026777469990774, 0.73, 0.21111111111111114, 0.5968692449355434, 0.6666666666666666, 0.7572537432970607, 0.6747335413341592, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1678282], dtype=float32), -0.76745504]. 
=============================================
[2019-04-04 09:19:52,267] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.5558950e-10 1.1553779e-09 4.6854494e-27 9.2566339e-25 4.4940726e-20
 1.0000000e+00 4.9234209e-19], sum to 1.0000
[2019-04-04 09:19:52,271] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9805
[2019-04-04 09:19:52,296] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 68.0, 0.0, 0.0, 26.0, 25.37670953597637, 0.3878976923144604, 0.0, 1.0, 47188.00851104312], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3713400.0000, 
sim time next is 3714000.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 26.0, 25.37086457256921, 0.3868564044873524, 0.0, 1.0, 46393.87917544634], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6142387143807676, 0.6289521348291175, 0.0, 1.0, 0.2209232341687921], 
reward next is 0.7791, 
noisyNet noise sample is [array([0.24059191], dtype=float32), 0.46622273]. 
=============================================
[2019-04-04 09:19:52,303] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.93875]
 [80.17091]
 [80.42194]
 [80.68918]
 [80.89361]], R is [[79.69754791]
 [79.67586517]
 [79.6713028 ]
 [79.67214966]
 [79.61928558]].
[2019-04-04 09:19:52,743] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.44803019e-10 1.22964776e-08 4.32956940e-25 1.30755336e-23
 3.14711488e-19 1.00000000e+00 4.33120144e-19], sum to 1.0000
[2019-04-04 09:19:52,744] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1031
[2019-04-04 09:19:52,779] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.82780537830124, 0.2769812020430907, 0.0, 1.0, 41144.1849516783], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3382200.0000, 
sim time next is 3382800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.85077324029807, 0.274203041431712, 0.0, 1.0, 41187.59130488877], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5708977700248393, 0.5914010138105706, 0.0, 1.0, 0.19613138716613698], 
reward next is 0.8039, 
noisyNet noise sample is [array([0.03157537], dtype=float32), 0.6818803]. 
=============================================
[2019-04-04 09:19:54,652] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2175703e-10 8.2654221e-09 9.6263636e-26 2.8143413e-24 1.3447925e-19
 1.0000000e+00 6.6556348e-19], sum to 1.0000
[2019-04-04 09:19:54,656] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8013
[2019-04-04 09:19:54,672] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 23.9622940779003, 0.1001938123920801, 0.0, 1.0, 58232.36975924458], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2961000.0000, 
sim time next is 2961600.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 23.88586607555141, 0.08692345320719057, 0.0, 1.0, 59879.26740965401], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.4904888396292841, 0.5289744844023968, 0.0, 1.0, 0.2851393686174], 
reward next is 0.7149, 
noisyNet noise sample is [array([-0.06620031], dtype=float32), -0.5038058]. 
=============================================
[2019-04-04 09:20:00,220] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.0381240e-12 7.8512996e-10 1.1078501e-29 3.1936999e-27 5.9773397e-22
 1.0000000e+00 4.4968995e-21], sum to 1.0000
[2019-04-04 09:20:00,224] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1820
[2019-04-04 09:20:00,266] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 77.0, 94.5, 552.0, 26.0, 25.54643977392708, 0.4060371063365319, 1.0, 1.0, 19102.11190557941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3747600.0000, 
sim time next is 3748200.0000, 
raw observation next is [-3.833333333333333, 77.0, 96.33333333333333, 593.0, 26.0, 25.72229249014521, 0.43880300891542, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3564173591874424, 0.77, 0.32111111111111107, 0.6552486187845303, 0.6666666666666666, 0.6435243741787676, 0.6462676696384734, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44832286], dtype=float32), 1.2510849]. 
=============================================
[2019-04-04 09:20:00,540] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2059083e-11 1.8928707e-10 3.3012780e-29 2.3774792e-27 8.8878820e-22
 1.0000000e+00 1.1871997e-21], sum to 1.0000
[2019-04-04 09:20:00,549] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7697
[2019-04-04 09:20:00,558] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 73.0, 111.6666666666667, 777.8333333333334, 26.0, 26.37608264090255, 0.5580858117730952, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3753600.0000, 
sim time next is 3754200.0000, 
raw observation next is [-3.0, 72.0, 112.3333333333333, 786.6666666666667, 26.0, 26.39312726870553, 0.5658907346805219, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.72, 0.37444444444444436, 0.8692449355432782, 0.6666666666666666, 0.6994272723921275, 0.6886302448935072, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8261405], dtype=float32), 1.5464677]. 
=============================================
[2019-04-04 09:20:01,023] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5828383e-10 8.9715979e-10 7.5566262e-26 1.6118542e-25 1.5112635e-20
 1.0000000e+00 3.1371649e-20], sum to 1.0000
[2019-04-04 09:20:01,031] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8307
[2019-04-04 09:20:01,064] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.18385380453795, 0.4494208347550774, 0.0, 1.0, 58415.66936495891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3526800.0000, 
sim time next is 3527400.0000, 
raw observation next is [0.3333333333333333, 68.66666666666666, 0.0, 0.0, 26.0, 25.10280122712948, 0.4463543103217671, 0.0, 1.0, 90450.33065253851], 
processed observation next is [1.0, 0.8260869565217391, 0.4718374884579871, 0.6866666666666665, 0.0, 0.0, 0.6666666666666666, 0.5919001022607899, 0.6487847701072557, 0.0, 1.0, 0.4307158602501834], 
reward next is 0.5693, 
noisyNet noise sample is [array([-0.14486022], dtype=float32), 0.067821816]. 
=============================================
[2019-04-04 09:20:04,482] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5371211e-10 7.5735160e-09 7.7722937e-27 9.6944330e-25 6.5516207e-20
 1.0000000e+00 9.4858662e-20], sum to 1.0000
[2019-04-04 09:20:04,482] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6668
[2019-04-04 09:20:04,545] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.333333333333334, 57.66666666666667, 111.5, 767.6666666666667, 26.0, 25.49972666906915, 0.4719241199047972, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3580800.0000, 
sim time next is 3581400.0000, 
raw observation next is [-4.166666666666667, 55.83333333333333, 112.0, 777.3333333333333, 26.0, 25.43030797296699, 0.4627827379570934, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3471837488457987, 0.5583333333333332, 0.37333333333333335, 0.8589318600368323, 0.6666666666666666, 0.6191923310805825, 0.6542609126523645, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4610097], dtype=float32), 0.03680004]. 
=============================================
[2019-04-04 09:20:04,724] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6585829e-10 1.9292299e-09 4.9053474e-27 2.6271790e-25 2.4456508e-20
 1.0000000e+00 2.2487471e-19], sum to 1.0000
[2019-04-04 09:20:04,727] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0073
[2019-04-04 09:20:04,750] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.333333333333334, 56.0, 55.83333333333334, 462.5, 26.0, 25.47945002848898, 0.4568034243897802, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3688800.0000, 
sim time next is 3689400.0000, 
raw observation next is [4.166666666666667, 57.5, 47.66666666666667, 403.0000000000001, 26.0, 25.45390552141603, 0.4495975277196586, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5780240073868884, 0.575, 0.1588888888888889, 0.44530386740331507, 0.6666666666666666, 0.6211587934513357, 0.6498658425732196, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39365998], dtype=float32), 1.6739876]. 
=============================================
[2019-04-04 09:20:06,113] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9919304e-11 2.4535640e-10 3.0522983e-29 1.5286883e-26 1.9594222e-22
 1.0000000e+00 1.0453406e-21], sum to 1.0000
[2019-04-04 09:20:06,114] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4467
[2019-04-04 09:20:06,130] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.1, 100.0, 0.0, 0.0, 26.0, 25.3606906563835, 0.2977899908975266, 0.0, 1.0, 99007.51993933818], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3121800.0000, 
sim time next is 3122400.0000, 
raw observation next is [2.2, 100.0, 0.0, 0.0, 26.0, 25.29821525672278, 0.2981036287009481, 0.0, 1.0, 77793.39354088478], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6081846047268984, 0.5993678762336493, 0.0, 1.0, 0.3704447311470704], 
reward next is 0.6296, 
noisyNet noise sample is [array([1.3086482], dtype=float32), 1.5510142]. 
=============================================
[2019-04-04 09:20:12,553] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.62729039e-09 8.54376569e-09 5.54826306e-24 1.33171156e-22
 2.21576871e-18 1.00000000e+00 1.08812954e-17], sum to 1.0000
[2019-04-04 09:20:12,563] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1432
[2019-04-04 09:20:12,598] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.36823395816525, 0.4579051198300307, 0.0, 1.0, 26898.71449135005], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4046400.0000, 
sim time next is 4047000.0000, 
raw observation next is [-4.0, 30.66666666666667, 0.0, 0.0, 26.0, 25.36629398703732, 0.4528314624323121, 1.0, 1.0, 26925.97072852976], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.3066666666666667, 0.0, 0.0, 0.6666666666666666, 0.61385783225311, 0.6509438208107707, 1.0, 1.0, 0.12821890823109408], 
reward next is 0.8718, 
noisyNet noise sample is [array([0.53161174], dtype=float32), 1.6159266]. 
=============================================
[2019-04-04 09:20:12,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[70.968994]
 [71.80108 ]
 [72.77621 ]
 [72.38807 ]
 [73.57334 ]], R is [[71.84882355]
 [72.00224304]
 [72.1542511 ]
 [72.30463409]
 [72.45253754]].
[2019-04-04 09:20:14,455] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5186712e-09 5.5785692e-09 1.1081821e-25 1.9134545e-23 5.1309432e-20
 1.0000000e+00 1.3469392e-18], sum to 1.0000
[2019-04-04 09:20:14,455] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4974
[2019-04-04 09:20:14,469] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 75.0, 0.0, 0.0, 26.0, 25.45886588856584, 0.4584494664845432, 0.0, 1.0, 37584.97524802411], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3795600.0000, 
sim time next is 3796200.0000, 
raw observation next is [-3.0, 74.0, 0.0, 0.0, 26.0, 25.40298541722128, 0.450785239667469, 0.0, 1.0, 66525.4046441733], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6169154514351067, 0.650261746555823, 0.0, 1.0, 0.31678764116272995], 
reward next is 0.6832, 
noisyNet noise sample is [array([-0.8146968], dtype=float32), -1.5717757]. 
=============================================
[2019-04-04 09:20:18,467] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.9077489e-10 3.0185717e-08 7.0285766e-25 1.7943247e-23 1.3575345e-18
 1.0000000e+00 1.6613266e-18], sum to 1.0000
[2019-04-04 09:20:18,467] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3220
[2019-04-04 09:20:18,481] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 39.5, 0.0, 0.0, 26.0, 25.41620301602092, 0.4332837988965233, 0.0, 1.0, 21751.1407871322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4150200.0000, 
sim time next is 4150800.0000, 
raw observation next is [-1.0, 39.0, 0.0, 0.0, 26.0, 25.38941797636522, 0.4249032428797966, 0.0, 1.0, 43784.68505634259], 
processed observation next is [0.0, 0.043478260869565216, 0.4349030470914128, 0.39, 0.0, 0.0, 0.6666666666666666, 0.6157848313637683, 0.6416344142932655, 0.0, 1.0, 0.20849850026829805], 
reward next is 0.7915, 
noisyNet noise sample is [array([0.87392044], dtype=float32), 0.3342468]. 
=============================================
[2019-04-04 09:20:25,440] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.5753769e-12 8.4520385e-10 8.6991698e-29 2.0021987e-27 1.7150273e-22
 1.0000000e+00 1.3700441e-21], sum to 1.0000
[2019-04-04 09:20:25,443] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8117
[2019-04-04 09:20:25,468] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.16666666666667, 27.33333333333334, 107.6666666666667, 729.6666666666667, 26.0, 25.65056888328702, 0.4707984550366519, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3665400.0000, 
sim time next is 3666000.0000, 
raw observation next is [11.33333333333333, 26.66666666666667, 109.3333333333333, 746.3333333333334, 26.0, 25.63765463590512, 0.4746945818000386, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.7765466297322253, 0.2666666666666667, 0.36444444444444435, 0.8246777163904236, 0.6666666666666666, 0.6364712196587599, 0.6582315272666796, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5987707], dtype=float32), -0.1940874]. 
=============================================
[2019-04-04 09:20:25,477] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[89.609764]
 [89.692825]
 [89.77403 ]
 [89.75142 ]
 [89.66222 ]], R is [[89.60144806]
 [89.70543671]
 [89.80838013]
 [89.91029358]
 [90.01119232]].
[2019-04-04 09:20:32,210] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7349083e-11 2.0467370e-09 2.3636693e-28 1.4974289e-26 3.9009131e-21
 1.0000000e+00 8.8807694e-21], sum to 1.0000
[2019-04-04 09:20:32,211] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6056
[2019-04-04 09:20:32,232] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 52.0, 120.5, 834.5, 26.0, 25.21134794371913, 0.4030611469022774, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4276800.0000, 
sim time next is 4277400.0000, 
raw observation next is [7.0, 52.0, 120.3333333333333, 838.6666666666667, 26.0, 25.23102554764804, 0.4070431874518772, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.401111111111111, 0.9267034990791898, 0.6666666666666666, 0.6025854623040034, 0.6356810624839591, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9794811], dtype=float32), -0.6213527]. 
=============================================
[2019-04-04 09:20:34,903] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.8614101e-11 3.2816241e-09 4.1237400e-28 2.8933147e-26 4.3149182e-22
 1.0000000e+00 5.3592410e-21], sum to 1.0000
[2019-04-04 09:20:34,904] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8693
[2019-04-04 09:20:34,938] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.65, 69.16666666666666, 0.0, 0.0, 26.0, 25.45521983551161, 0.3651030756638449, 0.0, 1.0, 89231.09638617112], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4337400.0000, 
sim time next is 4338000.0000, 
raw observation next is [3.6, 69.0, 0.0, 0.0, 26.0, 25.43905142396177, 0.3679342802282031, 0.0, 1.0, 70105.26306260165], 
processed observation next is [1.0, 0.21739130434782608, 0.5623268698060943, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6199209519968143, 0.6226447600760677, 0.0, 1.0, 0.3338345860123888], 
reward next is 0.6662, 
noisyNet noise sample is [array([0.4030249], dtype=float32), 1.4767673]. 
=============================================
[2019-04-04 09:20:34,966] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.89431 ]
 [85.666084]
 [85.69651 ]
 [85.85354 ]
 [86.116585]], R is [[85.81336212]
 [85.53031921]
 [85.48634338]
 [85.54225159]
 [85.68682861]].
[2019-04-04 09:20:37,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7166282e-10 2.0433863e-09 1.2801594e-26 1.2378574e-24 1.7953477e-20
 1.0000000e+00 5.8315568e-20], sum to 1.0000
[2019-04-04 09:20:37,291] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7057
[2019-04-04 09:20:37,307] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 26.0, 25.71966180432339, 0.5897693970202159, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4136400.0000, 
sim time next is 4137000.0000, 
raw observation next is [1.0, 36.66666666666666, 0.0, 0.0, 26.0, 25.86550842223248, 0.5939934594095875, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.3666666666666666, 0.0, 0.0, 0.6666666666666666, 0.65545903518604, 0.6979978198031959, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4809861], dtype=float32), -1.094509]. 
=============================================
[2019-04-04 09:20:37,326] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[79.89116]
 [80.49484]
 [80.80147]
 [80.72681]
 [80.44219]], R is [[79.66355896]
 [79.86692047]
 [79.8564682 ]
 [79.19261932]
 [78.4571991 ]].
[2019-04-04 09:20:41,198] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.2717496e-12 1.3820778e-10 1.4796318e-28 1.0188893e-26 4.9541788e-22
 1.0000000e+00 1.2380293e-21], sum to 1.0000
[2019-04-04 09:20:41,200] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7356
[2019-04-04 09:20:41,234] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 78.0, 45.0, 9.166666666666664, 26.0, 26.10339237218344, 0.5829729608424307, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4466400.0000, 
sim time next is 4467000.0000, 
raw observation next is [0.0, 78.0, 41.0, 18.33333333333333, 26.0, 26.16163906306696, 0.5856321330802436, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.13666666666666666, 0.020257826887661135, 0.6666666666666666, 0.6801365885889133, 0.6952107110267479, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.88371164], dtype=float32), 0.89093935]. 
=============================================
[2019-04-04 09:20:41,243] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[87.08742 ]
 [87.12668 ]
 [87.295944]
 [87.526764]
 [87.906944]], R is [[87.2114563 ]
 [87.33934021]
 [87.46595001]
 [87.59129333]
 [87.71537781]].
[2019-04-04 09:20:44,719] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.9276603e-12 2.3977104e-10 2.3469936e-28 6.8813539e-27 4.3269175e-22
 1.0000000e+00 6.2243031e-21], sum to 1.0000
[2019-04-04 09:20:44,720] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3314
[2019-04-04 09:20:44,765] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 50.0, 127.0, 0.0, 26.0, 24.69909721220045, 0.3710985224292727, 1.0, 1.0, 196283.3845438567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4537800.0000, 
sim time next is 4538400.0000, 
raw observation next is [2.0, 50.66666666666666, 147.0, 7.999999999999998, 26.0, 25.00728308039226, 0.415519989001877, 1.0, 1.0, 44586.31449476342], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.5066666666666666, 0.49, 0.008839779005524859, 0.6666666666666666, 0.5839402566993549, 0.6385066630006256, 1.0, 1.0, 0.21231578330839723], 
reward next is 0.7877, 
noisyNet noise sample is [array([-0.5139034], dtype=float32), 1.9484229]. 
=============================================
[2019-04-04 09:20:48,237] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4285202e-10 3.5672278e-09 6.3461753e-27 2.7081871e-25 2.4671276e-20
 1.0000000e+00 1.8194720e-20], sum to 1.0000
[2019-04-04 09:20:48,237] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1249
[2019-04-04 09:20:48,257] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 53.66666666666667, 0.0, 0.0, 26.0, 25.66854995631374, 0.520829219730259, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4669800.0000, 
sim time next is 4670400.0000, 
raw observation next is [2.0, 55.33333333333334, 0.0, 0.0, 26.0, 25.70403774452722, 0.5119813928214869, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.5533333333333335, 0.0, 0.0, 0.6666666666666666, 0.6420031453772683, 0.670660464273829, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2458379], dtype=float32), 0.70802546]. 
=============================================
[2019-04-04 09:20:54,905] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.8690762e-11 3.0768837e-10 8.0090427e-28 2.4333397e-27 5.1739430e-22
 1.0000000e+00 2.5460143e-21], sum to 1.0000
[2019-04-04 09:20:54,905] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4899
[2019-04-04 09:20:54,933] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 75.0, 25.0, 55.0, 26.0, 25.89782284037953, 0.5010175922278068, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4469400.0000, 
sim time next is 4470000.0000, 
raw observation next is [0.0, 74.0, 20.83333333333334, 45.83333333333334, 26.0, 25.69367769758369, 0.5001989546350954, 1.0, 1.0, 30599.13808561741], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.74, 0.06944444444444446, 0.050644567219152864, 0.6666666666666666, 0.6411398081319742, 0.6667329848783652, 1.0, 1.0, 0.1457101813600829], 
reward next is 0.8543, 
noisyNet noise sample is [array([0.04115781], dtype=float32), -0.84216666]. 
=============================================
[2019-04-04 09:20:54,942] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[86.79732 ]
 [86.980316]
 [87.04707 ]
 [87.11613 ]
 [87.158264]], R is [[86.6585083 ]
 [86.79192352]
 [86.9240036 ]
 [87.05476379]
 [87.18421936]].
[2019-04-04 09:21:05,226] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2339667e-12 1.6202073e-10 1.1739859e-29 2.5806395e-28 1.7434369e-23
 1.0000000e+00 1.8666792e-22], sum to 1.0000
[2019-04-04 09:21:05,228] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5280
[2019-04-04 09:21:05,246] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 130.5, 0.9999999999999998, 26.0, 26.37285604440177, 0.5678572703489067, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4700400.0000, 
sim time next is 4701000.0000, 
raw observation next is [0.0, 92.0, 146.0, 2.0, 26.0, 26.39939500178368, 0.5755324790510904, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.4866666666666667, 0.0022099447513812156, 0.6666666666666666, 0.6999495834819734, 0.6918441596836967, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6352724], dtype=float32), 0.3430095]. 
=============================================
[2019-04-04 09:21:05,256] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[92.031136]
 [91.910255]
 [91.8825  ]
 [91.79832 ]
 [91.75295 ]], R is [[92.27145386]
 [92.34873962]
 [92.42525482]
 [92.50099945]
 [92.57598877]].
[2019-04-04 09:21:05,623] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1238404e-10 1.3916728e-09 2.2851003e-27 2.0198758e-25 2.3913497e-20
 1.0000000e+00 4.0710363e-20], sum to 1.0000
[2019-04-04 09:21:05,625] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9316
[2019-04-04 09:21:05,668] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.733333333333333, 58.66666666666666, 269.0, 169.0, 26.0, 25.26849270334934, 0.3408680656853693, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4875000.0000, 
sim time next is 4875600.0000, 
raw observation next is [-1.466666666666667, 57.33333333333334, 284.5, 166.5, 26.0, 25.20180872532139, 0.3339613477849896, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.42197599261311175, 0.5733333333333335, 0.9483333333333334, 0.1839779005524862, 0.6666666666666666, 0.6001507271101157, 0.6113204492616632, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.186212], dtype=float32), -1.1058877]. 
=============================================
[2019-04-04 09:21:12,119] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:12,119] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:12,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run23
[2019-04-04 09:21:13,315] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:13,316] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:13,335] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run23
[2019-04-04 09:21:16,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:16,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:16,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run23
[2019-04-04 09:21:17,106] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:17,106] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:17,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run23
[2019-04-04 09:21:17,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:17,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:17,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run23
[2019-04-04 09:21:21,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:21,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:21,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run23
[2019-04-04 09:21:23,446] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6103014e-10 2.4278266e-09 1.8786769e-26 3.1019564e-25 3.2382929e-20
 1.0000000e+00 3.2721947e-19], sum to 1.0000
[2019-04-04 09:21:23,446] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8278
[2019-04-04 09:21:23,465] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5833333333333333, 73.0, 0.0, 0.0, 26.0, 25.54287796509252, 0.4891181752416459, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4495800.0000, 
sim time next is 4496400.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.58525910557882, 0.4842134121350963, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6321049254649017, 0.6614044707116987, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13394964], dtype=float32), 1.7924407]. 
=============================================
[2019-04-04 09:21:24,726] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.09367385e-11 4.29989322e-10 8.98490254e-29 2.05398533e-26
 2.15442018e-21 1.00000000e+00 4.83563485e-21], sum to 1.0000
[2019-04-04 09:21:24,726] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2686
[2019-04-04 09:21:24,775] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8666666666666667, 72.33333333333333, 18.5, 11.0, 26.0, 25.59855651400763, 0.4296890230639438, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4520400.0000, 
sim time next is 4521000.0000, 
raw observation next is [-0.8333333333333334, 72.66666666666667, 36.99999999999999, 22.0, 26.0, 25.50966555730325, 0.4238374539411739, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.43951985226223456, 0.7266666666666667, 0.12333333333333331, 0.02430939226519337, 0.6666666666666666, 0.6258054631086042, 0.6412791513137246, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3945721], dtype=float32), 0.17620987]. 
=============================================
[2019-04-04 09:21:24,795] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[85.2668 ]
 [83.99482]
 [82.80464]
 [81.5255 ]
 [81.59166]], R is [[86.62903595]
 [86.76274872]
 [86.89511871]
 [87.02616882]
 [86.96338654]].
[2019-04-04 09:21:25,080] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:25,080] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:25,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run23
[2019-04-04 09:21:27,829] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5361041e-13 9.5307776e-11 2.3045305e-30 3.0707506e-29 1.5089915e-23
 1.0000000e+00 3.2326968e-23], sum to 1.0000
[2019-04-04 09:21:27,829] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2284
[2019-04-04 09:21:27,871] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.166666666666668, 25.83333333333334, 123.6666666666667, 858.3333333333334, 26.0, 27.13273413287125, 0.8473520082968374, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5055000.0000, 
sim time next is 5055600.0000, 
raw observation next is [8.333333333333334, 25.66666666666667, 123.8333333333333, 861.6666666666667, 26.0, 27.45328759870075, 0.6675709142989107, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6934441366574331, 0.2566666666666667, 0.4127777777777777, 0.9521178637200738, 0.6666666666666666, 0.7877739665583959, 0.7225236380996369, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0040403], dtype=float32), -1.3932563]. 
=============================================
[2019-04-04 09:21:28,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:28,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:28,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run23
[2019-04-04 09:21:28,962] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:28,963] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:28,986] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run23
[2019-04-04 09:21:30,780] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:30,780] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:30,789] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run23
[2019-04-04 09:21:31,047] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.9506072e-12 3.9327357e-11 5.5602598e-30 1.5950008e-28 8.5645890e-23
 1.0000000e+00 5.1432444e-22], sum to 1.0000
[2019-04-04 09:21:31,047] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7966
[2019-04-04 09:21:31,055] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 26.0, 28.59772189135163, 1.095942210790192, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5077200.0000, 
sim time next is 5077800.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 26.0, 28.455429099856, 1.075028519524866, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.0, 0.0, 0.6666666666666666, 0.8712857583213335, 0.8583428398416221, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4918014], dtype=float32), 0.2830589]. 
=============================================
[2019-04-04 09:21:31,248] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.96908143e-10 2.11125228e-09 8.74926986e-26 1.03437556e-23
 1.38815318e-19 1.00000000e+00 7.68174562e-19], sum to 1.0000
[2019-04-04 09:21:31,248] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6970
[2019-04-04 09:21:31,279] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.35642078191744, 0.3567745077487232, 0.0, 1.0, 61955.48851000627], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4839600.0000, 
sim time next is 4840200.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.29488337975809, 0.3512401061712947, 0.0, 1.0, 50691.14075645404], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6079069483131742, 0.6170800353904315, 0.0, 1.0, 0.24138638455454306], 
reward next is 0.7586, 
noisyNet noise sample is [array([-1.8056709], dtype=float32), -1.6447163]. 
=============================================
[2019-04-04 09:21:31,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:31,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:31,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run23
[2019-04-04 09:21:33,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:33,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:33,172] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run23
[2019-04-04 09:21:40,516] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2105479e-10 4.1394146e-09 6.7084036e-25 4.5241671e-23 2.7357528e-19
 1.0000000e+00 1.1650697e-18], sum to 1.0000
[2019-04-04 09:21:40,518] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7172
[2019-04-04 09:21:40,548] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1666666666666667, 39.16666666666666, 0.0, 0.0, 26.0, 25.38302429779206, 0.3488012937640299, 0.0, 1.0, 52747.7047587083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4921800.0000, 
sim time next is 4922400.0000, 
raw observation next is [0.3333333333333333, 39.33333333333334, 0.0, 0.0, 26.0, 25.36970475693459, 0.3502780708333663, 0.0, 1.0, 50710.77533774789], 
processed observation next is [0.0, 1.0, 0.4718374884579871, 0.3933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6141420630778823, 0.6167593569444555, 0.0, 1.0, 0.24147988256070424], 
reward next is 0.7585, 
noisyNet noise sample is [array([-1.2059953], dtype=float32), -0.30051422]. 
=============================================
[2019-04-04 09:21:43,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:43,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:43,452] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run23
[2019-04-04 09:21:46,160] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8140602e-10 5.7206511e-09 1.2845130e-26 1.1028016e-24 7.2710682e-20
 1.0000000e+00 1.3210376e-19], sum to 1.0000
[2019-04-04 09:21:46,160] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2148
[2019-04-04 09:21:46,230] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 69.0, 117.5, 260.8333333333334, 26.0, 24.40235324840997, 0.2654942244284128, 0.0, 1.0, 202487.9849510309], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4868400.0000, 
sim time next is 4869000.0000, 
raw observation next is [-3.5, 68.0, 141.0, 313.0, 26.0, 24.80509418951284, 0.3384438834282639, 0.0, 1.0, 8346.298220997533], 
processed observation next is [0.0, 0.34782608695652173, 0.36565096952908593, 0.68, 0.47, 0.34585635359116024, 0.6666666666666666, 0.5670911824594033, 0.6128146278094213, 0.0, 1.0, 0.03974427724284539], 
reward next is 0.9603, 
noisyNet noise sample is [array([0.7711409], dtype=float32), -1.8097094]. 
=============================================
[2019-04-04 09:21:46,237] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.85665]
 [78.83405]
 [77.81164]
 [76.91566]
 [76.26215]], R is [[82.15104675]
 [81.36531067]
 [81.36557007]
 [81.36477661]
 [81.36320496]].
[2019-04-04 09:21:47,811] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:47,811] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:47,814] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run23
[2019-04-04 09:21:51,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:21:51,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:21:51,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run23
[2019-04-04 09:21:54,973] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.6726204e-11 1.0939148e-09 2.5406413e-27 4.5073130e-26 1.1756488e-21
 1.0000000e+00 5.6294804e-20], sum to 1.0000
[2019-04-04 09:21:54,973] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6267
[2019-04-04 09:21:55,062] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.699999999999999, 93.0, 41.66666666666666, 0.0, 26.0, 23.28598681914837, -0.1226141324581852, 0.0, 1.0, 58381.46038731085], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 34800.0000, 
sim time next is 35400.0000, 
raw observation next is [7.7, 93.0, 45.33333333333333, 0.0, 26.0, 23.38419836032465, -0.1002904867048895, 0.0, 1.0, 58165.86569307836], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.15111111111111108, 0.0, 0.6666666666666666, 0.4486831966937208, 0.46656983776503685, 0.0, 1.0, 0.2769803128241827], 
reward next is 0.7230, 
noisyNet noise sample is [array([-0.9134391], dtype=float32), -0.87197584]. 
=============================================
[2019-04-04 09:21:55,844] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.0970798e-11 9.5296548e-10 2.2387140e-28 3.2549671e-25 5.7031849e-21
 1.0000000e+00 5.0843377e-20], sum to 1.0000
[2019-04-04 09:21:55,844] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4704
[2019-04-04 09:21:55,934] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.699999999999999, 93.0, 62.5, 0.0, 26.0, 23.91365207261305, 0.016836052997192, 0.0, 1.0, 56686.95043770633], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 38400.0000, 
sim time next is 39000.0000, 
raw observation next is [7.7, 93.0, 65.0, 0.0, 26.0, 23.99495956858281, 0.03288106656444258, 0.0, 1.0, 56510.06948051655], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.21666666666666667, 0.0, 0.6666666666666666, 0.4995799640485676, 0.5109603555214809, 0.0, 1.0, 0.2690955689548407], 
reward next is 0.7309, 
noisyNet noise sample is [array([-1.5990808], dtype=float32), 2.0842226]. 
=============================================
[2019-04-04 09:21:55,961] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.63289 ]
 [86.303925]
 [85.97937 ]
 [85.672134]
 [85.37103 ]], R is [[86.79357147]
 [86.65570068]
 [86.51835632]
 [86.38186646]
 [86.2450943 ]].
[2019-04-04 09:22:05,660] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.5102577e-11 1.0185522e-09 4.2888356e-25 6.8981855e-24 1.4119349e-19
 1.0000000e+00 9.4659892e-19], sum to 1.0000
[2019-04-04 09:22:05,660] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0193
[2019-04-04 09:22:05,732] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.0, 38.33333333333334, 31.66666666666666, 605.6666666666667, 26.0, 25.91610289816067, 0.3046761938644502, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 402600.0000, 
sim time next is 403200.0000, 
raw observation next is [-8.9, 38.0, 29.0, 555.0, 26.0, 25.76063225726051, 0.3996183246728098, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.38, 0.09666666666666666, 0.6132596685082873, 0.6666666666666666, 0.6467193547717093, 0.6332061082242699, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49304098], dtype=float32), 0.4993448]. 
=============================================
[2019-04-04 09:22:08,568] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8284313e-08 9.5299725e-08 7.3145142e-22 2.3243186e-20 5.8927930e-17
 9.9999988e-01 2.5757179e-16], sum to 1.0000
[2019-04-04 09:22:08,568] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5152
[2019-04-04 09:22:08,602] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.57892224971059, -0.06679103742811132, 0.0, 1.0, 45445.56087529077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 436200.0000, 
sim time next is 436800.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.5874010208961, -0.07478199377538243, 0.0, 1.0, 45486.13177114395], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 0.6666666666666666, 0.4656167517413416, 0.4750726687415392, 0.0, 1.0, 0.21660062748163786], 
reward next is 0.7834, 
noisyNet noise sample is [array([0.2778999], dtype=float32), 0.04504325]. 
=============================================
[2019-04-04 09:22:08,736] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3377389e-11 8.3703799e-10 5.7865963e-27 1.0555773e-24 6.5521455e-20
 1.0000000e+00 6.6069862e-20], sum to 1.0000
[2019-04-04 09:22:08,736] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6179
[2019-04-04 09:22:08,773] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 60.0, 96.5, 585.0, 26.0, 25.77381915701307, 0.3553629705807088, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 298800.0000, 
sim time next is 299400.0000, 
raw observation next is [-10.6, 58.16666666666666, 99.66666666666666, 609.3333333333334, 26.0, 25.7406621159269, 0.3516506992651131, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.5816666666666666, 0.3322222222222222, 0.6732965009208104, 0.6666666666666666, 0.6450551763272415, 0.6172168997550377, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5520634], dtype=float32), 0.99178565]. 
=============================================
[2019-04-04 09:22:09,078] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7256505e-10 3.6630956e-09 6.6056057e-26 1.9084454e-24 1.3170273e-19
 1.0000000e+00 9.4061603e-20], sum to 1.0000
[2019-04-04 09:22:09,079] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7885
[2019-04-04 09:22:09,150] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.366666666666667, 27.0, 127.3333333333333, 0.0, 26.0, 25.11029177217221, 0.1441641448079572, 1.0, 1.0, 51081.78628941248], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 477600.0000, 
sim time next is 478200.0000, 
raw observation next is [-1.283333333333333, 27.5, 125.6666666666667, 0.0, 26.0, 25.01157050621832, 0.1487614421302516, 1.0, 1.0, 90565.28572423183], 
processed observation next is [1.0, 0.5217391304347826, 0.4270544783010157, 0.275, 0.418888888888889, 0.0, 0.6666666666666666, 0.58429754218486, 0.5495871473767505, 1.0, 1.0, 0.4312632653534849], 
reward next is 0.5687, 
noisyNet noise sample is [array([-0.23307069], dtype=float32), -0.8971443]. 
=============================================
[2019-04-04 09:22:09,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:22:09,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:22:09,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run23
[2019-04-04 09:22:16,825] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 09:22:16,833] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:22:16,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:22:16,835] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run31
[2019-04-04 09:22:16,880] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:22:16,881] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:22:16,883] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run31
[2019-04-04 09:22:16,972] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:22:16,972] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:22:16,974] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run31
[2019-04-04 09:23:11,138] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.25689405], dtype=float32), 0.28661305]
[2019-04-04 09:23:11,138] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [17.85, 64.0, 142.0, 372.0, 26.0, 26.43395864216221, 0.7250112948607067, 1.0, 0.0, 0.0]
[2019-04-04 09:23:11,138] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 09:23:11,140] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.0039494e-11 4.5987483e-10 1.2481061e-28 8.5168089e-27 7.9216493e-22
 1.0000000e+00 3.5795288e-21], sampled 0.530874881450766
[2019-04-04 09:23:29,459] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.25689405], dtype=float32), 0.28661305]
[2019-04-04 09:23:29,459] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.049833816, 90.26954200666667, 18.25736975666667, 0.0, 26.0, 25.7981616427361, 0.4716418545768583, 1.0, 1.0, 0.0]
[2019-04-04 09:23:29,459] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:23:29,460] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2100071e-11 2.3742325e-10 1.2498213e-28 1.3846524e-26 6.8305560e-22
 1.0000000e+00 4.7670802e-21], sampled 0.9981040000618473
[2019-04-04 09:23:36,089] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.25689405], dtype=float32), 0.28661305]
[2019-04-04 09:23:36,089] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.1, 88.0, 15.5, 0.0, 26.0, 25.74591497674071, 0.5234089903722774, 1.0, 1.0, 0.0]
[2019-04-04 09:23:36,089] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 09:23:36,090] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.2270616e-11 1.7772245e-10 4.0030282e-29 4.6852706e-27 4.3937493e-22
 1.0000000e+00 3.0141619e-21], sampled 0.06451018925376795
[2019-04-04 09:23:44,528] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.25689405], dtype=float32), 0.28661305]
[2019-04-04 09:23:44,528] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-8.0755261085, 69.62388254000001, 188.9246187, 43.4853645, 26.0, 24.99500084674542, 0.2680443023329264, 0.0, 1.0, 44015.27304985409]
[2019-04-04 09:23:44,529] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:23:44,530] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.74096726e-10 8.39156478e-09 1.73501574e-25 1.16630734e-23
 3.05964648e-19 1.00000000e+00 1.38868880e-18], sampled 0.5065077305316634
[2019-04-04 09:25:26,538] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 09:25:59,333] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 09:26:04,859] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 09:26:05,894] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 3000000, evaluation results [3000000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 09:26:07,809] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0146496e-11 8.3598656e-10 5.9731725e-28 3.7514333e-26 4.6511804e-22
 1.0000000e+00 5.1326044e-21], sum to 1.0000
[2019-04-04 09:26:07,809] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9249
[2019-04-04 09:26:07,852] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 26.0, 24.97026228076015, 0.3169965935311123, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 562800.0000, 
sim time next is 563400.0000, 
raw observation next is [-1.0, 80.5, 130.0, 396.0, 26.0, 24.96635215800529, 0.3143935647424858, 0.0, 1.0, 18730.72778200659], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.805, 0.43333333333333335, 0.4375690607734807, 0.6666666666666666, 0.5805293465004407, 0.6047978549141619, 0.0, 1.0, 0.089193941819079], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.0622583], dtype=float32), -1.5571872]. 
=============================================
[2019-04-04 09:26:07,978] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7515638e-11 1.4365916e-09 1.3341895e-27 8.1346474e-26 9.3732375e-21
 1.0000000e+00 4.8606971e-20], sum to 1.0000
[2019-04-04 09:26:07,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8147
[2019-04-04 09:26:08,019] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.633333333333333, 96.66666666666666, 0.0, 0.0, 26.0, 24.84276599367649, 0.2405629281880052, 0.0, 1.0, 39923.88670582083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 517200.0000, 
sim time next is 517800.0000, 
raw observation next is [3.716666666666666, 96.83333333333334, 0.0, 0.0, 26.0, 24.88396069463973, 0.2412304247799812, 0.0, 1.0, 39846.87247087662], 
processed observation next is [1.0, 1.0, 0.5655586334256695, 0.9683333333333334, 0.0, 0.0, 0.6666666666666666, 0.5736633912199774, 0.580410141593327, 0.0, 1.0, 0.18974701176607914], 
reward next is 0.8103, 
noisyNet noise sample is [array([-1.5938773], dtype=float32), -0.3860358]. 
=============================================
[2019-04-04 09:26:13,183] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4036604e-10 1.0880385e-09 2.9857979e-25 2.9520721e-24 1.6329883e-19
 1.0000000e+00 1.8123863e-19], sum to 1.0000
[2019-04-04 09:26:13,187] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7863
[2019-04-04 09:26:13,301] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.2, 39.0, 37.0, 707.0, 26.0, 26.33558924052229, 0.3557428223438013, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 401400.0000, 
sim time next is 402000.0000, 
raw observation next is [-9.100000000000001, 38.66666666666667, 34.33333333333334, 656.3333333333334, 26.0, 26.20811688913422, 0.4764981768221332, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.21052631578947364, 0.3866666666666667, 0.11444444444444447, 0.7252302025782689, 0.6666666666666666, 0.684009740761185, 0.6588327256073777, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4518354], dtype=float32), -0.7261614]. 
=============================================
[2019-04-04 09:26:13,305] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[76.070915]
 [76.41174 ]
 [76.5983  ]
 [76.948425]
 [77.32173 ]], R is [[75.97220612]
 [76.21248627]
 [76.45036316]
 [76.68585968]
 [76.91899872]].
[2019-04-04 09:26:15,053] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.8948946e-09 1.6033500e-08 2.2978322e-23 3.5783119e-22 9.8007386e-18
 1.0000000e+00 1.9488493e-17], sum to 1.0000
[2019-04-04 09:26:15,053] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5789
[2019-04-04 09:26:15,095] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.3, 44.5, 0.0, 0.0, 26.0, 25.08650079925003, 0.2612822941688109, 0.0, 1.0, 41680.1417399521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 419400.0000, 
sim time next is 420000.0000, 
raw observation next is [-10.4, 45.33333333333334, 0.0, 0.0, 26.0, 24.99432641034216, 0.2405135611217903, 0.0, 1.0, 44555.48480602348], 
processed observation next is [1.0, 0.8695652173913043, 0.1745152354570637, 0.4533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5828605341951801, 0.5801711870405968, 0.0, 1.0, 0.21216897526677847], 
reward next is 0.7878, 
noisyNet noise sample is [array([-0.5140866], dtype=float32), -2.8517108]. 
=============================================
[2019-04-04 09:26:15,107] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[69.000175]
 [70.25415 ]
 [71.60638 ]
 [73.109024]
 [74.58956 ]], R is [[68.10447693]
 [68.2249527 ]
 [68.36070251]
 [68.41487885]
 [68.49864197]].
[2019-04-04 09:26:19,765] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.6962129e-11 6.6065409e-10 5.3905331e-27 2.6723201e-25 1.4682513e-21
 1.0000000e+00 4.6731472e-21], sum to 1.0000
[2019-04-04 09:26:19,765] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5127
[2019-04-04 09:26:19,859] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 24.72632602510936, 0.1784825765669412, 1.0, 1.0, 102215.6176915765], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 502200.0000, 
sim time next is 502800.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 24.66558872402265, 0.2037345146213128, 1.0, 1.0, 108776.8538690467], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5554657270018876, 0.567911504873771, 1.0, 1.0, 0.5179850184240319], 
reward next is 0.4820, 
noisyNet noise sample is [array([-2.8051064], dtype=float32), 0.74383515]. 
=============================================
[2019-04-04 09:26:31,400] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3047375e-09 4.5950155e-09 1.2230292e-25 5.1627586e-24 1.0085487e-19
 1.0000000e+00 1.5633473e-18], sum to 1.0000
[2019-04-04 09:26:31,420] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8265
[2019-04-04 09:26:31,434] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 86.33333333333333, 0.0, 0.0, 26.0, 24.65283716192106, 0.1945068730278245, 0.0, 1.0, 42341.64749510723], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 604200.0000, 
sim time next is 604800.0000, 
raw observation next is [-3.4, 87.0, 0.0, 0.0, 26.0, 24.62049495244818, 0.1875262628143352, 0.0, 1.0, 42316.77268261398], 
processed observation next is [0.0, 0.0, 0.368421052631579, 0.87, 0.0, 0.0, 0.6666666666666666, 0.551707912704015, 0.5625087542714451, 0.0, 1.0, 0.20150844134578083], 
reward next is 0.7985, 
noisyNet noise sample is [array([0.94559354], dtype=float32), 0.383503]. 
=============================================
[2019-04-04 09:26:42,258] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4849516e-13 1.0888471e-11 1.1879411e-31 3.2809649e-29 2.0008579e-24
 1.0000000e+00 2.0800482e-23], sum to 1.0000
[2019-04-04 09:26:42,258] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0344
[2019-04-04 09:26:42,331] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 100.0, 0.0, 0.0, 26.0, 25.01040607304332, 0.3434451089343452, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 933000.0000, 
sim time next is 933600.0000, 
raw observation next is [4.600000000000001, 100.0, 0.0, 0.0, 26.0, 25.05600376380322, 0.3204468568380619, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5900277008310251, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5880003136502682, 0.6068156189460207, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2066756], dtype=float32), 0.15754358]. 
=============================================
[2019-04-04 09:26:50,482] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.5541128e-11 4.5351001e-10 2.0750817e-28 2.2247543e-26 2.1593887e-22
 1.0000000e+00 2.4301096e-21], sum to 1.0000
[2019-04-04 09:26:50,482] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6381
[2019-04-04 09:26:50,514] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 99.33333333333334, 0.0, 0.0, 26.0, 25.05745261694072, 0.3817583280821175, 0.0, 1.0, 41148.35507629056], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 940200.0000, 
sim time next is 940800.0000, 
raw observation next is [5.0, 98.66666666666667, 0.0, 0.0, 26.0, 25.09173189731098, 0.3861298812918736, 0.0, 1.0, 40655.88209889648], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.9866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5909776581092482, 0.6287099604306245, 0.0, 1.0, 0.1935994385661737], 
reward next is 0.8064, 
noisyNet noise sample is [array([2.0088239], dtype=float32), -0.82759595]. 
=============================================
[2019-04-04 09:26:51,740] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.9296072e-09 4.4282665e-08 2.9403191e-22 3.4323027e-21 7.1665486e-17
 1.0000000e+00 2.4561364e-16], sum to 1.0000
[2019-04-04 09:26:51,743] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2167
[2019-04-04 09:26:51,767] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.60675253345813, -0.0625677641537301, 0.0, 1.0, 45416.12501326669], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 435600.0000, 
sim time next is 436200.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.57892224971059, -0.06679103742811132, 0.0, 1.0, 45445.56087529077], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 0.6666666666666666, 0.46491018747588253, 0.4777363208572962, 0.0, 1.0, 0.21640743273947988], 
reward next is 0.7836, 
noisyNet noise sample is [array([0.95993173], dtype=float32), 1.2062864]. 
=============================================
[2019-04-04 09:26:54,815] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.2164640e-12 5.2702662e-11 2.1227739e-30 2.5787894e-28 3.3490831e-23
 1.0000000e+00 2.1831934e-22], sum to 1.0000
[2019-04-04 09:26:54,820] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0624
[2019-04-04 09:26:54,838] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.883333333333334, 83.0, 0.0, 0.0, 26.0, 25.50033470501452, 0.4322384680603573, 0.0, 1.0, 21544.98382040865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 965400.0000, 
sim time next is 966000.0000, 
raw observation next is [8.066666666666666, 83.0, 0.0, 0.0, 26.0, 25.42227413437334, 0.4255358411565655, 0.0, 1.0, 69439.4633271932], 
processed observation next is [1.0, 0.17391304347826086, 0.6860572483841183, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6185228445311116, 0.6418452803855218, 0.0, 1.0, 0.3306641110818724], 
reward next is 0.6693, 
noisyNet noise sample is [array([-1.2754312], dtype=float32), 1.3945125]. 
=============================================
[2019-04-04 09:26:54,864] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[91.675934]
 [91.68616 ]
 [91.77628 ]
 [91.76611 ]
 [91.78311 ]], R is [[91.7093277 ]
 [91.68964386]
 [91.77275085]
 [91.76570892]
 [91.7587204 ]].
[2019-04-04 09:27:02,988] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5933889e-13 3.2524459e-11 6.0739134e-33 1.2852701e-30 2.2549853e-25
 1.0000000e+00 3.7352438e-24], sum to 1.0000
[2019-04-04 09:27:02,992] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7797
[2019-04-04 09:27:03,081] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.600000000000001, 92.66666666666667, 22.5, 0.0, 26.0, 25.33550733677816, 0.4872072899389393, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 980400.0000, 
sim time next is 981000.0000, 
raw observation next is [9.7, 92.5, 27.0, 0.0, 26.0, 25.71156757701782, 0.5083115925931501, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7313019390581719, 0.925, 0.09, 0.0, 0.6666666666666666, 0.6426306314181517, 0.66943719753105, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6146695], dtype=float32), -1.2483423]. 
=============================================
[2019-04-04 09:27:03,089] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[100.76776 ]
 [100.40356 ]
 [ 99.801994]
 [ 98.84055 ]
 [ 97.731415]], R is [[100.92588806]
 [100.91663361]
 [100.9074707 ]
 [100.89839935]
 [100.88941956]].
[2019-04-04 09:27:03,663] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.5910249e-13 1.3759335e-10 1.3960207e-30 8.7992959e-29 3.2514458e-23
 1.0000000e+00 1.4659616e-22], sum to 1.0000
[2019-04-04 09:27:03,666] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6600
[2019-04-04 09:27:03,676] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8, 92.0, 102.0, 0.0, 26.0, 26.10111083056042, 0.5895895758227315, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1333800.0000, 
sim time next is 1334400.0000, 
raw observation next is [0.9000000000000001, 92.0, 106.1666666666667, 0.0, 26.0, 26.09989361132651, 0.5899688169544077, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.48753462603878117, 0.92, 0.353888888888889, 0.0, 0.6666666666666666, 0.6749911342772092, 0.696656272318136, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2311832], dtype=float32), -1.8895917]. 
=============================================
[2019-04-04 09:27:09,391] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.4672052e-13 8.8535512e-11 3.5692024e-31 8.4926266e-30 4.9338430e-24
 1.0000000e+00 4.7303025e-23], sum to 1.0000
[2019-04-04 09:27:09,392] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7538
[2019-04-04 09:27:09,421] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 24.84753070932601, 0.4273176523760929, 0.0, 1.0, 196650.5451134171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1024200.0000, 
sim time next is 1024800.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 24.83251530042941, 0.4730710096951243, 0.0, 1.0, 198382.711794217], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5693762750357841, 0.6576903365650414, 0.0, 1.0, 0.9446795799724619], 
reward next is 0.0553, 
noisyNet noise sample is [array([-1.0039499], dtype=float32), 0.08895675]. 
=============================================
[2019-04-04 09:27:13,268] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.9642234e-11 3.7899986e-10 3.6052381e-28 1.2410381e-26 3.4179154e-22
 1.0000000e+00 6.3818329e-21], sum to 1.0000
[2019-04-04 09:27:13,282] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2682
[2019-04-04 09:27:13,296] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.20020881970086, 0.4388543285841586, 0.0, 1.0, 38601.127304995], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1401000.0000, 
sim time next is 1401600.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.19241018278611, 0.4465236264244321, 0.0, 1.0, 38548.75284554109], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5993675152321757, 0.648841208808144, 0.0, 1.0, 0.18356548974067183], 
reward next is 0.8164, 
noisyNet noise sample is [array([1.1231294], dtype=float32), -1.7288463]. 
=============================================
[2019-04-04 09:27:18,957] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.58218402e-12 4.97981413e-11 1.18432645e-30 8.47725630e-29
 5.64226388e-23 1.00000000e+00 8.80439654e-22], sum to 1.0000
[2019-04-04 09:27:18,959] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9131
[2019-04-04 09:27:18,977] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.8, 98.0, 0.0, 0.0, 26.0, 25.40368155782577, 0.6048596937741478, 0.0, 1.0, 23113.67498513456], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1283400.0000, 
sim time next is 1284000.0000, 
raw observation next is [5.7, 98.66666666666666, 0.0, 0.0, 26.0, 25.50047278851907, 0.6086856166769475, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.6204986149584488, 0.9866666666666666, 0.0, 0.0, 0.6666666666666666, 0.6250393990432558, 0.7028952055589825, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.98786], dtype=float32), 0.8935472]. 
=============================================
[2019-04-04 09:27:19,005] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[92.522484]
 [92.82509 ]
 [93.03846 ]
 [93.23641 ]
 [93.40108 ]], R is [[92.18577576]
 [92.15385437]
 [92.03856659]
 [91.91904449]
 [91.78368378]].
[2019-04-04 09:27:25,193] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9136998e-11 5.3670962e-10 1.6732115e-28 4.4673603e-26 5.0409186e-22
 1.0000000e+00 2.8411186e-21], sum to 1.0000
[2019-04-04 09:27:25,196] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9540
[2019-04-04 09:27:25,207] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 93.0, 0.0, 0.0, 26.0, 25.50531159113831, 0.4808904607263477, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1477800.0000, 
sim time next is 1478400.0000, 
raw observation next is [2.2, 93.33333333333334, 0.0, 0.0, 26.0, 25.48070549980789, 0.4676443709675802, 0.0, 1.0, 18756.85714918138], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6233921249839908, 0.6558814569891934, 0.0, 1.0, 0.08931836737705418], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.85942554], dtype=float32), -1.41988]. 
=============================================
[2019-04-04 09:27:25,819] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.6914345e-11 4.1959222e-10 3.0652165e-28 1.8565638e-25 4.2894990e-21
 1.0000000e+00 7.0217075e-20], sum to 1.0000
[2019-04-04 09:27:25,820] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1892
[2019-04-04 09:27:25,834] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.25, 95.5, 0.0, 0.0, 26.0, 25.36973625436103, 0.514055944850092, 0.0, 1.0, 67083.72682445725], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1377000.0000, 
sim time next is 1377600.0000, 
raw observation next is [0.1666666666666667, 95.33333333333333, 0.0, 0.0, 26.0, 25.32473207092447, 0.5119309860490026, 0.0, 1.0, 53948.02395297067], 
processed observation next is [1.0, 0.9565217391304348, 0.4672206832871654, 0.9533333333333333, 0.0, 0.0, 0.6666666666666666, 0.6103943392437058, 0.6706436620163342, 0.0, 1.0, 0.25689535215700315], 
reward next is 0.7431, 
noisyNet noise sample is [array([-0.2176383], dtype=float32), -1.1369544]. 
=============================================
[2019-04-04 09:27:27,142] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1479976e-11 4.8366211e-10 4.6189819e-30 7.5105798e-28 3.0750051e-22
 1.0000000e+00 6.8881214e-22], sum to 1.0000
[2019-04-04 09:27:27,143] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5852
[2019-04-04 09:27:27,151] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 95.0, 0.0, 0.0, 26.0, 25.69235463489474, 0.599020800087236, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1647600.0000, 
sim time next is 1648200.0000, 
raw observation next is [7.1, 95.5, 0.0, 0.0, 26.0, 25.77378226754564, 0.5893740622999682, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6592797783933518, 0.955, 0.0, 0.0, 0.6666666666666666, 0.6478151889621367, 0.6964580207666561, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20253319], dtype=float32), -0.9690255]. 
=============================================
[2019-04-04 09:27:30,349] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4048907e-11 7.3991163e-10 1.4459408e-28 1.6491398e-26 1.0716621e-21
 1.0000000e+00 1.5727668e-20], sum to 1.0000
[2019-04-04 09:27:30,354] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2872
[2019-04-04 09:27:30,376] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 92.0, 0.0, 0.0, 26.0, 25.34732627950481, 0.4679777470488888, 0.0, 1.0, 58844.20187082824], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1476000.0000, 
sim time next is 1476600.0000, 
raw observation next is [2.2, 92.33333333333334, 0.0, 0.0, 26.0, 25.3610620694359, 0.4809079003057499, 0.0, 1.0, 45407.00488567488], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9233333333333335, 0.0, 0.0, 0.6666666666666666, 0.6134218391196583, 0.66030263343525, 0.0, 1.0, 0.216223832788928], 
reward next is 0.7838, 
noisyNet noise sample is [array([0.12384488], dtype=float32), -0.3206833]. 
=============================================
[2019-04-04 09:27:34,628] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.50991024e-12 1.65752578e-10 3.69760719e-30 1.19154705e-27
 4.79388248e-23 1.00000000e+00 4.73933596e-22], sum to 1.0000
[2019-04-04 09:27:34,628] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8222
[2019-04-04 09:27:34,650] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 96.66666666666666, 0.0, 0.0, 26.0, 25.20577800406416, 0.5795225045278707, 0.0, 1.0, 41817.76492747151], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1282200.0000, 
sim time next is 1282800.0000, 
raw observation next is [5.9, 97.33333333333333, 0.0, 0.0, 26.0, 25.31268299776696, 0.5923362786962912, 0.0, 1.0, 40687.42214815506], 
processed observation next is [0.0, 0.8695652173913043, 0.626038781163435, 0.9733333333333333, 0.0, 0.0, 0.6666666666666666, 0.6093902498139133, 0.6974454262320972, 0.0, 1.0, 0.19374962927692888], 
reward next is 0.8063, 
noisyNet noise sample is [array([1.5934601], dtype=float32), -0.4352745]. 
=============================================
[2019-04-04 09:27:54,136] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.1749363e-10 1.7599820e-08 2.9845369e-23 1.6038533e-21 4.7169801e-18
 1.0000000e+00 8.3224805e-17], sum to 1.0000
[2019-04-04 09:27:54,137] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9822
[2019-04-04 09:27:54,155] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 23.26335249717017, -0.09961464366134927, 0.0, 1.0, 47181.92428217013], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1842000.0000, 
sim time next is 1842600.0000, 
raw observation next is [-6.7, 78.0, 9.666666666666664, 0.0, 26.0, 23.23717175039581, -0.1040500355231768, 0.0, 1.0, 47149.18531443577], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.032222222222222215, 0.0, 0.6666666666666666, 0.4364309791996508, 0.46531665482560775, 0.0, 1.0, 0.22451993006874174], 
reward next is 0.7755, 
noisyNet noise sample is [array([0.46239248], dtype=float32), 1.4923769]. 
=============================================
[2019-04-04 09:28:02,712] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.4406767e-11 6.9557338e-10 5.6110253e-26 4.1081864e-25 4.2061013e-20
 1.0000000e+00 1.7054916e-19], sum to 1.0000
[2019-04-04 09:28:02,712] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3631
[2019-04-04 09:28:02,767] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 71.5, 0.0, 0.0, 26.0, 25.89063890896941, 0.513746972458774, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2139000.0000, 
sim time next is 2139600.0000, 
raw observation next is [-5.0, 72.0, 0.0, 0.0, 26.0, 26.08795524657295, 0.4952905593743821, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6739962705477458, 0.665096853124794, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11077651], dtype=float32), -0.071482494]. 
=============================================
[2019-04-04 09:28:03,433] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.6893948e-09 1.9481149e-08 9.0300411e-24 5.5580956e-23 3.2099103e-18
 1.0000000e+00 1.8145605e-17], sum to 1.0000
[2019-04-04 09:28:03,433] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9722
[2019-04-04 09:28:03,445] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 81.50000000000001, 0.0, 0.0, 26.0, 24.39418791201546, 0.1886713168485513, 0.0, 1.0, 42462.76034636118], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2160600.0000, 
sim time next is 2161200.0000, 
raw observation next is [-7.300000000000001, 81.0, 0.0, 0.0, 26.0, 24.34979197395594, 0.1924812192079093, 0.0, 1.0, 42484.3691143215], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.81, 0.0, 0.0, 0.6666666666666666, 0.5291493311629951, 0.5641604064026364, 0.0, 1.0, 0.20230651959200713], 
reward next is 0.7977, 
noisyNet noise sample is [array([0.93681896], dtype=float32), -0.21892291]. 
=============================================
[2019-04-04 09:28:04,568] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0396185e-09 2.3757345e-08 5.0297724e-24 8.8678966e-23 1.6230277e-18
 1.0000000e+00 5.3106302e-18], sum to 1.0000
[2019-04-04 09:28:04,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0829
[2019-04-04 09:28:04,586] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 77.0, 0.0, 0.0, 26.0, 23.86699548524596, 0.03272881416724058, 0.0, 1.0, 41939.80812001057], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2179800.0000, 
sim time next is 2180400.0000, 
raw observation next is [-6.199999999999999, 77.66666666666667, 0.0, 0.0, 26.0, 23.87992426328055, 0.02470334469524188, 0.0, 1.0, 41905.9602287776], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.4899936886067125, 0.5082344482317472, 0.0, 1.0, 0.1995521915656076], 
reward next is 0.8004, 
noisyNet noise sample is [array([-0.8082257], dtype=float32), -0.4524791]. 
=============================================
[2019-04-04 09:28:10,196] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2133584e-09 1.5953912e-08 7.3748295e-24 5.0007359e-22 1.1096962e-18
 1.0000000e+00 9.5724260e-18], sum to 1.0000
[2019-04-04 09:28:10,197] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8572
[2019-04-04 09:28:10,233] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.66666666666667, 0.0, 0.0, 26.0, 23.71107453459674, -0.007402014178009554, 0.0, 1.0, 41891.52558331854], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2182800.0000, 
sim time next is 2183400.0000, 
raw observation next is [-5.9, 77.0, 0.0, 0.0, 26.0, 23.68422525084171, -0.01091905981814091, 0.0, 1.0, 41897.64059913866], 
processed observation next is [1.0, 0.2608695652173913, 0.2991689750692521, 0.77, 0.0, 0.0, 0.6666666666666666, 0.4736854375701425, 0.496360313393953, 0.0, 1.0, 0.19951257428161265], 
reward next is 0.8005, 
noisyNet noise sample is [array([-1.2774609], dtype=float32), 0.10509699]. 
=============================================
[2019-04-04 09:28:13,678] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9707556e-10 1.7919777e-09 7.5231471e-26 2.4155954e-24 6.5224193e-20
 1.0000000e+00 3.2172761e-19], sum to 1.0000
[2019-04-04 09:28:13,681] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6169
[2019-04-04 09:28:13,747] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.3, 70.0, 29.66666666666667, 0.0, 26.0, 25.59671174053222, 0.3716831087566779, 1.0, 1.0, 88844.75374980614], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2220000.0000, 
sim time next is 2220600.0000, 
raw observation next is [-4.399999999999999, 70.5, 24.33333333333334, 0.0, 26.0, 25.5529608886911, 0.392827955358124, 1.0, 1.0, 60932.82779838773], 
processed observation next is [1.0, 0.6956521739130435, 0.3407202216066483, 0.705, 0.08111111111111113, 0.0, 0.6666666666666666, 0.629413407390925, 0.6309426517860414, 1.0, 1.0, 0.29015632284946535], 
reward next is 0.7098, 
noisyNet noise sample is [array([-0.56801033], dtype=float32), 0.21280675]. 
=============================================
[2019-04-04 09:28:13,979] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3823448e-09 9.1619281e-09 3.1099168e-26 2.1653351e-23 4.5361173e-19
 1.0000000e+00 2.9833362e-19], sum to 1.0000
[2019-04-04 09:28:13,979] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2114
[2019-04-04 09:28:14,005] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 26.0, 24.49343608127755, 0.1772573535801959, 0.0, 1.0, 43118.1546663743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2084400.0000, 
sim time next is 2085000.0000, 
raw observation next is [-5.100000000000001, 86.83333333333334, 0.0, 0.0, 26.0, 24.47069030983004, 0.1740438363997517, 0.0, 1.0, 43217.85449308831], 
processed observation next is [1.0, 0.13043478260869565, 0.32132963988919666, 0.8683333333333334, 0.0, 0.0, 0.6666666666666666, 0.5392241924858366, 0.5580146121332505, 0.0, 1.0, 0.20579930710994432], 
reward next is 0.7942, 
noisyNet noise sample is [array([1.4483061], dtype=float32), -0.671176]. 
=============================================
[2019-04-04 09:28:14,053] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.69508 ]
 [77.73187 ]
 [77.82809 ]
 [77.91643 ]
 [77.888725]], R is [[77.65581512]
 [77.67393494]
 [77.69239807]
 [77.71122742]
 [77.73031616]].
[2019-04-04 09:28:15,768] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8454173e-11 7.9262769e-10 1.3840285e-27 1.2652554e-25 1.6525203e-20
 1.0000000e+00 3.1799379e-20], sum to 1.0000
[2019-04-04 09:28:15,768] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2228
[2019-04-04 09:28:15,828] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 26.31075565217404, 0.5353325302774522, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2053200.0000, 
sim time next is 2053800.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 26.25482588447422, 0.5320916283008637, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6879021570395182, 0.6773638761002879, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0275216], dtype=float32), 0.40008435]. 
=============================================
[2019-04-04 09:28:18,443] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2968174e-10 1.5916913e-09 1.0535728e-26 2.3721469e-24 9.9775366e-20
 1.0000000e+00 2.6019832e-19], sum to 1.0000
[2019-04-04 09:28:18,444] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5883
[2019-04-04 09:28:18,488] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 54.0, 0.0, 0.0, 26.0, 25.19784842852333, 0.3335172212540468, 1.0, 1.0, 18694.94141599847], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2314800.0000, 
sim time next is 2315400.0000, 
raw observation next is [-1.283333333333333, 54.33333333333333, 0.0, 0.0, 26.0, 25.11628296940174, 0.3255899255339746, 1.0, 1.0, 63860.0655605661], 
processed observation next is [1.0, 0.8260869565217391, 0.4270544783010157, 0.5433333333333333, 0.0, 0.0, 0.6666666666666666, 0.5930235807834784, 0.6085299751779916, 1.0, 1.0, 0.30409555028841], 
reward next is 0.6959, 
noisyNet noise sample is [array([-1.0810536], dtype=float32), -0.42500234]. 
=============================================
[2019-04-04 09:28:19,540] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5269814e-10 5.6561174e-09 3.3436303e-24 3.2182761e-23 2.9059237e-19
 1.0000000e+00 3.6512902e-18], sum to 1.0000
[2019-04-04 09:28:19,542] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8406
[2019-04-04 09:28:19,583] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 70.5, 0.0, 0.0, 26.0, 25.24350301528813, 0.4005872187949717, 0.0, 1.0, 44673.05609530552], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2238600.0000, 
sim time next is 2239200.0000, 
raw observation next is [-5.6, 71.0, 0.0, 0.0, 26.0, 25.2214028081601, 0.3965567544318929, 0.0, 1.0, 44551.76223069141], 
processed observation next is [1.0, 0.9565217391304348, 0.30747922437673136, 0.71, 0.0, 0.0, 0.6666666666666666, 0.601783567346675, 0.6321855848106309, 0.0, 1.0, 0.21215124871757815], 
reward next is 0.7878, 
noisyNet noise sample is [array([0.15350711], dtype=float32), -0.18288794]. 
=============================================
[2019-04-04 09:28:30,550] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2786102e-09 2.1590623e-08 4.5424780e-24 1.6076135e-22 1.7278292e-18
 1.0000000e+00 2.6919433e-18], sum to 1.0000
[2019-04-04 09:28:30,560] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8566
[2019-04-04 09:28:30,597] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.733333333333334, 89.66666666666667, 0.0, 0.0, 26.0, 23.93692052610267, 0.03040749463099974, 0.0, 1.0, 43500.8386443672], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2263200.0000, 
sim time next is 2263800.0000, 
raw observation next is [-8.816666666666666, 90.33333333333334, 0.0, 0.0, 26.0, 23.87017311986497, 0.01826217168828542, 0.0, 1.0, 43468.55943720172], 
processed observation next is [1.0, 0.17391304347826086, 0.21837488457987075, 0.9033333333333334, 0.0, 0.0, 0.6666666666666666, 0.48918109332208076, 0.5060873905627618, 0.0, 1.0, 0.20699314017715106], 
reward next is 0.7930, 
noisyNet noise sample is [array([1.5557337], dtype=float32), -0.9139855]. 
=============================================
[2019-04-04 09:28:40,624] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.2833093e-10 1.2310910e-08 5.8507940e-25 9.6422500e-24 1.2500798e-19
 1.0000000e+00 9.6315763e-19], sum to 1.0000
[2019-04-04 09:28:40,627] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8533
[2019-04-04 09:28:40,644] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 55.33333333333333, 0.0, 0.0, 26.0, 25.40218378438302, 0.4230242712954465, 0.0, 1.0, 51810.36194285156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2324400.0000, 
sim time next is 2325000.0000, 
raw observation next is [-1.7, 55.66666666666667, 0.0, 0.0, 26.0, 25.3719994907346, 0.423621357890099, 0.0, 1.0, 60810.418440787], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.6143332908945499, 0.6412071192966997, 0.0, 1.0, 0.2895734211466047], 
reward next is 0.7104, 
noisyNet noise sample is [array([0.0420651], dtype=float32), -0.24055335]. 
=============================================
[2019-04-04 09:28:40,655] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[77.1373 ]
 [77.03448]
 [77.08717]
 [77.32192]
 [77.52972]], R is [[77.27111053]
 [77.25167847]
 [77.32518005]
 [77.46259308]
 [77.59861755]].
[2019-04-04 09:28:48,023] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7986470e-10 4.0032293e-09 1.3149450e-25 1.3924780e-23 1.3575655e-18
 1.0000000e+00 1.1530083e-18], sum to 1.0000
[2019-04-04 09:28:48,023] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4466
[2019-04-04 09:28:48,057] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 39.0, 0.0, 0.0, 26.0, 24.98778296993105, 0.2025172209535462, 0.0, 1.0, 39054.84183120468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2511000.0000, 
sim time next is 2511600.0000, 
raw observation next is [-1.7, 38.66666666666667, 0.0, 0.0, 26.0, 25.0228578503729, 0.2040245141752458, 0.0, 1.0, 38947.98409764707], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.3866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5852381541977417, 0.5680081713917486, 0.0, 1.0, 0.18546659094117654], 
reward next is 0.8145, 
noisyNet noise sample is [array([0.9076448], dtype=float32), -0.14575008]. 
=============================================
[2019-04-04 09:29:00,243] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.9468142e-09 3.6770626e-08 5.7324723e-23 1.2283339e-21 1.6956071e-17
 1.0000000e+00 1.0322980e-16], sum to 1.0000
[2019-04-04 09:29:00,243] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3757
[2019-04-04 09:29:00,260] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.633333333333333, 54.33333333333333, 0.0, 0.0, 26.0, 23.9452860327459, 0.007191863456882465, 0.0, 1.0, 43703.9362495738], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2428800.0000, 
sim time next is 2429400.0000, 
raw observation next is [-7.716666666666667, 54.66666666666667, 0.0, 0.0, 26.0, 23.89571443485238, -0.003121688060446525, 0.0, 1.0, 43783.77285202053], 
processed observation next is [0.0, 0.08695652173913043, 0.24884579870729456, 0.5466666666666667, 0.0, 0.0, 0.6666666666666666, 0.4913095362376983, 0.4989594373131845, 0.0, 1.0, 0.208494156438193], 
reward next is 0.7915, 
noisyNet noise sample is [array([1.6236718], dtype=float32), 1.7458408]. 
=============================================
[2019-04-04 09:29:12,892] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9912073e-10 2.4293367e-09 2.6115314e-25 2.3696249e-23 4.7219109e-19
 1.0000000e+00 9.8986491e-19], sum to 1.0000
[2019-04-04 09:29:12,893] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8460
[2019-04-04 09:29:12,958] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.166666666666667, 60.83333333333333, 0.0, 0.0, 26.0, 25.01217155059833, 0.3131507246121719, 0.0, 1.0, 36561.31839217097], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3006600.0000, 
sim time next is 3007200.0000, 
raw observation next is [-2.333333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 25.00823202934442, 0.3211626715836646, 0.0, 1.0, 173720.4842518793], 
processed observation next is [0.0, 0.8260869565217391, 0.3979686057248385, 0.6166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5840193357787017, 0.6070542238612215, 0.0, 1.0, 0.8272404011994252], 
reward next is 0.1728, 
noisyNet noise sample is [array([1.783287], dtype=float32), 1.1730595]. 
=============================================
[2019-04-04 09:29:15,413] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.7216705e-12 1.8594130e-10 6.9235815e-29 1.9698119e-27 8.6406285e-22
 1.0000000e+00 4.0223317e-21], sum to 1.0000
[2019-04-04 09:29:15,416] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2472
[2019-04-04 09:29:15,462] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 52.5, 174.0, 508.0, 26.0, 26.01832948085698, 0.4521229902171777, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2802600.0000, 
sim time next is 2803200.0000, 
raw observation next is [-1.666666666666667, 51.66666666666666, 165.8333333333333, 550.5, 26.0, 26.02863145029203, 0.45587645292657, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4164358264081256, 0.5166666666666666, 0.5527777777777776, 0.6082872928176796, 0.6666666666666666, 0.669052620857669, 0.65195881764219, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23368986], dtype=float32), -1.0834233]. 
=============================================
[2019-04-04 09:29:20,770] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2816086e-11 1.1010487e-09 9.0823264e-28 5.9723131e-26 7.6807420e-21
 1.0000000e+00 9.0651555e-21], sum to 1.0000
[2019-04-04 09:29:20,770] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3179
[2019-04-04 09:29:20,789] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 49.5, 128.3333333333333, 178.6666666666667, 26.0, 26.04084797719316, 0.4618623417278385, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2649000.0000, 
sim time next is 2649600.0000, 
raw observation next is [0.5, 50.0, 115.0, 165.0, 26.0, 25.96363503390269, 0.4592148332300778, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4764542936288089, 0.5, 0.38333333333333336, 0.18232044198895028, 0.6666666666666666, 0.6636362528252242, 0.6530716110766926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34672183], dtype=float32), 0.16318652]. 
=============================================
[2019-04-04 09:29:24,981] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.8756447e-11 1.9820030e-09 7.7256191e-27 4.0251169e-25 2.6445909e-20
 1.0000000e+00 2.2361711e-20], sum to 1.0000
[2019-04-04 09:29:24,988] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1885
[2019-04-04 09:29:25,037] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 243.0, 240.6666666666667, 26.0, 24.98313058345538, 0.3449925514396718, 0.0, 1.0, 33477.26784447939], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2978400.0000, 
sim time next is 2979000.0000, 
raw observation next is [-3.0, 65.0, 256.0, 284.0, 26.0, 24.96451159143248, 0.3548766270479951, 0.0, 1.0, 49118.04998012708], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.8533333333333334, 0.3138121546961326, 0.6666666666666666, 0.5803759659527067, 0.6182922090159984, 0.0, 1.0, 0.23389547609584324], 
reward next is 0.7661, 
noisyNet noise sample is [array([0.39591193], dtype=float32), 0.35232052]. 
=============================================
[2019-04-04 09:29:25,041] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[82.52549 ]
 [81.93622 ]
 [81.52346 ]
 [81.23829 ]
 [81.124306]], R is [[83.19934082]
 [83.20793152]
 [83.28661346]
 [83.36449432]
 [83.44158173]].
[2019-04-04 09:29:27,565] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.07720066e-11 9.42448480e-11 5.71575322e-28 3.34128107e-26
 5.27036027e-21 1.00000000e+00 1.75756267e-20], sum to 1.0000
[2019-04-04 09:29:27,567] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0378
[2019-04-04 09:29:27,620] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.133333333333333, 54.33333333333333, 96.66666666666667, 693.3333333333334, 26.0, 26.77187813220095, 0.5559290625408814, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2731800.0000, 
sim time next is 2732400.0000, 
raw observation next is [-4.0, 54.0, 94.0, 673.5, 26.0, 26.03710124918757, 0.5715649199430146, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.54, 0.31333333333333335, 0.7441988950276243, 0.6666666666666666, 0.6697584374322977, 0.6905216399810049, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.649597], dtype=float32), -0.8684517]. 
=============================================
[2019-04-04 09:29:31,889] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.78085799e-13 1.66242381e-10 1.15412834e-29 5.63566463e-29
 1.22124087e-22 1.00000000e+00 5.96237417e-23], sum to 1.0000
[2019-04-04 09:29:31,889] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8732
[2019-04-04 09:29:31,926] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 80.33333333333333, 114.3333333333333, 821.1666666666667, 26.0, 26.59063479999635, 0.746350289677899, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3242400.0000, 
sim time next is 3243000.0000, 
raw observation next is [-2.0, 82.66666666666667, 113.6666666666667, 819.3333333333334, 26.0, 26.53459369562999, 0.6214020365154822, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.8266666666666667, 0.378888888888889, 0.905340699815838, 0.6666666666666666, 0.7112161413024992, 0.7071340121718274, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9582534], dtype=float32), -0.40929514]. 
=============================================
[2019-04-04 09:29:31,944] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.7113  ]
 [90.762085]
 [90.83055 ]
 [90.916374]
 [91.05372 ]], R is [[90.73040009]
 [90.82309723]
 [90.91486359]
 [91.00571442]
 [91.09565735]].
[2019-04-04 09:29:34,959] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3899860e-09 3.1823358e-08 8.4588001e-24 2.2163389e-22 1.6267157e-18
 1.0000000e+00 2.7239986e-17], sum to 1.0000
[2019-04-04 09:29:34,960] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6930
[2019-04-04 09:29:34,979] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 66.16666666666667, 28.33333333333333, 166.3333333333333, 26.0, 23.57482058342619, -0.03758021454588736, 0.0, 1.0, 40758.50613142909], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3052200.0000, 
sim time next is 3052800.0000, 
raw observation next is [-6.0, 64.0, 42.0, 214.5, 26.0, 23.5599079640424, -0.02591098440141355, 0.0, 1.0, 40645.80537820391], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.14, 0.23701657458563535, 0.6666666666666666, 0.4633256636701999, 0.4913630051995288, 0.0, 1.0, 0.1935514541819234], 
reward next is 0.8064, 
noisyNet noise sample is [array([0.12949778], dtype=float32), 0.14205049]. 
=============================================
[2019-04-04 09:29:40,609] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2630347e-10 3.7757991e-10 4.1801578e-27 3.9484670e-26 1.3833624e-21
 1.0000000e+00 8.3006177e-21], sum to 1.0000
[2019-04-04 09:29:40,609] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6768
[2019-04-04 09:29:40,635] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 94.66666666666666, 0.0, 0.0, 26.0, 25.20214959492505, 0.4525492672318814, 0.0, 1.0, 40946.90713709512], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3220800.0000, 
sim time next is 3221400.0000, 
raw observation next is [-3.0, 93.33333333333334, 0.0, 0.0, 26.0, 25.15208248237416, 0.4450180516566997, 0.0, 1.0, 41014.71892265779], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.9333333333333335, 0.0, 0.0, 0.6666666666666666, 0.5960068735311799, 0.6483393505522332, 0.0, 1.0, 0.19530818534598948], 
reward next is 0.8047, 
noisyNet noise sample is [array([-2.0786302], dtype=float32), 0.20447828]. 
=============================================
[2019-04-04 09:29:43,347] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.7994171e-13 8.0701425e-11 9.6082937e-30 1.9372385e-28 1.6847314e-23
 1.0000000e+00 1.7409853e-22], sum to 1.0000
[2019-04-04 09:29:43,349] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0621
[2019-04-04 09:29:43,375] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 26.0, 25.75898767244954, 0.6537104408804584, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3183000.0000, 
sim time next is 3183600.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 26.0, 25.67056835549376, 0.6506129681696556, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.6666666666666666, 0.63921402962448, 0.7168709893898852, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([0.9297935], dtype=float32), -0.38809627]. 
=============================================
[2019-04-04 09:29:46,077] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.19414225e-09 3.16606155e-08 2.96348063e-23 2.43995582e-21
 7.68998879e-18 1.00000000e+00 1.06979765e-17], sum to 1.0000
[2019-04-04 09:29:46,077] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1257
[2019-04-04 09:29:46,185] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.0, 78.66666666666667, 0.0, 0.0, 26.0, 23.7971153974031, 0.1495399085657987, 1.0, 1.0, 202374.3336252436], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3309600.0000, 
sim time next is 3310200.0000, 
raw observation next is [-11.0, 80.0, 2.0, 94.0, 26.0, 24.15900388804849, 0.2604731611055239, 1.0, 1.0, 203136.9626396077], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.8, 0.006666666666666667, 0.10386740331491713, 0.6666666666666666, 0.5132503240040407, 0.5868243870351746, 1.0, 1.0, 0.9673188697124175], 
reward next is 0.0327, 
noisyNet noise sample is [array([-0.4675842], dtype=float32), 0.707854]. 
=============================================
[2019-04-04 09:29:52,111] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.1135724e-10 1.8747055e-09 1.0954212e-26 9.2145689e-25 3.8904530e-20
 1.0000000e+00 4.9139078e-19], sum to 1.0000
[2019-04-04 09:29:52,126] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4174
[2019-04-04 09:29:52,140] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 72.0, 0.0, 0.0, 26.0, 25.4638917790399, 0.455649773914276, 0.0, 1.0, 143832.2998297152], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3537000.0000, 
sim time next is 3537600.0000, 
raw observation next is [-1.0, 70.0, 0.0, 0.0, 26.0, 25.27228074665354, 0.4554576340415566, 0.0, 1.0, 141578.6204448651], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6060233955544616, 0.6518192113471856, 0.0, 1.0, 0.67418390688031], 
reward next is 0.3258, 
noisyNet noise sample is [array([0.3571043], dtype=float32), 0.9024888]. 
=============================================
[2019-04-04 09:29:57,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.3349768e-12 4.3736087e-10 9.1888555e-29 2.5448121e-27 9.9501598e-22
 1.0000000e+00 1.3974604e-21], sum to 1.0000
[2019-04-04 09:29:57,218] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5578
[2019-04-04 09:29:57,232] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.333333333333334, 31.0, 115.1666666666667, 807.1666666666666, 26.0, 25.68359359334551, 0.5004311870657622, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3669600.0000, 
sim time next is 3670200.0000, 
raw observation next is [8.0, 34.5, 116.0, 816.0, 26.0, 25.6867894692689, 0.4954896167419909, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6842105263157896, 0.345, 0.38666666666666666, 0.901657458563536, 0.6666666666666666, 0.6405657891057416, 0.6651632055806637, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6240703], dtype=float32), -1.5238458]. 
=============================================
[2019-04-04 09:30:01,344] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3770022e-09 7.3186666e-09 1.6089125e-24 3.3416047e-23 3.1958960e-18
 1.0000000e+00 1.1152518e-17], sum to 1.0000
[2019-04-04 09:30:01,344] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4681
[2019-04-04 09:30:01,359] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.84008398536729, 0.246376479061709, 0.0, 1.0, 37931.16879212891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3024000.0000, 
sim time next is 3024600.0000, 
raw observation next is [-4.166666666666667, 66.0, 0.0, 0.0, 26.0, 24.80523955170657, 0.2405401219831908, 0.0, 1.0, 37891.24811552476], 
processed observation next is [0.0, 0.0, 0.3471837488457987, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5671032959755475, 0.5801800406610637, 0.0, 1.0, 0.18043451483583217], 
reward next is 0.8196, 
noisyNet noise sample is [array([0.8703923], dtype=float32), -0.19914904]. 
=============================================
[2019-04-04 09:30:03,759] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.7306852e-10 1.2463435e-09 4.6447857e-27 3.4454533e-25 1.4622879e-20
 1.0000000e+00 3.6127281e-20], sum to 1.0000
[2019-04-04 09:30:03,764] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6176
[2019-04-04 09:30:03,778] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 76.0, 0.0, 0.0, 26.0, 25.70721875866396, 0.5073779360134476, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3535800.0000, 
sim time next is 3536400.0000, 
raw observation next is [-1.0, 74.0, 0.0, 0.0, 26.0, 25.68242989942167, 0.4713688236872011, 0.0, 1.0, 21402.0364450249], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6402024916184725, 0.657122941229067, 0.0, 1.0, 0.10191445926202333], 
reward next is 0.8981, 
noisyNet noise sample is [array([-1.9066895], dtype=float32), 1.2787046]. 
=============================================
[2019-04-04 09:30:05,171] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4954489e-10 1.9538857e-09 7.0347423e-26 5.3342040e-25 3.2246098e-20
 1.0000000e+00 1.0763871e-19], sum to 1.0000
[2019-04-04 09:30:05,177] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9357
[2019-04-04 09:30:05,232] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.333333333333333, 67.0, 0.0, 0.0, 26.0, 25.11960373549391, 0.4638115643804243, 0.0, 1.0, 198787.7383368569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3788400.0000, 
sim time next is 3789000.0000, 
raw observation next is [-2.5, 68.0, 0.0, 0.0, 26.0, 25.11585148557431, 0.5037857409494001, 0.0, 1.0, 170408.7271059874], 
processed observation next is [1.0, 0.8695652173913043, 0.39335180055401664, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5929876237978592, 0.6679285803164667, 0.0, 1.0, 0.8114701290761305], 
reward next is 0.1885, 
noisyNet noise sample is [array([1.3089966], dtype=float32), 0.6393934]. 
=============================================
[2019-04-04 09:30:05,240] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[80.216484]
 [80.36994 ]
 [81.47475 ]
 [80.96786 ]
 [80.586685]], R is [[79.31240082]
 [78.57266998]
 [78.50273895]
 [78.41248322]
 [78.34293365]].
[2019-04-04 09:30:07,651] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5984620e-11 1.0170245e-09 2.4670421e-28 8.1300205e-27 1.3808738e-21
 1.0000000e+00 6.1877738e-21], sum to 1.0000
[2019-04-04 09:30:07,652] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8093
[2019-04-04 09:30:07,669] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.733333333333333, 100.0, 0.0, 0.0, 26.0, 25.19728202977266, 0.2924963568600783, 0.0, 1.0, 53979.69019559796], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3126000.0000, 
sim time next is 3126600.0000, 
raw observation next is [2.8, 100.0, 0.0, 0.0, 26.0, 25.24630562746479, 0.2965724311340123, 0.0, 1.0, 53902.00188430025], 
processed observation next is [1.0, 0.17391304347826086, 0.5401662049861496, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6038588022887325, 0.5988574770446707, 0.0, 1.0, 0.2566761994490488], 
reward next is 0.7433, 
noisyNet noise sample is [array([2.287649], dtype=float32), -1.0066304]. 
=============================================
[2019-04-04 09:30:08,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5182706e-11 1.0449603e-09 1.2734137e-26 2.1117545e-25 2.1451217e-20
 1.0000000e+00 7.3460928e-20], sum to 1.0000
[2019-04-04 09:30:08,843] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2261
[2019-04-04 09:30:08,857] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.166666666666667, 57.5, 47.66666666666667, 403.0000000000001, 26.0, 25.45355402273856, 0.4494696927774073, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3689400.0000, 
sim time next is 3690000.0000, 
raw observation next is [4.0, 59.0, 39.5, 343.5, 26.0, 25.4443545988156, 0.4364071670294322, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.13166666666666665, 0.37955801104972375, 0.6666666666666666, 0.6203628832346334, 0.6454690556764774, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35175967], dtype=float32), 0.65700895]. 
=============================================
[2019-04-04 09:30:08,873] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.38227]
 [80.84911]
 [81.22324]
 [81.47137]
 [81.71595]], R is [[80.05205536]
 [80.25153351]
 [80.44902039]
 [80.64453125]
 [80.83808899]].
[2019-04-04 09:30:12,950] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.9919708e-09 3.5997434e-08 2.1049848e-23 2.2818407e-21 8.5272188e-18
 1.0000000e+00 5.2083930e-17], sum to 1.0000
[2019-04-04 09:30:12,950] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1988
[2019-04-04 09:30:12,972] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.833333333333334, 57.16666666666667, 0.0, 0.0, 26.0, 24.90200192413804, 0.3278481915907205, 0.0, 1.0, 44110.67856541937], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3973800.0000, 
sim time next is 3974400.0000, 
raw observation next is [-10.0, 58.0, 0.0, 0.0, 26.0, 24.85356004560278, 0.3175795989767824, 0.0, 1.0, 44114.88235627265], 
processed observation next is [1.0, 0.0, 0.18559556786703602, 0.58, 0.0, 0.0, 0.6666666666666666, 0.5711300038002317, 0.6058598663255942, 0.0, 1.0, 0.21007086836320307], 
reward next is 0.7899, 
noisyNet noise sample is [array([-1.880545], dtype=float32), 0.25262192]. 
=============================================
[2019-04-04 09:30:27,975] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.5184215e-11 7.8957219e-10 4.4920660e-28 6.2436208e-26 4.7624996e-21
 1.0000000e+00 2.8163125e-20], sum to 1.0000
[2019-04-04 09:30:27,975] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0520
[2019-04-04 09:30:28,104] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.733333333333333, 58.66666666666667, 62.33333333333334, 501.3333333333334, 26.0, 25.57079570519402, 0.4588003017261277, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4293600.0000, 
sim time next is 4294200.0000, 
raw observation next is [6.666666666666666, 59.33333333333333, 54.66666666666667, 446.6666666666667, 26.0, 25.56755853784593, 0.4576600133920009, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6472760849492153, 0.5933333333333333, 0.18222222222222223, 0.49355432780847147, 0.6666666666666666, 0.6306298781538274, 0.6525533377973337, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5711954], dtype=float32), -0.54962206]. 
=============================================
[2019-04-04 09:30:35,072] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-04 09:30:35,074] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:30:35,074] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:30:35,075] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:30:35,076] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run32
[2019-04-04 09:30:35,106] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:30:35,114] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:30:35,114] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:30:35,123] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run32
[2019-04-04 09:30:35,146] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run32
[2019-04-04 09:32:01,812] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.25938138], dtype=float32), 0.28954744]
[2019-04-04 09:32:01,812] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [8.033333333333335, 93.0, 17.33333333333333, 41.66666666666666, 26.0, 25.52880274956879, 0.5709053052129477, 0.0, 1.0, 18746.9824491047]
[2019-04-04 09:32:01,813] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 09:32:01,813] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.7796229e-12 2.7552349e-10 4.1085833e-30 6.5328516e-28 8.6853358e-23
 1.0000000e+00 6.7948737e-22], sampled 0.8374427686428699
[2019-04-04 09:33:28,626] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.25938138], dtype=float32), 0.28954744]
[2019-04-04 09:33:28,627] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.6076346466666667, 74.81804984333334, 0.0, 0.0, 26.0, 25.30259447372659, 0.4188832233093664, 0.0, 1.0, 46501.24535499237]
[2019-04-04 09:33:28,627] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:33:28,628] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.0448322e-10 8.8131369e-10 7.1914571e-28 6.2264009e-26 3.9081069e-21
 1.0000000e+00 2.7138713e-20], sampled 0.4630845379667079
[2019-04-04 09:33:43,533] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 09:34:12,659] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 09:34:18,817] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7307 275786560.0346 1233.1784
[2019-04-04 09:34:19,859] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 3100000, evaluation results [3100000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.730666501954, 275786560.0345853, 1233.1784155230887]
[2019-04-04 09:34:36,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7899258e-10 9.0346237e-09 4.9855639e-26 4.3622138e-24 1.2357657e-19
 1.0000000e+00 4.2697494e-19], sum to 1.0000
[2019-04-04 09:34:36,460] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3104
[2019-04-04 09:34:36,521] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 36.66666666666666, 17.66666666666666, 26.0, 25.32360431052815, 0.3212496792100126, 0.0, 1.0, 39076.68303164816], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4261800.0000, 
sim time next is 4262400.0000, 
raw observation next is [3.0, 49.0, 55.0, 26.5, 26.0, 25.32209189538507, 0.3258617240862324, 0.0, 1.0, 39004.32224053835], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.49, 0.18333333333333332, 0.029281767955801105, 0.6666666666666666, 0.6101743246154226, 0.6086205746954109, 0.0, 1.0, 0.18573486781208737], 
reward next is 0.8143, 
noisyNet noise sample is [array([-0.1846298], dtype=float32), -0.07346104]. 
=============================================
[2019-04-04 09:34:52,193] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9856711e-11 3.6148973e-10 5.0201011e-28 4.8305312e-27 2.5440919e-21
 1.0000000e+00 2.3625870e-21], sum to 1.0000
[2019-04-04 09:34:52,193] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1601
[2019-04-04 09:34:52,204] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.35796322233101, 0.4543577967429419, 0.0, 1.0, 20045.06960445028], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4473600.0000, 
sim time next is 4474200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.1653486370082, 0.4408230575254217, 0.0, 1.0, 57613.37984855966], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5971123864173501, 0.6469410191751406, 0.0, 1.0, 0.2743494278502841], 
reward next is 0.7257, 
noisyNet noise sample is [array([0.07733241], dtype=float32), 0.13891524]. 
=============================================
[2019-04-04 09:34:55,969] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0434253e-12 8.6593577e-10 1.2236842e-30 8.3757172e-28 2.7380232e-23
 1.0000000e+00 9.8004191e-22], sum to 1.0000
[2019-04-04 09:34:55,970] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6231
[2019-04-04 09:34:56,041] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 52.5, 0.0, 26.0, 25.68803751441244, 0.5056493258407423, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4695600.0000, 
sim time next is 4696200.0000, 
raw observation next is [0.0, 92.0, 63.0, 0.0, 26.0, 25.96363973913056, 0.5208930878122416, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.21, 0.0, 0.6666666666666666, 0.6636366449275467, 0.6736310292707471, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4845163], dtype=float32), 1.2834665]. 
=============================================
[2019-04-04 09:35:08,594] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6509879e-12 1.6999069e-10 8.3207787e-29 9.4510128e-27 4.7774547e-22
 1.0000000e+00 5.5514474e-21], sum to 1.0000
[2019-04-04 09:35:08,596] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3051
[2019-04-04 09:35:08,630] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 78.0, 45.0, 9.166666666666664, 26.0, 25.79391655572734, 0.5196455533099833, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4466400.0000, 
sim time next is 4467000.0000, 
raw observation next is [0.0, 78.0, 41.0, 18.33333333333333, 26.0, 25.09018209962402, 0.4819766499755307, 1.0, 1.0, 129388.4722897834], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.13666666666666666, 0.020257826887661135, 0.6666666666666666, 0.5908485083020016, 0.6606588833251769, 1.0, 1.0, 0.6161355823323019], 
reward next is 0.3839, 
noisyNet noise sample is [array([1.2341006], dtype=float32), 0.5812149]. 
=============================================
[2019-04-04 09:35:08,650] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.506905]
 [86.61663 ]
 [86.79798 ]
 [86.89634 ]
 [87.08082 ]], R is [[86.59043884]
 [86.72453308]
 [86.85729218]
 [86.98872375]
 [87.11883545]].
[2019-04-04 09:35:17,175] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:17,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:17,208] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run24
[2019-04-04 09:35:17,290] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7180850e-11 1.8164169e-09 1.5239770e-27 5.5988380e-26 3.7555090e-21
 1.0000000e+00 3.0208743e-20], sum to 1.0000
[2019-04-04 09:35:17,294] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7987
[2019-04-04 09:35:17,347] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.833333333333333, 44.83333333333334, 62.00000000000001, 373.3333333333334, 26.0, 25.18660057762263, 0.2710633768556437, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4954200.0000, 
sim time next is 4954800.0000, 
raw observation next is [-1.666666666666667, 43.66666666666667, 77.5, 466.6666666666667, 26.0, 25.11934407536977, 0.2968361437051864, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4164358264081256, 0.4366666666666667, 0.25833333333333336, 0.5156537753222836, 0.6666666666666666, 0.5932786729474809, 0.5989453812350621, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06524704], dtype=float32), -1.1253586]. 
=============================================
[2019-04-04 09:35:17,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:17,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:17,863] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run24
[2019-04-04 09:35:20,371] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:20,371] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:20,375] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run24
[2019-04-04 09:35:20,718] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4558243e-11 5.9053606e-10 5.5593139e-27 1.2027969e-24 3.3462259e-21
 1.0000000e+00 3.2337993e-20], sum to 1.0000
[2019-04-04 09:35:20,719] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0572
[2019-04-04 09:35:20,770] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 44.83333333333334, 275.0, 388.6666666666666, 26.0, 25.03669436759947, 0.3590309517171126, 0.0, 1.0, 20252.89748689447], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4885800.0000, 
sim time next is 4886400.0000, 
raw observation next is [1.6, 44.66666666666667, 273.5, 388.3333333333334, 26.0, 25.05357638366955, 0.3642851610570263, 0.0, 1.0, 18698.70195062321], 
processed observation next is [0.0, 0.5652173913043478, 0.5069252077562327, 0.4466666666666667, 0.9116666666666666, 0.42909760589318613, 0.6666666666666666, 0.5877980319724626, 0.6214283870190088, 0.0, 1.0, 0.08904143786011053], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.30819705], dtype=float32), -0.049673837]. 
=============================================
[2019-04-04 09:35:21,643] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:21,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:21,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run24
[2019-04-04 09:35:26,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:26,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:26,259] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run24
[2019-04-04 09:35:26,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:26,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:26,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run24
[2019-04-04 09:35:33,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:33,354] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:33,363] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run24
[2019-04-04 09:35:33,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:33,844] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:33,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run24
[2019-04-04 09:35:34,601] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:34,601] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:34,604] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run24
[2019-04-04 09:35:37,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:37,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:37,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run24
[2019-04-04 09:35:37,455] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:37,455] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:37,463] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run24
[2019-04-04 09:35:38,049] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.6797409e-11 7.1661505e-10 3.0832326e-26 2.1960543e-24 2.0931132e-20
 1.0000000e+00 1.3076159e-19], sum to 1.0000
[2019-04-04 09:35:38,049] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4580
[2019-04-04 09:35:38,150] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.299999999999999, 62.66666666666666, 0.0, 0.0, 26.0, 25.08837852912905, 0.3535037538711989, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 150600.0000, 
sim time next is 151200.0000, 
raw observation next is [-7.3, 61.0, 0.0, 0.0, 26.0, 25.26711745692474, 0.3632878285078789, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.26038781163434904, 0.61, 0.0, 0.0, 0.6666666666666666, 0.605593121410395, 0.6210959428359596, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4058356], dtype=float32), 1.0607963]. 
=============================================
[2019-04-04 09:35:39,179] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:39,179] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:39,218] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run24
[2019-04-04 09:35:43,240] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.1703851e-11 9.7203723e-10 1.9266639e-27 4.9088901e-25 4.6795691e-21
 1.0000000e+00 2.0516503e-20], sum to 1.0000
[2019-04-04 09:35:43,241] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0666
[2019-04-04 09:35:43,256] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.333333333333333, 34.33333333333333, 0.0, 0.0, 26.0, 25.75078740043274, 0.578327611414514, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5002800.0000, 
sim time next is 5003400.0000, 
raw observation next is [3.166666666666667, 35.66666666666667, 0.0, 0.0, 26.0, 25.78685543002248, 0.5740874304693787, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5503231763619576, 0.3566666666666667, 0.0, 0.0, 0.6666666666666666, 0.64890461916854, 0.6913624768231262, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2410658], dtype=float32), -2.0086288]. 
=============================================
[2019-04-04 09:35:48,438] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.2598102e-11 1.8287997e-09 6.4994466e-27 6.6845702e-25 3.3869878e-21
 1.0000000e+00 5.1313151e-20], sum to 1.0000
[2019-04-04 09:35:48,439] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5923
[2019-04-04 09:35:48,492] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 65.0, 141.0, 0.0, 26.0, 25.23466149185509, 0.214864659259422, 1.0, 1.0, 25197.95894102298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 216000.0000, 
sim time next is 216600.0000, 
raw observation next is [-4.916666666666667, 65.0, 137.0, 0.0, 26.0, 25.18399049226423, 0.2227902878324182, 1.0, 1.0, 53855.1140032996], 
processed observation next is [1.0, 0.5217391304347826, 0.32640812557710064, 0.65, 0.45666666666666667, 0.0, 0.6666666666666666, 0.5986658743553525, 0.5742634292774728, 1.0, 1.0, 0.25645292382523616], 
reward next is 0.7435, 
noisyNet noise sample is [array([1.0025321], dtype=float32), -0.9866298]. 
=============================================
[2019-04-04 09:35:50,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:50,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:50,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run24
[2019-04-04 09:35:52,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8130206e-12 1.3972043e-10 3.8244237e-30 9.6772689e-29 1.6332964e-23
 1.0000000e+00 3.3310542e-23], sum to 1.0000
[2019-04-04 09:35:52,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9630
[2019-04-04 09:35:52,691] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.333333333333334, 24.16666666666667, 120.0, 861.6666666666666, 26.0, 27.69511786659898, 0.9321891198165847, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5058600.0000, 
sim time next is 5059200.0000, 
raw observation next is [9.666666666666668, 23.33333333333334, 119.0, 860.8333333333334, 26.0, 27.81105277073525, 0.9572472419460066, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.7303785780240075, 0.2333333333333334, 0.39666666666666667, 0.9511970534069982, 0.6666666666666666, 0.8175877308946043, 0.8190824139820022, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1558664], dtype=float32), -2.0265121]. 
=============================================
[2019-04-04 09:35:56,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:56,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:56,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run24
[2019-04-04 09:35:58,259] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:35:58,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:35:58,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run24
[2019-04-04 09:36:10,607] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5470397e-11 1.4463077e-09 5.5739846e-28 4.0980729e-26 2.4109685e-21
 1.0000000e+00 1.2259425e-20], sum to 1.0000
[2019-04-04 09:36:10,608] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8578
[2019-04-04 09:36:10,663] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.8, 83.33333333333334, 44.66666666666667, 0.0, 26.0, 24.50110650266059, 0.1817570585806212, 0.0, 1.0, 20765.34044412126], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 56400.0000, 
sim time next is 57000.0000, 
raw observation next is [6.699999999999999, 82.66666666666667, 39.33333333333334, 0.0, 26.0, 24.51504275655866, 0.1854598810933138, 0.0, 1.0, 22896.91173825506], 
processed observation next is [0.0, 0.6521739130434783, 0.6481994459833795, 0.8266666666666667, 0.13111111111111115, 0.0, 0.6666666666666666, 0.5429202297132217, 0.5618199603644379, 0.0, 1.0, 0.10903291303930981], 
reward next is 0.8910, 
noisyNet noise sample is [array([-1.6972121], dtype=float32), 0.038674816]. 
=============================================
[2019-04-04 09:36:10,673] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[86.26161 ]
 [86.53033 ]
 [86.698296]
 [86.795586]
 [86.87281 ]], R is [[86.0437851 ]
 [86.08446503]
 [86.03619385]
 [85.93977356]
 [85.82391357]].
[2019-04-04 09:36:11,621] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2083024e-09 6.7674466e-09 1.9301232e-25 1.8505061e-23 4.3170649e-19
 1.0000000e+00 9.2365924e-19], sum to 1.0000
[2019-04-04 09:36:11,622] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6193
[2019-04-04 09:36:11,657] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 26.0, 24.00102754010927, 0.04466062116917624, 0.0, 1.0, 45465.48674201586], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 265800.0000, 
sim time next is 266400.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.00638756047659, 0.03476890688988955, 0.0, 1.0, 45501.28542248489], 
processed observation next is [1.0, 0.08695652173913043, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5005322967063824, 0.5115896356299632, 0.0, 1.0, 0.21667278772611853], 
reward next is 0.7833, 
noisyNet noise sample is [array([0.30639514], dtype=float32), 0.88348466]. 
=============================================
[2019-04-04 09:36:12,096] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0151770e-10 4.1510053e-09 4.6820747e-26 2.6598323e-24 7.1844119e-20
 1.0000000e+00 5.7850308e-19], sum to 1.0000
[2019-04-04 09:36:12,097] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4850
[2019-04-04 09:36:12,154] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8, 32.66666666666666, 114.8333333333333, 0.0, 26.0, 25.27989450909949, 0.2358182550522447, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 481200.0000, 
sim time next is 481800.0000, 
raw observation next is [-0.7, 33.83333333333334, 110.6666666666667, 0.0, 26.0, 25.51264619098174, 0.2560989948331419, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.443213296398892, 0.33833333333333343, 0.368888888888889, 0.0, 0.6666666666666666, 0.6260538492484784, 0.5853663316110472, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-3.1691072], dtype=float32), 0.72601473]. 
=============================================
[2019-04-04 09:36:14,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:36:14,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:36:14,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run24
[2019-04-04 09:36:14,419] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.49026097e-10 1.76894654e-09 1.33922364e-25 1.70380308e-24
 1.04772907e-19 1.00000000e+00 8.60235525e-20], sum to 1.0000
[2019-04-04 09:36:14,420] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9849
[2019-04-04 09:36:14,502] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.8, 63.66666666666666, 92.33333333333334, 426.0, 26.0, 25.75508582514959, 0.3512868785192806, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 294600.0000, 
sim time next is 295200.0000, 
raw observation next is [-11.7, 63.0, 91.0, 447.5, 26.0, 25.79444508085086, 0.3558713007912011, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.13850415512465375, 0.63, 0.30333333333333334, 0.494475138121547, 0.6666666666666666, 0.649537090070905, 0.6186237669304003, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7448689], dtype=float32), 1.0604964]. 
=============================================
[2019-04-04 09:36:14,590] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.8612424e-10 3.8261425e-09 1.8813538e-24 2.1276538e-23 2.3338101e-18
 1.0000000e+00 1.8271595e-18], sum to 1.0000
[2019-04-04 09:36:14,590] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1018
[2019-04-04 09:36:14,674] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.783333333333333, 32.16666666666666, 74.0, 0.0, 26.0, 25.56416622334733, 0.2001961524814204, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 467400.0000, 
sim time next is 468000.0000, 
raw observation next is [-4.5, 32.0, 80.0, 0.0, 26.0, 25.54764880981215, 0.1979969723767957, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3379501385041552, 0.32, 0.26666666666666666, 0.0, 0.6666666666666666, 0.6289707341510123, 0.5659989907922652, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.017123], dtype=float32), -0.97417665]. 
=============================================
[2019-04-04 09:36:14,682] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[74.06485]
 [73.88763]
 [73.73366]
 [73.67966]
 [73.62584]], R is [[74.47547913]
 [74.73072815]
 [74.98342133]
 [75.23358917]
 [75.48125458]].
[2019-04-04 09:36:25,350] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.9830360e-10 7.1025927e-09 4.5735296e-25 1.1050804e-23 2.8102931e-19
 1.0000000e+00 8.3588385e-19], sum to 1.0000
[2019-04-04 09:36:25,350] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4108
[2019-04-04 09:36:25,396] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 71.0, 77.0, 25.5, 26.0, 25.13008609069737, 0.2543150253935838, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 637200.0000, 
sim time next is 637800.0000, 
raw observation next is [-3.9, 70.0, 96.33333333333334, 34.00000000000001, 26.0, 25.23761189391551, 0.2571782653964879, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.7, 0.3211111111111111, 0.03756906077348067, 0.6666666666666666, 0.6031343244929591, 0.585726088465496, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46223217], dtype=float32), 1.8360459]. 
=============================================
[2019-04-04 09:36:28,544] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6865584e-11 1.0591793e-09 2.7455706e-26 1.5910176e-24 2.1873899e-19
 1.0000000e+00 1.1758340e-19], sum to 1.0000
[2019-04-04 09:36:28,544] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3642
[2019-04-04 09:36:28,584] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 25.0, 125.5, 0.0, 26.0, 25.1469614267217, 0.1672459796800471, 1.0, 1.0, 18707.68445409154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 475200.0000, 
sim time next is 475800.0000, 
raw observation next is [-1.616666666666667, 25.5, 126.6666666666667, 0.0, 26.0, 25.19105607175109, 0.1731369157066442, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4178208679593721, 0.255, 0.42222222222222233, 0.0, 0.6666666666666666, 0.599254672645924, 0.557712305235548, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9329951], dtype=float32), 0.039318692]. 
=============================================
[2019-04-04 09:36:31,589] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6016128e-10 3.5684256e-09 7.0050984e-26 2.0929903e-23 1.1537343e-18
 1.0000000e+00 1.7767835e-18], sum to 1.0000
[2019-04-04 09:36:31,589] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0930
[2019-04-04 09:36:31,661] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.89288839064172, 0.1914499326312292, 0.0, 1.0, 42266.57270808345], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 681600.0000, 
sim time next is 682200.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.85303491571751, 0.1846846996798284, 0.0, 1.0, 42208.08255108017], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5710862429764593, 0.5615615665599428, 0.0, 1.0, 0.20099086929085797], 
reward next is 0.7990, 
noisyNet noise sample is [array([2.235061], dtype=float32), -1.6214871]. 
=============================================
[2019-04-04 09:36:32,260] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1133897e-10 2.8482110e-09 1.2051617e-25 4.4967751e-23 5.6160988e-19
 1.0000000e+00 3.0454997e-18], sum to 1.0000
[2019-04-04 09:36:32,261] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5397
[2019-04-04 09:36:32,297] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.733333333333333, 71.33333333333334, 0.0, 0.0, 26.0, 24.42363202453812, 0.09745307611526964, 0.0, 1.0, 40991.5008643446], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 692400.0000, 
sim time next is 693000.0000, 
raw observation next is [-3.65, 71.5, 0.0, 0.0, 26.0, 24.39741087173324, 0.09159021722447118, 0.0, 1.0, 40964.45937169067], 
processed observation next is [1.0, 0.0, 0.3614958448753463, 0.715, 0.0, 0.0, 0.6666666666666666, 0.5331175726444366, 0.5305300724081571, 0.0, 1.0, 0.19506885415090794], 
reward next is 0.8049, 
noisyNet noise sample is [array([0.40094987], dtype=float32), -1.4921455]. 
=============================================
[2019-04-04 09:36:32,351] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.79995 ]
 [76.755905]
 [76.459175]
 [76.268616]
 [76.35359 ]], R is [[76.88990021]
 [76.92580414]
 [76.9610672 ]
 [76.99556732]
 [77.0293045 ]].
[2019-04-04 09:36:33,482] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0426322e-11 9.4645891e-10 3.1231934e-28 3.1238639e-26 4.3719807e-22
 1.0000000e+00 2.9019550e-21], sum to 1.0000
[2019-04-04 09:36:33,483] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8026
[2019-04-04 09:36:33,506] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.3, 88.0, 0.0, 0.0, 26.0, 24.927642114463, 0.2437813532758226, 0.0, 1.0, 39626.52366343861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 525600.0000, 
sim time next is 526200.0000, 
raw observation next is [4.216666666666667, 87.66666666666667, 0.0, 0.0, 26.0, 24.89696569251651, 0.2399983508712857, 0.0, 1.0, 39653.4434080095], 
processed observation next is [0.0, 0.08695652173913043, 0.5794090489381348, 0.8766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5747471410430425, 0.5799994502904285, 0.0, 1.0, 0.18882592099052142], 
reward next is 0.8112, 
noisyNet noise sample is [array([-1.2702807], dtype=float32), 1.5080957]. 
=============================================
[2019-04-04 09:36:35,667] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3515137e-10 3.7400203e-09 2.0829818e-27 2.4917695e-25 4.8958845e-21
 1.0000000e+00 1.4487466e-19], sum to 1.0000
[2019-04-04 09:36:35,668] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3775
[2019-04-04 09:36:35,754] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 12.0, 37.99999999999999, 26.0, 24.31860810675678, 0.1464576016374901, 0.0, 1.0, 41086.84912184721], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 546600.0000, 
sim time next is 547200.0000, 
raw observation next is [0.5, 92.0, 17.5, 54.5, 26.0, 24.34837812765612, 0.1476811022201212, 0.0, 1.0, 41025.87807236706], 
processed observation next is [0.0, 0.34782608695652173, 0.4764542936288089, 0.92, 0.058333333333333334, 0.06022099447513812, 0.6666666666666666, 0.5290315106380099, 0.5492270340733737, 0.0, 1.0, 0.19536132415412885], 
reward next is 0.8046, 
noisyNet noise sample is [array([-0.15714645], dtype=float32), 0.41002733]. 
=============================================
[2019-04-04 09:36:42,149] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.2646227e-11 9.5829700e-10 3.3623124e-27 9.8232689e-26 2.5537959e-21
 1.0000000e+00 8.5392173e-21], sum to 1.0000
[2019-04-04 09:36:42,151] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6140
[2019-04-04 09:36:42,202] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 72.33333333333334, 102.6666666666667, 0.0, 26.0, 25.61386364740836, 0.3119579178440714, 1.0, 1.0, 27607.86632318267], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 821400.0000, 
sim time next is 822000.0000, 
raw observation next is [-4.5, 73.66666666666667, 100.8333333333333, 0.0, 26.0, 25.58438049535899, 0.3029497696853317, 1.0, 1.0, 27700.45550589011], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7366666666666667, 0.336111111111111, 0.0, 0.6666666666666666, 0.6320317079465824, 0.6009832565617773, 1.0, 1.0, 0.1319069309804291], 
reward next is 0.8681, 
noisyNet noise sample is [array([0.19140705], dtype=float32), -0.6963748]. 
=============================================
[2019-04-04 09:36:42,205] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.30351 ]
 [83.356155]
 [83.370514]
 [83.32837 ]
 [83.21265 ]], R is [[83.29064941]
 [83.32627106]
 [83.3620224 ]
 [83.39815521]
 [83.43559265]].
[2019-04-04 09:36:43,005] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2351219e-09 4.9219270e-08 3.1287805e-24 1.1130856e-22 1.7946033e-18
 1.0000000e+00 8.4244019e-18], sum to 1.0000
[2019-04-04 09:36:43,005] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9707
[2019-04-04 09:36:43,119] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 73.5, 0.0, 0.0, 26.0, 23.53173018695798, -0.06406332185658857, 0.0, 1.0, 43922.25202320616], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 631800.0000, 
sim time next is 632400.0000, 
raw observation next is [-4.5, 75.33333333333333, 0.0, 0.0, 26.0, 23.50690069322408, -0.07007673186033077, 0.0, 1.0, 43947.63327978215], 
processed observation next is [0.0, 0.30434782608695654, 0.3379501385041552, 0.7533333333333333, 0.0, 0.0, 0.6666666666666666, 0.4589083911020066, 0.47664108937988975, 0.0, 1.0, 0.20927444418943883], 
reward next is 0.7907, 
noisyNet noise sample is [array([-0.37221488], dtype=float32), -0.114733614]. 
=============================================
[2019-04-04 09:36:54,997] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.2981096e-12 1.8243185e-10 5.2384885e-29 3.0551903e-27 1.6434972e-22
 1.0000000e+00 2.7372832e-22], sum to 1.0000
[2019-04-04 09:36:54,997] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1457
[2019-04-04 09:36:55,052] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 82.0, 48.0, 0.0, 26.0, 25.44412265636153, 0.2931817251208586, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 898200.0000, 
sim time next is 898800.0000, 
raw observation next is [1.1, 82.66666666666667, 52.83333333333333, 0.0, 26.0, 25.47928042190486, 0.288141582262684, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8266666666666667, 0.1761111111111111, 0.0, 0.6666666666666666, 0.6232733684920717, 0.5960471940875613, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1760169], dtype=float32), 1.378621]. 
=============================================
[2019-04-04 09:37:06,181] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.1072961e-12 3.8148532e-10 5.3563319e-31 7.6130061e-29 4.2501731e-24
 1.0000000e+00 5.6632266e-22], sum to 1.0000
[2019-04-04 09:37:06,181] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6461
[2019-04-04 09:37:06,194] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.92117634287681, 0.6065342207709152, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1032000.0000, 
sim time next is 1032600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.86774153011006, 0.6026986746054934, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6556451275091716, 0.7008995582018311, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0837069], dtype=float32), 0.047283772]. 
=============================================
[2019-04-04 09:37:10,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.8652740e-12 5.1996962e-10 1.4171781e-29 3.3815809e-27 5.8356234e-22
 1.0000000e+00 1.8296397e-21], sum to 1.0000
[2019-04-04 09:37:10,672] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9442
[2019-04-04 09:37:10,686] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.633333333333334, 88.33333333333334, 93.66666666666667, 0.0, 26.0, 25.35778761299803, 0.2790794002182057, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 904800.0000, 
sim time next is 905400.0000, 
raw observation next is [1.9, 90.5, 97.0, 0.0, 26.0, 25.35464579227338, 0.2806896550437646, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.515235457063712, 0.905, 0.3233333333333333, 0.0, 0.6666666666666666, 0.6128871493561151, 0.5935632183479215, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2259959], dtype=float32), 1.348947]. 
=============================================
[2019-04-04 09:37:15,199] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1317075e-13 9.7710583e-11 1.7829570e-31 7.4419543e-30 2.3774014e-24
 1.0000000e+00 6.0048041e-24], sum to 1.0000
[2019-04-04 09:37:15,202] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7711
[2019-04-04 09:37:15,239] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.11666666666667, 80.5, 0.0, 0.0, 26.0, 25.62216666231978, 0.5970340078864028, 0.0, 1.0, 24752.07907255277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1062600.0000, 
sim time next is 1063200.0000, 
raw observation next is [12.93333333333334, 81.0, 0.0, 0.0, 26.0, 25.68458653687527, 0.622005800978403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.8208679593721148, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6403822114062724, 0.7073352669928009, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2526643], dtype=float32), -2.0908568]. 
=============================================
[2019-04-04 09:37:15,407] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8075881e-11 1.5442679e-10 1.2220248e-28 3.4951183e-27 4.2445548e-22
 1.0000000e+00 9.2774697e-22], sum to 1.0000
[2019-04-04 09:37:15,409] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1425
[2019-04-04 09:37:15,416] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.73333333333334, 54.33333333333334, 0.0, 0.0, 26.0, 26.49482871840005, 0.7746503049422818, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1102800.0000, 
sim time next is 1103400.0000, 
raw observation next is [15.55, 55.0, 0.0, 0.0, 26.0, 26.47225811067944, 0.7600300936241794, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8933518005540168, 0.55, 0.0, 0.0, 0.6666666666666666, 0.7060215092232868, 0.7533433645413932, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08257397], dtype=float32), -1.3322226]. 
=============================================
[2019-04-04 09:37:23,812] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.6230003e-12 1.9355934e-11 3.5344241e-30 5.1215359e-28 6.2417489e-23
 1.0000000e+00 5.5823216e-22], sum to 1.0000
[2019-04-04 09:37:23,812] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9116
[2019-04-04 09:37:23,842] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.200000000000001, 95.0, 0.0, 0.0, 26.0, 25.41073624746596, 0.5907076452900097, 0.0, 1.0, 40479.36197009816], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1297200.0000, 
sim time next is 1297800.0000, 
raw observation next is [4.1, 94.5, 0.0, 0.0, 26.0, 25.48787644524988, 0.5924300119105895, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.5761772853185596, 0.945, 0.0, 0.0, 0.6666666666666666, 0.6239897037708234, 0.6974766706368632, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2009292], dtype=float32), -1.1604496]. 
=============================================
[2019-04-04 09:37:23,873] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4251423e-09 4.5874344e-08 3.3765830e-25 8.1646252e-24 3.6076345e-19
 1.0000000e+00 1.9059050e-18], sum to 1.0000
[2019-04-04 09:37:23,874] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1941
[2019-04-04 09:37:23,884] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.9, 86.33333333333334, 0.0, 0.0, 26.0, 24.01895772853899, 0.2528394977723757, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1218000.0000, 
sim time next is 1218600.0000, 
raw observation next is [15.8, 88.0, 0.0, 0.0, 26.0, 24.00487721772116, 0.24561751889424, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9002770083102495, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5004064348100966, 0.58187250629808, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3261421], dtype=float32), -1.7947923]. 
=============================================
[2019-04-04 09:37:25,824] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5248551e-12 1.8054734e-10 3.8756797e-30 3.2815049e-28 1.1264555e-22
 1.0000000e+00 4.6464281e-22], sum to 1.0000
[2019-04-04 09:37:25,825] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9098
[2019-04-04 09:37:25,894] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.8, 93.0, 96.0, 0.0, 26.0, 25.06159378071878, 0.1595961227258618, 1.0, 1.0, 36146.47687778457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 912600.0000, 
sim time next is 913200.0000, 
raw observation next is [3.8, 93.0, 95.0, 0.0, 26.0, 24.75888959810454, 0.2824096606408779, 1.0, 1.0, 187256.1929940673], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.31666666666666665, 0.0, 0.6666666666666666, 0.5632407998420449, 0.5941365535469593, 1.0, 1.0, 0.8916961571146061], 
reward next is 0.1083, 
noisyNet noise sample is [array([0.0683699], dtype=float32), -0.95145273]. 
=============================================
[2019-04-04 09:37:28,261] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6604340e-13 2.9611823e-11 6.1883530e-32 3.4012273e-30 8.8153727e-25
 1.0000000e+00 7.8063877e-24], sum to 1.0000
[2019-04-04 09:37:28,262] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7311
[2019-04-04 09:37:28,312] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.56666666666667, 53.0, 41.83333333333334, 30.83333333333334, 26.0, 27.51160435628253, 0.7542803724095822, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1615200.0000, 
sim time next is 1615800.0000, 
raw observation next is [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 26.0, 27.00002913858884, 0.764495754323134, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8056325023084026, 0.535, 0.11222222222222224, 0.027255985267034995, 0.6666666666666666, 0.7500024282157366, 0.7548319181077113, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3538067], dtype=float32), 1.3196863]. 
=============================================
[2019-04-04 09:37:32,533] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.1990170e-12 1.5067150e-10 5.8364457e-30 1.2991283e-27 6.7506393e-23
 1.0000000e+00 4.0390431e-22], sum to 1.0000
[2019-04-04 09:37:32,534] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8226
[2019-04-04 09:37:32,560] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 67.66666666666667, 0.0, 26.0, 25.99102095195037, 0.50674508010658, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1419600.0000, 
sim time next is 1420200.0000, 
raw observation next is [0.0, 95.0, 72.0, 0.0, 26.0, 25.9434746670807, 0.5004600256327009, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.24, 0.0, 0.6666666666666666, 0.661956222256725, 0.6668200085442336, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6333239], dtype=float32), 1.4620256]. 
=============================================
[2019-04-04 09:37:32,713] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3837091e-12 8.2122052e-11 3.2781235e-30 1.1597522e-27 3.8321919e-23
 1.0000000e+00 3.9340567e-22], sum to 1.0000
[2019-04-04 09:37:32,713] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0801
[2019-04-04 09:37:32,722] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.08333333333333333, 94.5, 94.0, 0.0, 26.0, 25.69825238609049, 0.4825856049728071, 1.0, 1.0, 18681.81399721254], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1426200.0000, 
sim time next is 1426800.0000, 
raw observation next is [0.1666666666666667, 94.0, 95.0, 0.0, 26.0, 25.71778119150883, 0.4740729945735416, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4672206832871654, 0.94, 0.31666666666666665, 0.0, 0.6666666666666666, 0.6431484326257358, 0.6580243315245139, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9666675], dtype=float32), 1.6294173]. 
=============================================
[2019-04-04 09:37:37,864] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.9110182e-12 1.4800913e-10 2.9508949e-28 1.2167233e-26 2.8228490e-22
 1.0000000e+00 2.6631462e-21], sum to 1.0000
[2019-04-04 09:37:37,866] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6352
[2019-04-04 09:37:37,915] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.35, 84.5, 30.0, 0.0, 26.0, 24.79973379541556, 0.4167441600487294, 1.0, 1.0, 196399.3894362309], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1701000.0000, 
sim time next is 1701600.0000, 
raw observation next is [1.266666666666667, 85.66666666666667, 25.16666666666667, 0.0, 26.0, 25.09917823550921, 0.4803350018509138, 1.0, 1.0, 6231.073815387373], 
processed observation next is [1.0, 0.6956521739130435, 0.4976915974145891, 0.8566666666666667, 0.0838888888888889, 0.0, 0.6666666666666666, 0.5915981862924342, 0.6601116672836379, 1.0, 1.0, 0.029671780073273202], 
reward next is 0.9703, 
noisyNet noise sample is [array([1.2383164], dtype=float32), 0.36717713]. 
=============================================
[2019-04-04 09:37:42,382] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5644120e-11 1.4539188e-10 3.0825944e-28 7.8229202e-27 1.2932969e-22
 1.0000000e+00 5.3371458e-21], sum to 1.0000
[2019-04-04 09:37:42,385] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7971
[2019-04-04 09:37:42,409] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.27468660239872, 0.4169926568546303, 0.0, 1.0, 38318.81149989086], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 949800.0000, 
sim time next is 950400.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.26547455336659, 0.4139481887940726, 0.0, 1.0, 38260.90481608272], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6054562127805493, 0.6379827295980242, 0.0, 1.0, 0.18219478483848917], 
reward next is 0.8178, 
noisyNet noise sample is [array([-0.19746335], dtype=float32), -0.35485262]. 
=============================================
[2019-04-04 09:37:50,166] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0713834e-11 6.2011241e-10 5.6297369e-28 1.9672749e-26 3.1169187e-22
 1.0000000e+00 1.1687498e-20], sum to 1.0000
[2019-04-04 09:37:50,172] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0906
[2019-04-04 09:37:50,245] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.14206773123145, 0.4614339040166627, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1408800.0000, 
sim time next is 1409400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.45273500032571, 0.4729432189747944, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6210612500271425, 0.6576477396582648, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46846056], dtype=float32), 0.61351854]. 
=============================================
[2019-04-04 09:38:06,892] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2914908e-12 8.6433924e-12 4.6337155e-31 5.7052726e-29 1.0521085e-23
 1.0000000e+00 3.3593064e-23], sum to 1.0000
[2019-04-04 09:38:06,892] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9828
[2019-04-04 09:38:06,924] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.85, 92.0, 53.0, 0.0, 26.0, 25.88313748051812, 0.5534475131535236, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1675800.0000, 
sim time next is 1676400.0000, 
raw observation next is [1.733333333333333, 92.0, 55.16666666666667, 0.0, 26.0, 25.92848307028386, 0.5577596359608527, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5106186518928902, 0.92, 0.1838888888888889, 0.0, 0.6666666666666666, 0.660706922523655, 0.6859198786536176, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2915219], dtype=float32), 2.2452722]. 
=============================================
[2019-04-04 09:38:07,652] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7730973e-10 8.2412424e-09 1.2776287e-24 4.7954185e-23 8.2493035e-18
 1.0000000e+00 8.0267621e-18], sum to 1.0000
[2019-04-04 09:38:07,662] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3892
[2019-04-04 09:38:07,730] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 76.33333333333333, 43.33333333333333, 0.0, 26.0, 25.00627838350367, 0.2595391630017359, 0.0, 1.0, 52534.28728579702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1872600.0000, 
sim time next is 1873200.0000, 
raw observation next is [-4.5, 77.66666666666667, 36.16666666666666, 0.0, 26.0, 25.00871988984074, 0.2638889445349635, 0.0, 1.0, 46279.53455586803], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.7766666666666667, 0.12055555555555553, 0.0, 0.6666666666666666, 0.5840599908200618, 0.5879629815116545, 0.0, 1.0, 0.22037873598032395], 
reward next is 0.7796, 
noisyNet noise sample is [array([0.03297455], dtype=float32), -0.03329289]. 
=============================================
[2019-04-04 09:38:19,828] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0649445e-09 8.8410061e-09 7.9869390e-24 2.1336287e-22 3.4454244e-19
 1.0000000e+00 8.2053025e-18], sum to 1.0000
[2019-04-04 09:38:19,829] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7558
[2019-04-04 09:38:19,850] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.9, 77.0, 0.0, 0.0, 26.0, 23.68422525084171, -0.01091905981814091, 0.0, 1.0, 41897.64059913866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2183400.0000, 
sim time next is 2184000.0000, 
raw observation next is [-5.8, 76.33333333333334, 0.0, 0.0, 26.0, 23.67406370369281, -0.01200752396728736, 0.0, 1.0, 41898.1363749678], 
processed observation next is [1.0, 0.2608695652173913, 0.30193905817174516, 0.7633333333333334, 0.0, 0.0, 0.6666666666666666, 0.4728386419744008, 0.4959974920109042, 0.0, 1.0, 0.1995149351188943], 
reward next is 0.8005, 
noisyNet noise sample is [array([-1.6958523], dtype=float32), 0.16697109]. 
=============================================
[2019-04-04 09:38:19,877] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[73.08462 ]
 [73.165   ]
 [73.228485]
 [73.30465 ]
 [73.36167 ]], R is [[73.08345032]
 [73.15309906]
 [73.22208405]
 [73.29037476]
 [73.35794067]].
[2019-04-04 09:38:21,488] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1558716e-09 1.3684445e-08 1.3930200e-23 1.8561064e-22 6.3474875e-18
 1.0000000e+00 1.8855132e-17], sum to 1.0000
[2019-04-04 09:38:21,488] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0925
[2019-04-04 09:38:21,512] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.199999999999999, 79.0, 0.0, 0.0, 26.0, 23.66575062099458, -0.01370085113833766, 0.0, 1.0, 47075.1249657478], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1834800.0000, 
sim time next is 1835400.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.63940594835257, -0.0219774711502735, 0.0, 1.0, 47079.64049845625], 
processed observation next is [0.0, 0.21739130434782608, 0.2908587257617729, 0.79, 0.0, 0.0, 0.6666666666666666, 0.46995049569604763, 0.4926741762832421, 0.0, 1.0, 0.22418876427836307], 
reward next is 0.7758, 
noisyNet noise sample is [array([-0.03122228], dtype=float32), 0.25147843]. 
=============================================
[2019-04-04 09:38:23,107] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8040474e-11 2.2959472e-09 1.9774008e-27 5.8517061e-26 1.1938308e-20
 1.0000000e+00 1.5753608e-20], sum to 1.0000
[2019-04-04 09:38:23,107] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2207
[2019-04-04 09:38:23,177] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 69.0, 126.1666666666667, 47.49999999999999, 26.0, 26.37473571133808, 0.5124377398828379, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2212800.0000, 
sim time next is 2213400.0000, 
raw observation next is [-3.9, 68.5, 132.3333333333333, 94.99999999999999, 26.0, 26.37663961691351, 0.5143729098261195, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.685, 0.44111111111111095, 0.10497237569060772, 0.6666666666666666, 0.6980533014094593, 0.6714576366087065, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10511487], dtype=float32), 1.7028924]. 
=============================================
[2019-04-04 09:38:28,262] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4369425e-09 3.8226848e-09 7.9254415e-26 2.1414964e-23 2.7609037e-19
 1.0000000e+00 7.6120577e-19], sum to 1.0000
[2019-04-04 09:38:28,264] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8071
[2019-04-04 09:38:28,303] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.783333333333333, 83.0, 0.0, 0.0, 26.0, 25.268028025828, 0.4175395602829695, 0.0, 1.0, 43317.65508273181], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2149800.0000, 
sim time next is 2150400.0000, 
raw observation next is [-5.966666666666667, 83.0, 0.0, 0.0, 26.0, 25.27757834205332, 0.413672457834399, 0.0, 1.0, 43086.33272279808], 
processed observation next is [1.0, 0.9130434782608695, 0.2973222530009234, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6064648618377767, 0.637890819278133, 0.0, 1.0, 0.20517301296570514], 
reward next is 0.7948, 
noisyNet noise sample is [array([-1.6234878], dtype=float32), 0.3753958]. 
=============================================
[2019-04-04 09:38:28,601] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.03279746e-10 2.80857138e-09 5.49438108e-26 2.41953205e-25
 3.35155308e-21 1.00000000e+00 2.73367683e-19], sum to 1.0000
[2019-04-04 09:38:28,601] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4837
[2019-04-04 09:38:28,670] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 87.0, 97.0, 0.0, 26.0, 24.96955792847374, 0.3400224375561655, 0.0, 1.0, 18738.88830610051], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1765800.0000, 
sim time next is 1766400.0000, 
raw observation next is [-2.3, 87.0, 100.6666666666667, 0.0, 26.0, 24.9547816724825, 0.333335327445405, 0.0, 1.0, 33970.08946800172], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.33555555555555566, 0.0, 0.6666666666666666, 0.5795651393735417, 0.611111775815135, 0.0, 1.0, 0.1617623308000082], 
reward next is 0.8382, 
noisyNet noise sample is [array([-0.13949475], dtype=float32), 0.027808499]. 
=============================================
[2019-04-04 09:38:45,346] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.8856225e-11 2.1620231e-10 4.7653928e-27 1.5909015e-25 1.7334977e-21
 1.0000000e+00 2.0463977e-20], sum to 1.0000
[2019-04-04 09:38:45,347] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3089
[2019-04-04 09:38:45,383] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 52.33333333333334, 0.0, 0.0, 26.0, 25.59389466084579, 0.418846898110051, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2311800.0000, 
sim time next is 2312400.0000, 
raw observation next is [-1.2, 52.66666666666667, 0.0, 0.0, 26.0, 25.61565647182639, 0.3922011159591281, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.42936288088642666, 0.5266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6346380393188659, 0.6307337053197094, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25077328], dtype=float32), -1.450263]. 
=============================================
[2019-04-04 09:38:47,194] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.2850868e-10 7.5696169e-09 5.0485505e-25 2.0838603e-23 7.8160022e-19
 1.0000000e+00 3.5356322e-18], sum to 1.0000
[2019-04-04 09:38:47,196] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4077
[2019-04-04 09:38:47,246] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.2, 46.33333333333334, 80.16666666666667, 105.1666666666667, 26.0, 24.96868172097729, 0.2869648826910849, 0.0, 1.0, 47475.62720673336], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2391600.0000, 
sim time next is 2392200.0000, 
raw observation next is [-0.3, 46.0, 79.0, 58.0, 26.0, 24.95386236344752, 0.2846462888304614, 0.0, 1.0, 49397.71835914705], 
processed observation next is [0.0, 0.6956521739130435, 0.4542936288088643, 0.46, 0.2633333333333333, 0.06408839779005525, 0.6666666666666666, 0.5794885302872933, 0.5948820962768204, 0.0, 1.0, 0.23522723028165263], 
reward next is 0.7648, 
noisyNet noise sample is [array([-0.2471703], dtype=float32), -0.087548606]. 
=============================================
[2019-04-04 09:38:48,756] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.97824432e-10 1.92964200e-09 1.02268254e-25 3.62933800e-24
 1.18226573e-19 1.00000000e+00 5.37605722e-19], sum to 1.0000
[2019-04-04 09:38:48,756] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9966
[2019-04-04 09:38:48,840] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.22659278604367, 0.4172574119596196, 0.0, 1.0, 46124.55371933618], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2148600.0000, 
sim time next is 2149200.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.25893521544404, 0.4173768206839886, 0.0, 1.0, 43960.34000687009], 
processed observation next is [1.0, 0.9130434782608695, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.60491126795367, 0.6391256068946629, 0.0, 1.0, 0.2093349524136671], 
reward next is 0.7907, 
noisyNet noise sample is [array([-1.377935], dtype=float32), 0.5779274]. 
=============================================
[2019-04-04 09:39:40,549] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-04 09:39:40,567] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:39:40,567] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:39:40,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run33
[2019-04-04 09:39:40,618] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:39:40,619] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:39:40,621] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run33
[2019-04-04 09:39:40,681] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:39:40,682] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:39:40,718] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run33
[2019-04-04 09:42:52,538] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 09:43:21,181] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.4981 263445398.1310 1557.1271
[2019-04-04 09:43:26,746] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 09:43:27,777] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 3200000, evaluation results [3200000.0, 7241.498104138273, 263445398.13096312, 1557.1271045198032, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 09:43:29,442] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2285031e-09 2.3867699e-09 2.4318018e-25 2.5122830e-23 1.6931944e-19
 1.0000000e+00 2.0010930e-18], sum to 1.0000
[2019-04-04 09:43:29,442] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2961
[2019-04-04 09:43:29,460] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.12207493336014, 0.3212465910923743, 0.0, 1.0, 38766.80752917886], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3017400.0000, 
sim time next is 3018000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.14347998162659, 0.3159453418563955, 0.0, 1.0, 38639.12592372071], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5952899984688825, 0.6053151139521319, 0.0, 1.0, 0.1839958377320034], 
reward next is 0.8160, 
noisyNet noise sample is [array([0.49195242], dtype=float32), 0.7452125]. 
=============================================
[2019-04-04 09:43:29,486] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.48758 ]
 [77.59459 ]
 [77.72397 ]
 [77.82455 ]
 [77.912544]], R is [[77.41164398]
 [77.45292664]
 [77.49317169]
 [77.53236389]
 [77.57047272]].
[2019-04-04 09:43:29,801] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6789198e-09 2.0261595e-08 5.0676528e-25 2.2384293e-23 1.2107462e-18
 1.0000000e+00 6.6510694e-18], sum to 1.0000
[2019-04-04 09:43:29,801] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1100
[2019-04-04 09:43:29,839] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93079208640976, 0.2752204650914976, 0.0, 1.0, 38150.93490819046], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3021600.0000, 
sim time next is 3022200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93795231173934, 0.2698984712044595, 0.0, 1.0, 38075.94099414045], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5781626926449451, 0.5899661570681531, 0.0, 1.0, 0.18131400473400217], 
reward next is 0.8187, 
noisyNet noise sample is [array([0.46615198], dtype=float32), 1.5571815]. 
=============================================
[2019-04-04 09:43:51,524] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0055444e-12 2.2021090e-11 2.7095856e-30 2.9456653e-28 1.6227644e-22
 1.0000000e+00 2.3287540e-22], sum to 1.0000
[2019-04-04 09:43:51,525] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7139
[2019-04-04 09:43:51,548] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 71.0, 80.66666666666667, 656.8333333333334, 26.0, 26.18362660758967, 0.7733598661251562, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3253200.0000, 
sim time next is 3253800.0000, 
raw observation next is [-2.833333333333333, 71.0, 76.33333333333333, 627.6666666666666, 26.0, 26.58954287743919, 0.8037845349363048, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3841181902123731, 0.71, 0.2544444444444444, 0.6935543278084714, 0.6666666666666666, 0.7157952397865991, 0.7679281783121016, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8846219], dtype=float32), 0.17798471]. 
=============================================
[2019-04-04 09:43:53,337] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0723693e-13 1.6330323e-11 3.2481953e-32 3.7400678e-29 1.0979246e-23
 1.0000000e+00 3.1191412e-23], sum to 1.0000
[2019-04-04 09:43:53,340] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1842
[2019-04-04 09:43:53,376] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 95.66666666666667, 558.6666666666666, 26.0, 26.06075511023645, 0.6154218858333348, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3229800.0000, 
sim time next is 3230400.0000, 
raw observation next is [-3.0, 92.0, 98.33333333333334, 605.8333333333334, 26.0, 26.09327978780437, 0.6314759527196782, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.92, 0.32777777777777783, 0.6694290976058932, 0.6666666666666666, 0.6744399823170308, 0.7104919842398928, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.239384], dtype=float32), -0.80681986]. 
=============================================
[2019-04-04 09:44:10,534] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2780131e-10 7.1899628e-09 3.1315254e-25 5.1204914e-24 1.7093345e-19
 1.0000000e+00 3.2746548e-19], sum to 1.0000
[2019-04-04 09:44:10,534] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7617
[2019-04-04 09:44:10,552] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 69.0, 0.0, 0.0, 26.0, 25.26723646065329, 0.4136756566042714, 0.0, 1.0, 40859.11040471873], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3550800.0000, 
sim time next is 3551400.0000, 
raw observation next is [-3.0, 68.0, 0.0, 0.0, 26.0, 25.23821151948068, 0.4063593037827806, 0.0, 1.0, 40818.07926413833], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6031842932900565, 0.6354531012609269, 0.0, 1.0, 0.19437180601970633], 
reward next is 0.8056, 
noisyNet noise sample is [array([0.1437303], dtype=float32), -0.14000206]. 
=============================================
[2019-04-04 09:44:11,983] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.01856395e-11 1.87983273e-09 2.23165622e-26 4.43051796e-25
 1.01425494e-19 1.00000000e+00 8.09365921e-20], sum to 1.0000
[2019-04-04 09:44:11,984] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9662
[2019-04-04 09:44:12,039] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 70.0, 94.0, 550.5, 26.0, 25.90916299864695, 0.5278328227812389, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3574800.0000, 
sim time next is 3575400.0000, 
raw observation next is [-5.833333333333333, 69.16666666666667, 96.0, 592.3333333333334, 26.0, 25.92786835303638, 0.5285005007446486, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.30101569713758086, 0.6916666666666668, 0.32, 0.6545119705340701, 0.6666666666666666, 0.6606556960863651, 0.6761668335815495, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4490073], dtype=float32), -0.89237475]. 
=============================================
[2019-04-04 09:44:12,597] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7499649e-10 1.8028143e-09 6.6544177e-26 2.7518634e-25 8.9664543e-20
 1.0000000e+00 2.8447654e-19], sum to 1.0000
[2019-04-04 09:44:12,597] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0628
[2019-04-04 09:44:12,615] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 70.0, 0.0, 0.0, 26.0, 25.05065878515662, 0.3885281151807028, 0.0, 1.0, 43721.30841386945], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3286800.0000, 
sim time next is 3287400.0000, 
raw observation next is [-7.166666666666667, 71.16666666666667, 0.0, 0.0, 26.0, 25.0423442539505, 0.3863455746856019, 0.0, 1.0, 43749.62003195604], 
processed observation next is [1.0, 0.043478260869565216, 0.26408125577100644, 0.7116666666666667, 0.0, 0.0, 0.6666666666666666, 0.5868620211625416, 0.6287818582285339, 0.0, 1.0, 0.2083315239616954], 
reward next is 0.7917, 
noisyNet noise sample is [array([0.82177824], dtype=float32), 1.0405228]. 
=============================================
[2019-04-04 09:44:17,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2167679e-09 8.9226422e-09 2.2963589e-24 8.2771161e-23 1.4272827e-18
 1.0000000e+00 5.3356677e-18], sum to 1.0000
[2019-04-04 09:44:17,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4237
[2019-04-04 09:44:17,353] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 80.33333333333334, 0.0, 0.0, 26.0, 25.00562467940301, 0.2831062008675246, 0.0, 1.0, 53396.36910306997], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3088200.0000, 
sim time next is 3088800.0000, 
raw observation next is [-0.6, 82.0, 0.0, 0.0, 26.0, 24.99176819956359, 0.2838153664125689, 0.0, 1.0, 50345.09180038461], 
processed observation next is [0.0, 0.782608695652174, 0.44598337950138506, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5826473499636325, 0.5946051221375229, 0.0, 1.0, 0.23973853238278386], 
reward next is 0.7603, 
noisyNet noise sample is [array([-0.6515972], dtype=float32), 0.8929954]. 
=============================================
[2019-04-04 09:44:20,395] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7181014e-11 1.8545253e-10 2.9119553e-30 2.8174723e-28 1.3955950e-22
 1.0000000e+00 2.2647215e-22], sum to 1.0000
[2019-04-04 09:44:20,395] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4916
[2019-04-04 09:44:20,411] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 111.3333333333333, 800.8333333333334, 26.0, 26.29847878225935, 0.6139510776248985, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3418800.0000, 
sim time next is 3419400.0000, 
raw observation next is [3.0, 49.0, 109.6666666666667, 795.6666666666666, 26.0, 26.40693778187495, 0.6293754430979085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.3655555555555557, 0.8791896869244935, 0.6666666666666666, 0.7005781484895793, 0.7097918143659695, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.169096], dtype=float32), -0.7470478]. 
=============================================
[2019-04-04 09:44:24,078] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8841404e-09 4.3238360e-09 2.2578861e-25 5.0639644e-24 5.1387196e-20
 1.0000000e+00 1.0939599e-18], sum to 1.0000
[2019-04-04 09:44:24,078] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0810
[2019-04-04 09:44:24,096] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 75.0, 0.0, 0.0, 26.0, 25.55381601882433, 0.5128501731090925, 0.0, 1.0, 43848.50434067893], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3793200.0000, 
sim time next is 3793800.0000, 
raw observation next is [-3.0, 76.0, 0.0, 0.0, 26.0, 25.4972007870675, 0.5091679068265786, 0.0, 1.0, 71417.27534584944], 
processed observation next is [1.0, 0.9130434782608695, 0.3795013850415513, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6247667322556248, 0.6697226356088596, 0.0, 1.0, 0.340082263551664], 
reward next is 0.6599, 
noisyNet noise sample is [array([-1.3826425], dtype=float32), 1.503731]. 
=============================================
[2019-04-04 09:44:24,207] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.3727911e-10 4.8542015e-09 3.4683855e-25 5.3189672e-24 6.9724915e-19
 1.0000000e+00 7.9326728e-18], sum to 1.0000
[2019-04-04 09:44:24,208] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2877
[2019-04-04 09:44:24,232] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 59.0, 15.0, 165.0, 26.0, 25.30659093338998, 0.3902015901878302, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3691800.0000, 
sim time next is 3692400.0000, 
raw observation next is [4.0, 59.0, 12.5, 137.5, 26.0, 25.26085264557643, 0.375156519071405, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.041666666666666664, 0.15193370165745856, 0.6666666666666666, 0.6050710537980359, 0.6250521730238017, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5424108], dtype=float32), 0.2904305]. 
=============================================
[2019-04-04 09:44:29,981] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.1030822e-10 6.4293006e-09 5.0131050e-25 1.7338001e-23 5.3295216e-19
 1.0000000e+00 2.2105174e-18], sum to 1.0000
[2019-04-04 09:44:29,982] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3017
[2019-04-04 09:44:29,992] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333334, 42.66666666666666, 82.16666666666667, 668.3333333333333, 26.0, 25.34720552112378, 0.4726173148702186, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3598800.0000, 
sim time next is 3599400.0000, 
raw observation next is [-0.1666666666666666, 42.83333333333334, 78.33333333333334, 637.6666666666667, 26.0, 25.35009482877533, 0.4665717693599697, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.4579870729455217, 0.42833333333333345, 0.2611111111111111, 0.7046040515653776, 0.6666666666666666, 0.6125079023979442, 0.6555239231199899, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9722846], dtype=float32), 0.3867082]. 
=============================================
[2019-04-04 09:44:32,522] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.59607344e-11 1.40943435e-09 4.70002095e-27 9.95313816e-26
 1.09394216e-20 1.00000000e+00 1.51220345e-20], sum to 1.0000
[2019-04-04 09:44:32,530] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6131
[2019-04-04 09:44:32,545] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 49.0, 118.8333333333333, 804.1666666666666, 26.0, 26.32507708406003, 0.6071846587730633, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3928800.0000, 
sim time next is 3929400.0000, 
raw observation next is [-6.0, 49.0, 120.0, 810.0, 26.0, 26.35142423346651, 0.609989746416046, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.296398891966759, 0.49, 0.4, 0.8950276243093923, 0.6666666666666666, 0.6959520194555425, 0.7033299154720153, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4865057], dtype=float32), 0.118326545]. 
=============================================
[2019-04-04 09:44:41,255] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9123652e-10 1.3481335e-08 1.4432923e-25 1.7247495e-23 9.5873632e-20
 1.0000000e+00 4.6905830e-19], sum to 1.0000
[2019-04-04 09:44:41,256] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4280
[2019-04-04 09:44:41,296] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 75.0, 0.0, 0.0, 26.0, 25.42238973546829, 0.3925573682607741, 0.0, 1.0, 40876.15773154307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3810000.0000, 
sim time next is 3810600.0000, 
raw observation next is [-4.0, 74.0, 0.0, 0.0, 26.0, 25.39492007453062, 0.3764395423191373, 0.0, 1.0, 55891.53488406067], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6162433395442184, 0.6254798474397124, 0.0, 1.0, 0.2661501661145746], 
reward next is 0.7338, 
noisyNet noise sample is [array([1.789993], dtype=float32), 0.31891245]. 
=============================================
[2019-04-04 09:44:43,875] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.49081164e-11 4.48621612e-10 8.26188557e-29 1.37088695e-27
 1.44320096e-22 1.00000000e+00 7.96640816e-22], sum to 1.0000
[2019-04-04 09:44:43,875] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8409
[2019-04-04 09:44:43,925] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 96.0, 563.5, 26.0, 26.07642400611243, 0.5107616365783638, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3834000.0000, 
sim time next is 3834600.0000, 
raw observation next is [-3.666666666666667, 69.16666666666667, 97.66666666666667, 602.3333333333334, 26.0, 26.1376765997461, 0.5203983390054151, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3610341643582641, 0.6916666666666668, 0.3255555555555556, 0.6655616942909761, 0.6666666666666666, 0.6781397166455084, 0.673466113001805, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3151008], dtype=float32), -0.7050814]. 
=============================================
[2019-04-04 09:44:44,130] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0442425e-10 2.3727442e-09 2.9977568e-26 7.3900899e-25 9.1040778e-20
 1.0000000e+00 2.7369742e-19], sum to 1.0000
[2019-04-04 09:44:44,130] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7004
[2019-04-04 09:44:44,170] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.333333333333334, 39.0, 117.8333333333333, 810.1666666666666, 26.0, 26.48160869593675, 0.5820356741515926, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4015200.0000, 
sim time next is 4015800.0000, 
raw observation next is [-7.0, 38.5, 119.0, 816.0, 26.0, 26.45866945388077, 0.5827799361896371, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.2686980609418283, 0.385, 0.39666666666666667, 0.901657458563536, 0.6666666666666666, 0.7048891211567309, 0.6942599787298791, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4226762], dtype=float32), 0.8729464]. 
=============================================
[2019-04-04 09:44:50,409] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.1294136e-11 5.6870233e-09 3.7680416e-26 9.7959564e-25 1.2595094e-19
 1.0000000e+00 3.3146332e-19], sum to 1.0000
[2019-04-04 09:44:50,413] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0890
[2019-04-04 09:44:50,437] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 48.33333333333334, 0.0, 0.0, 26.0, 25.38308340702506, 0.3252484532164843, 0.0, 1.0, 38400.32162562429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4254600.0000, 
sim time next is 4255200.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.37650431555439, 0.3237305469099407, 0.0, 1.0, 41776.44133995981], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6147086929628657, 0.6079101823033136, 0.0, 1.0, 0.19893543495218957], 
reward next is 0.8011, 
noisyNet noise sample is [array([-0.9928806], dtype=float32), 0.42308587]. 
=============================================
[2019-04-04 09:44:51,487] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.71773284e-09 1.51180313e-08 1.07758455e-23 2.21321301e-22
 5.34736366e-18 1.00000000e+00 1.13337641e-17], sum to 1.0000
[2019-04-04 09:44:51,490] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5404
[2019-04-04 09:44:51,506] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.16666666666667, 58.83333333333334, 0.0, 0.0, 26.0, 24.72624676304253, 0.2764948966777076, 0.0, 1.0, 43842.50480785495], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3978600.0000, 
sim time next is 3979200.0000, 
raw observation next is [-11.33333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 24.73370907817024, 0.2671236238937156, 0.0, 1.0, 43785.71735076367], 
processed observation next is [1.0, 0.043478260869565216, 0.14866112650046176, 0.5966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5611424231808533, 0.5890412079645718, 0.0, 1.0, 0.20850341595601748], 
reward next is 0.7915, 
noisyNet noise sample is [array([-0.84366655], dtype=float32), 0.36197424]. 
=============================================
[2019-04-04 09:44:52,331] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.0283835e-11 2.7251530e-09 9.3977745e-26 2.7263696e-24 1.5211257e-19
 1.0000000e+00 1.7321113e-19], sum to 1.0000
[2019-04-04 09:44:52,331] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2658
[2019-04-04 09:44:52,369] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333334, 33.33333333333334, 118.1666666666667, 814.0, 26.0, 25.10951050098607, 0.3848678519640376, 0.0, 1.0, 9352.41442966665], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4188000.0000, 
sim time next is 4188600.0000, 
raw observation next is [0.0, 32.5, 119.0, 822.0, 26.0, 25.10250560496703, 0.3835022812213336, 0.0, 1.0, 9351.8023234229], 
processed observation next is [0.0, 0.4782608695652174, 0.46260387811634357, 0.325, 0.39666666666666667, 0.9082872928176795, 0.6666666666666666, 0.5918754670805857, 0.6278340937404445, 0.0, 1.0, 0.04453239201629953], 
reward next is 0.9555, 
noisyNet noise sample is [array([-1.1501056], dtype=float32), -0.38464186]. 
=============================================
[2019-04-04 09:45:00,974] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6112312e-11 3.0615577e-10 2.3695104e-28 6.9143512e-27 1.3432736e-21
 1.0000000e+00 3.4516038e-21], sum to 1.0000
[2019-04-04 09:45:00,981] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3216
[2019-04-04 09:45:00,997] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.25, 73.0, 0.0, 0.0, 26.0, 25.78729451476448, 0.473415845427186, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4307400.0000, 
sim time next is 4308000.0000, 
raw observation next is [5.199999999999999, 73.0, 0.0, 0.0, 26.0, 25.80819900145363, 0.4680271936802159, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.6066481994459835, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6506832501211358, 0.6560090645600719, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34371674], dtype=float32), 1.176527]. 
=============================================
[2019-04-04 09:45:01,014] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.85645]
 [86.90505]
 [86.91319]
 [86.87599]
 [86.38512]], R is [[86.79003906]
 [86.92214203]
 [87.05292511]
 [87.18239594]
 [87.02402496]].
[2019-04-04 09:45:03,785] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0686710e-11 3.9345610e-10 5.3533987e-28 8.3500403e-27 2.2952402e-22
 1.0000000e+00 7.8712585e-21], sum to 1.0000
[2019-04-04 09:45:03,789] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2739
[2019-04-04 09:45:03,839] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.04696258953278, 0.4734634452198943, 1.0, 1.0, 18711.26700921649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4476600.0000, 
sim time next is 4477200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.07038435894614, 0.468327033009329, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5891986965788449, 0.6561090110031097, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7760786], dtype=float32), -0.60808355]. 
=============================================
[2019-04-04 09:45:09,566] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.0919987e-11 2.9986275e-10 1.6385343e-28 4.1487212e-26 3.9794605e-21
 1.0000000e+00 3.1055281e-20], sum to 1.0000
[2019-04-04 09:45:09,567] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5421
[2019-04-04 09:45:09,584] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.8, 76.0, 0.0, 0.0, 26.0, 25.50574536854075, 0.4144086941815637, 0.0, 1.0, 62524.51731677207], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4312800.0000, 
sim time next is 4313400.0000, 
raw observation next is [4.733333333333333, 75.83333333333334, 0.0, 0.0, 26.0, 25.49981648676922, 0.4175732998085318, 0.0, 1.0, 48108.14706495027], 
processed observation next is [0.0, 0.9565217391304348, 0.5937211449676825, 0.7583333333333334, 0.0, 0.0, 0.6666666666666666, 0.6249847072307683, 0.6391910999361773, 0.0, 1.0, 0.22908641459500131], 
reward next is 0.7709, 
noisyNet noise sample is [array([0.7045769], dtype=float32), -0.96951556]. 
=============================================
[2019-04-04 09:45:18,110] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2007009e-11 4.3803711e-10 2.3810178e-28 1.5276105e-25 3.0095204e-21
 1.0000000e+00 4.3291487e-21], sum to 1.0000
[2019-04-04 09:45:18,110] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1311
[2019-04-04 09:45:18,126] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 36.5, 0.0, 0.0, 26.0, 25.0594202152044, 0.4963422779797543, 1.0, 1.0, 13081.96817024911], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4128600.0000, 
sim time next is 4129200.0000, 
raw observation next is [3.0, 37.0, 0.0, 0.0, 26.0, 25.32309475377136, 0.5137804906689443, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5457063711911359, 0.37, 0.0, 0.0, 0.6666666666666666, 0.6102578961476134, 0.6712601635563148, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32703352], dtype=float32), 1.4848937]. 
=============================================
[2019-04-04 09:45:20,768] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.5331631e-10 4.4176289e-08 4.8842392e-23 5.6203767e-22 1.7667678e-17
 1.0000000e+00 2.0816898e-17], sum to 1.0000
[2019-04-04 09:45:20,770] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2503
[2019-04-04 09:45:20,802] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 63.00000000000001, 0.0, 0.0, 26.0, 24.34582878521737, 0.1574518595533868, 0.0, 1.0, 43588.42509113], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3986400.0000, 
sim time next is 3987000.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.29170067719379, 0.1472875693109435, 0.0, 1.0, 43596.35608925058], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5243083897661492, 0.5490958564369811, 0.0, 1.0, 0.207601695663098], 
reward next is 0.7924, 
noisyNet noise sample is [array([1.1693343], dtype=float32), 0.31873322]. 
=============================================
[2019-04-04 09:45:20,824] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[69.3836  ]
 [69.509125]
 [69.61334 ]
 [69.713104]
 [69.87486 ]], R is [[69.34920502]
 [69.44815063]
 [69.54592133]
 [69.64239502]
 [69.73764038]].
[2019-04-04 09:45:22,533] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8458300e-10 1.9548660e-09 3.3134962e-26 3.6022757e-24 2.1007856e-19
 1.0000000e+00 1.1613506e-19], sum to 1.0000
[2019-04-04 09:45:22,535] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6809
[2019-04-04 09:45:22,549] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.033333333333333, 68.16666666666666, 0.0, 0.0, 26.0, 25.02294445420228, 0.2921228561393449, 0.0, 1.0, 52920.19010680541], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4301400.0000, 
sim time next is 4302000.0000, 
raw observation next is [6.0, 69.0, 0.0, 0.0, 26.0, 24.97210297990229, 0.2886423514264017, 0.0, 1.0, 56673.47280600753], 
processed observation next is [0.0, 0.8260869565217391, 0.6288088642659281, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5810085816585241, 0.5962141171421339, 0.0, 1.0, 0.2698736800286073], 
reward next is 0.7301, 
noisyNet noise sample is [array([0.32208878], dtype=float32), 0.38398287]. 
=============================================
[2019-04-04 09:45:22,557] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[79.95209 ]
 [79.3476  ]
 [78.9731  ]
 [78.931946]
 [79.09566 ]], R is [[80.49176788]
 [80.4348526 ]
 [80.46398163]
 [80.6593399 ]
 [80.85274506]].
[2019-04-04 09:45:37,175] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:45:37,175] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:45:37,183] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run25
[2019-04-04 09:45:40,174] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:45:40,174] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:45:40,179] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run25
[2019-04-04 09:45:41,216] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:45:41,216] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:45:41,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run25
[2019-04-04 09:45:43,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:45:43,854] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:45:43,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run25
[2019-04-04 09:45:46,116] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:45:46,116] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:45:46,119] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run25
[2019-04-04 09:45:46,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:45:46,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:45:46,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run25
[2019-04-04 09:45:53,778] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5933825e-10 1.6809303e-09 5.1145711e-26 8.1059984e-25 1.8816590e-20
 1.0000000e+00 2.4486661e-20], sum to 1.0000
[2019-04-04 09:45:53,779] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5634
[2019-04-04 09:45:53,793] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 47.5, 0.0, 0.0, 26.0, 25.55187557501937, 0.4442996934959288, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5020200.0000, 
sim time next is 5020800.0000, 
raw observation next is [-0.3333333333333333, 50.0, 0.0, 0.0, 26.0, 25.58832743535685, 0.4273751819855846, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4533702677747, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6323606196130708, 0.6424583939951949, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6004118], dtype=float32), -1.0286727]. 
=============================================
[2019-04-04 09:45:55,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:45:55,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:45:55,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run25
[2019-04-04 09:45:55,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:45:55,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:45:55,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run25
[2019-04-04 09:45:55,888] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:45:55,889] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:45:55,892] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run25
[2019-04-04 09:45:57,486] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:45:57,486] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:45:57,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run25
[2019-04-04 09:45:57,663] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:45:57,663] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:45:57,667] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run25
[2019-04-04 09:45:59,456] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5110036e-12 5.5783721e-11 1.2026374e-30 8.6248897e-29 5.0094778e-23
 1.0000000e+00 3.2653051e-23], sum to 1.0000
[2019-04-04 09:45:59,456] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7880
[2019-04-04 09:45:59,505] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.666666666666667, 49.0, 123.1666666666667, 851.3333333333334, 26.0, 26.0222279606549, 0.6650915160248404, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4624800.0000, 
sim time next is 4625400.0000, 
raw observation next is [3.833333333333333, 49.0, 126.3333333333333, 843.6666666666667, 26.0, 26.30920247871842, 0.6880108274211353, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5687903970452447, 0.49, 0.421111111111111, 0.9322283609576428, 0.6666666666666666, 0.6924335398932016, 0.7293369424737118, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2988998], dtype=float32), -0.22616442]. 
=============================================
[2019-04-04 09:45:59,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:45:59,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:45:59,716] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run25
[2019-04-04 09:46:12,049] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1555092e-10 3.6529020e-09 1.7121409e-26 1.7077594e-24 1.6868369e-20
 1.0000000e+00 7.7572449e-20], sum to 1.0000
[2019-04-04 09:46:12,058] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6213
[2019-04-04 09:46:12,073] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.2, 81.66666666666667, 0.0, 0.0, 26.0, 23.98805607905735, 0.0620555236769914, 0.0, 1.0, 43451.85834264755], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 99600.0000, 
sim time next is 100200.0000, 
raw observation next is [-3.3, 80.33333333333334, 0.0, 0.0, 26.0, 23.96206893636498, 0.05492950351684742, 0.0, 1.0, 43544.23825256022], 
processed observation next is [1.0, 0.13043478260869565, 0.37119113573407203, 0.8033333333333335, 0.0, 0.0, 0.6666666666666666, 0.4968390780304149, 0.5183098345056157, 0.0, 1.0, 0.20735351548838202], 
reward next is 0.7926, 
noisyNet noise sample is [array([-0.13129774], dtype=float32), -2.001513]. 
=============================================
[2019-04-04 09:46:12,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:46:12,862] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:46:12,866] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run25
[2019-04-04 09:46:18,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:46:18,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:46:18,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run25
[2019-04-04 09:46:20,012] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:46:20,013] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:46:20,018] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run25
[2019-04-04 09:46:21,921] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9429772e-10 2.3636471e-09 2.6050128e-26 6.3184026e-25 3.3736691e-20
 1.0000000e+00 7.6569621e-20], sum to 1.0000
[2019-04-04 09:46:21,922] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8845
[2019-04-04 09:46:21,976] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 44.00000000000001, 91.0, 673.3333333333333, 26.0, 25.77137750574359, 0.4708705380643021, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 306600.0000, 
sim time next is 307200.0000, 
raw observation next is [-9.5, 44.0, 93.0, 652.1666666666666, 26.0, 25.97094155626662, 0.5032931614020371, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.31, 0.7206261510128913, 0.6666666666666666, 0.664245129688885, 0.6677643871340123, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3060447], dtype=float32), -0.8511101]. 
=============================================
[2019-04-04 09:46:27,156] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0753095e-11 6.2122874e-10 8.1069178e-27 3.8082011e-25 1.7362582e-20
 1.0000000e+00 6.8956212e-20], sum to 1.0000
[2019-04-04 09:46:27,156] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2688
[2019-04-04 09:46:27,225] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.9860038516081, 0.212970498584752, 1.0, 1.0, 122702.7608786116], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 239400.0000, 
sim time next is 240000.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.81472652724255, 0.2239248037988446, 1.0, 1.0, 183159.1213545575], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5678938772702123, 0.5746416012662815, 1.0, 1.0, 0.8721862921645595], 
reward next is 0.1278, 
noisyNet noise sample is [array([0.1224738], dtype=float32), 2.1098237]. 
=============================================
[2019-04-04 09:46:27,260] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[80.72564 ]
 [80.187035]
 [80.13946 ]
 [80.19965 ]
 [80.31805 ]], R is [[80.91668701]
 [80.52322388]
 [80.59183502]
 [80.68766022]
 [80.88078308]].
[2019-04-04 09:46:28,046] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0227672e-10 1.2010524e-09 1.5288922e-26 8.8162474e-25 1.4392658e-20
 1.0000000e+00 2.0955979e-20], sum to 1.0000
[2019-04-04 09:46:28,046] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5549
[2019-04-04 09:46:28,073] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.833333333333333, 35.0, 0.0, 0.0, 26.0, 25.52726217692653, 0.4963743296096856, 0.0, 1.0, 18748.04428420163], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5008200.0000, 
sim time next is 5008800.0000, 
raw observation next is [2.666666666666667, 36.0, 0.0, 0.0, 26.0, 25.56169149581447, 0.4916532292248041, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.5364727608494922, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6301409579845391, 0.6638844097416013, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02123936], dtype=float32), -1.791987]. 
=============================================
[2019-04-04 09:46:35,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:46:35,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:46:35,444] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run25
[2019-04-04 09:47:00,487] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.1221863e-10 4.2271320e-09 1.1132113e-25 7.6706870e-24 1.5133815e-19
 1.0000000e+00 2.7722066e-19], sum to 1.0000
[2019-04-04 09:47:00,487] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9121
[2019-04-04 09:47:00,587] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.4, 60.0, 0.0, 0.0, 26.0, 25.01415507529488, 0.3138981392171897, 0.0, 1.0, 48549.37927791881], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 765600.0000, 
sim time next is 766200.0000, 
raw observation next is [-5.5, 60.5, 0.0, 0.0, 26.0, 24.99096251737947, 0.3062670323100874, 0.0, 1.0, 45529.56740340295], 
processed observation next is [1.0, 0.8695652173913043, 0.3102493074792244, 0.605, 0.0, 0.0, 0.6666666666666666, 0.5825802097816224, 0.6020890107700291, 0.0, 1.0, 0.21680746382572832], 
reward next is 0.7832, 
noisyNet noise sample is [array([-0.84663266], dtype=float32), 0.014466648]. 
=============================================
[2019-04-04 09:47:05,941] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.2657227e-11 5.6778837e-10 1.3912752e-27 4.6334090e-26 3.0498896e-21
 1.0000000e+00 2.3710481e-20], sum to 1.0000
[2019-04-04 09:47:05,942] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6755
[2019-04-04 09:47:06,073] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 85.33333333333334, 52.33333333333333, 0.0, 26.0, 25.67937809347852, 0.3509598485462918, 1.0, 1.0, 22005.41812779501], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 832200.0000, 
sim time next is 832800.0000, 
raw observation next is [-3.9, 84.66666666666667, 50.66666666666666, 0.0, 26.0, 24.92793568812104, 0.3439370558389168, 1.0, 1.0, 193178.6556842889], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.8466666666666667, 0.16888888888888887, 0.0, 0.6666666666666666, 0.5773279740100866, 0.6146456852796389, 1.0, 1.0, 0.9198983604013757], 
reward next is 0.0801, 
noisyNet noise sample is [array([1.6662104], dtype=float32), -0.67059547]. 
=============================================
[2019-04-04 09:47:07,647] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.309173e-09 2.236189e-08 6.508417e-23 3.944606e-22 9.521840e-18
 1.000000e+00 9.912371e-18], sum to 1.0000
[2019-04-04 09:47:07,647] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3923
[2019-04-04 09:47:07,721] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.3, 44.5, 0.0, 0.0, 26.0, 25.08647567909733, 0.2612665951545362, 0.0, 1.0, 41678.9329481505], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 419400.0000, 
sim time next is 420000.0000, 
raw observation next is [-10.4, 45.33333333333334, 0.0, 0.0, 26.0, 24.99429608169856, 0.2404732428808004, 0.0, 1.0, 44555.30048896237], 
processed observation next is [1.0, 0.8695652173913043, 0.1745152354570637, 0.4533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5828580068082134, 0.5801577476269335, 0.0, 1.0, 0.21216809756648747], 
reward next is 0.7878, 
noisyNet noise sample is [array([0.30176905], dtype=float32), 0.34875485]. 
=============================================
[2019-04-04 09:47:07,729] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[69.92244 ]
 [71.25366 ]
 [72.71929 ]
 [74.35893 ]
 [76.031654]], R is [[69.02193451]
 [69.13324738]
 [69.25993347]
 [69.30514526]
 [69.38005829]].
[2019-04-04 09:47:10,636] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.6945003e-10 1.9072011e-09 1.8429031e-25 5.4161183e-25 3.5804748e-19
 1.0000000e+00 3.7826439e-19], sum to 1.0000
[2019-04-04 09:47:10,640] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1041
[2019-04-04 09:47:10,670] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.9852773223724, 0.2906754786365842, 0.0, 1.0, 43257.47692760088], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 853200.0000, 
sim time next is 853800.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.94607574243452, 0.2828401695653838, 0.0, 1.0, 42643.26591630851], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5788396452028767, 0.5942800565217946, 0.0, 1.0, 0.20306317103004054], 
reward next is 0.7969, 
noisyNet noise sample is [array([0.5687251], dtype=float32), 0.33563042]. 
=============================================
[2019-04-04 09:47:13,925] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.8826089e-13 3.2185660e-11 1.2049885e-30 6.5184380e-28 3.3870229e-23
 1.0000000e+00 6.2371553e-23], sum to 1.0000
[2019-04-04 09:47:13,925] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7536
[2019-04-04 09:47:13,940] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.25, 95.0, 104.0, 0.0, 26.0, 25.22382141692294, 0.258847927491735, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 909000.0000, 
sim time next is 909600.0000, 
raw observation next is [3.433333333333334, 94.33333333333334, 102.6666666666667, 0.0, 26.0, 25.20177631312075, 0.2543113633604627, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5577100646352725, 0.9433333333333335, 0.3422222222222223, 0.0, 0.6666666666666666, 0.6001480260933958, 0.5847704544534876, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6955061], dtype=float32), 0.3117104]. 
=============================================
[2019-04-04 09:47:27,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.8912805e-12 3.6382754e-11 1.3977638e-29 5.2714672e-28 5.6294475e-24
 1.0000000e+00 7.3675506e-23], sum to 1.0000
[2019-04-04 09:47:27,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3616
[2019-04-04 09:47:27,592] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.9, 92.16666666666667, 12.0, 0.0, 26.0, 24.5101702530163, 0.3086919119286715, 1.0, 1.0, 196475.32414183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 924600.0000, 
sim time next is 925200.0000, 
raw observation next is [5.0, 92.0, 9.0, 0.0, 26.0, 24.99044345530142, 0.3533451154878812, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6011080332409973, 0.92, 0.03, 0.0, 0.6666666666666666, 0.5825369546084517, 0.6177817051626271, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0098575], dtype=float32), 1.8367394]. 
=============================================
[2019-04-04 09:47:33,274] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.7450170e-11 1.8901072e-10 1.3536905e-28 2.0196503e-27 1.1695113e-22
 1.0000000e+00 2.5893657e-21], sum to 1.0000
[2019-04-04 09:47:33,274] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7022
[2019-04-04 09:47:33,330] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.9670653035517, 0.4576913349279345, 1.0, 1.0, 67410.6078861627], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1367400.0000, 
sim time next is 1368000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.90458592281151, 0.462702716108531, 1.0, 1.0, 84083.72690126527], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5753821602342925, 0.6542342387028436, 1.0, 1.0, 0.4003986995298346], 
reward next is 0.5996, 
noisyNet noise sample is [array([0.03991387], dtype=float32), 0.8725933]. 
=============================================
[2019-04-04 09:47:33,344] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[87.06554 ]
 [86.32178 ]
 [87.820816]
 [89.5332  ]
 [91.38089 ]], R is [[87.76554108]
 [87.5668869 ]
 [87.60214996]
 [87.72612762]
 [87.84886932]].
[2019-04-04 09:47:40,603] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0110514e-10 1.1322402e-09 2.4499548e-28 3.3111460e-26 1.5693439e-21
 1.0000000e+00 8.9072674e-21], sum to 1.0000
[2019-04-04 09:47:40,603] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0140
[2019-04-04 09:47:40,615] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.35, 92.0, 0.0, 0.0, 26.0, 25.36074191733339, 0.4814286809023218, 0.0, 1.0, 87271.98881933163], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1463400.0000, 
sim time next is 1464000.0000, 
raw observation next is [1.433333333333333, 92.0, 0.0, 0.0, 26.0, 25.31060814992483, 0.4834933215247154, 0.0, 1.0, 60988.42064947961], 
processed observation next is [1.0, 0.9565217391304348, 0.502308402585411, 0.92, 0.0, 0.0, 0.6666666666666666, 0.609217345827069, 0.6611644405082385, 0.0, 1.0, 0.2904210507118077], 
reward next is 0.7096, 
noisyNet noise sample is [array([-1.0826458], dtype=float32), -0.40654805]. 
=============================================
[2019-04-04 09:47:40,719] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[86.53211 ]
 [86.130325]
 [85.85779 ]
 [85.625565]
 [85.43857 ]], R is [[86.67246246]
 [86.39015961]
 [86.22338867]
 [86.09946442]
 [86.05097961]].
[2019-04-04 09:47:43,939] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.3384220e-12 1.3949755e-10 1.3811527e-29 7.3033965e-28 4.7539639e-23
 1.0000000e+00 3.1821066e-22], sum to 1.0000
[2019-04-04 09:47:43,939] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0636
[2019-04-04 09:47:43,994] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 72.0, 0.0, 26.0, 25.53236008417364, 0.4925055921112884, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1434600.0000, 
sim time next is 1435200.0000, 
raw observation next is [1.1, 92.0, 67.66666666666667, 0.0, 26.0, 25.81534677440508, 0.5192126921428099, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.22555555555555556, 0.0, 0.6666666666666666, 0.6512788978670899, 0.6730708973809366, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8635841], dtype=float32), -0.3480324]. 
=============================================
[2019-04-04 09:47:51,454] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9904873e-11 1.8856432e-09 1.2895244e-28 6.7992646e-27 3.8093548e-22
 1.0000000e+00 7.1692376e-22], sum to 1.0000
[2019-04-04 09:47:51,454] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2587
[2019-04-04 09:47:51,487] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.266666666666667, 92.0, 0.0, 0.0, 26.0, 25.35417703860162, 0.5299962290331832, 0.0, 1.0, 45991.24661397842], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1320000.0000, 
sim time next is 1320600.0000, 
raw observation next is [1.183333333333333, 92.0, 0.0, 0.0, 26.0, 25.41026716782804, 0.5278991354783967, 0.0, 1.0, 21569.56273278581], 
processed observation next is [1.0, 0.2608695652173913, 0.49538319482917825, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6175222639856699, 0.6759663784927988, 0.0, 1.0, 0.10271220348945623], 
reward next is 0.8973, 
noisyNet noise sample is [array([0.6645896], dtype=float32), 0.41192338]. 
=============================================
[2019-04-04 09:47:59,208] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-04 09:47:59,209] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:47:59,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:47:59,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run34
[2019-04-04 09:47:59,243] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:47:59,244] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:47:59,245] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:47:59,247] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:47:59,252] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run34
[2019-04-04 09:47:59,280] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run34
[2019-04-04 09:48:26,962] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.26523453], dtype=float32), 0.29515913]
[2019-04-04 09:48:26,963] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-11.23333333333333, 65.33333333333334, 100.1666666666667, 594.8333333333334, 26.0, 25.65545526804141, 0.3732574187757657, 1.0, 1.0, 38234.50124292028]
[2019-04-04 09:48:26,963] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 09:48:26,963] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.0973122e-10 2.2563544e-09 4.4015240e-26 1.1082671e-24 9.2710410e-20
 1.0000000e+00 1.7618664e-19], sampled 0.6114431008744688
[2019-04-04 09:50:57,332] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.26523453], dtype=float32), 0.29515913]
[2019-04-04 09:50:57,332] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.0, 71.0, 0.0, 0.0, 26.0, 25.11805428492085, 0.3397329588196894, 0.0, 1.0, 44368.07542743854]
[2019-04-04 09:50:57,332] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 09:50:57,333] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.4493699e-10 7.4719448e-09 8.3506498e-26 4.0255028e-24 1.2639664e-19
 1.0000000e+00 5.0512436e-19], sampled 0.16255454279067638
[2019-04-04 09:51:07,760] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 09:51:25,306] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.26523453], dtype=float32), 0.29515913]
[2019-04-04 09:51:25,307] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.262653878333333, 59.985144985, 0.0, 0.0, 26.0, 25.57434841768686, 0.5180443059427636, 0.0, 1.0, 52017.4553865931]
[2019-04-04 09:51:25,307] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:51:25,308] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.6300873e-11 1.8245629e-09 6.4721786e-28 4.4635787e-26 3.3136563e-21
 1.0000000e+00 1.7078344e-20], sampled 0.7083500781010158
[2019-04-04 09:51:37,854] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.26523453], dtype=float32), 0.29515913]
[2019-04-04 09:51:37,854] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.666666666666667, 52.33333333333334, 0.0, 0.0, 26.0, 25.33591323692676, 0.3581783695203959, 0.0, 1.0, 46067.94785970592]
[2019-04-04 09:51:37,854] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 09:51:37,855] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.0856345e-10 9.7528208e-09 1.6240872e-25 6.6775900e-24 2.3647716e-19
 1.0000000e+00 8.1199187e-19], sampled 0.13161114733548496
[2019-04-04 09:51:39,779] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 09:51:40,400] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.26523453], dtype=float32), 0.29515913]
[2019-04-04 09:51:40,400] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.378541431, 61.80686614833333, 0.0, 0.0, 26.0, 25.26094074424252, 0.3393613154883817, 0.0, 1.0, 38802.12278062756]
[2019-04-04 09:51:40,401] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:51:40,402] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [9.02728114e-10 7.89601184e-09 3.01254640e-25 1.22620255e-23
 4.09294757e-19 1.00000000e+00 1.61636096e-18], sampled 0.4643331342866155
[2019-04-04 09:51:48,030] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 09:51:49,069] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 3300000, evaluation results [3300000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 09:51:58,559] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8087151e-11 1.3551554e-09 3.9376781e-28 1.7978712e-26 3.1683105e-22
 1.0000000e+00 2.4450900e-21], sum to 1.0000
[2019-04-04 09:51:58,559] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8080
[2019-04-04 09:51:58,656] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 26.0, 25.49721344091149, 0.4669725871571109, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1495800.0000, 
sim time next is 1496400.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 26.0, 25.59916012964676, 0.4606866259828561, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6332633441372298, 0.653562208660952, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01777958], dtype=float32), 0.34136948]. 
=============================================
[2019-04-04 09:52:04,365] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.8003294e-10 2.1456504e-08 9.0304458e-26 3.6061962e-23 8.5734608e-20
 1.0000000e+00 2.5582364e-18], sum to 1.0000
[2019-04-04 09:52:04,382] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2787
[2019-04-04 09:52:04,396] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.8, 66.33333333333333, 0.0, 0.0, 26.0, 24.75530546937011, 0.4134333579482296, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1191000.0000, 
sim time next is 1191600.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 26.0, 24.73275408079162, 0.4083854008196493, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5610628400659682, 0.6361284669398831, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.96706545], dtype=float32), -0.06590724]. 
=============================================
[2019-04-04 09:52:06,470] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.7712421e-11 4.0301353e-09 9.9884304e-27 3.7066691e-25 1.1933481e-20
 1.0000000e+00 2.3084502e-19], sum to 1.0000
[2019-04-04 09:52:06,470] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6662
[2019-04-04 09:52:06,536] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 84.33333333333333, 32.5, 0.0, 26.0, 25.14662596556043, 0.3772613953967406, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1759200.0000, 
sim time next is 1759800.0000, 
raw observation next is [-1.7, 83.66666666666667, 39.0, 0.0, 26.0, 25.21908165115707, 0.3722471807686765, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.4155124653739613, 0.8366666666666667, 0.13, 0.0, 0.6666666666666666, 0.6015901375964224, 0.6240823935895589, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27444997], dtype=float32), -0.51004136]. 
=============================================
[2019-04-04 09:52:17,557] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.5339814e-09 1.4573629e-08 6.2195226e-24 2.0908128e-22 1.0016750e-17
 1.0000000e+00 7.6870846e-18], sum to 1.0000
[2019-04-04 09:52:17,598] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0500
[2019-04-04 09:52:17,673] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.9, 78.0, 0.0, 0.0, 26.0, 24.0039979166686, -0.0001561880225752257, 0.0, 1.0, 44927.04006382676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1908600.0000, 
sim time next is 1909200.0000, 
raw observation next is [-8.0, 78.0, 0.0, 0.0, 26.0, 23.95224899300541, -0.008195107039153071, 0.0, 1.0, 44869.50574566777], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4960207494171174, 0.49726829765361563, 0.0, 1.0, 0.21366431307460843], 
reward next is 0.7863, 
noisyNet noise sample is [array([0.11130354], dtype=float32), 0.6192222]. 
=============================================
[2019-04-04 09:52:40,562] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5499166e-09 1.8915662e-08 6.5493513e-26 1.3321122e-23 4.1875163e-19
 1.0000000e+00 2.2952099e-18], sum to 1.0000
[2019-04-04 09:52:40,562] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8043
[2019-04-04 09:52:40,639] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 91.0, 17.5, 11.0, 26.0, 24.98529498360413, 0.181294501640451, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1929600.0000, 
sim time next is 1930200.0000, 
raw observation next is [-9.4, 90.16666666666667, 22.66666666666667, 14.33333333333334, 26.0, 25.13455262307534, 0.1799801618434729, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.20221606648199447, 0.9016666666666667, 0.07555555555555557, 0.015837937384898717, 0.6666666666666666, 0.594546051922945, 0.5599933872811577, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03097447], dtype=float32), -0.62126696]. 
=============================================
[2019-04-04 09:52:48,189] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.6032912e-10 1.2526311e-08 1.7887883e-24 4.2747274e-23 3.8743266e-19
 1.0000000e+00 2.9491884e-18], sum to 1.0000
[2019-04-04 09:52:48,190] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4298
[2019-04-04 09:52:48,238] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 86.0, 0.0, 0.0, 26.0, 24.12912042182202, 0.0977800970735589, 0.0, 1.0, 43749.01016495428], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2257200.0000, 
sim time next is 2257800.0000, 
raw observation next is [-7.9, 86.16666666666667, 0.0, 0.0, 26.0, 24.12671279334603, 0.100453989848291, 0.0, 1.0, 43737.09222634228], 
processed observation next is [1.0, 0.13043478260869565, 0.24376731301939059, 0.8616666666666667, 0.0, 0.0, 0.6666666666666666, 0.5105593994455025, 0.5334846632827637, 0.0, 1.0, 0.20827186774448705], 
reward next is 0.7917, 
noisyNet noise sample is [array([-0.13378397], dtype=float32), 0.3090695]. 
=============================================
[2019-04-04 09:53:12,378] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5044356e-09 5.4095679e-08 2.8848441e-23 1.0669135e-22 2.2318272e-18
 1.0000000e+00 1.9167772e-17], sum to 1.0000
[2019-04-04 09:53:12,391] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9763
[2019-04-04 09:53:12,416] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.40964648222254, -0.09015007620074189, 0.0, 1.0, 43041.73159504122], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2271600.0000, 
sim time next is 2272200.0000, 
raw observation next is [-9.5, 91.00000000000001, 0.0, 0.0, 26.0, 23.32763402322342, -0.1013975546867303, 0.0, 1.0, 43019.4081302207], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.9100000000000001, 0.0, 0.0, 0.6666666666666666, 0.4439695019352851, 0.46620081510442324, 0.0, 1.0, 0.20485432442962237], 
reward next is 0.7951, 
noisyNet noise sample is [array([-1.1612415], dtype=float32), 1.2612007]. 
=============================================
[2019-04-04 09:53:12,661] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5191087e-10 2.0120567e-08 5.2809554e-25 4.3197414e-23 7.0251405e-19
 1.0000000e+00 1.3029858e-18], sum to 1.0000
[2019-04-04 09:53:12,661] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0360
[2019-04-04 09:53:12,686] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 86.33333333333334, 0.0, 0.0, 26.0, 24.15635632653285, 0.09977555250091248, 0.0, 1.0, 43682.06169077683], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2258400.0000, 
sim time next is 2259000.0000, 
raw observation next is [-8.1, 86.5, 0.0, 0.0, 26.0, 24.17360041802624, 0.08459748717397525, 0.0, 1.0, 43643.5419218542], 
processed observation next is [1.0, 0.13043478260869565, 0.23822714681440446, 0.865, 0.0, 0.0, 0.6666666666666666, 0.5144667015021867, 0.528199162391325, 0.0, 1.0, 0.20782639010406762], 
reward next is 0.7922, 
noisyNet noise sample is [array([-0.26227885], dtype=float32), -0.9290821]. 
=============================================
[2019-04-04 09:53:12,707] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[74.69714]
 [74.7508 ]
 [74.76823]
 [74.786  ]
 [74.83405]], R is [[74.68128204]
 [74.72646332]
 [74.77092743]
 [74.814888  ]
 [74.85745239]].
[2019-04-04 09:53:20,133] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1435833e-10 3.9476049e-09 1.6142663e-25 9.9518574e-24 5.1669491e-19
 1.0000000e+00 7.8756512e-19], sum to 1.0000
[2019-04-04 09:53:20,136] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1155
[2019-04-04 09:53:20,167] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.716666666666667, 64.5, 0.0, 0.0, 26.0, 24.45867799707506, 0.171624819462872, 0.0, 1.0, 40101.42456039156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2346600.0000, 
sim time next is 2347200.0000, 
raw observation next is [-2.8, 65.0, 0.0, 0.0, 26.0, 24.42502008624383, 0.1717038562271072, 0.0, 1.0, 40217.43098249937], 
processed observation next is [0.0, 0.17391304347826086, 0.38504155124653744, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5354183405203191, 0.557234618742369, 0.0, 1.0, 0.19151157610713984], 
reward next is 0.8085, 
noisyNet noise sample is [array([-1.8031312], dtype=float32), 1.454666]. 
=============================================
[2019-04-04 09:53:44,334] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.3150470e-11 3.7421608e-09 2.7738774e-27 3.5670746e-26 4.4213252e-21
 1.0000000e+00 2.4684079e-20], sum to 1.0000
[2019-04-04 09:53:44,334] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6036
[2019-04-04 09:53:44,355] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.92064870867575, 0.2598245080553075, 0.0, 1.0, 55605.71007299794], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2865600.0000, 
sim time next is 2866200.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.92127598746496, 0.2581053946143239, 0.0, 1.0, 55708.26410397994], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5767729989554132, 0.5860351315381079, 0.0, 1.0, 0.2652774481141902], 
reward next is 0.7347, 
noisyNet noise sample is [array([1.4011332], dtype=float32), 0.00899343]. 
=============================================
[2019-04-04 09:53:52,035] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5686744e-11 9.2576585e-10 2.0330538e-27 1.6470973e-25 3.0947531e-21
 1.0000000e+00 1.9668381e-20], sum to 1.0000
[2019-04-04 09:53:52,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5031
[2019-04-04 09:53:52,061] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 83.83333333333334, 0.0, 0.0, 26.0, 25.2738424061761, 0.4795840062607046, 0.0, 1.0, 50967.34643909354], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2926200.0000, 
sim time next is 2926800.0000, 
raw observation next is [-1.0, 85.0, 0.0, 0.0, 26.0, 25.38446375204367, 0.4872166686795121, 0.0, 1.0, 39434.07477419823], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.85, 0.0, 0.0, 0.6666666666666666, 0.6153719793369724, 0.662405556226504, 0.0, 1.0, 0.18778130844856297], 
reward next is 0.8122, 
noisyNet noise sample is [array([2.4853487], dtype=float32), 1.0450535]. 
=============================================
[2019-04-04 09:53:55,106] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2524600e-10 5.0355511e-09 1.0026529e-26 5.5931200e-25 2.7710845e-20
 1.0000000e+00 4.0853665e-19], sum to 1.0000
[2019-04-04 09:53:55,109] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7008
[2019-04-04 09:53:55,149] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 104.0, 767.3333333333334, 26.0, 25.11105607057395, 0.4129695866293375, 0.0, 1.0, 18712.95613905228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2988600.0000, 
sim time next is 2989200.0000, 
raw observation next is [-2.0, 60.00000000000001, 102.5, 759.1666666666667, 26.0, 25.1486207520684, 0.4187886686817798, 0.0, 1.0, 18711.19869482028], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6000000000000001, 0.3416666666666667, 0.8388581952117865, 0.6666666666666666, 0.5957183960057, 0.6395962228939266, 0.0, 1.0, 0.08910094616581085], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.6108489], dtype=float32), 0.68841064]. 
=============================================
[2019-04-04 09:53:55,575] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5232680e-10 6.5146852e-09 4.2453279e-24 6.6053381e-23 2.6220708e-19
 1.0000000e+00 1.5575138e-18], sum to 1.0000
[2019-04-04 09:53:55,577] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0202
[2019-04-04 09:53:55,597] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.58924903235012, 0.1877108215244062, 0.0, 1.0, 41051.48715101536], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2775600.0000, 
sim time next is 2776200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.56610150657846, 0.1801164243818435, 0.0, 1.0, 40962.18864885659], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5471751255482049, 0.5600388081272811, 0.0, 1.0, 0.19505804118503137], 
reward next is 0.8049, 
noisyNet noise sample is [array([0.39512944], dtype=float32), -0.19577743]. 
=============================================
[2019-04-04 09:54:03,153] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3932213e-12 6.7720621e-11 1.1801734e-29 3.8697025e-27 7.2621872e-23
 1.0000000e+00 8.3622342e-23], sum to 1.0000
[2019-04-04 09:54:03,156] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1915
[2019-04-04 09:54:03,216] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.666666666666667, 33.33333333333334, 237.1666666666667, 258.3333333333333, 26.0, 25.86607507919634, 0.4809621615907574, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2812800.0000, 
sim time next is 2813400.0000, 
raw observation next is [5.0, 32.5, 249.0, 173.0, 26.0, 25.97397008534024, 0.4757877827349585, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6011080332409973, 0.325, 0.83, 0.19116022099447513, 0.6666666666666666, 0.6644975071116868, 0.6585959275783195, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40152085], dtype=float32), 0.643725]. 
=============================================
[2019-04-04 09:54:04,724] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0038157e-11 1.2102415e-09 2.8875218e-29 9.0647822e-27 4.5830203e-22
 1.0000000e+00 6.6838410e-22], sum to 1.0000
[2019-04-04 09:54:04,726] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5136
[2019-04-04 09:54:04,752] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.933333333333333, 100.0, 0.0, 0.0, 26.0, 25.26574266381709, 0.2988020174733481, 0.0, 1.0, 53785.87068932156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3127800.0000, 
sim time next is 3128400.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 26.0, 25.28959355688119, 0.3264403287357778, 0.0, 1.0, 54132.98238778719], 
processed observation next is [1.0, 0.21739130434782608, 0.5457063711911359, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6074661297400992, 0.608813442911926, 0.0, 1.0, 0.2577761066085104], 
reward next is 0.7422, 
noisyNet noise sample is [array([-1.2203916], dtype=float32), -0.018867254]. 
=============================================
[2019-04-04 09:54:06,292] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9154608e-12 4.6810322e-10 8.7096917e-30 1.8315955e-28 4.5794527e-23
 1.0000000e+00 6.9405038e-22], sum to 1.0000
[2019-04-04 09:54:06,296] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7896
[2019-04-04 09:54:06,346] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 57.00000000000001, 274.0, 26.0, 25.56230129063942, 0.516859984617234, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3226200.0000, 
sim time next is 3226800.0000, 
raw observation next is [-3.0, 92.0, 71.00000000000001, 322.0000000000001, 26.0, 25.53964188698459, 0.5276698461254069, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.23666666666666672, 0.3558011049723758, 0.6666666666666666, 0.6283034905820492, 0.675889948708469, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8538534], dtype=float32), -2.0541258]. 
=============================================
[2019-04-04 09:54:06,675] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.22347712e-11 2.34498465e-09 1.12469286e-26 5.50894899e-25
 1.01090163e-20 1.00000000e+00 1.02941005e-19], sum to 1.0000
[2019-04-04 09:54:06,678] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5047
[2019-04-04 09:54:06,731] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 142.3333333333333, 118.1666666666667, 26.0, 25.2632408209351, 0.3510032227721096, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2971200.0000, 
sim time next is 2971800.0000, 
raw observation next is [-4.0, 71.0, 154.0, 132.0, 26.0, 25.25170574357222, 0.3399100048688968, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.5133333333333333, 0.14585635359116023, 0.6666666666666666, 0.6043088119643517, 0.613303334956299, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15371946], dtype=float32), 0.24121529]. 
=============================================
[2019-04-04 09:54:08,784] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4103043e-10 1.5259689e-09 2.5787047e-25 1.1126903e-23 5.3693932e-20
 1.0000000e+00 1.5214332e-19], sum to 1.0000
[2019-04-04 09:54:08,787] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7914
[2019-04-04 09:54:08,811] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.97881222362728, 0.3490209482543565, 0.0, 1.0, 43816.30794589702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3291000.0000, 
sim time next is 3291600.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.93396243826961, 0.3468215930581953, 0.0, 1.0, 43831.28182407385], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5778302031891341, 0.6156071976860651, 0.0, 1.0, 0.2087203896384469], 
reward next is 0.7913, 
noisyNet noise sample is [array([1.6590867], dtype=float32), 0.2072305]. 
=============================================
[2019-04-04 09:54:18,818] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.70565385e-12 1.77261025e-10 5.20444575e-30 1.17415725e-27
 1.23132642e-22 1.00000000e+00 7.97583056e-23], sum to 1.0000
[2019-04-04 09:54:18,819] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2018
[2019-04-04 09:54:18,829] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.666666666666667, 48.66666666666666, 110.0, 770.6666666666667, 26.0, 26.56194727807935, 0.6346934217956243, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3408000.0000, 
sim time next is 3408600.0000, 
raw observation next is [2.833333333333333, 48.83333333333334, 111.0, 777.3333333333333, 26.0, 26.61375415919871, 0.6260421388667424, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.541089566020314, 0.48833333333333345, 0.37, 0.8589318600368323, 0.6666666666666666, 0.7178128465998924, 0.7086807129555809, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34857383], dtype=float32), 0.79938996]. 
=============================================
[2019-04-04 09:54:20,402] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3975160e-09 4.6967148e-08 2.2225426e-24 3.8876600e-22 1.1247623e-17
 1.0000000e+00 1.4219739e-17], sum to 1.0000
[2019-04-04 09:54:20,416] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8634
[2019-04-04 09:54:20,443] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 68.33333333333333, 14.66666666666666, 118.1666666666667, 26.0, 23.60128199499143, -0.04189955212326194, 0.0, 1.0, 40809.08473756488], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3051600.0000, 
sim time next is 3052200.0000, 
raw observation next is [-6.0, 66.16666666666667, 28.33333333333333, 166.3333333333333, 26.0, 23.57484467863249, -0.03754881190343886, 0.0, 1.0, 40758.49389258982], 
processed observation next is [0.0, 0.30434782608695654, 0.296398891966759, 0.6616666666666667, 0.09444444444444443, 0.18379373848987104, 0.6666666666666666, 0.46457038988604094, 0.48748372936552037, 0.0, 1.0, 0.19408806615518961], 
reward next is 0.8059, 
noisyNet noise sample is [array([-0.25931063], dtype=float32), 0.82948583]. 
=============================================
[2019-04-04 09:54:24,808] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1195915e-09 1.5372517e-08 3.8286939e-26 1.0692104e-24 2.7197699e-19
 1.0000000e+00 1.7947118e-19], sum to 1.0000
[2019-04-04 09:54:24,811] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4102
[2019-04-04 09:54:24,830] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 67.33333333333333, 0.0, 0.0, 26.0, 25.28312974642449, 0.4283465290737851, 0.0, 1.0, 41633.22304824161], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3548400.0000, 
sim time next is 3549000.0000, 
raw observation next is [-2.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 25.26358488615541, 0.4227217808857579, 0.0, 1.0, 41341.897082626], 
processed observation next is [0.0, 0.043478260869565216, 0.3841181902123731, 0.6916666666666668, 0.0, 0.0, 0.6666666666666666, 0.6052987405129509, 0.6409072602952527, 0.0, 1.0, 0.19686617658393332], 
reward next is 0.8031, 
noisyNet noise sample is [array([-0.8630936], dtype=float32), -2.2934413]. 
=============================================
[2019-04-04 09:54:24,833] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.11974 ]
 [79.115906]
 [79.25386 ]
 [79.253815]
 [79.42473 ]], R is [[79.19398499]
 [79.20379639]
 [79.21056366]
 [79.20949554]
 [79.18789673]].
[2019-04-04 09:54:33,804] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6384992e-10 1.8706763e-09 3.0141312e-26 7.1118887e-25 5.3103819e-20
 1.0000000e+00 3.8003886e-20], sum to 1.0000
[2019-04-04 09:54:33,806] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9562
[2019-04-04 09:54:33,821] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.43365758379669, 0.4670642463774375, 0.0, 1.0, 18763.23678944263], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3544200.0000, 
sim time next is 3544800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.42331566688306, 0.4558436557443978, 0.0, 1.0, 27994.92683058568], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6186096389069217, 0.6519478852481325, 0.0, 1.0, 0.13330917538374132], 
reward next is 0.8667, 
noisyNet noise sample is [array([-0.15097013], dtype=float32), 0.20337538]. 
=============================================
[2019-04-04 09:54:38,937] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7764263e-10 2.4254383e-09 3.1938128e-25 1.0761380e-24 1.6133478e-19
 1.0000000e+00 6.5340527e-19], sum to 1.0000
[2019-04-04 09:54:38,938] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0523
[2019-04-04 09:54:38,949] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 59.0, 12.5, 137.5, 26.0, 25.26085264557643, 0.375156519071405, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3692400.0000, 
sim time next is 3693000.0000, 
raw observation next is [4.0, 59.0, 0.0, 0.0, 26.0, 25.21185257893992, 0.3506281797166195, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.0, 0.0, 0.6666666666666666, 0.6009877149116599, 0.6168760599055398, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4709475], dtype=float32), -0.34192523]. 
=============================================
[2019-04-04 09:54:38,971] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[77.450325]
 [77.935074]
 [78.58469 ]
 [79.2179  ]
 [79.83133 ]], R is [[76.87983704]
 [77.11103821]
 [77.33992767]
 [77.56652832]
 [77.79086304]].
[2019-04-04 09:54:42,743] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.9896479e-11 9.7904462e-10 1.9163955e-26 1.4434025e-25 8.4958081e-21
 1.0000000e+00 2.1874305e-20], sum to 1.0000
[2019-04-04 09:54:42,744] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2447
[2019-04-04 09:54:42,765] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.09457698244581, 0.5138896008058634, 0.0, 1.0, 176895.1096957341], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3529800.0000, 
sim time next is 3530400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.19873575719157, 0.5527042484944661, 0.0, 1.0, 99876.54630343697], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5998946464326309, 0.6842347494981554, 0.0, 1.0, 0.475602601444938], 
reward next is 0.5244, 
noisyNet noise sample is [array([-0.3553501], dtype=float32), 0.91352606]. 
=============================================
[2019-04-04 09:54:43,171] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5672312e-11 1.9992443e-09 1.6165270e-26 3.7214888e-25 2.2745244e-20
 1.0000000e+00 6.3802780e-20], sum to 1.0000
[2019-04-04 09:54:43,171] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0100
[2019-04-04 09:54:43,212] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 157.3333333333333, 727.3333333333334, 26.0, 25.06007972440652, 0.3995171861438551, 0.0, 1.0, 27737.21779283871], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2983800.0000, 
sim time next is 2984400.0000, 
raw observation next is [-3.0, 65.0, 145.5, 745.5, 26.0, 25.06945416199754, 0.4026636917765954, 0.0, 1.0, 25374.3172812904], 
processed observation next is [0.0, 0.5652173913043478, 0.3795013850415513, 0.65, 0.485, 0.8237569060773481, 0.6666666666666666, 0.5891211801664618, 0.6342212305921985, 0.0, 1.0, 0.12083008229185906], 
reward next is 0.8792, 
noisyNet noise sample is [array([-0.1373785], dtype=float32), 0.82150894]. 
=============================================
[2019-04-04 09:54:47,877] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6906343e-09 4.0411425e-09 2.7949855e-25 9.0262183e-24 7.7058672e-20
 1.0000000e+00 1.2435406e-18], sum to 1.0000
[2019-04-04 09:54:47,878] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7766
[2019-04-04 09:54:47,974] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.333333333333334, 62.0, 5.0, 135.8333333333333, 26.0, 24.63974944397295, 0.3510473397003071, 1.0, 1.0, 202408.5437745771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3914400.0000, 
sim time next is 3915000.0000, 
raw observation next is [-7.5, 61.0, 6.0, 163.0, 26.0, 25.15133132929391, 0.3756300706309814, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.2548476454293629, 0.61, 0.02, 0.18011049723756906, 0.6666666666666666, 0.5959442774411592, 0.6252100235436605, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28596207], dtype=float32), 1.3910683]. 
=============================================
[2019-04-04 09:54:47,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[77.85314 ]
 [74.98927 ]
 [75.30034 ]
 [75.59613 ]
 [75.906265]], R is [[79.65306091]
 [78.89267731]
 [78.90016937]
 [78.90815735]
 [78.91676331]].
[2019-04-04 09:54:48,791] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1879818e-12 7.9147008e-11 2.5620505e-29 2.2593558e-28 1.0600979e-22
 1.0000000e+00 2.0884373e-22], sum to 1.0000
[2019-04-04 09:54:48,792] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8303
[2019-04-04 09:54:48,802] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 106.3333333333333, 789.3333333333334, 26.0, 26.47720208870515, 0.6855082562615107, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3507000.0000, 
sim time next is 3507600.0000, 
raw observation next is [3.0, 49.0, 104.1666666666667, 785.1666666666667, 26.0, 26.54188462157446, 0.7003240206606279, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.3472222222222223, 0.8675874769797423, 0.6666666666666666, 0.7118237184645384, 0.7334413402202093, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07444792], dtype=float32), -0.07533903]. 
=============================================
[2019-04-04 09:54:49,924] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.3278451e-10 3.3643381e-09 1.5658716e-26 2.1249031e-24 2.3496200e-19
 1.0000000e+00 1.7481355e-19], sum to 1.0000
[2019-04-04 09:54:49,927] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2874
[2019-04-04 09:54:49,948] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.33607999920739, 0.4396284035335473, 0.0, 1.0, 42354.78905017174], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3889200.0000, 
sim time next is 3889800.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.33954015869021, 0.4373681577692241, 0.0, 1.0, 40645.56966058978], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6116283465575174, 0.6457893859230747, 0.0, 1.0, 0.1935503317170942], 
reward next is 0.8064, 
noisyNet noise sample is [array([-1.4319571], dtype=float32), -0.20529674]. 
=============================================
[2019-04-04 09:54:51,233] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.4971221e-10 3.1964777e-09 1.0146045e-24 1.2925072e-23 2.6919125e-18
 1.0000000e+00 2.8350278e-18], sum to 1.0000
[2019-04-04 09:54:51,235] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3224
[2019-04-04 09:54:51,256] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 25.05753936482609, 0.3246581156473243, 0.0, 1.0, 50331.61781059264], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3694800.0000, 
sim time next is 3695400.0000, 
raw observation next is [4.0, 59.0, 0.0, 0.0, 26.0, 25.00237739927498, 0.3200075755725264, 0.0, 1.0, 58872.78515552622], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5835314499395817, 0.6066691918575088, 0.0, 1.0, 0.28034659597869627], 
reward next is 0.7197, 
noisyNet noise sample is [array([0.16539164], dtype=float32), 0.6940308]. 
=============================================
[2019-04-04 09:54:56,461] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0280314e-10 9.9331592e-09 2.2033021e-26 3.7813052e-25 5.7495604e-21
 1.0000000e+00 1.4712579e-19], sum to 1.0000
[2019-04-04 09:54:56,462] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1026
[2019-04-04 09:54:56,470] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.166666666666666, 27.33333333333333, 0.0, 0.0, 26.0, 25.47450123135517, 0.3782335464260771, 0.0, 1.0, 36726.58655489114], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3640200.0000, 
sim time next is 3640800.0000, 
raw observation next is [8.133333333333333, 27.66666666666667, 0.0, 0.0, 26.0, 25.58811054716416, 0.3783989453896491, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.687903970452447, 0.2766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6323425455970133, 0.6261329817965496, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1349104], dtype=float32), 0.33481812]. 
=============================================
[2019-04-04 09:54:59,150] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0780795e-09 2.2598012e-08 2.1685402e-25 1.0769517e-23 2.7921701e-19
 1.0000000e+00 8.6602429e-19], sum to 1.0000
[2019-04-04 09:54:59,154] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0808
[2019-04-04 09:54:59,182] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.7092586551794, 0.2306813620945843, 0.0, 1.0, 42838.74289356911], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3826800.0000, 
sim time next is 3827400.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.67794609851656, 0.223714851271624, 0.0, 1.0, 42782.27077960264], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5564955082097134, 0.5745716170905414, 0.0, 1.0, 0.20372509895048874], 
reward next is 0.7963, 
noisyNet noise sample is [array([-0.63056165], dtype=float32), 1.5687906]. 
=============================================
[2019-04-04 09:55:04,230] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.4619509e-10 1.7524318e-08 4.1583570e-25 8.2370150e-24 2.0292268e-19
 1.0000000e+00 2.1959186e-18], sum to 1.0000
[2019-04-04 09:55:04,230] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1637
[2019-04-04 09:55:04,254] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.5, 61.5, 0.0, 0.0, 26.0, 24.91646921109416, 0.2834501959202468, 0.0, 1.0, 42172.99310111064], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3911400.0000, 
sim time next is 3912000.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 26.0, 24.88488191500209, 0.268035678420005, 0.0, 1.0, 42333.27009512814], 
processed observation next is [1.0, 0.2608695652173913, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5737401595835075, 0.5893452261400016, 0.0, 1.0, 0.20158700045299113], 
reward next is 0.7984, 
noisyNet noise sample is [array([0.4301713], dtype=float32), -0.01268122]. 
=============================================
[2019-04-04 09:55:04,261] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[76.64154 ]
 [76.970406]
 [77.30835 ]
 [77.66367 ]
 [78.04448 ]], R is [[76.3609314 ]
 [76.39649963]
 [76.43241882]
 [76.46856689]
 [76.50505066]].
[2019-04-04 09:55:16,029] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9488728e-10 2.4990781e-08 7.7954111e-23 1.3635477e-21 7.7219709e-18
 1.0000000e+00 3.6470185e-17], sum to 1.0000
[2019-04-04 09:55:16,037] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7473
[2019-04-04 09:55:16,054] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.30210764143006, 0.1672751205715018, 0.0, 1.0, 43629.37500738484], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3985800.0000, 
sim time next is 3986400.0000, 
raw observation next is [-12.0, 63.00000000000001, 0.0, 0.0, 26.0, 24.33137535311473, 0.1547034263509527, 0.0, 1.0, 43591.62235624521], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.6300000000000001, 0.0, 0.0, 0.6666666666666666, 0.5276146127595608, 0.551567808783651, 0.0, 1.0, 0.20757915407735814], 
reward next is 0.7924, 
noisyNet noise sample is [array([0.01348347], dtype=float32), 0.4301231]. 
=============================================
[2019-04-04 09:55:24,391] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4060141e-10 7.9145126e-09 5.0957586e-26 1.6244412e-24 2.1765443e-19
 1.0000000e+00 2.6615444e-19], sum to 1.0000
[2019-04-04 09:55:24,395] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2353
[2019-04-04 09:55:24,419] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.47539049874435, 0.4377973618467235, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3892800.0000, 
sim time next is 3893400.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.46512111685252, 0.4225160503869707, 0.0, 1.0, 18758.00362928643], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6220934264043766, 0.6408386834623235, 0.0, 1.0, 0.08932382680612587], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.4821677], dtype=float32), 0.52997094]. 
=============================================
[2019-04-04 09:55:32,304] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.7255279e-11 1.6050137e-09 3.7722371e-28 9.0908627e-26 2.9541380e-21
 1.0000000e+00 1.5188867e-20], sum to 1.0000
[2019-04-04 09:55:32,305] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4031
[2019-04-04 09:55:32,328] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.166666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.54481373489709, 0.3699106131621615, 0.0, 1.0, 18745.22293622548], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4342800.0000, 
sim time next is 4343400.0000, 
raw observation next is [3.1, 73.0, 0.0, 0.0, 26.0, 25.50186325091608, 0.356472715436669, 0.0, 1.0, 41749.56516813437], 
processed observation next is [1.0, 0.2608695652173913, 0.5484764542936289, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6251552709096734, 0.6188242384788897, 0.0, 1.0, 0.19880745318159226], 
reward next is 0.8012, 
noisyNet noise sample is [array([-1.2338537], dtype=float32), 0.85156584]. 
=============================================
[2019-04-04 09:55:34,749] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0733469e-10 6.5144619e-09 5.8206299e-25 1.7795260e-23 1.0340429e-18
 1.0000000e+00 1.2989808e-18], sum to 1.0000
[2019-04-04 09:55:34,749] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3007
[2019-04-04 09:55:34,766] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.4716847403741, 0.2379575496930361, 0.0, 1.0, 40833.63693423656], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4765800.0000, 
sim time next is 4766400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.42534354139185, 0.2287178082184641, 0.0, 1.0, 40924.44628091031], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5354452951159875, 0.5762392694061548, 0.0, 1.0, 0.19487831562338245], 
reward next is 0.8051, 
noisyNet noise sample is [array([-0.86236084], dtype=float32), -0.4936454]. 
=============================================
[2019-04-04 09:55:35,868] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.51873788e-10 8.60433946e-09 1.46961557e-25 3.22994621e-24
 3.45334567e-19 1.00000000e+00 1.07768955e-19], sum to 1.0000
[2019-04-04 09:55:35,868] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7940
[2019-04-04 09:55:35,880] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.92910219753575, 0.3433225175117061, 0.0, 1.0, 40790.1924819708], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4758000.0000, 
sim time next is 4758600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.88978086189749, 0.3360410628705162, 0.0, 1.0, 40741.24003038948], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.574148405158124, 0.6120136876235054, 0.0, 1.0, 0.1940059049066166], 
reward next is 0.8060, 
noisyNet noise sample is [array([0.6802089], dtype=float32), 0.25484005]. 
=============================================
[2019-04-04 09:55:41,764] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0247889e-09 4.3882025e-08 1.4188330e-22 1.0837161e-20 1.9064804e-17
 1.0000000e+00 4.9478642e-17], sum to 1.0000
[2019-04-04 09:55:41,768] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5014
[2019-04-04 09:55:41,783] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.66666666666667, 67.0, 0.0, 0.0, 26.0, 23.969714757389, 0.06684340655518946, 0.0, 1.0, 43759.56993317055], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3991200.0000, 
sim time next is 3991800.0000, 
raw observation next is [-12.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.90680196957302, 0.05362692325071938, 0.0, 1.0, 43793.45552980535], 
processed observation next is [1.0, 0.17391304347826086, 0.10710987996306563, 0.68, 0.0, 0.0, 0.6666666666666666, 0.4922334974644184, 0.5178756410835731, 0.0, 1.0, 0.20854026442764453], 
reward next is 0.7915, 
noisyNet noise sample is [array([-0.57946765], dtype=float32), -1.1320662]. 
=============================================
[2019-04-04 09:55:43,549] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.27044848e-12 8.09752612e-11 4.86171072e-30 1.14464517e-28
 5.70115645e-23 1.00000000e+00 1.02246957e-22], sum to 1.0000
[2019-04-04 09:55:43,550] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7864
[2019-04-04 09:55:43,560] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.233333333333333, 49.0, 135.8333333333333, 820.6666666666666, 26.0, 26.80633331387475, 0.7718744695985459, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4627200.0000, 
sim time next is 4627800.0000, 
raw observation next is [4.35, 49.0, 139.0, 813.0, 26.0, 26.94443404217964, 0.7920955946272595, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5831024930747922, 0.49, 0.4633333333333333, 0.8983425414364641, 0.6666666666666666, 0.74536950351497, 0.7640318648757533, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5320085], dtype=float32), -0.38830435]. 
=============================================
[2019-04-04 09:55:44,044] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.44421797e-10 7.44671924e-09 1.79050077e-25 2.88315940e-24
 1.08320006e-19 1.00000000e+00 1.17058485e-18], sum to 1.0000
[2019-04-04 09:55:44,046] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0869
[2019-04-04 09:55:44,066] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.22901027629465, 0.3397253901781769, 0.0, 1.0, 39681.80228184686], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4842000.0000, 
sim time next is 4842600.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.21210644186593, 0.335744812322218, 0.0, 1.0, 39400.18692792133], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6010088701554942, 0.6119149374407393, 0.0, 1.0, 0.18761993775200633], 
reward next is 0.8124, 
noisyNet noise sample is [array([-0.5886801], dtype=float32), 1.4368719]. 
=============================================
[2019-04-04 09:55:44,396] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9229608e-10 5.9053971e-09 4.9580283e-25 6.0660715e-23 1.3971426e-18
 1.0000000e+00 3.5218211e-18], sum to 1.0000
[2019-04-04 09:55:44,400] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6544
[2019-04-04 09:55:44,432] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 26.0, 24.97261230933863, 0.2913035727826439, 0.0, 1.0, 32135.8126420743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4906800.0000, 
sim time next is 4907400.0000, 
raw observation next is [1.0, 45.83333333333334, 0.0, 0.0, 26.0, 24.97224140495646, 0.307916860087476, 0.0, 1.0, 28972.16186179798], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4583333333333334, 0.0, 0.0, 0.6666666666666666, 0.581020117079705, 0.602638953362492, 0.0, 1.0, 0.13796267553237135], 
reward next is 0.8620, 
noisyNet noise sample is [array([-1.4597936], dtype=float32), -0.31749773]. 
=============================================
[2019-04-04 09:55:47,416] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2169966e-12 2.6815719e-10 1.4277749e-30 8.7806523e-29 3.1119985e-23
 1.0000000e+00 3.5073595e-23], sum to 1.0000
[2019-04-04 09:55:47,419] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1710
[2019-04-04 09:55:47,432] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 209.6666666666667, 6.0, 26.0, 26.48195052342597, 0.5986997508662572, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4704600.0000, 
sim time next is 4705200.0000, 
raw observation next is [0.0, 92.0, 210.5, 6.0, 26.0, 26.47892000505702, 0.5933693985711689, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.92, 0.7016666666666667, 0.0066298342541436465, 0.6666666666666666, 0.706576667088085, 0.697789799523723, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.1164482], dtype=float32), 0.45212936]. 
=============================================
[2019-04-04 09:55:51,449] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3356341e-12 2.3296789e-10 7.4499966e-29 7.2374165e-28 1.6703786e-22
 1.0000000e+00 1.4222151e-21], sum to 1.0000
[2019-04-04 09:55:51,450] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6846
[2019-04-04 09:55:51,458] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 89.0, 102.0, 0.0, 26.0, 26.30773257449772, 0.5828733671998952, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4451400.0000, 
sim time next is 4452000.0000, 
raw observation next is [0.3333333333333334, 90.0, 117.6666666666667, 0.9999999999999998, 26.0, 26.1900624332228, 0.5645082588346216, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4718374884579871, 0.9, 0.3922222222222223, 0.0011049723756906074, 0.6666666666666666, 0.6825052027685666, 0.6881694196115405, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7730875], dtype=float32), -0.31786907]. 
=============================================
[2019-04-04 09:55:51,503] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[87.44933 ]
 [87.6792  ]
 [87.94541 ]
 [88.3071  ]
 [88.761566]], R is [[87.62236786]
 [87.74614716]
 [87.86868286]
 [87.98999786]
 [88.11009979]].
[2019-04-04 09:55:51,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:55:51,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:55:51,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run26
[2019-04-04 09:55:53,422] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:55:53,422] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:55:53,434] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run26
[2019-04-04 09:55:54,675] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:55:54,675] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:55:54,679] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run26
[2019-04-04 09:55:57,205] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.0101895e-10 1.9836000e-08 1.0606429e-24 5.1487582e-23 6.8679286e-19
 1.0000000e+00 2.7869679e-18], sum to 1.0000
[2019-04-04 09:55:57,206] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7187
[2019-04-04 09:55:57,220] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 68.0, 0.0, 0.0, 26.0, 24.41981501051524, 0.140039107574511, 0.0, 1.0, 39593.17724543506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4865400.0000, 
sim time next is 4866000.0000, 
raw observation next is [-4.0, 69.0, 23.5, 52.16666666666666, 26.0, 24.39226497992848, 0.1438442675727114, 0.0, 1.0, 39564.89206187944], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.69, 0.07833333333333334, 0.05764272559852669, 0.6666666666666666, 0.5326887483273733, 0.5479480891909038, 0.0, 1.0, 0.1884042479137116], 
reward next is 0.8116, 
noisyNet noise sample is [array([0.35195377], dtype=float32), 0.46785504]. 
=============================================
[2019-04-04 09:55:57,240] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[75.69464]
 [75.75549]
 [75.80821]
 [75.88948]
 [75.97895]], R is [[76.11992645]
 [76.1701889 ]
 [76.22003174]
 [76.26948547]
 [76.31859589]].
[2019-04-04 09:55:58,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:55:58,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:55:58,146] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run26
[2019-04-04 09:55:58,819] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:55:58,819] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:55:58,850] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run26
[2019-04-04 09:56:00,536] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.9184817e-10 1.2521031e-08 3.9199243e-25 8.2516705e-24 4.2028315e-19
 1.0000000e+00 1.0427893e-18], sum to 1.0000
[2019-04-04 09:56:00,538] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8325
[2019-04-04 09:56:00,551] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.01456860883383, 0.2341929762795808, 0.0, 1.0, 38658.54846981997], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4946400.0000, 
sim time next is 4947000.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.03284804188025, 0.2371823602593648, 0.0, 1.0, 38643.43876188427], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5860706701566875, 0.5790607867531216, 0.0, 1.0, 0.18401637505659174], 
reward next is 0.8160, 
noisyNet noise sample is [array([1.0409591], dtype=float32), -0.26448193]. 
=============================================
[2019-04-04 09:56:00,557] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.45621 ]
 [76.600334]
 [76.728386]
 [76.87624 ]
 [77.00278 ]], R is [[76.36621857]
 [76.41846466]
 [76.47025299]
 [76.52173615]
 [76.57292175]].
[2019-04-04 09:56:00,679] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:56:00,679] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:56:00,703] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run26
[2019-04-04 09:56:01,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5204568e-10 2.2712641e-09 1.5115422e-25 1.0149839e-24 1.7113439e-19
 1.0000000e+00 2.9328966e-19], sum to 1.0000
[2019-04-04 09:56:01,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7624
[2019-04-04 09:56:01,764] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.666666666666667, 44.66666666666667, 223.8333333333333, 382.0, 26.0, 25.06780340484223, 0.3681100097490737, 0.0, 1.0, 18689.54943639398], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4891200.0000, 
sim time next is 4891800.0000, 
raw observation next is [2.833333333333333, 44.83333333333333, 211.6666666666667, 390.0, 26.0, 25.06760510681794, 0.3697907314977647, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.541089566020314, 0.4483333333333333, 0.7055555555555557, 0.430939226519337, 0.6666666666666666, 0.5889670922348283, 0.6232635771659215, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25505558], dtype=float32), 0.05752469]. 
=============================================
[2019-04-04 09:56:02,810] A3C_AGENT_WORKER-Thread-16 INFO:Local step 212500, global step 3393461: loss 0.4994
[2019-04-04 09:56:02,810] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 212500, global step 3393461: learning rate 0.0000
[2019-04-04 09:56:04,417] A3C_AGENT_WORKER-Thread-15 INFO:Local step 212500, global step 3394111: loss 0.5047
[2019-04-04 09:56:04,418] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 212500, global step 3394111: learning rate 0.0000
[2019-04-04 09:56:04,671] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1792578e-10 6.7781392e-10 7.9509653e-27 2.3495142e-25 5.7537730e-21
 1.0000000e+00 3.5052515e-20], sum to 1.0000
[2019-04-04 09:56:04,672] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3790
[2019-04-04 09:56:04,679] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 26.0, 26.07846349058602, 0.5994400677087498, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4996200.0000, 
sim time next is 4996800.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 26.0, 26.04917917699689, 0.5871874516033936, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6288088642659281, 0.23, 0.0, 0.0, 0.6666666666666666, 0.6707649314164076, 0.6957291505344645, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14350854], dtype=float32), -0.35304934]. 
=============================================
[2019-04-04 09:56:05,705] A3C_AGENT_WORKER-Thread-2 INFO:Local step 212500, global step 3394724: loss 0.4948
[2019-04-04 09:56:05,706] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 212500, global step 3394724: learning rate 0.0000
[2019-04-04 09:56:09,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:56:09,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:56:09,255] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run26
[2019-04-04 09:56:09,483] A3C_AGENT_WORKER-Thread-6 INFO:Local step 212500, global step 3396157: loss 0.5056
[2019-04-04 09:56:09,484] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 212500, global step 3396157: learning rate 0.0000
[2019-04-04 09:56:09,574] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:56:09,575] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:56:09,615] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run26
[2019-04-04 09:56:10,849] A3C_AGENT_WORKER-Thread-4 INFO:Local step 212500, global step 3396426: loss 0.5003
[2019-04-04 09:56:10,858] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 212500, global step 3396426: learning rate 0.0000
[2019-04-04 09:56:11,862] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:56:11,863] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:56:11,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run26
[2019-04-04 09:56:12,070] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:56:12,071] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:56:12,076] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run26
[2019-04-04 09:56:14,163] A3C_AGENT_WORKER-Thread-3 INFO:Local step 212500, global step 3396979: loss 0.4966
[2019-04-04 09:56:14,164] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 212500, global step 3396979: learning rate 0.0000
[2019-04-04 09:56:16,273] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7271158e-11 7.0601247e-10 1.3758220e-27 2.9731364e-25 1.0553902e-20
 1.0000000e+00 3.7659837e-20], sum to 1.0000
[2019-04-04 09:56:16,274] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0756
[2019-04-04 09:56:16,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:56:16,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:56:16,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run26
[2019-04-04 09:56:16,370] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.666666666666667, 38.0, 208.0, 597.0, 26.0, 25.14282636235414, 0.4335088887819259, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4801200.0000, 
sim time next is 4801800.0000, 
raw observation next is [2.833333333333333, 37.5, 196.0, 626.0, 26.0, 25.13613329246367, 0.433955080993593, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.541089566020314, 0.375, 0.6533333333333333, 0.6917127071823205, 0.6666666666666666, 0.5946777743719723, 0.644651693664531, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7092954], dtype=float32), 0.61042583]. 
=============================================
[2019-04-04 09:56:19,205] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0431306e-12 3.1756972e-11 2.4205956e-31 2.2848180e-30 3.8729722e-24
 1.0000000e+00 7.3425477e-24], sum to 1.0000
[2019-04-04 09:56:19,205] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2030
[2019-04-04 09:56:19,227] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.33333333333333, 21.66666666666666, 116.8333333333333, 853.1666666666667, 26.0, 27.5556685253427, 0.927284787031228, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5060400.0000, 
sim time next is 5061000.0000, 
raw observation next is [10.66666666666667, 20.83333333333334, 115.6666666666667, 846.3333333333333, 26.0, 26.9592952231437, 0.891899201051462, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.7580794090489382, 0.2083333333333334, 0.38555555555555565, 0.9351749539594842, 0.6666666666666666, 0.7466079352619751, 0.7972997336838207, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.95961225], dtype=float32), 1.6355233]. 
=============================================
[2019-04-04 09:56:19,335] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[95.05671]
 [94.8739 ]
 [94.76416]
 [94.53408]
 [94.27567]], R is [[95.15677643]
 [95.20520782]
 [95.25315857]
 [95.30062866]
 [95.34762573]].
[2019-04-04 09:56:24,318] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.4403329e-09 5.8491012e-09 9.5307145e-24 3.1997322e-22 1.3196481e-18
 1.0000000e+00 3.4939877e-18], sum to 1.0000
[2019-04-04 09:56:24,318] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2846
[2019-04-04 09:56:24,363] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 69.0, 0.0, 0.0, 26.0, 24.53584637074417, 0.1906810797279336, 0.0, 1.0, 45854.47657743898], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 163200.0000, 
sim time next is 163800.0000, 
raw observation next is [-8.4, 69.5, 0.0, 0.0, 26.0, 24.44995635935216, 0.1770119743063205, 0.0, 1.0, 45818.9281230924], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.695, 0.0, 0.0, 0.6666666666666666, 0.5374963632793467, 0.5590039914354402, 0.0, 1.0, 0.21818537201472574], 
reward next is 0.7818, 
noisyNet noise sample is [array([-0.00630851], dtype=float32), 0.41537458]. 
=============================================
[2019-04-04 09:56:24,369] A3C_AGENT_WORKER-Thread-11 INFO:Local step 212500, global step 3398818: loss 0.5082
[2019-04-04 09:56:24,370] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 212500, global step 3398818: learning rate 0.0000
[2019-04-04 09:56:24,399] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:56:24,399] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:56:24,443] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run26
[2019-04-04 09:56:25,882] A3C_AGENT_WORKER-Thread-19 INFO:Local step 212500, global step 3399120: loss 0.4996
[2019-04-04 09:56:25,882] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 212500, global step 3399120: learning rate 0.0000
[2019-04-04 09:56:26,783] A3C_AGENT_WORKER-Thread-18 INFO:Local step 212500, global step 3399319: loss 0.5078
[2019-04-04 09:56:26,783] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 212500, global step 3399319: learning rate 0.0000
[2019-04-04 09:56:27,320] A3C_AGENT_WORKER-Thread-12 INFO:Local step 212500, global step 3399439: loss 0.4842
[2019-04-04 09:56:27,320] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 212500, global step 3399439: learning rate 0.0000
[2019-04-04 09:56:29,734] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-04 09:56:29,741] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:56:29,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:56:29,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run35
[2019-04-04 09:56:29,814] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:56:29,817] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:56:29,820] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run35
[2019-04-04 09:56:29,815] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:56:29,938] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:56:29,941] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run35
[2019-04-04 09:57:30,110] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.26335272], dtype=float32), 0.29794872]
[2019-04-04 09:57:30,110] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.1, 58.0, 0.0, 0.0, 26.0, 24.88877613459296, 0.4063226461363294, 0.0, 1.0, 67578.58851281657]
[2019-04-04 09:57:30,110] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 09:57:30,111] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.5002136e-10 2.3092852e-09 2.3102048e-25 8.6553329e-24 1.6736533e-19
 1.0000000e+00 8.1232341e-19], sampled 0.48149189033268713
[2019-04-04 09:58:11,075] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.26335272], dtype=float32), 0.29794872]
[2019-04-04 09:58:11,075] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.585858141666667, 72.27645834333333, 154.73933965, 0.0, 26.0, 25.05606929962993, 0.2434963994559918, 1.0, 1.0, 129035.2657175986]
[2019-04-04 09:58:11,075] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:58:11,076] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.3286649e-11 8.5622776e-10 2.4070280e-27 1.8210713e-25 7.0186776e-21
 1.0000000e+00 2.7087414e-20], sampled 0.09818751217142907
[2019-04-04 09:58:42,496] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.26335272], dtype=float32), 0.29794872]
[2019-04-04 09:58:42,496] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-10.738118, 84.75417288666667, 0.0, 0.0, 26.0, 23.6436916667849, 0.002934639611344737, 0.0, 1.0, 44877.84784841223]
[2019-04-04 09:58:42,496] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:58:42,497] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.8038566e-09 1.9198520e-08 9.5838696e-24 5.0241397e-22 4.0539472e-18
 1.0000000e+00 1.8669117e-17], sampled 0.029304285927417073
[2019-04-04 09:59:37,702] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 10:00:13,699] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 10:00:19,122] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 10:00:20,166] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 3400000, evaluation results [3400000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 10:00:21,746] A3C_AGENT_WORKER-Thread-13 INFO:Local step 212500, global step 3400332: loss 0.4960
[2019-04-04 10:00:21,747] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 212500, global step 3400332: learning rate 0.0000
[2019-04-04 10:00:27,558] A3C_AGENT_WORKER-Thread-16 INFO:Local step 213000, global step 3401388: loss 0.0019
[2019-04-04 10:00:27,558] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 213000, global step 3401388: learning rate 0.0000
[2019-04-04 10:00:28,696] A3C_AGENT_WORKER-Thread-10 INFO:Local step 212500, global step 3401635: loss 0.4864
[2019-04-04 10:00:28,697] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 212500, global step 3401635: learning rate 0.0000
[2019-04-04 10:00:30,174] A3C_AGENT_WORKER-Thread-15 INFO:Local step 213000, global step 3401980: loss 0.0013
[2019-04-04 10:00:30,219] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 213000, global step 3401980: learning rate 0.0000
[2019-04-04 10:00:34,900] A3C_AGENT_WORKER-Thread-2 INFO:Local step 213000, global step 3402956: loss 0.0019
[2019-04-04 10:00:34,944] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 213000, global step 3402956: learning rate 0.0000
[2019-04-04 10:00:37,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:00:37,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:00:37,692] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run26
[2019-04-04 10:00:40,008] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:00:40,008] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:00:40,012] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run26
[2019-04-04 10:00:41,102] A3C_AGENT_WORKER-Thread-6 INFO:Local step 213000, global step 3404059: loss 0.0020
[2019-04-04 10:00:41,104] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 213000, global step 3404059: learning rate 0.0000
[2019-04-04 10:00:41,628] A3C_AGENT_WORKER-Thread-4 INFO:Local step 213000, global step 3404175: loss 0.0014
[2019-04-04 10:00:41,628] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 213000, global step 3404175: learning rate 0.0000
[2019-04-04 10:00:42,422] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.3865116e-10 1.2799050e-08 1.0208206e-23 1.4802799e-22 3.4498318e-18
 1.0000000e+00 9.8070211e-18], sum to 1.0000
[2019-04-04 10:00:42,422] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7550
[2019-04-04 10:00:42,498] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.916666666666667, 32.83333333333334, 49.0, 0.0, 26.0, 25.55872239528254, 0.2006265602407246, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 465000.0000, 
sim time next is 465600.0000, 
raw observation next is [-5.633333333333334, 32.66666666666667, 55.50000000000001, 0.0, 26.0, 25.53908626653034, 0.1981979462165576, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.30655586334256696, 0.3266666666666667, 0.18500000000000003, 0.0, 0.6666666666666666, 0.6282571888775283, 0.5660659820721858, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34129444], dtype=float32), 0.80689096]. 
=============================================
[2019-04-04 10:00:43,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:00:43,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:00:43,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run26
[2019-04-04 10:00:45,129] A3C_AGENT_WORKER-Thread-3 INFO:Local step 213000, global step 3404769: loss 0.0021
[2019-04-04 10:00:45,129] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 213000, global step 3404769: learning rate 0.0000
[2019-04-04 10:00:52,037] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8034229e-11 5.7481453e-10 3.3102745e-26 5.0730299e-25 5.9913592e-21
 1.0000000e+00 2.8374050e-20], sum to 1.0000
[2019-04-04 10:00:52,038] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9626
[2019-04-04 10:00:52,092] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 63.5, 18.0, 0.0, 26.0, 25.29294299514745, 0.2733345677193959, 1.0, 1.0, 35724.52795636204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 232200.0000, 
sim time next is 232800.0000, 
raw observation next is [-3.4, 64.0, 15.0, 0.0, 26.0, 25.33269028382034, 0.2775016130794676, 1.0, 1.0, 32760.21878189845], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.64, 0.05, 0.0, 0.6666666666666666, 0.6110575236516951, 0.5925005376931559, 1.0, 1.0, 0.15600104181856406], 
reward next is 0.8440, 
noisyNet noise sample is [array([0.16245596], dtype=float32), -1.2227814]. 
=============================================
[2019-04-04 10:00:52,112] A3C_AGENT_WORKER-Thread-5 INFO:Local step 212500, global step 3406139: loss 0.5022
[2019-04-04 10:00:52,124] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 212500, global step 3406139: learning rate 0.0000
[2019-04-04 10:00:53,788] A3C_AGENT_WORKER-Thread-11 INFO:Local step 213000, global step 3406541: loss 0.0025
[2019-04-04 10:00:53,790] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 213000, global step 3406541: learning rate 0.0000
[2019-04-04 10:00:54,268] A3C_AGENT_WORKER-Thread-14 INFO:Local step 212500, global step 3406648: loss 0.5120
[2019-04-04 10:00:54,269] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 212500, global step 3406648: learning rate 0.0000
[2019-04-04 10:00:56,102] A3C_AGENT_WORKER-Thread-19 INFO:Local step 213000, global step 3407132: loss 0.0033
[2019-04-04 10:00:56,106] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 213000, global step 3407132: learning rate 0.0000
[2019-04-04 10:00:56,978] A3C_AGENT_WORKER-Thread-12 INFO:Local step 213000, global step 3407391: loss 0.0029
[2019-04-04 10:00:56,979] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 213000, global step 3407391: learning rate 0.0000
[2019-04-04 10:00:57,412] A3C_AGENT_WORKER-Thread-18 INFO:Local step 213000, global step 3407486: loss 0.0028
[2019-04-04 10:00:57,413] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 213000, global step 3407486: learning rate 0.0000
[2019-04-04 10:00:57,478] A3C_AGENT_WORKER-Thread-17 INFO:Local step 212500, global step 3407500: loss 0.4974
[2019-04-04 10:00:57,478] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 212500, global step 3407500: learning rate 0.0000
[2019-04-04 10:00:59,105] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0400172e-13 1.6752377e-11 2.4354415e-32 5.9060549e-30 6.3292373e-24
 1.0000000e+00 2.7867998e-24], sum to 1.0000
[2019-04-04 10:00:59,108] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4627
[2019-04-04 10:00:59,115] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.66666666666667, 19.33333333333334, 108.5, 806.6666666666666, 26.0, 28.42059398450408, 1.077778786164859, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5064000.0000, 
sim time next is 5064600.0000, 
raw observation next is [11.83333333333333, 19.16666666666667, 106.0, 794.3333333333334, 26.0, 28.5152730502184, 1.090729919232029, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7903970452446908, 0.1916666666666667, 0.35333333333333333, 0.8777163904235727, 0.6666666666666666, 0.8762727541848667, 0.8635766397440096, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8466635], dtype=float32), -0.7083779]. 
=============================================
[2019-04-04 10:00:59,198] A3C_AGENT_WORKER-Thread-13 INFO:Local step 213000, global step 3407952: loss 0.0034
[2019-04-04 10:00:59,198] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 213000, global step 3407952: learning rate 0.0000
[2019-04-04 10:01:00,258] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.6491401e-11 1.3468118e-09 2.0610020e-26 3.3489517e-25 5.9517002e-21
 1.0000000e+00 1.2527530e-19], sum to 1.0000
[2019-04-04 10:01:00,260] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9439
[2019-04-04 10:01:00,312] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 61.0, 41.0, 4.5, 26.0, 25.37777015165775, 0.2564031532495906, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 118800.0000, 
sim time next is 119400.0000, 
raw observation next is [-7.8, 63.16666666666667, 42.33333333333334, 2.999999999999999, 26.0, 25.31498845580347, 0.2533325595360833, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.6316666666666667, 0.14111111111111113, 0.0033149171270718224, 0.6666666666666666, 0.6095823713169558, 0.5844441865120278, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39344102], dtype=float32), -0.89166415]. 
=============================================
[2019-04-04 10:01:02,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:01:02,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:01:02,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run26
[2019-04-04 10:01:04,076] A3C_AGENT_WORKER-Thread-16 INFO:Local step 213500, global step 3409127: loss 0.3363
[2019-04-04 10:01:04,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 213500, global step 3409127: learning rate 0.0000
[2019-04-04 10:01:05,739] A3C_AGENT_WORKER-Thread-10 INFO:Local step 213000, global step 3409562: loss 0.0028
[2019-04-04 10:01:05,740] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 213000, global step 3409562: learning rate 0.0000
[2019-04-04 10:01:05,762] A3C_AGENT_WORKER-Thread-15 INFO:Local step 213500, global step 3409568: loss 0.3468
[2019-04-04 10:01:05,763] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 213500, global step 3409568: learning rate 0.0000
[2019-04-04 10:01:09,234] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6065450e-10 8.9035807e-10 1.4601319e-26 1.0868563e-24 4.6408703e-20
 1.0000000e+00 2.7303940e-19], sum to 1.0000
[2019-04-04 10:01:09,235] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0524
[2019-04-04 10:01:09,266] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 83.0, 0.0, 0.0, 26.0, 24.85457395876141, 0.2462125079218569, 0.0, 1.0, 42900.84296731681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 598800.0000, 
sim time next is 599400.0000, 
raw observation next is [-3.1, 83.0, 0.0, 0.0, 26.0, 24.85826504054144, 0.241453128248175, 0.0, 1.0, 42839.54325290764], 
processed observation next is [0.0, 0.9565217391304348, 0.37673130193905824, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5715220867117866, 0.5804843760827251, 0.0, 1.0, 0.20399782501384592], 
reward next is 0.7960, 
noisyNet noise sample is [array([1.0429724], dtype=float32), 1.0454403]. 
=============================================
[2019-04-04 10:01:10,467] A3C_AGENT_WORKER-Thread-2 INFO:Local step 213500, global step 3410759: loss 0.3727
[2019-04-04 10:01:10,467] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 213500, global step 3410759: learning rate 0.0000
[2019-04-04 10:01:12,871] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.8380392e-11 8.1612578e-10 7.6127196e-28 7.0691761e-26 1.9636668e-20
 1.0000000e+00 3.8657769e-20], sum to 1.0000
[2019-04-04 10:01:12,871] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2713
[2019-04-04 10:01:12,956] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 25.03397639193323, 0.2163734678090988, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 500400.0000, 
sim time next is 501000.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 24.92949352611359, 0.1986937607693117, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.6666666666666666, 0.577457793842799, 0.5662312535897706, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17045331], dtype=float32), 1.0153743]. 
=============================================
[2019-04-04 10:01:12,974] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.22372 ]
 [84.686325]
 [86.21427 ]
 [85.8765  ]
 [85.63509 ]], R is [[83.8549881 ]
 [84.01644135]
 [84.17627716]
 [84.33451843]
 [84.49117279]].
[2019-04-04 10:01:14,524] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1667273e-11 1.2753790e-09 9.7427240e-28 8.2399289e-26 3.2776507e-21
 1.0000000e+00 1.6768413e-20], sum to 1.0000
[2019-04-04 10:01:14,526] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6968
[2019-04-04 10:01:14,544] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.516666666666667, 85.50000000000001, 0.0, 0.0, 26.0, 24.52785407590132, 0.178226500107922, 0.0, 1.0, 40623.16899193232], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 537000.0000, 
sim time next is 537600.0000, 
raw observation next is [1.433333333333334, 86.0, 0.0, 0.0, 26.0, 24.5093359676825, 0.174073149575321, 0.0, 1.0, 40685.21221557412], 
processed observation next is [0.0, 0.21739130434782608, 0.502308402585411, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5424446639735416, 0.5580243831917736, 0.0, 1.0, 0.19373910578844822], 
reward next is 0.8063, 
noisyNet noise sample is [array([-1.0336136], dtype=float32), 1.0943233]. 
=============================================
[2019-04-04 10:01:15,036] A3C_AGENT_WORKER-Thread-4 INFO:Local step 213500, global step 3411990: loss 0.3636
[2019-04-04 10:01:15,036] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 213500, global step 3411990: learning rate 0.0000
[2019-04-04 10:01:15,126] A3C_AGENT_WORKER-Thread-6 INFO:Local step 213500, global step 3412013: loss 0.3887
[2019-04-04 10:01:15,126] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 213500, global step 3412013: learning rate 0.0000
[2019-04-04 10:01:15,440] A3C_AGENT_WORKER-Thread-20 INFO:Local step 212500, global step 3412115: loss 0.5531
[2019-04-04 10:01:15,441] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 212500, global step 3412115: learning rate 0.0000
[2019-04-04 10:01:17,220] A3C_AGENT_WORKER-Thread-3 INFO:Local step 213500, global step 3412592: loss 0.3686
[2019-04-04 10:01:17,221] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 213500, global step 3412592: learning rate 0.0000
[2019-04-04 10:01:20,557] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0147868e-11 1.1841674e-09 7.8798561e-27 7.4921345e-25 1.8848415e-20
 1.0000000e+00 1.4224173e-19], sum to 1.0000
[2019-04-04 10:01:20,557] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1798
[2019-04-04 10:01:20,600] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 25.13093605142689, 0.3122029363831542, 0.0, 1.0, 44577.35021337021], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 590400.0000, 
sim time next is 591000.0000, 
raw observation next is [-2.8, 86.33333333333333, 0.0, 0.0, 26.0, 25.1509105933375, 0.3093885662941265, 0.0, 1.0, 43777.39896328506], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5959092161114583, 0.6031295220980422, 0.0, 1.0, 0.2084638045870717], 
reward next is 0.7915, 
noisyNet noise sample is [array([-1.4204133], dtype=float32), 0.30861223]. 
=============================================
[2019-04-04 10:01:20,606] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.25954 ]
 [81.23335 ]
 [81.19589 ]
 [81.054146]
 [80.736694]], R is [[81.21587372]
 [81.1914444 ]
 [81.15454102]
 [81.07735443]
 [80.8883667 ]].
[2019-04-04 10:01:21,739] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.0941239e-11 8.1425061e-10 4.2946124e-26 2.7339814e-25 5.9809495e-20
 1.0000000e+00 1.7110959e-19], sum to 1.0000
[2019-04-04 10:01:21,739] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9216
[2019-04-04 10:01:21,771] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 58.16666666666666, 99.66666666666666, 609.3333333333334, 26.0, 25.7407925734356, 0.3516900064981905, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 299400.0000, 
sim time next is 300000.0000, 
raw observation next is [-10.6, 56.33333333333334, 102.8333333333333, 633.6666666666667, 26.0, 25.70229106321803, 0.3388599037427655, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.5633333333333335, 0.3427777777777777, 0.7001841620626151, 0.6666666666666666, 0.6418575886015025, 0.6129533012475885, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0325344], dtype=float32), -0.6977537]. 
=============================================
[2019-04-04 10:01:21,774] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[79.64564 ]
 [79.53652 ]
 [79.51037 ]
 [79.455666]
 [79.45377 ]], R is [[79.95820618]
 [80.15862274]
 [80.35704041]
 [80.55347443]
 [80.74794006]].
[2019-04-04 10:01:22,229] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9823459e-11 3.1784381e-10 1.5689022e-27 8.2110929e-26 3.2463679e-21
 1.0000000e+00 1.9592149e-20], sum to 1.0000
[2019-04-04 10:01:22,240] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7765
[2019-04-04 10:01:22,283] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 83.33333333333334, 0.0, 0.0, 26.0, 25.38380412727237, 0.295211689264852, 1.0, 1.0, 18709.90084189543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 841200.0000, 
sim time next is 841800.0000, 
raw observation next is [-3.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.16534337330609, 0.2882112853256677, 1.0, 1.0, 55203.53091954767], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5971119477755075, 0.5960704284418893, 1.0, 1.0, 0.26287395675975084], 
reward next is 0.7371, 
noisyNet noise sample is [array([0.9606201], dtype=float32), 0.15911672]. 
=============================================
[2019-04-04 10:01:23,077] A3C_AGENT_WORKER-Thread-5 INFO:Local step 213000, global step 3414287: loss 0.0084
[2019-04-04 10:01:23,077] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 213000, global step 3414287: learning rate 0.0000
[2019-04-04 10:01:23,783] A3C_AGENT_WORKER-Thread-14 INFO:Local step 213000, global step 3414523: loss 0.0062
[2019-04-04 10:01:23,813] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 213000, global step 3414533: learning rate 0.0000
[2019-04-04 10:01:24,260] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6665699e-10 4.2790083e-09 3.5802552e-27 2.8374087e-25 4.1107051e-20
 1.0000000e+00 1.3496587e-20], sum to 1.0000
[2019-04-04 10:01:24,260] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9223
[2019-04-04 10:01:24,311] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 98.5, 0.0, 26.0, 25.59599933236032, 0.299128002212281, 1.0, 1.0, 18713.59845057247], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 817200.0000, 
sim time next is 817800.0000, 
raw observation next is [-4.5, 71.0, 102.3333333333333, 0.0, 26.0, 25.5926523955466, 0.3027286802099574, 1.0, 1.0, 18908.12404436246], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.341111111111111, 0.0, 0.6666666666666666, 0.6327210329622167, 0.6009095600699857, 1.0, 1.0, 0.09003868592553552], 
reward next is 0.9100, 
noisyNet noise sample is [array([-0.78626096], dtype=float32), 0.8601737]. 
=============================================
[2019-04-04 10:01:24,750] A3C_AGENT_WORKER-Thread-11 INFO:Local step 213500, global step 3414824: loss 0.3563
[2019-04-04 10:01:24,751] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 213500, global step 3414824: learning rate 0.0000
[2019-04-04 10:01:26,040] A3C_AGENT_WORKER-Thread-19 INFO:Local step 213500, global step 3415131: loss 0.3686
[2019-04-04 10:01:26,055] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 213500, global step 3415131: learning rate 0.0000
[2019-04-04 10:01:27,768] A3C_AGENT_WORKER-Thread-18 INFO:Local step 213500, global step 3415619: loss 0.3678
[2019-04-04 10:01:27,769] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 213500, global step 3415619: learning rate 0.0000
[2019-04-04 10:01:27,853] A3C_AGENT_WORKER-Thread-17 INFO:Local step 213000, global step 3415639: loss 0.0085
[2019-04-04 10:01:27,864] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 213000, global step 3415639: learning rate 0.0000
[2019-04-04 10:01:27,872] A3C_AGENT_WORKER-Thread-12 INFO:Local step 213500, global step 3415645: loss 0.3782
[2019-04-04 10:01:27,917] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 213500, global step 3415660: learning rate 0.0000
[2019-04-04 10:01:29,039] A3C_AGENT_WORKER-Thread-13 INFO:Local step 213500, global step 3415971: loss 0.3828
[2019-04-04 10:01:29,039] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 213500, global step 3415971: learning rate 0.0000
[2019-04-04 10:01:30,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.6498785e-10 1.8207050e-08 3.2533780e-25 9.6185495e-25 2.2296987e-19
 1.0000000e+00 3.1995173e-19], sum to 1.0000
[2019-04-04 10:01:30,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5822
[2019-04-04 10:01:31,013] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3, 72.0, 0.0, 0.0, 26.0, 24.67845012460859, 0.174935759721608, 0.0, 1.0, 38904.08696562258], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 883800.0000, 
sim time next is 884400.0000, 
raw observation next is [-0.2, 72.0, 0.0, 0.0, 26.0, 24.62256271173307, 0.1641017119679689, 0.0, 1.0, 38889.70082789037], 
processed observation next is [1.0, 0.21739130434782608, 0.4570637119113574, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5518802259777559, 0.5547005706559897, 0.0, 1.0, 0.1851890515613827], 
reward next is 0.8148, 
noisyNet noise sample is [array([-0.03178152], dtype=float32), 0.2173972]. 
=============================================
[2019-04-04 10:01:31,090] A3C_AGENT_WORKER-Thread-16 INFO:Local step 214000, global step 3416552: loss 0.0075
[2019-04-04 10:01:31,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 214000, global step 3416552: learning rate 0.0000
[2019-04-04 10:01:31,518] A3C_AGENT_WORKER-Thread-15 INFO:Local step 214000, global step 3416718: loss 0.0078
[2019-04-04 10:01:31,521] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 214000, global step 3416718: learning rate 0.0000
[2019-04-04 10:01:31,610] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5452542e-11 3.0399000e-10 7.4726774e-30 4.4346540e-28 8.9246043e-23
 1.0000000e+00 3.9993945e-22], sum to 1.0000
[2019-04-04 10:01:31,610] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0495
[2019-04-04 10:01:31,642] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.45110019290419, 0.4448129560153344, 0.0, 1.0, 54983.21056094742], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 970800.0000, 
sim time next is 971400.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 26.0, 25.45994589277844, 0.467837792626999, 0.0, 1.0, 40248.97528848458], 
processed observation next is [1.0, 0.21739130434782608, 0.7063711911357342, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6216621577315365, 0.6559459308756663, 0.0, 1.0, 0.1916617870880218], 
reward next is 0.8083, 
noisyNet noise sample is [array([1.4772574], dtype=float32), -0.77334905]. 
=============================================
[2019-04-04 10:01:34,381] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.42795034e-10 3.36403660e-09 2.55424878e-25 1.15224905e-23
 2.38564731e-19 1.00000000e+00 1.04669491e-18], sum to 1.0000
[2019-04-04 10:01:34,381] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2302
[2019-04-04 10:01:34,447] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.35, 32.5, 62.0, 0.0, 26.0, 25.54059419436192, 0.2006207042171787, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 466200.0000, 
sim time next is 466800.0000, 
raw observation next is [-5.066666666666666, 32.33333333333334, 68.0, 0.0, 26.0, 25.55585905932324, 0.2024909888042913, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.32225300092336107, 0.3233333333333334, 0.22666666666666666, 0.0, 0.6666666666666666, 0.6296549216102699, 0.5674969962680971, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0534914], dtype=float32), 0.78071576]. 
=============================================
[2019-04-04 10:01:35,622] A3C_AGENT_WORKER-Thread-10 INFO:Local step 213500, global step 3418025: loss 0.3911
[2019-04-04 10:01:35,622] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 213500, global step 3418025: learning rate 0.0000
[2019-04-04 10:01:36,683] A3C_AGENT_WORKER-Thread-2 INFO:Local step 214000, global step 3418454: loss 0.0121
[2019-04-04 10:01:36,690] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 214000, global step 3418456: learning rate 0.0000
[2019-04-04 10:01:40,211] A3C_AGENT_WORKER-Thread-6 INFO:Local step 214000, global step 3419737: loss 0.0131
[2019-04-04 10:01:40,215] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 214000, global step 3419738: learning rate 0.0000
[2019-04-04 10:01:40,397] A3C_AGENT_WORKER-Thread-4 INFO:Local step 214000, global step 3419804: loss 0.0125
[2019-04-04 10:01:40,412] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 214000, global step 3419804: learning rate 0.0000
[2019-04-04 10:01:42,411] A3C_AGENT_WORKER-Thread-3 INFO:Local step 214000, global step 3420448: loss 0.0132
[2019-04-04 10:01:42,414] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 214000, global step 3420449: learning rate 0.0000
[2019-04-04 10:01:43,659] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1878987e-11 2.3797592e-10 3.2974412e-28 8.0246436e-27 1.3048640e-21
 1.0000000e+00 1.0275650e-21], sum to 1.0000
[2019-04-04 10:01:43,662] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6262
[2019-04-04 10:01:43,711] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.816666666666666, 85.5, 0.0, 0.0, 26.0, 25.12973078088299, 0.3209241752993115, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 846600.0000, 
sim time next is 847200.0000, 
raw observation next is [-3.733333333333333, 85.0, 0.0, 0.0, 26.0, 25.16248335716616, 0.2941368811934754, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.35918744228993543, 0.85, 0.0, 0.0, 0.6666666666666666, 0.59687361309718, 0.5980456270644918, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1060378], dtype=float32), 0.2566505]. 
=============================================
[2019-04-04 10:01:44,037] A3C_AGENT_WORKER-Thread-20 INFO:Local step 213000, global step 3421125: loss 0.0162
[2019-04-04 10:01:44,038] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 213000, global step 3421125: learning rate 0.0000
[2019-04-04 10:01:46,067] A3C_AGENT_WORKER-Thread-16 INFO:Local step 214500, global step 3422079: loss 0.0311
[2019-04-04 10:01:46,069] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 214500, global step 3422079: learning rate 0.0000
[2019-04-04 10:01:46,247] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0252621e-11 4.0089657e-10 3.0251042e-29 7.2807571e-27 6.2535465e-23
 1.0000000e+00 1.0867128e-21], sum to 1.0000
[2019-04-04 10:01:46,247] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8142
[2019-04-04 10:01:46,287] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 81.33333333333334, 44.83333333333333, 0.0, 26.0, 25.49584009968726, 0.2904191675515213, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 897600.0000, 
sim time next is 898200.0000, 
raw observation next is [1.1, 82.0, 48.0, 0.0, 26.0, 25.44419499519813, 0.2931971927883601, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.82, 0.16, 0.0, 0.6666666666666666, 0.6203495829331777, 0.5977323975961201, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26004106], dtype=float32), 0.57002145]. 
=============================================
[2019-04-04 10:01:46,296] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5501541e-12 1.5941576e-10 3.3521880e-31 2.3342789e-28 2.5736757e-23
 1.0000000e+00 3.7186421e-22], sum to 1.0000
[2019-04-04 10:01:46,307] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8586
[2019-04-04 10:01:46,328] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.600000000000001, 92.66666666666667, 24.0, 0.0, 26.0, 25.76935417062395, 0.3958766085144577, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 922800.0000, 
sim time next is 923400.0000, 
raw observation next is [4.7, 92.5, 18.0, 0.0, 26.0, 25.72368529263851, 0.2909608648121785, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.592797783933518, 0.925, 0.06, 0.0, 0.6666666666666666, 0.6436404410532092, 0.5969869549373928, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05092622], dtype=float32), 1.1239922]. 
=============================================
[2019-04-04 10:01:46,810] A3C_AGENT_WORKER-Thread-15 INFO:Local step 214500, global step 3422408: loss 0.0315
[2019-04-04 10:01:46,811] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 214500, global step 3422408: learning rate 0.0000
[2019-04-04 10:01:47,998] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.4950543e-14 7.7357339e-12 3.5767889e-34 3.8426996e-32 1.5191255e-26
 1.0000000e+00 3.9901515e-26], sum to 1.0000
[2019-04-04 10:01:47,999] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6845
[2019-04-04 10:01:48,008] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.66666666666667, 78.33333333333334, 109.3333333333333, 77.99999999999999, 26.0, 27.04837380246572, 0.8494201794219874, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1073400.0000, 
sim time next is 1074000.0000, 
raw observation next is [14.03333333333333, 76.66666666666667, 111.6666666666667, 38.99999999999999, 26.0, 27.12968058150722, 0.8574915747213345, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8513388734995383, 0.7666666666666667, 0.37222222222222234, 0.043093922651933694, 0.6666666666666666, 0.7608067151256016, 0.7858305249071115, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7321358], dtype=float32), -0.037960894]. 
=============================================
[2019-04-04 10:01:48,035] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[105.89775]
 [106.22265]
 [106.41639]
 [106.47145]
 [106.29925]], R is [[105.43387604]
 [105.37953949]
 [105.32574463]
 [105.27249146]
 [105.21976471]].
[2019-04-04 10:01:48,706] A3C_AGENT_WORKER-Thread-11 INFO:Local step 214000, global step 3423220: loss 0.0123
[2019-04-04 10:01:48,708] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 214000, global step 3423221: learning rate 0.0000
[2019-04-04 10:01:48,824] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.5817434e-12 3.1167516e-11 2.2024909e-30 9.0853362e-29 2.4705696e-24
 1.0000000e+00 3.7372838e-23], sum to 1.0000
[2019-04-04 10:01:48,824] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3502
[2019-04-04 10:01:48,833] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.8, 69.33333333333333, 0.0, 0.0, 26.0, 25.74895384882176, 0.6625392377276776, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1122000.0000, 
sim time next is 1122600.0000, 
raw observation next is [11.7, 70.16666666666667, 0.0, 0.0, 26.0, 25.75656402952429, 0.6588106717647851, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.7867036011080333, 0.7016666666666667, 0.0, 0.0, 0.6666666666666666, 0.6463803357936909, 0.7196035572549283, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1740365], dtype=float32), 0.10735014]. 
=============================================
[2019-04-04 10:01:50,163] A3C_AGENT_WORKER-Thread-5 INFO:Local step 213500, global step 3423779: loss 0.4057
[2019-04-04 10:01:50,163] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 213500, global step 3423779: learning rate 0.0000
[2019-04-04 10:01:50,584] A3C_AGENT_WORKER-Thread-19 INFO:Local step 214000, global step 3423958: loss 0.0105
[2019-04-04 10:01:50,586] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 214000, global step 3423958: learning rate 0.0000
[2019-04-04 10:01:50,640] A3C_AGENT_WORKER-Thread-2 INFO:Local step 214500, global step 3423986: loss 0.0305
[2019-04-04 10:01:50,641] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 214500, global step 3423986: learning rate 0.0000
[2019-04-04 10:01:50,748] A3C_AGENT_WORKER-Thread-14 INFO:Local step 213500, global step 3424037: loss 0.3962
[2019-04-04 10:01:50,748] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 213500, global step 3424037: learning rate 0.0000
[2019-04-04 10:01:51,236] A3C_AGENT_WORKER-Thread-12 INFO:Local step 214000, global step 3424296: loss 0.0086
[2019-04-04 10:01:51,243] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 214000, global step 3424296: learning rate 0.0000
[2019-04-04 10:01:51,393] A3C_AGENT_WORKER-Thread-18 INFO:Local step 214000, global step 3424380: loss 0.0087
[2019-04-04 10:01:51,401] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 214000, global step 3424380: learning rate 0.0000
[2019-04-04 10:01:52,649] A3C_AGENT_WORKER-Thread-13 INFO:Local step 214000, global step 3425030: loss 0.0095
[2019-04-04 10:01:52,650] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 214000, global step 3425030: learning rate 0.0000
[2019-04-04 10:01:54,250] A3C_AGENT_WORKER-Thread-6 INFO:Local step 214500, global step 3425814: loss 0.0305
[2019-04-04 10:01:54,253] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 214500, global step 3425815: learning rate 0.0000
[2019-04-04 10:01:54,374] A3C_AGENT_WORKER-Thread-4 INFO:Local step 214500, global step 3425875: loss 0.0296
[2019-04-04 10:01:54,376] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 214500, global step 3425875: learning rate 0.0000
[2019-04-04 10:01:54,578] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1819044e-12 1.9854603e-10 2.4906513e-30 2.5670311e-28 3.2431463e-23
 1.0000000e+00 1.1840383e-21], sum to 1.0000
[2019-04-04 10:01:54,580] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1511
[2019-04-04 10:01:54,593] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 100.0, 35.0, 0.0, 26.0, 24.62594864520409, 0.4218986575682062, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1267200.0000, 
sim time next is 1267800.0000, 
raw observation next is [13.53333333333333, 100.0, 29.66666666666666, 0.0, 26.0, 24.61589366270319, 0.4214415712908554, 0.0, 1.0, 19685.54749608948], 
processed observation next is [0.0, 0.6956521739130435, 0.8374884579870729, 1.0, 0.09888888888888887, 0.0, 0.6666666666666666, 0.5513244718919325, 0.6404805237636185, 0.0, 1.0, 0.09374070236233085], 
reward next is 0.9063, 
noisyNet noise sample is [array([-0.16184884], dtype=float32), -0.6404473]. 
=============================================
[2019-04-04 10:01:54,695] A3C_AGENT_WORKER-Thread-17 INFO:Local step 213500, global step 3426029: loss 0.3916
[2019-04-04 10:01:54,697] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 213500, global step 3426029: learning rate 0.0000
[2019-04-04 10:01:55,675] A3C_AGENT_WORKER-Thread-3 INFO:Local step 214500, global step 3426477: loss 0.0297
[2019-04-04 10:01:55,675] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 214500, global step 3426477: learning rate 0.0000
[2019-04-04 10:01:57,274] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2623682e-09 1.3145551e-08 8.3603124e-25 9.9672071e-24 1.3270082e-18
 1.0000000e+00 9.4006409e-19], sum to 1.0000
[2019-04-04 10:01:57,275] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2789
[2019-04-04 10:01:57,288] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 72.0, 0.0, 0.0, 26.0, 24.39130444055913, 0.09194005420540252, 0.0, 1.0, 40955.07942279371], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 694800.0000, 
sim time next is 695400.0000, 
raw observation next is [-3.4, 72.5, 0.0, 0.0, 26.0, 24.40841555816129, 0.09371431867344222, 0.0, 1.0, 40981.58785094752], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.725, 0.0, 0.0, 0.6666666666666666, 0.5340346298467743, 0.5312381062244808, 0.0, 1.0, 0.19515041833784535], 
reward next is 0.8048, 
noisyNet noise sample is [array([-0.76599365], dtype=float32), -0.40645787]. 
=============================================
[2019-04-04 10:01:58,050] A3C_AGENT_WORKER-Thread-10 INFO:Local step 214000, global step 3427619: loss 0.0052
[2019-04-04 10:01:58,053] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 214000, global step 3427620: learning rate 0.0000
[2019-04-04 10:02:02,303] A3C_AGENT_WORKER-Thread-11 INFO:Local step 214500, global step 3429769: loss 0.0287
[2019-04-04 10:02:02,305] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 214500, global step 3429769: learning rate 0.0000
[2019-04-04 10:02:03,428] A3C_AGENT_WORKER-Thread-16 INFO:Local step 215000, global step 3430355: loss 0.2906
[2019-04-04 10:02:03,433] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 215000, global step 3430355: learning rate 0.0000
[2019-04-04 10:02:03,738] A3C_AGENT_WORKER-Thread-19 INFO:Local step 214500, global step 3430492: loss 0.0279
[2019-04-04 10:02:03,739] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 214500, global step 3430493: learning rate 0.0000
[2019-04-04 10:02:04,042] A3C_AGENT_WORKER-Thread-12 INFO:Local step 214500, global step 3430633: loss 0.0282
[2019-04-04 10:02:04,042] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 214500, global step 3430633: learning rate 0.0000
[2019-04-04 10:02:04,428] A3C_AGENT_WORKER-Thread-15 INFO:Local step 215000, global step 3430837: loss 0.2780
[2019-04-04 10:02:04,430] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 215000, global step 3430837: learning rate 0.0000
[2019-04-04 10:02:04,469] A3C_AGENT_WORKER-Thread-18 INFO:Local step 214500, global step 3430854: loss 0.0279
[2019-04-04 10:02:04,474] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 214500, global step 3430854: learning rate 0.0000
[2019-04-04 10:02:04,753] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.02983809e-12 4.62077321e-10 3.88982659e-29 8.59064615e-28
 1.02829773e-22 1.00000000e+00 2.94227144e-22], sum to 1.0000
[2019-04-04 10:02:04,775] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6663
[2019-04-04 10:02:04,814] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 22.66666666666666, 0.0, 26.0, 25.59986680016079, 0.4871786981100868, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1441200.0000, 
sim time next is 1441800.0000, 
raw observation next is [1.1, 92.0, 18.0, 0.0, 26.0, 25.80606929791018, 0.4981458413407355, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.06, 0.0, 0.6666666666666666, 0.6505057748258484, 0.6660486137802452, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40467587], dtype=float32), -0.6251044]. 
=============================================
[2019-04-04 10:02:05,694] A3C_AGENT_WORKER-Thread-13 INFO:Local step 214500, global step 3431341: loss 0.0277
[2019-04-04 10:02:05,699] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 214500, global step 3431341: learning rate 0.0000
[2019-04-04 10:02:07,459] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5700516e-11 2.2422955e-09 4.1164316e-28 9.2868296e-27 2.2270951e-21
 1.0000000e+00 1.8276725e-21], sum to 1.0000
[2019-04-04 10:02:07,462] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6533
[2019-04-04 10:02:07,513] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 93.66666666666667, 0.0, 0.0, 26.0, 25.43935510775873, 0.4597108029793159, 0.0, 1.0, 38942.76562124836], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1479000.0000, 
sim time next is 1479600.0000, 
raw observation next is [2.2, 94.0, 0.0, 0.0, 26.0, 25.38766949577047, 0.453111672173585, 0.0, 1.0, 63667.65719639319], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.94, 0.0, 0.0, 0.6666666666666666, 0.6156391246475392, 0.6510372240578617, 0.0, 1.0, 0.3031793199828247], 
reward next is 0.6968, 
noisyNet noise sample is [array([0.48302063], dtype=float32), 1.164047]. 
=============================================
[2019-04-04 10:02:07,972] A3C_AGENT_WORKER-Thread-2 INFO:Local step 215000, global step 3432382: loss 0.2500
[2019-04-04 10:02:07,973] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 215000, global step 3432382: learning rate 0.0000
[2019-04-04 10:02:08,565] A3C_AGENT_WORKER-Thread-20 INFO:Local step 213500, global step 3432660: loss 0.3419
[2019-04-04 10:02:08,568] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 213500, global step 3432660: learning rate 0.0000
[2019-04-04 10:02:10,207] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.86429431e-12 3.17751485e-11 7.09557660e-32 7.37064449e-30
 5.39564372e-24 1.00000000e+00 1.07760924e-23], sum to 1.0000
[2019-04-04 10:02:10,207] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1000
[2019-04-04 10:02:10,224] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.7, 51.66666666666667, 76.33333333333333, 539.6666666666666, 26.0, 25.99701545753517, 0.6898716494369279, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1523400.0000, 
sim time next is 1524000.0000, 
raw observation next is [11.8, 51.33333333333334, 76.66666666666667, 508.8333333333334, 26.0, 26.4355068260707, 0.7178440016166582, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7894736842105264, 0.5133333333333334, 0.2555555555555556, 0.5622467771639044, 0.6666666666666666, 0.7029589021725583, 0.7392813338722194, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.55448216], dtype=float32), -1.1860074]. 
=============================================
[2019-04-04 10:02:10,246] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[95.892784]
 [96.13366 ]
 [96.20576 ]
 [96.25438 ]
 [96.287285]], R is [[95.80177307]
 [95.84375763]
 [95.88532257]
 [95.9264679 ]
 [95.96720123]].
[2019-04-04 10:02:11,298] A3C_AGENT_WORKER-Thread-10 INFO:Local step 214500, global step 3433893: loss 0.0276
[2019-04-04 10:02:11,300] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 214500, global step 3433894: learning rate 0.0000
[2019-04-04 10:02:11,834] A3C_AGENT_WORKER-Thread-5 INFO:Local step 214000, global step 3434165: loss 0.0037
[2019-04-04 10:02:11,854] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 214000, global step 3434165: learning rate 0.0000
[2019-04-04 10:02:11,863] A3C_AGENT_WORKER-Thread-6 INFO:Local step 215000, global step 3434180: loss 0.2133
[2019-04-04 10:02:11,864] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 215000, global step 3434180: learning rate 0.0000
[2019-04-04 10:02:12,377] A3C_AGENT_WORKER-Thread-4 INFO:Local step 215000, global step 3434414: loss 0.2022
[2019-04-04 10:02:12,379] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 215000, global step 3434414: learning rate 0.0000
[2019-04-04 10:02:12,509] A3C_AGENT_WORKER-Thread-3 INFO:Local step 215000, global step 3434468: loss 0.2019
[2019-04-04 10:02:12,510] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 215000, global step 3434468: learning rate 0.0000
[2019-04-04 10:02:12,690] A3C_AGENT_WORKER-Thread-14 INFO:Local step 214000, global step 3434546: loss 0.0045
[2019-04-04 10:02:12,691] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 214000, global step 3434546: learning rate 0.0000
[2019-04-04 10:02:13,160] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.57740235e-11 1.28592881e-09 3.37696432e-28 1.77793563e-26
 1.48802007e-21 1.00000000e+00 1.01647775e-20], sum to 1.0000
[2019-04-04 10:02:13,160] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9679
[2019-04-04 10:02:13,181] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.08757107965064, 0.4078779578935505, 0.0, 1.0, 38556.09838312528], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1405800.0000, 
sim time next is 1406400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.04831794053608, 0.4068136996196913, 0.0, 1.0, 38593.37630509838], 
processed observation next is [1.0, 0.2608695652173913, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5873598283780067, 0.6356045665398972, 0.0, 1.0, 0.1837779824052304], 
reward next is 0.8162, 
noisyNet noise sample is [array([-0.35380867], dtype=float32), 0.2034065]. 
=============================================
[2019-04-04 10:02:15,788] A3C_AGENT_WORKER-Thread-17 INFO:Local step 214000, global step 3435901: loss 0.0068
[2019-04-04 10:02:15,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 214000, global step 3435901: learning rate 0.0000
[2019-04-04 10:02:18,479] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.3338139e-11 5.4725041e-10 4.7016659e-29 1.5824269e-26 5.1617206e-22
 1.0000000e+00 8.0499582e-22], sum to 1.0000
[2019-04-04 10:02:18,485] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9870
[2019-04-04 10:02:18,502] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.98333333333333, 49.83333333333334, 23.66666666666667, 0.9999999999999998, 26.0, 27.94564213112188, 1.026271132527999, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1097400.0000, 
sim time next is 1098000.0000, 
raw observation next is [17.7, 50.0, 18.0, 1.5, 26.0, 27.96416594652597, 1.027870009688245, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.9529085872576178, 0.5, 0.06, 0.0016574585635359116, 0.6666666666666666, 0.8303471622104975, 0.8426233365627483, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4392784], dtype=float32), 1.0018873]. 
=============================================
[2019-04-04 10:02:18,526] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.10203]
 [86.16029]
 [86.19711]
 [86.27786]
 [86.32302]], R is [[86.24542999]
 [86.38297272]
 [86.51914215]
 [86.65395355]
 [86.78741455]].
[2019-04-04 10:02:19,694] A3C_AGENT_WORKER-Thread-11 INFO:Local step 215000, global step 3437611: loss 0.1713
[2019-04-04 10:02:19,704] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 215000, global step 3437611: learning rate 0.0000
[2019-04-04 10:02:21,223] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.5034527e-12 1.3108181e-10 2.0078707e-30 2.9814568e-28 2.6719864e-23
 1.0000000e+00 4.3546036e-22], sum to 1.0000
[2019-04-04 10:02:21,223] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4551
[2019-04-04 10:02:21,235] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.933333333333333, 97.33333333333334, 75.5, 118.0, 26.0, 26.08557421801837, 0.5320640654010781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1507200.0000, 
sim time next is 1507800.0000, 
raw observation next is [3.116666666666667, 96.66666666666666, 78.0, 235.9999999999999, 26.0, 26.04645600683811, 0.5472631076609404, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5489381348107111, 0.9666666666666666, 0.26, 0.2607734806629833, 0.6666666666666666, 0.6705380005698425, 0.6824210358869802, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.93236107], dtype=float32), -0.054307748]. 
=============================================
[2019-04-04 10:02:21,727] A3C_AGENT_WORKER-Thread-19 INFO:Local step 215000, global step 3438548: loss 0.1508
[2019-04-04 10:02:21,730] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 215000, global step 3438549: learning rate 0.0000
[2019-04-04 10:02:21,820] A3C_AGENT_WORKER-Thread-12 INFO:Local step 215000, global step 3438590: loss 0.1493
[2019-04-04 10:02:21,821] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 215000, global step 3438590: learning rate 0.0000
[2019-04-04 10:02:22,514] A3C_AGENT_WORKER-Thread-18 INFO:Local step 215000, global step 3438933: loss 0.1424
[2019-04-04 10:02:22,514] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 215000, global step 3438933: learning rate 0.0000
[2019-04-04 10:02:23,181] A3C_AGENT_WORKER-Thread-16 INFO:Local step 215500, global step 3439252: loss 2.6865
[2019-04-04 10:02:23,183] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 215500, global step 3439252: learning rate 0.0000
[2019-04-04 10:02:23,866] A3C_AGENT_WORKER-Thread-13 INFO:Local step 215000, global step 3439572: loss 0.1421
[2019-04-04 10:02:23,868] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 215000, global step 3439572: learning rate 0.0000
[2019-04-04 10:02:24,265] A3C_AGENT_WORKER-Thread-15 INFO:Local step 215500, global step 3439718: loss 2.7890
[2019-04-04 10:02:24,268] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 215500, global step 3439719: learning rate 0.0000
[2019-04-04 10:02:24,874] A3C_AGENT_WORKER-Thread-5 INFO:Local step 214500, global step 3439989: loss 0.0285
[2019-04-04 10:02:24,877] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 214500, global step 3439989: learning rate 0.0000
[2019-04-04 10:02:26,235] A3C_AGENT_WORKER-Thread-14 INFO:Local step 214500, global step 3440560: loss 0.0279
[2019-04-04 10:02:26,237] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 214500, global step 3440562: learning rate 0.0000
[2019-04-04 10:02:27,636] A3C_AGENT_WORKER-Thread-2 INFO:Local step 215500, global step 3441218: loss 2.7671
[2019-04-04 10:02:27,637] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 215500, global step 3441218: learning rate 0.0000
[2019-04-04 10:02:29,163] A3C_AGENT_WORKER-Thread-10 INFO:Local step 215000, global step 3441873: loss 0.1250
[2019-04-04 10:02:29,166] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 215000, global step 3441874: learning rate 0.0000
[2019-04-04 10:02:29,306] A3C_AGENT_WORKER-Thread-17 INFO:Local step 214500, global step 3441931: loss 0.0290
[2019-04-04 10:02:29,307] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 214500, global step 3441931: learning rate 0.0000
[2019-04-04 10:02:30,529] A3C_AGENT_WORKER-Thread-20 INFO:Local step 214000, global step 3442405: loss 0.0190
[2019-04-04 10:02:30,532] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 214000, global step 3442405: learning rate 0.0000
[2019-04-04 10:02:31,389] A3C_AGENT_WORKER-Thread-6 INFO:Local step 215500, global step 3442708: loss 2.6424
[2019-04-04 10:02:31,389] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 215500, global step 3442708: learning rate 0.0000
[2019-04-04 10:02:32,439] A3C_AGENT_WORKER-Thread-4 INFO:Local step 215500, global step 3443068: loss 2.6217
[2019-04-04 10:02:32,442] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 215500, global step 3443069: learning rate 0.0000
[2019-04-04 10:02:32,760] A3C_AGENT_WORKER-Thread-3 INFO:Local step 215500, global step 3443184: loss 2.7276
[2019-04-04 10:02:32,761] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 215500, global step 3443184: learning rate 0.0000
[2019-04-04 10:02:36,183] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4722429e-12 1.3228187e-10 2.5272690e-30 1.2176227e-28 2.7829981e-23
 1.0000000e+00 1.2404585e-22], sum to 1.0000
[2019-04-04 10:02:36,184] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6108
[2019-04-04 10:02:36,212] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 26.0, 25.62678640425813, 0.5804110847965385, 0.0, 1.0, 48121.06188623726], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1061400.0000, 
sim time next is 1062000.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 26.0, 25.60644914512243, 0.5862622304403459, 0.0, 1.0, 44611.7739729213], 
processed observation next is [1.0, 0.30434782608695654, 0.8310249307479226, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6338707620935358, 0.6954207434801153, 0.0, 1.0, 0.21243701891867287], 
reward next is 0.7876, 
noisyNet noise sample is [array([-0.9660916], dtype=float32), -0.6492924]. 
=============================================
[2019-04-04 10:02:36,222] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[93.60928]
 [93.51726]
 [93.40814]
 [93.20341]
 [93.36461]], R is [[93.52417755]
 [93.35978699]
 [93.1966095 ]
 [92.9695816 ]
 [93.03988647]].
[2019-04-04 10:02:40,329] A3C_AGENT_WORKER-Thread-11 INFO:Local step 215500, global step 3445719: loss 2.6159
[2019-04-04 10:02:40,330] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 215500, global step 3445719: learning rate 0.0000
[2019-04-04 10:02:40,616] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7447591e-10 4.1777387e-10 2.3585187e-27 8.7173543e-26 9.5366566e-21
 1.0000000e+00 2.4398269e-20], sum to 1.0000
[2019-04-04 10:02:40,616] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4774
[2019-04-04 10:02:40,629] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.9, 92.0, 0.0, 0.0, 26.0, 25.46828280023246, 0.465669243548722, 0.0, 1.0, 56136.43757299529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1474200.0000, 
sim time next is 1474800.0000, 
raw observation next is [2.0, 92.0, 0.0, 0.0, 26.0, 25.42452838978479, 0.4653074187391583, 0.0, 1.0, 67652.67395380404], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6187106991487324, 0.6551024729130528, 0.0, 1.0, 0.3221555902562097], 
reward next is 0.6778, 
noisyNet noise sample is [array([0.1689995], dtype=float32), 0.3793862]. 
=============================================
[2019-04-04 10:02:40,962] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8056327e-09 2.5459631e-08 1.5368413e-23 1.8055598e-21 3.5838109e-18
 1.0000000e+00 2.6843870e-17], sum to 1.0000
[2019-04-04 10:02:40,963] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9232
[2019-04-04 10:02:41,070] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 78.0, 18.33333333333334, 0.0, 26.0, 23.19127645368262, -0.1133028905278737, 0.0, 1.0, 47044.02575825291], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1843800.0000, 
sim time next is 1844400.0000, 
raw observation next is [-6.700000000000001, 78.0, 22.66666666666667, 0.0, 26.0, 23.17053974744449, -0.04150665085476016, 0.0, 1.0, 202310.3786485448], 
processed observation next is [0.0, 0.34782608695652173, 0.2770083102493075, 0.78, 0.07555555555555557, 0.0, 0.6666666666666666, 0.4308783122870408, 0.48616444971507994, 0.0, 1.0, 0.963382755469261], 
reward next is 0.0366, 
noisyNet noise sample is [array([2.2559729], dtype=float32), 0.58293927]. 
=============================================
[2019-04-04 10:02:41,788] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2125755e-10 5.4786895e-09 1.2322361e-25 1.6610055e-23 2.1683731e-19
 1.0000000e+00 9.3477168e-19], sum to 1.0000
[2019-04-04 10:02:41,789] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0038
[2019-04-04 10:02:41,846] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.783333333333333, 78.0, 138.6666666666667, 79.66666666666667, 26.0, 25.0930738080213, 0.2335756503095507, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1849800.0000, 
sim time next is 1850400.0000, 
raw observation next is [-5.6, 78.0, 134.0, 72.5, 26.0, 25.01957806530488, 0.2180394975251159, 0.0, 1.0, 18757.3493607312], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.78, 0.44666666666666666, 0.08011049723756906, 0.6666666666666666, 0.5849648387754067, 0.572679832508372, 0.0, 1.0, 0.08932071124157714], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.34878612], dtype=float32), -0.52146983]. 
=============================================
[2019-04-04 10:02:42,881] A3C_AGENT_WORKER-Thread-19 INFO:Local step 215500, global step 3446498: loss 2.5973
[2019-04-04 10:02:42,881] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 215500, global step 3446498: learning rate 0.0000
[2019-04-04 10:02:42,948] A3C_AGENT_WORKER-Thread-12 INFO:Local step 215500, global step 3446520: loss 2.6317
[2019-04-04 10:02:42,949] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 215500, global step 3446520: learning rate 0.0000
[2019-04-04 10:02:43,375] A3C_AGENT_WORKER-Thread-18 INFO:Local step 215500, global step 3446692: loss 2.6396
[2019-04-04 10:02:43,377] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 215500, global step 3446693: learning rate 0.0000
[2019-04-04 10:02:44,858] A3C_AGENT_WORKER-Thread-5 INFO:Local step 215000, global step 3447259: loss 0.1137
[2019-04-04 10:02:44,882] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 215000, global step 3447259: learning rate 0.0000
[2019-04-04 10:02:45,515] A3C_AGENT_WORKER-Thread-13 INFO:Local step 215500, global step 3447445: loss 2.6484
[2019-04-04 10:02:45,516] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 215500, global step 3447445: learning rate 0.0000
[2019-04-04 10:02:46,137] A3C_AGENT_WORKER-Thread-14 INFO:Local step 215000, global step 3447628: loss 0.1222
[2019-04-04 10:02:46,155] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 215000, global step 3447628: learning rate 0.0000
[2019-04-04 10:02:46,512] A3C_AGENT_WORKER-Thread-20 INFO:Local step 214500, global step 3447768: loss 0.0294
[2019-04-04 10:02:46,514] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 214500, global step 3447768: learning rate 0.0000
[2019-04-04 10:02:49,137] A3C_AGENT_WORKER-Thread-16 INFO:Local step 216000, global step 3448655: loss 0.1497
[2019-04-04 10:02:49,138] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 216000, global step 3448655: learning rate 0.0000
[2019-04-04 10:02:49,293] A3C_AGENT_WORKER-Thread-17 INFO:Local step 215000, global step 3448705: loss 0.1070
[2019-04-04 10:02:49,294] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 215000, global step 3448705: learning rate 0.0000
[2019-04-04 10:02:50,449] A3C_AGENT_WORKER-Thread-10 INFO:Local step 215500, global step 3449079: loss 2.6018
[2019-04-04 10:02:50,450] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 215500, global step 3449079: learning rate 0.0000
[2019-04-04 10:02:50,620] A3C_AGENT_WORKER-Thread-15 INFO:Local step 216000, global step 3449136: loss 0.1498
[2019-04-04 10:02:50,622] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 216000, global step 3449137: learning rate 0.0000
[2019-04-04 10:02:51,040] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.12334975e-09 1.02337046e-08 4.24692932e-25 7.27552056e-24
 3.93559378e-19 1.00000000e+00 5.05868962e-18], sum to 1.0000
[2019-04-04 10:02:51,055] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2645
[2019-04-04 10:02:51,155] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 75.0, 0.0, 0.0, 26.0, 23.92722970640725, 0.1457399139942662, 1.0, 1.0, 202712.3337479954], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2187000.0000, 
sim time next is 2187600.0000, 
raw observation next is [-5.6, 75.0, 8.499999999999998, 43.66666666666666, 26.0, 24.57436344321753, 0.2695301684499496, 1.0, 1.0, 128403.550681884], 
processed observation next is [1.0, 0.30434782608695654, 0.30747922437673136, 0.75, 0.02833333333333333, 0.04825046040515653, 0.6666666666666666, 0.5478636202681274, 0.5898433894833165, 1.0, 1.0, 0.6114454794375428], 
reward next is 0.3886, 
noisyNet noise sample is [array([-1.2082735], dtype=float32), 0.27831557]. 
=============================================
[2019-04-04 10:02:52,561] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.75265147e-09 8.86013218e-09 1.98884111e-25 1.21723115e-23
 3.66785894e-19 1.00000000e+00 1.34820896e-18], sum to 1.0000
[2019-04-04 10:02:52,561] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8852
[2019-04-04 10:02:52,579] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.4, 89.33333333333334, 0.0, 0.0, 26.0, 24.38834446244927, 0.1460217016253325, 0.0, 1.0, 43453.19759809693], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2086800.0000, 
sim time next is 2087400.0000, 
raw observation next is [-5.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.33002214882667, 0.1566965362283043, 0.0, 1.0, 43830.69728576837], 
processed observation next is [1.0, 0.13043478260869565, 0.3102493074792244, 0.9016666666666667, 0.0, 0.0, 0.6666666666666666, 0.5275018457355559, 0.5522321787427681, 0.0, 1.0, 0.20871760612270654], 
reward next is 0.7913, 
noisyNet noise sample is [array([-0.74523765], dtype=float32), -0.8844886]. 
=============================================
[2019-04-04 10:02:53,995] A3C_AGENT_WORKER-Thread-2 INFO:Local step 216000, global step 3450272: loss 0.1671
[2019-04-04 10:02:54,026] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 216000, global step 3450272: learning rate 0.0000
[2019-04-04 10:02:55,744] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7241755e-10 9.9863451e-10 4.9567018e-26 1.6607131e-24 8.1344237e-20
 1.0000000e+00 1.5762752e-19], sum to 1.0000
[2019-04-04 10:02:55,750] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1039
[2019-04-04 10:02:55,807] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 66.5, 26.0, 0.0, 26.0, 25.98545423765922, 0.458471065821533, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2133000.0000, 
sim time next is 2133600.0000, 
raw observation next is [-4.5, 67.0, 22.0, 0.0, 26.0, 26.0476226322876, 0.4638877089359026, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.67, 0.07333333333333333, 0.0, 0.6666666666666666, 0.6706352193572999, 0.6546292363119676, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9033161], dtype=float32), 0.09274675]. 
=============================================
[2019-04-04 10:02:58,185] A3C_AGENT_WORKER-Thread-6 INFO:Local step 216000, global step 3451521: loss 0.1742
[2019-04-04 10:02:58,187] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 216000, global step 3451521: learning rate 0.0000
[2019-04-04 10:02:59,603] A3C_AGENT_WORKER-Thread-3 INFO:Local step 216000, global step 3451920: loss 0.1618
[2019-04-04 10:02:59,603] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 216000, global step 3451920: learning rate 0.0000
[2019-04-04 10:02:59,777] A3C_AGENT_WORKER-Thread-4 INFO:Local step 216000, global step 3451975: loss 0.1794
[2019-04-04 10:02:59,778] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 216000, global step 3451975: learning rate 0.0000
[2019-04-04 10:03:05,810] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5839414e-09 2.4984061e-08 5.8150400e-24 1.7680444e-22 2.3219020e-18
 1.0000000e+00 9.7845631e-18], sum to 1.0000
[2019-04-04 10:03:05,811] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6133
[2019-04-04 10:03:05,834] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.60206128633327, -0.02961260475398345, 0.0, 1.0, 47076.74928496582], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1836000.0000, 
sim time next is 1836600.0000, 
raw observation next is [-6.283333333333333, 78.83333333333334, 0.0, 0.0, 26.0, 23.5627916583503, -0.03732287942367261, 0.0, 1.0, 47067.67103482843], 
processed observation next is [0.0, 0.2608695652173913, 0.288550323176362, 0.7883333333333334, 0.0, 0.0, 0.6666666666666666, 0.4635659715291916, 0.4875590401921091, 0.0, 1.0, 0.22413176683251634], 
reward next is 0.7759, 
noisyNet noise sample is [array([-0.6185124], dtype=float32), -0.49887848]. 
=============================================
[2019-04-04 10:03:07,950] A3C_AGENT_WORKER-Thread-20 INFO:Local step 215000, global step 3454376: loss 0.1332
[2019-04-04 10:03:07,951] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 215000, global step 3454376: learning rate 0.0000
[2019-04-04 10:03:08,334] A3C_AGENT_WORKER-Thread-11 INFO:Local step 216000, global step 3454491: loss 0.1683
[2019-04-04 10:03:08,335] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 216000, global step 3454491: learning rate 0.0000
[2019-04-04 10:03:08,464] A3C_AGENT_WORKER-Thread-5 INFO:Local step 215500, global step 3454522: loss 2.6616
[2019-04-04 10:03:08,464] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 215500, global step 3454522: learning rate 0.0000
[2019-04-04 10:03:09,280] A3C_AGENT_WORKER-Thread-14 INFO:Local step 215500, global step 3454761: loss 2.6507
[2019-04-04 10:03:09,280] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 215500, global step 3454761: learning rate 0.0000
[2019-04-04 10:03:10,160] A3C_AGENT_WORKER-Thread-12 INFO:Local step 216000, global step 3455019: loss 0.1783
[2019-04-04 10:03:10,161] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 216000, global step 3455019: learning rate 0.0000
[2019-04-04 10:03:11,408] A3C_AGENT_WORKER-Thread-18 INFO:Local step 216000, global step 3455384: loss 0.1823
[2019-04-04 10:03:11,414] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 216000, global step 3455386: learning rate 0.0000
[2019-04-04 10:03:11,487] A3C_AGENT_WORKER-Thread-19 INFO:Local step 216000, global step 3455411: loss 0.1760
[2019-04-04 10:03:11,488] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 216000, global step 3455411: learning rate 0.0000
[2019-04-04 10:03:12,059] A3C_AGENT_WORKER-Thread-17 INFO:Local step 215500, global step 3455598: loss 2.6618
[2019-04-04 10:03:12,060] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 215500, global step 3455598: learning rate 0.0000
[2019-04-04 10:03:12,515] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2291071e-10 3.9958219e-09 3.3074473e-26 2.1406133e-24 9.2918950e-20
 1.0000000e+00 1.5658831e-19], sum to 1.0000
[2019-04-04 10:03:12,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6067
[2019-04-04 10:03:12,531] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.2, 45.66666666666667, 57.16666666666667, 6.999999999999998, 26.0, 25.8961209235601, 0.4087784644154839, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2305200.0000, 
sim time next is 2305800.0000, 
raw observation next is [-0.3, 46.5, 41.0, 0.0, 26.0, 25.88769305489619, 0.3947280279520318, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4542936288088643, 0.465, 0.13666666666666666, 0.0, 0.6666666666666666, 0.6573077545746825, 0.6315760093173439, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1726117], dtype=float32), 1.454351]. 
=============================================
[2019-04-04 10:03:13,127] A3C_AGENT_WORKER-Thread-13 INFO:Local step 216000, global step 3455958: loss 0.1804
[2019-04-04 10:03:13,152] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 216000, global step 3455964: learning rate 0.0000
[2019-04-04 10:03:15,939] A3C_AGENT_WORKER-Thread-16 INFO:Local step 216500, global step 3456804: loss 1.2322
[2019-04-04 10:03:15,941] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 216500, global step 3456804: learning rate 0.0000
[2019-04-04 10:03:17,346] A3C_AGENT_WORKER-Thread-15 INFO:Local step 216500, global step 3457206: loss 1.2282
[2019-04-04 10:03:17,346] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 216500, global step 3457206: learning rate 0.0000
[2019-04-04 10:03:18,185] A3C_AGENT_WORKER-Thread-10 INFO:Local step 216000, global step 3457456: loss 0.1874
[2019-04-04 10:03:18,186] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 216000, global step 3457456: learning rate 0.0000
[2019-04-04 10:03:20,634] A3C_AGENT_WORKER-Thread-2 INFO:Local step 216500, global step 3458208: loss 1.2442
[2019-04-04 10:03:20,634] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 216500, global step 3458208: learning rate 0.0000
[2019-04-04 10:03:23,023] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5849636e-10 3.7057801e-09 9.3940472e-26 1.8987657e-24 4.7993058e-20
 1.0000000e+00 4.5667794e-19], sum to 1.0000
[2019-04-04 10:03:23,023] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2675
[2019-04-04 10:03:23,075] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.96403307828236, 0.3667286291366849, 0.0, 1.0, 55118.5851913457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1972800.0000, 
sim time next is 1973400.0000, 
raw observation next is [-5.600000000000001, 82.16666666666667, 0.0, 0.0, 26.0, 25.16418641323638, 0.3812225877123681, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.3074792243767313, 0.8216666666666668, 0.0, 0.0, 0.6666666666666666, 0.5970155344363649, 0.6270741959041227, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5078287], dtype=float32), 1.532174]. 
=============================================
[2019-04-04 10:03:25,024] A3C_AGENT_WORKER-Thread-6 INFO:Local step 216500, global step 3459583: loss 1.2232
[2019-04-04 10:03:25,027] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 216500, global step 3459583: learning rate 0.0000
[2019-04-04 10:03:25,446] A3C_AGENT_WORKER-Thread-3 INFO:Local step 216500, global step 3459718: loss 1.2221
[2019-04-04 10:03:25,446] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 216500, global step 3459718: learning rate 0.0000
[2019-04-04 10:03:26,346] A3C_AGENT_WORKER-Thread-4 INFO:Local step 216500, global step 3459995: loss 1.2219
[2019-04-04 10:03:26,347] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 216500, global step 3459995: learning rate 0.0000
[2019-04-04 10:03:30,128] A3C_AGENT_WORKER-Thread-20 INFO:Local step 215500, global step 3461270: loss 2.7097
[2019-04-04 10:03:30,132] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 215500, global step 3461271: learning rate 0.0000
[2019-04-04 10:03:32,283] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.4536735e-10 5.1546118e-09 8.9999464e-25 1.4964840e-22 1.0698665e-18
 1.0000000e+00 5.9863880e-18], sum to 1.0000
[2019-04-04 10:03:32,283] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1327
[2019-04-04 10:03:32,339] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.15, 44.5, 0.0, 0.0, 26.0, 24.9555783386111, 0.2654994742668469, 0.0, 1.0, 44731.00351534628], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2395800.0000, 
sim time next is 2396400.0000, 
raw observation next is [-1.333333333333333, 44.33333333333334, 0.0, 0.0, 26.0, 24.94734783743263, 0.264075408182971, 0.0, 1.0, 48937.28507836546], 
processed observation next is [0.0, 0.7391304347826086, 0.42566943674976926, 0.4433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5789456531193858, 0.5880251360609904, 0.0, 1.0, 0.23303469084935932], 
reward next is 0.7670, 
noisyNet noise sample is [array([0.52368486], dtype=float32), -0.20138341]. 
=============================================
[2019-04-04 10:03:34,737] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4500056e-09 7.3602573e-09 6.4096120e-24 2.3000258e-22 4.0198682e-18
 1.0000000e+00 1.7515816e-17], sum to 1.0000
[2019-04-04 10:03:34,737] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4505
[2019-04-04 10:03:34,795] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.516666666666667, 44.16666666666667, 0.0, 0.0, 26.0, 24.94313216871746, 0.2640570528446897, 0.0, 1.0, 49274.20403465608], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2397000.0000, 
sim time next is 2397600.0000, 
raw observation next is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.94425657321375, 0.2644134595768341, 0.0, 1.0, 46850.16578123447], 
processed observation next is [0.0, 0.782608695652174, 0.4155124653739613, 0.44, 0.0, 0.0, 0.6666666666666666, 0.5786880477678125, 0.5881378198589448, 0.0, 1.0, 0.22309602752968796], 
reward next is 0.7769, 
noisyNet noise sample is [array([1.3400704], dtype=float32), 0.3176433]. 
=============================================
[2019-04-04 10:03:34,867] A3C_AGENT_WORKER-Thread-11 INFO:Local step 216500, global step 3462859: loss 1.2295
[2019-04-04 10:03:34,867] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 216500, global step 3462859: learning rate 0.0000
[2019-04-04 10:03:35,678] A3C_AGENT_WORKER-Thread-5 INFO:Local step 216000, global step 3463116: loss 0.2339
[2019-04-04 10:03:35,678] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 216000, global step 3463116: learning rate 0.0000
[2019-04-04 10:03:36,155] A3C_AGENT_WORKER-Thread-12 INFO:Local step 216500, global step 3463251: loss 1.2521
[2019-04-04 10:03:36,156] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 216500, global step 3463251: learning rate 0.0000
[2019-04-04 10:03:36,949] A3C_AGENT_WORKER-Thread-14 INFO:Local step 216000, global step 3463477: loss 0.2385
[2019-04-04 10:03:36,957] A3C_AGENT_WORKER-Thread-18 INFO:Local step 216500, global step 3463480: loss 1.2481
[2019-04-04 10:03:37,005] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 216500, global step 3463493: learning rate 0.0000
[2019-04-04 10:03:37,008] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 216000, global step 3463493: learning rate 0.0000
[2019-04-04 10:03:37,474] A3C_AGENT_WORKER-Thread-19 INFO:Local step 216500, global step 3463648: loss 1.2487
[2019-04-04 10:03:37,475] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 216500, global step 3463648: learning rate 0.0000
[2019-04-04 10:03:39,026] A3C_AGENT_WORKER-Thread-16 INFO:Local step 217000, global step 3464141: loss 0.0664
[2019-04-04 10:03:39,026] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 217000, global step 3464141: learning rate 0.0000
[2019-04-04 10:03:39,691] A3C_AGENT_WORKER-Thread-17 INFO:Local step 216000, global step 3464342: loss 0.2577
[2019-04-04 10:03:39,692] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 216000, global step 3464342: learning rate 0.0000
[2019-04-04 10:03:39,891] A3C_AGENT_WORKER-Thread-13 INFO:Local step 216500, global step 3464407: loss 1.2315
[2019-04-04 10:03:39,892] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 216500, global step 3464407: learning rate 0.0000
[2019-04-04 10:03:40,540] A3C_AGENT_WORKER-Thread-15 INFO:Local step 217000, global step 3464635: loss 0.0693
[2019-04-04 10:03:40,541] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 217000, global step 3464635: learning rate 0.0000
[2019-04-04 10:03:44,099] A3C_AGENT_WORKER-Thread-10 INFO:Local step 216500, global step 3465875: loss 1.2617
[2019-04-04 10:03:44,100] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 216500, global step 3465875: learning rate 0.0000
[2019-04-04 10:03:44,550] A3C_AGENT_WORKER-Thread-2 INFO:Local step 217000, global step 3466026: loss 0.0735
[2019-04-04 10:03:44,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 217000, global step 3466027: learning rate 0.0000
[2019-04-04 10:03:47,962] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5258911e-11 3.4285679e-09 5.5888460e-28 3.9197580e-26 1.6795868e-21
 1.0000000e+00 3.2239932e-21], sum to 1.0000
[2019-04-04 10:03:47,962] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0222
[2019-04-04 10:03:48,016] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 77.0, 77.0, 0.0, 26.0, 25.49852462175828, 0.3346379919224273, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2622600.0000, 
sim time next is 2623200.0000, 
raw observation next is [-6.9, 76.33333333333334, 79.66666666666667, 15.16666666666666, 26.0, 25.70691299275032, 0.3579626825846192, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.27146814404432135, 0.7633333333333334, 0.26555555555555554, 0.01675874769797421, 0.6666666666666666, 0.6422427493958599, 0.619320894194873, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28204033], dtype=float32), -2.0202422]. 
=============================================
[2019-04-04 10:03:48,036] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.4888116e-10 2.7232512e-09 1.1552672e-25 3.5529546e-24 1.1132360e-19
 1.0000000e+00 3.3992747e-19], sum to 1.0000
[2019-04-04 10:03:48,036] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1678
[2019-04-04 10:03:48,101] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 26.0, 25.01223537869182, 0.3252990395828078, 0.0, 1.0, 72571.96969759038], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2576400.0000, 
sim time next is 2577000.0000, 
raw observation next is [-1.516666666666667, 42.66666666666667, 0.0, 0.0, 26.0, 24.98104561169529, 0.3284642584726459, 0.0, 1.0, 76036.09413439865], 
processed observation next is [1.0, 0.8260869565217391, 0.4205909510618652, 0.4266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5817538009746075, 0.6094880861575486, 0.0, 1.0, 0.36207663873523166], 
reward next is 0.6379, 
noisyNet noise sample is [array([0.40024874], dtype=float32), 1.9413418]. 
=============================================
[2019-04-04 10:03:48,114] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[77.99181 ]
 [79.204346]
 [80.76918 ]
 [82.74434 ]
 [82.67383 ]], R is [[77.01674652]
 [76.90100098]
 [77.04289246]
 [77.27246094]
 [77.40026093]].
[2019-04-04 10:03:48,170] A3C_AGENT_WORKER-Thread-6 INFO:Local step 217000, global step 3467241: loss 0.0921
[2019-04-04 10:03:48,171] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 217000, global step 3467241: learning rate 0.0000
[2019-04-04 10:03:48,319] A3C_AGENT_WORKER-Thread-3 INFO:Local step 217000, global step 3467291: loss 0.0856
[2019-04-04 10:03:48,319] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 217000, global step 3467291: learning rate 0.0000
[2019-04-04 10:03:49,508] A3C_AGENT_WORKER-Thread-4 INFO:Local step 217000, global step 3467735: loss 0.0878
[2019-04-04 10:03:49,508] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 217000, global step 3467735: learning rate 0.0000
[2019-04-04 10:03:53,627] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8637018e-10 8.2531653e-09 1.3183201e-25 1.4817537e-23 1.0272682e-19
 1.0000000e+00 5.3635644e-18], sum to 1.0000
[2019-04-04 10:03:53,627] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4904
[2019-04-04 10:03:53,646] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 79.33333333333333, 0.0, 0.0, 26.0, 24.23167808553755, 0.1724102141462261, 0.0, 1.0, 42666.16720506459], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2958000.0000, 
sim time next is 2958600.0000, 
raw observation next is [-3.833333333333333, 78.16666666666667, 0.0, 0.0, 26.0, 24.20472659229788, 0.1644678350489162, 0.0, 1.0, 42637.4796995032], 
processed observation next is [0.0, 0.21739130434782608, 0.3564173591874424, 0.7816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5170605493581567, 0.5548226116829721, 0.0, 1.0, 0.2030356176166819], 
reward next is 0.7970, 
noisyNet noise sample is [array([-0.26672393], dtype=float32), -0.44462588]. 
=============================================
[2019-04-04 10:03:56,774] A3C_AGENT_WORKER-Thread-20 INFO:Local step 216000, global step 3470179: loss 0.2746
[2019-04-04 10:03:56,788] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 216000, global step 3470179: learning rate 0.0000
[2019-04-04 10:03:57,806] A3C_AGENT_WORKER-Thread-11 INFO:Local step 217000, global step 3470522: loss 0.0769
[2019-04-04 10:03:57,808] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 217000, global step 3470522: learning rate 0.0000
[2019-04-04 10:03:59,064] A3C_AGENT_WORKER-Thread-12 INFO:Local step 217000, global step 3470953: loss 0.0808
[2019-04-04 10:03:59,064] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 217000, global step 3470953: learning rate 0.0000
[2019-04-04 10:04:00,169] A3C_AGENT_WORKER-Thread-18 INFO:Local step 217000, global step 3471303: loss 0.0788
[2019-04-04 10:04:00,171] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 217000, global step 3471304: learning rate 0.0000
[2019-04-04 10:04:00,376] A3C_AGENT_WORKER-Thread-19 INFO:Local step 217000, global step 3471375: loss 0.0768
[2019-04-04 10:04:00,379] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 217000, global step 3471375: learning rate 0.0000
[2019-04-04 10:04:01,087] A3C_AGENT_WORKER-Thread-5 INFO:Local step 216500, global step 3471638: loss 1.3339
[2019-04-04 10:04:01,087] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 216500, global step 3471638: learning rate 0.0000
[2019-04-04 10:04:02,138] A3C_AGENT_WORKER-Thread-16 INFO:Local step 217500, global step 3471987: loss 0.1506
[2019-04-04 10:04:02,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 217500, global step 3471989: learning rate 0.0000
[2019-04-04 10:04:02,536] A3C_AGENT_WORKER-Thread-13 INFO:Local step 217000, global step 3472131: loss 0.0676
[2019-04-04 10:04:02,548] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 217000, global step 3472131: learning rate 0.0000
[2019-04-04 10:04:02,602] A3C_AGENT_WORKER-Thread-14 INFO:Local step 216500, global step 3472154: loss 1.3469
[2019-04-04 10:04:02,606] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 216500, global step 3472154: learning rate 0.0000
[2019-04-04 10:04:03,667] A3C_AGENT_WORKER-Thread-15 INFO:Local step 217500, global step 3472489: loss 0.1541
[2019-04-04 10:04:03,668] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 217500, global step 3472489: learning rate 0.0000
[2019-04-04 10:04:04,921] A3C_AGENT_WORKER-Thread-17 INFO:Local step 216500, global step 3472945: loss 1.3464
[2019-04-04 10:04:04,924] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 216500, global step 3472945: learning rate 0.0000
[2019-04-04 10:04:06,893] A3C_AGENT_WORKER-Thread-10 INFO:Local step 217000, global step 3473668: loss 0.0798
[2019-04-04 10:04:06,894] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 217000, global step 3473668: learning rate 0.0000
[2019-04-04 10:04:07,014] A3C_AGENT_WORKER-Thread-2 INFO:Local step 217500, global step 3473721: loss 0.1489
[2019-04-04 10:04:07,015] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 217500, global step 3473722: learning rate 0.0000
[2019-04-04 10:04:10,367] A3C_AGENT_WORKER-Thread-3 INFO:Local step 217500, global step 3475083: loss 0.1505
[2019-04-04 10:04:10,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 217500, global step 3475083: learning rate 0.0000
[2019-04-04 10:04:10,651] A3C_AGENT_WORKER-Thread-6 INFO:Local step 217500, global step 3475186: loss 0.1488
[2019-04-04 10:04:10,652] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 217500, global step 3475186: learning rate 0.0000
[2019-04-04 10:04:12,208] A3C_AGENT_WORKER-Thread-4 INFO:Local step 217500, global step 3475820: loss 0.1537
[2019-04-04 10:04:12,211] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 217500, global step 3475821: learning rate 0.0000
[2019-04-04 10:04:16,706] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7087685e-10 2.2295312e-09 1.3084003e-25 2.2887853e-24 6.7594880e-20
 1.0000000e+00 2.3608146e-19], sum to 1.0000
[2019-04-04 10:04:16,706] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4271
[2019-04-04 10:04:16,720] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.833333333333334, 79.33333333333333, 0.0, 0.0, 26.0, 25.5155076689512, 0.594410836544495, 0.0, 1.0, 18749.83764524996], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3271800.0000, 
sim time next is 3272400.0000, 
raw observation next is [-5.0, 81.0, 0.0, 0.0, 26.0, 25.66622503107299, 0.5950807394529035, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.32409972299168976, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6388520859227492, 0.6983602464843012, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.81281376], dtype=float32), -0.24196707]. 
=============================================
[2019-04-04 10:04:19,238] A3C_AGENT_WORKER-Thread-11 INFO:Local step 217500, global step 3478491: loss 0.1562
[2019-04-04 10:04:19,239] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 217500, global step 3478491: learning rate 0.0000
[2019-04-04 10:04:20,412] A3C_AGENT_WORKER-Thread-12 INFO:Local step 217500, global step 3478940: loss 0.1564
[2019-04-04 10:04:20,413] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 217500, global step 3478941: learning rate 0.0000
[2019-04-04 10:04:21,300] A3C_AGENT_WORKER-Thread-20 INFO:Local step 216500, global step 3479277: loss 1.3823
[2019-04-04 10:04:21,304] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 216500, global step 3479278: learning rate 0.0000
[2019-04-04 10:04:21,642] A3C_AGENT_WORKER-Thread-16 INFO:Local step 218000, global step 3479407: loss 1.0241
[2019-04-04 10:04:21,643] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 218000, global step 3479407: learning rate 0.0000
[2019-04-04 10:04:21,810] A3C_AGENT_WORKER-Thread-18 INFO:Local step 217500, global step 3479468: loss 0.1686
[2019-04-04 10:04:21,812] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 217500, global step 3479468: learning rate 0.0000
[2019-04-04 10:04:22,335] A3C_AGENT_WORKER-Thread-19 INFO:Local step 217500, global step 3479667: loss 0.1718
[2019-04-04 10:04:22,336] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 217500, global step 3479667: learning rate 0.0000
[2019-04-04 10:04:22,512] A3C_AGENT_WORKER-Thread-15 INFO:Local step 218000, global step 3479735: loss 1.0236
[2019-04-04 10:04:22,513] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 218000, global step 3479735: learning rate 0.0000
[2019-04-04 10:04:23,167] A3C_AGENT_WORKER-Thread-5 INFO:Local step 217000, global step 3479975: loss 0.0512
[2019-04-04 10:04:23,171] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 217000, global step 3479977: learning rate 0.0000
[2019-04-04 10:04:23,609] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2044269e-09 1.4047277e-08 1.0461169e-24 3.9172674e-23 4.6905116e-19
 1.0000000e+00 8.3239044e-18], sum to 1.0000
[2019-04-04 10:04:23,616] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1126
[2019-04-04 10:04:23,628] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.66364262874959, 0.2486464830684262, 0.0, 1.0, 43363.88917734687], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2766000.0000, 
sim time next is 2766600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.64120014719525, 0.2461485832070079, 0.0, 1.0, 43140.07923676342], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5534333455996041, 0.5820495277356693, 0.0, 1.0, 0.20542894874649248], 
reward next is 0.7946, 
noisyNet noise sample is [array([0.5753395], dtype=float32), 2.01812]. 
=============================================
[2019-04-04 10:04:24,122] A3C_AGENT_WORKER-Thread-13 INFO:Local step 217500, global step 3480378: loss 0.1679
[2019-04-04 10:04:24,124] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 217500, global step 3480379: learning rate 0.0000
[2019-04-04 10:04:24,144] A3C_AGENT_WORKER-Thread-14 INFO:Local step 217000, global step 3480385: loss 0.0515
[2019-04-04 10:04:24,146] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 217000, global step 3480388: learning rate 0.0000
[2019-04-04 10:04:25,708] A3C_AGENT_WORKER-Thread-2 INFO:Local step 218000, global step 3481089: loss 1.0431
[2019-04-04 10:04:25,709] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 218000, global step 3481090: learning rate 0.0000
[2019-04-04 10:04:26,458] A3C_AGENT_WORKER-Thread-17 INFO:Local step 217000, global step 3481424: loss 0.0544
[2019-04-04 10:04:26,458] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 217000, global step 3481424: learning rate 0.0000
[2019-04-04 10:04:27,142] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.5531352e-12 2.7765162e-10 1.0191064e-27 2.1610820e-26 1.1061908e-21
 1.0000000e+00 6.2098125e-21], sum to 1.0000
[2019-04-04 10:04:27,143] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3667
[2019-04-04 10:04:27,176] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 24.98559883959826, 0.4260191682615519, 0.0, 1.0, 94808.19138469317], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3440400.0000, 
sim time next is 3441000.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 24.98235813754905, 0.4360535231200394, 1.0, 1.0, 59151.31660126976], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5818631781290874, 0.6453511743733465, 1.0, 1.0, 0.28167293619652267], 
reward next is 0.7183, 
noisyNet noise sample is [array([-0.6045511], dtype=float32), 1.2423086]. 
=============================================
[2019-04-04 10:04:27,188] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[84.34561]
 [85.3684 ]
 [84.48581]
 [85.90873]
 [85.70691]], R is [[85.17670441]
 [84.87346649]
 [84.61167145]
 [84.57926178]
 [84.7334671 ]].
[2019-04-04 10:04:27,694] A3C_AGENT_WORKER-Thread-10 INFO:Local step 217500, global step 3482010: loss 0.1745
[2019-04-04 10:04:27,695] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 217500, global step 3482010: learning rate 0.0000
[2019-04-04 10:04:29,014] A3C_AGENT_WORKER-Thread-6 INFO:Local step 218000, global step 3482569: loss 1.0266
[2019-04-04 10:04:29,021] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 218000, global step 3482572: learning rate 0.0000
[2019-04-04 10:04:29,161] A3C_AGENT_WORKER-Thread-3 INFO:Local step 218000, global step 3482646: loss 1.0350
[2019-04-04 10:04:29,169] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 218000, global step 3482646: learning rate 0.0000
[2019-04-04 10:04:30,623] A3C_AGENT_WORKER-Thread-4 INFO:Local step 218000, global step 3483405: loss 1.0096
[2019-04-04 10:04:30,627] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 218000, global step 3483405: learning rate 0.0000
[2019-04-04 10:04:34,071] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.5376000e-10 2.1448976e-08 7.6983846e-25 1.0712400e-23 1.9164754e-19
 1.0000000e+00 3.6911185e-18], sum to 1.0000
[2019-04-04 10:04:34,071] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6542
[2019-04-04 10:04:34,092] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 70.0, 45.5, 273.0, 26.0, 24.20483961964074, 0.1987446204206891, 0.0, 1.0, 41420.32562502008], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3571200.0000, 
sim time next is 3571800.0000, 
raw observation next is [-6.833333333333334, 70.0, 59.66666666666667, 323.6666666666667, 26.0, 24.1953769810359, 0.2037232567514967, 0.0, 1.0, 41301.35311929811], 
processed observation next is [0.0, 0.34782608695652173, 0.27331486611265005, 0.7, 0.1988888888888889, 0.3576427255985267, 0.6666666666666666, 0.516281415086325, 0.5679077522504988, 0.0, 1.0, 0.19667311009189575], 
reward next is 0.8033, 
noisyNet noise sample is [array([0.9923869], dtype=float32), 0.0001835335]. 
=============================================
[2019-04-04 10:04:36,652] A3C_AGENT_WORKER-Thread-11 INFO:Local step 218000, global step 3486127: loss 0.8895
[2019-04-04 10:04:36,658] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 218000, global step 3486127: learning rate 0.0000
[2019-04-04 10:04:37,965] A3C_AGENT_WORKER-Thread-12 INFO:Local step 218000, global step 3486690: loss 0.9273
[2019-04-04 10:04:37,971] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 218000, global step 3486692: learning rate 0.0000
[2019-04-04 10:04:37,996] A3C_AGENT_WORKER-Thread-16 INFO:Local step 218500, global step 3486709: loss 8.9450
[2019-04-04 10:04:37,997] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 218500, global step 3486709: learning rate 0.0000
[2019-04-04 10:04:39,172] A3C_AGENT_WORKER-Thread-18 INFO:Local step 218000, global step 3487215: loss 0.9041
[2019-04-04 10:04:39,174] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 218000, global step 3487215: learning rate 0.0000
[2019-04-04 10:04:39,411] A3C_AGENT_WORKER-Thread-15 INFO:Local step 218500, global step 3487346: loss 8.9429
[2019-04-04 10:04:39,413] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 218500, global step 3487346: learning rate 0.0000
[2019-04-04 10:04:39,824] A3C_AGENT_WORKER-Thread-19 INFO:Local step 218000, global step 3487555: loss 0.9346
[2019-04-04 10:04:39,826] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 218000, global step 3487555: learning rate 0.0000
[2019-04-04 10:04:39,943] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.5305001e-09 5.8494916e-09 1.3877934e-26 2.7311161e-24 9.1191327e-19
 1.0000000e+00 2.8606112e-18], sum to 1.0000
[2019-04-04 10:04:39,944] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3351
[2019-04-04 10:04:39,952] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 67.0, 0.0, 0.0, 26.0, 25.51597923177305, 0.4030115921958948, 0.0, 1.0, 82663.0685763576], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3706200.0000, 
sim time next is 3706800.0000, 
raw observation next is [0.6666666666666667, 68.66666666666667, 0.0, 0.0, 26.0, 25.45375880727784, 0.4048932638732558, 0.0, 1.0, 89476.66854211177], 
processed observation next is [0.0, 0.9130434782608695, 0.4810710987996307, 0.6866666666666668, 0.0, 0.0, 0.6666666666666666, 0.6211465672731533, 0.6349644212910853, 0.0, 1.0, 0.4260793740100561], 
reward next is 0.5739, 
noisyNet noise sample is [array([-0.30922917], dtype=float32), -1.0984101]. 
=============================================
[2019-04-04 10:04:41,189] A3C_AGENT_WORKER-Thread-13 INFO:Local step 218000, global step 3488162: loss 0.9136
[2019-04-04 10:04:41,193] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 218000, global step 3488164: learning rate 0.0000
[2019-04-04 10:04:41,302] A3C_AGENT_WORKER-Thread-20 INFO:Local step 217000, global step 3488218: loss 0.0315
[2019-04-04 10:04:41,303] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 217000, global step 3488218: learning rate 0.0000
[2019-04-04 10:04:41,998] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.7947832e-10 7.8826048e-09 7.4763745e-25 2.0393186e-23 5.9405687e-19
 1.0000000e+00 7.8492575e-19], sum to 1.0000
[2019-04-04 10:04:41,998] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0508
[2019-04-04 10:04:42,011] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.58324336535351, 0.1942639775639764, 0.0, 1.0, 38056.83393186396], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3028200.0000, 
sim time next is 3028800.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.60031698048386, 0.1895443154541918, 0.0, 1.0, 38095.48688355942], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5500264150403217, 0.5631814384847306, 0.0, 1.0, 0.181407080397902], 
reward next is 0.8186, 
noisyNet noise sample is [array([1.0907589], dtype=float32), 1.8305103]. 
=============================================
[2019-04-04 10:04:42,320] A3C_AGENT_WORKER-Thread-2 INFO:Local step 218500, global step 3488680: loss 8.9206
[2019-04-04 10:04:42,320] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 218500, global step 3488680: learning rate 0.0000
[2019-04-04 10:04:43,320] A3C_AGENT_WORKER-Thread-5 INFO:Local step 217500, global step 3489130: loss 0.1788
[2019-04-04 10:04:43,320] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 217500, global step 3489130: learning rate 0.0000
[2019-04-04 10:04:43,971] A3C_AGENT_WORKER-Thread-14 INFO:Local step 217500, global step 3489410: loss 0.1828
[2019-04-04 10:04:43,984] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 217500, global step 3489418: learning rate 0.0000
[2019-04-04 10:04:44,708] A3C_AGENT_WORKER-Thread-10 INFO:Local step 218000, global step 3489785: loss 0.9462
[2019-04-04 10:04:44,716] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 218000, global step 3489785: learning rate 0.0000
[2019-04-04 10:04:44,912] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.10249185e-10 2.46717891e-09 1.02075273e-26 1.97000927e-25
 1.33134712e-20 1.00000000e+00 9.68540092e-20], sum to 1.0000
[2019-04-04 10:04:44,913] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1161
[2019-04-04 10:04:44,935] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.5, 27.0, 118.0, 0.0, 26.0, 25.92091289639496, 0.4121785671108053, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2817000.0000, 
sim time next is 2817600.0000, 
raw observation next is [6.666666666666666, 26.0, 114.1666666666667, 0.0, 26.0, 25.85769130177954, 0.3072778393010481, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6472760849492153, 0.26, 0.38055555555555565, 0.0, 0.6666666666666666, 0.6548076084816282, 0.6024259464336826, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.198494], dtype=float32), 0.737485]. 
=============================================
[2019-04-04 10:04:45,272] A3C_AGENT_WORKER-Thread-6 INFO:Local step 218500, global step 3490087: loss 8.9195
[2019-04-04 10:04:45,273] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 218500, global step 3490087: learning rate 0.0000
[2019-04-04 10:04:45,687] A3C_AGENT_WORKER-Thread-3 INFO:Local step 218500, global step 3490316: loss 8.8989
[2019-04-04 10:04:45,688] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 218500, global step 3490316: learning rate 0.0000
[2019-04-04 10:04:46,447] A3C_AGENT_WORKER-Thread-17 INFO:Local step 217500, global step 3490668: loss 0.1772
[2019-04-04 10:04:46,452] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 217500, global step 3490668: learning rate 0.0000
[2019-04-04 10:04:46,459] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1797159e-10 5.6038649e-09 6.6576680e-26 5.2288990e-24 2.1134282e-20
 1.0000000e+00 3.1907538e-19], sum to 1.0000
[2019-04-04 10:04:46,459] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4728
[2019-04-04 10:04:46,504] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 54.0, 102.5, 697.0, 26.0, 25.24259154494801, 0.317509780929122, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3060000.0000, 
sim time next is 3060600.0000, 
raw observation next is [-4.0, 54.0, 103.6666666666667, 717.6666666666667, 26.0, 25.20018558071985, 0.3178488087000712, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.34555555555555567, 0.7930018416206263, 0.6666666666666666, 0.6000154650599875, 0.6059496029000238, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34589434], dtype=float32), -0.19832912]. 
=============================================
[2019-04-04 10:04:47,296] A3C_AGENT_WORKER-Thread-4 INFO:Local step 218500, global step 3491103: loss 8.8700
[2019-04-04 10:04:47,298] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 218500, global step 3491104: learning rate 0.0000
[2019-04-04 10:04:52,127] A3C_AGENT_WORKER-Thread-11 INFO:Local step 218500, global step 3493628: loss 8.9155
[2019-04-04 10:04:52,128] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 218500, global step 3493628: learning rate 0.0000
[2019-04-04 10:04:53,458] A3C_AGENT_WORKER-Thread-12 INFO:Local step 218500, global step 3494377: loss 8.9099
[2019-04-04 10:04:53,460] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 218500, global step 3494377: learning rate 0.0000
[2019-04-04 10:04:54,224] A3C_AGENT_WORKER-Thread-16 INFO:Local step 219000, global step 3494752: loss 7.4584
[2019-04-04 10:04:54,225] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 219000, global step 3494752: learning rate 0.0000
[2019-04-04 10:04:55,203] A3C_AGENT_WORKER-Thread-18 INFO:Local step 218500, global step 3495107: loss 8.9011
[2019-04-04 10:04:55,203] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 218500, global step 3495107: learning rate 0.0000
[2019-04-04 10:04:55,482] A3C_AGENT_WORKER-Thread-15 INFO:Local step 219000, global step 3495208: loss 7.3206
[2019-04-04 10:04:55,485] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 219000, global step 3495210: learning rate 0.0000
[2019-04-04 10:04:56,046] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2908706e-09 6.9137811e-09 7.3287204e-25 2.8827540e-23 4.4782964e-19
 1.0000000e+00 5.8534046e-18], sum to 1.0000
[2019-04-04 10:04:56,046] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7828
[2019-04-04 10:04:56,084] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.166666666666667, 51.66666666666667, 0.0, 0.0, 26.0, 25.40787898411963, 0.3844421805686482, 0.0, 1.0, 35118.6726749068], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3622200.0000, 
sim time next is 3622800.0000, 
raw observation next is [-2.333333333333333, 53.33333333333334, 0.0, 0.0, 26.0, 25.38964750505538, 0.3793990042757614, 0.0, 1.0, 45743.76644619827], 
processed observation next is [0.0, 0.9565217391304348, 0.3979686057248385, 0.5333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6158039587546149, 0.6264663347585872, 0.0, 1.0, 0.21782745926761082], 
reward next is 0.7822, 
noisyNet noise sample is [array([-0.20965603], dtype=float32), -1.1191316]. 
=============================================
[2019-04-04 10:04:56,289] A3C_AGENT_WORKER-Thread-19 INFO:Local step 218500, global step 3495551: loss 8.8667
[2019-04-04 10:04:56,291] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 218500, global step 3495553: learning rate 0.0000
[2019-04-04 10:04:58,291] A3C_AGENT_WORKER-Thread-2 INFO:Local step 219000, global step 3496308: loss 7.3423
[2019-04-04 10:04:58,292] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 219000, global step 3496308: learning rate 0.0000
[2019-04-04 10:04:58,476] A3C_AGENT_WORKER-Thread-13 INFO:Local step 218500, global step 3496357: loss 8.9076
[2019-04-04 10:04:58,487] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 218500, global step 3496357: learning rate 0.0000
[2019-04-04 10:05:01,188] A3C_AGENT_WORKER-Thread-5 INFO:Local step 218000, global step 3497359: loss 0.8779
[2019-04-04 10:05:01,216] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 218000, global step 3497359: learning rate 0.0000
[2019-04-04 10:05:01,742] A3C_AGENT_WORKER-Thread-20 INFO:Local step 217500, global step 3497561: loss 0.1833
[2019-04-04 10:05:01,743] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 217500, global step 3497561: learning rate 0.0000
[2019-04-04 10:05:02,027] A3C_AGENT_WORKER-Thread-14 INFO:Local step 218000, global step 3497684: loss 0.8773
[2019-04-04 10:05:02,029] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 218000, global step 3497685: learning rate 0.0000
[2019-04-04 10:05:02,162] A3C_AGENT_WORKER-Thread-10 INFO:Local step 218500, global step 3497737: loss 8.9391
[2019-04-04 10:05:02,165] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 218500, global step 3497738: learning rate 0.0000
[2019-04-04 10:05:02,817] A3C_AGENT_WORKER-Thread-6 INFO:Local step 219000, global step 3497979: loss 7.3325
[2019-04-04 10:05:02,817] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 219000, global step 3497979: learning rate 0.0000
[2019-04-04 10:05:02,918] A3C_AGENT_WORKER-Thread-3 INFO:Local step 219000, global step 3498016: loss 7.3240
[2019-04-04 10:05:02,921] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 219000, global step 3498017: learning rate 0.0000
[2019-04-04 10:05:04,415] A3C_AGENT_WORKER-Thread-4 INFO:Local step 219000, global step 3498554: loss 7.3950
[2019-04-04 10:05:04,416] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 219000, global step 3498554: learning rate 0.0000
[2019-04-04 10:05:05,493] A3C_AGENT_WORKER-Thread-17 INFO:Local step 218000, global step 3498882: loss 0.8657
[2019-04-04 10:05:05,494] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 218000, global step 3498882: learning rate 0.0000
[2019-04-04 10:05:08,914] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-04 10:05:08,921] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:05:08,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:05:08,923] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run36
[2019-04-04 10:05:08,959] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:05:08,963] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:05:08,961] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:05:08,963] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:05:08,969] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run36
[2019-04-04 10:05:09,007] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run36
[2019-04-04 10:06:56,523] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.26737094], dtype=float32), 0.29962766]
[2019-04-04 10:06:56,523] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.9, 84.66666666666667, 0.0, 0.0, 26.0, 25.39742569837657, 0.4291520648016933, 0.0, 1.0, 36209.47238762285]
[2019-04-04 10:06:56,523] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 10:06:56,524] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.9421930e-10 2.1951834e-09 8.0128707e-26 5.7640674e-24 1.0973448e-19
 1.0000000e+00 5.3151876e-19], sampled 0.40604100362600914
[2019-04-04 10:08:17,817] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 10:08:48,243] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 10:08:50,737] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 10:08:51,769] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 3500000, evaluation results [3500000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 10:08:53,611] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2937792e-10 6.9408275e-09 4.2321475e-25 9.0589536e-24 3.7606740e-19
 1.0000000e+00 2.8373645e-18], sum to 1.0000
[2019-04-04 10:08:53,611] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2381
[2019-04-04 10:08:53,756] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.166666666666667, 63.0, 0.0, 0.0, 26.0, 24.64138884229653, 0.2364954928161705, 0.0, 1.0, 42752.44652320597], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3913800.0000, 
sim time next is 3914400.0000, 
raw observation next is [-7.333333333333334, 62.0, 5.0, 135.8333333333333, 26.0, 24.63974944397295, 0.3510473397003071, 1.0, 1.0, 202408.5437745771], 
processed observation next is [1.0, 0.30434782608695654, 0.2594644506001847, 0.62, 0.016666666666666666, 0.1500920810313075, 0.6666666666666666, 0.5533124536644124, 0.6170157799001024, 1.0, 1.0, 0.9638502084503671], 
reward next is 0.0361, 
noisyNet noise sample is [array([0.11758713], dtype=float32), -1.1827786]. 
=============================================
[2019-04-04 10:08:54,059] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6674661e-12 3.1772621e-10 9.1798912e-28 3.2524315e-27 1.4944026e-21
 1.0000000e+00 2.6222168e-22], sum to 1.0000
[2019-04-04 10:08:54,059] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5678
[2019-04-04 10:08:54,120] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.666666666666666, 42.66666666666667, 116.5, 825.8333333333334, 26.0, 25.72461963556362, 0.6516634072868236, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3936000.0000, 
sim time next is 3936600.0000, 
raw observation next is [-5.5, 41.5, 116.0, 824.0, 26.0, 26.30851586748598, 0.6938704777635504, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3102493074792244, 0.415, 0.38666666666666666, 0.9104972375690608, 0.6666666666666666, 0.6923763222904983, 0.7312901592545168, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9483676], dtype=float32), -0.17109586]. 
=============================================
[2019-04-04 10:08:56,603] A3C_AGENT_WORKER-Thread-11 INFO:Local step 219000, global step 3501550: loss 7.4404
[2019-04-04 10:08:56,604] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 219000, global step 3501550: learning rate 0.0000
[2019-04-04 10:08:58,694] A3C_AGENT_WORKER-Thread-12 INFO:Local step 219000, global step 3502175: loss 6.9485
[2019-04-04 10:08:58,695] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 219000, global step 3502175: learning rate 0.0000
[2019-04-04 10:08:59,502] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4295062e-10 3.3722789e-09 5.6749144e-26 5.4118666e-24 8.8053467e-20
 1.0000000e+00 2.5982637e-19], sum to 1.0000
[2019-04-04 10:08:59,508] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6078
[2019-04-04 10:08:59,531] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.333333333333334, 74.0, 0.0, 0.0, 26.0, 25.21223127589737, 0.3529539074027324, 0.0, 1.0, 41019.62105874462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3906600.0000, 
sim time next is 3907200.0000, 
raw observation next is [-4.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.21442428862093, 0.3415776827169387, 0.0, 1.0, 41100.13441992427], 
processed observation next is [1.0, 0.21739130434782608, 0.3333333333333333, 0.71, 0.0, 0.0, 0.6666666666666666, 0.601202024051744, 0.6138592275723129, 0.0, 1.0, 0.19571492580916316], 
reward next is 0.8043, 
noisyNet noise sample is [array([0.10287033], dtype=float32), 0.77427316]. 
=============================================
[2019-04-04 10:09:00,293] A3C_AGENT_WORKER-Thread-16 INFO:Local step 219500, global step 3502688: loss 0.0006
[2019-04-04 10:09:00,295] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 219500, global step 3502688: learning rate 0.0000
[2019-04-04 10:09:00,611] A3C_AGENT_WORKER-Thread-18 INFO:Local step 219000, global step 3502775: loss 7.0053
[2019-04-04 10:09:00,613] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 219000, global step 3502775: learning rate 0.0000
[2019-04-04 10:09:02,384] A3C_AGENT_WORKER-Thread-15 INFO:Local step 219500, global step 3503294: loss 0.0009
[2019-04-04 10:09:02,384] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 219500, global step 3503294: learning rate 0.0000
[2019-04-04 10:09:02,989] A3C_AGENT_WORKER-Thread-19 INFO:Local step 219000, global step 3503494: loss 7.4381
[2019-04-04 10:09:02,991] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 219000, global step 3503496: learning rate 0.0000
[2019-04-04 10:09:03,672] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.0193091e-09 6.8116350e-08 3.6141164e-22 4.1905160e-21 3.5742143e-17
 9.9999988e-01 1.4120231e-16], sum to 1.0000
[2019-04-04 10:09:03,673] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3803
[2019-04-04 10:09:03,688] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.0, 67.0, 0.0, 0.0, 26.0, 23.72295404440339, 0.01967444336171912, 0.0, 1.0, 43821.18513515228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3993600.0000, 
sim time next is 3994200.0000, 
raw observation next is [-13.0, 66.0, 0.0, 0.0, 26.0, 23.68647104575859, 0.005360513786127984, 0.0, 1.0, 43795.13356183017], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.66, 0.0, 0.0, 0.6666666666666666, 0.47387258714654923, 0.5017868379287093, 0.0, 1.0, 0.20854825505633412], 
reward next is 0.7915, 
noisyNet noise sample is [array([-0.39568877], dtype=float32), -1.2785422]. 
=============================================
[2019-04-04 10:09:04,728] A3C_AGENT_WORKER-Thread-13 INFO:Local step 219000, global step 3504005: loss 7.5650
[2019-04-04 10:09:04,729] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 219000, global step 3504005: learning rate 0.0000
[2019-04-04 10:09:06,570] A3C_AGENT_WORKER-Thread-2 INFO:Local step 219500, global step 3504533: loss 0.0004
[2019-04-04 10:09:06,607] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 219500, global step 3504535: learning rate 0.0000
[2019-04-04 10:09:08,765] A3C_AGENT_WORKER-Thread-5 INFO:Local step 218500, global step 3505170: loss 8.9814
[2019-04-04 10:09:08,766] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 218500, global step 3505170: learning rate 0.0000
[2019-04-04 10:09:09,076] A3C_AGENT_WORKER-Thread-14 INFO:Local step 218500, global step 3505275: loss 8.9753
[2019-04-04 10:09:09,077] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 218500, global step 3505275: learning rate 0.0000
[2019-04-04 10:09:09,295] A3C_AGENT_WORKER-Thread-10 INFO:Local step 219000, global step 3505341: loss 7.6300
[2019-04-04 10:09:09,295] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 219000, global step 3505341: learning rate 0.0000
[2019-04-04 10:09:10,134] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1509264e-09 3.9514241e-09 3.0373327e-25 4.2635559e-23 1.6268585e-18
 1.0000000e+00 5.3259070e-18], sum to 1.0000
[2019-04-04 10:09:10,138] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1165
[2019-04-04 10:09:10,166] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.54077134286621, 0.40788440018609, 0.0, 1.0, 18746.09412486411], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3617400.0000, 
sim time next is 3618000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.50356455952674, 0.40226706980678, 0.0, 1.0, 37384.40733853273], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6252970466272284, 0.6340890232689267, 0.0, 1.0, 0.17802098732634633], 
reward next is 0.8220, 
noisyNet noise sample is [array([0.7125852], dtype=float32), 0.98128927]. 
=============================================
[2019-04-04 10:09:10,171] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[76.16799 ]
 [76.304405]
 [76.49624 ]
 [76.69985 ]
 [76.7779  ]], R is [[76.24494934]
 [76.39323425]
 [76.62930298]
 [76.86301422]
 [77.09438324]].
[2019-04-04 10:09:10,643] A3C_AGENT_WORKER-Thread-20 INFO:Local step 218000, global step 3505799: loss 0.9868
[2019-04-04 10:09:10,643] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 218000, global step 3505799: learning rate 0.0000
[2019-04-04 10:09:10,926] A3C_AGENT_WORKER-Thread-6 INFO:Local step 219500, global step 3505905: loss 0.0002
[2019-04-04 10:09:10,928] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 219500, global step 3505905: learning rate 0.0000
[2019-04-04 10:09:11,088] A3C_AGENT_WORKER-Thread-3 INFO:Local step 219500, global step 3505961: loss 0.0003
[2019-04-04 10:09:11,088] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 219500, global step 3505961: learning rate 0.0000
[2019-04-04 10:09:13,231] A3C_AGENT_WORKER-Thread-17 INFO:Local step 218500, global step 3506738: loss 8.9098
[2019-04-04 10:09:13,237] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 218500, global step 3506741: learning rate 0.0000
[2019-04-04 10:09:13,516] A3C_AGENT_WORKER-Thread-4 INFO:Local step 219500, global step 3506865: loss 0.0002
[2019-04-04 10:09:13,517] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 219500, global step 3506865: learning rate 0.0000
[2019-04-04 10:09:20,176] A3C_AGENT_WORKER-Thread-11 INFO:Local step 219500, global step 3509952: loss 0.0002
[2019-04-04 10:09:20,177] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 219500, global step 3509952: learning rate 0.0000
[2019-04-04 10:09:21,102] A3C_AGENT_WORKER-Thread-16 INFO:Local step 220000, global step 3510430: loss 0.4692
[2019-04-04 10:09:21,104] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 220000, global step 3510430: learning rate 0.0000
[2019-04-04 10:09:21,121] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.78832729e-11 2.61114907e-09 4.62625128e-28 1.00178124e-26
 1.75361038e-21 1.00000000e+00 1.96886483e-20], sum to 1.0000
[2019-04-04 10:09:21,122] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1311
[2019-04-04 10:09:21,148] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.166666666666667, 54.16666666666666, 196.6666666666667, 446.3333333333334, 26.0, 25.57059079596834, 0.4070062286848784, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4270200.0000, 
sim time next is 4270800.0000, 
raw observation next is [4.333333333333334, 54.33333333333334, 200.3333333333333, 525.1666666666667, 26.0, 25.50842103031189, 0.4061646324517872, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.58264081255771, 0.5433333333333334, 0.6677777777777776, 0.5802946593001842, 0.6666666666666666, 0.6257017525259908, 0.6353882108172624, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3331095], dtype=float32), -0.0008751172]. 
=============================================
[2019-04-04 10:09:21,217] A3C_AGENT_WORKER-Thread-12 INFO:Local step 219500, global step 3510490: loss 0.0002
[2019-04-04 10:09:21,218] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 219500, global step 3510490: learning rate 0.0000
[2019-04-04 10:09:22,221] A3C_AGENT_WORKER-Thread-15 INFO:Local step 220000, global step 3510964: loss 0.4712
[2019-04-04 10:09:22,222] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 220000, global step 3510964: learning rate 0.0000
[2019-04-04 10:09:22,928] A3C_AGENT_WORKER-Thread-18 INFO:Local step 219500, global step 3511311: loss 0.0002
[2019-04-04 10:09:22,928] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 219500, global step 3511311: learning rate 0.0000
[2019-04-04 10:09:24,199] A3C_AGENT_WORKER-Thread-19 INFO:Local step 219500, global step 3511887: loss 0.0003
[2019-04-04 10:09:24,201] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 219500, global step 3511887: learning rate 0.0000
[2019-04-04 10:09:24,766] A3C_AGENT_WORKER-Thread-2 INFO:Local step 220000, global step 3512143: loss 0.4969
[2019-04-04 10:09:24,767] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 220000, global step 3512143: learning rate 0.0000
[2019-04-04 10:09:25,065] A3C_AGENT_WORKER-Thread-13 INFO:Local step 219500, global step 3512269: loss 0.0002
[2019-04-04 10:09:25,067] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 219500, global step 3512270: learning rate 0.0000
[2019-04-04 10:09:26,911] A3C_AGENT_WORKER-Thread-5 INFO:Local step 219000, global step 3513152: loss 7.7786
[2019-04-04 10:09:26,913] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 219000, global step 3513152: learning rate 0.0000
[2019-04-04 10:09:27,771] A3C_AGENT_WORKER-Thread-3 INFO:Local step 220000, global step 3513565: loss 0.4885
[2019-04-04 10:09:27,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 220000, global step 3513567: learning rate 0.0000
[2019-04-04 10:09:27,818] A3C_AGENT_WORKER-Thread-14 INFO:Local step 219000, global step 3513589: loss 7.8195
[2019-04-04 10:09:27,819] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 219000, global step 3513589: learning rate 0.0000
[2019-04-04 10:09:27,872] A3C_AGENT_WORKER-Thread-6 INFO:Local step 220000, global step 3513619: loss 0.4834
[2019-04-04 10:09:27,873] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 220000, global step 3513619: learning rate 0.0000
[2019-04-04 10:09:28,294] A3C_AGENT_WORKER-Thread-10 INFO:Local step 219500, global step 3513815: loss 0.0003
[2019-04-04 10:09:28,295] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 219500, global step 3513815: learning rate 0.0000
[2019-04-04 10:09:29,083] A3C_AGENT_WORKER-Thread-20 INFO:Local step 218500, global step 3514224: loss 8.9743
[2019-04-04 10:09:29,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 218500, global step 3514225: learning rate 0.0000
[2019-04-04 10:09:29,770] A3C_AGENT_WORKER-Thread-4 INFO:Local step 220000, global step 3514564: loss 0.4869
[2019-04-04 10:09:29,770] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 220000, global step 3514564: learning rate 0.0000
[2019-04-04 10:09:30,928] A3C_AGENT_WORKER-Thread-17 INFO:Local step 219000, global step 3515092: loss 7.7394
[2019-04-04 10:09:30,929] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 219000, global step 3515093: learning rate 0.0000
[2019-04-04 10:09:31,045] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4826458e-13 9.0594069e-12 2.6638324e-32 9.3674355e-31 1.6476072e-24
 1.0000000e+00 2.3241367e-24], sum to 1.0000
[2019-04-04 10:09:31,065] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3048
[2019-04-04 10:09:31,083] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.83333333333333, 51.33333333333334, 0.0, 0.0, 26.0, 27.82353219763309, 1.002614030499858, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4389000.0000, 
sim time next is 4389600.0000, 
raw observation next is [11.66666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 27.72179996429167, 0.9823310803061531, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.785780240073869, 0.5266666666666667, 0.0, 0.0, 0.6666666666666666, 0.8101499970243058, 0.8274436934353844, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3365924], dtype=float32), -0.82034045]. 
=============================================
[2019-04-04 10:09:32,016] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1424217e-10 1.6077959e-09 7.8944830e-26 1.7721074e-24 5.6752351e-20
 1.0000000e+00 7.2714883e-19], sum to 1.0000
[2019-04-04 10:09:32,017] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0572
[2019-04-04 10:09:32,029] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.36818814896056, 0.3964003562589438, 0.0, 1.0, 41793.53916371293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3717600.0000, 
sim time next is 3718200.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.49731616549926, 0.4009323513425718, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6247763471249383, 0.6336441171141906, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40319702], dtype=float32), 0.8291555]. 
=============================================
[2019-04-04 10:09:32,067] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5776967e-12 1.2412187e-10 3.5574616e-29 4.2800380e-27 1.5410572e-22
 1.0000000e+00 3.5805922e-22], sum to 1.0000
[2019-04-04 10:09:32,073] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1514
[2019-04-04 10:09:32,086] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 89.0, 102.0, 0.0, 26.0, 26.30771808342054, 0.5828701471283939, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4451400.0000, 
sim time next is 4452000.0000, 
raw observation next is [0.3333333333333334, 90.0, 117.6666666666667, 0.9999999999999998, 26.0, 26.19004805470792, 0.5645050499531185, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4718374884579871, 0.9, 0.3922222222222223, 0.0011049723756906074, 0.6666666666666666, 0.6825040045589933, 0.6881683499843728, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.76234007], dtype=float32), 1.8579766]. 
=============================================
[2019-04-04 10:09:32,095] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[87.2049  ]
 [87.426735]
 [87.68372 ]
 [88.03678 ]
 [88.48611 ]], R is [[87.39230347]
 [87.51837921]
 [87.64319611]
 [87.76676178]
 [87.88909149]].
[2019-04-04 10:09:36,377] A3C_AGENT_WORKER-Thread-11 INFO:Local step 220000, global step 3517563: loss 0.5023
[2019-04-04 10:09:36,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 220000, global step 3517563: learning rate 0.0000
[2019-04-04 10:09:37,715] A3C_AGENT_WORKER-Thread-12 INFO:Local step 220000, global step 3518184: loss 0.7091
[2019-04-04 10:09:37,716] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 220000, global step 3518184: learning rate 0.0000
[2019-04-04 10:09:38,660] A3C_AGENT_WORKER-Thread-16 INFO:Local step 220500, global step 3518601: loss 0.0173
[2019-04-04 10:09:38,661] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 220500, global step 3518601: learning rate 0.0000
[2019-04-04 10:09:38,886] A3C_AGENT_WORKER-Thread-18 INFO:Local step 220000, global step 3518701: loss 0.7191
[2019-04-04 10:09:38,887] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 220000, global step 3518701: learning rate 0.0000
[2019-04-04 10:09:39,652] A3C_AGENT_WORKER-Thread-15 INFO:Local step 220500, global step 3519057: loss 0.0174
[2019-04-04 10:09:39,655] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 220500, global step 3519058: learning rate 0.0000
[2019-04-04 10:09:40,444] A3C_AGENT_WORKER-Thread-19 INFO:Local step 220000, global step 3519411: loss 0.5136
[2019-04-04 10:09:40,458] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 220000, global step 3519411: learning rate 0.0000
[2019-04-04 10:09:41,302] A3C_AGENT_WORKER-Thread-13 INFO:Local step 220000, global step 3519781: loss 0.5129
[2019-04-04 10:09:41,311] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 220000, global step 3519781: learning rate 0.0000
[2019-04-04 10:09:42,098] A3C_AGENT_WORKER-Thread-2 INFO:Local step 220500, global step 3520140: loss 0.0163
[2019-04-04 10:09:42,099] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 220500, global step 3520140: learning rate 0.0000
[2019-04-04 10:09:44,524] A3C_AGENT_WORKER-Thread-10 INFO:Local step 220000, global step 3521276: loss 0.5088
[2019-04-04 10:09:44,524] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 220000, global step 3521276: learning rate 0.0000
[2019-04-04 10:09:45,008] A3C_AGENT_WORKER-Thread-3 INFO:Local step 220500, global step 3521504: loss 0.0189
[2019-04-04 10:09:45,020] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 220500, global step 3521508: learning rate 0.0000
[2019-04-04 10:09:45,378] A3C_AGENT_WORKER-Thread-5 INFO:Local step 219500, global step 3521659: loss 0.0002
[2019-04-04 10:09:45,380] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 219500, global step 3521659: learning rate 0.0000
[2019-04-04 10:09:45,480] A3C_AGENT_WORKER-Thread-6 INFO:Local step 220500, global step 3521706: loss 0.0172
[2019-04-04 10:09:45,481] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 220500, global step 3521706: learning rate 0.0000
[2019-04-04 10:09:46,083] A3C_AGENT_WORKER-Thread-14 INFO:Local step 219500, global step 3521970: loss 0.0003
[2019-04-04 10:09:46,096] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 219500, global step 3521975: learning rate 0.0000
[2019-04-04 10:09:46,530] A3C_AGENT_WORKER-Thread-20 INFO:Local step 219000, global step 3522179: loss 7.8723
[2019-04-04 10:09:46,531] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 219000, global step 3522179: learning rate 0.0000
[2019-04-04 10:09:47,243] A3C_AGENT_WORKER-Thread-4 INFO:Local step 220500, global step 3522510: loss 0.0209
[2019-04-04 10:09:47,247] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 220500, global step 3522512: learning rate 0.0000
[2019-04-04 10:09:49,241] A3C_AGENT_WORKER-Thread-17 INFO:Local step 219500, global step 3523432: loss 0.0003
[2019-04-04 10:09:49,264] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 219500, global step 3523441: learning rate 0.0000
[2019-04-04 10:09:53,095] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:09:53,095] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:09:53,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run27
[2019-04-04 10:09:53,953] A3C_AGENT_WORKER-Thread-11 INFO:Local step 220500, global step 3525622: loss 0.0196
[2019-04-04 10:09:53,954] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 220500, global step 3525622: learning rate 0.0000
[2019-04-04 10:09:53,994] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:09:53,995] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:09:54,006] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run27
[2019-04-04 10:09:55,553] A3C_AGENT_WORKER-Thread-12 INFO:Local step 220500, global step 3526235: loss 0.0178
[2019-04-04 10:09:55,562] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 220500, global step 3526235: learning rate 0.0000
[2019-04-04 10:09:56,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:09:56,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:09:56,231] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run27
[2019-04-04 10:09:56,395] A3C_AGENT_WORKER-Thread-18 INFO:Local step 220500, global step 3526556: loss 0.0182
[2019-04-04 10:09:56,396] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 220500, global step 3526556: learning rate 0.0000
[2019-04-04 10:09:58,234] A3C_AGENT_WORKER-Thread-19 INFO:Local step 220500, global step 3527169: loss 0.0170
[2019-04-04 10:09:58,235] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 220500, global step 3527169: learning rate 0.0000
[2019-04-04 10:09:58,853] A3C_AGENT_WORKER-Thread-13 INFO:Local step 220500, global step 3527397: loss 0.0179
[2019-04-04 10:09:58,856] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 220500, global step 3527397: learning rate 0.0000
[2019-04-04 10:09:59,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:09:59,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:09:59,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run27
[2019-04-04 10:10:00,246] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:10:00,246] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:10:00,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run27
[2019-04-04 10:10:01,905] A3C_AGENT_WORKER-Thread-5 INFO:Local step 220000, global step 3528371: loss 0.4987
[2019-04-04 10:10:01,907] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 220000, global step 3528371: learning rate 0.0000
[2019-04-04 10:10:02,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:10:02,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:10:02,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run27
[2019-04-04 10:10:03,136] A3C_AGENT_WORKER-Thread-14 INFO:Local step 220000, global step 3528732: loss 0.4965
[2019-04-04 10:10:03,147] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 220000, global step 3528732: learning rate 0.0000
[2019-04-04 10:10:03,175] A3C_AGENT_WORKER-Thread-10 INFO:Local step 220500, global step 3528749: loss 0.0172
[2019-04-04 10:10:03,175] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 220500, global step 3528749: learning rate 0.0000
[2019-04-04 10:10:05,171] A3C_AGENT_WORKER-Thread-20 INFO:Local step 219500, global step 3529304: loss 0.0003
[2019-04-04 10:10:05,172] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 219500, global step 3529304: learning rate 0.0000
[2019-04-04 10:10:05,815] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3533173e-10 2.5729185e-09 1.0568811e-27 2.3703005e-25 2.4071560e-20
 1.0000000e+00 1.5300705e-19], sum to 1.0000
[2019-04-04 10:10:05,815] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0473
[2019-04-04 10:10:05,874] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.066666666666667, 88.0, 0.0, 0.0, 26.0, 24.68501333288506, 0.2396993119939093, 0.0, 1.0, 53056.50876382782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 70800.0000, 
sim time next is 71400.0000, 
raw observation next is [2.883333333333334, 88.5, 0.0, 0.0, 26.0, 24.73796006764269, 0.2418941292324946, 0.0, 1.0, 44856.57227632184], 
processed observation next is [0.0, 0.8260869565217391, 0.5424746075715605, 0.885, 0.0, 0.0, 0.6666666666666666, 0.5614966723035574, 0.5806313764108315, 0.0, 1.0, 0.2136027251253421], 
reward next is 0.7864, 
noisyNet noise sample is [array([-1.5558162], dtype=float32), -0.26177698]. 
=============================================
[2019-04-04 10:10:06,863] A3C_AGENT_WORKER-Thread-17 INFO:Local step 220000, global step 3529877: loss 0.4821
[2019-04-04 10:10:06,879] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 220000, global step 3529878: learning rate 0.0000
[2019-04-04 10:10:08,752] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3266321e-09 2.3811509e-08 2.3512191e-24 5.1662289e-23 9.8731182e-19
 1.0000000e+00 2.9332786e-18], sum to 1.0000
[2019-04-04 10:10:08,753] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6944
[2019-04-04 10:10:08,774] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.9, 72.66666666666667, 0.0, 0.0, 26.0, 23.29341931938578, -0.08432109880622625, 0.0, 1.0, 45444.31828477838], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 109200.0000, 
sim time next is 109800.0000, 
raw observation next is [-7.0, 71.5, 0.0, 0.0, 26.0, 23.23476794148569, -0.0905194975935499, 0.0, 1.0, 45591.59474631147], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.715, 0.0, 0.0, 0.6666666666666666, 0.4362306617904741, 0.4698268341354834, 0.0, 1.0, 0.21710283212529272], 
reward next is 0.7829, 
noisyNet noise sample is [array([-0.8710124], dtype=float32), 0.22865629]. 
=============================================
[2019-04-04 10:10:09,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:10:09,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:10:09,534] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run27
[2019-04-04 10:10:11,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:10:11,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:10:11,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run27
[2019-04-04 10:10:12,212] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:10:12,212] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:10:12,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run27
[2019-04-04 10:10:14,008] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:10:14,008] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:10:14,014] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run27
[2019-04-04 10:10:14,116] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:10:14,118] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:10:14,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run27
[2019-04-04 10:10:16,028] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7045146e-13 3.2995207e-11 2.5905565e-32 1.5175708e-29 2.4122252e-24
 1.0000000e+00 5.2973615e-24], sum to 1.0000
[2019-04-04 10:10:16,028] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2318
[2019-04-04 10:10:16,062] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.666666666666668, 23.33333333333334, 119.0, 860.8333333333334, 26.0, 27.81099519370996, 0.9572360185003072, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5059200.0000, 
sim time next is 5059800.0000, 
raw observation next is [10.0, 22.5, 118.0, 860.0, 26.0, 27.93756642412049, 0.8539347068771689, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.739612188365651, 0.225, 0.3933333333333333, 0.9502762430939227, 0.6666666666666666, 0.8281305353433742, 0.7846449022923897, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5753214], dtype=float32), 1.493477]. 
=============================================
[2019-04-04 10:10:19,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:10:19,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:10:19,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run27
[2019-04-04 10:10:20,869] A3C_AGENT_WORKER-Thread-5 INFO:Local step 220500, global step 3534003: loss 0.0180
[2019-04-04 10:10:20,870] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 220500, global step 3534003: learning rate 0.0000
[2019-04-04 10:10:23,554] A3C_AGENT_WORKER-Thread-14 INFO:Local step 220500, global step 3534714: loss 0.0205
[2019-04-04 10:10:23,555] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 220500, global step 3534714: learning rate 0.0000
[2019-04-04 10:10:23,850] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8316342e-09 2.9164680e-08 9.4255417e-23 1.3145966e-21 5.5953805e-18
 1.0000000e+00 1.8624448e-17], sum to 1.0000
[2019-04-04 10:10:23,851] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0297
[2019-04-04 10:10:23,867] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.98720661296953, -0.1857536209753457, 0.0, 1.0, 44403.45852623034], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 189000.0000, 
sim time next is 189600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.92311389050367, -0.1939472792808723, 0.0, 1.0, 44493.50666788389], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.41025949087530594, 0.4353509069063759, 0.0, 1.0, 0.21187384127563758], 
reward next is 0.7881, 
noisyNet noise sample is [array([0.47965768], dtype=float32), -0.6977153]. 
=============================================
[2019-04-04 10:10:24,013] A3C_AGENT_WORKER-Thread-20 INFO:Local step 220000, global step 3534878: loss 0.4952
[2019-04-04 10:10:24,014] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 220000, global step 3534878: learning rate 0.0000
[2019-04-04 10:10:27,191] A3C_AGENT_WORKER-Thread-17 INFO:Local step 220500, global step 3535809: loss 0.0181
[2019-04-04 10:10:27,192] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 220500, global step 3535809: learning rate 0.0000
[2019-04-04 10:10:27,352] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.7470678e-11 2.7782447e-09 1.4565381e-26 1.7234792e-24 6.4158875e-20
 1.0000000e+00 6.3240165e-19], sum to 1.0000
[2019-04-04 10:10:27,355] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9965
[2019-04-04 10:10:27,425] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 71.83333333333334, 93.33333333333333, 12.0, 26.0, 25.35739618364329, 0.2593612016832074, 1.0, 1.0, 38273.78716986112], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 121800.0000, 
sim time next is 122400.0000, 
raw observation next is [-7.8, 74.0, 117.5, 18.0, 26.0, 25.32323701989976, 0.26033048686473, 1.0, 1.0, 41012.64107075182], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.74, 0.39166666666666666, 0.019889502762430938, 0.6666666666666666, 0.6102697516583134, 0.58677682895491, 1.0, 1.0, 0.19529829081310393], 
reward next is 0.8047, 
noisyNet noise sample is [array([0.41817948], dtype=float32), -1.1312383]. 
=============================================
[2019-04-04 10:10:27,781] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4526015e-09 7.0423751e-09 2.1838575e-24 1.0424570e-22 7.5282261e-19
 1.0000000e+00 1.1408541e-18], sum to 1.0000
[2019-04-04 10:10:27,781] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6002
[2019-04-04 10:10:27,802] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.00638756047659, 0.03476890688988955, 0.0, 1.0, 45501.28542248489], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 266400.0000, 
sim time next is 267000.0000, 
raw observation next is [-7.566666666666666, 70.33333333333334, 0.0, 0.0, 26.0, 23.95405683582521, 0.02305164515454614, 0.0, 1.0, 45548.45025268466], 
processed observation next is [1.0, 0.08695652173913043, 0.2530009233610342, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.4961714029854341, 0.507683881718182, 0.0, 1.0, 0.21689738215564125], 
reward next is 0.7831, 
noisyNet noise sample is [array([-0.34932253], dtype=float32), 1.0303044]. 
=============================================
[2019-04-04 10:10:27,870] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.26167 ]
 [74.49725 ]
 [74.72253 ]
 [74.932755]
 [75.11968 ]], R is [[74.14861298]
 [74.19045258]
 [74.23204803]
 [74.27347565]
 [74.31489563]].
[2019-04-04 10:10:31,543] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2832951e-09 1.0511782e-07 3.0779721e-23 1.1785315e-21 1.9184155e-17
 9.9999988e-01 6.2199714e-17], sum to 1.0000
[2019-04-04 10:10:31,543] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2678
[2019-04-04 10:10:31,557] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.3, 69.0, 0.0, 0.0, 26.0, 23.47571074947326, -0.06727419285243265, 0.0, 1.0, 46220.37176590045], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 272400.0000, 
sim time next is 273000.0000, 
raw observation next is [-9.4, 69.5, 0.0, 0.0, 26.0, 23.4673958655684, -0.07784223821738571, 0.0, 1.0, 46359.41989431022], 
processed observation next is [1.0, 0.13043478260869565, 0.20221606648199447, 0.695, 0.0, 0.0, 0.6666666666666666, 0.4556163221306999, 0.4740525872608714, 0.0, 1.0, 0.22075914235385818], 
reward next is 0.7792, 
noisyNet noise sample is [array([0.08111525], dtype=float32), 0.48525804]. 
=============================================
[2019-04-04 10:10:31,564] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[71.16327 ]
 [71.44042 ]
 [71.74397 ]
 [72.068794]
 [72.35944 ]], R is [[70.97678375]
 [71.04691315]
 [71.11695099]
 [71.18682098]
 [71.25655365]].
[2019-04-04 10:10:36,522] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.7172288e-12 1.3117811e-10 9.2662960e-29 1.3384613e-27 1.2081884e-21
 1.0000000e+00 8.2631759e-22], sum to 1.0000
[2019-04-04 10:10:36,522] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9845
[2019-04-04 10:10:36,568] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.833333333333334, 19.0, 0.0, 0.0, 26.0, 27.40956879010525, 0.9148982539520637, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5083800.0000, 
sim time next is 5084400.0000, 
raw observation next is [9.666666666666668, 19.0, 0.0, 0.0, 26.0, 27.33725726528861, 0.9009888309001889, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7303785780240075, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7781047721073842, 0.800329610300063, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.911056], dtype=float32), 0.722211]. 
=============================================
[2019-04-04 10:10:38,142] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:10:38,146] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:10:38,156] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run27
[2019-04-04 10:10:40,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:10:40,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:10:40,588] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run27
[2019-04-04 10:10:44,295] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:10:44,295] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:10:44,319] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run27
[2019-04-04 10:10:45,248] A3C_AGENT_WORKER-Thread-20 INFO:Local step 220500, global step 3540880: loss 0.0193
[2019-04-04 10:10:45,255] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 220500, global step 3540880: learning rate 0.0000
[2019-04-04 10:10:48,723] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.6763327e-11 3.8180872e-09 1.8586595e-27 2.6006057e-25 7.2297161e-21
 1.0000000e+00 9.9883513e-20], sum to 1.0000
[2019-04-04 10:10:48,725] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0411
[2019-04-04 10:10:48,773] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.57954844357544, 0.2954994554805543, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 236400.0000, 
sim time next is 237000.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.34561492296997, 0.2851334554443422, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6121345769141641, 0.5950444851481141, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11609737], dtype=float32), 0.6336887]. 
=============================================
[2019-04-04 10:10:48,780] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[80.957306]
 [81.029434]
 [81.078964]
 [81.124146]
 [80.17438 ]], R is [[81.05922699]
 [81.24863434]
 [81.4361496 ]
 [81.62178802]
 [80.85436249]].
[2019-04-04 10:11:03,091] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:11:03,091] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:11:03,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run27
[2019-04-04 10:11:11,896] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.18386786e-10 3.58905350e-09 7.38890203e-26 1.41595503e-24
 6.59007104e-20 1.00000000e+00 1.00448645e-19], sum to 1.0000
[2019-04-04 10:11:11,896] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0332
[2019-04-04 10:11:11,954] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.2, 59.0, 0.0, 0.0, 26.0, 25.0106884611921, 0.3235896516323959, 0.0, 1.0, 77174.59066099752], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 764400.0000, 
sim time next is 765000.0000, 
raw observation next is [-5.3, 59.5, 0.0, 0.0, 26.0, 25.02822150046961, 0.3207075039618367, 0.0, 1.0, 57295.63407129903], 
processed observation next is [1.0, 0.8695652173913043, 0.31578947368421056, 0.595, 0.0, 0.0, 0.6666666666666666, 0.5856851250391341, 0.6069025013206123, 0.0, 1.0, 0.2728363527204716], 
reward next is 0.7272, 
noisyNet noise sample is [array([0.78116286], dtype=float32), 0.5724791]. 
=============================================
[2019-04-04 10:11:11,962] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[81.09354 ]
 [82.94759 ]
 [82.927345]
 [82.56936 ]
 [82.22517 ]], R is [[79.20437622]
 [79.04483032]
 [78.94947052]
 [78.63000488]
 [78.3825531 ]].
[2019-04-04 10:11:12,763] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7518108e-11 1.1130717e-09 3.0848881e-28 1.7464770e-25 1.0605353e-21
 1.0000000e+00 1.7277937e-21], sum to 1.0000
[2019-04-04 10:11:12,781] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9918
[2019-04-04 10:11:12,796] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.216666666666667, 87.66666666666667, 0.0, 0.0, 26.0, 24.89656417540523, 0.239911015293525, 0.0, 1.0, 39653.62893323804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 526200.0000, 
sim time next is 526800.0000, 
raw observation next is [4.133333333333333, 87.33333333333334, 0.0, 0.0, 26.0, 24.87004101045364, 0.2364615018867271, 0.0, 1.0, 39681.22722427696], 
processed observation next is [0.0, 0.08695652173913043, 0.577100646352724, 0.8733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5725034175378033, 0.578820500628909, 0.0, 1.0, 0.18895822487750935], 
reward next is 0.8110, 
noisyNet noise sample is [array([0.95066065], dtype=float32), 0.708695]. 
=============================================
[2019-04-04 10:11:19,151] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2245773e-10 3.9334824e-09 1.7466903e-25 7.7042349e-24 3.5356288e-20
 1.0000000e+00 2.2623674e-19], sum to 1.0000
[2019-04-04 10:11:19,154] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5089
[2019-04-04 10:11:19,168] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.566666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 23.56137770127332, -0.03120344680623312, 0.0, 1.0, 44362.30475851987], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 105600.0000, 
sim time next is 106200.0000, 
raw observation next is [-5.85, 74.5, 0.0, 0.0, 26.0, 23.48068237372269, -0.03973754094427333, 0.0, 1.0, 44523.55260481204], 
processed observation next is [1.0, 0.21739130434782608, 0.30055401662049863, 0.745, 0.0, 0.0, 0.6666666666666666, 0.4567235311435575, 0.48675415301857555, 0.0, 1.0, 0.21201691716577162], 
reward next is 0.7880, 
noisyNet noise sample is [array([1.6752646], dtype=float32), -0.63881665]. 
=============================================
[2019-04-04 10:11:25,215] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8670338e-11 1.8203421e-10 2.0946703e-28 4.7258583e-26 1.8490958e-22
 1.0000000e+00 2.9205346e-21], sum to 1.0000
[2019-04-04 10:11:25,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4249
[2019-04-04 10:11:25,291] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 84.66666666666667, 50.66666666666666, 0.0, 26.0, 24.92793367301951, 0.3439365475422023, 1.0, 1.0, 193179.0757814735], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 832800.0000, 
sim time next is 833400.0000, 
raw observation next is [-3.9, 84.0, 49.0, 0.0, 26.0, 25.42519222329284, 0.3791694564053052, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.84, 0.16333333333333333, 0.0, 0.6666666666666666, 0.6187660186077366, 0.6263898188017684, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.50361836], dtype=float32), 1.2762198]. 
=============================================
[2019-04-04 10:11:59,961] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3439768e-12 1.0668530e-10 1.1588084e-30 2.1669740e-28 5.8409182e-24
 1.0000000e+00 9.4004061e-23], sum to 1.0000
[2019-04-04 10:11:59,961] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6270
[2019-04-04 10:11:59,970] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.9, 99.33333333333334, 99.0, 0.0, 26.0, 24.95041830661247, 0.4701366504162479, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1255800.0000, 
sim time next is 1256400.0000, 
raw observation next is [13.8, 100.0, 98.0, 0.0, 26.0, 24.91864395510834, 0.4657085209867924, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.32666666666666666, 0.0, 0.6666666666666666, 0.576553662925695, 0.6552361736622642, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.90236115], dtype=float32), 0.187832]. 
=============================================
[2019-04-04 10:12:01,765] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3511942e-11 1.1196598e-09 1.3293175e-27 1.3307338e-26 9.1415837e-22
 1.0000000e+00 3.1335153e-21], sum to 1.0000
[2019-04-04 10:12:01,767] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6874
[2019-04-04 10:12:01,774] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.46666666666667, 64.33333333333334, 155.0, 0.0, 26.0, 25.17735943603493, 0.5083819048259243, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1164000.0000, 
sim time next is 1164600.0000, 
raw observation next is [18.55, 64.0, 160.0, 0.0, 26.0, 25.15130936033887, 0.5062050993795273, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.976454293628809, 0.64, 0.5333333333333333, 0.0, 0.6666666666666666, 0.5959424466949059, 0.6687350331265091, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.040491], dtype=float32), 0.7320468]. 
=============================================
[2019-04-04 10:12:02,027] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2507291e-12 5.6374821e-10 9.1515948e-30 9.2620947e-28 9.9471567e-23
 1.0000000e+00 2.7689472e-22], sum to 1.0000
[2019-04-04 10:12:02,027] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6571
[2019-04-04 10:12:02,076] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.333333333333334, 80.0, 0.0, 0.0, 26.0, 25.44027000114641, 0.4990347353612755, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1581600.0000, 
sim time next is 1582200.0000, 
raw observation next is [5.25, 80.5, 0.0, 0.0, 26.0, 25.51871447192914, 0.49514197046501, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.60803324099723, 0.805, 0.0, 0.0, 0.6666666666666666, 0.6265595393274284, 0.6650473234883366, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0842404], dtype=float32), -1.9750946]. 
=============================================
[2019-04-04 10:12:03,457] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.68052808e-12 5.22154098e-10 1.04970905e-27 1.03828036e-25
 1.64490933e-21 1.00000000e+00 9.27931534e-21], sum to 1.0000
[2019-04-04 10:12:03,458] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7064
[2019-04-04 10:12:03,518] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 73.66666666666667, 100.8333333333333, 0.0, 26.0, 25.58438049535899, 0.3029497696853317, 1.0, 1.0, 27700.45550589011], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 822000.0000, 
sim time next is 822600.0000, 
raw observation next is [-4.5, 75.0, 99.0, 0.0, 26.0, 25.51651485372726, 0.292360921801498, 1.0, 1.0, 27797.11344947165], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.75, 0.33, 0.0, 0.6666666666666666, 0.6263762378106049, 0.5974536406004993, 1.0, 1.0, 0.13236720690224596], 
reward next is 0.8676, 
noisyNet noise sample is [array([0.65735686], dtype=float32), -1.6912316]. 
=============================================
[2019-04-04 10:12:07,249] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.8013604e-11 4.5148354e-10 5.9786180e-29 4.0508524e-27 5.2171489e-22
 1.0000000e+00 8.5106973e-21], sum to 1.0000
[2019-04-04 10:12:07,250] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1122
[2019-04-04 10:12:07,266] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.95, 98.0, 0.0, 0.0, 26.0, 25.45623197853476, 0.5845008186386841, 0.0, 1.0, 41039.18660768709], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1294200.0000, 
sim time next is 1294800.0000, 
raw observation next is [4.766666666666667, 97.33333333333334, 0.0, 0.0, 26.0, 25.44248940789268, 0.5830872851743819, 0.0, 1.0, 45420.90218797008], 
processed observation next is [0.0, 1.0, 0.5946445060018468, 0.9733333333333334, 0.0, 0.0, 0.6666666666666666, 0.6202074506577233, 0.6943624283914606, 0.0, 1.0, 0.21629001041890514], 
reward next is 0.7837, 
noisyNet noise sample is [array([1.1530242], dtype=float32), 1.3253218]. 
=============================================
[2019-04-04 10:12:09,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.8847859e-13 1.2237457e-10 5.0364001e-30 8.2825231e-29 5.1201228e-23
 1.0000000e+00 3.9673607e-23], sum to 1.0000
[2019-04-04 10:12:09,923] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1471
[2019-04-04 10:12:09,958] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 86.0, 107.0, 0.0, 26.0, 25.72850842898086, 0.4836082315428227, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1686600.0000, 
sim time next is 1687200.0000, 
raw observation next is [1.1, 86.66666666666667, 105.8333333333333, 0.0, 26.0, 25.5962663802712, 0.3685088842205732, 1.0, 1.0, 94340.80365552373], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.8666666666666667, 0.3527777777777777, 0.0, 0.6666666666666666, 0.6330221983559333, 0.6228362947401911, 1.0, 1.0, 0.44924192216916065], 
reward next is 0.5508, 
noisyNet noise sample is [array([1.196634], dtype=float32), -0.6716317]. 
=============================================
[2019-04-04 10:12:12,846] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0340092e-10 2.6356723e-09 2.2158610e-26 3.5863007e-25 2.5954391e-20
 1.0000000e+00 1.6212068e-20], sum to 1.0000
[2019-04-04 10:12:12,848] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4055
[2019-04-04 10:12:12,908] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.383333333333333, 83.0, 123.6666666666667, 0.0, 26.0, 24.94195376157225, 0.345634631688796, 0.0, 1.0, 45480.91563623573], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1771800.0000, 
sim time next is 1772400.0000, 
raw observation next is [-2.466666666666667, 83.0, 124.8333333333333, 0.0, 26.0, 24.94110343126335, 0.3471836890907887, 0.0, 1.0, 46867.40142435212], 
processed observation next is [0.0, 0.5217391304347826, 0.39427516158818104, 0.83, 0.416111111111111, 0.0, 0.6666666666666666, 0.5784252859386125, 0.6157278963635963, 0.0, 1.0, 0.22317810202072438], 
reward next is 0.7768, 
noisyNet noise sample is [array([0.12377301], dtype=float32), 0.21320143]. 
=============================================
[2019-04-04 10:12:15,744] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.4641948e-13 8.5561142e-12 1.3977351e-32 4.3304339e-30 6.3777802e-25
 1.0000000e+00 6.4136234e-24], sum to 1.0000
[2019-04-04 10:12:15,745] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2102
[2019-04-04 10:12:15,749] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.5, 77.0, 20.83333333333334, 0.0, 26.0, 26.45133463895038, 0.6776089465763605, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1010400.0000, 
sim time next is 1011000.0000, 
raw observation next is [15.5, 77.5, 16.66666666666667, 0.0, 26.0, 26.66829882773401, 0.6985185669245633, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.775, 0.05555555555555557, 0.0, 0.6666666666666666, 0.7223582356445007, 0.7328395223081877, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07276854], dtype=float32), 0.7551371]. 
=============================================
[2019-04-04 10:12:15,751] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[98.48657]
 [98.46354]
 [98.58003]
 [98.77681]
 [98.93306]], R is [[98.53890228]
 [98.55351257]
 [98.56797791]
 [98.58229828]
 [98.59647369]].
[2019-04-04 10:12:17,496] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.1270342e-11 3.3238807e-09 2.7387490e-27 1.1407940e-24 7.4849260e-20
 1.0000000e+00 2.4936693e-19], sum to 1.0000
[2019-04-04 10:12:17,500] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8569
[2019-04-04 10:12:17,530] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.366666666666667, 87.0, 0.0, 0.0, 26.0, 24.85728993861055, 0.3330268272928063, 0.0, 1.0, 43916.59201856366], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1750800.0000, 
sim time next is 1751400.0000, 
raw observation next is [-1.45, 87.0, 0.0, 0.0, 26.0, 24.83080955476942, 0.3274670580144173, 0.0, 1.0, 43955.36259813925], 
processed observation next is [0.0, 0.2608695652173913, 0.422437673130194, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5692341295641183, 0.6091556860048057, 0.0, 1.0, 0.20931125046732973], 
reward next is 0.7907, 
noisyNet noise sample is [array([-0.19684018], dtype=float32), -0.39738068]. 
=============================================
[2019-04-04 10:12:20,279] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2544720e-09 6.8029067e-09 1.3411466e-24 2.8156099e-23 2.1397370e-19
 1.0000000e+00 3.7536815e-18], sum to 1.0000
[2019-04-04 10:12:20,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4991
[2019-04-04 10:12:20,288] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.51666666666667, 75.5, 0.0, 0.0, 26.0, 24.30238008143382, 0.3139660911385096, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1206600.0000, 
sim time next is 1207200.0000, 
raw observation next is [16.43333333333334, 76.0, 0.0, 0.0, 26.0, 24.28095059938482, 0.3117135312712474, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 1.0, 0.9178208679593725, 0.76, 0.0, 0.0, 0.6666666666666666, 0.523412549948735, 0.6039045104237492, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11822379], dtype=float32), 0.61829346]. 
=============================================
[2019-04-04 10:12:20,640] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.9173690e-12 1.7440670e-10 1.0468962e-30 1.4615124e-28 7.3440607e-24
 1.0000000e+00 6.7476562e-22], sum to 1.0000
[2019-04-04 10:12:20,644] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3899
[2019-04-04 10:12:20,658] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 100.0, 92.0, 0.0, 26.0, 24.807981013721, 0.4533110297562424, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1258800.0000, 
sim time next is 1259400.0000, 
raw observation next is [13.8, 100.0, 89.0, 0.0, 26.0, 24.79224152592196, 0.4509893656227959, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.2966666666666667, 0.0, 0.6666666666666666, 0.5660201271601633, 0.6503297885409319, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03156743], dtype=float32), 1.2742858]. 
=============================================
[2019-04-04 10:12:22,407] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3558742e-09 4.9133702e-09 5.7704667e-25 3.0956000e-23 1.8676151e-19
 1.0000000e+00 2.0531241e-18], sum to 1.0000
[2019-04-04 10:12:22,424] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9082
[2019-04-04 10:12:22,454] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 82.66666666666667, 0.0, 0.0, 26.0, 24.75033798994303, 0.2388590852016356, 0.0, 1.0, 45643.30430147342], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1810200.0000, 
sim time next is 1810800.0000, 
raw observation next is [-5.0, 82.0, 0.0, 0.0, 26.0, 24.71751718325553, 0.2319264872696371, 0.0, 1.0, 45585.15786900414], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5597930986046276, 0.577308829089879, 0.0, 1.0, 0.21707218032859116], 
reward next is 0.7829, 
noisyNet noise sample is [array([0.21491969], dtype=float32), -1.0032622]. 
=============================================
[2019-04-04 10:12:29,924] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2325956e-12 2.1865239e-10 9.3134897e-31 1.1953479e-28 5.2933242e-23
 1.0000000e+00 1.6124655e-22], sum to 1.0000
[2019-04-04 10:12:29,925] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0212
[2019-04-04 10:12:30,014] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.00000000000001, 102.3333333333333, 0.0, 26.0, 24.87345196032695, 0.4863838599737074, 1.0, 1.0, 75315.98397253585], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1689000.0000, 
sim time next is 1689600.0000, 
raw observation next is [1.1, 88.0, 101.1666666666667, 0.0, 26.0, 25.36532656553873, 0.5594290858443645, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3372222222222223, 0.0, 0.6666666666666666, 0.6137772137948941, 0.6864763619481214, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3732617], dtype=float32), 1.583559]. 
=============================================
[2019-04-04 10:12:35,086] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2936164e-09 7.8917211e-09 8.9428615e-25 5.5425758e-23 6.5440306e-19
 1.0000000e+00 4.0013253e-18], sum to 1.0000
[2019-04-04 10:12:35,097] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0900
[2019-04-04 10:12:35,117] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 85.0, 0.0, 0.0, 26.0, 23.95956089371145, 0.06116845082144631, 0.0, 1.0, 46830.46698121445], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1827000.0000, 
sim time next is 1827600.0000, 
raw observation next is [-6.199999999999999, 84.33333333333333, 0.0, 0.0, 26.0, 23.93667880711924, 0.05164418088140885, 0.0, 1.0, 46850.60992292387], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.8433333333333333, 0.0, 0.0, 0.6666666666666666, 0.4947232339266033, 0.5172147269604696, 0.0, 1.0, 0.22309814249011364], 
reward next is 0.7769, 
noisyNet noise sample is [array([0.15227665], dtype=float32), -1.1126332]. 
=============================================
[2019-04-04 10:12:52,700] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3858104e-10 2.6711799e-09 1.6040451e-26 1.5336136e-24 1.5580113e-20
 1.0000000e+00 1.5545155e-19], sum to 1.0000
[2019-04-04 10:12:52,700] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1899
[2019-04-04 10:12:52,767] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.916666666666667, 77.66666666666667, 0.0, 0.0, 26.0, 25.36889601550757, 0.3383794901670777, 1.0, 1.0, 31225.57614864849], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1966200.0000, 
sim time next is 1966800.0000, 
raw observation next is [-4.833333333333334, 76.33333333333334, 0.0, 0.0, 26.0, 25.31884517400547, 0.3106445996862082, 1.0, 1.0, 29428.88134483072], 
processed observation next is [1.0, 0.782608695652174, 0.32871652816251157, 0.7633333333333334, 0.0, 0.0, 0.6666666666666666, 0.6099037645004559, 0.6035481998954028, 1.0, 1.0, 0.1401375302134796], 
reward next is 0.8599, 
noisyNet noise sample is [array([0.3578101], dtype=float32), -0.033598654]. 
=============================================
[2019-04-04 10:12:58,636] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4150425e-09 1.6566764e-09 2.0904188e-25 7.2065757e-24 6.1783425e-20
 1.0000000e+00 4.3095456e-19], sum to 1.0000
[2019-04-04 10:12:58,639] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2552
[2019-04-04 10:12:58,664] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.75020727658442, 0.2614209294493259, 0.0, 1.0, 42669.79333486462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2073000.0000, 
sim time next is 2073600.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 26.0, 24.71242524942515, 0.2535448426576029, 0.0, 1.0, 42695.21773464028], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 0.6666666666666666, 0.559368770785429, 0.5845149475525343, 0.0, 1.0, 0.2033105606411442], 
reward next is 0.7967, 
noisyNet noise sample is [array([-0.8218036], dtype=float32), 0.32820928]. 
=============================================
[2019-04-04 10:13:07,374] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1849658e-09 2.5150158e-08 1.4298131e-23 3.0787961e-22 3.2304490e-18
 1.0000000e+00 2.3097649e-17], sum to 1.0000
[2019-04-04 10:13:07,464] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8064
[2019-04-04 10:13:07,501] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 77.0, 0.0, 0.0, 26.0, 23.86699548524596, 0.03272881416724058, 0.0, 1.0, 41939.80812001057], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2179800.0000, 
sim time next is 2180400.0000, 
raw observation next is [-6.199999999999999, 77.66666666666667, 0.0, 0.0, 26.0, 23.87992426328055, 0.02470334469524188, 0.0, 1.0, 41905.9602287776], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.4899936886067125, 0.5082344482317472, 0.0, 1.0, 0.1995521915656076], 
reward next is 0.8004, 
noisyNet noise sample is [array([0.01986524], dtype=float32), 0.47014627]. 
=============================================
[2019-04-04 10:13:17,057] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.04987725e-08 6.52992753e-08 3.13058440e-23 7.05943420e-22
 2.47217960e-18 9.99999881e-01 1.62306551e-17], sum to 1.0000
[2019-04-04 10:13:17,058] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5774
[2019-04-04 10:13:17,146] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.40948940203087, -0.09018449569682807, 0.0, 1.0, 43041.82098807226], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2271600.0000, 
sim time next is 2272200.0000, 
raw observation next is [-9.5, 91.00000000000001, 0.0, 0.0, 26.0, 23.32747807033447, -0.1014317316874055, 0.0, 1.0, 43019.49739951664], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.9100000000000001, 0.0, 0.0, 0.6666666666666666, 0.443956505861206, 0.4661894227708648, 0.0, 1.0, 0.20485474952150778], 
reward next is 0.7951, 
noisyNet noise sample is [array([0.5270927], dtype=float32), 0.12632647]. 
=============================================
[2019-04-04 10:13:20,414] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5447857e-11 4.4614190e-10 6.6974493e-28 1.2277827e-26 4.7067635e-21
 1.0000000e+00 1.4334688e-20], sum to 1.0000
[2019-04-04 10:13:20,415] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8181
[2019-04-04 10:13:20,485] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.333333333333333, 49.00000000000001, 227.8333333333333, 69.83333333333333, 26.0, 25.28085895697983, 0.2401469322942308, 1.0, 1.0, 7477.258160976296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2294400.0000, 
sim time next is 2295000.0000, 
raw observation next is [-1.15, 48.0, 221.0, 69.0, 26.0, 25.02726676135889, 0.3269678040444552, 1.0, 1.0, 119782.7184146446], 
processed observation next is [1.0, 0.5652173913043478, 0.4307479224376732, 0.48, 0.7366666666666667, 0.07624309392265194, 0.6666666666666666, 0.5856055634465743, 0.6089892680148185, 1.0, 1.0, 0.5703938972125933], 
reward next is 0.4296, 
noisyNet noise sample is [array([0.5007986], dtype=float32), -0.27324635]. 
=============================================
[2019-04-04 10:13:20,525] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[83.12085]
 [83.55593]
 [83.97516]
 [84.18186]
 [84.24854]], R is [[82.86156464]
 [82.99734497]
 [83.16737366]
 [83.24666595]
 [83.32516479]].
[2019-04-04 10:13:37,466] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0593771e-09 6.2702306e-08 1.2984174e-23 6.2767554e-22 6.0440735e-18
 9.9999988e-01 1.8124019e-17], sum to 1.0000
[2019-04-04 10:13:37,466] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2880
[2019-04-04 10:13:37,536] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 48.0, 0.0, 0.0, 26.0, 24.4006672738174, 0.1007959137186688, 0.0, 1.0, 43320.60136058913], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2422800.0000, 
sim time next is 2423400.0000, 
raw observation next is [-6.383333333333334, 48.83333333333333, 0.0, 0.0, 26.0, 24.34109444545035, 0.09033266471949557, 0.0, 1.0, 43385.70521556772], 
processed observation next is [0.0, 0.043478260869565216, 0.28578024007386893, 0.4883333333333333, 0.0, 0.0, 0.6666666666666666, 0.5284245371208623, 0.5301108882398319, 0.0, 1.0, 0.2065985962646082], 
reward next is 0.7934, 
noisyNet noise sample is [array([-0.6765737], dtype=float32), 0.2991171]. 
=============================================
[2019-04-04 10:13:49,365] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-04 10:13:49,377] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:13:49,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:49,379] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run37
[2019-04-04 10:13:49,472] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:13:49,476] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:13:49,477] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:49,477] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:49,480] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run37
[2019-04-04 10:13:49,524] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run37
[2019-04-04 10:14:32,103] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.26386726], dtype=float32), 0.3049513]
[2019-04-04 10:14:32,103] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.067848409333334, 88.96408070000001, 0.0, 0.0, 26.0, 24.65998598701533, 0.1780051747644048, 0.0, 1.0, 39941.39894786411]
[2019-04-04 10:14:32,104] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:14:32,105] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.8429698e-11 1.8148862e-09 9.5992181e-28 8.5390323e-26 3.4751736e-21
 1.0000000e+00 2.6357382e-20], sampled 0.47319203270892785
[2019-04-04 10:14:34,858] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.26386726], dtype=float32), 0.3049513]
[2019-04-04 10:14:34,858] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-10.75, 56.83333333333334, 151.0, 376.3333333333334, 26.0, 25.03953495997439, 0.1512177663601295, 0.0, 1.0, 0.0]
[2019-04-04 10:14:34,858] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:14:34,859] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.9892710e-09 2.6930355e-08 2.3776201e-23 8.1459670e-22 7.9279529e-18
 1.0000000e+00 3.2051154e-17], sampled 0.5535071775367848
[2019-04-04 10:15:00,051] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.26386726], dtype=float32), 0.3049513]
[2019-04-04 10:15:00,051] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.6, 81.0, 0.0, 0.0, 26.0, 25.36707832605758, 0.38125780751936, 0.0, 1.0, 45033.609884027]
[2019-04-04 10:15:00,051] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:15:00,052] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.7622576e-10 1.2757440e-09 4.8619778e-27 4.2782796e-25 1.3677212e-20
 1.0000000e+00 1.2227345e-19], sampled 0.10080352253497349
[2019-04-04 10:17:01,088] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.26386726], dtype=float32), 0.3049513]
[2019-04-04 10:17:01,088] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.866666666666667, 83.33333333333334, 0.0, 0.0, 26.0, 25.8257528856516, 0.5775491823791445, 1.0, 1.0, 0.0]
[2019-04-04 10:17:01,088] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:17:01,089] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.2981669e-12 6.0025193e-11 5.4852599e-30 6.0577777e-28 7.2335434e-23
 1.0000000e+00 4.4468304e-22], sampled 0.19304203957260513
[2019-04-04 10:17:03,312] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 10:17:37,439] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 10:17:40,527] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 10:17:41,581] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 3600000, evaluation results [3600000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 10:17:55,429] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.9950719e-10 2.3897309e-09 2.5947875e-26 3.5785129e-24 7.7853780e-20
 1.0000000e+00 1.2436776e-19], sum to 1.0000
[2019-04-04 10:17:55,435] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0812
[2019-04-04 10:17:55,450] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.833333333333333, 66.33333333333334, 0.0, 0.0, 26.0, 25.48282836991311, 0.4510419422037065, 0.0, 1.0, 18757.1657505403], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2668800.0000, 
sim time next is 2669400.0000, 
raw observation next is [-2.15, 67.0, 0.0, 0.0, 26.0, 25.46109691301113, 0.4427315418937534, 0.0, 1.0, 31339.54885363049], 
processed observation next is [1.0, 0.9130434782608695, 0.4030470914127424, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6217580760842608, 0.6475771806312511, 0.0, 1.0, 0.14923594692204994], 
reward next is 0.8508, 
noisyNet noise sample is [array([-1.1293377], dtype=float32), -0.20595463]. 
=============================================
[2019-04-04 10:18:24,109] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8090148e-10 4.4621360e-09 2.5675849e-24 3.4008762e-23 2.0802044e-18
 1.0000000e+00 2.1141479e-18], sum to 1.0000
[2019-04-04 10:18:24,109] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5469
[2019-04-04 10:18:24,178] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3, 77.0, 7.0, 88.0, 26.0, 25.05694536594121, 0.2955681006274938, 0.0, 1.0, 39042.32908633356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3087000.0000, 
sim time next is 3087600.0000, 
raw observation next is [-0.4, 78.66666666666667, 0.0, 0.0, 26.0, 25.0314977737832, 0.2849752542656641, 0.0, 1.0, 48055.70277741516], 
processed observation next is [0.0, 0.7391304347826086, 0.45152354570637127, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5859581478152668, 0.594991751421888, 0.0, 1.0, 0.22883667989245313], 
reward next is 0.7712, 
noisyNet noise sample is [array([-0.16432332], dtype=float32), -0.48559758]. 
=============================================
[2019-04-04 10:18:50,806] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1743769e-09 1.4141529e-08 3.0348556e-24 2.6547767e-22 1.7896195e-18
 1.0000000e+00 2.8009421e-17], sum to 1.0000
[2019-04-04 10:18:50,806] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2224
[2019-04-04 10:18:50,821] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.44715650073534, 0.217205481214319, 0.0, 1.0, 41119.29973461839], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3567000.0000, 
sim time next is 3567600.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.40501154832712, 0.2083362439663888, 0.0, 1.0, 41225.71831172188], 
processed observation next is [0.0, 0.30434782608695654, 0.296398891966759, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5337509623605934, 0.569445414655463, 0.0, 1.0, 0.19631294434153276], 
reward next is 0.8037, 
noisyNet noise sample is [array([0.6291559], dtype=float32), -0.21857788]. 
=============================================
[2019-04-04 10:19:08,542] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.84370639e-12 2.91724249e-11 1.73377405e-30 4.91176504e-29
 1.11017184e-23 1.00000000e+00 7.18607288e-23], sum to 1.0000
[2019-04-04 10:19:08,551] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3302
[2019-04-04 10:19:08,573] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 105.0, 702.5, 26.0, 26.32279072044449, 0.6766562969455455, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3232800.0000, 
sim time next is 3233400.0000, 
raw observation next is [-2.9, 90.0, 106.3333333333333, 719.0, 26.0, 26.36574407487655, 0.6879520423904107, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.38227146814404434, 0.9, 0.35444444444444434, 0.7944751381215469, 0.6666666666666666, 0.6971453395730457, 0.7293173474634703, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4316005], dtype=float32), -0.8787292]. 
=============================================
[2019-04-04 10:19:09,302] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6580402e-11 4.0605394e-11 9.3071377e-30 5.8984508e-28 2.1694619e-22
 1.0000000e+00 5.9372720e-23], sum to 1.0000
[2019-04-04 10:19:09,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4839
[2019-04-04 10:19:09,333] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 100.0, 106.0, 790.5, 26.0, 26.69074664831339, 0.8015230422764908, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3247200.0000, 
sim time next is 3247800.0000, 
raw observation next is [-3.666666666666667, 95.16666666666667, 104.3333333333333, 783.3333333333334, 26.0, 26.7566514353292, 0.812479271725648, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3610341643582641, 0.9516666666666667, 0.3477777777777777, 0.8655616942909761, 0.6666666666666666, 0.7297209529441, 0.7708264239085493, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09641741], dtype=float32), 0.93023765]. 
=============================================
[2019-04-04 10:19:16,517] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8569806e-10 2.4391520e-09 6.8391985e-26 7.0689132e-24 8.7071442e-20
 1.0000000e+00 1.0155912e-18], sum to 1.0000
[2019-04-04 10:19:16,529] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0872
[2019-04-04 10:19:16,550] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 67.0, 0.0, 0.0, 26.0, 25.38726562540651, 0.3898805739093169, 0.0, 1.0, 43632.86456612797], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3712800.0000, 
sim time next is 3713400.0000, 
raw observation next is [-3.0, 68.0, 0.0, 0.0, 26.0, 25.37682830552636, 0.3879252589611705, 0.0, 1.0, 47176.7220824535], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6147356921271966, 0.6293084196537235, 0.0, 1.0, 0.22465105753549286], 
reward next is 0.7753, 
noisyNet noise sample is [array([-0.4915744], dtype=float32), -1.4453806]. 
=============================================
[2019-04-04 10:19:17,059] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3414760e-10 3.3824488e-10 2.3904200e-27 9.4281934e-25 1.4433235e-20
 1.0000000e+00 2.9406474e-20], sum to 1.0000
[2019-04-04 10:19:17,059] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8063
[2019-04-04 10:19:17,078] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 54.5, 64.0, 522.0, 26.0, 25.49433936147977, 0.4675428400757573, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3688200.0000, 
sim time next is 3688800.0000, 
raw observation next is [4.333333333333334, 56.0, 55.83333333333334, 462.5, 26.0, 25.47945002848898, 0.4568034243897802, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.58264081255771, 0.56, 0.18611111111111114, 0.511049723756906, 0.6666666666666666, 0.6232875023740817, 0.6522678081299268, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3917818], dtype=float32), 1.4087269]. 
=============================================
[2019-04-04 10:19:24,168] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6274562e-12 3.9034653e-10 4.8173876e-29 4.7219457e-27 1.4721259e-22
 1.0000000e+00 4.8722761e-22], sum to 1.0000
[2019-04-04 10:19:24,169] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7427
[2019-04-04 10:19:24,206] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 74.0, 89.0, 429.0, 26.0, 25.33770370505429, 0.357648246379705, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3745800.0000, 
sim time next is 3746400.0000, 
raw observation next is [-4.0, 75.0, 90.83333333333334, 470.0, 26.0, 25.50312241587314, 0.3723044097792463, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.75, 0.3027777777777778, 0.5193370165745856, 0.6666666666666666, 0.6252602013227616, 0.6241014699264155, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0208371], dtype=float32), -0.29636407]. 
=============================================
[2019-04-04 10:19:25,563] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0675933e-11 2.0757975e-10 1.3266326e-27 5.2499599e-26 6.8766255e-21
 1.0000000e+00 5.6788704e-21], sum to 1.0000
[2019-04-04 10:19:25,570] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7786
[2019-04-04 10:19:25,585] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 67.0, 10.0, 100.8333333333333, 26.0, 25.73569984122184, 0.52995645168803, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3433200.0000, 
sim time next is 3433800.0000, 
raw observation next is [2.0, 67.0, 0.0, 0.0, 26.0, 25.71794982184244, 0.3730156912699203, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6431624851535366, 0.6243385637566401, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.76701736], dtype=float32), 0.6852955]. 
=============================================
[2019-04-04 10:19:28,661] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2143992e-11 3.3489517e-10 1.3502512e-27 5.9549322e-26 1.0486362e-20
 1.0000000e+00 3.2266878e-21], sum to 1.0000
[2019-04-04 10:19:28,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0869
[2019-04-04 10:19:28,693] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.166666666666667, 26.5, 110.6666666666667, 818.0, 26.0, 26.86703002556503, 0.7363013120690867, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4024200.0000, 
sim time next is 4024800.0000, 
raw observation next is [-3.0, 26.0, 109.0, 812.0, 26.0, 27.03802574200939, 0.7538693042481698, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.26, 0.36333333333333334, 0.8972375690607735, 0.6666666666666666, 0.7531688118341157, 0.7512897680827232, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08925108], dtype=float32), 0.3335075]. 
=============================================
[2019-04-04 10:19:42,315] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.3276739e-09 2.5842024e-08 4.8156542e-24 8.2970678e-22 2.2923497e-17
 1.0000000e+00 1.4003448e-17], sum to 1.0000
[2019-04-04 10:19:42,315] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4511
[2019-04-04 10:19:42,341] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.65, 41.16666666666667, 0.0, 0.0, 26.0, 25.03858589723593, 0.3198596027906837, 0.0, 1.0, 27629.58968782133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4212600.0000, 
sim time next is 4213200.0000, 
raw observation next is [1.6, 41.33333333333334, 0.0, 0.0, 26.0, 25.02407277992058, 0.317799157599879, 0.0, 1.0, 31417.72012522017], 
processed observation next is [0.0, 0.782608695652174, 0.5069252077562327, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.585339398326715, 0.605933052533293, 0.0, 1.0, 0.149608191072477], 
reward next is 0.8504, 
noisyNet noise sample is [array([0.64070237], dtype=float32), -0.035398155]. 
=============================================
[2019-04-04 10:19:42,472] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3411638e-10 3.8606993e-09 5.8849913e-27 2.0935471e-25 7.1088653e-21
 1.0000000e+00 5.0228010e-21], sum to 1.0000
[2019-04-04 10:19:42,490] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0171
[2019-04-04 10:19:42,518] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.45, 70.0, 0.0, 0.0, 26.0, 25.48846495493887, 0.3829942847119245, 0.0, 1.0, 18756.24919138314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4339800.0000, 
sim time next is 4340400.0000, 
raw observation next is [3.4, 70.33333333333333, 0.0, 0.0, 26.0, 25.48711044508437, 0.3743907440530638, 0.0, 1.0, 18753.78987440851], 
processed observation next is [1.0, 0.21739130434782608, 0.556786703601108, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.6239258704236974, 0.6247969146843546, 0.0, 1.0, 0.0893037613067072], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.7977484], dtype=float32), 0.019361978]. 
=============================================
[2019-04-04 10:19:42,860] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.8974443e-11 1.0584886e-09 4.6150761e-28 2.0683516e-25 1.8406946e-20
 1.0000000e+00 1.5005164e-19], sum to 1.0000
[2019-04-04 10:19:42,861] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3718
[2019-04-04 10:19:42,871] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.9, 75.0, 0.0, 0.0, 26.0, 25.58169613319064, 0.4090602464708974, 0.0, 1.0, 61895.59298231456], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4311600.0000, 
sim time next is 4312200.0000, 
raw observation next is [4.85, 75.5, 0.0, 0.0, 26.0, 25.53007549257372, 0.4107245606815211, 0.0, 1.0, 72558.4382326508], 
processed observation next is [0.0, 0.9130434782608695, 0.5969529085872576, 0.755, 0.0, 0.0, 0.6666666666666666, 0.6275062910478099, 0.6369081868938403, 0.0, 1.0, 0.3455163725364324], 
reward next is 0.6545, 
noisyNet noise sample is [array([-1.3472055], dtype=float32), 0.72302306]. 
=============================================
[2019-04-04 10:19:52,489] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.02475571e-11 4.56178595e-10 4.25416634e-30 1.19002075e-27
 1.73285541e-22 1.00000000e+00 4.99181588e-22], sum to 1.0000
[2019-04-04 10:19:52,490] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9328
[2019-04-04 10:19:52,519] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.533333333333333, 60.66666666666666, 0.0, 0.0, 26.0, 26.50097747705335, 0.7655430550749439, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4398600.0000, 
sim time next is 4399200.0000, 
raw observation next is [9.4, 61.0, 0.0, 0.0, 26.0, 26.44582523346865, 0.7546773998617672, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.7229916897506927, 0.61, 0.0, 0.0, 0.6666666666666666, 0.7038187694557209, 0.7515591332872558, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4369559], dtype=float32), 0.0046724346]. 
=============================================
[2019-04-04 10:19:54,927] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3754042e-09 9.2105843e-09 1.0585658e-23 1.1563138e-22 1.4604666e-18
 1.0000000e+00 7.2742106e-18], sum to 1.0000
[2019-04-04 10:19:54,929] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7819
[2019-04-04 10:19:55,002] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.33333333333333, 65.0, 15.5, 73.99999999999999, 26.0, 24.12768308345118, 0.1932730319634381, 1.0, 1.0, 152490.8549105871], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4002000.0000, 
sim time next is 4002600.0000, 
raw observation next is [-13.16666666666667, 64.0, 30.99999999999999, 148.0, 26.0, 24.70761087009443, 0.283735756354121, 1.0, 1.0, 104626.6001694993], 
processed observation next is [1.0, 0.30434782608695654, 0.09787626962142189, 0.64, 0.10333333333333329, 0.16353591160220995, 0.6666666666666666, 0.5589675725078692, 0.5945785854513737, 1.0, 1.0, 0.4982219055690443], 
reward next is 0.5018, 
noisyNet noise sample is [array([0.6727663], dtype=float32), -0.88613087]. 
=============================================
[2019-04-04 10:19:57,026] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.8682095e-09 2.5317361e-08 8.4918747e-24 2.2729013e-21 1.3542404e-17
 1.0000000e+00 5.9748242e-17], sum to 1.0000
[2019-04-04 10:19:57,029] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7187
[2019-04-04 10:19:57,046] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.31538781434478, 0.1675479142193624, 0.0, 1.0, 43761.52558020704], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3984000.0000, 
sim time next is 3984600.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.26272556757615, 0.1657580356239948, 0.0, 1.0, 43748.98248055012], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5218937972980126, 0.5552526785413315, 0.0, 1.0, 0.2083284880026196], 
reward next is 0.7917, 
noisyNet noise sample is [array([-0.31381586], dtype=float32), -1.4901495]. 
=============================================
[2019-04-04 10:19:57,683] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1458237e-11 8.5318086e-10 4.6347808e-29 6.0504137e-27 2.0290534e-22
 1.0000000e+00 1.8644881e-21], sum to 1.0000
[2019-04-04 10:19:57,689] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3766
[2019-04-04 10:19:57,701] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 67.0, 0.0, 0.0, 26.0, 25.81444853510607, 0.5534210818802856, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4420800.0000, 
sim time next is 4421400.0000, 
raw observation next is [4.383333333333333, 67.16666666666667, 0.0, 0.0, 26.0, 25.72466092013496, 0.5496699693168642, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5840258541089567, 0.6716666666666667, 0.0, 0.0, 0.6666666666666666, 0.6437217433445799, 0.6832233231056214, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9445011], dtype=float32), 0.36295104]. 
=============================================
[2019-04-04 10:19:58,460] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.1889148e-11 5.1610827e-10 4.5661338e-27 4.1785676e-25 1.6478619e-20
 1.0000000e+00 1.2924549e-20], sum to 1.0000
[2019-04-04 10:19:58,460] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6519
[2019-04-04 10:19:58,508] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.333333333333333, 80.33333333333334, 0.0, 0.0, 26.0, 24.97653669756257, 0.4463633711969133, 0.0, 1.0, 199452.2233050118], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4738800.0000, 
sim time next is 4739400.0000, 
raw observation next is [-1.5, 81.5, 0.0, 0.0, 26.0, 25.00052650121628, 0.4795050392159526, 0.0, 1.0, 132119.7971654323], 
processed observation next is [1.0, 0.8695652173913043, 0.4210526315789474, 0.815, 0.0, 0.0, 0.6666666666666666, 0.5833772084346901, 0.6598350130719842, 0.0, 1.0, 0.6291418912639634], 
reward next is 0.3709, 
noisyNet noise sample is [array([-1.2448317], dtype=float32), 0.0026698036]. 
=============================================
[2019-04-04 10:19:59,097] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2348692e-10 5.4354734e-09 7.4687600e-26 7.0875676e-25 2.8670553e-19
 1.0000000e+00 1.0927671e-18], sum to 1.0000
[2019-04-04 10:19:59,099] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3439
[2019-04-04 10:19:59,116] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 81.83333333333333, 0.0, 0.0, 26.0, 25.06649548594388, 0.3964726447586895, 0.0, 1.0, 41479.64434455329], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4752600.0000, 
sim time next is 4753200.0000, 
raw observation next is [-4.0, 79.66666666666667, 0.0, 0.0, 26.0, 25.08604591969294, 0.3902105725113473, 0.0, 1.0, 41414.9824701062], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5905038266410783, 0.6300701908371158, 0.0, 1.0, 0.19721420223860098], 
reward next is 0.8028, 
noisyNet noise sample is [array([1.3082792], dtype=float32), 0.21836242]. 
=============================================
[2019-04-04 10:20:15,125] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.14446504e-13 4.92123425e-11 1.56604755e-30 1.51727638e-28
 9.08248298e-24 1.00000000e+00 8.80133216e-24], sum to 1.0000
[2019-04-04 10:20:15,129] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1895
[2019-04-04 10:20:15,137] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.800000000000001, 49.33333333333334, 192.3333333333333, 634.6666666666666, 26.0, 26.61602456143804, 0.7837875811430163, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4630800.0000, 
sim time next is 4631400.0000, 
raw observation next is [4.85, 49.5, 203.0, 599.0, 26.0, 26.96332118097992, 0.821964082353469, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5969529085872576, 0.495, 0.6766666666666666, 0.661878453038674, 0.6666666666666666, 0.7469434317483268, 0.7739880274511562, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49382842], dtype=float32), 0.58294165]. 
=============================================
[2019-04-04 10:20:19,659] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:20:19,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:20:19,665] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run28
[2019-04-04 10:20:19,717] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2613120e-13 1.6057806e-11 1.4696321e-33 1.9310831e-30 1.1017756e-24
 1.0000000e+00 8.9065700e-25], sum to 1.0000
[2019-04-04 10:20:19,718] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8082
[2019-04-04 10:20:19,748] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.75, 32.5, 133.6666666666667, 209.6666666666666, 26.0, 28.307050700926, 1.118632405013594, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4374600.0000, 
sim time next is 4375200.0000, 
raw observation next is [13.6, 33.0, 118.3333333333333, 104.8333333333333, 26.0, 28.50922685497763, 1.137282067736551, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8393351800554018, 0.33, 0.3944444444444443, 0.11583793738489867, 0.6666666666666666, 0.8757689045814692, 0.8790940225788503, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39193055], dtype=float32), 1.6747744]. 
=============================================
[2019-04-04 10:20:20,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:20:20,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:20:20,404] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run28
[2019-04-04 10:20:24,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:20:24,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:20:24,871] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run28
[2019-04-04 10:20:26,893] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5576765e-10 2.6679263e-09 6.1155081e-25 1.0769195e-22 2.5028291e-18
 1.0000000e+00 4.3758272e-18], sum to 1.0000
[2019-04-04 10:20:26,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1154
[2019-04-04 10:20:26,930] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 43.66666666666667, 0.0, 0.0, 26.0, 24.9821480293387, 0.3295273424455937, 0.0, 1.0, 41032.61675735615], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4821000.0000, 
sim time next is 4821600.0000, 
raw observation next is [1.0, 44.33333333333334, 0.0, 0.0, 26.0, 24.98759711771302, 0.3456174583162077, 0.0, 1.0, 198796.7257963168], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5822997598094183, 0.6152058194387359, 0.0, 1.0, 0.9466510752205562], 
reward next is 0.0533, 
noisyNet noise sample is [array([-1.4454179], dtype=float32), -0.5414303]. 
=============================================
[2019-04-04 10:20:26,933] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.1303601e-13 1.9568187e-11 4.8201193e-32 2.0610541e-29 2.8582900e-24
 1.0000000e+00 1.4037688e-23], sum to 1.0000
[2019-04-04 10:20:26,935] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3260
[2019-04-04 10:20:26,943] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.33333333333333, 17.0, 30.0, 243.3333333333333, 26.0, 28.47251627121828, 1.134931579941896, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5074800.0000, 
sim time next is 5075400.0000, 
raw observation next is [11.16666666666667, 17.0, 24.0, 194.6666666666667, 26.0, 28.50654611907455, 1.121669154515396, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7719298245614037, 0.17, 0.08, 0.21510128913443835, 0.6666666666666666, 0.8755455099228792, 0.8738897181717986, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9960192], dtype=float32), -0.87851983]. 
=============================================
[2019-04-04 10:20:27,180] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.2147783e-10 4.0480779e-09 3.9615473e-26 7.9522512e-24 3.5812808e-19
 1.0000000e+00 9.8294885e-19], sum to 1.0000
[2019-04-04 10:20:27,187] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5861
[2019-04-04 10:20:27,201] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666667, 48.33333333333334, 0.0, 0.0, 26.0, 25.50316942688148, 0.4386154803556848, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4825200.0000, 
sim time next is 4825800.0000, 
raw observation next is [0.5, 49.0, 0.0, 0.0, 26.0, 25.52800476960709, 0.4303032256322803, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4764542936288089, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6273337308005908, 0.6434344085440934, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5573164], dtype=float32), 0.65336317]. 
=============================================
[2019-04-04 10:20:27,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:20:27,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:20:27,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run28
[2019-04-04 10:20:28,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:20:28,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:20:28,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run28
[2019-04-04 10:20:29,010] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5850481e-12 1.6890959e-10 2.5152385e-29 9.1101552e-28 4.8795301e-22
 1.0000000e+00 3.7731553e-22], sum to 1.0000
[2019-04-04 10:20:29,011] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2526
[2019-04-04 10:20:29,059] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333333, 33.0, 109.5, 731.3333333333333, 26.0, 26.18421855904848, 0.4931808952662344, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4959600.0000, 
sim time next is 4960200.0000, 
raw observation next is [0.6666666666666667, 31.5, 111.0, 745.6666666666667, 26.0, 26.33186565815473, 0.5173229189890091, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4810710987996307, 0.315, 0.37, 0.8239410681399633, 0.6666666666666666, 0.6943221381795608, 0.6724409729963363, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33863148], dtype=float32), -0.14591958]. 
=============================================
[2019-04-04 10:20:30,567] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:20:30,567] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:20:30,580] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run28
[2019-04-04 10:20:33,876] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:20:33,876] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:20:33,883] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run28
[2019-04-04 10:20:36,583] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0901007e-13 3.8693281e-11 4.7898388e-32 1.4923734e-30 1.6821106e-24
 1.0000000e+00 5.8862351e-24], sum to 1.0000
[2019-04-04 10:20:36,583] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5733
[2019-04-04 10:20:36,632] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.0, 17.33333333333334, 62.66666666666667, 487.3333333333334, 26.0, 29.14120984256841, 1.034469936569848, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5071800.0000, 
sim time next is 5072400.0000, 
raw observation next is [12.0, 17.0, 56.0, 438.5, 26.0, 29.16177144236566, 1.202290236998601, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7950138504155125, 0.17, 0.18666666666666668, 0.4845303867403315, 0.6666666666666666, 0.9301476201971383, 0.9007634123328669, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22188881], dtype=float32), 1.4519473]. 
=============================================
[2019-04-04 10:20:37,834] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:20:37,834] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:20:37,874] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run28
[2019-04-04 10:20:38,132] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7658012e-13 3.1302575e-11 4.1161404e-32 3.7414785e-30 3.6679476e-24
 1.0000000e+00 1.6241190e-24], sum to 1.0000
[2019-04-04 10:20:38,133] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6933
[2019-04-04 10:20:38,177] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.666666666666668, 23.33333333333334, 119.0, 860.8333333333334, 26.0, 27.81105277073525, 0.9572472419460066, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5059200.0000, 
sim time next is 5059800.0000, 
raw observation next is [10.0, 22.5, 118.0, 860.0, 26.0, 27.93762383284785, 0.8539463224916285, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.739612188365651, 0.225, 0.3933333333333333, 0.9502762430939227, 0.6666666666666666, 0.8281353194039877, 0.7846487741638762, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8256405], dtype=float32), -0.2251674]. 
=============================================
[2019-04-04 10:20:38,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:20:38,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:20:38,856] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:20:38,858] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:20:38,866] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run28
[2019-04-04 10:20:38,898] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run28
[2019-04-04 10:20:41,379] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:20:41,379] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:20:41,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run28
[2019-04-04 10:20:45,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:20:45,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:20:45,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run28
[2019-04-04 10:20:59,648] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5476908e-13 6.1953012e-12 3.6046255e-32 1.5011892e-30 2.5510074e-24
 1.0000000e+00 1.7158959e-24], sum to 1.0000
[2019-04-04 10:20:59,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4090
[2019-04-04 10:20:59,654] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.0, 19.0, 89.33333333333333, 691.6666666666667, 26.0, 28.1941689814093, 1.098590506428466, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5068200.0000, 
sim time next is 5068800.0000, 
raw observation next is [12.0, 19.0, 86.0, 665.0, 26.0, 28.56646554861764, 1.135011925452081, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.19, 0.2866666666666667, 0.7348066298342542, 0.6666666666666666, 0.8805387957181366, 0.8783373084840269, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.332621], dtype=float32), -1.16885]. 
=============================================
[2019-04-04 10:21:00,122] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.2070705e-09 2.6937290e-08 7.3747211e-23 1.0299746e-21 8.5694150e-18
 1.0000000e+00 8.6936497e-17], sum to 1.0000
[2019-04-04 10:21:00,122] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0400
[2019-04-04 10:21:00,239] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.54738901152137, -0.2980058871274366, 0.0, 1.0, 45023.61995836028], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 198600.0000, 
sim time next is 199200.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.4833243543006, -0.2223548512970458, 1.0, 1.0, 202343.6265148717], 
processed observation next is [1.0, 0.30434782608695654, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3736103628583833, 0.42588171623431803, 1.0, 1.0, 0.9635410786422461], 
reward next is 0.0365, 
noisyNet noise sample is [array([0.20343609], dtype=float32), -0.6196993]. 
=============================================
[2019-04-04 10:21:02,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:21:02,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:21:02,126] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run28
[2019-04-04 10:21:04,156] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:21:04,156] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:21:04,160] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run28
[2019-04-04 10:21:08,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:21:08,506] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:21:08,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run28
[2019-04-04 10:21:29,107] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:21:29,107] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:21:29,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run28
[2019-04-04 10:21:38,858] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2841953e-10 3.3454295e-09 8.1652392e-26 7.2105358e-24 2.9237143e-19
 1.0000000e+00 4.1205982e-19], sum to 1.0000
[2019-04-04 10:21:38,859] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8708
[2019-04-04 10:21:38,934] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 87.0, 0.0, 0.0, 26.0, 24.94341702800877, 0.289773327911177, 0.0, 1.0, 56483.26170906942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 581400.0000, 
sim time next is 582000.0000, 
raw observation next is [-2.1, 87.0, 0.0, 0.0, 26.0, 24.95242024973768, 0.2902364223350555, 0.0, 1.0, 45156.56541811807], 
processed observation next is [0.0, 0.7391304347826086, 0.404432132963989, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5793683541448068, 0.5967454741116852, 0.0, 1.0, 0.21503126389580035], 
reward next is 0.7850, 
noisyNet noise sample is [array([-0.31592014], dtype=float32), 1.694841]. 
=============================================
[2019-04-04 10:21:38,944] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[77.769806]
 [77.56694 ]
 [77.527466]
 [77.74203 ]
 [77.80819 ]], R is [[78.01325226]
 [77.9641571 ]
 [77.95606995]
 [78.02158356]
 [78.11574554]].
[2019-04-04 10:21:48,152] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6554680e-11 1.4561785e-09 4.4778998e-28 1.5347532e-26 1.8647362e-22
 1.0000000e+00 2.7166667e-21], sum to 1.0000
[2019-04-04 10:21:48,153] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9072
[2019-04-04 10:21:48,206] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.2, 79.0, 91.0, 0.0, 26.0, 25.31681556082867, 0.3981053664835263, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 826200.0000, 
sim time next is 826800.0000, 
raw observation next is [-4.1, 79.0, 85.66666666666667, 0.0, 26.0, 25.99768506443109, 0.4421725494014455, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3490304709141275, 0.79, 0.28555555555555556, 0.0, 0.6666666666666666, 0.6664737553692573, 0.6473908498004818, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5213118], dtype=float32), -0.02879044]. 
=============================================
[2019-04-04 10:21:50,563] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.0903461e-12 3.5145145e-10 1.3719482e-28 5.8978689e-27 1.9143326e-21
 1.0000000e+00 6.0545186e-21], sum to 1.0000
[2019-04-04 10:21:50,563] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4982
[2019-04-04 10:21:50,611] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.04313846052711, 0.3204044266404515, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 846000.0000, 
sim time next is 846600.0000, 
raw observation next is [-3.816666666666666, 85.5, 0.0, 0.0, 26.0, 25.12960741272861, 0.3209055797935099, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3568790397045245, 0.855, 0.0, 0.0, 0.6666666666666666, 0.5941339510607175, 0.6069685265978366, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14347684], dtype=float32), 1.6926758]. 
=============================================
[2019-04-04 10:21:58,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5388637e-12 3.8835704e-10 8.2804026e-30 1.5314649e-28 4.3953730e-23
 1.0000000e+00 5.9552854e-22], sum to 1.0000
[2019-04-04 10:21:58,289] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4068
[2019-04-04 10:21:58,306] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.05, 85.5, 0.0, 0.0, 26.0, 25.38992755361038, 0.4330291290588104, 0.0, 1.0, 28283.60122370607], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 955800.0000, 
sim time next is 956400.0000, 
raw observation next is [6.233333333333333, 84.33333333333334, 0.0, 0.0, 26.0, 25.36227389002146, 0.4493212746445303, 0.0, 1.0, 42847.53506113266], 
processed observation next is [1.0, 0.043478260869565216, 0.6352723915050786, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6135228241684549, 0.6497737582148434, 0.0, 1.0, 0.20403588124348884], 
reward next is 0.7960, 
noisyNet noise sample is [array([-0.09581396], dtype=float32), 1.0222105]. 
=============================================
[2019-04-04 10:22:07,217] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.5830212e-14 8.4702176e-12 8.0891529e-35 2.9210976e-32 3.0954418e-26
 1.0000000e+00 4.5642380e-26], sum to 1.0000
[2019-04-04 10:22:07,218] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5048
[2019-04-04 10:22:07,228] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.75, 81.5, 100.0, 234.0, 26.0, 26.67392404448757, 0.7853444796646095, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1071000.0000, 
sim time next is 1071600.0000, 
raw observation next is [12.93333333333333, 81.0, 102.3333333333333, 195.0, 26.0, 26.7336841412382, 0.8035264276476862, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.8208679593721145, 0.81, 0.341111111111111, 0.2154696132596685, 0.6666666666666666, 0.72780701176985, 0.7678421425492288, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15642546], dtype=float32), 0.9778064]. 
=============================================
[2019-04-04 10:22:10,225] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1845814e-10 5.7573759e-09 9.3179202e-26 2.2335407e-24 2.1020110e-20
 1.0000000e+00 1.1790724e-19], sum to 1.0000
[2019-04-04 10:22:10,225] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2957
[2019-04-04 10:22:10,303] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 79.00000000000001, 0.0, 0.0, 26.0, 24.7242430156281, 0.2174288630734761, 0.0, 1.0, 39390.11948675929], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 871800.0000, 
sim time next is 872400.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 26.0, 24.77947150158725, 0.210718218260205, 0.0, 1.0, 39351.17271848558], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5649559584656041, 0.570239406086735, 0.0, 1.0, 0.18738653675469322], 
reward next is 0.8126, 
noisyNet noise sample is [array([-0.45782295], dtype=float32), 0.27246803]. 
=============================================
[2019-04-04 10:22:12,759] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1754989e-13 1.2643187e-11 9.7428434e-34 1.1924995e-31 2.0600914e-25
 1.0000000e+00 1.1286437e-24], sum to 1.0000
[2019-04-04 10:22:12,759] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0500
[2019-04-04 10:22:12,814] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.61666666666667, 86.0, 125.0, 0.0, 26.0, 25.6469106409182, 0.5560839064335747, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 996600.0000, 
sim time next is 997200.0000, 
raw observation next is [12.7, 86.0, 123.5, 0.0, 26.0, 25.95070354710873, 0.5811338451229963, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8144044321329641, 0.86, 0.4116666666666667, 0.0, 0.6666666666666666, 0.6625586289257276, 0.6937112817076655, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40874115], dtype=float32), -0.51044375]. 
=============================================
[2019-04-04 10:22:14,172] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0289579e-09 3.1309295e-09 8.0696804e-26 5.2914841e-24 1.3743110e-20
 1.0000000e+00 5.5227479e-19], sum to 1.0000
[2019-04-04 10:22:14,177] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0734
[2019-04-04 10:22:14,220] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.7, 89.66666666666667, 0.0, 0.0, 26.0, 23.97651280687598, 0.2394108551602085, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1219200.0000, 
sim time next is 1219800.0000, 
raw observation next is [15.6, 91.33333333333333, 0.0, 0.0, 26.0, 23.94489308636353, 0.2355858082030159, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.8947368421052633, 0.9133333333333333, 0.0, 0.0, 0.6666666666666666, 0.4954077571969607, 0.5785286027343387, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32897756], dtype=float32), -1.6124811]. 
=============================================
[2019-04-04 10:22:18,212] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0430525e-13 6.1898559e-12 3.4871270e-35 3.8414097e-32 2.9000554e-26
 1.0000000e+00 1.1978466e-25], sum to 1.0000
[2019-04-04 10:22:18,214] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7502
[2019-04-04 10:22:18,264] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.56666666666667, 82.0, 87.00000000000001, 206.5, 26.0, 26.56359825573097, 0.7688858622328221, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1070400.0000, 
sim time next is 1071000.0000, 
raw observation next is [12.75, 81.5, 100.0, 234.0, 26.0, 26.67392439899315, 0.7853445728443408, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.8157894736842106, 0.815, 0.3333333333333333, 0.2585635359116022, 0.6666666666666666, 0.7228270332494292, 0.7617815242814469, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28622955], dtype=float32), -0.41843933]. 
=============================================
[2019-04-04 10:22:18,273] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[106.28658]
 [105.8728 ]
 [105.59113]
 [105.4115 ]
 [105.28459]], R is [[106.70587921]
 [106.63882446]
 [106.57243347]
 [106.50671387]
 [106.44165039]].
[2019-04-04 10:22:20,600] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0551659e-12 4.7948651e-10 7.6127738e-29 2.3848835e-26 7.8066932e-22
 1.0000000e+00 2.1706302e-21], sum to 1.0000
[2019-04-04 10:22:20,601] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0636
[2019-04-04 10:22:20,654] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.2, 74.66666666666667, 77.0, 32.66666666666667, 26.0, 25.7984770934475, 0.3170985073640292, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 724200.0000, 
sim time next is 724800.0000, 
raw observation next is [-2.1, 73.33333333333334, 89.0, 40.83333333333334, 26.0, 25.8563467953992, 0.3285238162198523, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.404432132963989, 0.7333333333333334, 0.2966666666666667, 0.045119705340699826, 0.6666666666666666, 0.6546955662832667, 0.6095079387399508, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00377397], dtype=float32), 0.87950903]. 
=============================================
[2019-04-04 10:22:22,379] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-04 10:22:22,381] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:22:22,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:22:22,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run38
[2019-04-04 10:22:22,431] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:22:22,431] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:22:22,432] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:22:22,432] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:22:22,435] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run38
[2019-04-04 10:22:22,458] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run38
[2019-04-04 10:25:28,934] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 10:25:37,381] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.26733056], dtype=float32), 0.30932152]
[2019-04-04 10:25:37,381] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [6.9, 58.5, 229.0, 385.0, 26.0, 25.42774409428822, 0.4434226385205982, 0.0, 1.0, 0.0]
[2019-04-04 10:25:37,381] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 10:25:37,382] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.3362606e-11 3.9376091e-10 6.9952075e-29 8.9614298e-27 1.0534148e-21
 1.0000000e+00 2.9266016e-21], sampled 0.02091104979563785
[2019-04-04 10:25:44,873] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.26733056], dtype=float32), 0.30932152]
[2019-04-04 10:25:44,874] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.759982197666667, 78.78258124166666, 0.0, 0.0, 26.0, 25.53084459857288, 0.5042827520687779, 0.0, 1.0, 19544.8663817247]
[2019-04-04 10:25:44,874] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:25:44,875] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.7715096e-11 1.1902382e-09 3.0094865e-28 2.9497500e-26 1.6099648e-21
 1.0000000e+00 9.5595300e-21], sampled 0.600744782633791
[2019-04-04 10:25:59,852] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 10:26:04,979] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 10:26:06,017] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 3700000, evaluation results [3700000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 10:26:06,756] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5963098e-14 1.5288427e-11 3.0817150e-34 5.8085872e-32 3.7304107e-27
 1.0000000e+00 2.5712002e-25], sum to 1.0000
[2019-04-04 10:26:06,759] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6834
[2019-04-04 10:26:06,784] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.38333333333333, 82.5, 74.0, 179.0, 26.0, 26.52330474029613, 0.7453522347085189, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1069800.0000, 
sim time next is 1070400.0000, 
raw observation next is [12.56666666666667, 82.0, 87.00000000000001, 206.5, 26.0, 26.56359812947635, 0.7688858276402165, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.8107109879963068, 0.82, 0.29000000000000004, 0.2281767955801105, 0.6666666666666666, 0.7136331774563626, 0.7562952758800722, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36399493], dtype=float32), -1.0292501]. 
=============================================
[2019-04-04 10:26:34,100] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8860494e-13 5.1331203e-11 2.3927416e-31 3.2093657e-29 1.7021400e-24
 1.0000000e+00 1.3129614e-23], sum to 1.0000
[2019-04-04 10:26:34,100] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8779
[2019-04-04 10:26:34,168] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.75, 92.0, 30.0, 0.0, 26.0, 25.61122017443603, 0.5147728739825009, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1672200.0000, 
sim time next is 1672800.0000, 
raw observation next is [2.566666666666667, 92.0, 33.83333333333333, 0.0, 26.0, 25.72070728200434, 0.5391394004218794, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5337026777469991, 0.92, 0.11277777777777777, 0.0, 0.6666666666666666, 0.6433922735003618, 0.6797131334739598, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4045413], dtype=float32), 0.918859]. 
=============================================
[2019-04-04 10:26:38,627] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.09082339e-12 1.09054155e-10 1.69242298e-30 1.67524886e-28
 2.82605854e-23 1.00000000e+00 2.94633742e-22], sum to 1.0000
[2019-04-04 10:26:38,627] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7149
[2019-04-04 10:26:38,675] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.9000000000000001, 92.0, 106.1666666666667, 0.0, 26.0, 26.09989357611176, 0.5899688073030912, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1334400.0000, 
sim time next is 1335000.0000, 
raw observation next is [1.0, 92.0, 110.3333333333333, 0.0, 26.0, 26.10695415854154, 0.5913986052358008, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.92, 0.36777777777777765, 0.0, 0.6666666666666666, 0.6755795132117951, 0.6971328684119337, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5695422], dtype=float32), 0.1832742]. 
=============================================
[2019-04-04 10:26:38,777] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[92.08403]
 [91.88451]
 [91.76155]
 [91.58007]
 [91.43788]], R is [[92.2773819 ]
 [92.35460663]
 [92.43106079]
 [92.50675201]
 [92.58168793]].
[2019-04-04 10:26:40,766] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6918138e-14 3.3516898e-12 1.1436466e-34 6.3922707e-33 7.5255007e-26
 1.0000000e+00 7.1208705e-26], sum to 1.0000
[2019-04-04 10:26:40,767] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3892
[2019-04-04 10:26:40,864] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.6, 65.0, 217.5, 266.0, 26.0, 27.17890211020765, 0.6305334108251802, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1080000.0000, 
sim time next is 1080600.0000, 
raw observation next is [16.88333333333334, 63.5, 205.3333333333333, 283.0, 26.0, 27.04366137955662, 0.8663217282759034, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.9302862419205914, 0.635, 0.6844444444444443, 0.312707182320442, 0.6666666666666666, 0.753638448296385, 0.7887739094253011, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.07478722], dtype=float32), 1.2963197]. 
=============================================
[2019-04-04 10:26:42,235] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4189979e-11 4.3809895e-10 4.3012929e-30 7.5469063e-29 3.5207512e-23
 1.0000000e+00 1.8694941e-22], sum to 1.0000
[2019-04-04 10:26:42,236] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6669
[2019-04-04 10:26:42,339] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.33333333333333, 77.66666666666667, 0.0, 0.0, 26.0, 25.65489203047144, 0.6403226067851164, 0.0, 1.0, 22279.31023488069], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1128000.0000, 
sim time next is 1128600.0000, 
raw observation next is [10.25, 78.0, 0.0, 0.0, 26.0, 25.65906534410261, 0.6394094914384292, 0.0, 1.0, 20757.2012979367], 
processed observation next is [0.0, 0.043478260869565216, 0.7465373961218837, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6382554453418843, 0.713136497146143, 0.0, 1.0, 0.09884381570446048], 
reward next is 0.9012, 
noisyNet noise sample is [array([-2.132561], dtype=float32), 0.075948246]. 
=============================================
[2019-04-04 10:26:46,067] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1008096e-11 4.4220416e-10 5.0841012e-29 2.8543755e-26 8.3218836e-23
 1.0000000e+00 2.7880467e-22], sum to 1.0000
[2019-04-04 10:26:46,067] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9383
[2019-04-04 10:26:46,077] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.966666666666667, 75.33333333333333, 0.0, 0.0, 26.0, 26.17129154636807, 0.7103570542355199, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1546800.0000, 
sim time next is 1547400.0000, 
raw observation next is [6.783333333333333, 75.66666666666667, 0.0, 0.0, 26.0, 26.15124887371511, 0.7012561136544452, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6505078485687905, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6792707394762593, 0.7337520378848151, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2469738], dtype=float32), 0.38526362]. 
=============================================
[2019-04-04 10:26:52,923] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7472301e-10 1.7437751e-09 2.5757344e-26 3.3270394e-24 6.7854782e-20
 1.0000000e+00 5.6590890e-19], sum to 1.0000
[2019-04-04 10:26:52,925] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7728
[2019-04-04 10:26:52,979] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 83.0, 109.0, 0.0, 26.0, 25.06023731665324, 0.3534792834970973, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1778400.0000, 
sim time next is 1779000.0000, 
raw observation next is [-2.8, 83.66666666666667, 105.6666666666667, 0.0, 26.0, 25.04354702312006, 0.3414250002295366, 0.0, 1.0, 27091.36915607728], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8366666666666667, 0.3522222222222223, 0.0, 0.6666666666666666, 0.5869622519266716, 0.6138083334098455, 0.0, 1.0, 0.12900651979084418], 
reward next is 0.8710, 
noisyNet noise sample is [array([-1.0399382], dtype=float32), 1.1554872]. 
=============================================
[2019-04-04 10:26:52,983] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[78.7503 ]
 [79.16068]
 [79.45298]
 [79.5671 ]
 [79.65589]], R is [[78.5756073 ]
 [78.78984833]
 [78.91274261]
 [78.88671112]
 [78.83675385]].
[2019-04-04 10:26:58,739] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.36760566e-12 1.10624974e-10 7.34263725e-30 7.81096326e-27
 4.06753754e-23 1.00000000e+00 7.10867174e-22], sum to 1.0000
[2019-04-04 10:26:58,740] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9245
[2019-04-04 10:26:58,760] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.466666666666667, 92.33333333333333, 0.0, 0.0, 26.0, 25.38970078195608, 0.552692464230826, 0.0, 1.0, 109330.7317154679], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1302000.0000, 
sim time next is 1302600.0000, 
raw observation next is [3.383333333333333, 92.16666666666667, 0.0, 0.0, 26.0, 25.33094390210919, 0.5575661081219495, 0.0, 1.0, 80990.84256700113], 
processed observation next is [1.0, 0.043478260869565216, 0.5563250230840259, 0.9216666666666667, 0.0, 0.0, 0.6666666666666666, 0.6109119918424325, 0.6858553693739832, 0.0, 1.0, 0.38567067889048157], 
reward next is 0.6143, 
noisyNet noise sample is [array([0.683652], dtype=float32), -0.8374263]. 
=============================================
[2019-04-04 10:26:59,107] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3525075e-12 4.7070119e-11 5.2123932e-30 1.0873309e-28 1.7999395e-23
 1.0000000e+00 6.6778483e-23], sum to 1.0000
[2019-04-04 10:26:59,107] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1523
[2019-04-04 10:26:59,125] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.5, 60.83333333333334, 0.0, 0.0, 26.0, 26.16447717679625, 0.6567068633647012, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1536600.0000, 
sim time next is 1537200.0000, 
raw observation next is [9.4, 61.0, 0.0, 0.0, 26.0, 26.06723284454635, 0.6381987891692704, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7229916897506927, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6722694037121958, 0.7127329297230901, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5276665], dtype=float32), -1.1800948]. 
=============================================
[2019-04-04 10:27:03,712] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0581944e-08 1.2344134e-08 8.9217023e-25 7.3040278e-23 4.5118189e-19
 1.0000000e+00 1.1910981e-18], sum to 1.0000
[2019-04-04 10:27:03,713] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1063
[2019-04-04 10:27:03,730] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.25813438357073, 0.1318255587536841, 0.0, 1.0, 41703.85576488706], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1995600.0000, 
sim time next is 1996200.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.33137021355545, 0.1340124439971207, 0.0, 1.0, 41640.45409929976], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.527614184462954, 0.5446708146657069, 0.0, 1.0, 0.19828787666333217], 
reward next is 0.8017, 
noisyNet noise sample is [array([-1.1524299], dtype=float32), 1.1662412]. 
=============================================
[2019-04-04 10:27:05,763] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1439534e-11 6.2584560e-10 2.4807167e-28 5.7277611e-27 9.6709507e-23
 1.0000000e+00 6.1903912e-22], sum to 1.0000
[2019-04-04 10:27:05,763] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3356
[2019-04-04 10:27:05,769] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.9, 53.16666666666666, 150.3333333333333, 0.0, 26.0, 27.08472863629737, 0.9199156958707384, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1087800.0000, 
sim time next is 1088400.0000, 
raw observation next is [19.0, 52.33333333333334, 145.1666666666667, 0.0, 26.0, 27.40726109730817, 0.9538065445659057, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.9889196675900279, 0.5233333333333334, 0.48388888888888903, 0.0, 0.6666666666666666, 0.7839384247756808, 0.8179355148553019, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0441526], dtype=float32), -0.17786825]. 
=============================================
[2019-04-04 10:27:08,581] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.1146075e-12 6.2803390e-10 1.8079716e-29 1.2149184e-27 8.9671933e-23
 1.0000000e+00 2.3029857e-21], sum to 1.0000
[2019-04-04 10:27:08,581] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5716
[2019-04-04 10:27:08,615] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.6, 80.0, 0.0, 0.0, 26.0, 25.6940805247928, 0.6138908251934154, 0.0, 1.0, 18721.72873481722], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1143000.0000, 
sim time next is 1143600.0000, 
raw observation next is [11.6, 81.0, 0.0, 0.0, 26.0, 25.68264704056194, 0.609943613325946, 0.0, 1.0, 18720.7724837526], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6402205867134949, 0.7033145377753153, 0.0, 1.0, 0.08914653563691714], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.0668932], dtype=float32), 0.19849458]. 
=============================================
[2019-04-04 10:27:20,153] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3452200e-09 2.5415044e-08 3.4957519e-23 4.4608643e-22 2.7802676e-18
 1.0000000e+00 2.1817420e-17], sum to 1.0000
[2019-04-04 10:27:20,153] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6995
[2019-04-04 10:27:20,168] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.84220468040011, -0.04368505932205657, 0.0, 1.0, 44849.5148479788], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1914000.0000, 
sim time next is 1914600.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.81440838939429, -0.04667588964282277, 0.0, 1.0, 44942.88705322437], 
processed observation next is [1.0, 0.13043478260869565, 0.2299168975069252, 0.78, 0.0, 0.0, 0.6666666666666666, 0.48453403244952425, 0.4844413701190591, 0.0, 1.0, 0.214013747872497], 
reward next is 0.7860, 
noisyNet noise sample is [array([0.04195862], dtype=float32), -1.1278572]. 
=============================================
[2019-04-04 10:27:31,208] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6563211e-11 2.7685734e-10 5.1496512e-27 1.4552623e-24 2.0281732e-20
 1.0000000e+00 9.7725418e-20], sum to 1.0000
[2019-04-04 10:27:31,208] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7737
[2019-04-04 10:27:31,319] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 25.008584373802, 0.3376473495414554, 1.0, 1.0, 141966.6324934565], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2058600.0000, 
sim time next is 2059200.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.91251592992491, 0.3660793151746995, 0.0, 1.0, 152091.1017079971], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5760429941604093, 0.6220264383915665, 0.0, 1.0, 0.7242433414666528], 
reward next is 0.2758, 
noisyNet noise sample is [array([0.5487496], dtype=float32), -0.020010749]. 
=============================================
[2019-04-04 10:27:34,760] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.8298915e-10 3.2819121e-09 9.7642077e-25 3.7806581e-24 2.4159242e-19
 1.0000000e+00 1.1491529e-18], sum to 1.0000
[2019-04-04 10:27:34,760] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8734
[2019-04-04 10:27:34,778] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 57.5, 0.0, 0.0, 26.0, 25.33105744933168, 0.3653145147663617, 0.0, 1.0, 39646.36398118924], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2327400.0000, 
sim time next is 2328000.0000, 
raw observation next is [-2.1, 58.0, 0.0, 0.0, 26.0, 25.26992018373328, 0.3552117471373228, 0.0, 1.0, 39136.10283157401], 
processed observation next is [1.0, 0.9565217391304348, 0.404432132963989, 0.58, 0.0, 0.0, 0.6666666666666666, 0.6058266819777733, 0.6184039157124409, 0.0, 1.0, 0.18636239443606673], 
reward next is 0.8136, 
noisyNet noise sample is [array([2.1368594], dtype=float32), 0.08233107]. 
=============================================
[2019-04-04 10:27:34,815] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.01398 ]
 [76.910934]
 [76.81103 ]
 [76.70186 ]
 [76.4974  ]], R is [[77.06773376]
 [77.10826874]
 [77.15180969]
 [77.17636871]
 [77.17105865]].
[2019-04-04 10:27:39,455] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.9341272e-10 3.0388756e-09 9.5288869e-26 5.8700453e-24 9.1622958e-20
 1.0000000e+00 6.7470486e-19], sum to 1.0000
[2019-04-04 10:27:39,455] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4831
[2019-04-04 10:27:39,528] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.3, 78.5, 0.0, 0.0, 26.0, 25.38064546644452, 0.3814161330258899, 0.0, 1.0, 34459.72555863416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2143800.0000, 
sim time next is 2144400.0000, 
raw observation next is [-5.4, 80.0, 0.0, 0.0, 26.0, 25.23365122140458, 0.3635708431501394, 1.0, 1.0, 37480.47000611304], 
processed observation next is [1.0, 0.8260869565217391, 0.31301939058171746, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6028042684503815, 0.6211902810500465, 1.0, 1.0, 0.1784784286005383], 
reward next is 0.8215, 
noisyNet noise sample is [array([1.6424366], dtype=float32), -0.16422659]. 
=============================================
[2019-04-04 10:27:46,811] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3358288e-09 1.0154588e-08 6.0395388e-24 2.3269779e-22 1.5457527e-18
 1.0000000e+00 3.0932250e-17], sum to 1.0000
[2019-04-04 10:27:46,812] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8395
[2019-04-04 10:27:46,826] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.716666666666667, 77.5, 0.0, 0.0, 26.0, 24.05541235711795, 0.01605247217690827, 0.0, 1.0, 45072.13527013813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1907400.0000, 
sim time next is 1908000.0000, 
raw observation next is [-7.8, 78.0, 0.0, 0.0, 26.0, 24.02642377843596, 0.00942349622325564, 0.0, 1.0, 44998.02777661925], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5022019815363299, 0.5031411654077519, 0.0, 1.0, 0.21427632274580596], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.4144603], dtype=float32), 0.49188715]. 
=============================================
[2019-04-04 10:27:46,833] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.685936]
 [72.91917 ]
 [73.02945 ]
 [73.002846]
 [73.28242 ]], R is [[72.85480499]
 [72.91162872]
 [72.96753693]
 [73.0226593 ]
 [73.07718658]].
[2019-04-04 10:27:50,425] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.7418693e-10 1.9428597e-09 6.4004186e-26 2.4157796e-24 1.1517768e-19
 1.0000000e+00 1.4867167e-19], sum to 1.0000
[2019-04-04 10:27:50,439] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3199
[2019-04-04 10:27:50,539] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.8, 70.5, 0.0, 0.0, 26.0, 25.09527870508664, 0.3455202310221422, 1.0, 1.0, 110779.2661981412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2230200.0000, 
sim time next is 2230800.0000, 
raw observation next is [-4.866666666666666, 70.66666666666666, 0.0, 0.0, 26.0, 24.99861267582316, 0.3613658022200326, 1.0, 1.0, 135714.5318282975], 
processed observation next is [1.0, 0.8260869565217391, 0.3277931671283472, 0.7066666666666666, 0.0, 0.0, 0.6666666666666666, 0.5832177229852634, 0.6204552674066776, 1.0, 1.0, 0.6462596753728452], 
reward next is 0.3537, 
noisyNet noise sample is [array([-0.48428956], dtype=float32), -0.03435844]. 
=============================================
[2019-04-04 10:27:55,176] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.9483341e-10 1.9312852e-08 4.0244720e-25 1.4671245e-23 3.0699464e-19
 1.0000000e+00 4.0273584e-18], sum to 1.0000
[2019-04-04 10:27:55,176] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9992
[2019-04-04 10:27:55,207] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.1, 79.66666666666667, 0.0, 0.0, 26.0, 24.27733906854792, 0.1392631824983881, 0.0, 1.0, 43999.29523521043], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2252400.0000, 
sim time next is 2253000.0000, 
raw observation next is [-7.199999999999999, 80.83333333333334, 0.0, 0.0, 26.0, 24.22270241326876, 0.1329108803912304, 0.0, 1.0, 43989.3520407092], 
processed observation next is [1.0, 0.043478260869565216, 0.26315789473684215, 0.8083333333333335, 0.0, 0.0, 0.6666666666666666, 0.5185585344390633, 0.5443036267970768, 0.0, 1.0, 0.2094731049557581], 
reward next is 0.7905, 
noisyNet noise sample is [array([1.1441709], dtype=float32), -0.86508024]. 
=============================================
[2019-04-04 10:27:55,216] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.13974 ]
 [75.209274]
 [75.25304 ]
 [75.3799  ]
 [75.39812 ]], R is [[75.10124969]
 [75.14071655]
 [75.17959595]
 [75.2178421 ]
 [75.25548553]].
[2019-04-04 10:27:58,434] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2976798e-11 2.1242334e-10 1.0305295e-27 1.2516356e-26 5.8018183e-22
 1.0000000e+00 6.5766323e-21], sum to 1.0000
[2019-04-04 10:27:58,434] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5827
[2019-04-04 10:27:58,483] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.283333333333333, 66.66666666666667, 112.6666666666667, 152.3333333333333, 26.0, 25.86916515468575, 0.3641132432542695, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2627400.0000, 
sim time next is 2628000.0000, 
raw observation next is [-5.0, 65.0, 122.5, 183.0, 26.0, 25.84888873727002, 0.3632664000250728, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.32409972299168976, 0.65, 0.4083333333333333, 0.2022099447513812, 0.6666666666666666, 0.6540740614391684, 0.6210888000083576, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3416312], dtype=float32), -0.36186022]. 
=============================================
[2019-04-04 10:27:58,486] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.41996 ]
 [83.20052 ]
 [83.17518 ]
 [83.37676 ]
 [83.583275]], R is [[83.84421539]
 [84.00577545]
 [84.16571808]
 [84.32405853]
 [84.4808197 ]].
[2019-04-04 10:28:03,216] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9724414e-10 6.0407137e-09 8.3221646e-24 1.7158256e-22 1.2563957e-18
 1.0000000e+00 4.3966420e-18], sum to 1.0000
[2019-04-04 10:28:03,243] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0291
[2019-04-04 10:28:03,259] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-14.3, 88.33333333333334, 0.0, 0.0, 26.0, 23.6019375177071, 0.01515930511721441, 0.0, 1.0, 44530.64267967972], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2691600.0000, 
sim time next is 2692200.0000, 
raw observation next is [-14.65, 89.66666666666667, 0.0, 0.0, 26.0, 23.56516295144016, 0.004145370678751694, 0.0, 1.0, 44517.43610821177], 
processed observation next is [1.0, 0.13043478260869565, 0.05678670360110801, 0.8966666666666667, 0.0, 0.0, 0.6666666666666666, 0.4637635792866801, 0.5013817902262506, 0.0, 1.0, 0.2119877909914846], 
reward next is 0.7880, 
noisyNet noise sample is [array([-1.0462891], dtype=float32), -0.5843143]. 
=============================================
[2019-04-04 10:28:06,388] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4352183e-09 3.3351228e-08 1.1854238e-24 3.7495647e-23 5.8835113e-19
 1.0000000e+00 9.1194109e-19], sum to 1.0000
[2019-04-04 10:28:06,390] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7502
[2019-04-04 10:28:06,418] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 70.0, 0.0, 0.0, 26.0, 24.83795776552677, 0.2503804404491389, 0.0, 1.0, 41725.97291368392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2596800.0000, 
sim time next is 2597400.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.88616492037586, 0.2471875592096576, 0.0, 1.0, 41686.42731974442], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5738470766979885, 0.5823958530698858, 0.0, 1.0, 0.19850679676068772], 
reward next is 0.8015, 
noisyNet noise sample is [array([0.12916732], dtype=float32), -0.58351463]. 
=============================================
[2019-04-04 10:28:09,602] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0477249e-09 4.4665320e-08 2.4518496e-23 7.6812065e-22 4.2223422e-18
 1.0000000e+00 8.2619949e-18], sum to 1.0000
[2019-04-04 10:28:09,602] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2089
[2019-04-04 10:28:09,618] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.1, 47.16666666666666, 0.0, 0.0, 26.0, 24.45960986504697, 0.1121176677236142, 0.0, 1.0, 43253.5792858729], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2422200.0000, 
sim time next is 2422800.0000, 
raw observation next is [-6.2, 48.0, 0.0, 0.0, 26.0, 24.40066727376657, 0.1007959137057637, 0.0, 1.0, 43320.60136062266], 
processed observation next is [0.0, 0.043478260869565216, 0.2908587257617729, 0.48, 0.0, 0.0, 0.6666666666666666, 0.5333889394805474, 0.5335986379019212, 0.0, 1.0, 0.20628857790772698], 
reward next is 0.7937, 
noisyNet noise sample is [array([0.79933923], dtype=float32), 0.9558391]. 
=============================================
[2019-04-04 10:28:18,806] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5305890e-12 3.8807937e-10 2.1408201e-29 2.2480602e-27 3.6954909e-23
 1.0000000e+00 5.8112099e-22], sum to 1.0000
[2019-04-04 10:28:18,807] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0125
[2019-04-04 10:28:18,831] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.2500000000000001, 43.0, 232.0, 71.0, 26.0, 25.68301449804298, 0.3192590568281075, 1.0, 1.0, 18682.3102311897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2547000.0000, 
sim time next is 2547600.0000, 
raw observation next is [0.5333333333333334, 41.66666666666667, 229.6666666666667, 62.83333333333334, 26.0, 25.70799034943887, 0.3262546682916667, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4773776546629733, 0.41666666666666674, 0.7655555555555558, 0.06942909760589319, 0.6666666666666666, 0.6423325291199058, 0.6087515560972222, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33156538], dtype=float32), 0.5801726]. 
=============================================
[2019-04-04 10:28:19,981] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6358521e-09 1.4628102e-08 7.0534755e-25 1.5103334e-23 8.5864258e-19
 1.0000000e+00 4.6835028e-19], sum to 1.0000
[2019-04-04 10:28:19,981] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2332
[2019-04-04 10:28:19,997] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.75044950973248, 0.2487181539293922, 0.0, 1.0, 42515.98780911854], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2768400.0000, 
sim time next is 2769000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.7298701848174, 0.2520360118068934, 0.0, 1.0, 42375.69983092202], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.56082251540145, 0.5840120039356311, 0.0, 1.0, 0.20178904681391438], 
reward next is 0.7982, 
noisyNet noise sample is [array([-0.37141022], dtype=float32), 0.5556701]. 
=============================================
[2019-04-04 10:28:20,023] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[75.70759 ]
 [75.8024  ]
 [75.842354]
 [75.84787 ]
 [75.93676 ]], R is [[75.77526855]
 [75.81506348]
 [75.85360718]
 [75.89069366]
 [75.92636108]].
[2019-04-04 10:28:41,520] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2127868e-10 3.3751424e-09 2.2142894e-26 4.9469196e-25 5.6152393e-20
 1.0000000e+00 7.3678993e-20], sum to 1.0000
[2019-04-04 10:28:41,521] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9431
[2019-04-04 10:28:41,540] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.48260353408074, 0.4346205361278719, 0.0, 1.0, 18756.36719757331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2843400.0000, 
sim time next is 2844000.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.49540489787267, 0.432169868648464, 0.0, 1.0, 18752.42857460346], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.6246170748227226, 0.6440566228828214, 0.0, 1.0, 0.08929727892668314], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.90737534], dtype=float32), 0.11323013]. 
=============================================
[2019-04-04 10:28:41,547] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.44187]
 [81.4987 ]
 [81.6198 ]
 [81.57892]
 [81.43519]], R is [[81.49385071]
 [81.58959198]
 [81.68435669]
 [81.70827484]
 [81.65558624]].
[2019-04-04 10:28:56,738] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3336848e-11 3.0585517e-10 8.4466915e-28 4.5197440e-26 2.0150697e-21
 1.0000000e+00 1.5761482e-20], sum to 1.0000
[2019-04-04 10:28:56,738] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0101
[2019-04-04 10:28:56,754] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 67.0, 0.0, 0.0, 26.0, 25.73247613922467, 0.4904372516254664, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3434400.0000, 
sim time next is 3435000.0000, 
raw observation next is [1.833333333333333, 69.0, 0.0, 0.0, 26.0, 25.36164299137058, 0.4461176489254406, 1.0, 1.0, 97268.85813151131], 
processed observation next is [1.0, 0.782608695652174, 0.5133887349953832, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6134702492808817, 0.6487058829751469, 1.0, 1.0, 0.4631850387214824], 
reward next is 0.5368, 
noisyNet noise sample is [array([0.85124886], dtype=float32), 0.80157524]. 
=============================================
[2019-04-04 10:28:56,760] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.405556]
 [83.73059 ]
 [84.40662 ]
 [84.93237 ]
 [85.64378 ]], R is [[83.52017212]
 [83.68497467]
 [83.84812927]
 [84.00965118]
 [84.16955566]].
[2019-04-04 10:28:58,263] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.14730482e-10 1.47141144e-09 7.35038000e-28 1.87323095e-26
 1.25935441e-21 1.00000000e+00 1.33984026e-20], sum to 1.0000
[2019-04-04 10:28:58,265] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1919
[2019-04-04 10:28:58,279] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.43270626839787, 0.4489321576355005, 0.0, 1.0, 50079.02398787718], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3459600.0000, 
sim time next is 3460200.0000, 
raw observation next is [1.0, 79.00000000000001, 0.0, 0.0, 26.0, 25.42104113657188, 0.4438307277997307, 0.0, 1.0, 49728.5877836527], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7900000000000001, 0.0, 0.0, 0.6666666666666666, 0.6184200947143234, 0.6479435759332436, 0.0, 1.0, 0.23680279896977477], 
reward next is 0.7632, 
noisyNet noise sample is [array([0.38942432], dtype=float32), -0.41820553]. 
=============================================
[2019-04-04 10:29:02,569] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.11214887e-13 1.02160835e-10 1.26991833e-30 8.64511433e-29
 9.56794325e-24 1.00000000e+00 8.92562603e-23], sum to 1.0000
[2019-04-04 10:29:02,577] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7056
[2019-04-04 10:29:02,635] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 93.0, 87.5, 134.5, 26.0, 24.80324729708973, 0.297618491782389, 1.0, 1.0, 88820.89384951346], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2881200.0000, 
sim time next is 2881800.0000, 
raw observation next is [1.5, 93.0, 105.0, 156.0, 26.0, 25.08078242628812, 0.3143476209099918, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5041551246537397, 0.93, 0.35, 0.1723756906077348, 0.6666666666666666, 0.5900652021906767, 0.6047825403033306, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39064956], dtype=float32), 0.2982056]. 
=============================================
[2019-04-04 10:29:02,967] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5329929e-10 1.5759406e-09 7.7618936e-27 3.0496641e-24 5.2682723e-20
 1.0000000e+00 5.0498756e-19], sum to 1.0000
[2019-04-04 10:29:02,967] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7853
[2019-04-04 10:29:02,981] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 63.0, 0.0, 0.0, 26.0, 25.37034572430062, 0.479889004063579, 0.0, 1.0, 43547.41441090048], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3540600.0000, 
sim time next is 3541200.0000, 
raw observation next is [-1.666666666666667, 62.0, 0.0, 0.0, 26.0, 25.38350521603598, 0.4762178935621011, 0.0, 1.0, 36889.33122106987], 
processed observation next is [1.0, 1.0, 0.4164358264081256, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6152921013363318, 0.6587392978540337, 0.0, 1.0, 0.17566348200509463], 
reward next is 0.8243, 
noisyNet noise sample is [array([-2.291108], dtype=float32), 2.1906202]. 
=============================================
[2019-04-04 10:29:05,979] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2331048e-12 5.3350255e-11 1.6619131e-29 1.6557063e-28 1.6728145e-22
 1.0000000e+00 2.6623932e-22], sum to 1.0000
[2019-04-04 10:29:05,980] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6474773e-11 4.4729322e-09 2.4172060e-26 1.5312111e-24 2.2043934e-20
 1.0000000e+00 9.1729625e-20], sum to 1.0000
[2019-04-04 10:29:05,991] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3054
[2019-04-04 10:29:05,994] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0127
[2019-04-04 10:29:06,035] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.333333333333333, 71.0, 87.66666666666667, 699.8333333333334, 26.0, 26.29847800326547, 0.7527029670188439, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3252000.0000, 
sim time next is 3252600.0000, 
raw observation next is [-2.5, 71.0, 85.0, 686.0, 26.0, 25.60759702948918, 0.7154558964886827, 1.0, 1.0, 115882.451862587], 
processed observation next is [1.0, 0.6521739130434783, 0.39335180055401664, 0.71, 0.2833333333333333, 0.7580110497237569, 0.6666666666666666, 0.6339664191240985, 0.7384852988295609, 1.0, 1.0, 0.5518211993456523], 
reward next is 0.4482, 
noisyNet noise sample is [array([-1.2689422], dtype=float32), 0.3874417]. 
=============================================
[2019-04-04 10:29:06,050] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.333333333333334, 70.0, 90.0, 466.8333333333333, 26.0, 25.33831806462065, 0.4853902807997727, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3573600.0000, 
sim time next is 3574200.0000, 
raw observation next is [-6.166666666666666, 70.0, 92.0, 508.6666666666666, 26.0, 25.73467235693603, 0.5157287852537477, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.29178208679593726, 0.7, 0.30666666666666664, 0.5620626151012891, 0.6666666666666666, 0.6445560297446692, 0.6719095950845825, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.82290643], dtype=float32), -0.51429504]. 
=============================================
[2019-04-04 10:29:09,227] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.7316672e-10 1.0073163e-08 8.8076419e-25 3.6849473e-23 2.1957752e-19
 1.0000000e+00 1.2224226e-18], sum to 1.0000
[2019-04-04 10:29:09,227] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1482
[2019-04-04 10:29:09,241] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.83977598718884, 0.3093039836941421, 0.0, 1.0, 43908.25010039363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3294000.0000, 
sim time next is 3294600.0000, 
raw observation next is [-8.15, 77.0, 0.0, 0.0, 26.0, 24.79025885486099, 0.2962520335436986, 0.0, 1.0, 43930.81620955041], 
processed observation next is [1.0, 0.13043478260869565, 0.2368421052631579, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5658549045717493, 0.5987506778478996, 0.0, 1.0, 0.209194362902621], 
reward next is 0.7908, 
noisyNet noise sample is [array([-0.38617417], dtype=float32), -1.3325698]. 
=============================================
[2019-04-04 10:29:11,520] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7504888e-11 6.9367345e-10 1.7795157e-27 1.4367444e-25 6.9194910e-21
 1.0000000e+00 1.8265515e-20], sum to 1.0000
[2019-04-04 10:29:11,520] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0176
[2019-04-04 10:29:11,544] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 47.33333333333334, 64.5, 529.8333333333333, 26.0, 26.5730525684216, 0.6803965637273612, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3342000.0000, 
sim time next is 3342600.0000, 
raw observation next is [-2.0, 48.0, 60.0, 501.0, 26.0, 26.65521768856553, 0.6754748042776946, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.48, 0.2, 0.5535911602209945, 0.6666666666666666, 0.7212681407137941, 0.7251582680925649, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36400235], dtype=float32), -0.121694535]. 
=============================================
[2019-04-04 10:29:16,888] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4239541e-11 8.5228588e-11 7.1010611e-28 8.7033275e-27 2.1136690e-21
 1.0000000e+00 8.0755519e-21], sum to 1.0000
[2019-04-04 10:29:16,890] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2719
[2019-04-04 10:29:16,920] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 69.0, 0.0, 0.0, 26.0, 25.43693441215918, 0.4771214798754467, 1.0, 1.0, 51263.23555105139], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3784800.0000, 
sim time next is 3785400.0000, 
raw observation next is [-2.0, 68.0, 0.0, 0.0, 26.0, 25.33237097731445, 0.4634688676797418, 0.0, 1.0, 34870.15864389407], 
processed observation next is [1.0, 0.8260869565217391, 0.40720221606648205, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6110309147762042, 0.6544896225599139, 0.0, 1.0, 0.16604837449473367], 
reward next is 0.8340, 
noisyNet noise sample is [array([0.490868], dtype=float32), -0.07604293]. 
=============================================
[2019-04-04 10:29:27,233] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9089153e-12 4.7815654e-11 1.2713476e-31 3.2371789e-29 4.2254896e-23
 1.0000000e+00 5.3092194e-23], sum to 1.0000
[2019-04-04 10:29:27,234] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3959
[2019-04-04 10:29:27,264] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 100.0, 16.33333333333333, 179.8333333333333, 26.0, 27.44278731385767, 0.6862506867427248, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3172800.0000, 
sim time next is 3173400.0000, 
raw observation next is [6.0, 100.0, 8.0, 116.0, 26.0, 27.24313973352098, 0.848160303632992, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.02666666666666667, 0.1281767955801105, 0.6666666666666666, 0.7702616444600817, 0.7827201012109973, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.66462344], dtype=float32), 0.23771189]. 
=============================================
[2019-04-04 10:29:29,483] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9268790e-10 7.5673068e-09 1.3450309e-23 8.9869868e-23 2.4948506e-18
 1.0000000e+00 6.0953839e-18], sum to 1.0000
[2019-04-04 10:29:29,484] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3336
[2019-04-04 10:29:29,506] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 49.0, 0.0, 0.0, 26.0, 25.41331197138265, 0.4958125170503582, 0.0, 1.0, 60129.24111769332], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3967200.0000, 
sim time next is 3967800.0000, 
raw observation next is [-8.166666666666668, 49.66666666666667, 0.0, 0.0, 26.0, 25.40039873450786, 0.4470061553218342, 0.0, 1.0, 57014.64574861499], 
processed observation next is [1.0, 0.9565217391304348, 0.2363804247460757, 0.4966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6166998945423217, 0.6490020517739447, 0.0, 1.0, 0.2714983130886428], 
reward next is 0.7285, 
noisyNet noise sample is [array([-0.57630956], dtype=float32), 1.681738]. 
=============================================
[2019-04-04 10:29:44,424] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2279343e-12 2.4107796e-10 1.4727105e-29 2.7187527e-27 1.1774975e-22
 1.0000000e+00 2.3713171e-22], sum to 1.0000
[2019-04-04 10:29:44,425] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2654
[2019-04-04 10:29:44,438] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 60.00000000000001, 115.8333333333333, 814.1666666666666, 26.0, 26.51700728635396, 0.6314061927602029, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3842400.0000, 
sim time next is 3843000.0000, 
raw observation next is [-1.0, 60.0, 117.0, 822.0, 26.0, 26.49445089932291, 0.6394980011668578, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.39, 0.9082872928176795, 0.6666666666666666, 0.7078709082769091, 0.7131660003889526, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.58230114], dtype=float32), -0.4878741]. 
=============================================
[2019-04-04 10:29:44,451] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[88.11854]
 [88.10021]
 [88.11698]
 [88.15718]
 [88.22678]], R is [[88.27921295]
 [88.39642334]
 [88.5124588 ]
 [88.62733459]
 [88.74105835]].
[2019-04-04 10:29:47,653] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0362698e-11 1.1135177e-09 1.0679085e-27 3.0604993e-26 1.9574807e-21
 1.0000000e+00 7.4228506e-21], sum to 1.0000
[2019-04-04 10:29:47,658] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8238
[2019-04-04 10:29:47,675] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 35.5, 23.0, 57.0, 26.0, 26.63649050326467, 0.6274849456486477, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4123800.0000, 
sim time next is 4124400.0000, 
raw observation next is [3.0, 35.0, 19.16666666666667, 47.5, 26.0, 26.50330770650427, 0.6804268519838507, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.35, 0.0638888888888889, 0.052486187845303865, 0.6666666666666666, 0.7086089755420225, 0.7268089506612836, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.76998246], dtype=float32), -1.0001804]. 
=============================================
[2019-04-04 10:29:49,115] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7233799e-10 6.0110494e-09 6.3859806e-26 4.2810736e-24 2.8403195e-19
 1.0000000e+00 4.7750386e-19], sum to 1.0000
[2019-04-04 10:29:49,115] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4291
[2019-04-04 10:29:49,143] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 32.5, 119.0, 822.0, 26.0, 25.11500641410059, 0.3811733805558144, 0.0, 1.0, 9350.951144204759], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4188600.0000, 
sim time next is 4189200.0000, 
raw observation next is [0.3333333333333333, 31.66666666666667, 118.8333333333333, 826.1666666666666, 26.0, 25.07100153552656, 0.3800239784868635, 0.0, 1.0, 19313.8941631898], 
processed observation next is [0.0, 0.4782608695652174, 0.4718374884579871, 0.3166666666666667, 0.396111111111111, 0.9128913443830571, 0.6666666666666666, 0.5892501279605465, 0.6266746594956212, 0.0, 1.0, 0.0919709245866181], 
reward next is 0.9080, 
noisyNet noise sample is [array([2.3876889], dtype=float32), 0.30696383]. 
=============================================
[2019-04-04 10:29:56,109] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.99966454e-12 2.53910337e-10 5.88601807e-29 1.18196385e-26
 6.77426919e-23 1.00000000e+00 8.23413265e-22], sum to 1.0000
[2019-04-04 10:29:56,113] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5315
[2019-04-04 10:29:56,145] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 79.16666666666666, 63.66666666666666, 0.0, 26.0, 26.27854590195335, 0.6133649160351088, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4463400.0000, 
sim time next is 4464000.0000, 
raw observation next is [0.0, 78.0, 60.0, 0.0, 26.0, 26.27342764758215, 0.6133411032652768, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.2, 0.0, 0.6666666666666666, 0.6894523039651791, 0.7044470344217589, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7056348], dtype=float32), 0.060686797]. 
=============================================
[2019-04-04 10:29:56,149] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[87.04133 ]
 [87.32175 ]
 [87.564476]
 [87.82658 ]
 [88.14731 ]], R is [[86.97377014]
 [87.10403442]
 [87.23299408]
 [87.36066437]
 [87.48706055]].
[2019-04-04 10:30:02,940] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1519753e-11 4.1784617e-11 3.7114172e-28 1.2599566e-26 5.6602893e-22
 1.0000000e+00 3.9812671e-21], sum to 1.0000
[2019-04-04 10:30:02,940] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8670
[2019-04-04 10:30:02,980] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.13578633341375, 0.4486768863163771, 1.0, 1.0, 64232.63470234593], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3787200.0000, 
sim time next is 3787800.0000, 
raw observation next is [-2.166666666666667, 66.0, 0.0, 0.0, 26.0, 25.11973373499973, 0.4516500623100326, 0.0, 1.0, 59710.13265600373], 
processed observation next is [1.0, 0.8695652173913043, 0.4025854108956602, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5933111445833109, 0.6505500207700109, 0.0, 1.0, 0.28433396502858915], 
reward next is 0.7157, 
noisyNet noise sample is [array([0.06842356], dtype=float32), -0.78731453]. 
=============================================
[2019-04-04 10:30:13,093] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2417577e-10 2.0649391e-09 1.1932158e-28 4.5114413e-26 5.5666097e-21
 1.0000000e+00 2.9625908e-21], sum to 1.0000
[2019-04-04 10:30:13,093] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6912
[2019-04-04 10:30:13,113] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.8, 65.0, 0.0, 0.0, 26.0, 25.91182146870536, 0.6263789036828692, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4410000.0000, 
sim time next is 4410600.0000, 
raw observation next is [6.683333333333334, 65.16666666666667, 0.0, 0.0, 26.0, 25.91900605327369, 0.6125410570021578, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6477377654662975, 0.6516666666666667, 0.0, 0.0, 0.6666666666666666, 0.6599171711061409, 0.7041803523340526, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32905805], dtype=float32), 0.918538]. 
=============================================
[2019-04-04 10:30:25,897] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.1327784e-11 7.0015888e-10 7.7787602e-27 2.3397651e-25 1.2639464e-20
 1.0000000e+00 5.2298672e-20], sum to 1.0000
[2019-04-04 10:30:25,897] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0013
[2019-04-04 10:30:25,913] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.28169511487704, 0.4678858404894612, 0.0, 1.0, 48043.95638788949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4575600.0000, 
sim time next is 4576200.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.34722260244308, 0.4732377725825255, 0.0, 1.0, 41131.00736144852], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.61226855020359, 0.6577459241941751, 0.0, 1.0, 0.19586193981642153], 
reward next is 0.8041, 
noisyNet noise sample is [array([0.7496275], dtype=float32), 0.9989044]. 
=============================================
[2019-04-04 10:30:31,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:30:31,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:30:31,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run29
[2019-04-04 10:30:31,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:30:31,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:30:31,632] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run29
[2019-04-04 10:30:33,188] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2526239e-12 1.3175592e-09 1.8270438e-30 3.0151188e-27 2.3557091e-23
 1.0000000e+00 3.5985027e-22], sum to 1.0000
[2019-04-04 10:30:33,188] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4607
[2019-04-04 10:30:33,242] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 74.0, 46.0, 245.5, 26.0, 25.34876935757393, 0.3800988838781341, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4348800.0000, 
sim time next is 4349400.0000, 
raw observation next is [3.55, 71.16666666666667, 61.33333333333334, 327.3333333333334, 26.0, 25.43395117555718, 0.3994086357956284, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5609418282548477, 0.7116666666666667, 0.20444444444444448, 0.36169429097605904, 0.6666666666666666, 0.6194959312964317, 0.6331362119318761, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6792512], dtype=float32), -1.3321502]. 
=============================================
[2019-04-04 10:30:33,900] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:30:33,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:30:33,913] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run29
[2019-04-04 10:30:37,558] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:30:37,559] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:30:37,562] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run29
[2019-04-04 10:30:41,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:30:41,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:30:41,354] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run29
[2019-04-04 10:30:42,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:30:42,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:30:42,282] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run29
[2019-04-04 10:30:43,022] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-04 10:30:43,056] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:30:43,056] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:30:43,070] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:30:43,070] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:30:43,073] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run39
[2019-04-04 10:30:43,158] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run39
[2019-04-04 10:30:43,235] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:30:43,236] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:30:43,250] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run39
[2019-04-04 10:31:52,828] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.2656855], dtype=float32), 0.30886203]
[2019-04-04 10:31:52,829] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [15.03400999833333, 100.0, 77.72649395333335, 0.0, 26.0, 25.53083848071888, 0.5936693001747767, 0.0, 0.0, 0.0]
[2019-04-04 10:31:52,830] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:31:52,831] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.2844020e-11 1.5725866e-09 1.2771193e-28 1.9452233e-26 6.5848878e-22
 1.0000000e+00 7.6902408e-21], sampled 0.4845378446536539
[2019-04-04 10:32:00,091] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2656855], dtype=float32), 0.30886203]
[2019-04-04 10:32:00,091] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [5.7, 86.5, 0.0, 0.0, 26.0, 25.42474815836709, 0.4979445501392055, 1.0, 1.0, 0.0]
[2019-04-04 10:32:00,091] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:32:00,093] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.8565054e-12 3.4661107e-10 9.1682977e-30 1.3738291e-27 9.2244958e-23
 1.0000000e+00 1.0218345e-21], sampled 0.5081139720195599
[2019-04-04 10:33:25,980] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.2656855], dtype=float32), 0.30886203]
[2019-04-04 10:33:25,980] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.0, 79.00000000000001, 0.0, 0.0, 26.0, 25.03367236499799, 0.4427914997126339, 1.0, 1.0, 18704.54644224883]
[2019-04-04 10:33:25,980] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 10:33:25,981] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.2607933e-11 4.2899481e-10 8.6162783e-28 7.5273958e-26 3.4239332e-21
 1.0000000e+00 2.3484987e-20], sampled 0.6096340408048946
[2019-04-04 10:33:40,400] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.2656855], dtype=float32), 0.30886203]
[2019-04-04 10:33:40,400] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.0, 56.33333333333334, 76.83333333333334, 415.5000000000001, 26.0, 25.66757814434812, 0.4581106897049016, 1.0, 1.0, 9370.64502307263]
[2019-04-04 10:33:40,400] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 10:33:40,401] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.7002562e-12 9.3922248e-10 2.3852633e-28 2.2404249e-26 1.1717178e-21
 1.0000000e+00 5.2133119e-21], sampled 0.5256366079122682
[2019-04-04 10:33:44,757] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 10:34:20,414] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 10:34:28,164] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 10:34:29,215] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 3800000, evaluation results [3800000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 10:34:35,555] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4793082e-12 9.4101865e-11 1.1334857e-30 1.2073548e-28 1.8685876e-23
 1.0000000e+00 3.6493453e-23], sum to 1.0000
[2019-04-04 10:34:35,556] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7587
[2019-04-04 10:34:35,619] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666666, 57.33333333333334, 134.8333333333333, 724.0, 26.0, 26.55359774650552, 0.6613902107120356, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4616400.0000, 
sim time next is 4617000.0000, 
raw observation next is [1.0, 56.0, 129.0, 767.0, 26.0, 26.64746032834829, 0.6701905047337519, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.56, 0.43, 0.8475138121546961, 0.6666666666666666, 0.7206216940290243, 0.7233968349112506, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7108878], dtype=float32), -1.1774365]. 
=============================================
[2019-04-04 10:34:35,692] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[92.53885 ]
 [92.66049 ]
 [92.789276]
 [92.83137 ]
 [92.78682 ]], R is [[92.45070648]
 [92.52619934]
 [92.60093689]
 [92.67492676]
 [92.74817657]].
[2019-04-04 10:34:38,067] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5020899e-10 4.2948707e-09 1.9969432e-27 2.1904979e-25 6.1981447e-21
 1.0000000e+00 6.3250508e-20], sum to 1.0000
[2019-04-04 10:34:38,067] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3618
[2019-04-04 10:34:38,121] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 86.0, 0.0, 0.0, 26.0, 24.54659692194853, 0.1947913318725229, 0.0, 1.0, 42206.84572453996], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 61200.0000, 
sim time next is 61800.0000, 
raw observation next is [5.316666666666667, 86.5, 0.0, 0.0, 26.0, 24.59588953466572, 0.1949139555179409, 0.0, 1.0, 18737.61210500876], 
processed observation next is [0.0, 0.7391304347826086, 0.6098799630655587, 0.865, 0.0, 0.0, 0.6666666666666666, 0.5496574612221433, 0.5649713185059803, 0.0, 1.0, 0.08922672430956552], 
reward next is 0.9108, 
noisyNet noise sample is [array([1.6682585], dtype=float32), 0.4800609]. 
=============================================
[2019-04-04 10:34:38,608] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9600060e-10 2.8469496e-10 2.7292477e-27 1.0821394e-25 5.6396875e-21
 1.0000000e+00 7.2449284e-21], sum to 1.0000
[2019-04-04 10:34:38,608] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7869
[2019-04-04 10:34:38,721] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.316666666666667, 86.5, 0.0, 0.0, 26.0, 24.59588953466572, 0.1949139555179409, 0.0, 1.0, 18737.61210500876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 61800.0000, 
sim time next is 62400.0000, 
raw observation next is [5.133333333333334, 87.0, 0.0, 0.0, 26.0, 24.5913671190604, 0.1902323921188069, 0.0, 1.0, 33482.10949838657], 
processed observation next is [0.0, 0.7391304347826086, 0.6048014773776548, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5492805932550334, 0.5634107973729356, 0.0, 1.0, 0.15943861665898368], 
reward next is 0.8406, 
noisyNet noise sample is [array([-1.0097343], dtype=float32), -0.36077666]. 
=============================================
[2019-04-04 10:34:50,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:34:50,800] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:34:50,844] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run29
[2019-04-04 10:34:54,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:34:54,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:34:54,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run29
[2019-04-04 10:34:55,212] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:34:55,212] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:34:55,216] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run29
[2019-04-04 10:34:58,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:34:58,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:34:58,116] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run29
[2019-04-04 10:35:01,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:35:01,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:35:01,019] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run29
[2019-04-04 10:35:01,310] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4283888e-10 6.1262990e-09 3.8413776e-26 5.3871676e-25 7.6592989e-20
 1.0000000e+00 1.1641939e-19], sum to 1.0000
[2019-04-04 10:35:01,310] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8965
[2019-04-04 10:35:01,374] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 42.0, 162.0, 795.6666666666667, 26.0, 25.07793484478888, 0.4279364307060512, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4796400.0000, 
sim time next is 4797000.0000, 
raw observation next is [1.5, 41.5, 170.0, 787.0, 26.0, 25.08925776170359, 0.4318014553906663, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5041551246537397, 0.415, 0.5666666666666667, 0.8696132596685083, 0.6666666666666666, 0.5907714801419658, 0.6439338184635554, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8267491], dtype=float32), 0.1260855]. 
=============================================
[2019-04-04 10:35:01,480] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[80.492226]
 [80.38202 ]
 [80.32646 ]
 [80.21238 ]
 [80.181786]], R is [[80.82211304]
 [81.01389313]
 [81.20375824]
 [81.34718323]
 [81.43533325]].
[2019-04-04 10:35:03,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:35:03,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:35:03,654] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run29
[2019-04-04 10:35:37,063] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:35:37,063] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:35:37,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run29
[2019-04-04 10:35:37,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:35:37,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:35:37,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run29
[2019-04-04 10:35:44,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:35:44,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:35:44,733] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run29
[2019-04-04 10:35:55,794] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2238646e-10 4.7468580e-09 6.6159952e-26 8.7451077e-24 5.8741146e-19
 1.0000000e+00 1.4264771e-18], sum to 1.0000
[2019-04-04 10:35:55,795] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7018
[2019-04-04 10:35:55,861] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 66.0, 123.3333333333333, 34.00000000000001, 26.0, 25.05340661673108, 0.2119245111954272, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 640200.0000, 
sim time next is 640800.0000, 
raw observation next is [-3.9, 65.0, 117.5, 25.5, 26.0, 24.99688492800137, 0.1926373777704105, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3545706371191136, 0.65, 0.39166666666666666, 0.0281767955801105, 0.6666666666666666, 0.5830737440001142, 0.5642124592568035, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3152696], dtype=float32), 2.1090832]. 
=============================================
[2019-04-04 10:36:00,187] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:36:00,188] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:36:00,199] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run29
[2019-04-04 10:36:01,108] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9695849e-10 6.9294122e-09 1.6554878e-25 1.3527545e-23 1.7132251e-19
 1.0000000e+00 6.5874860e-19], sum to 1.0000
[2019-04-04 10:36:01,114] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8440
[2019-04-04 10:36:01,158] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.383333333333333, 75.83333333333333, 0.0, 0.0, 26.0, 24.26170349965273, 0.03987847385401653, 0.0, 1.0, 41568.54109014428], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 708600.0000, 
sim time next is 709200.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.26798432594072, 0.03811557802216782, 0.0, 1.0, 41565.92977841769], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5223320271617267, 0.5127051926740559, 0.0, 1.0, 0.19793299894484614], 
reward next is 0.8021, 
noisyNet noise sample is [array([0.09185702], dtype=float32), -0.054004982]. 
=============================================
[2019-04-04 10:36:01,647] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2371151e-10 5.9663532e-09 2.8158434e-25 1.3729517e-23 1.8163523e-19
 1.0000000e+00 4.5134051e-18], sum to 1.0000
[2019-04-04 10:36:01,647] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8703
[2019-04-04 10:36:01,670] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.28855333315888, 0.06225539594690893, 0.0, 1.0, 41178.19235461151], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 699600.0000, 
sim time next is 700200.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.29415073382472, 0.06449416752323411, 0.0, 1.0, 41219.2805520627], 
processed observation next is [1.0, 0.08695652173913043, 0.368421052631579, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5245125611520599, 0.5214980558410781, 0.0, 1.0, 0.19628228834315573], 
reward next is 0.8037, 
noisyNet noise sample is [array([-0.45338482], dtype=float32), 0.8249635]. 
=============================================
[2019-04-04 10:36:03,887] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.16835396e-11 1.28957067e-10 1.24901107e-28 1.30644949e-26
 5.06605615e-22 1.00000000e+00 1.15220191e-21], sum to 1.0000
[2019-04-04 10:36:03,887] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8028
[2019-04-04 10:36:03,900] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1666666666666667, 46.33333333333334, 80.83333333333334, 600.1666666666666, 26.0, 25.93577638798653, 0.4544810173413052, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 744000.0000, 
sim time next is 744600.0000, 
raw observation next is [0.08333333333333331, 46.66666666666667, 81.66666666666667, 486.3333333333334, 26.0, 25.97542258725878, 0.4441193111462207, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4649122807017544, 0.46666666666666673, 0.27222222222222225, 0.5373848987108657, 0.6666666666666666, 0.6646185489382317, 0.6480397703820736, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04825953], dtype=float32), -1.7590665]. 
=============================================
[2019-04-04 10:36:08,012] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.7140314e-10 8.7914840e-09 1.6014543e-24 5.0691082e-23 1.1186344e-18
 1.0000000e+00 1.2235889e-18], sum to 1.0000
[2019-04-04 10:36:08,012] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8393
[2019-04-04 10:36:08,025] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 72.0, 0.0, 0.0, 26.0, 23.99171389074302, 0.06186535718369105, 0.0, 1.0, 41501.46920167027], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 786000.0000, 
sim time next is 786600.0000, 
raw observation next is [-7.8, 72.5, 0.0, 0.0, 26.0, 23.95357094598801, 0.05416769340211649, 0.0, 1.0, 41473.17649680252], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.725, 0.0, 0.0, 0.6666666666666666, 0.4961309121656674, 0.5180558978007055, 0.0, 1.0, 0.1974913166514406], 
reward next is 0.8025, 
noisyNet noise sample is [array([-0.4938425], dtype=float32), 0.7489982]. 
=============================================
[2019-04-04 10:36:24,232] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6878382e-11 1.3153720e-09 8.3435174e-26 2.3016033e-24 3.4260645e-20
 1.0000000e+00 8.1060191e-20], sum to 1.0000
[2019-04-04 10:36:24,234] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7265
[2019-04-04 10:36:24,289] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 58.00000000000001, 97.83333333333333, 62.16666666666667, 26.0, 24.90291658099656, 0.2274834089734812, 0.0, 1.0, 41806.55729296053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 656400.0000, 
sim time next is 657000.0000, 
raw observation next is [-0.8999999999999999, 57.0, 81.0, 56.0, 26.0, 24.88794578405737, 0.2247426428662101, 0.0, 1.0, 45935.80554375175], 
processed observation next is [0.0, 0.6086956521739131, 0.43767313019390586, 0.57, 0.27, 0.061878453038674036, 0.6666666666666666, 0.5739954820047807, 0.5749142142887367, 0.0, 1.0, 0.2187419311607226], 
reward next is 0.7813, 
noisyNet noise sample is [array([0.50355905], dtype=float32), -0.85191303]. 
=============================================
[2019-04-04 10:36:24,294] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.33552 ]
 [78.967735]
 [79.56487 ]
 [80.00274 ]
 [80.30808 ]], R is [[77.6545105 ]
 [77.67888641]
 [77.73810577]
 [77.78482056]
 [77.88162994]].
[2019-04-04 10:36:24,311] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6717043e-11 7.2449685e-10 9.4618260e-29 9.6005404e-27 3.1768428e-22
 1.0000000e+00 3.9132836e-21], sum to 1.0000
[2019-04-04 10:36:24,311] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0335
[2019-04-04 10:36:24,373] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 76.0, 9.666666666666664, 0.0, 26.0, 25.49463964297966, 0.2790805702847839, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 719400.0000, 
sim time next is 720000.0000, 
raw observation next is [-2.3, 76.0, 14.5, 0.0, 26.0, 25.6168682738347, 0.2850177096609972, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.04833333333333333, 0.0, 0.6666666666666666, 0.6347390228195584, 0.5950059032203324, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7202437], dtype=float32), -0.31728956]. 
=============================================
[2019-04-04 10:36:24,375] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[86.80249 ]
 [85.63378 ]
 [84.20685 ]
 [81.45377 ]
 [78.371544]], R is [[87.75972748]
 [87.88213348]
 [88.00331116]
 [87.23892975]
 [86.40264893]].
[2019-04-04 10:36:26,401] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4096715e-12 1.3018278e-11 3.3519288e-33 5.7695247e-31 2.5875929e-25
 1.0000000e+00 1.5375676e-24], sum to 1.0000
[2019-04-04 10:36:26,407] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4923
[2019-04-04 10:36:26,427] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 25.99844262222411, 0.5712475465055048, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1015800.0000, 
sim time next is 1016400.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.95749834560334, 0.5418030819013118, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6631248621336118, 0.6806010273004373, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5526799], dtype=float32), -0.15219933]. 
=============================================
[2019-04-04 10:36:29,738] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.4930056e-12 8.1599151e-11 3.3990222e-30 7.1625729e-28 2.3830059e-22
 1.0000000e+00 3.7246046e-22], sum to 1.0000
[2019-04-04 10:36:29,739] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1109
[2019-04-04 10:36:29,759] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [19.11666666666667, 49.16666666666667, 54.0, 0.0, 26.0, 27.72136614412108, 0.9971248320740161, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1095000.0000, 
sim time next is 1095600.0000, 
raw observation next is [18.83333333333333, 49.33333333333334, 44.5, 0.0, 26.0, 27.80882912825913, 1.00730801300034, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.9843028624192061, 0.4933333333333334, 0.14833333333333334, 0.0, 0.6666666666666666, 0.8174024273549275, 0.83576933766678, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.52836037], dtype=float32), 0.5825905]. 
=============================================
[2019-04-04 10:36:38,007] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3547981e-10 9.9336472e-10 1.5745020e-27 2.8344971e-26 3.3858768e-21
 1.0000000e+00 9.1274413e-21], sum to 1.0000
[2019-04-04 10:36:38,007] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1304
[2019-04-04 10:36:38,055] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 26.0, 24.97026228076015, 0.3169965935311123, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 562800.0000, 
sim time next is 563400.0000, 
raw observation next is [-1.0, 80.5, 130.0, 396.0, 26.0, 24.96635215800529, 0.3143935647424858, 0.0, 1.0, 18730.72778200659], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.805, 0.43333333333333335, 0.4375690607734807, 0.6666666666666666, 0.5805293465004407, 0.6047978549141619, 0.0, 1.0, 0.089193941819079], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.17344856], dtype=float32), -1.1829003]. 
=============================================
[2019-04-04 10:36:40,282] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7312991e-11 2.2418624e-10 6.2545260e-30 1.9478287e-27 1.0949513e-22
 1.0000000e+00 9.7914132e-22], sum to 1.0000
[2019-04-04 10:36:40,285] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8826
[2019-04-04 10:36:40,299] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.45001321078474, 0.5864774353251553, 0.0, 1.0, 56249.90509535497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1287000.0000, 
sim time next is 1287600.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.43356038561261, 0.5900047325583417, 0.0, 1.0, 53318.42179421529], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6194633654677174, 0.6966682441861138, 0.0, 1.0, 0.25389724663912044], 
reward next is 0.7461, 
noisyNet noise sample is [array([1.0231642], dtype=float32), -0.5666572]. 
=============================================
[2019-04-04 10:36:44,340] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2151227e-10 3.5525685e-09 2.3374279e-25 8.5392363e-24 7.0521212e-20
 1.0000000e+00 3.7792256e-19], sum to 1.0000
[2019-04-04 10:36:44,341] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9862
[2019-04-04 10:36:44,415] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.816666666666667, 61.66666666666666, 96.66666666666667, 58.66666666666666, 26.0, 24.85477696972632, 0.2199823548603955, 0.0, 1.0, 42598.06916823611], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 647400.0000, 
sim time next is 648000.0000, 
raw observation next is [-2.7, 61.0, 100.5, 69.0, 26.0, 24.87410640857608, 0.2233088664916302, 0.0, 1.0, 32013.46867953728], 
processed observation next is [0.0, 0.5217391304347826, 0.38781163434903054, 0.61, 0.335, 0.07624309392265194, 0.6666666666666666, 0.5728422007146733, 0.5744362888305434, 0.0, 1.0, 0.15244508895017753], 
reward next is 0.8476, 
noisyNet noise sample is [array([0.88992155], dtype=float32), 0.44507274]. 
=============================================
[2019-04-04 10:36:44,419] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.208466]
 [78.2155  ]
 [78.18536 ]
 [78.097786]
 [78.100746]], R is [[78.22574615]
 [78.24063873]
 [78.2101059 ]
 [78.15316772]
 [78.122612  ]].
[2019-04-04 10:36:51,812] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9717216e-12 9.8505870e-11 2.8056609e-30 4.1919242e-28 1.0400733e-23
 1.0000000e+00 2.7773255e-22], sum to 1.0000
[2019-04-04 10:36:51,816] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7112
[2019-04-04 10:36:51,827] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.8, 69.33333333333333, 0.0, 0.0, 26.0, 25.74895419666925, 0.6625393549980526, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1122000.0000, 
sim time next is 1122600.0000, 
raw observation next is [11.7, 70.16666666666667, 0.0, 0.0, 26.0, 25.75656444253736, 0.6588107991078515, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.7867036011080333, 0.7016666666666667, 0.0, 0.0, 0.6666666666666666, 0.6463803702114467, 0.7196035997026171, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.95498675], dtype=float32), -1.4378214]. 
=============================================
[2019-04-04 10:36:53,196] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.71204813e-13 1.06525677e-10 1.67839416e-30 8.65342894e-29
 1.23917616e-23 1.00000000e+00 5.77190339e-23], sum to 1.0000
[2019-04-04 10:36:53,196] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1403
[2019-04-04 10:36:53,229] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.9, 100.0, 47.0, 0.0, 26.0, 25.95014959357743, 0.5192077025943368, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1503000.0000, 
sim time next is 1503600.0000, 
raw observation next is [2.0, 100.0, 51.33333333333334, 0.0, 26.0, 26.0353517813452, 0.5281498871276882, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.518005540166205, 1.0, 0.17111111111111113, 0.0, 0.6666666666666666, 0.6696126484454332, 0.6760499623758961, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9230767], dtype=float32), -1.5725318]. 
=============================================
[2019-04-04 10:36:54,038] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3391221e-09 5.1403166e-09 1.1870659e-25 5.7387927e-24 7.6880738e-20
 1.0000000e+00 1.2062008e-18], sum to 1.0000
[2019-04-04 10:36:54,039] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6774
[2019-04-04 10:36:54,047] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 26.0, 24.7075252289686, 0.4033742588871501, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1192200.0000, 
sim time next is 1192800.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 26.0, 24.68305734964438, 0.3985045341895923, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5569214458036983, 0.6328348447298641, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6084359], dtype=float32), 0.45460063]. 
=============================================
[2019-04-04 10:37:18,005] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1177522e-09 4.4780371e-09 5.6813257e-26 2.0151205e-24 9.8164806e-20
 1.0000000e+00 9.1205542e-20], sum to 1.0000
[2019-04-04 10:37:18,005] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8532
[2019-04-04 10:37:18,011] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.3, 65.0, 163.0, 0.0, 26.0, 25.06707606266914, 0.4990339334045396, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1170600.0000, 
sim time next is 1171200.0000, 
raw observation next is [18.3, 65.0, 161.0, 0.0, 26.0, 25.05552281551257, 0.4982244114122328, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.5366666666666666, 0.0, 0.6666666666666666, 0.5879602346260476, 0.6660748038040776, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3221055], dtype=float32), 1.2936206]. 
=============================================
[2019-04-04 10:37:21,341] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2446259e-13 6.2132291e-12 1.1770429e-31 2.2303412e-30 4.9538123e-25
 1.0000000e+00 9.6927734e-24], sum to 1.0000
[2019-04-04 10:37:21,358] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5626
[2019-04-04 10:37:21,375] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 49.0, 165.6666666666667, 0.0, 26.0, 27.23319574438889, 0.8450324440777366, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1605000.0000, 
sim time next is 1605600.0000, 
raw observation next is [13.8, 49.0, 160.5, 0.0, 26.0, 27.3023873323706, 0.8474252623866438, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.844875346260388, 0.49, 0.535, 0.0, 0.6666666666666666, 0.7751989443642167, 0.7824750874622146, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7669417], dtype=float32), 1.0177015]. 
=============================================
[2019-04-04 10:37:22,074] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6558096e-11 8.3713220e-10 2.5514025e-27 3.2879254e-25 1.0859716e-20
 1.0000000e+00 2.0902966e-20], sum to 1.0000
[2019-04-04 10:37:22,080] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0233
[2019-04-04 10:37:22,118] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.733333333333333, 64.0, 152.0, 0.6666666666666665, 26.0, 25.44677164183246, 0.3166039548937208, 1.0, 1.0, 23031.33955246952], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1948800.0000, 
sim time next is 1949400.0000, 
raw observation next is [-3.65, 63.5, 137.0, 0.0, 26.0, 25.54633656131591, 0.3306396502598146, 1.0, 1.0, 23294.03547501322], 
processed observation next is [1.0, 0.5652173913043478, 0.3614958448753463, 0.635, 0.45666666666666667, 0.0, 0.6666666666666666, 0.6288613801096593, 0.6102132167532716, 1.0, 1.0, 0.11092397845244391], 
reward next is 0.8891, 
noisyNet noise sample is [array([-0.07383366], dtype=float32), 1.2431623]. 
=============================================
[2019-04-04 10:37:26,448] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.10094316e-10 4.38769820e-09 3.12275423e-27 6.64640088e-26
 7.84969844e-21 1.00000000e+00 2.82817640e-20], sum to 1.0000
[2019-04-04 10:37:26,450] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9637
[2019-04-04 10:37:26,469] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.2, 91.0, 0.0, 0.0, 26.0, 25.30825494827818, 0.4436592557008449, 0.0, 1.0, 42906.89894739845], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1735200.0000, 
sim time next is 1735800.0000, 
raw observation next is [0.1666666666666667, 91.00000000000001, 0.0, 0.0, 26.0, 25.28754318655522, 0.4401968867700554, 0.0, 1.0, 42919.50722499871], 
processed observation next is [0.0, 0.08695652173913043, 0.4672206832871654, 0.9100000000000001, 0.0, 0.0, 0.6666666666666666, 0.6072952655462682, 0.6467322955900184, 0.0, 1.0, 0.2043786058333272], 
reward next is 0.7956, 
noisyNet noise sample is [array([-0.39029455], dtype=float32), 0.16892888]. 
=============================================
[2019-04-04 10:37:36,678] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7674087e-09 2.6573186e-08 2.5596438e-24 7.8916345e-23 3.9166480e-19
 1.0000000e+00 3.7538246e-18], sum to 1.0000
[2019-04-04 10:37:36,679] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8293
[2019-04-04 10:37:36,710] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 78.83333333333333, 0.0, 0.0, 26.0, 23.83248435741245, 0.02462201038530959, 0.0, 1.0, 43529.76299001243], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2098200.0000, 
sim time next is 2098800.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 23.79955480443573, 0.0190198850787707, 0.0, 1.0, 43439.39486182722], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4832962337029774, 0.5063399616929235, 0.0, 1.0, 0.2068542612467963], 
reward next is 0.7931, 
noisyNet noise sample is [array([1.1080962], dtype=float32), -0.16294906]. 
=============================================
[2019-04-04 10:37:37,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.1739520e-12 7.1053807e-10 5.5813405e-30 5.0247570e-28 2.8210406e-22
 1.0000000e+00 1.1479113e-21], sum to 1.0000
[2019-04-04 10:37:37,986] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7226
[2019-04-04 10:37:37,992] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.06666666666667, 72.33333333333334, 74.33333333333334, 0.0, 26.0, 25.71616325703399, 0.6173800828545112, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1156800.0000, 
sim time next is 1157400.0000, 
raw observation next is [16.35, 71.0, 83.0, 0.0, 26.0, 25.72488114466018, 0.5971124775423157, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.9155124653739612, 0.71, 0.27666666666666667, 0.0, 0.6666666666666666, 0.6437400953883484, 0.6990374925141052, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0364653], dtype=float32), -0.7343852]. 
=============================================
[2019-04-04 10:37:40,485] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3530551e-10 3.1830201e-09 3.5129798e-25 2.7874498e-23 1.8031182e-19
 1.0000000e+00 2.8324765e-18], sum to 1.0000
[2019-04-04 10:37:40,486] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3348
[2019-04-04 10:37:40,512] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.22659278604367, 0.4172574119596196, 0.0, 1.0, 46124.55371933618], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2148600.0000, 
sim time next is 2149200.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.25893521544404, 0.4173768206839886, 0.0, 1.0, 43960.34000687009], 
processed observation next is [1.0, 0.9130434782608695, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.60491126795367, 0.6391256068946629, 0.0, 1.0, 0.2093349524136671], 
reward next is 0.7907, 
noisyNet noise sample is [array([2.680651], dtype=float32), -0.85655546]. 
=============================================
[2019-04-04 10:37:41,080] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9825276e-10 1.0864174e-08 8.8269492e-25 2.3269147e-23 1.9732947e-19
 1.0000000e+00 9.4910466e-19], sum to 1.0000
[2019-04-04 10:37:41,080] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4557
[2019-04-04 10:37:41,137] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 178.0, 62.0, 26.0, 25.0309421813448, 0.2816476034463118, 0.0, 1.0, 28232.63819342128], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1864800.0000, 
sim time next is 1865400.0000, 
raw observation next is [-4.5, 73.0, 180.6666666666667, 69.33333333333334, 26.0, 25.03441142584728, 0.2807187251872977, 0.0, 1.0, 33619.74796497702], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.73, 0.6022222222222223, 0.07661141804788214, 0.6666666666666666, 0.5862009521539401, 0.5935729083957658, 0.0, 1.0, 0.160094037928462], 
reward next is 0.8399, 
noisyNet noise sample is [array([1.3905839], dtype=float32), -0.6641418]. 
=============================================
[2019-04-04 10:37:41,696] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.2701133e-12 7.2105280e-11 3.1543930e-30 9.3687314e-29 3.7238498e-23
 1.0000000e+00 1.6476654e-22], sum to 1.0000
[2019-04-04 10:37:41,696] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2170
[2019-04-04 10:37:41,701] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.033333333333333, 63.0, 0.0, 0.0, 26.0, 25.95199833718124, 0.625449799456209, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1537800.0000, 
sim time next is 1538400.0000, 
raw observation next is [8.666666666666668, 65.0, 0.0, 0.0, 26.0, 25.87761407934062, 0.6090308656574304, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7026777469990768, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6564678399450518, 0.7030102885524768, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36489937], dtype=float32), 1.8033339]. 
=============================================
[2019-04-04 10:37:41,988] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9751760e-09 1.9846672e-08 7.3457978e-24 3.0492848e-21 1.0384629e-17
 1.0000000e+00 3.1796151e-17], sum to 1.0000
[2019-04-04 10:37:41,988] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7077
[2019-04-04 10:37:42,061] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 77.66666666666667, 36.16666666666666, 0.0, 26.0, 25.00871988984074, 0.2638889445349635, 0.0, 1.0, 46279.53455586803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1873200.0000, 
sim time next is 1873800.0000, 
raw observation next is [-4.5, 79.0, 29.0, 0.0, 26.0, 25.04282917204261, 0.2619375271604173, 0.0, 1.0, 26588.13201205072], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.79, 0.09666666666666666, 0.0, 0.6666666666666666, 0.5869024310035508, 0.5873125090534724, 0.0, 1.0, 0.12661015243833676], 
reward next is 0.8734, 
noisyNet noise sample is [array([-0.7067351], dtype=float32), 0.56976014]. 
=============================================
[2019-04-04 10:37:52,656] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9712356e-09 2.8205550e-08 1.3193622e-24 8.2118262e-23 6.2030476e-19
 1.0000000e+00 2.4778824e-18], sum to 1.0000
[2019-04-04 10:37:52,656] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2251
[2019-04-04 10:37:52,718] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.1813748656308, 0.07387266340140326, 0.0, 1.0, 41282.91976536193], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2002200.0000, 
sim time next is 2002800.0000, 
raw observation next is [-5.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.10572648304938, 0.0882830716154946, 0.0, 1.0, 41600.01590546433], 
processed observation next is [1.0, 0.17391304347826086, 0.30193905817174516, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5088105402541151, 0.5294276905384981, 0.0, 1.0, 0.19809531383554443], 
reward next is 0.8019, 
noisyNet noise sample is [array([-1.2896681], dtype=float32), 0.81087416]. 
=============================================
[2019-04-04 10:37:54,794] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.9461198e-10 2.7181553e-09 1.0753578e-25 4.0991402e-24 1.3934740e-20
 1.0000000e+00 3.9116011e-19], sum to 1.0000
[2019-04-04 10:37:54,811] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5276
[2019-04-04 10:37:54,825] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 86.0, 0.0, 0.0, 26.0, 24.98085207279602, 0.306160140129588, 0.0, 1.0, 42287.44611559393], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2070000.0000, 
sim time next is 2070600.0000, 
raw observation next is [-4.5, 86.83333333333334, 0.0, 0.0, 26.0, 24.92562742319028, 0.295955352989227, 0.0, 1.0, 42398.85750771189], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.8683333333333334, 0.0, 0.0, 0.6666666666666666, 0.5771356185991902, 0.5986517843297423, 0.0, 1.0, 0.2018993214652947], 
reward next is 0.7981, 
noisyNet noise sample is [array([0.16982216], dtype=float32), -0.7275717]. 
=============================================
[2019-04-04 10:38:06,358] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.1299906e-09 2.4885043e-08 8.7424058e-24 7.4220820e-22 1.6962346e-17
 1.0000000e+00 6.1161867e-17], sum to 1.0000
[2019-04-04 10:38:06,371] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0277
[2019-04-04 10:38:06,390] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 42.66666666666667, 0.0, 0.0, 26.0, 24.60975746021429, 0.1545323855749183, 0.0, 1.0, 43120.03570579622], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2418600.0000, 
sim time next is 2419200.0000, 
raw observation next is [-5.6, 43.0, 0.0, 0.0, 26.0, 24.58117727067605, 0.1483217726471891, 0.0, 1.0, 43131.57199094433], 
processed observation next is [0.0, 0.0, 0.30747922437673136, 0.43, 0.0, 0.0, 0.6666666666666666, 0.5484314392230042, 0.5494405908823964, 0.0, 1.0, 0.20538843805211585], 
reward next is 0.7946, 
noisyNet noise sample is [array([0.79965055], dtype=float32), 0.71187896]. 
=============================================
[2019-04-04 10:38:08,390] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6069372e-09 6.4263361e-08 1.0669740e-23 4.9378391e-22 9.1685278e-18
 9.9999988e-01 6.2259978e-18], sum to 1.0000
[2019-04-04 10:38:08,391] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9645
[2019-04-04 10:38:08,463] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.033333333333333, 52.66666666666666, 43.5, 457.5, 26.0, 24.00711317638646, 0.1052523159219054, 0.0, 1.0, 154200.2296520234], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2450400.0000, 
sim time next is 2451000.0000, 
raw observation next is [-7.666666666666666, 51.33333333333334, 47.0, 499.0, 26.0, 24.50147119339717, 0.1863566138594631, 0.0, 1.0, 105814.4032489819], 
processed observation next is [0.0, 0.34782608695652173, 0.25023084025854114, 0.5133333333333334, 0.15666666666666668, 0.5513812154696133, 0.6666666666666666, 0.5417892661164307, 0.5621188712864876, 0.0, 1.0, 0.5038781107094376], 
reward next is 0.4961, 
noisyNet noise sample is [array([-1.7507102], dtype=float32), 0.2536301]. 
=============================================
[2019-04-04 10:38:08,470] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[72.92649]
 [71.56299]
 [69.86947]
 [68.19154]
 [67.51695]], R is [[73.63805389]
 [73.16738892]
 [72.46840668]
 [71.78005981]
 [71.85145569]].
[2019-04-04 10:38:08,740] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0091812e-10 5.5836575e-09 1.9587775e-24 4.0509995e-23 2.5585865e-19
 1.0000000e+00 7.9965323e-19], sum to 1.0000
[2019-04-04 10:38:08,741] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2748
[2019-04-04 10:38:08,784] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.600000000000001, 85.5, 0.0, 0.0, 26.0, 24.99774696322585, 0.2302543796385635, 0.0, 1.0, 44909.35934324843], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1887000.0000, 
sim time next is 1887600.0000, 
raw observation next is [-5.6, 85.0, 0.0, 0.0, 26.0, 24.9559572664103, 0.2298938944509678, 0.0, 1.0, 44893.05127757676], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5796631055341918, 0.5766312981503227, 0.0, 1.0, 0.2137764346551274], 
reward next is 0.7862, 
noisyNet noise sample is [array([-0.7148536], dtype=float32), -0.3426466]. 
=============================================
[2019-04-04 10:38:10,117] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.9730534e-09 4.8941427e-08 2.8872978e-24 3.3216658e-22 1.1067544e-17
 1.0000000e+00 1.0894462e-17], sum to 1.0000
[2019-04-04 10:38:10,118] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0741
[2019-04-04 10:38:10,139] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 78.0, 0.0, 0.0, 26.0, 24.02642378052837, 0.00942343089310121, 0.0, 1.0, 44998.02777500107], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1908000.0000, 
sim time next is 1908600.0000, 
raw observation next is [-7.9, 78.0, 0.0, 0.0, 26.0, 24.0039979180434, -0.0001561876626392326, 0.0, 1.0, 44927.04006280062], 
processed observation next is [1.0, 0.08695652173913043, 0.24376731301939059, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5003331598369499, 0.4999479374457869, 0.0, 1.0, 0.21393828601333628], 
reward next is 0.7861, 
noisyNet noise sample is [array([-1.1153738], dtype=float32), 0.8599325]. 
=============================================
[2019-04-04 10:38:10,912] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6975271e-12 4.2409878e-10 2.4409527e-28 1.1616168e-26 1.1893800e-21
 1.0000000e+00 4.5252244e-21], sum to 1.0000
[2019-04-04 10:38:10,912] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1966
[2019-04-04 10:38:10,920] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.8, 26.0, 165.0, 378.5, 26.0, 25.72783229577758, 0.3669711005998975, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2556000.0000, 
sim time next is 2556600.0000, 
raw observation next is [3.716666666666666, 26.5, 163.0, 344.0, 26.0, 25.733367382853, 0.3778445093611407, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5655586334256695, 0.265, 0.5433333333333333, 0.38011049723756907, 0.6666666666666666, 0.6444472819044167, 0.6259481697870469, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2501463], dtype=float32), -0.34443805]. 
=============================================
[2019-04-04 10:38:12,312] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6514722e-10 7.8817628e-09 5.8056174e-25 1.5590137e-23 9.3634506e-20
 1.0000000e+00 3.0970606e-18], sum to 1.0000
[2019-04-04 10:38:12,312] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7372
[2019-04-04 10:38:12,334] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.95, 33.0, 0.0, 0.0, 26.0, 25.31341518904873, 0.2849892866684872, 0.0, 1.0, 40192.38084991752], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2493000.0000, 
sim time next is 2493600.0000, 
raw observation next is [-1.033333333333333, 34.33333333333333, 0.0, 0.0, 26.0, 25.31390745162699, 0.2828378454572695, 0.0, 1.0, 40064.83428629406], 
processed observation next is [0.0, 0.8695652173913043, 0.43397968605724846, 0.34333333333333327, 0.0, 0.0, 0.6666666666666666, 0.6094922876355824, 0.5942792818190898, 0.0, 1.0, 0.19078492517282886], 
reward next is 0.8092, 
noisyNet noise sample is [array([0.8559787], dtype=float32), -0.3425814]. 
=============================================
[2019-04-04 10:38:24,735] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5713724e-12 3.7935932e-10 6.2626976e-28 1.2095463e-25 5.2507943e-21
 1.0000000e+00 5.8019761e-21], sum to 1.0000
[2019-04-04 10:38:24,735] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5623
[2019-04-04 10:38:24,800] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 61.5, 113.0, 799.0, 26.0, 25.86483538246843, 0.3619502188077265, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2723400.0000, 
sim time next is 2724000.0000, 
raw observation next is [-6.666666666666667, 60.66666666666666, 112.3333333333333, 797.1666666666667, 26.0, 25.29426394596282, 0.4015509190729253, 1.0, 1.0, 166630.5289785643], 
processed observation next is [1.0, 0.5217391304347826, 0.27793167128347185, 0.6066666666666666, 0.37444444444444436, 0.8808471454880296, 0.6666666666666666, 0.6078553288302349, 0.6338503063576417, 1.0, 1.0, 0.7934787094217348], 
reward next is 0.2065, 
noisyNet noise sample is [array([-0.43674678], dtype=float32), -0.3950174]. 
=============================================
[2019-04-04 10:38:24,803] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[81.25255 ]
 [81.25122 ]
 [81.2348  ]
 [81.232925]
 [81.23926 ]], R is [[81.5809021 ]
 [81.76509094]
 [81.9474411 ]
 [82.12796783]
 [82.3066864 ]].
[2019-04-04 10:38:26,817] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4956949e-09 5.7457363e-09 1.5108339e-24 5.2662148e-23 7.2784537e-19
 1.0000000e+00 6.1904274e-18], sum to 1.0000
[2019-04-04 10:38:26,817] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8838
[2019-04-04 10:38:26,848] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.34743712311475, 0.4394078466682749, 0.0, 1.0, 48973.43977221643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2757600.0000, 
sim time next is 2758200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.3419254212427, 0.3884692412779556, 0.0, 1.0, 48232.8365881076], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.6118271184368916, 0.6294897470926518, 0.0, 1.0, 0.22968017422908382], 
reward next is 0.7703, 
noisyNet noise sample is [array([-0.02235948], dtype=float32), -0.34146404]. 
=============================================
[2019-04-04 10:38:28,081] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0018014e-09 1.5539165e-08 3.5189906e-24 1.1682360e-22 1.5864891e-18
 1.0000000e+00 9.6333090e-18], sum to 1.0000
[2019-04-04 10:38:28,081] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3841
[2019-04-04 10:38:28,098] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.63972806748424, 0.1953581263165057, 0.0, 1.0, 41150.38282318524], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2775000.0000, 
sim time next is 2775600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.58893506862823, 0.1876427689905971, 0.0, 1.0, 41051.58476002089], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.549077922385686, 0.5625475896635324, 0.0, 1.0, 0.19548373695248042], 
reward next is 0.8045, 
noisyNet noise sample is [array([0.41822276], dtype=float32), 0.93675685]. 
=============================================
[2019-04-04 10:38:39,528] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.7236044e-11 6.5179790e-09 4.6618490e-25 1.0163521e-23 1.3126135e-19
 1.0000000e+00 7.8359145e-19], sum to 1.0000
[2019-04-04 10:38:39,530] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9944
[2019-04-04 10:38:39,544] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 69.0, 0.0, 0.0, 26.0, 24.77660116535049, 0.2452729850617457, 0.0, 1.0, 41773.92781756782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2596200.0000, 
sim time next is 2596800.0000, 
raw observation next is [-5.0, 70.0, 0.0, 0.0, 26.0, 24.83795776562423, 0.2503804404752119, 0.0, 1.0, 41725.97291366019], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.7, 0.0, 0.0, 0.6666666666666666, 0.569829813802019, 0.5834601468250706, 0.0, 1.0, 0.19869510911266758], 
reward next is 0.8013, 
noisyNet noise sample is [array([-0.8037373], dtype=float32), -0.75797814]. 
=============================================
[2019-04-04 10:38:45,681] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.4101463e-12 1.7994502e-10 7.5992531e-29 1.1168603e-26 8.3816515e-23
 1.0000000e+00 1.1213884e-21], sum to 1.0000
[2019-04-04 10:38:45,699] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3655
[2019-04-04 10:38:45,756] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.733333333333333, 51.66666666666667, 241.5, 151.0, 26.0, 25.77486686494223, 0.3909109622776804, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2636400.0000, 
sim time next is 2637000.0000, 
raw observation next is [-1.45, 50.5, 245.0, 147.0, 26.0, 25.71924457396132, 0.2802656686980838, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.422437673130194, 0.505, 0.8166666666666667, 0.16243093922651933, 0.6666666666666666, 0.6432703811634433, 0.5934218895660279, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2250752], dtype=float32), 0.038943253]. 
=============================================
[2019-04-04 10:38:45,779] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[86.320305]
 [86.50571 ]
 [86.68051 ]
 [86.78611 ]
 [86.86279 ]], R is [[86.21887207]
 [86.35668182]
 [86.49311829]
 [86.62818909]
 [86.76190948]].
[2019-04-04 10:39:24,919] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2279339e-09 1.1282957e-08 5.5002977e-25 8.1680867e-23 7.1834562e-19
 1.0000000e+00 4.5708077e-18], sum to 1.0000
[2019-04-04 10:39:24,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1466
[2019-04-04 10:39:24,953] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.07626644816209, 0.3704569244602664, 0.0, 1.0, 162506.6798828827], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3612600.0000, 
sim time next is 3613200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.12022342478552, 0.3934091482558784, 0.0, 1.0, 87780.208681883], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5933519520654601, 0.6311363827519595, 0.0, 1.0, 0.4180009937232524], 
reward next is 0.5820, 
noisyNet noise sample is [array([1.5321442], dtype=float32), -0.23772687]. 
=============================================
[2019-04-04 10:39:27,884] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.1434920e-12 2.0032817e-10 8.5484673e-30 1.0089256e-27 1.2439414e-22
 1.0000000e+00 1.0590476e-21], sum to 1.0000
[2019-04-04 10:39:27,884] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6537
[2019-04-04 10:39:27,918] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.83333333333333, 24.66666666666666, 112.6666666666667, 780.6666666666667, 26.0, 25.65735632306515, 0.495284566610926, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3667800.0000, 
sim time next is 3668400.0000, 
raw observation next is [12.0, 24.0, 113.5, 789.5, 26.0, 25.68766442348429, 0.4991903413894219, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.7950138504155125, 0.24, 0.37833333333333335, 0.8723756906077348, 0.6666666666666666, 0.6406387019570241, 0.6663967804631407, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65850043], dtype=float32), -0.44539672]. 
=============================================
[2019-04-04 10:39:30,903] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.4701335e-11 1.3033180e-08 4.3749895e-27 4.7297179e-26 2.7382350e-21
 1.0000000e+00 2.3616512e-20], sum to 1.0000
[2019-04-04 10:39:30,907] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1297
[2019-04-04 10:39:30,948] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.333333333333334, 30.33333333333334, 18.16666666666666, 168.0, 26.0, 25.45176232461094, 0.3721604142791353, 0.0, 1.0, 39637.5989764605], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3656400.0000, 
sim time next is 3657000.0000, 
raw observation next is [8.166666666666666, 31.16666666666667, 32.33333333333333, 215.0, 26.0, 25.46169849063133, 0.3835480248315149, 0.0, 1.0, 29651.87678953702], 
processed observation next is [0.0, 0.30434782608695654, 0.6888273314866113, 0.3116666666666667, 0.10777777777777776, 0.23756906077348067, 0.6666666666666666, 0.6218082075526109, 0.627849341610505, 0.0, 1.0, 0.1411994132835096], 
reward next is 0.8588, 
noisyNet noise sample is [array([-0.13397925], dtype=float32), -0.5922123]. 
=============================================
[2019-04-04 10:39:31,042] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[85.16962 ]
 [84.43411 ]
 [83.867355]
 [83.7879  ]
 [83.75434 ]], R is [[85.97776794]
 [85.92923737]
 [85.85805511]
 [85.79712677]
 [85.76863861]].
[2019-04-04 10:39:39,019] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1374788e-10 5.2123892e-09 2.8422619e-25 1.0030255e-23 5.0750752e-20
 1.0000000e+00 1.3925712e-19], sum to 1.0000
[2019-04-04 10:39:39,040] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9032
[2019-04-04 10:39:39,054] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.30907238091333, 0.3380392414813653, 0.0, 1.0, 41044.95226634765], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3725400.0000, 
sim time next is 3726000.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.28968135283294, 0.3319272295585285, 0.0, 1.0, 41085.28771637758], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6074734460694117, 0.6106424098528428, 0.0, 1.0, 0.1956442272208456], 
reward next is 0.8044, 
noisyNet noise sample is [array([-2.1800997], dtype=float32), -1.3313965]. 
=============================================
[2019-04-04 10:39:39,058] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[78.24052 ]
 [78.17928 ]
 [78.32271 ]
 [78.35115 ]
 [78.388466]], R is [[78.21260071]
 [78.2350235 ]
 [78.25731659]
 [78.27902985]
 [78.29868317]].
[2019-04-04 10:39:46,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4237803e-11 1.8506489e-10 1.4136961e-28 1.7908483e-26 3.9582794e-21
 1.0000000e+00 9.2793509e-21], sum to 1.0000
[2019-04-04 10:39:46,237] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9458
[2019-04-04 10:39:46,343] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.666666666666667, 39.33333333333334, 0.0, 0.0, 26.0, 25.29262542137564, 0.4006014867821659, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2834400.0000, 
sim time next is 2835000.0000, 
raw observation next is [2.5, 40.5, 0.0, 0.0, 26.0, 25.41454799667424, 0.3943384816616369, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5318559556786704, 0.405, 0.0, 0.0, 0.6666666666666666, 0.6178789997228534, 0.6314461605538789, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5527232], dtype=float32), -2.4921823]. 
=============================================
[2019-04-04 10:39:46,361] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.09157]
 [86.42822]
 [86.67823]
 [86.31611]
 [85.31713]], R is [[84.25208282]
 [84.40956116]
 [84.56546783]
 [84.3024826 ]
 [83.50844574]].
[2019-04-04 10:39:46,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.9809617e-11 1.8049854e-09 9.7374479e-28 3.2065579e-25 6.9645094e-21
 1.0000000e+00 4.4306304e-20], sum to 1.0000
[2019-04-04 10:39:46,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4563
[2019-04-04 10:39:46,508] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23692757623222, 0.3775296839661653, 0.0, 1.0, 41756.37014034262], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3478800.0000, 
sim time next is 3479400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23099529121959, 0.3779887670564275, 0.0, 1.0, 41685.16943898441], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6025829409349658, 0.6259962556854758, 0.0, 1.0, 0.19850080685230673], 
reward next is 0.8015, 
noisyNet noise sample is [array([0.3745228], dtype=float32), 1.4528891]. 
=============================================
[2019-04-04 10:39:46,986] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-04 10:39:46,989] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:39:46,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:39:46,989] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:39:46,990] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:39:46,990] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:39:46,991] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:39:46,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run40
[2019-04-04 10:39:47,022] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run40
[2019-04-04 10:39:47,056] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run40
[2019-04-04 10:40:51,074] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.26585424], dtype=float32), 0.311902]
[2019-04-04 10:40:51,075] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [10.943260738, 89.61334294000001, 0.0, 0.0, 26.0, 25.62155722345694, 0.6247863639097995, 0.0, 1.0, 18734.86253086636]
[2019-04-04 10:40:51,075] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:40:51,076] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.2548760e-12 3.6763786e-10 6.6131055e-30 8.7977231e-28 9.1881122e-23
 1.0000000e+00 7.6281209e-22], sampled 0.029574963032450663
[2019-04-04 10:42:54,603] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 10:43:08,102] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.26585424], dtype=float32), 0.311902]
[2019-04-04 10:43:08,102] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [10.54627993, 16.34059098, 76.889112245, 832.24544225, 26.0, 27.19612662771986, 0.8408889196736538, 1.0, 1.0, 0.0]
[2019-04-04 10:43:08,103] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:43:08,103] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.4751018e-13 2.2811343e-11 3.7261772e-32 4.8360003e-30 2.8258846e-24
 1.0000000e+00 5.4733627e-24], sampled 0.18025649893035955
[2019-04-04 10:43:21,355] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.5698 263430344.9894 1551.9755
[2019-04-04 10:43:28,668] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6735 275798561.6764 1233.0993
[2019-04-04 10:43:29,696] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 3900000, evaluation results [3900000.0, 7241.569785764876, 263430344.989374, 1551.9755349129598, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.673515826508, 275798561.67643094, 1233.0993326628943]
[2019-04-04 10:43:49,766] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2807669e-09 1.5120051e-08 5.3589668e-24 1.3253512e-22 1.2742693e-18
 1.0000000e+00 3.1591930e-18], sum to 1.0000
[2019-04-04 10:43:49,770] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2545
[2019-04-04 10:43:49,829] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.17022359151943, 0.3197100740166268, 0.0, 1.0, 40863.00327190929], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4068000.0000, 
sim time next is 4068600.0000, 
raw observation next is [-5.833333333333333, 40.50000000000001, 0.0, 0.0, 26.0, 25.1396656943636, 0.322093349321145, 0.0, 1.0, 40871.39773825724], 
processed observation next is [1.0, 0.08695652173913043, 0.30101569713758086, 0.4050000000000001, 0.0, 0.0, 0.6666666666666666, 0.5949721411969667, 0.6073644497737151, 0.0, 1.0, 0.19462570351551067], 
reward next is 0.8054, 
noisyNet noise sample is [array([-1.1432295], dtype=float32), 0.28195253]. 
=============================================
[2019-04-04 10:43:51,567] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.5405167e-11 3.1904884e-10 1.2223571e-26 1.4088908e-25 2.5051845e-21
 1.0000000e+00 1.2787381e-20], sum to 1.0000
[2019-04-04 10:43:51,589] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4619
[2019-04-04 10:43:51,602] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.85556696028615, 0.573776826355825, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3781200.0000, 
sim time next is 3781800.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.99293122730335, 0.5694563202751146, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6660776022752793, 0.6898187734250382, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3348411], dtype=float32), -1.3784642]. 
=============================================
[2019-04-04 10:43:58,925] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2419767e-11 1.1588340e-09 6.8429750e-28 3.1368696e-25 1.8189506e-21
 1.0000000e+00 8.2241859e-21], sum to 1.0000
[2019-04-04 10:43:58,926] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7997
[2019-04-04 10:43:58,939] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.25, 71.5, 0.0, 0.0, 26.0, 25.5287102743948, 0.3710625672348828, 0.0, 1.0, 88549.0610706369], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4329000.0000, 
sim time next is 4329600.0000, 
raw observation next is [4.166666666666667, 71.33333333333333, 0.0, 0.0, 26.0, 25.45296204758824, 0.3796402639036613, 0.0, 1.0, 98969.02388120146], 
processed observation next is [1.0, 0.08695652173913043, 0.5780240073868884, 0.7133333333333333, 0.0, 0.0, 0.6666666666666666, 0.6210801706323533, 0.6265467546345538, 0.0, 1.0, 0.4712810661009593], 
reward next is 0.5287, 
noisyNet noise sample is [array([0.43675977], dtype=float32), -0.59963995]. 
=============================================
[2019-04-04 10:43:59,527] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8289893e-10 4.2113988e-09 2.4789407e-26 8.9923659e-24 1.0033720e-19
 1.0000000e+00 1.4490285e-19], sum to 1.0000
[2019-04-04 10:43:59,527] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1716
[2019-04-04 10:43:59,543] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 57.5, 0.0, 0.0, 26.0, 25.72912805398077, 0.5698116428191674, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3879000.0000, 
sim time next is 3879600.0000, 
raw observation next is [-1.0, 56.66666666666667, 0.0, 0.0, 26.0, 25.75110499856644, 0.563032262123541, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.5666666666666668, 0.0, 0.0, 0.6666666666666666, 0.6459254165472034, 0.687677420707847, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.75485927], dtype=float32), 0.63259435]. 
=============================================
[2019-04-04 10:44:07,617] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0969925e-09 1.3197757e-08 2.4336312e-24 3.4689069e-22 1.5598565e-18
 1.0000000e+00 7.4826212e-18], sum to 1.0000
[2019-04-04 10:44:07,618] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1423
[2019-04-04 10:44:07,631] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.666666666666666, 40.0, 0.0, 0.0, 26.0, 25.17829127913205, 0.320927920506658, 0.0, 1.0, 40841.78570736818], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4069200.0000, 
sim time next is 4069800.0000, 
raw observation next is [-5.5, 39.5, 0.0, 0.0, 26.0, 25.17839088274604, 0.3170838330757765, 0.0, 1.0, 40810.40456870673], 
processed observation next is [1.0, 0.08695652173913043, 0.3102493074792244, 0.395, 0.0, 0.0, 0.6666666666666666, 0.5981992402288366, 0.6056946110252589, 0.0, 1.0, 0.1943352598509844], 
reward next is 0.8057, 
noisyNet noise sample is [array([0.70501673], dtype=float32), 1.6555436]. 
=============================================
[2019-04-04 10:44:18,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2035217e-10 2.1833995e-09 1.1005437e-26 7.1404864e-25 6.4931723e-21
 1.0000000e+00 1.5790557e-19], sum to 1.0000
[2019-04-04 10:44:18,427] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3739
[2019-04-04 10:44:18,441] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.43372729413926, 0.4669768913819838, 0.0, 1.0, 18763.23054729054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3544200.0000, 
sim time next is 3544800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.42332000466836, 0.4558003444788273, 0.0, 1.0, 28013.63339986143], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6186100003890299, 0.6519334481596091, 0.0, 1.0, 0.13339825428505442], 
reward next is 0.8666, 
noisyNet noise sample is [array([-1.4564241], dtype=float32), -0.9525874]. 
=============================================
[2019-04-04 10:44:30,916] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9782007e-11 1.8434666e-09 9.0762700e-27 7.0556824e-25 2.1997072e-19
 1.0000000e+00 1.6875458e-19], sum to 1.0000
[2019-04-04 10:44:30,920] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5723
[2019-04-04 10:44:30,936] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 184.0, 655.0, 26.0, 25.13209451473363, 0.4361052369952593, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4802400.0000, 
sim time next is 4803000.0000, 
raw observation next is [3.0, 37.0, 172.0, 684.0, 26.0, 25.13191643917532, 0.4373853345565388, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.5733333333333334, 0.7558011049723757, 0.6666666666666666, 0.5943263699312767, 0.6457951115188463, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.261506], dtype=float32), -0.37074605]. 
=============================================
[2019-04-04 10:44:30,962] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.95331 ]
 [79.97017 ]
 [79.978584]
 [80.01605 ]
 [80.082016]], R is [[80.14245605]
 [80.34103394]
 [80.53762817]
 [80.73225403]
 [80.92493439]].
[2019-04-04 10:44:33,879] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0344061e-10 2.8768203e-09 4.4883021e-26 3.4614743e-24 1.2462896e-19
 1.0000000e+00 2.0086329e-19], sum to 1.0000
[2019-04-04 10:44:33,889] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4485
[2019-04-04 10:44:33,906] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 25.016246074017, 0.2819933795647833, 0.0, 1.0, 39176.37684463122], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4849200.0000, 
sim time next is 4849800.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.98567167088429, 0.2753736897993791, 0.0, 1.0, 39195.49813963148], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5821393059070242, 0.5917912299331264, 0.0, 1.0, 0.1866452292363404], 
reward next is 0.8134, 
noisyNet noise sample is [array([-0.99677324], dtype=float32), -0.5185185]. 
=============================================
[2019-04-04 10:44:33,935] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.7601621e-11 6.0440328e-09 1.6318998e-25 2.5935731e-24 3.2648864e-20
 1.0000000e+00 1.2940638e-19], sum to 1.0000
[2019-04-04 10:44:33,937] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4754
[2019-04-04 10:44:33,952] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 75.5, 0.0, 0.0, 26.0, 25.10962143196898, 0.3429162598277068, 0.0, 1.0, 36256.89028860336], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4602600.0000, 
sim time next is 4603200.0000, 
raw observation next is [-2.866666666666667, 76.0, 0.0, 0.0, 26.0, 25.06705548204502, 0.3397441864518473, 0.0, 1.0, 36253.69521175508], 
processed observation next is [1.0, 0.2608695652173913, 0.3831948291782087, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5889212901704184, 0.6132480621506158, 0.0, 1.0, 0.1726366438655004], 
reward next is 0.8274, 
noisyNet noise sample is [array([0.6715975], dtype=float32), -1.3037409]. 
=============================================
[2019-04-04 10:44:42,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:44:42,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:44:42,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run30
[2019-04-04 10:44:42,286] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.0942206e-12 4.2976771e-11 3.6500823e-31 1.4490219e-28 2.0523125e-23
 1.0000000e+00 6.6452702e-23], sum to 1.0000
[2019-04-04 10:44:42,287] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3824
[2019-04-04 10:44:42,319] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 46.5, 195.0, 129.0, 26.0, 27.55779990320909, 0.8718147050812711, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4635000.0000, 
sim time next is 4635600.0000, 
raw observation next is [5.666666666666667, 45.33333333333333, 182.0, 132.0, 26.0, 27.09370619252866, 0.8542574750922217, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6195752539242845, 0.4533333333333333, 0.6066666666666667, 0.14585635359116023, 0.6666666666666666, 0.7578088493773883, 0.7847524916974072, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16047086], dtype=float32), -1.4421129]. 
=============================================
[2019-04-04 10:44:42,672] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.2843038e-11 8.0690077e-10 4.9795906e-28 3.5015523e-26 1.3569732e-21
 1.0000000e+00 5.4930536e-21], sum to 1.0000
[2019-04-04 10:44:42,672] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1923
[2019-04-04 10:44:42,694] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 26.0, 26.30033462450302, 0.6372437779577039, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4994400.0000, 
sim time next is 4995000.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 26.0, 26.22341614435505, 0.6208850324160257, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.6288088642659281, 0.23, 0.0, 0.0, 0.6666666666666666, 0.6852846786962541, 0.7069616774720086, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2867987], dtype=float32), -1.5233372]. 
=============================================
[2019-04-04 10:44:42,707] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[83.89059 ]
 [85.80224 ]
 [85.628426]
 [85.437614]
 [85.29848 ]], R is [[82.37115479]
 [82.5474472 ]
 [82.72197723]
 [82.89476013]
 [83.06581116]].
[2019-04-04 10:44:45,139] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:44:45,139] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:44:45,142] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run30
[2019-04-04 10:44:45,841] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:44:45,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:44:45,878] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run30
[2019-04-04 10:44:47,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:44:47,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:44:47,724] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run30
[2019-04-04 10:44:48,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:44:48,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:44:48,228] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run30
[2019-04-04 10:44:49,259] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:44:49,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:44:49,270] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run30
[2019-04-04 10:44:55,381] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4273974e-10 5.4386023e-10 1.1957635e-27 8.6187570e-25 1.3767562e-20
 1.0000000e+00 2.9576916e-20], sum to 1.0000
[2019-04-04 10:44:55,381] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0622
[2019-04-04 10:44:55,394] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.2, 95.0, 0.0, 0.0, 26.0, 24.3743861556693, 0.1667507946082195, 0.0, 1.0, 40068.55234990948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 84000.0000, 
sim time next is 84600.0000, 
raw observation next is [0.15, 95.0, 0.0, 0.0, 26.0, 24.36038413277703, 0.1625912666507545, 0.0, 1.0, 40066.18713503161], 
processed observation next is [0.0, 1.0, 0.46675900277008314, 0.95, 0.0, 0.0, 0.6666666666666666, 0.5300320110647524, 0.5541970888835849, 0.0, 1.0, 0.19079136730967433], 
reward next is 0.8092, 
noisyNet noise sample is [array([-0.08360042], dtype=float32), -0.2586422]. 
=============================================
[2019-04-04 10:44:58,450] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.3035432e-11 6.7422423e-10 7.9430484e-28 2.6940808e-25 2.8632707e-21
 1.0000000e+00 6.6308213e-20], sum to 1.0000
[2019-04-04 10:44:58,450] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2889
[2019-04-04 10:44:58,500] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.9, 86.5, 0.0, 0.0, 26.0, 24.60974898495417, 0.2081693935301873, 0.0, 1.0, 41370.80362477632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 67800.0000, 
sim time next is 68400.0000, 
raw observation next is [3.8, 86.0, 0.0, 0.0, 26.0, 24.6141731054648, 0.2096767767395061, 0.0, 1.0, 41075.03694510517], 
processed observation next is [0.0, 0.8260869565217391, 0.5678670360110805, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5511810921220667, 0.5698922589131686, 0.0, 1.0, 0.19559541402431033], 
reward next is 0.8044, 
noisyNet noise sample is [array([-0.06435298], dtype=float32), 1.3711228]. 
=============================================
[2019-04-04 10:45:00,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3831214e-11 5.4721463e-11 2.8914994e-30 4.3291169e-28 3.9061953e-23
 1.0000000e+00 2.3217643e-22], sum to 1.0000
[2019-04-04 10:45:00,671] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1369
[2019-04-04 10:45:00,681] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 25.5, 34.0, 304.0, 26.0, 27.50523803938095, 0.8256053838767308, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4987800.0000, 
sim time next is 4988400.0000, 
raw observation next is [6.666666666666667, 25.33333333333333, 28.33333333333334, 253.3333333333333, 26.0, 27.33721509660292, 0.8476868517210754, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6472760849492153, 0.2533333333333333, 0.09444444444444447, 0.2799263351749539, 0.6666666666666666, 0.7781012580502434, 0.7825622839070251, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.583659], dtype=float32), -0.70974475]. 
=============================================
[2019-04-04 10:45:04,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.19503182e-11 3.99693478e-09 2.79612383e-27 5.12359537e-25
 5.69583605e-21 1.00000000e+00 1.02780075e-20], sum to 1.0000
[2019-04-04 10:45:04,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3206
[2019-04-04 10:45:04,896] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.916666666666667, 70.16666666666667, 0.0, 0.0, 26.0, 25.54846506410328, 0.3728674793001491, 0.0, 1.0, 25904.9582745208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4333800.0000, 
sim time next is 4334400.0000, 
raw observation next is [3.9, 70.0, 0.0, 0.0, 26.0, 25.46836530801291, 0.3937565183090779, 0.0, 1.0, 75153.88082656816], 
processed observation next is [1.0, 0.17391304347826086, 0.5706371191135734, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6223637756677425, 0.6312521727696926, 0.0, 1.0, 0.3578756229836579], 
reward next is 0.6421, 
noisyNet noise sample is [array([0.01576798], dtype=float32), 0.44409382]. 
=============================================
[2019-04-04 10:45:05,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:45:05,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:45:05,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run30
[2019-04-04 10:45:07,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:45:07,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:45:07,018] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run30
[2019-04-04 10:45:07,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:45:07,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:45:07,364] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run30
[2019-04-04 10:45:07,932] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.3390605e-13 5.2784551e-11 1.7485191e-30 1.0898640e-28 6.6909770e-24
 1.0000000e+00 1.6817778e-23], sum to 1.0000
[2019-04-04 10:45:07,932] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2287
[2019-04-04 10:45:07,963] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.333333333333333, 43.0, 112.6666666666667, 716.5, 26.0, 26.72798572962902, 0.6791556930360327, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5046000.0000, 
sim time next is 5046600.0000, 
raw observation next is [2.666666666666667, 42.0, 113.3333333333333, 735.0, 26.0, 26.91678768272547, 0.6924787923291279, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5364727608494922, 0.42, 0.37777777777777766, 0.8121546961325967, 0.6666666666666666, 0.7430656402271225, 0.7308262641097093, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24369396], dtype=float32), 0.23325239]. 
=============================================
[2019-04-04 10:45:11,296] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:45:11,296] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:45:11,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run30
[2019-04-04 10:45:11,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:45:11,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:45:11,594] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run30
[2019-04-04 10:45:11,678] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:45:11,679] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:45:11,682] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run30
[2019-04-04 10:45:12,199] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.52177402e-12 1.02124406e-10 1.59690543e-29 1.16531771e-27
 6.81808252e-23 1.00000000e+00 2.54955074e-22], sum to 1.0000
[2019-04-04 10:45:12,199] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2133
[2019-04-04 10:45:12,210] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.800000000000001, 47.0, 52.83333333333333, 145.3333333333333, 26.0, 27.51086249646776, 0.8213989875330491, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4641600.0000, 
sim time next is 4642200.0000, 
raw observation next is [4.6, 47.5, 40.0, 145.0, 26.0, 27.31556206194488, 0.7747644191579942, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.590027700831025, 0.475, 0.13333333333333333, 0.16022099447513813, 0.6666666666666666, 0.7762968384954068, 0.7582548063859981, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4327819], dtype=float32), -1.2034895]. 
=============================================
[2019-04-04 10:45:24,825] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.9243627e-11 3.1004674e-09 3.1236017e-26 5.0690447e-25 5.6731135e-20
 1.0000000e+00 7.5212761e-20], sum to 1.0000
[2019-04-04 10:45:24,825] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1572
[2019-04-04 10:45:24,894] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.633333333333333, 63.33333333333333, 38.33333333333334, 7.5, 26.0, 25.31661259227346, 0.2734858402594875, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 117600.0000, 
sim time next is 118200.0000, 
raw observation next is [-7.716666666666667, 62.16666666666666, 39.66666666666666, 6.000000000000001, 26.0, 25.373363911383, 0.2700325866177501, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.24884579870729456, 0.6216666666666666, 0.13222222222222219, 0.006629834254143647, 0.6666666666666666, 0.6144469926152499, 0.5900108622059167, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86275136], dtype=float32), 1.4631587]. 
=============================================
[2019-04-04 10:45:38,057] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:45:38,059] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:45:38,064] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run30
[2019-04-04 10:45:41,536] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:45:41,536] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:45:41,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run30
[2019-04-04 10:45:46,156] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.7297241e-10 2.9830782e-09 3.2690889e-26 2.0873601e-24 4.1827405e-20
 1.0000000e+00 4.3945438e-19], sum to 1.0000
[2019-04-04 10:45:46,156] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2565
[2019-04-04 10:45:46,170] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.3540048646928, -0.5355774535964773, 0.0, 1.0, 40269.56274193379], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 26400.0000, 
sim time next is 27000.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.38567164288263, -0.5295393678796717, 0.0, 1.0, 40263.15668689913], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.2821393035735526, 0.3234868773734428, 0.0, 1.0, 0.1917293175566625], 
reward next is 0.8083, 
noisyNet noise sample is [array([-0.66429985], dtype=float32), 1.5331367]. 
=============================================
[2019-04-04 10:45:46,176] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[79.94343 ]
 [79.882744]
 [79.83916 ]
 [79.78321 ]
 [79.715965]], R is [[80.00636292]
 [80.01454163]
 [80.02262115]
 [80.03062439]
 [80.0385437 ]].
[2019-04-04 10:45:47,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:45:47,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:45:47,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run30
[2019-04-04 10:45:54,697] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2679124e-10 2.1486635e-09 3.8502698e-25 9.3291685e-24 3.3851958e-19
 1.0000000e+00 1.7539275e-18], sum to 1.0000
[2019-04-04 10:45:54,698] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2125
[2019-04-04 10:45:54,761] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8999999999999999, 57.0, 81.0, 56.0, 26.0, 24.88794536962607, 0.2247427680445503, 0.0, 1.0, 45935.82944490803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 657000.0000, 
sim time next is 657600.0000, 
raw observation next is [-0.8, 56.0, 81.33333333333333, 53.0, 26.0, 24.87894537921234, 0.225008504181214, 0.0, 1.0, 44759.97192856259], 
processed observation next is [0.0, 0.6086956521739131, 0.4404432132963989, 0.56, 0.2711111111111111, 0.05856353591160221, 0.6666666666666666, 0.573245448267695, 0.5750028347270714, 0.0, 1.0, 0.21314272346934568], 
reward next is 0.7869, 
noisyNet noise sample is [array([-1.2651644], dtype=float32), -0.31231654]. 
=============================================
[2019-04-04 10:45:59,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:45:59,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:45:59,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run30
[2019-04-04 10:46:02,966] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0033684e-11 4.1252252e-10 1.1507482e-28 4.8775077e-27 1.9527662e-22
 1.0000000e+00 7.1774468e-22], sum to 1.0000
[2019-04-04 10:46:02,967] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4496
[2019-04-04 10:46:03,012] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.516666666666667, 67.66666666666667, 126.3333333333333, 61.66666666666666, 26.0, 25.93732787991429, 0.3318164891858978, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 727800.0000, 
sim time next is 728400.0000, 
raw observation next is [-1.333333333333333, 67.33333333333334, 132.6666666666667, 64.83333333333334, 26.0, 25.88302284618719, 0.3311871862132216, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.6733333333333335, 0.4422222222222224, 0.07163904235727442, 0.6666666666666666, 0.656918570515599, 0.6103957287377405, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.699726], dtype=float32), -1.2571921]. 
=============================================
[2019-04-04 10:46:07,786] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1503109e-11 6.8401407e-10 3.9610771e-27 2.3375973e-25 2.9496243e-20
 1.0000000e+00 2.6703805e-20], sum to 1.0000
[2019-04-04 10:46:07,787] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9480
[2019-04-04 10:46:07,913] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 62.5, 30.66666666666666, 0.0, 26.0, 25.88841318559674, 0.2099215181730404, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 231000.0000, 
sim time next is 231600.0000, 
raw observation next is [-3.4, 63.0, 24.33333333333333, 0.0, 26.0, 25.74030906463165, 0.3080007746520372, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.63, 0.08111111111111109, 0.0, 0.6666666666666666, 0.6450257553859707, 0.6026669248840124, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4912624], dtype=float32), 0.37291732]. 
=============================================
[2019-04-04 10:46:10,199] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3564797e-10 2.1341069e-09 1.3918759e-26 2.6746861e-25 2.3305515e-20
 1.0000000e+00 3.8721084e-20], sum to 1.0000
[2019-04-04 10:46:10,199] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6878
[2019-04-04 10:46:10,254] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.9, 64.66666666666667, 0.0, 0.0, 26.0, 25.07133613239284, 0.3346424472551301, 1.0, 1.0, 21983.12619771644], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 155400.0000, 
sim time next is 156000.0000, 
raw observation next is [-8.0, 65.33333333333334, 0.0, 0.0, 26.0, 25.16535250428964, 0.3231470301526972, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.24099722991689754, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5971127086908034, 0.6077156767175658, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47490725], dtype=float32), 0.8929111]. 
=============================================
[2019-04-04 10:46:10,258] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[82.08833 ]
 [82.3504  ]
 [82.140976]
 [81.58783 ]
 [80.994064]], R is [[81.9643631 ]
 [82.04003906]
 [81.82707977]
 [81.31622314]
 [80.83289337]].
[2019-04-04 10:46:11,112] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2803267e-09 2.1269142e-09 1.3169285e-24 5.3553728e-23 3.7815187e-19
 1.0000000e+00 1.2302350e-18], sum to 1.0000
[2019-04-04 10:46:11,112] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4118
[2019-04-04 10:46:11,175] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 68.5, 0.0, 0.0, 26.0, 24.60760041399288, 0.2050969217860293, 0.0, 1.0, 45836.30398297768], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 162600.0000, 
sim time next is 163200.0000, 
raw observation next is [-8.4, 69.0, 0.0, 0.0, 26.0, 24.53584637074417, 0.1906810797279336, 0.0, 1.0, 45854.47657743898], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5446538642286809, 0.5635603599093112, 0.0, 1.0, 0.21835465036875706], 
reward next is 0.7816, 
noisyNet noise sample is [array([-0.87570834], dtype=float32), -0.36325034]. 
=============================================
[2019-04-04 10:46:19,463] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.0275846e-10 5.4748037e-09 1.0800362e-25 1.4536928e-23 5.2836889e-20
 1.0000000e+00 7.2223083e-19], sum to 1.0000
[2019-04-04 10:46:19,463] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9127
[2019-04-04 10:46:19,484] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 73.5, 0.0, 0.0, 26.0, 24.4003973064064, 0.08525708954478706, 0.0, 1.0, 41048.74570111687], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 696600.0000, 
sim time next is 697200.0000, 
raw observation next is [-3.4, 74.0, 0.0, 0.0, 26.0, 24.40274404251483, 0.07921768724742435, 0.0, 1.0, 41081.81169888649], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5335620035429024, 0.5264058957491414, 0.0, 1.0, 0.19562767475660234], 
reward next is 0.8044, 
noisyNet noise sample is [array([-1.3485007], dtype=float32), -0.14508763]. 
=============================================
[2019-04-04 10:46:21,270] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.11138624e-11 1.52049151e-10 6.92989316e-30 2.83646292e-28
 3.62339339e-22 1.00000000e+00 3.12605003e-23], sum to 1.0000
[2019-04-04 10:46:21,281] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6077
[2019-04-04 10:46:21,318] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.48080131277396, 0.4610966607964453, 0.0, 1.0, 18757.29922169276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 969000.0000, 
sim time next is 969600.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 26.0, 25.53913361081933, 0.45606427201086, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.7063711911357342, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6282611342349442, 0.6520214240036201, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6551401], dtype=float32), 1.7074515]. 
=============================================
[2019-04-04 10:46:22,941] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.7069111e-12 9.0623321e-11 9.1080218e-31 5.4758994e-28 6.2446750e-24
 1.0000000e+00 2.4821631e-22], sum to 1.0000
[2019-04-04 10:46:22,941] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6300
[2019-04-04 10:46:22,986] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 24.67225078103023, 0.3103696567001642, 0.0, 1.0, 168195.256806504], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 937200.0000, 
sim time next is 937800.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 24.70086450921662, 0.336654431689496, 0.0, 1.0, 103112.1873685872], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5584053757680515, 0.6122181438964986, 0.0, 1.0, 0.4910104160408914], 
reward next is 0.5090, 
noisyNet noise sample is [array([-0.72655123], dtype=float32), 0.11853774]. 
=============================================
[2019-04-04 10:46:23,157] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.8085646e-12 1.0271240e-10 8.4154856e-30 3.6042635e-27 3.0753553e-23
 1.0000000e+00 1.5111014e-21], sum to 1.0000
[2019-04-04 10:46:23,159] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2267
[2019-04-04 10:46:23,194] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 25.00862546807605, 0.3763025320974342, 0.0, 1.0, 42459.59631102762], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 939600.0000, 
sim time next is 940200.0000, 
raw observation next is [5.0, 99.33333333333334, 0.0, 0.0, 26.0, 25.05745261694072, 0.3817583280821175, 0.0, 1.0, 41148.35507629056], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.9933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5881210514117265, 0.6272527760273725, 0.0, 1.0, 0.195944547982336], 
reward next is 0.8041, 
noisyNet noise sample is [array([0.5390355], dtype=float32), -1.0850182]. 
=============================================
[2019-04-04 10:46:46,214] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4389153e-13 2.4696342e-11 1.8529356e-31 8.9971427e-30 5.2180565e-25
 1.0000000e+00 1.3545463e-23], sum to 1.0000
[2019-04-04 10:46:46,215] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1205
[2019-04-04 10:46:46,233] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.55, 79.0, 0.0, 0.0, 26.0, 25.68678716663592, 0.6097492725975568, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1056600.0000, 
sim time next is 1057200.0000, 
raw observation next is [13.46666666666667, 79.33333333333333, 0.0, 0.0, 26.0, 25.78850898447957, 0.6124780938199318, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.8356417359187445, 0.7933333333333333, 0.0, 0.0, 0.6666666666666666, 0.6490424153732975, 0.7041593646066439, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4911529], dtype=float32), -0.4638263]. 
=============================================
[2019-04-04 10:46:47,432] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.37976122e-11 1.55550806e-09 2.31544973e-29 4.98310877e-27
 1.04465846e-22 1.00000000e+00 4.23013864e-21], sum to 1.0000
[2019-04-04 10:46:47,433] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0255
[2019-04-04 10:46:47,478] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 15.0, 0.0, 26.0, 25.30308636643645, 0.4691419704004343, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1412400.0000, 
sim time next is 1413000.0000, 
raw observation next is [-0.6, 100.0, 18.0, 0.0, 26.0, 25.55988724041561, 0.4802937192224375, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.06, 0.0, 0.6666666666666666, 0.6299906033679674, 0.6600979064074791, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03946589], dtype=float32), -0.008409858]. 
=============================================
[2019-04-04 10:46:47,485] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[92.7656  ]
 [92.27925 ]
 [91.63059 ]
 [90.525055]
 [89.1445  ]], R is [[92.93240356]
 [93.00308228]
 [93.07305145]
 [93.14231873]
 [93.21089935]].
[2019-04-04 10:46:47,794] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.8205072e-14 5.4883871e-12 1.0111297e-34 4.4593977e-31 4.2880645e-26
 1.0000000e+00 7.6561243e-25], sum to 1.0000
[2019-04-04 10:46:47,795] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5842
[2019-04-04 10:46:47,808] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.95, 79.5, 0.0, 0.0, 26.0, 26.55537339159071, 0.6500191628964427, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1013400.0000, 
sim time next is 1014000.0000, 
raw observation next is [14.76666666666667, 80.0, 0.0, 0.0, 26.0, 25.86486321200488, 0.5870051415170545, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8716528162511544, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6554052676670734, 0.6956683805056848, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1307404], dtype=float32), 0.59935224]. 
=============================================
[2019-04-04 10:46:47,837] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[101.46008]
 [101.5145 ]
 [101.60474]
 [101.58202]
 [101.59294]], R is [[101.42371368]
 [101.40947723]
 [101.39538574]
 [101.38143158]
 [101.36761475]].
[2019-04-04 10:46:48,038] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1321564e-12 9.8593460e-11 1.2164847e-29 7.1359282e-28 1.1292694e-22
 1.0000000e+00 2.6294988e-22], sum to 1.0000
[2019-04-04 10:46:48,039] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0931
[2019-04-04 10:46:48,050] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [19.4, 49.0, 82.5, 0.0, 26.0, 26.72357767076501, 0.9032164576194494, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1093200.0000, 
sim time next is 1093800.0000, 
raw observation next is [19.4, 49.0, 73.0, 0.0, 26.0, 27.21418040627201, 0.945011141919245, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.24333333333333335, 0.0, 0.6666666666666666, 0.7678483671893342, 0.8150037139730816, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.84048945], dtype=float32), 0.78344923]. 
=============================================
[2019-04-04 10:46:48,284] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4911687e-12 1.6656039e-10 3.8141087e-30 2.3280540e-28 2.2097531e-23
 1.0000000e+00 1.6420872e-22], sum to 1.0000
[2019-04-04 10:46:48,286] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3598
[2019-04-04 10:46:48,291] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.2, 67.0, 106.5, 0.0, 26.0, 25.53199212890809, 0.5501607978885606, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1159200.0000, 
sim time next is 1159800.0000, 
raw observation next is [17.38333333333333, 66.66666666666667, 114.3333333333333, 0.0, 26.0, 25.4554789324602, 0.5388838761119618, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.9441366574330563, 0.6666666666666667, 0.381111111111111, 0.0, 0.6666666666666666, 0.62128991103835, 0.6796279587039873, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19532682], dtype=float32), 0.44357798]. 
=============================================
[2019-04-04 10:46:49,889] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.27377256e-10 3.55111185e-09 3.01345457e-25 4.88535484e-24
 1.04084136e-19 1.00000000e+00 2.46098149e-19], sum to 1.0000
[2019-04-04 10:46:49,889] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2849
[2019-04-04 10:46:49,904] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.21187390931218, 0.02527840456824113, 0.0, 1.0, 41644.02677304776], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 711000.0000, 
sim time next is 711600.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.18729244031784, 0.03180908812882842, 0.0, 1.0, 41683.46460290565], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.51560770335982, 0.5106030293762761, 0.0, 1.0, 0.198492688585265], 
reward next is 0.8015, 
noisyNet noise sample is [array([1.4423244], dtype=float32), -0.39723235]. 
=============================================
[2019-04-04 10:46:58,165] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0386606e-13 9.8212077e-11 3.2282719e-31 5.0338865e-29 3.3152065e-24
 1.0000000e+00 5.3575185e-23], sum to 1.0000
[2019-04-04 10:46:58,169] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0842
[2019-04-04 10:46:58,180] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.616666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 25.48761857925103, 0.5596153273749805, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1627800.0000, 
sim time next is 1628400.0000, 
raw observation next is [7.533333333333333, 74.66666666666667, 0.0, 0.0, 26.0, 25.39618208207875, 0.5592340427234507, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.6712834718374886, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.6163485068398957, 0.6864113475744835, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([-0.40545756], dtype=float32), 0.67282325]. 
=============================================
[2019-04-04 10:47:13,236] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5026170e-12 1.1833089e-10 1.3447293e-29 1.3266681e-27 3.4040560e-23
 1.0000000e+00 4.0123836e-22], sum to 1.0000
[2019-04-04 10:47:13,237] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5458
[2019-04-04 10:47:13,275] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.1, 96.16666666666666, 0.0, 0.0, 26.0, 25.74938300443856, 0.5711720858092414, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1649400.0000, 
sim time next is 1650000.0000, 
raw observation next is [7.0, 96.33333333333333, 0.0, 0.0, 26.0, 25.69118402517243, 0.5731964551282139, 0.0, 1.0, 34472.01465016885], 
processed observation next is [1.0, 0.08695652173913043, 0.6565096952908588, 0.9633333333333333, 0.0, 0.0, 0.6666666666666666, 0.6409320020977024, 0.691065485042738, 0.0, 1.0, 0.16415245071508977], 
reward next is 0.8358, 
noisyNet noise sample is [array([-0.6085944], dtype=float32), -0.52820516]. 
=============================================
[2019-04-04 10:47:13,289] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[89.88186]
 [89.88617]
 [89.92244]
 [89.97848]
 [90.02452]], R is [[89.89541626]
 [89.99645996]
 [90.09649658]
 [90.19553375]
 [90.2935791 ]].
[2019-04-04 10:47:19,947] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1497314e-11 5.3939120e-10 2.2154446e-28 5.9037691e-26 1.6137771e-21
 1.0000000e+00 2.3800727e-21], sum to 1.0000
[2019-04-04 10:47:19,948] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5878
[2019-04-04 10:47:20,003] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.1, 82.5, 85.0, 549.0, 26.0, 25.64493418666689, 0.300471492740175, 1.0, 1.0, 18740.78732618513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1935000.0000, 
sim time next is 1935600.0000, 
raw observation next is [-7.833333333333334, 81.33333333333334, 99.33333333333334, 496.8333333333333, 26.0, 25.64870335517166, 0.3023859188993075, 1.0, 1.0, 19329.86041505032], 
processed observation next is [1.0, 0.391304347826087, 0.2456140350877193, 0.8133333333333335, 0.33111111111111113, 0.548987108655617, 0.6666666666666666, 0.6373919462643048, 0.6007953062997692, 1.0, 1.0, 0.09204695435738247], 
reward next is 0.9080, 
noisyNet noise sample is [array([0.03517231], dtype=float32), -0.086833194]. 
=============================================
[2019-04-04 10:47:28,758] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4872376e-09 6.8551906e-09 1.0648139e-25 5.6198782e-24 1.5477463e-19
 1.0000000e+00 8.2837661e-19], sum to 1.0000
[2019-04-04 10:47:28,758] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0743
[2019-04-04 10:47:28,804] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 83.66666666666667, 105.6666666666667, 0.0, 26.0, 25.04354702312006, 0.3414250002295366, 0.0, 1.0, 27091.36915607728], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1779000.0000, 
sim time next is 1779600.0000, 
raw observation next is [-2.8, 84.33333333333334, 102.3333333333333, 0.0, 26.0, 25.00035839317619, 0.3359900545732739, 0.0, 1.0, 58093.72344347384], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8433333333333334, 0.341111111111111, 0.0, 0.6666666666666666, 0.5833631994313491, 0.6119966848577579, 0.0, 1.0, 0.2766367783022564], 
reward next is 0.7234, 
noisyNet noise sample is [array([-0.05374075], dtype=float32), -0.6351908]. 
=============================================
[2019-04-04 10:47:31,906] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1893585e-09 1.4318434e-08 5.0155555e-24 9.6072284e-23 2.3350122e-18
 1.0000000e+00 1.5777507e-17], sum to 1.0000
[2019-04-04 10:47:31,908] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7813
[2019-04-04 10:47:31,923] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.199999999999999, 84.33333333333333, 0.0, 0.0, 26.0, 23.93667749660732, 0.05164380607293101, 0.0, 1.0, 46850.61108947425], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1827600.0000, 
sim time next is 1828200.0000, 
raw observation next is [-6.2, 83.66666666666667, 0.0, 0.0, 26.0, 23.89940333723852, 0.04321750220572765, 0.0, 1.0, 46886.29313161335], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.4916169447698768, 0.5144058340685759, 0.0, 1.0, 0.22326806253149212], 
reward next is 0.7767, 
noisyNet noise sample is [array([1.9293413], dtype=float32), 0.26915887]. 
=============================================
[2019-04-04 10:47:33,943] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0506641e-09 5.3679488e-09 5.9336689e-24 6.0330252e-23 9.1635937e-19
 1.0000000e+00 5.1285219e-18], sum to 1.0000
[2019-04-04 10:47:33,943] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1213
[2019-04-04 10:47:33,968] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 78.83333333333333, 0.0, 0.0, 26.0, 23.83248435741245, 0.02462201038530959, 0.0, 1.0, 43529.76299001243], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2098200.0000, 
sim time next is 2098800.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 23.79955480443573, 0.0190198850787707, 0.0, 1.0, 43439.39486182722], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4832962337029774, 0.5063399616929235, 0.0, 1.0, 0.2068542612467963], 
reward next is 0.7931, 
noisyNet noise sample is [array([-1.7476503], dtype=float32), 1.1004697]. 
=============================================
[2019-04-04 10:47:34,853] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4473053e-11 7.6744000e-10 2.2705274e-28 3.4214568e-26 3.5680752e-22
 1.0000000e+00 2.1383978e-20], sum to 1.0000
[2019-04-04 10:47:34,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9431
[2019-04-04 10:47:34,889] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.32788201017861, 0.4468540018151462, 0.0, 1.0, 36923.79409380144], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1489200.0000, 
sim time next is 1489800.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.33085354398818, 0.450198498111988, 0.0, 1.0, 37003.31834619569], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.610904461999015, 0.6500661660373294, 0.0, 1.0, 0.1762062778390271], 
reward next is 0.8238, 
noisyNet noise sample is [array([-1.2464538], dtype=float32), 1.814282]. 
=============================================
[2019-04-04 10:47:40,622] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.8655346e-12 1.9759439e-10 8.6744402e-31 6.9516345e-29 2.9481799e-23
 1.0000000e+00 1.3423917e-22], sum to 1.0000
[2019-04-04 10:47:40,625] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3129
[2019-04-04 10:47:40,663] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.2, 50.0, 82.0, 253.0, 26.0, 26.98153580880275, 0.7942011728113694, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1526400.0000, 
sim time next is 1527000.0000, 
raw observation next is [11.91666666666667, 51.33333333333334, 83.66666666666667, 177.9999999999999, 26.0, 27.08150667472447, 0.8075722039733969, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7927054478301017, 0.5133333333333334, 0.2788888888888889, 0.19668508287292805, 0.6666666666666666, 0.7567922228937057, 0.769190734657799, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6905717], dtype=float32), -0.3716583]. 
=============================================
[2019-04-04 10:47:40,737] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[93.125046]
 [93.30459 ]
 [93.53329 ]
 [93.63911 ]
 [93.66866 ]], R is [[92.97220612]
 [93.0424881 ]
 [93.11206055]
 [93.18093872]
 [93.24913025]].
[2019-04-04 10:47:45,558] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2003619e-11 5.9592531e-10 9.1999478e-27 2.2111505e-25 6.5282411e-21
 1.0000000e+00 1.1994088e-20], sum to 1.0000
[2019-04-04 10:47:45,559] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3897
[2019-04-04 10:47:45,668] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.95, 89.0, 45.0, 16.0, 26.0, 25.36050931919737, 0.2907238275267743, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2277000.0000, 
sim time next is 2277600.0000, 
raw observation next is [-8.766666666666666, 88.33333333333334, 54.33333333333333, 19.83333333333333, 26.0, 25.52222153912304, 0.3077901539282507, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2197599261311173, 0.8833333333333334, 0.18111111111111108, 0.021915285451197048, 0.6666666666666666, 0.6268517949269201, 0.6025967179760836, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14691274], dtype=float32), 0.27399868]. 
=============================================
[2019-04-04 10:47:51,455] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.3468196e-13 8.6940413e-11 1.1420620e-30 2.3572480e-29 5.6891828e-24
 1.0000000e+00 9.0326297e-23], sum to 1.0000
[2019-04-04 10:47:51,459] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7123
[2019-04-04 10:47:51,505] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 101.1666666666667, 0.0, 26.0, 25.36532487300239, 0.559427425863828, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1689600.0000, 
sim time next is 1690200.0000, 
raw observation next is [1.1, 88.0, 100.0, 0.0, 26.0, 25.79262818773086, 0.5837730798756605, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3333333333333333, 0.0, 0.6666666666666666, 0.6493856823109049, 0.6945910266252202, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.63647264], dtype=float32), -1.9215912]. 
=============================================
[2019-04-04 10:48:07,085] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1393795e-10 2.0065141e-09 3.4658823e-27 1.2721326e-25 1.2097293e-20
 1.0000000e+00 5.4057083e-20], sum to 1.0000
[2019-04-04 10:48:07,085] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8374
[2019-04-04 10:48:07,103] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.25, 91.16666666666667, 0.0, 0.0, 26.0, 25.32768726345083, 0.4483729229428112, 0.0, 1.0, 42904.48344992023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1734600.0000, 
sim time next is 1735200.0000, 
raw observation next is [0.2, 91.0, 0.0, 0.0, 26.0, 25.30825494827818, 0.4436592557008449, 0.0, 1.0, 42906.89894739845], 
processed observation next is [0.0, 0.08695652173913043, 0.46814404432132967, 0.91, 0.0, 0.0, 0.6666666666666666, 0.6090212456898483, 0.6478864185669483, 0.0, 1.0, 0.2043185664161831], 
reward next is 0.7957, 
noisyNet noise sample is [array([0.27742183], dtype=float32), -1.0045772]. 
=============================================
[2019-04-04 10:48:09,424] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-04 10:48:09,444] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:48:09,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:09,462] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:48:09,462] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:09,479] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:48:09,479] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:09,554] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run41
[2019-04-04 10:48:09,640] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run41
[2019-04-04 10:48:09,756] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run41
[2019-04-04 10:48:27,073] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.26682976], dtype=float32), 0.3159226]
[2019-04-04 10:48:27,073] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.9, 78.0, 0.0, 0.0, 26.0, 22.88744151424562, -0.1961068043343705, 0.0, 1.0, 44575.34330968801]
[2019-04-04 10:48:27,073] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 10:48:27,074] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.6704320e-09 5.4308543e-08 1.2422816e-22 3.7890377e-21 2.2391792e-17
 1.0000000e+00 9.7555352e-17], sampled 0.5793138340286461
[2019-04-04 10:49:48,999] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.26682976], dtype=float32), 0.3159226]
[2019-04-04 10:49:48,999] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.833333333333333, 76.66666666666667, 45.5, 114.5, 26.0, 25.43375664931526, 0.4650040048713393, 1.0, 1.0, 0.0]
[2019-04-04 10:49:48,999] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:49:48,999] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.4058381e-12 3.9867304e-10 3.6216532e-29 4.1889369e-27 2.4310171e-22
 1.0000000e+00 1.7486806e-21], sampled 0.29900569050281456
[2019-04-04 10:50:56,226] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.26682976], dtype=float32), 0.3159226]
[2019-04-04 10:50:56,226] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.7, 74.5, 0.0, 0.0, 26.0, 24.97287117384137, 0.3480562586694416, 0.0, 1.0, 47470.71942152203]
[2019-04-04 10:50:56,226] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:50:56,227] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.48447898e-10 5.28180433e-09 1.02812753e-25 4.30212671e-24
 1.39760566e-19 1.00000000e+00 4.86398227e-19], sampled 0.10354308327272477
[2019-04-04 10:51:22,528] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 10:51:56,352] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 10:52:03,274] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 10:52:04,326] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 4000000, evaluation results [4000000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 10:52:05,046] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7860932e-10 9.4793720e-09 2.7574221e-25 6.7099202e-23 2.3195370e-19
 1.0000000e+00 5.8942070e-18], sum to 1.0000
[2019-04-04 10:52:05,047] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4885
[2019-04-04 10:52:05,114] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 38.33333333333334, 0.0, 0.0, 26.0, 25.00764261220479, 0.1944143110393959, 0.0, 1.0, 38913.86824580214], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2512200.0000, 
sim time next is 2512800.0000, 
raw observation next is [-1.7, 38.0, 0.0, 0.0, 26.0, 24.9783164077102, 0.1891796457483486, 0.0, 1.0, 38901.00335190815], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.38, 0.0, 0.0, 0.6666666666666666, 0.5815263673091833, 0.5630598819161162, 0.0, 1.0, 0.18524287310432452], 
reward next is 0.8148, 
noisyNet noise sample is [array([0.157985], dtype=float32), -0.5715886]. 
=============================================
[2019-04-04 10:52:31,067] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.8600752e-10 3.2683644e-08 2.0133148e-24 3.3469756e-23 1.1105263e-18
 1.0000000e+00 5.3357087e-18], sum to 1.0000
[2019-04-04 10:52:31,067] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3753
[2019-04-04 10:52:31,095] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 26.0, 24.11768161320151, 0.1426129551079932, 0.0, 1.0, 44489.67766470956], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2685600.0000, 
sim time next is 2686200.0000, 
raw observation next is [-11.31666666666667, 77.16666666666667, 0.0, 0.0, 26.0, 24.13780042217854, 0.1402121825911058, 0.0, 1.0, 44457.12905847288], 
processed observation next is [1.0, 0.08695652173913043, 0.14912280701754377, 0.7716666666666667, 0.0, 0.0, 0.6666666666666666, 0.5114833685148783, 0.5467373941970353, 0.0, 1.0, 0.21170061456415656], 
reward next is 0.7883, 
noisyNet noise sample is [array([-1.1317228], dtype=float32), 1.0885743]. 
=============================================
[2019-04-04 10:52:43,117] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.2452321e-10 1.0310075e-08 2.6227442e-24 3.6383502e-23 1.6847486e-18
 1.0000000e+00 1.8207792e-18], sum to 1.0000
[2019-04-04 10:52:43,117] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8185
[2019-04-04 10:52:43,195] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.95, 39.5, 76.0, 777.0, 26.0, 25.00895035075938, 0.2284379567251811, 0.0, 1.0, 18744.69522215709], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2457000.0000, 
sim time next is 2457600.0000, 
raw observation next is [-3.4, 38.33333333333334, 77.66666666666667, 785.6666666666666, 26.0, 24.9590171810696, 0.2305152568910202, 0.0, 1.0, 41246.86440988403], 
processed observation next is [0.0, 0.43478260869565216, 0.368421052631579, 0.3833333333333334, 0.2588888888888889, 0.8681399631675875, 0.6666666666666666, 0.5799180984224668, 0.5768384189636734, 0.0, 1.0, 0.19641364004706682], 
reward next is 0.8036, 
noisyNet noise sample is [array([-0.75220233], dtype=float32), -1.4718477]. 
=============================================
[2019-04-04 10:52:50,544] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1570351e-11 1.4709680e-09 6.2510293e-27 1.0580964e-25 2.9236000e-21
 1.0000000e+00 8.8069540e-21], sum to 1.0000
[2019-04-04 10:52:50,544] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9403
[2019-04-04 10:52:50,647] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 75.0, 8.499999999999998, 43.66666666666666, 26.0, 24.57436344321753, 0.2695301684499496, 1.0, 1.0, 128403.550681884], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2187600.0000, 
sim time next is 2188200.0000, 
raw observation next is [-5.6, 75.0, 15.0, 87.33333333333331, 26.0, 25.2202794939473, 0.3099709990197004, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.30747922437673136, 0.75, 0.05, 0.09650092081031306, 0.6666666666666666, 0.6016899578289415, 0.6033236663399001, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7148303], dtype=float32), 0.46157825]. 
=============================================
[2019-04-04 10:52:52,417] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2204038e-12 6.8845873e-11 9.3678331e-30 7.2230467e-28 3.0643139e-22
 1.0000000e+00 6.4825484e-23], sum to 1.0000
[2019-04-04 10:52:52,417] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6042
[2019-04-04 10:52:52,484] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.333333333333334, 31.66666666666667, 227.1666666666667, 144.1666666666667, 26.0, 25.99306499658707, 0.3211541348560743, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2814000.0000, 
sim time next is 2814600.0000, 
raw observation next is [5.666666666666666, 30.83333333333333, 205.3333333333333, 115.3333333333333, 26.0, 25.91481586539268, 0.4433567312374745, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6195752539242845, 0.3083333333333333, 0.6844444444444443, 0.12744014732965006, 0.6666666666666666, 0.6595679887827233, 0.6477855770791582, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60561496], dtype=float32), 1.7864364]. 
=============================================
[2019-04-04 10:53:01,321] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7713343e-12 1.7395090e-10 1.5520189e-29 1.4837531e-27 1.0609394e-22
 1.0000000e+00 3.6870018e-22], sum to 1.0000
[2019-04-04 10:53:01,321] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0882
[2019-04-04 10:53:01,390] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 47.0, 204.5, 179.0, 26.0, 24.92923088518324, 0.3889045023118554, 1.0, 1.0, 65585.86101125668], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2638800.0000, 
sim time next is 2639400.0000, 
raw observation next is [-0.4166666666666667, 46.33333333333334, 191.0, 189.6666666666667, 26.0, 25.46031412402484, 0.4455401773193672, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.45106186518928904, 0.46333333333333343, 0.6366666666666667, 0.20957642725598533, 0.6666666666666666, 0.6216928436687367, 0.6485133924397891, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17706698], dtype=float32), 0.04044403]. 
=============================================
[2019-04-04 10:53:12,371] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0409274e-10 4.5628497e-09 9.3979796e-28 1.7040751e-25 1.1426468e-21
 1.0000000e+00 9.9467112e-21], sum to 1.0000
[2019-04-04 10:53:12,375] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8611
[2019-04-04 10:53:12,391] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.9216746468656, 0.2578790663546431, 0.0, 1.0, 55707.94438553952], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2866200.0000, 
sim time next is 2866800.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.9091503965053, 0.2503120424096703, 0.0, 1.0, 55732.5156238617], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5757625330421083, 0.5834373474698901, 0.0, 1.0, 0.26539293154219856], 
reward next is 0.7346, 
noisyNet noise sample is [array([0.77946824], dtype=float32), -0.58440435]. 
=============================================
[2019-04-04 10:53:32,063] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.7321975e-11 1.3719327e-09 2.0751313e-26 1.5221898e-24 7.2589299e-20
 1.0000000e+00 8.0110000e-20], sum to 1.0000
[2019-04-04 10:53:32,063] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0746
[2019-04-04 10:53:32,109] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 58.33333333333334, 0.0, 0.0, 26.0, 25.13000386030601, 0.4520508189433523, 0.0, 1.0, 198947.9670021382], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3356400.0000, 
sim time next is 3357000.0000, 
raw observation next is [-3.5, 60.0, 0.0, 0.0, 26.0, 25.11012016630418, 0.4746150021582029, 0.0, 1.0, 155376.4927457259], 
processed observation next is [1.0, 0.8695652173913043, 0.36565096952908593, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5925100138586817, 0.658205000719401, 0.0, 1.0, 0.7398880606939329], 
reward next is 0.2601, 
noisyNet noise sample is [array([-0.36295232], dtype=float32), -0.18329948]. 
=============================================
[2019-04-04 10:53:32,127] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[79.22271 ]
 [79.233086]
 [80.25128 ]
 [79.21306 ]
 [78.39615 ]], R is [[78.35926056]
 [77.6282959 ]
 [77.51496887]
 [77.51165771]
 [77.6164093 ]].
[2019-04-04 10:53:32,810] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5783251e-10 3.0633657e-08 7.2151025e-24 2.6483537e-22 3.4701050e-18
 1.0000000e+00 3.5347846e-17], sum to 1.0000
[2019-04-04 10:53:32,810] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2986
[2019-04-04 10:53:32,820] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.2113661826549, 0.09871443371297679, 0.0, 1.0, 39621.6907900564], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3036000.0000, 
sim time next is 3036600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.18252824625887, 0.09052494890158751, 0.0, 1.0, 39745.01637977498], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.515210687188239, 0.5301749829671959, 0.0, 1.0, 0.18926198276083323], 
reward next is 0.8107, 
noisyNet noise sample is [array([-0.91964746], dtype=float32), 0.45837584]. 
=============================================
[2019-04-04 10:53:38,420] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.7119067e-11 5.4417154e-10 9.1785257e-28 5.5440682e-26 3.6998533e-21
 1.0000000e+00 8.0604405e-21], sum to 1.0000
[2019-04-04 10:53:38,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4360
[2019-04-04 10:53:38,440] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666666, 100.0, 0.0, 0.0, 26.0, 25.30084073474019, 0.3191742798235228, 0.0, 1.0, 39327.66544596108], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3112800.0000, 
sim time next is 3113400.0000, 
raw observation next is [0.8333333333333334, 100.0, 0.0, 0.0, 26.0, 25.27190832324687, 0.335292921180169, 0.0, 1.0, 39535.78427293998], 
processed observation next is [1.0, 0.0, 0.4856879039704525, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6059923602705725, 0.6117643070600564, 0.0, 1.0, 0.18826563939495228], 
reward next is 0.8117, 
noisyNet noise sample is [array([0.1473108], dtype=float32), 0.8629088]. 
=============================================
[2019-04-04 10:53:40,890] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.6194029e-14 1.6225405e-12 5.0264478e-34 1.9658895e-31 4.8827972e-26
 1.0000000e+00 2.1440608e-25], sum to 1.0000
[2019-04-04 10:53:40,891] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9492
[2019-04-04 10:53:40,927] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.833333333333334, 94.16666666666666, 113.6666666666667, 811.0, 26.0, 27.16585768630426, 0.8023743668491043, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3153000.0000, 
sim time next is 3153600.0000, 
raw observation next is [8.0, 93.0, 113.5, 814.0, 26.0, 27.22060185370187, 0.8141174447292379, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6842105263157896, 0.93, 0.37833333333333335, 0.8994475138121547, 0.6666666666666666, 0.7683834878084891, 0.7713724815764126, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.58919555], dtype=float32), 1.1671892]. 
=============================================
[2019-04-04 10:53:41,436] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6557914e-10 1.0472574e-08 5.9481700e-25 5.4261122e-23 9.0836849e-19
 1.0000000e+00 2.6336302e-18], sum to 1.0000
[2019-04-04 10:53:41,439] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8452
[2019-04-04 10:53:41,450] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.66288253139169, 0.2650494489204266, 0.0, 1.0, 40864.27809170401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3564000.0000, 
sim time next is 3564600.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.63270653198259, 0.2559220114446601, 0.0, 1.0, 40872.00433839033], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5527255443318824, 0.58530733714822, 0.0, 1.0, 0.19462859208757302], 
reward next is 0.8054, 
noisyNet noise sample is [array([0.0153324], dtype=float32), -1.2067493]. 
=============================================
[2019-04-04 10:53:57,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.7863814e-10 5.6311227e-09 3.0125480e-24 1.6994298e-22 7.7915916e-19
 1.0000000e+00 2.5689264e-17], sum to 1.0000
[2019-04-04 10:53:57,740] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0419
[2019-04-04 10:53:57,754] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.96197191121356, 0.2769486789147004, 0.0, 1.0, 38224.09633423785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3021000.0000, 
sim time next is 3021600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93079208640976, 0.2752204650914976, 0.0, 1.0, 38150.93490819046], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5775660072008133, 0.5917401550304991, 0.0, 1.0, 0.18167111861043075], 
reward next is 0.8183, 
noisyNet noise sample is [array([-0.37229568], dtype=float32), -1.9963349]. 
=============================================
[2019-04-04 10:54:02,284] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5575720e-13 1.5214859e-11 2.0832570e-30 3.2255670e-29 2.5306642e-23
 1.0000000e+00 3.3469118e-23], sum to 1.0000
[2019-04-04 10:54:02,290] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8332
[2019-04-04 10:54:02,317] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 48.0, 102.8333333333333, 771.1666666666667, 26.0, 26.88494127168119, 0.7497018347685565, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3854400.0000, 
sim time next is 3855000.0000, 
raw observation next is [2.0, 48.0, 99.66666666666666, 760.3333333333333, 26.0, 26.95834924470662, 0.6416084354882713, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.3322222222222222, 0.840147329650092, 0.6666666666666666, 0.7465291037255518, 0.7138694784960905, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.50040925], dtype=float32), -0.35066184]. 
=============================================
[2019-04-04 10:54:02,338] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[88.50436 ]
 [88.48956 ]
 [88.474846]
 [88.487015]
 [88.4944  ]], R is [[88.54272461]
 [88.65729523]
 [88.77072144]
 [88.88301849]
 [88.9941864 ]].
[2019-04-04 10:54:02,643] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.2598379e-09 8.3494323e-09 3.7022477e-24 1.2019268e-22 2.2334708e-18
 1.0000000e+00 1.4290268e-17], sum to 1.0000
[2019-04-04 10:54:02,647] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6446
[2019-04-04 10:54:02,688] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666666, 83.66666666666667, 0.0, 0.0, 26.0, 24.98965336440444, 0.2849679159105007, 0.0, 1.0, 42175.39906365092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3089400.0000, 
sim time next is 3090000.0000, 
raw observation next is [-0.7333333333333334, 85.33333333333334, 0.0, 0.0, 26.0, 24.99513074219652, 0.2847377348755945, 0.0, 1.0, 34589.24043558438], 
processed observation next is [0.0, 0.782608695652174, 0.44228993536472766, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.58292756184971, 0.5949125782918648, 0.0, 1.0, 0.164710668740878], 
reward next is 0.8353, 
noisyNet noise sample is [array([-0.5160793], dtype=float32), -0.9836825]. 
=============================================
[2019-04-04 10:54:02,697] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.833405]
 [73.38214 ]
 [73.06283 ]
 [73.006134]
 [73.34473 ]], R is [[74.4744339 ]
 [74.52885437]
 [74.54383087]
 [74.54412842]
 [74.56985474]].
[2019-04-04 10:54:04,870] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.3590021e-11 1.4892793e-08 4.7648707e-25 9.0557406e-24 1.5156000e-19
 1.0000000e+00 2.8749626e-19], sum to 1.0000
[2019-04-04 10:54:04,873] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6582
[2019-04-04 10:54:04,909] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 24.97751383105772, 0.3429615502428796, 0.0, 1.0, 40934.28562160228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3558600.0000, 
sim time next is 3559200.0000, 
raw observation next is [-4.666666666666666, 67.0, 0.0, 0.0, 26.0, 24.97033723341626, 0.3352563297323847, 0.0, 1.0, 40884.05439616479], 
processed observation next is [0.0, 0.17391304347826086, 0.33333333333333337, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5808614361180217, 0.6117521099107949, 0.0, 1.0, 0.19468597331507045], 
reward next is 0.8053, 
noisyNet noise sample is [array([-0.36282066], dtype=float32), 1.9092289]. 
=============================================
[2019-04-04 10:54:08,086] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6147575e-11 1.0996232e-10 6.0016058e-29 1.8140331e-26 7.7596342e-22
 1.0000000e+00 3.3977915e-22], sum to 1.0000
[2019-04-04 10:54:08,086] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9803
[2019-04-04 10:54:08,101] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.84811050546541, 0.7114718237774659, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3189000.0000, 
sim time next is 3189600.0000, 
raw observation next is [2.0, 100.0, 0.0, 0.0, 26.0, 25.87857918515993, 0.7016243068533145, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 1.0, 0.0, 0.0, 0.6666666666666666, 0.656548265429994, 0.7338747689511048, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1064836], dtype=float32), 1.6162362]. 
=============================================
[2019-04-04 10:54:10,356] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.3655964e-12 1.2437426e-10 5.5929410e-28 7.8796758e-27 2.1257415e-21
 1.0000000e+00 6.6451724e-22], sum to 1.0000
[2019-04-04 10:54:10,357] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6912
[2019-04-04 10:54:10,406] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.333333333333334, 38.0, 102.1666666666667, 777.3333333333334, 26.0, 26.98063411735293, 0.7798111095581369, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3940800.0000, 
sim time next is 3941400.0000, 
raw observation next is [-4.166666666666667, 38.0, 99.33333333333334, 766.6666666666666, 26.0, 27.02365366793607, 0.6664034756655687, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3471837488457987, 0.38, 0.33111111111111113, 0.8471454880294659, 0.6666666666666666, 0.7519711389946725, 0.7221344918885229, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08719614], dtype=float32), 0.22664146]. 
=============================================
[2019-04-04 10:54:18,173] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6711736e-10 4.0530224e-09 1.9065081e-26 1.2549740e-24 2.6513790e-20
 1.0000000e+00 7.1578497e-20], sum to 1.0000
[2019-04-04 10:54:18,174] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1000
[2019-04-04 10:54:18,200] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.333333333333334, 74.0, 0.0, 0.0, 26.0, 25.21108834401704, 0.3526845233417641, 0.0, 1.0, 41009.70950421529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3906600.0000, 
sim time next is 3907200.0000, 
raw observation next is [-4.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.21311060952518, 0.3412880367972412, 0.0, 1.0, 41096.10164511613], 
processed observation next is [1.0, 0.21739130434782608, 0.3333333333333333, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6010925507937651, 0.6137626789324138, 0.0, 1.0, 0.1956957221196006], 
reward next is 0.8043, 
noisyNet noise sample is [array([0.21526286], dtype=float32), -0.5463032]. 
=============================================
[2019-04-04 10:54:37,392] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4402637e-11 1.3878452e-09 3.3934955e-27 4.9023781e-25 5.5686275e-21
 1.0000000e+00 2.2184643e-20], sum to 1.0000
[2019-04-04 10:54:37,393] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9890
[2019-04-04 10:54:37,408] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8833333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.32278418869429, 0.4162088825359428, 0.0, 1.0, 44341.61713865168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4507800.0000, 
sim time next is 4508400.0000, 
raw observation next is [-0.8666666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.26674313807343, 0.4081163122871824, 0.0, 1.0, 42836.74760912837], 
processed observation next is [1.0, 0.17391304347826086, 0.4385964912280702, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6055619281727859, 0.6360387707623941, 0.0, 1.0, 0.2039845124244208], 
reward next is 0.7960, 
noisyNet noise sample is [array([-1.6473827], dtype=float32), 0.12387938]. 
=============================================
[2019-04-04 10:54:41,373] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8437179e-11 2.4965838e-10 4.7231298e-29 7.4801289e-28 2.4001128e-21
 1.0000000e+00 2.5122184e-21], sum to 1.0000
[2019-04-04 10:54:41,374] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0730
[2019-04-04 10:54:41,423] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.1333333333333333, 72.16666666666666, 115.0, 44.00000000000001, 26.0, 25.925798897685, 0.5087866390391557, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4524600.0000, 
sim time next is 4525200.0000, 
raw observation next is [0.0, 72.0, 117.0, 33.0, 26.0, 26.03589725025416, 0.5213216215684114, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.72, 0.39, 0.036464088397790057, 0.6666666666666666, 0.6696581041878465, 0.6737738738561371, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14969635], dtype=float32), -0.25549018]. 
=============================================
[2019-04-04 10:54:42,594] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.0430431e-11 7.8701462e-10 4.6709382e-28 1.2410340e-25 1.4462824e-21
 1.0000000e+00 1.6954104e-20], sum to 1.0000
[2019-04-04 10:54:42,595] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3241
[2019-04-04 10:54:42,614] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.99293122730335, 0.5694563202751146, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3781800.0000, 
sim time next is 3782400.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.92542436650391, 0.5530929300152352, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6604520305419926, 0.6843643100050784, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30180237], dtype=float32), -1.0071598]. 
=============================================
[2019-04-04 10:54:55,726] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1927611e-12 1.0732534e-10 1.7815932e-28 3.8345238e-27 1.6697161e-22
 1.0000000e+00 6.0979296e-22], sum to 1.0000
[2019-04-04 10:54:55,726] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9011
[2019-04-04 10:54:55,731] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 122.0, 0.0, 26.0, 26.44513134220929, 0.6305754277803385, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4449600.0000, 
sim time next is 4450200.0000, 
raw observation next is [0.8333333333333334, 87.0, 115.3333333333333, 0.0, 26.0, 26.43566631449908, 0.6222212052182103, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4856879039704525, 0.87, 0.3844444444444443, 0.0, 0.6666666666666666, 0.7029721928749234, 0.70740706840607, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.88844854], dtype=float32), -0.45745715]. 
=============================================
[2019-04-04 10:55:01,251] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4074149e-11 9.8204267e-10 2.2294053e-30 4.5704186e-28 5.4401849e-23
 1.0000000e+00 1.3741844e-21], sum to 1.0000
[2019-04-04 10:55:01,275] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0867
[2019-04-04 10:55:01,285] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 59.66666666666667, 204.6666666666667, 15.0, 26.0, 26.17621292804095, 0.5427284043271279, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4530000.0000, 
sim time next is 4530600.0000, 
raw observation next is [1.5, 59.0, 221.0, 18.0, 26.0, 26.2050105790231, 0.5507811084114114, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5041551246537397, 0.59, 0.7366666666666667, 0.019889502762430938, 0.6666666666666666, 0.6837508815852583, 0.6835937028038037, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2502156], dtype=float32), -0.8587108]. 
=============================================
[2019-04-04 10:55:07,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:55:07,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:55:07,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run31
[2019-04-04 10:55:12,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:55:12,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:55:12,559] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run31
[2019-04-04 10:55:14,766] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:55:14,766] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:55:14,768] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run31
[2019-04-04 10:55:16,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:55:16,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:55:16,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run31
[2019-04-04 10:55:16,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:55:16,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:55:16,442] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run31
[2019-04-04 10:55:16,803] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:55:16,803] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:55:16,807] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run31
[2019-04-04 10:55:19,293] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6267690e-10 4.3639701e-09 7.8262379e-26 7.3663666e-24 1.6890269e-19
 1.0000000e+00 6.7719576e-19], sum to 1.0000
[2019-04-04 10:55:19,294] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4676
[2019-04-04 10:55:19,336] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.42018563747419, 0.398769494746913, 0.0, 1.0, 26283.29589231608], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4831800.0000, 
sim time next is 4832400.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.4021136260202, 0.392895754940746, 0.0, 1.0, 40883.64912220329], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.61684280216835, 0.6309652516469154, 0.0, 1.0, 0.1946840434390633], 
reward next is 0.8053, 
noisyNet noise sample is [array([-0.6006098], dtype=float32), -0.5128828]. 
=============================================
[2019-04-04 10:55:21,252] A3C_AGENT_WORKER-Thread-15 INFO:Local step 255000, global step 4071418: loss 0.1362
[2019-04-04 10:55:21,252] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 255000, global step 4071418: learning rate 0.0000
[2019-04-04 10:55:23,359] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0324973e-13 8.2304892e-11 1.2081552e-30 1.1940445e-28 1.4947603e-23
 1.0000000e+00 9.3191026e-24], sum to 1.0000
[2019-04-04 10:55:23,360] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0732
[2019-04-04 10:55:23,376] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.833333333333334, 25.66666666666667, 114.3333333333333, 846.3333333333333, 26.0, 27.02271780930804, 0.7724555807165903, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4974600.0000, 
sim time next is 4975200.0000, 
raw observation next is [8.0, 26.0, 113.0, 839.5, 26.0, 27.22100045288598, 0.8000057163522296, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.37666666666666665, 0.9276243093922651, 0.6666666666666666, 0.7684167044071651, 0.7666685721174099, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8947203], dtype=float32), -0.8578205]. 
=============================================
[2019-04-04 10:55:25,689] A3C_AGENT_WORKER-Thread-16 INFO:Local step 255000, global step 4072970: loss 0.1452
[2019-04-04 10:55:25,691] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 255000, global step 4072970: learning rate 0.0000
[2019-04-04 10:55:26,738] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3087144e-12 4.5499569e-11 5.5530530e-30 4.7170820e-28 2.7687256e-22
 1.0000000e+00 5.9394951e-22], sum to 1.0000
[2019-04-04 10:55:26,738] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0862
[2019-04-04 10:55:26,744] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.5, 18.0, 0.0, 0.0, 26.0, 27.76012808678516, 0.9772095793829733, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5081400.0000, 
sim time next is 5082000.0000, 
raw observation next is [10.33333333333333, 18.33333333333333, 0.0, 0.0, 26.0, 27.66467744440866, 0.9606290719494811, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7488457987072946, 0.1833333333333333, 0.0, 0.0, 0.6666666666666666, 0.8053897870340551, 0.820209690649827, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.4361844], dtype=float32), -0.8222724]. 
=============================================
[2019-04-04 10:55:26,766] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[91.83289 ]
 [91.27461 ]
 [93.15966 ]
 [92.84292 ]
 [92.484406]], R is [[90.17150116]
 [90.26979065]
 [90.36709595]
 [90.46342468]
 [90.55879211]].
[2019-04-04 10:55:27,656] A3C_AGENT_WORKER-Thread-3 INFO:Local step 255000, global step 4073695: loss 0.1394
[2019-04-04 10:55:27,680] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 255000, global step 4073695: learning rate 0.0000
[2019-04-04 10:55:28,711] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:55:28,711] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:55:28,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run31
[2019-04-04 10:55:29,390] A3C_AGENT_WORKER-Thread-6 INFO:Local step 255000, global step 4074382: loss 0.1363
[2019-04-04 10:55:29,392] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 255000, global step 4074382: learning rate 0.0000
[2019-04-04 10:55:29,398] A3C_AGENT_WORKER-Thread-4 INFO:Local step 255000, global step 4074385: loss 0.1329
[2019-04-04 10:55:29,400] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 255000, global step 4074385: learning rate 0.0000
[2019-04-04 10:55:29,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:55:29,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:55:29,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run31
[2019-04-04 10:55:29,786] A3C_AGENT_WORKER-Thread-2 INFO:Local step 255000, global step 4074546: loss 0.1358
[2019-04-04 10:55:29,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 255000, global step 4074546: learning rate 0.0000
[2019-04-04 10:55:31,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:55:31,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:55:31,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run31
[2019-04-04 10:55:37,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:55:37,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:55:37,196] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run31
[2019-04-04 10:55:37,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:55:37,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:55:37,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run31
[2019-04-04 10:55:37,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:55:37,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:55:37,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run31
[2019-04-04 10:55:40,475] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.6114681e-10 6.9314217e-09 1.8668491e-23 1.0022726e-22 6.0524237e-19
 1.0000000e+00 2.9077328e-18], sum to 1.0000
[2019-04-04 10:55:40,476] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1349
[2019-04-04 10:55:40,565] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.83857292428236, -0.1278052470119021, 0.0, 1.0, 203489.4693139135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 199800.0000, 
sim time next is 200400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.50746784772471, -0.02983135377222553, 1.0, 1.0, 158122.9218508889], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.458955653977059, 0.4900562154092582, 1.0, 1.0, 0.7529662945280424], 
reward next is 0.2470, 
noisyNet noise sample is [array([1.2463009], dtype=float32), -0.75969696]. 
=============================================
[2019-04-04 10:55:41,780] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2963063e-11 5.4020044e-10 1.4719881e-26 4.2187532e-25 3.7228472e-20
 1.0000000e+00 1.6407052e-20], sum to 1.0000
[2019-04-04 10:55:41,780] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0736
[2019-04-04 10:55:41,838] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.933333333333334, 74.0, 116.5, 0.0, 26.0, 25.34325909390685, 0.2132915202223944, 1.0, 1.0, 24617.38142291658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 210000.0000, 
sim time next is 210600.0000, 
raw observation next is [-6.75, 73.5, 124.0, 0.0, 26.0, 25.30227397952233, 0.2101453197735029, 1.0, 1.0, 27332.17757494136], 
processed observation next is [1.0, 0.43478260869565216, 0.275623268698061, 0.735, 0.41333333333333333, 0.0, 0.6666666666666666, 0.6085228316268608, 0.5700484399245009, 1.0, 1.0, 0.13015322654733982], 
reward next is 0.8698, 
noisyNet noise sample is [array([-0.13487385], dtype=float32), 0.5024582]. 
=============================================
[2019-04-04 10:55:41,935] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4272128e-11 3.9613552e-09 2.1841220e-26 9.6464022e-25 7.8866204e-20
 1.0000000e+00 1.2277543e-19], sum to 1.0000
[2019-04-04 10:55:41,935] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6885
[2019-04-04 10:55:41,986] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.033333333333333, 77.0, 71.50000000000001, 49.33333333333332, 26.0, 25.35951376320678, 0.2270007075942393, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 206400.0000, 
sim time next is 207000.0000, 
raw observation next is [-7.85, 76.5, 79.0, 0.0, 26.0, 25.40586105610969, 0.2209010073730975, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24515235457063714, 0.765, 0.2633333333333333, 0.0, 0.6666666666666666, 0.6171550880091408, 0.5736336691243659, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28670937], dtype=float32), -0.4811276]. 
=============================================
[2019-04-04 10:55:42,001] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[79.529854]
 [80.36757 ]
 [81.19335 ]
 [81.919624]
 [82.38407 ]], R is [[79.07460022]
 [79.28385162]
 [79.49101257]
 [79.69610596]
 [79.89914703]].
[2019-04-04 10:55:42,796] A3C_AGENT_WORKER-Thread-12 INFO:Local step 255000, global step 4077940: loss 0.1389
[2019-04-04 10:55:42,796] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 255000, global step 4077940: learning rate 0.0000
[2019-04-04 10:55:42,864] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7466615e-10 3.4110736e-09 5.4861894e-27 6.4656736e-25 8.9375936e-21
 1.0000000e+00 4.1650353e-20], sum to 1.0000
[2019-04-04 10:55:42,864] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4712
[2019-04-04 10:55:42,900] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.83333333333334, 0.0, 0.0, 26.0, 25.91561127992879, 0.6624141231509898, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4655400.0000, 
sim time next is 4656000.0000, 
raw observation next is [2.0, 53.66666666666667, 0.0, 0.0, 26.0, 25.97074917543661, 0.6590977798351438, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.5366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6642290979530507, 0.719699259945048, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20826133], dtype=float32), -0.65496814]. 
=============================================
[2019-04-04 10:55:42,917] A3C_AGENT_WORKER-Thread-18 INFO:Local step 255000, global step 4077966: loss 0.1423
[2019-04-04 10:55:42,918] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 255000, global step 4077966: learning rate 0.0000
[2019-04-04 10:55:42,925] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[82.46595 ]
 [82.54512 ]
 [82.659874]
 [82.67887 ]
 [82.024086]], R is [[82.58068848]
 [82.75488281]
 [82.92733765]
 [82.90688324]
 [82.22392273]].
[2019-04-04 10:55:45,159] A3C_AGENT_WORKER-Thread-11 INFO:Local step 255000, global step 4078614: loss 0.1353
[2019-04-04 10:55:45,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 255000, global step 4078614: learning rate 0.0000
[2019-04-04 10:55:45,186] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9066234e-10 6.7171775e-09 4.2198174e-24 2.1662769e-23 6.6189687e-20
 1.0000000e+00 3.2955841e-18], sum to 1.0000
[2019-04-04 10:55:45,187] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8255
[2019-04-04 10:55:45,205] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.283333333333333, 74.16666666666667, 0.0, 0.0, 26.0, 23.64412093647865, -0.01723398274624596, 0.0, 1.0, 44216.5450763031], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 105000.0000, 
sim time next is 105600.0000, 
raw observation next is [-5.566666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 23.55876202813148, -0.03182550776897982, 0.0, 1.0, 44364.45467316863], 
processed observation next is [1.0, 0.21739130434782608, 0.3084025854108957, 0.7433333333333334, 0.0, 0.0, 0.6666666666666666, 0.4632301690109566, 0.4893914974103401, 0.0, 1.0, 0.21125930796746967], 
reward next is 0.7887, 
noisyNet noise sample is [array([-0.50631917], dtype=float32), 0.46801582]. 
=============================================
[2019-04-04 10:55:47,922] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2240034e-11 5.9829197e-10 7.8998974e-28 1.0608205e-25 7.0524401e-21
 1.0000000e+00 1.0950360e-20], sum to 1.0000
[2019-04-04 10:55:47,923] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0548
[2019-04-04 10:55:47,987] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 61.0, 157.0, 308.0, 26.0, 25.57265863610166, 0.3914542556862253, 1.0, 1.0, 47602.41413190322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 129600.0000, 
sim time next is 130200.0000, 
raw observation next is [-8.3, 61.0, 148.0, 406.3333333333334, 26.0, 25.71184389530071, 0.4038812196402607, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.23268698060941828, 0.61, 0.49333333333333335, 0.44898710865561703, 0.6666666666666666, 0.6426536579417258, 0.6346270732134203, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8193944], dtype=float32), -0.049016736]. 
=============================================
[2019-04-04 10:55:49,080] A3C_AGENT_WORKER-Thread-15 INFO:Local step 255500, global step 4079857: loss 7.8939
[2019-04-04 10:55:49,082] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 255500, global step 4079857: learning rate 0.0000
[2019-04-04 10:55:50,533] A3C_AGENT_WORKER-Thread-13 INFO:Local step 255000, global step 4080221: loss 0.1263
[2019-04-04 10:55:50,533] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 255000, global step 4080221: learning rate 0.0000
[2019-04-04 10:55:51,349] A3C_AGENT_WORKER-Thread-10 INFO:Local step 255000, global step 4080447: loss 0.1233
[2019-04-04 10:55:51,360] A3C_AGENT_WORKER-Thread-19 INFO:Local step 255000, global step 4080451: loss 0.1291
[2019-04-04 10:55:51,360] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 255000, global step 4080451: learning rate 0.0000
[2019-04-04 10:55:51,370] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 255000, global step 4080452: learning rate 0.0000
[2019-04-04 10:55:53,162] A3C_AGENT_WORKER-Thread-16 INFO:Local step 255500, global step 4081097: loss 7.9021
[2019-04-04 10:55:53,163] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 255500, global step 4081097: learning rate 0.0000
[2019-04-04 10:55:54,190] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.8871112e-11 3.0699736e-09 4.0068711e-25 6.0426035e-24 7.8649289e-20
 1.0000000e+00 2.4186350e-19], sum to 1.0000
[2019-04-04 10:55:54,190] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6630
[2019-04-04 10:55:54,253] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.41666666666667, 86.0, 34.33333333333334, 651.0, 26.0, 25.38356487944959, 0.2122684626246538, 1.0, 1.0, 18801.31329496738], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 378600.0000, 
sim time next is 379200.0000, 
raw observation next is [-15.23333333333333, 82.0, 36.66666666666666, 694.5, 26.0, 25.35711787261956, 0.220502922722089, 1.0, 1.0, 38015.83886369481], 
processed observation next is [1.0, 0.391304347826087, 0.04062788550323182, 0.82, 0.12222222222222219, 0.7674033149171271, 0.6666666666666666, 0.6130931560516301, 0.5735009742406963, 1.0, 1.0, 0.18102780411283242], 
reward next is 0.8190, 
noisyNet noise sample is [array([-0.28490686], dtype=float32), 1.0113206]. 
=============================================
[2019-04-04 10:55:55,151] A3C_AGENT_WORKER-Thread-3 INFO:Local step 255500, global step 4081665: loss 7.8949
[2019-04-04 10:55:55,155] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 255500, global step 4081665: learning rate 0.0000
[2019-04-04 10:55:57,472] A3C_AGENT_WORKER-Thread-4 INFO:Local step 255500, global step 4082385: loss 7.8861
[2019-04-04 10:55:57,473] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 255500, global step 4082385: learning rate 0.0000
[2019-04-04 10:55:57,712] A3C_AGENT_WORKER-Thread-6 INFO:Local step 255500, global step 4082448: loss 7.8817
[2019-04-04 10:55:57,713] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 255500, global step 4082448: learning rate 0.0000
[2019-04-04 10:55:58,139] A3C_AGENT_WORKER-Thread-2 INFO:Local step 255500, global step 4082553: loss 7.8861
[2019-04-04 10:55:58,140] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 255500, global step 4082553: learning rate 0.0000
[2019-04-04 10:56:05,040] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3627167e-08 4.6413227e-08 1.6926048e-21 1.8388837e-20 9.4632903e-17
 9.9999988e-01 2.2021534e-16], sum to 1.0000
[2019-04-04 10:56:05,042] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4303
[2019-04-04 10:56:05,114] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 52.0, 0.0, 0.0, 26.0, 22.70322388863458, -0.2792862664719163, 0.0, 1.0, 46747.15353025655], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 450000.0000, 
sim time next is 450600.0000, 
raw observation next is [-10.41666666666667, 50.66666666666667, 0.0, 0.0, 26.0, 22.67274610968487, -0.2808550272282899, 0.0, 1.0, 46752.29234111556], 
processed observation next is [1.0, 0.21739130434782608, 0.17405355493998145, 0.5066666666666667, 0.0, 0.0, 0.6666666666666666, 0.38939550914040577, 0.40638165759057004, 0.0, 1.0, 0.2226299635291217], 
reward next is 0.7774, 
noisyNet noise sample is [array([-0.4245864], dtype=float32), 0.36830536]. 
=============================================
[2019-04-04 10:56:06,891] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:56:06,891] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:56:06,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run31
[2019-04-04 10:56:06,922] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:56:06,924] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:56:06,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run31
[2019-04-04 10:56:07,852] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3101707e-09 4.7402349e-09 1.8293011e-25 6.0745003e-23 5.9112270e-19
 1.0000000e+00 1.7384603e-18], sum to 1.0000
[2019-04-04 10:56:07,852] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0598
[2019-04-04 10:56:07,873] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 81.50000000000001, 0.0, 0.0, 26.0, 24.35661873669568, 0.1320930022999356, 0.0, 1.0, 44256.90502000415], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 256200.0000, 
sim time next is 256800.0000, 
raw observation next is [-4.1, 81.0, 0.0, 0.0, 26.0, 24.3174094935393, 0.1237637148357233, 0.0, 1.0, 44300.22081558574], 
processed observation next is [1.0, 1.0, 0.3490304709141275, 0.81, 0.0, 0.0, 0.6666666666666666, 0.5264507911282751, 0.5412545716119078, 0.0, 1.0, 0.21095343245517018], 
reward next is 0.7890, 
noisyNet noise sample is [array([0.90717834], dtype=float32), 1.1244518]. 
=============================================
[2019-04-04 10:56:10,330] A3C_AGENT_WORKER-Thread-12 INFO:Local step 255500, global step 4086226: loss 7.8876
[2019-04-04 10:56:10,330] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 255500, global step 4086226: learning rate 0.0000
[2019-04-04 10:56:11,090] A3C_AGENT_WORKER-Thread-18 INFO:Local step 255500, global step 4086415: loss 7.8909
[2019-04-04 10:56:11,091] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 255500, global step 4086415: learning rate 0.0000
[2019-04-04 10:56:12,934] A3C_AGENT_WORKER-Thread-11 INFO:Local step 255500, global step 4086922: loss 7.8935
[2019-04-04 10:56:12,935] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 255500, global step 4086922: learning rate 0.0000
[2019-04-04 10:56:13,630] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:56:13,630] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:56:13,675] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run31
[2019-04-04 10:56:17,091] A3C_AGENT_WORKER-Thread-15 INFO:Local step 256000, global step 4087994: loss 0.5296
[2019-04-04 10:56:17,091] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 256000, global step 4087994: learning rate 0.0000
[2019-04-04 10:56:18,043] A3C_AGENT_WORKER-Thread-19 INFO:Local step 255500, global step 4088308: loss 7.9248
[2019-04-04 10:56:18,044] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 255500, global step 4088308: learning rate 0.0000
[2019-04-04 10:56:18,128] A3C_AGENT_WORKER-Thread-13 INFO:Local step 255500, global step 4088331: loss 7.9072
[2019-04-04 10:56:18,131] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 255500, global step 4088331: learning rate 0.0000
[2019-04-04 10:56:18,774] A3C_AGENT_WORKER-Thread-10 INFO:Local step 255500, global step 4088507: loss 7.9154
[2019-04-04 10:56:18,779] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 255500, global step 4088507: learning rate 0.0000
[2019-04-04 10:56:20,343] A3C_AGENT_WORKER-Thread-14 INFO:Local step 255000, global step 4088989: loss 0.1527
[2019-04-04 10:56:20,344] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 255000, global step 4088989: learning rate 0.0000
[2019-04-04 10:56:20,590] A3C_AGENT_WORKER-Thread-5 INFO:Local step 255000, global step 4089073: loss 0.1502
[2019-04-04 10:56:20,590] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 255000, global step 4089073: learning rate 0.0000
[2019-04-04 10:56:20,999] A3C_AGENT_WORKER-Thread-16 INFO:Local step 256000, global step 4089227: loss 0.5270
[2019-04-04 10:56:20,999] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 256000, global step 4089227: learning rate 0.0000
[2019-04-04 10:56:21,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.6948797e-10 6.4686496e-09 9.6512606e-25 3.0180594e-23 7.2763439e-19
 1.0000000e+00 3.8651615e-18], sum to 1.0000
[2019-04-04 10:56:21,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9234
[2019-04-04 10:56:21,264] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.933333333333334, 62.33333333333333, 92.83333333333334, 48.33333333333333, 26.0, 24.84607794507676, 0.2131143181437341, 0.0, 1.0, 52108.64159602727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 646800.0000, 
sim time next is 647400.0000, 
raw observation next is [-2.816666666666667, 61.66666666666666, 96.66666666666667, 58.66666666666666, 26.0, 24.85477532830184, 0.2199827514507892, 0.0, 1.0, 42597.84229034848], 
processed observation next is [0.0, 0.4782608695652174, 0.38457987072945526, 0.6166666666666666, 0.32222222222222224, 0.06482504604051564, 0.6666666666666666, 0.5712312773584868, 0.5733275838169297, 0.0, 1.0, 0.20284686804927848], 
reward next is 0.7972, 
noisyNet noise sample is [array([0.11358989], dtype=float32), 0.26767302]. 
=============================================
[2019-04-04 10:56:22,316] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.8065445e-11 1.3212584e-09 1.8895233e-25 8.2640778e-25 1.0559666e-19
 1.0000000e+00 1.4799265e-19], sum to 1.0000
[2019-04-04 10:56:22,317] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1021
[2019-04-04 10:56:22,362] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.2, 39.0, 37.0, 707.0, 26.0, 26.33530061043959, 0.3556928057536305, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 401400.0000, 
sim time next is 402000.0000, 
raw observation next is [-9.100000000000001, 38.66666666666667, 34.33333333333334, 656.3333333333334, 26.0, 26.20773951649481, 0.4764530745904536, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.21052631578947364, 0.3866666666666667, 0.11444444444444447, 0.7252302025782689, 0.6666666666666666, 0.6839782930412343, 0.6588176915301512, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8153527], dtype=float32), -0.70105445]. 
=============================================
[2019-04-04 10:56:22,370] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[76.76965 ]
 [77.097626]
 [77.27097 ]
 [77.63115 ]
 [77.9884  ]], R is [[76.66136169]
 [76.8947525 ]
 [77.12580872]
 [77.35455322]
 [77.58100891]].
[2019-04-04 10:56:22,463] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:56:22,463] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:56:22,470] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run31
[2019-04-04 10:56:24,451] A3C_AGENT_WORKER-Thread-3 INFO:Local step 256000, global step 4090092: loss 0.5163
[2019-04-04 10:56:24,451] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 256000, global step 4090092: learning rate 0.0000
[2019-04-04 10:56:25,102] A3C_AGENT_WORKER-Thread-4 INFO:Local step 256000, global step 4090287: loss 0.5132
[2019-04-04 10:56:25,106] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 256000, global step 4090289: learning rate 0.0000
[2019-04-04 10:56:25,397] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4690860e-10 7.4166084e-10 1.3862690e-27 5.7657985e-26 8.4720856e-21
 1.0000000e+00 3.8632857e-20], sum to 1.0000
[2019-04-04 10:56:25,402] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1683
[2019-04-04 10:56:25,420] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3856938e-11 5.5618860e-10 3.5940604e-28 7.6231851e-27 3.0730600e-21
 1.0000000e+00 4.0247567e-21], sum to 1.0000
[2019-04-04 10:56:25,421] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5652
[2019-04-04 10:56:25,423] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.1, 94.66666666666667, 0.0, 0.0, 26.0, 24.84140936145365, 0.23286656900516, 0.0, 1.0, 40669.4721206519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 513600.0000, 
sim time next is 514200.0000, 
raw observation next is [3.2, 95.33333333333333, 0.0, 0.0, 26.0, 24.83883540037029, 0.2328424515335057, 0.0, 1.0, 40535.159348651], 
processed observation next is [1.0, 0.9565217391304348, 0.551246537396122, 0.9533333333333333, 0.0, 0.0, 0.6666666666666666, 0.5699029500308574, 0.5776141505111686, 0.0, 1.0, 0.19302456832690953], 
reward next is 0.8070, 
noisyNet noise sample is [array([-1.6379286], dtype=float32), 0.9763133]. 
=============================================
[2019-04-04 10:56:25,476] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.8, 83.33333333333334, 44.66666666666667, 0.0, 26.0, 24.50110650266059, 0.1817570585806212, 0.0, 1.0, 20765.34044412126], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 56400.0000, 
sim time next is 57000.0000, 
raw observation next is [6.699999999999999, 82.66666666666667, 39.33333333333334, 0.0, 26.0, 24.51504275655866, 0.1854598810933138, 0.0, 1.0, 22896.91173825506], 
processed observation next is [0.0, 0.6521739130434783, 0.6481994459833795, 0.8266666666666667, 0.13111111111111115, 0.0, 0.6666666666666666, 0.5429202297132217, 0.5618199603644379, 0.0, 1.0, 0.10903291303930981], 
reward next is 0.8910, 
noisyNet noise sample is [array([0.89483595], dtype=float32), -0.46902564]. 
=============================================
[2019-04-04 10:56:25,484] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[85.65578]
 [85.93658]
 [86.10076]
 [86.1915 ]
 [86.25755]], R is [[85.44425964]
 [85.49093628]
 [85.44860077]
 [85.35805511]
 [85.24801636]].
[2019-04-04 10:56:25,830] A3C_AGENT_WORKER-Thread-6 INFO:Local step 256000, global step 4090545: loss 0.5053
[2019-04-04 10:56:25,831] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 256000, global step 4090546: learning rate 0.0000
[2019-04-04 10:56:25,876] A3C_AGENT_WORKER-Thread-2 INFO:Local step 256000, global step 4090563: loss 0.5115
[2019-04-04 10:56:25,877] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 256000, global step 4090563: learning rate 0.0000
[2019-04-04 10:56:27,058] A3C_AGENT_WORKER-Thread-17 INFO:Local step 255000, global step 4090960: loss 0.1625
[2019-04-04 10:56:27,058] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 255000, global step 4090960: learning rate 0.0000
[2019-04-04 10:56:35,512] A3C_AGENT_WORKER-Thread-20 INFO:Local step 255000, global step 4093576: loss 0.1717
[2019-04-04 10:56:35,512] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 255000, global step 4093576: learning rate 0.0000
[2019-04-04 10:56:37,693] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.8984060e-10 2.9578209e-09 7.9424765e-27 4.5420254e-25 2.4914813e-20
 1.0000000e+00 3.8423418e-20], sum to 1.0000
[2019-04-04 10:56:37,696] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4787
[2019-04-04 10:56:37,711] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 26.0, 24.6916633141035, 0.2088637793008469, 0.0, 1.0, 39416.20631019698], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 871200.0000, 
sim time next is 871800.0000, 
raw observation next is [-1.7, 79.00000000000001, 0.0, 0.0, 26.0, 24.72424378221826, 0.2174290738329015, 0.0, 1.0, 39390.11905270004], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.7900000000000001, 0.0, 0.0, 0.6666666666666666, 0.5603536485181883, 0.5724763579443005, 0.0, 1.0, 0.18757199548904782], 
reward next is 0.8124, 
noisyNet noise sample is [array([0.27770367], dtype=float32), -2.085987]. 
=============================================
[2019-04-04 10:56:37,899] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5434378e-10 5.9035274e-09 1.4649315e-24 4.1602492e-23 5.3620070e-19
 1.0000000e+00 2.2591518e-18], sum to 1.0000
[2019-04-04 10:56:37,899] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9757
[2019-04-04 10:56:37,959] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.1, 56.5, 18.0, 10.66666666666667, 26.0, 24.8738125902985, 0.21252186186454, 0.0, 1.0, 51106.98439196047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 665400.0000, 
sim time next is 666000.0000, 
raw observation next is [-1.2, 57.0, 13.5, 8.5, 26.0, 24.87708882489605, 0.2133209347652198, 0.0, 1.0, 46818.8056599299], 
processed observation next is [0.0, 0.7391304347826086, 0.42936288088642666, 0.57, 0.045, 0.009392265193370166, 0.6666666666666666, 0.573090735408004, 0.5711069782550733, 0.0, 1.0, 0.2229466936187138], 
reward next is 0.7771, 
noisyNet noise sample is [array([1.0531486], dtype=float32), 0.21799485]. 
=============================================
[2019-04-04 10:56:37,963] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[75.94463 ]
 [75.83016 ]
 [75.822716]
 [75.94165 ]
 [76.10377 ]], R is [[76.04315948]
 [76.03936005]
 [76.03003693]
 [76.04369354]
 [76.1023407 ]].
[2019-04-04 10:56:38,103] A3C_AGENT_WORKER-Thread-12 INFO:Local step 256000, global step 4094354: loss 0.4813
[2019-04-04 10:56:38,104] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 256000, global step 4094354: learning rate 0.0000
[2019-04-04 10:56:38,564] A3C_AGENT_WORKER-Thread-18 INFO:Local step 256000, global step 4094486: loss 0.4737
[2019-04-04 10:56:38,565] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 256000, global step 4094487: learning rate 0.0000
[2019-04-04 10:56:39,322] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7105795e-10 8.3339152e-10 6.9937473e-27 1.0113278e-24 2.2901786e-20
 1.0000000e+00 1.3571869e-19], sum to 1.0000
[2019-04-04 10:56:39,323] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7741
[2019-04-04 10:56:39,354] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 83.66666666666667, 0.0, 0.0, 26.0, 25.09870815402932, 0.2927236848424257, 0.0, 1.0, 42941.71212023772], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 593400.0000, 
sim time next is 594000.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 26.0, 25.06896119536903, 0.2861724005479393, 0.0, 1.0, 42981.73954394362], 
processed observation next is [0.0, 0.9130434782608695, 0.38504155124653744, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5890800996140859, 0.5953908001826465, 0.0, 1.0, 0.20467495020925536], 
reward next is 0.7953, 
noisyNet noise sample is [array([-0.2468864], dtype=float32), -0.51381254]. 
=============================================
[2019-04-04 10:56:39,357] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[80.62388]
 [80.76458]
 [80.88555]
 [81.01318]
 [81.05022]], R is [[80.48881531]
 [80.47944641]
 [80.4704361 ]
 [80.46060944]
 [80.44876862]].
[2019-04-04 10:56:39,953] A3C_AGENT_WORKER-Thread-15 INFO:Local step 256500, global step 4094993: loss 0.0007
[2019-04-04 10:56:39,954] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 256500, global step 4094993: learning rate 0.0000
[2019-04-04 10:56:40,012] A3C_AGENT_WORKER-Thread-11 INFO:Local step 256000, global step 4095014: loss 0.4797
[2019-04-04 10:56:40,013] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 256000, global step 4095014: learning rate 0.0000
[2019-04-04 10:56:44,193] A3C_AGENT_WORKER-Thread-16 INFO:Local step 256500, global step 4096573: loss 0.0010
[2019-04-04 10:56:44,197] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 256500, global step 4096573: learning rate 0.0000
[2019-04-04 10:56:44,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7055905e-12 4.2373655e-10 1.3486335e-29 1.1485692e-27 4.0905978e-23
 1.0000000e+00 3.0236612e-22], sum to 1.0000
[2019-04-04 10:56:44,234] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4168
[2019-04-04 10:56:44,256] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 83.0, 0.0, 0.0, 26.0, 25.69277958036484, 0.4794230530332571, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 972600.0000, 
sim time next is 973200.0000, 
raw observation next is [9.200000000000001, 83.0, 0.0, 0.0, 26.0, 25.70443283018223, 0.4640435899644521, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7174515235457064, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6420360691818526, 0.6546811966548174, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8523523], dtype=float32), 0.650936]. 
=============================================
[2019-04-04 10:56:44,575] A3C_AGENT_WORKER-Thread-19 INFO:Local step 256000, global step 4096746: loss 0.4661
[2019-04-04 10:56:44,576] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 256000, global step 4096746: learning rate 0.0000
[2019-04-04 10:56:45,136] A3C_AGENT_WORKER-Thread-13 INFO:Local step 256000, global step 4096999: loss 0.4691
[2019-04-04 10:56:45,139] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 256000, global step 4096999: learning rate 0.0000
[2019-04-04 10:56:45,182] A3C_AGENT_WORKER-Thread-10 INFO:Local step 256000, global step 4097021: loss 0.4700
[2019-04-04 10:56:45,183] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 256000, global step 4097021: learning rate 0.0000
[2019-04-04 10:56:45,468] A3C_AGENT_WORKER-Thread-5 INFO:Local step 255500, global step 4097141: loss 7.8278
[2019-04-04 10:56:45,469] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 255500, global step 4097141: learning rate 0.0000
[2019-04-04 10:56:46,127] A3C_AGENT_WORKER-Thread-3 INFO:Local step 256500, global step 4097354: loss 0.0008
[2019-04-04 10:56:46,134] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 256500, global step 4097357: learning rate 0.0000
[2019-04-04 10:56:46,138] A3C_AGENT_WORKER-Thread-14 INFO:Local step 255500, global step 4097358: loss 7.8333
[2019-04-04 10:56:46,138] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 255500, global step 4097358: learning rate 0.0000
[2019-04-04 10:56:46,900] A3C_AGENT_WORKER-Thread-4 INFO:Local step 256500, global step 4097598: loss 0.0008
[2019-04-04 10:56:46,905] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 256500, global step 4097602: learning rate 0.0000
[2019-04-04 10:56:49,731] A3C_AGENT_WORKER-Thread-6 INFO:Local step 256500, global step 4098302: loss 0.0012
[2019-04-04 10:56:49,732] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 256500, global step 4098302: learning rate 0.0000
[2019-04-04 10:56:49,827] A3C_AGENT_WORKER-Thread-2 INFO:Local step 256500, global step 4098327: loss 0.0014
[2019-04-04 10:56:49,837] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 256500, global step 4098327: learning rate 0.0000
[2019-04-04 10:56:52,121] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7205460e-13 2.2694226e-10 1.7647952e-31 4.4089956e-29 1.6939265e-24
 1.0000000e+00 3.6442954e-23], sum to 1.0000
[2019-04-04 10:56:52,121] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7101
[2019-04-04 10:56:52,150] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.1, 77.5, 0.0, 0.0, 26.0, 25.74390681065482, 0.552075879823151, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1045800.0000, 
sim time next is 1046400.0000, 
raw observation next is [14.2, 77.33333333333333, 0.0, 0.0, 26.0, 25.62602918164495, 0.5557659432849091, 0.0, 1.0, 139493.2669957486], 
processed observation next is [1.0, 0.08695652173913043, 0.8559556786703602, 0.7733333333333333, 0.0, 0.0, 0.6666666666666666, 0.6355024318037458, 0.685255314428303, 0.0, 1.0, 0.6642536523607075], 
reward next is 0.3357, 
noisyNet noise sample is [array([0.50670075], dtype=float32), 1.3452471]. 
=============================================
[2019-04-04 10:56:52,339] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.8056507e-12 1.6352211e-10 5.0794703e-28 1.0309705e-26 5.1693093e-21
 1.0000000e+00 3.0125641e-21], sum to 1.0000
[2019-04-04 10:56:52,340] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5291
[2019-04-04 10:56:52,380] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4, 45.66666666666667, 82.16666666666667, 26.33333333333334, 26.0, 25.47918554312814, 0.3709551744391008, 1.0, 1.0, 29836.82363255821], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 747600.0000, 
sim time next is 748200.0000, 
raw observation next is [-0.5, 45.33333333333333, 79.33333333333333, 21.66666666666667, 26.0, 25.59122230955138, 0.3852149099914141, 1.0, 1.0, 24271.85798995002], 
processed observation next is [1.0, 0.6521739130434783, 0.44875346260387816, 0.4533333333333333, 0.2644444444444444, 0.023941068139963172, 0.6666666666666666, 0.6326018591292817, 0.628404969997138, 1.0, 1.0, 0.11558027614261913], 
reward next is 0.8844, 
noisyNet noise sample is [array([-0.43134812], dtype=float32), 0.026914563]. 
=============================================
[2019-04-04 10:56:52,641] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5330607e-13 3.8634873e-11 2.0157427e-32 1.0019703e-30 1.1548529e-25
 1.0000000e+00 2.2014392e-24], sum to 1.0000
[2019-04-04 10:56:52,647] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3550
[2019-04-04 10:56:52,739] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 81.0, 81.66666666666667, 0.0, 26.0, 25.75015858897729, 0.5693208139485835, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1003800.0000, 
sim time next is 1004400.0000, 
raw observation next is [14.4, 81.0, 75.5, 0.0, 26.0, 24.30480128871091, 0.482859866546478, 1.0, 1.0, 195414.3100041063], 
processed observation next is [1.0, 0.6521739130434783, 0.8614958448753465, 0.81, 0.25166666666666665, 0.0, 0.6666666666666666, 0.5254001073925757, 0.660953288848826, 1.0, 1.0, 0.9305443333528871], 
reward next is 0.0695, 
noisyNet noise sample is [array([-0.23076302], dtype=float32), -0.127766]. 
=============================================
[2019-04-04 10:56:54,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7510673e-10 2.4978629e-08 1.4866213e-25 1.5843621e-23 4.2936614e-19
 1.0000000e+00 2.9192358e-18], sum to 1.0000
[2019-04-04 10:56:54,077] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6662
[2019-04-04 10:56:54,085] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 26.0, 23.82537196519164, 0.2138621155149616, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1223400.0000, 
sim time next is 1224000.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 26.0, 23.80664355602702, 0.2091738255093381, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8919667590027703, 0.93, 0.0, 0.0, 0.6666666666666666, 0.4838869630022516, 0.5697246085031127, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39024514], dtype=float32), -0.5835297]. 
=============================================
[2019-04-04 10:56:54,146] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[79.08799]
 [79.09016]
 [79.08807]
 [79.08229]
 [79.06452]], R is [[79.283638  ]
 [79.49079895]
 [79.69589233]
 [79.89893341]
 [80.09994507]].
[2019-04-04 10:56:55,348] A3C_AGENT_WORKER-Thread-17 INFO:Local step 255500, global step 4099861: loss 7.8001
[2019-04-04 10:56:55,349] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 255500, global step 4099861: learning rate 0.0000
[2019-04-04 10:56:55,862] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 10:56:55,865] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:56:55,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:56:55,867] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run42
[2019-04-04 10:56:55,909] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:56:55,910] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:56:55,924] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run42
[2019-04-04 10:56:55,989] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:56:55,990] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:56:55,992] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run42
[2019-04-04 10:58:10,061] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.2724537], dtype=float32), 0.3220823]
[2019-04-04 10:58:10,062] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.910255281333334, 92.47229410333334, 140.7703070833334, 495.2267165, 26.0, 25.57919965043389, 0.4364416412288863, 1.0, 1.0, 0.0]
[2019-04-04 10:58:10,062] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:58:10,062] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.3506671e-13 2.4317867e-11 4.7330604e-32 8.0931548e-30 1.3737316e-24
 1.0000000e+00 8.5372823e-24], sampled 0.3629268393998176
[2019-04-04 10:58:25,588] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.2724537], dtype=float32), 0.3220823]
[2019-04-04 10:58:25,588] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.425303401, 88.89293664, 0.0, 0.0, 26.0, 24.67633061160709, 0.1564746215436657, 0.0, 1.0, 45539.12783467587]
[2019-04-04 10:58:25,589] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:58:25,590] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.4213913e-10 3.3708449e-09 3.0202905e-25 2.1361605e-23 3.2654113e-19
 1.0000000e+00 2.1510402e-18], sampled 0.1869105664872457
[2019-04-04 10:58:53,347] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.2724537], dtype=float32), 0.3220823]
[2019-04-04 10:58:53,347] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.666666666666667, 37.0, 0.0, 0.0, 26.0, 25.12055416070714, 0.3834898324617479, 0.0, 1.0, 71948.24539863768]
[2019-04-04 10:58:53,347] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:58:53,348] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.9809736e-10 2.6392843e-09 1.4430343e-24 3.9880424e-23 9.1073826e-19
 1.0000000e+00 3.2869713e-18], sampled 0.5076031800210175
[2019-04-04 11:00:04,484] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 11:00:35,018] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 11:00:39,410] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 11:00:40,442] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 4100000, evaluation results [4100000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 11:00:41,950] A3C_AGENT_WORKER-Thread-15 INFO:Local step 257000, global step 4100437: loss 0.1718
[2019-04-04 11:00:41,989] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 257000, global step 4100446: learning rate 0.0000
[2019-04-04 11:00:44,094] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.8399796e-10 3.6269023e-09 3.2608188e-26 7.3443931e-25 6.5692390e-20
 1.0000000e+00 1.8132439e-19], sum to 1.0000
[2019-04-04 11:00:44,094] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4876
[2019-04-04 11:00:44,134] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 26.0, 24.55932522064283, 0.3747685535063565, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1197600.0000, 
sim time next is 1198200.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 26.0, 24.56310063450866, 0.3752251825763632, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.9529085872576178, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5469250528757218, 0.6250750608587877, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.888715], dtype=float32), -1.8464645]. 
=============================================
[2019-04-04 11:00:47,283] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5273934e-12 5.2724179e-11 7.7900876e-31 9.0448772e-29 6.2959340e-24
 1.0000000e+00 6.8639078e-23], sum to 1.0000
[2019-04-04 11:00:47,284] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0404
[2019-04-04 11:00:47,330] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 73.5, 0.0, 26.0, 26.06933093980359, 0.5858146544161991, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1332000.0000, 
sim time next is 1332600.0000, 
raw observation next is [0.6000000000000001, 92.0, 83.00000000000001, 0.0, 26.0, 26.08377060801283, 0.5894985798686035, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.479224376731302, 0.92, 0.2766666666666667, 0.0, 0.6666666666666666, 0.6736475506677359, 0.6964995266228678, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5089813], dtype=float32), -0.24272949]. 
=============================================
[2019-04-04 11:00:49,114] A3C_AGENT_WORKER-Thread-12 INFO:Local step 256500, global step 4102478: loss 0.0017
[2019-04-04 11:00:49,117] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 256500, global step 4102479: learning rate 0.0000
[2019-04-04 11:00:49,498] A3C_AGENT_WORKER-Thread-16 INFO:Local step 257000, global step 4102640: loss 0.1788
[2019-04-04 11:00:49,514] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 257000, global step 4102643: learning rate 0.0000
[2019-04-04 11:00:50,023] A3C_AGENT_WORKER-Thread-18 INFO:Local step 256500, global step 4102829: loss 0.0020
[2019-04-04 11:00:50,024] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 256500, global step 4102829: learning rate 0.0000
[2019-04-04 11:00:50,124] A3C_AGENT_WORKER-Thread-20 INFO:Local step 255500, global step 4102862: loss 7.7894
[2019-04-04 11:00:50,125] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 255500, global step 4102862: learning rate 0.0000
[2019-04-04 11:00:50,134] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.1606951e-11 7.1328672e-09 5.9760705e-27 5.3966290e-25 3.3789753e-20
 1.0000000e+00 1.3219349e-19], sum to 1.0000
[2019-04-04 11:00:50,139] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1040
[2019-04-04 11:00:50,146] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.3, 65.0, 112.0, 0.0, 26.0, 25.05497002171908, 0.4945160273632685, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1176600.0000, 
sim time next is 1177200.0000, 
raw observation next is [18.3, 65.0, 104.0, 0.0, 26.0, 25.04221103518655, 0.4970774331722702, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.9695290858725764, 0.65, 0.3466666666666667, 0.0, 0.6666666666666666, 0.5868509195988793, 0.6656924777240901, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1408327], dtype=float32), -1.8150096]. 
=============================================
[2019-04-04 11:00:51,752] A3C_AGENT_WORKER-Thread-11 INFO:Local step 256500, global step 4103364: loss 0.0029
[2019-04-04 11:00:51,752] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 256500, global step 4103364: learning rate 0.0000
[2019-04-04 11:00:52,688] A3C_AGENT_WORKER-Thread-3 INFO:Local step 257000, global step 4103636: loss 0.1783
[2019-04-04 11:00:52,693] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 257000, global step 4103638: learning rate 0.0000
[2019-04-04 11:00:54,576] A3C_AGENT_WORKER-Thread-4 INFO:Local step 257000, global step 4104143: loss 0.1844
[2019-04-04 11:00:54,594] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 257000, global step 4104147: learning rate 0.0000
[2019-04-04 11:00:56,514] A3C_AGENT_WORKER-Thread-6 INFO:Local step 257000, global step 4104730: loss 0.1858
[2019-04-04 11:00:56,515] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 257000, global step 4104730: learning rate 0.0000
[2019-04-04 11:00:57,105] A3C_AGENT_WORKER-Thread-2 INFO:Local step 257000, global step 4104909: loss 0.1818
[2019-04-04 11:00:57,117] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 257000, global step 4104909: learning rate 0.0000
[2019-04-04 11:00:57,183] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.43744887e-13 8.91955398e-11 2.50606086e-31 1.71366087e-29
 4.43345942e-25 1.00000000e+00 1.12303114e-23], sum to 1.0000
[2019-04-04 11:00:57,184] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9875
[2019-04-04 11:00:57,244] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.63333333333333, 78.66666666666667, 0.0, 0.0, 26.0, 25.58917289630282, 0.5980431796589701, 0.0, 1.0, 93292.53656225745], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1056000.0000, 
sim time next is 1056600.0000, 
raw observation next is [13.55, 79.0, 0.0, 0.0, 26.0, 25.68678716663592, 0.6097492725975568, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.8379501385041552, 0.79, 0.0, 0.0, 0.6666666666666666, 0.64056559721966, 0.703249757532519, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.73168725], dtype=float32), -1.8018416]. 
=============================================
[2019-04-04 11:00:58,843] A3C_AGENT_WORKER-Thread-19 INFO:Local step 256500, global step 4105479: loss 0.0016
[2019-04-04 11:00:58,849] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 256500, global step 4105479: learning rate 0.0000
[2019-04-04 11:01:00,748] A3C_AGENT_WORKER-Thread-10 INFO:Local step 256500, global step 4106072: loss 0.0014
[2019-04-04 11:01:00,748] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 256500, global step 4106072: learning rate 0.0000
[2019-04-04 11:01:01,059] A3C_AGENT_WORKER-Thread-13 INFO:Local step 256500, global step 4106152: loss 0.0014
[2019-04-04 11:01:01,062] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 256500, global step 4106152: learning rate 0.0000
[2019-04-04 11:01:05,141] A3C_AGENT_WORKER-Thread-14 INFO:Local step 256000, global step 4107178: loss 0.4912
[2019-04-04 11:01:05,142] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 256000, global step 4107178: learning rate 0.0000
[2019-04-04 11:01:05,250] A3C_AGENT_WORKER-Thread-5 INFO:Local step 256000, global step 4107213: loss 0.4904
[2019-04-04 11:01:05,251] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 256000, global step 4107213: learning rate 0.0000
[2019-04-04 11:01:10,324] A3C_AGENT_WORKER-Thread-15 INFO:Local step 257500, global step 4108613: loss 0.0295
[2019-04-04 11:01:10,366] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 257500, global step 4108614: learning rate 0.0000
[2019-04-04 11:01:11,947] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6067994e-13 8.6174706e-11 2.1368783e-30 1.2535552e-28 7.2515016e-23
 1.0000000e+00 8.6658434e-23], sum to 1.0000
[2019-04-04 11:01:11,947] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7518
[2019-04-04 11:01:11,983] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 61.5, 84.0, 779.0, 26.0, 25.83433564968857, 0.3894602823792581, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 732600.0000, 
sim time next is 733200.0000, 
raw observation next is [-0.6, 60.0, 91.83333333333333, 724.0, 26.0, 25.85743096718046, 0.3917333047020322, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.6, 0.3061111111111111, 0.8, 0.6666666666666666, 0.6547859139317049, 0.6305777682340107, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5421078], dtype=float32), 0.7128573]. 
=============================================
[2019-04-04 11:01:12,257] A3C_AGENT_WORKER-Thread-12 INFO:Local step 257000, global step 4109058: loss 0.1570
[2019-04-04 11:01:12,297] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 257000, global step 4109058: learning rate 0.0000
[2019-04-04 11:01:14,733] A3C_AGENT_WORKER-Thread-17 INFO:Local step 256000, global step 4109674: loss 0.4992
[2019-04-04 11:01:14,739] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 256000, global step 4109674: learning rate 0.0000
[2019-04-04 11:01:15,082] A3C_AGENT_WORKER-Thread-18 INFO:Local step 257000, global step 4109746: loss 0.1485
[2019-04-04 11:01:15,124] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 257000, global step 4109746: learning rate 0.0000
[2019-04-04 11:01:17,715] A3C_AGENT_WORKER-Thread-11 INFO:Local step 257000, global step 4110366: loss 0.1460
[2019-04-04 11:01:17,733] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 257000, global step 4110366: learning rate 0.0000
[2019-04-04 11:01:19,470] A3C_AGENT_WORKER-Thread-16 INFO:Local step 257500, global step 4110799: loss 0.0308
[2019-04-04 11:01:19,472] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 257500, global step 4110799: learning rate 0.0000
[2019-04-04 11:01:20,227] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7865684e-09 1.0076775e-08 1.5264688e-25 1.2398381e-23 7.5465380e-20
 1.0000000e+00 4.7515622e-19], sum to 1.0000
[2019-04-04 11:01:20,227] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5017
[2019-04-04 11:01:20,231] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.0, 65.0, 0.0, 0.0, 26.0, 24.79336267688942, 0.4237380302512479, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1189800.0000, 
sim time next is 1190400.0000, 
raw observation next is [17.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.77780888046087, 0.4180460693653203, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.9584487534626038, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5648174067050725, 0.6393486897884401, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3977158], dtype=float32), -1.2054418]. 
=============================================
[2019-04-04 11:01:21,483] A3C_AGENT_WORKER-Thread-3 INFO:Local step 257500, global step 4111696: loss 0.0298
[2019-04-04 11:01:21,485] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 257500, global step 4111696: learning rate 0.0000
[2019-04-04 11:01:21,795] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.2584410e-11 6.1140154e-10 3.0720643e-28 6.2709752e-26 2.0110689e-21
 1.0000000e+00 1.6019036e-20], sum to 1.0000
[2019-04-04 11:01:21,806] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1496
[2019-04-04 11:01:21,847] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 75.0, 99.0, 0.0, 26.0, 25.51651139907253, 0.2923600184670425, 1.0, 1.0, 27797.11690792053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 822600.0000, 
sim time next is 823200.0000, 
raw observation next is [-4.5, 76.33333333333333, 97.66666666666666, 0.0, 26.0, 25.43855783902186, 0.2890959122264865, 1.0, 1.0, 27884.98502131235], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7633333333333333, 0.32555555555555554, 0.0, 0.6666666666666666, 0.6198798199184884, 0.5963653040754955, 1.0, 1.0, 0.13278564295863024], 
reward next is 0.8672, 
noisyNet noise sample is [array([0.50760394], dtype=float32), -0.3114989]. 
=============================================
[2019-04-04 11:01:22,965] A3C_AGENT_WORKER-Thread-4 INFO:Local step 257500, global step 4112331: loss 0.0268
[2019-04-04 11:01:22,966] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 257500, global step 4112333: learning rate 0.0000
[2019-04-04 11:01:22,979] A3C_AGENT_WORKER-Thread-19 INFO:Local step 257000, global step 4112337: loss 0.1400
[2019-04-04 11:01:22,988] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 257000, global step 4112337: learning rate 0.0000
[2019-04-04 11:01:23,822] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.7586904e-12 6.5461046e-11 1.8036593e-29 3.6204394e-28 2.1235509e-23
 1.0000000e+00 8.0982219e-22], sum to 1.0000
[2019-04-04 11:01:23,822] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1892
[2019-04-04 11:01:23,878] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.57758321644909, 0.565647857631177, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1711800.0000, 
sim time next is 1712400.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.7084951012651, 0.572078462928711, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.6423745917720917, 0.690692820976237, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0183488], dtype=float32), 0.51677126]. 
=============================================
[2019-04-04 11:01:24,121] A3C_AGENT_WORKER-Thread-10 INFO:Local step 257000, global step 4112763: loss 0.1360
[2019-04-04 11:01:24,122] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 257000, global step 4112763: learning rate 0.0000
[2019-04-04 11:01:24,296] A3C_AGENT_WORKER-Thread-2 INFO:Local step 257500, global step 4112838: loss 0.0266
[2019-04-04 11:01:24,297] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 257500, global step 4112839: learning rate 0.0000
[2019-04-04 11:01:24,321] A3C_AGENT_WORKER-Thread-6 INFO:Local step 257500, global step 4112846: loss 0.0240
[2019-04-04 11:01:24,323] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 257500, global step 4112846: learning rate 0.0000
[2019-04-04 11:01:24,424] A3C_AGENT_WORKER-Thread-13 INFO:Local step 257000, global step 4112890: loss 0.1362
[2019-04-04 11:01:24,424] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 257000, global step 4112890: learning rate 0.0000
[2019-04-04 11:01:24,664] A3C_AGENT_WORKER-Thread-20 INFO:Local step 256000, global step 4112998: loss 0.5380
[2019-04-04 11:01:24,669] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 256000, global step 4112998: learning rate 0.0000
[2019-04-04 11:01:26,107] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.67350203e-13 1.25008484e-11 6.61178662e-32 2.10522612e-30
 2.51245296e-25 1.00000000e+00 2.54296255e-24], sum to 1.0000
[2019-04-04 11:01:26,110] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3277
[2019-04-04 11:01:26,118] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 49.0, 171.5, 20.66666666666666, 26.0, 26.88419361551897, 0.7823381353216771, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1603200.0000, 
sim time next is 1603800.0000, 
raw observation next is [13.8, 49.0, 176.0, 0.0, 26.0, 26.92384879198096, 0.8084208496655352, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.844875346260388, 0.49, 0.5866666666666667, 0.0, 0.6666666666666666, 0.7436540659984132, 0.7694736165551784, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54623324], dtype=float32), -0.87598044]. 
=============================================
[2019-04-04 11:01:32,973] A3C_AGENT_WORKER-Thread-14 INFO:Local step 256500, global step 4116312: loss 0.0046
[2019-04-04 11:01:32,974] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 256500, global step 4116312: learning rate 0.0000
[2019-04-04 11:01:33,118] A3C_AGENT_WORKER-Thread-5 INFO:Local step 256500, global step 4116374: loss 0.0037
[2019-04-04 11:01:33,118] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 256500, global step 4116374: learning rate 0.0000
[2019-04-04 11:01:34,467] A3C_AGENT_WORKER-Thread-12 INFO:Local step 257500, global step 4116860: loss 0.0165
[2019-04-04 11:01:34,469] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 257500, global step 4116860: learning rate 0.0000
[2019-04-04 11:01:36,495] A3C_AGENT_WORKER-Thread-18 INFO:Local step 257500, global step 4117590: loss 0.0144
[2019-04-04 11:01:36,496] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 257500, global step 4117590: learning rate 0.0000
[2019-04-04 11:01:36,554] A3C_AGENT_WORKER-Thread-15 INFO:Local step 258000, global step 4117611: loss 3.1987
[2019-04-04 11:01:36,554] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 258000, global step 4117611: learning rate 0.0000
[2019-04-04 11:01:37,304] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6784416e-12 2.7371001e-11 1.3158541e-31 1.0499643e-29 6.4280222e-25
 1.0000000e+00 1.3531931e-23], sum to 1.0000
[2019-04-04 11:01:37,304] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8026
[2019-04-04 11:01:37,313] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.1, 77.5, 0.0, 0.0, 26.0, 25.74390684315537, 0.5520758618012365, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1045800.0000, 
sim time next is 1046400.0000, 
raw observation next is [14.2, 77.33333333333333, 0.0, 0.0, 26.0, 25.62602920646047, 0.5557659125852553, 0.0, 1.0, 139493.180724878], 
processed observation next is [1.0, 0.08695652173913043, 0.8559556786703602, 0.7733333333333333, 0.0, 0.0, 0.6666666666666666, 0.6355024338717058, 0.6852553041950852, 0.0, 1.0, 0.6642532415470381], 
reward next is 0.3357, 
noisyNet noise sample is [array([0.06643943], dtype=float32), -0.07973367]. 
=============================================
[2019-04-04 11:01:37,703] A3C_AGENT_WORKER-Thread-11 INFO:Local step 257500, global step 4118064: loss 0.0129
[2019-04-04 11:01:37,706] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 257500, global step 4118066: learning rate 0.0000
[2019-04-04 11:01:38,162] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0265682e-10 3.0405336e-09 2.7988800e-25 2.9255714e-23 2.1415008e-19
 1.0000000e+00 1.3703755e-18], sum to 1.0000
[2019-04-04 11:01:38,163] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0666
[2019-04-04 11:01:38,187] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 84.66666666666667, 0.0, 0.0, 26.0, 24.85938705886499, 0.2590799943733808, 0.0, 1.0, 45786.21052959968], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1808400.0000, 
sim time next is 1809000.0000, 
raw observation next is [-5.0, 84.0, 0.0, 0.0, 26.0, 24.81704498981393, 0.2515388164804643, 0.0, 1.0, 45746.00818535707], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5680870824844941, 0.5838462721601547, 0.0, 1.0, 0.21783813421598602], 
reward next is 0.7822, 
noisyNet noise sample is [array([1.6626315], dtype=float32), -0.33181214]. 
=============================================
[2019-04-04 11:01:38,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[76.82645 ]
 [76.91115 ]
 [76.98622 ]
 [77.054306]
 [77.10589 ]], R is [[76.77339172]
 [76.78762817]
 [76.80157471]
 [76.81529236]
 [76.8287735 ]].
[2019-04-04 11:01:38,719] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.6001384e-12 4.0694578e-10 1.0155938e-28 6.9397188e-27 5.8764589e-22
 1.0000000e+00 2.1263984e-21], sum to 1.0000
[2019-04-04 11:01:38,719] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6345
[2019-04-04 11:01:38,738] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.37514654525567, 0.4504109931871644, 0.0, 1.0, 35118.38499901313], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1490400.0000, 
sim time next is 1491000.0000, 
raw observation next is [2.016666666666667, 96.66666666666666, 0.0, 0.0, 26.0, 25.37757541220643, 0.458152163907988, 0.0, 1.0, 34730.30388378398], 
processed observation next is [1.0, 0.2608695652173913, 0.5184672206832872, 0.9666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6147979510172025, 0.6527173879693293, 0.0, 1.0, 0.16538239944659036], 
reward next is 0.8346, 
noisyNet noise sample is [array([0.28251082], dtype=float32), -1.4132047]. 
=============================================
[2019-04-04 11:01:38,744] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[86.29975 ]
 [86.328804]
 [86.33366 ]
 [86.34521 ]
 [86.35331 ]], R is [[86.2310791 ]
 [86.20153809]
 [86.16331482]
 [86.12585449]
 [86.08888245]].
[2019-04-04 11:01:39,029] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5933574e-11 4.0739001e-10 3.2178735e-29 8.3130099e-28 3.2257386e-22
 1.0000000e+00 1.4857343e-21], sum to 1.0000
[2019-04-04 11:01:39,029] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1741
[2019-04-04 11:01:39,061] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.45110019290419, 0.4448129560153344, 0.0, 1.0, 54983.21056094742], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 970800.0000, 
sim time next is 971400.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 26.0, 25.45994589277844, 0.467837792626999, 0.0, 1.0, 40248.97528848458], 
processed observation next is [1.0, 0.21739130434782608, 0.7063711911357342, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6216621577315365, 0.6559459308756663, 0.0, 1.0, 0.1916617870880218], 
reward next is 0.8083, 
noisyNet noise sample is [array([0.6886035], dtype=float32), 1.5001414]. 
=============================================
[2019-04-04 11:01:39,107] A3C_AGENT_WORKER-Thread-17 INFO:Local step 256500, global step 4118627: loss 0.0050
[2019-04-04 11:01:39,107] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 256500, global step 4118627: learning rate 0.0000
[2019-04-04 11:01:39,780] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2349458e-10 1.1325685e-09 6.2882950e-26 4.1899183e-24 2.5455882e-20
 1.0000000e+00 7.8499161e-19], sum to 1.0000
[2019-04-04 11:01:39,780] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9869
[2019-04-04 11:01:39,798] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.9, 79.66666666666667, 0.0, 0.0, 26.0, 24.79005884108267, 0.2480154112083507, 0.0, 1.0, 41164.91642426485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 859800.0000, 
sim time next is 860400.0000, 
raw observation next is [-2.8, 79.0, 0.0, 0.0, 26.0, 24.79141790969985, 0.2419990197155561, 0.0, 1.0, 41034.75088299923], 
processed observation next is [1.0, 1.0, 0.38504155124653744, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5659514924749874, 0.5806663399051853, 0.0, 1.0, 0.19540357563332966], 
reward next is 0.8046, 
noisyNet noise sample is [array([-0.78421634], dtype=float32), 0.1884644]. 
=============================================
[2019-04-04 11:01:41,084] A3C_AGENT_WORKER-Thread-16 INFO:Local step 258000, global step 4119580: loss 3.1366
[2019-04-04 11:01:41,087] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 258000, global step 4119581: learning rate 0.0000
[2019-04-04 11:01:41,660] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.4535937e-12 2.2869950e-10 1.5941247e-28 5.2275547e-27 2.7466026e-22
 1.0000000e+00 1.4788302e-21], sum to 1.0000
[2019-04-04 11:01:41,662] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8997
[2019-04-04 11:01:41,693] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.516666666666667, 82.16666666666667, 37.66666666666667, 0.0, 26.0, 26.07920314543801, 0.4540562313181396, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1699800.0000, 
sim time next is 1700400.0000, 
raw observation next is [1.433333333333334, 83.33333333333334, 33.83333333333333, 0.0, 26.0, 25.58174070875814, 0.4656407066042886, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.502308402585411, 0.8333333333333335, 0.11277777777777777, 0.0, 0.6666666666666666, 0.631811725729845, 0.6552135688680961, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4921942], dtype=float32), 0.23009412]. 
=============================================
[2019-04-04 11:01:42,390] A3C_AGENT_WORKER-Thread-19 INFO:Local step 257500, global step 4120177: loss 0.0086
[2019-04-04 11:01:42,390] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 257500, global step 4120177: learning rate 0.0000
[2019-04-04 11:01:42,802] A3C_AGENT_WORKER-Thread-13 INFO:Local step 257500, global step 4120343: loss 0.0088
[2019-04-04 11:01:42,806] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 257500, global step 4120344: learning rate 0.0000
[2019-04-04 11:01:42,868] A3C_AGENT_WORKER-Thread-10 INFO:Local step 257500, global step 4120372: loss 0.0089
[2019-04-04 11:01:42,877] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 257500, global step 4120374: learning rate 0.0000
[2019-04-04 11:01:43,424] A3C_AGENT_WORKER-Thread-3 INFO:Local step 258000, global step 4120615: loss 3.0965
[2019-04-04 11:01:43,425] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 258000, global step 4120615: learning rate 0.0000
[2019-04-04 11:01:44,436] A3C_AGENT_WORKER-Thread-4 INFO:Local step 258000, global step 4121024: loss 3.0922
[2019-04-04 11:01:44,437] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 258000, global step 4121024: learning rate 0.0000
[2019-04-04 11:01:45,667] A3C_AGENT_WORKER-Thread-6 INFO:Local step 258000, global step 4121540: loss 3.1251
[2019-04-04 11:01:45,668] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 258000, global step 4121540: learning rate 0.0000
[2019-04-04 11:01:46,026] A3C_AGENT_WORKER-Thread-2 INFO:Local step 258000, global step 4121686: loss 3.1036
[2019-04-04 11:01:46,027] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 258000, global step 4121687: learning rate 0.0000
[2019-04-04 11:01:46,604] A3C_AGENT_WORKER-Thread-14 INFO:Local step 257000, global step 4121949: loss 0.0908
[2019-04-04 11:01:46,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 257000, global step 4121949: learning rate 0.0000
[2019-04-04 11:01:47,014] A3C_AGENT_WORKER-Thread-20 INFO:Local step 256500, global step 4122134: loss 0.0050
[2019-04-04 11:01:47,016] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 256500, global step 4122134: learning rate 0.0000
[2019-04-04 11:01:47,255] A3C_AGENT_WORKER-Thread-5 INFO:Local step 257000, global step 4122235: loss 0.0881
[2019-04-04 11:01:47,256] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 257000, global step 4122236: learning rate 0.0000
[2019-04-04 11:01:50,661] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.13751063e-12 3.36748060e-11 1.25078345e-30 1.14135760e-28
 1.23107219e-23 1.00000000e+00 1.83792827e-22], sum to 1.0000
[2019-04-04 11:01:50,661] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1548
[2019-04-04 11:01:50,674] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 26.06520979874185, 0.6319593821501984, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1039800.0000, 
sim time next is 1040400.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 26.03198824916384, 0.6330752410156313, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6693323540969868, 0.7110250803385437, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3649578], dtype=float32), 0.008986848]. 
=============================================
[2019-04-04 11:01:53,460] A3C_AGENT_WORKER-Thread-17 INFO:Local step 257000, global step 4124495: loss 0.0800
[2019-04-04 11:01:53,461] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 257000, global step 4124495: learning rate 0.0000
[2019-04-04 11:01:55,816] A3C_AGENT_WORKER-Thread-12 INFO:Local step 258000, global step 4125310: loss 3.0324
[2019-04-04 11:01:55,817] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 258000, global step 4125310: learning rate 0.0000
[2019-04-04 11:01:56,997] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0174029e-11 8.3855445e-10 1.7974776e-29 1.0133614e-27 7.9683490e-23
 1.0000000e+00 2.0018219e-22], sum to 1.0000
[2019-04-04 11:01:57,018] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1965
[2019-04-04 11:01:57,095] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.84889451980262, 0.5807561332935719, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1323600.0000, 
sim time next is 1324200.0000, 
raw observation next is [1.1, 92.0, 5.999999999999998, 0.0, 26.0, 25.85705554721438, 0.5601515772514487, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.019999999999999993, 0.0, 0.6666666666666666, 0.6547546289345316, 0.6867171924171496, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2617142], dtype=float32), -0.968339]. 
=============================================
[2019-04-04 11:01:57,751] A3C_AGENT_WORKER-Thread-18 INFO:Local step 258000, global step 4125842: loss 2.9935
[2019-04-04 11:01:57,752] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 258000, global step 4125843: learning rate 0.0000
[2019-04-04 11:01:58,607] A3C_AGENT_WORKER-Thread-11 INFO:Local step 258000, global step 4126113: loss 2.9989
[2019-04-04 11:01:58,607] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 258000, global step 4126113: learning rate 0.0000
[2019-04-04 11:02:01,489] A3C_AGENT_WORKER-Thread-20 INFO:Local step 257000, global step 4127127: loss 0.0802
[2019-04-04 11:02:01,490] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 257000, global step 4127127: learning rate 0.0000
[2019-04-04 11:02:02,206] A3C_AGENT_WORKER-Thread-15 INFO:Local step 258500, global step 4127356: loss 0.0989
[2019-04-04 11:02:02,227] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 258500, global step 4127357: learning rate 0.0000
[2019-04-04 11:02:05,166] A3C_AGENT_WORKER-Thread-13 INFO:Local step 258000, global step 4128231: loss 2.9778
[2019-04-04 11:02:05,166] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 258000, global step 4128231: learning rate 0.0000
[2019-04-04 11:02:05,269] A3C_AGENT_WORKER-Thread-19 INFO:Local step 258000, global step 4128259: loss 2.9332
[2019-04-04 11:02:05,271] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 258000, global step 4128259: learning rate 0.0000
[2019-04-04 11:02:05,301] A3C_AGENT_WORKER-Thread-10 INFO:Local step 258000, global step 4128268: loss 2.9912
[2019-04-04 11:02:05,302] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 258000, global step 4128268: learning rate 0.0000
[2019-04-04 11:02:05,368] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2710914e-11 2.5691904e-10 5.6935755e-29 5.3012547e-27 1.5663646e-22
 1.0000000e+00 1.0693018e-21], sum to 1.0000
[2019-04-04 11:02:05,368] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6283
[2019-04-04 11:02:05,426] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.366666666666667, 73.33333333333334, 0.0, 0.0, 26.0, 25.36753963852376, 0.5354140348638866, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1542000.0000, 
sim time next is 1542600.0000, 
raw observation next is [7.45, 73.5, 0.0, 0.0, 26.0, 25.29645543463646, 0.5534224820894946, 0.0, 1.0, 196664.0695884599], 
processed observation next is [1.0, 0.8695652173913043, 0.6689750692520776, 0.735, 0.0, 0.0, 0.6666666666666666, 0.6080379528863716, 0.6844741606964982, 0.0, 1.0, 0.9364955694688567], 
reward next is 0.0635, 
noisyNet noise sample is [array([0.3716232], dtype=float32), -0.7992783]. 
=============================================
[2019-04-04 11:02:06,362] A3C_AGENT_WORKER-Thread-16 INFO:Local step 258500, global step 4128579: loss 0.1021
[2019-04-04 11:02:06,362] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 258500, global step 4128579: learning rate 0.0000
[2019-04-04 11:02:07,138] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.9072791e-11 1.0041391e-09 3.3925006e-26 3.4241612e-25 1.2415865e-20
 1.0000000e+00 9.3502441e-20], sum to 1.0000
[2019-04-04 11:02:07,140] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4014
[2019-04-04 11:02:07,192] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 66.0, 36.0, 0.0, 26.0, 25.83404012427533, 0.4531389898222432, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2132400.0000, 
sim time next is 2133000.0000, 
raw observation next is [-4.5, 66.5, 26.0, 0.0, 26.0, 25.98545423751391, 0.4584710657665723, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.665, 0.08666666666666667, 0.0, 0.6666666666666666, 0.6654545197928258, 0.6528236885888574, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.25772893], dtype=float32), 1.4174384]. 
=============================================
[2019-04-04 11:02:07,202] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.71275 ]
 [78.868774]
 [79.22862 ]
 [78.47346 ]
 [78.25011 ]], R is [[78.6999588 ]
 [78.91295624]
 [79.12382507]
 [78.38764954]
 [78.24353027]].
[2019-04-04 11:02:07,254] A3C_AGENT_WORKER-Thread-5 INFO:Local step 257500, global step 4128874: loss 0.0077
[2019-04-04 11:02:07,254] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 257500, global step 4128874: learning rate 0.0000
[2019-04-04 11:02:07,364] A3C_AGENT_WORKER-Thread-14 INFO:Local step 257500, global step 4128916: loss 0.0072
[2019-04-04 11:02:07,400] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 257500, global step 4128916: learning rate 0.0000
[2019-04-04 11:02:10,201] A3C_AGENT_WORKER-Thread-3 INFO:Local step 258500, global step 4129905: loss 0.0982
[2019-04-04 11:02:10,202] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 258500, global step 4129905: learning rate 0.0000
[2019-04-04 11:02:10,851] A3C_AGENT_WORKER-Thread-4 INFO:Local step 258500, global step 4130147: loss 0.0912
[2019-04-04 11:02:10,852] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 258500, global step 4130147: learning rate 0.0000
[2019-04-04 11:02:11,001] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6373568e-13 9.5091105e-12 1.3153165e-32 1.8749857e-30 3.0048454e-25
 1.0000000e+00 9.8128617e-25], sum to 1.0000
[2019-04-04 11:02:11,001] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8536
[2019-04-04 11:02:11,062] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.333333333333334, 86.33333333333334, 98.0, 701.3333333333334, 26.0, 26.31505669262645, 0.5428608130758613, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1513200.0000, 
sim time next is 1513800.0000, 
raw observation next is [5.800000000000001, 83.0, 100.0, 700.0, 26.0, 25.86404210075208, 0.5672829693584897, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6232686980609419, 0.83, 0.3333333333333333, 0.7734806629834254, 0.6666666666666666, 0.65533684172934, 0.6890943231194965, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6409619], dtype=float32), 0.7035797]. 
=============================================
[2019-04-04 11:02:11,911] A3C_AGENT_WORKER-Thread-6 INFO:Local step 258500, global step 4130459: loss 0.0914
[2019-04-04 11:02:11,912] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 258500, global step 4130459: learning rate 0.0000
[2019-04-04 11:02:12,592] A3C_AGENT_WORKER-Thread-2 INFO:Local step 258500, global step 4130701: loss 0.0971
[2019-04-04 11:02:12,593] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 258500, global step 4130701: learning rate 0.0000
[2019-04-04 11:02:14,551] A3C_AGENT_WORKER-Thread-17 INFO:Local step 257500, global step 4131232: loss 0.0093
[2019-04-04 11:02:14,551] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 257500, global step 4131232: learning rate 0.0000
[2019-04-04 11:02:15,441] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.1605508e-12 4.2957288e-10 3.4227179e-29 2.5587529e-26 4.5866756e-22
 1.0000000e+00 1.7907844e-21], sum to 1.0000
[2019-04-04 11:02:15,441] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9533
[2019-04-04 11:02:15,470] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 15.5, 0.0, 26.0, 25.74591497964901, 0.5234089911970701, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1702800.0000, 
sim time next is 1703400.0000, 
raw observation next is [1.1, 88.00000000000001, 10.66666666666666, 0.0, 26.0, 25.88814625839878, 0.5343937241585534, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.8800000000000001, 0.035555555555555535, 0.0, 0.6666666666666666, 0.6573455215332317, 0.6781312413861844, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1077135], dtype=float32), -1.0248421]. 
=============================================
[2019-04-04 11:02:16,615] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7402578e-09 1.8044700e-08 4.7563197e-24 1.2225973e-22 6.8714142e-19
 1.0000000e+00 3.3582644e-18], sum to 1.0000
[2019-04-04 11:02:16,633] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9689
[2019-04-04 11:02:16,667] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 23.97656933036223, 0.03611917525087644, 0.0, 1.0, 41062.4675558489], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2012400.0000, 
sim time next is 2013000.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.05119192527281, 0.03478756762982827, 0.0, 1.0, 40983.57320172206], 
processed observation next is [1.0, 0.30434782608695654, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5042659937727342, 0.5115958558766095, 0.0, 1.0, 0.19515987238915267], 
reward next is 0.8048, 
noisyNet noise sample is [array([-1.0707225], dtype=float32), -0.35379612]. 
=============================================
[2019-04-04 11:02:16,687] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.37034 ]
 [74.428535]
 [74.50349 ]
 [74.57971 ]
 [74.64469 ]], R is [[74.37574768]
 [74.43645477]
 [74.49612427]
 [74.55498505]
 [74.6133194 ]].
[2019-04-04 11:02:22,941] A3C_AGENT_WORKER-Thread-12 INFO:Local step 258500, global step 4133799: loss 0.0806
[2019-04-04 11:02:22,944] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 258500, global step 4133799: learning rate 0.0000
[2019-04-04 11:02:23,097] A3C_AGENT_WORKER-Thread-20 INFO:Local step 257500, global step 4133853: loss 0.0110
[2019-04-04 11:02:23,100] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 257500, global step 4133853: learning rate 0.0000
[2019-04-04 11:02:24,014] A3C_AGENT_WORKER-Thread-18 INFO:Local step 258500, global step 4134171: loss 0.0811
[2019-04-04 11:02:24,020] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 258500, global step 4134171: learning rate 0.0000
[2019-04-04 11:02:25,664] A3C_AGENT_WORKER-Thread-11 INFO:Local step 258500, global step 4134796: loss 0.0787
[2019-04-04 11:02:25,668] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 258500, global step 4134797: learning rate 0.0000
[2019-04-04 11:02:28,513] A3C_AGENT_WORKER-Thread-15 INFO:Local step 259000, global step 4135685: loss 0.4188
[2019-04-04 11:02:28,526] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 259000, global step 4135685: learning rate 0.0000
[2019-04-04 11:02:30,239] A3C_AGENT_WORKER-Thread-5 INFO:Local step 258000, global step 4136131: loss 2.9805
[2019-04-04 11:02:30,240] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 258000, global step 4136131: learning rate 0.0000
[2019-04-04 11:02:30,628] A3C_AGENT_WORKER-Thread-14 INFO:Local step 258000, global step 4136252: loss 3.0328
[2019-04-04 11:02:30,631] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 258000, global step 4136253: learning rate 0.0000
[2019-04-04 11:02:31,457] A3C_AGENT_WORKER-Thread-13 INFO:Local step 258500, global step 4136518: loss 0.0777
[2019-04-04 11:02:31,461] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 258500, global step 4136519: learning rate 0.0000
[2019-04-04 11:02:31,913] A3C_AGENT_WORKER-Thread-19 INFO:Local step 258500, global step 4136653: loss 0.0780
[2019-04-04 11:02:31,918] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 258500, global step 4136655: learning rate 0.0000
[2019-04-04 11:02:32,288] A3C_AGENT_WORKER-Thread-10 INFO:Local step 258500, global step 4136796: loss 0.0768
[2019-04-04 11:02:32,291] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 258500, global step 4136797: learning rate 0.0000
[2019-04-04 11:02:33,060] A3C_AGENT_WORKER-Thread-16 INFO:Local step 259000, global step 4137076: loss 0.4167
[2019-04-04 11:02:33,061] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 259000, global step 4137076: learning rate 0.0000
[2019-04-04 11:02:35,964] A3C_AGENT_WORKER-Thread-3 INFO:Local step 259000, global step 4137985: loss 0.4187
[2019-04-04 11:02:35,965] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 259000, global step 4137985: learning rate 0.0000
[2019-04-04 11:02:37,156] A3C_AGENT_WORKER-Thread-4 INFO:Local step 259000, global step 4138296: loss 0.4154
[2019-04-04 11:02:37,159] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 259000, global step 4138296: learning rate 0.0000
[2019-04-04 11:02:38,490] A3C_AGENT_WORKER-Thread-17 INFO:Local step 258000, global step 4138706: loss 3.1047
[2019-04-04 11:02:38,491] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 258000, global step 4138706: learning rate 0.0000
[2019-04-04 11:02:38,542] A3C_AGENT_WORKER-Thread-6 INFO:Local step 259000, global step 4138722: loss 0.4028
[2019-04-04 11:02:38,543] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 259000, global step 4138722: learning rate 0.0000
[2019-04-04 11:02:39,060] A3C_AGENT_WORKER-Thread-2 INFO:Local step 259000, global step 4138919: loss 0.4058
[2019-04-04 11:02:39,069] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 259000, global step 4138919: learning rate 0.0000
[2019-04-04 11:02:44,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4125401e-09 3.0907739e-08 2.7143126e-23 3.3234150e-22 9.0405125e-18
 1.0000000e+00 4.7553568e-17], sum to 1.0000
[2019-04-04 11:02:44,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0702
[2019-04-04 11:02:44,414] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.933333333333334, 51.33333333333333, 0.0, 0.0, 26.0, 24.23664155420945, 0.06711914131752834, 0.0, 1.0, 43492.93544105245], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2425200.0000, 
sim time next is 2425800.0000, 
raw observation next is [-7.116666666666666, 52.16666666666667, 0.0, 0.0, 26.0, 24.18704178235494, 0.0560394105200401, 0.0, 1.0, 43527.38275079281], 
processed observation next is [0.0, 0.043478260869565216, 0.265466297322253, 0.5216666666666667, 0.0, 0.0, 0.6666666666666666, 0.5155868151962449, 0.5186798035066801, 0.0, 1.0, 0.20727325119425147], 
reward next is 0.7927, 
noisyNet noise sample is [array([0.6742898], dtype=float32), -0.25096574]. 
=============================================
[2019-04-04 11:02:45,742] A3C_AGENT_WORKER-Thread-20 INFO:Local step 258000, global step 4141187: loss 3.1087
[2019-04-04 11:02:45,742] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 258000, global step 4141187: learning rate 0.0000
[2019-04-04 11:02:46,462] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7859679e-10 1.0261303e-08 4.3772807e-25 1.4176312e-23 9.1889726e-19
 1.0000000e+00 5.7163247e-19], sum to 1.0000
[2019-04-04 11:02:46,463] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8023
[2019-04-04 11:02:46,501] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.2, 67.66666666666667, 0.0, 0.0, 26.0, 24.34566007377195, 0.1433298406542641, 0.0, 1.0, 40660.07305015771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2349600.0000, 
sim time next is 2350200.0000, 
raw observation next is [-3.3, 68.33333333333333, 0.0, 0.0, 26.0, 24.30879968124357, 0.1360223426311804, 0.0, 1.0, 40765.76080952785], 
processed observation next is [0.0, 0.17391304347826086, 0.37119113573407203, 0.6833333333333332, 0.0, 0.0, 0.6666666666666666, 0.5257333067702975, 0.5453407808770602, 0.0, 1.0, 0.19412267052156118], 
reward next is 0.8059, 
noisyNet noise sample is [array([-0.81859934], dtype=float32), 0.6311173]. 
=============================================
[2019-04-04 11:02:48,450] A3C_AGENT_WORKER-Thread-12 INFO:Local step 259000, global step 4142066: loss 0.3814
[2019-04-04 11:02:48,452] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 259000, global step 4142066: learning rate 0.0000
[2019-04-04 11:02:49,304] A3C_AGENT_WORKER-Thread-18 INFO:Local step 259000, global step 4142375: loss 0.3803
[2019-04-04 11:02:49,305] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 259000, global step 4142375: learning rate 0.0000
[2019-04-04 11:02:52,082] A3C_AGENT_WORKER-Thread-11 INFO:Local step 259000, global step 4143186: loss 0.3825
[2019-04-04 11:02:52,083] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 259000, global step 4143186: learning rate 0.0000
[2019-04-04 11:02:52,206] A3C_AGENT_WORKER-Thread-15 INFO:Local step 259500, global step 4143228: loss 0.0034
[2019-04-04 11:02:52,207] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 259500, global step 4143228: learning rate 0.0000
[2019-04-04 11:02:55,255] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6669062e-11 5.0308829e-10 2.0230739e-27 8.5312182e-26 4.2086190e-21
 1.0000000e+00 1.1138726e-20], sum to 1.0000
[2019-04-04 11:02:55,258] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8297
[2019-04-04 11:02:55,288] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.333333333333334, 64.0, 112.1666666666667, 784.0, 26.0, 25.9963383251225, 0.4760732267701397, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2720400.0000, 
sim time next is 2721000.0000, 
raw observation next is [-8.166666666666666, 64.0, 112.3333333333333, 787.0, 26.0, 25.98687942932154, 0.471455138247023, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.23638042474607576, 0.64, 0.37444444444444436, 0.8696132596685083, 0.6666666666666666, 0.665573285776795, 0.6571517127490076, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3460858], dtype=float32), -1.4179481]. 
=============================================
[2019-04-04 11:02:55,299] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.68204]
 [81.61039]
 [81.55041]
 [81.47487]
 [81.44344]], R is [[81.9147644 ]
 [82.0956192 ]
 [82.27466583]
 [82.45191956]
 [82.62740326]].
[2019-04-04 11:02:55,598] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7264803e-08 2.8406771e-07 1.0665475e-21 1.4960799e-20 8.0130879e-17
 9.9999964e-01 9.7213177e-17], sum to 1.0000
[2019-04-04 11:02:55,603] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3981
[2019-04-04 11:02:55,722] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.65827632632871, -0.1114806781234396, 0.0, 1.0, 202379.7707628406], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2704800.0000, 
sim time next is 2705400.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 26.0, 23.15334610897002, -0.00885021965184333, 1.0, 1.0, 203105.2441193045], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.6666666666666666, 0.42944550908083495, 0.49704992678271886, 1.0, 1.0, 0.9671678291395452], 
reward next is 0.0328, 
noisyNet noise sample is [array([-0.61299384], dtype=float32), -1.8201343]. 
=============================================
[2019-04-04 11:02:56,205] A3C_AGENT_WORKER-Thread-16 INFO:Local step 259500, global step 4144541: loss 0.0035
[2019-04-04 11:02:56,206] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 259500, global step 4144541: learning rate 0.0000
[2019-04-04 11:02:56,900] A3C_AGENT_WORKER-Thread-5 INFO:Local step 258500, global step 4144758: loss 0.0493
[2019-04-04 11:02:56,900] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 258500, global step 4144758: learning rate 0.0000
[2019-04-04 11:02:57,232] A3C_AGENT_WORKER-Thread-13 INFO:Local step 259000, global step 4144879: loss 0.3805
[2019-04-04 11:02:57,235] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 259000, global step 4144880: learning rate 0.0000
[2019-04-04 11:02:57,663] A3C_AGENT_WORKER-Thread-14 INFO:Local step 258500, global step 4145052: loss 0.0446
[2019-04-04 11:02:57,665] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 258500, global step 4145052: learning rate 0.0000
[2019-04-04 11:02:57,840] A3C_AGENT_WORKER-Thread-10 INFO:Local step 259000, global step 4145114: loss 0.3712
[2019-04-04 11:02:57,841] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 259000, global step 4145114: learning rate 0.0000
[2019-04-04 11:02:58,304] A3C_AGENT_WORKER-Thread-19 INFO:Local step 259000, global step 4145265: loss 0.3628
[2019-04-04 11:02:58,315] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 259000, global step 4145269: learning rate 0.0000
[2019-04-04 11:02:58,872] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.8265699e-11 4.7256199e-09 3.4949450e-27 4.5231445e-25 8.1653535e-21
 1.0000000e+00 2.6884103e-20], sum to 1.0000
[2019-04-04 11:02:58,873] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1333
[2019-04-04 11:02:58,929] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.199999999999999, 78.83333333333334, 24.66666666666666, 12.33333333333333, 26.0, 25.06778984453942, 0.2892643623506735, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2101800.0000, 
sim time next is 2102400.0000, 
raw observation next is [-7.3, 79.0, 36.5, 18.5, 26.0, 25.21510815225958, 0.3086296625583705, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.26038781163434904, 0.79, 0.12166666666666667, 0.020441988950276244, 0.6666666666666666, 0.6012590126882985, 0.6028765541861235, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32102028], dtype=float32), 0.24131693]. 
=============================================
[2019-04-04 11:02:59,495] A3C_AGENT_WORKER-Thread-3 INFO:Local step 259500, global step 4145632: loss 0.0020
[2019-04-04 11:02:59,495] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 259500, global step 4145632: learning rate 0.0000
[2019-04-04 11:03:00,351] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.3791303e-12 3.9206197e-10 2.6879586e-28 3.0588987e-27 6.4162725e-22
 1.0000000e+00 6.4942094e-22], sum to 1.0000
[2019-04-04 11:03:00,353] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3612
[2019-04-04 11:03:00,425] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.1, 79.0, 140.6666666666667, 0.0, 26.0, 25.81266566215692, 0.4526215358627085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2036400.0000, 
sim time next is 2037000.0000, 
raw observation next is [-4.0, 79.0, 133.3333333333333, 0.0, 26.0, 26.13050848416141, 0.4760679664045336, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3518005540166205, 0.79, 0.4444444444444443, 0.0, 0.6666666666666666, 0.6775423736801175, 0.6586893221348445, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32566413], dtype=float32), 1.6816783]. 
=============================================
[2019-04-04 11:03:00,429] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[86.02651]
 [86.35985]
 [86.69175]
 [85.92098]
 [85.11773]], R is [[85.82954407]
 [85.97125244]
 [86.11154175]
 [85.29660797]
 [84.50047302]].
[2019-04-04 11:03:00,511] A3C_AGENT_WORKER-Thread-4 INFO:Local step 259500, global step 4145983: loss 0.0013
[2019-04-04 11:03:00,512] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 259500, global step 4145983: learning rate 0.0000
[2019-04-04 11:03:01,502] A3C_AGENT_WORKER-Thread-6 INFO:Local step 259500, global step 4146314: loss 0.0016
[2019-04-04 11:03:01,504] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 259500, global step 4146316: learning rate 0.0000
[2019-04-04 11:03:01,748] A3C_AGENT_WORKER-Thread-2 INFO:Local step 259500, global step 4146396: loss 0.0012
[2019-04-04 11:03:01,749] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 259500, global step 4146396: learning rate 0.0000
[2019-04-04 11:03:05,209] A3C_AGENT_WORKER-Thread-17 INFO:Local step 258500, global step 4147594: loss 0.0408
[2019-04-04 11:03:05,211] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 258500, global step 4147596: learning rate 0.0000
[2019-04-04 11:03:07,181] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5572786e-12 8.1586587e-10 4.1072088e-28 2.0569493e-26 1.9515087e-21
 1.0000000e+00 6.9023512e-22], sum to 1.0000
[2019-04-04 11:03:07,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0630
[2019-04-04 11:03:07,248] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 75.0, 250.5, 80.5, 26.0, 25.75074458511294, 0.3830382122222156, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2113200.0000, 
sim time next is 2113800.0000, 
raw observation next is [-7.199999999999999, 73.16666666666667, 264.6666666666666, 87.33333333333334, 26.0, 25.69906095442378, 0.4001137357794908, 1.0, 1.0, 136736.3839992308], 
processed observation next is [1.0, 0.4782608695652174, 0.26315789473684215, 0.7316666666666667, 0.8822222222222219, 0.09650092081031308, 0.6666666666666666, 0.6415884128686482, 0.6333712452598302, 1.0, 1.0, 0.6511256380915753], 
reward next is 0.3489, 
noisyNet noise sample is [array([-1.7283869], dtype=float32), -0.44921687]. 
=============================================
[2019-04-04 11:03:11,815] A3C_AGENT_WORKER-Thread-12 INFO:Local step 259500, global step 4149665: loss 0.0021
[2019-04-04 11:03:11,819] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 259500, global step 4149666: learning rate 0.0000
[2019-04-04 11:03:12,525] A3C_AGENT_WORKER-Thread-18 INFO:Local step 259500, global step 4149955: loss 0.0037
[2019-04-04 11:03:12,528] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 259500, global step 4149956: learning rate 0.0000
[2019-04-04 11:03:12,569] A3C_AGENT_WORKER-Thread-20 INFO:Local step 258500, global step 4149969: loss 0.0482
[2019-04-04 11:03:12,569] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 258500, global step 4149969: learning rate 0.0000
[2019-04-04 11:03:14,756] A3C_AGENT_WORKER-Thread-11 INFO:Local step 259500, global step 4150724: loss 0.0053
[2019-04-04 11:03:14,786] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 259500, global step 4150724: learning rate 0.0000
[2019-04-04 11:03:16,421] A3C_AGENT_WORKER-Thread-15 INFO:Local step 260000, global step 4151167: loss 0.1146
[2019-04-04 11:03:16,421] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 260000, global step 4151167: learning rate 0.0000
[2019-04-04 11:03:19,925] A3C_AGENT_WORKER-Thread-13 INFO:Local step 259500, global step 4152262: loss 0.0028
[2019-04-04 11:03:19,927] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 259500, global step 4152262: learning rate 0.0000
[2019-04-04 11:03:20,177] A3C_AGENT_WORKER-Thread-16 INFO:Local step 260000, global step 4152362: loss 0.1047
[2019-04-04 11:03:20,178] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 260000, global step 4152362: learning rate 0.0000
[2019-04-04 11:03:21,042] A3C_AGENT_WORKER-Thread-19 INFO:Local step 259500, global step 4152714: loss 0.0037
[2019-04-04 11:03:21,042] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 259500, global step 4152714: learning rate 0.0000
[2019-04-04 11:03:21,785] A3C_AGENT_WORKER-Thread-10 INFO:Local step 259500, global step 4152981: loss 0.0028
[2019-04-04 11:03:21,789] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 259500, global step 4152983: learning rate 0.0000
[2019-04-04 11:03:23,050] A3C_AGENT_WORKER-Thread-5 INFO:Local step 259000, global step 4153401: loss 0.3667
[2019-04-04 11:03:23,050] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 259000, global step 4153401: learning rate 0.0000
[2019-04-04 11:03:23,620] A3C_AGENT_WORKER-Thread-3 INFO:Local step 260000, global step 4153596: loss 0.1108
[2019-04-04 11:03:23,620] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 260000, global step 4153596: learning rate 0.0000
[2019-04-04 11:03:23,847] A3C_AGENT_WORKER-Thread-14 INFO:Local step 259000, global step 4153668: loss 0.3601
[2019-04-04 11:03:23,851] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 259000, global step 4153669: learning rate 0.0000
[2019-04-04 11:03:24,117] A3C_AGENT_WORKER-Thread-4 INFO:Local step 260000, global step 4153753: loss 0.1034
[2019-04-04 11:03:24,118] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 260000, global step 4153753: learning rate 0.0000
[2019-04-04 11:03:25,187] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9325103e-10 4.6913944e-09 5.8897583e-25 4.3316554e-23 1.7404842e-18
 1.0000000e+00 6.7113165e-19], sum to 1.0000
[2019-04-04 11:03:25,202] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8757
[2019-04-04 11:03:25,247] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.09999999999999999, 73.66666666666667, 23.66666666666666, 220.6666666666666, 26.0, 25.08701448648451, 0.316626949156011, 0.0, 1.0, 19414.85004148959], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3085800.0000, 
sim time next is 3086400.0000, 
raw observation next is [-0.2, 75.33333333333334, 15.33333333333333, 154.3333333333333, 26.0, 25.07574642736787, 0.3059438162459511, 0.0, 1.0, 28863.75581481401], 
processed observation next is [0.0, 0.7391304347826086, 0.4570637119113574, 0.7533333333333334, 0.0511111111111111, 0.17053406998158374, 0.6666666666666666, 0.5896455356139892, 0.6019812720819837, 0.0, 1.0, 0.1374464562610191], 
reward next is 0.8626, 
noisyNet noise sample is [array([-0.5168028], dtype=float32), 0.04298505]. 
=============================================
[2019-04-04 11:03:25,599] A3C_AGENT_WORKER-Thread-6 INFO:Local step 260000, global step 4154270: loss 0.1075
[2019-04-04 11:03:25,599] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 260000, global step 4154270: learning rate 0.0000
[2019-04-04 11:03:25,850] A3C_AGENT_WORKER-Thread-2 INFO:Local step 260000, global step 4154373: loss 0.1011
[2019-04-04 11:03:25,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 260000, global step 4154374: learning rate 0.0000
[2019-04-04 11:03:27,774] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9527064e-11 2.7223618e-10 3.4244452e-27 2.7750867e-25 6.3804940e-21
 1.0000000e+00 1.4238118e-20], sum to 1.0000
[2019-04-04 11:03:27,774] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8093
[2019-04-04 11:03:27,790] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 93.33333333333334, 0.0, 0.0, 26.0, 25.5215314503308, 0.3645888169507576, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3096600.0000, 
sim time next is 3097200.0000, 
raw observation next is [-1.0, 94.66666666666667, 0.0, 0.0, 26.0, 25.50467929698366, 0.3557194579526426, 0.0, 1.0, 18751.87322573367], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9466666666666668, 0.0, 0.0, 0.6666666666666666, 0.625389941415305, 0.6185731526508809, 0.0, 1.0, 0.08929463440825558], 
reward next is 0.9107, 
noisyNet noise sample is [array([1.3524706], dtype=float32), 0.7970545]. 
=============================================
[2019-04-04 11:03:29,248] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.0543156e-11 3.5252381e-09 7.9524170e-28 9.2356029e-27 7.7743300e-22
 1.0000000e+00 3.4044880e-21], sum to 1.0000
[2019-04-04 11:03:29,248] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8732
[2019-04-04 11:03:29,270] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.1, 100.0, 0.0, 0.0, 26.0, 25.36066768115455, 0.297796089237199, 0.0, 1.0, 99004.49299359068], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3121800.0000, 
sim time next is 3122400.0000, 
raw observation next is [2.2, 100.0, 0.0, 0.0, 26.0, 25.29819326721236, 0.2981093299646997, 0.0, 1.0, 77791.23539210935], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6081827722676966, 0.5993697766548999, 0.0, 1.0, 0.37043445424813976], 
reward next is 0.6296, 
noisyNet noise sample is [array([0.3761884], dtype=float32), -0.88504463]. 
=============================================
[2019-04-04 11:03:30,976] A3C_AGENT_WORKER-Thread-17 INFO:Local step 259000, global step 4156417: loss 0.3535
[2019-04-04 11:03:30,977] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 259000, global step 4156417: learning rate 0.0000
[2019-04-04 11:03:33,754] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0668441e-11 1.9752694e-10 4.1435766e-28 2.8995797e-26 3.5132265e-22
 1.0000000e+00 1.6808055e-20], sum to 1.0000
[2019-04-04 11:03:33,754] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6138
[2019-04-04 11:03:33,803] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.416666666666667, 73.33333333333333, 87.66666666666667, 60.66666666666667, 26.0, 25.8365895151169, 0.3531739161155074, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2625000.0000, 
sim time next is 2625600.0000, 
raw observation next is [-6.133333333333335, 71.66666666666667, 90.33333333333333, 75.83333333333334, 26.0, 25.82779172399819, 0.3593648272531154, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.29270544783010155, 0.7166666666666667, 0.3011111111111111, 0.0837937384898711, 0.6666666666666666, 0.6523159769998491, 0.6197882757510385, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12513527], dtype=float32), 1.5197855]. 
=============================================
[2019-04-04 11:03:34,618] A3C_AGENT_WORKER-Thread-12 INFO:Local step 260000, global step 4157907: loss 0.1166
[2019-04-04 11:03:34,630] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 260000, global step 4157909: learning rate 0.0000
[2019-04-04 11:03:36,063] A3C_AGENT_WORKER-Thread-18 INFO:Local step 260000, global step 4158379: loss 0.1218
[2019-04-04 11:03:36,064] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 260000, global step 4158379: learning rate 0.0000
[2019-04-04 11:03:36,446] A3C_AGENT_WORKER-Thread-15 INFO:Local step 260500, global step 4158507: loss 0.0023
[2019-04-04 11:03:36,450] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 260500, global step 4158507: learning rate 0.0000
[2019-04-04 11:03:37,690] A3C_AGENT_WORKER-Thread-11 INFO:Local step 260000, global step 4158951: loss 0.1178
[2019-04-04 11:03:37,691] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 260000, global step 4158951: learning rate 0.0000
[2019-04-04 11:03:37,925] A3C_AGENT_WORKER-Thread-20 INFO:Local step 259000, global step 4159040: loss 0.3835
[2019-04-04 11:03:37,927] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 259000, global step 4159040: learning rate 0.0000
[2019-04-04 11:03:39,133] A3C_AGENT_WORKER-Thread-16 INFO:Local step 260500, global step 4159507: loss 0.0026
[2019-04-04 11:03:39,150] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 260500, global step 4159512: learning rate 0.0000
[2019-04-04 11:03:39,240] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.7482936e-09 6.5046677e-08 1.0131788e-21 2.2083910e-20 4.7018375e-17
 9.9999988e-01 6.9123050e-16], sum to 1.0000
[2019-04-04 11:03:39,240] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4595
[2019-04-04 11:03:39,263] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.78028221352201, -0.1963134636477862, 0.0, 1.0, 43241.98676739512], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2703600.0000, 
sim time next is 2704200.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.69845057491651, -0.2064646904140155, 0.0, 1.0, 43299.03822468359], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.6666666666666666, 0.3915375479097092, 0.4311784365286615, 0.0, 1.0, 0.2061858963080171], 
reward next is 0.7938, 
noisyNet noise sample is [array([0.6409543], dtype=float32), 1.0063782]. 
=============================================
[2019-04-04 11:03:41,093] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.45475513e-12 5.41915623e-10 1.06872245e-29 4.47042660e-27
 1.58457455e-22 1.00000000e+00 4.34360658e-21], sum to 1.0000
[2019-04-04 11:03:41,094] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1820
[2019-04-04 11:03:41,159] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.833333333333333, 60.0, 58.66666666666667, 317.0, 26.0, 25.55692882846579, 0.3807228681440273, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3399000.0000, 
sim time next is 3399600.0000, 
raw observation next is [-1.666666666666667, 60.00000000000001, 72.83333333333333, 369.5, 26.0, 25.49433370361218, 0.4223484903349826, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4164358264081256, 0.6000000000000001, 0.24277777777777776, 0.40828729281767956, 0.6666666666666666, 0.6245278086343484, 0.6407828301116608, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14125453], dtype=float32), 0.45726857]. 
=============================================
[2019-04-04 11:03:42,554] A3C_AGENT_WORKER-Thread-3 INFO:Local step 260500, global step 4160848: loss 0.0017
[2019-04-04 11:03:42,560] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 260500, global step 4160848: learning rate 0.0000
[2019-04-04 11:03:42,748] A3C_AGENT_WORKER-Thread-13 INFO:Local step 260000, global step 4160935: loss 0.1141
[2019-04-04 11:03:42,751] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 260000, global step 4160936: learning rate 0.0000
[2019-04-04 11:03:43,147] A3C_AGENT_WORKER-Thread-4 INFO:Local step 260500, global step 4161114: loss 0.0018
[2019-04-04 11:03:43,149] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 260500, global step 4161114: learning rate 0.0000
[2019-04-04 11:03:43,380] A3C_AGENT_WORKER-Thread-19 INFO:Local step 260000, global step 4161213: loss 0.1142
[2019-04-04 11:03:43,380] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 260000, global step 4161213: learning rate 0.0000
[2019-04-04 11:03:44,219] A3C_AGENT_WORKER-Thread-10 INFO:Local step 260000, global step 4161528: loss 0.1175
[2019-04-04 11:03:44,219] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 260000, global step 4161528: learning rate 0.0000
[2019-04-04 11:03:44,311] A3C_AGENT_WORKER-Thread-6 INFO:Local step 260500, global step 4161565: loss 0.0021
[2019-04-04 11:03:44,320] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 260500, global step 4161565: learning rate 0.0000
[2019-04-04 11:03:44,832] A3C_AGENT_WORKER-Thread-2 INFO:Local step 260500, global step 4161778: loss 0.0019
[2019-04-04 11:03:44,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 260500, global step 4161778: learning rate 0.0000
[2019-04-04 11:03:44,933] A3C_AGENT_WORKER-Thread-14 INFO:Local step 259500, global step 4161823: loss 0.0056
[2019-04-04 11:03:44,936] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 259500, global step 4161825: learning rate 0.0000
[2019-04-04 11:03:45,084] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3025110e-10 1.0410239e-08 3.2739702e-25 1.8418367e-23 4.9067713e-19
 1.0000000e+00 1.8751045e-18], sum to 1.0000
[2019-04-04 11:03:45,085] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8614
[2019-04-04 11:03:45,139] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.666666666666666, 75.0, 0.0, 0.0, 26.0, 24.81792137625499, 0.2609573280064819, 0.0, 1.0, 41678.80722723153], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3385200.0000, 
sim time next is 3385800.0000, 
raw observation next is [-5.5, 74.0, 0.0, 0.0, 26.0, 24.83594522795639, 0.2562206972489047, 0.0, 1.0, 41826.74446566809], 
processed observation next is [1.0, 0.17391304347826086, 0.3102493074792244, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5696621023296992, 0.5854068990829683, 0.0, 1.0, 0.19917497364603853], 
reward next is 0.8008, 
noisyNet noise sample is [array([-1.0584959], dtype=float32), -0.27981183]. 
=============================================
[2019-04-04 11:03:45,210] A3C_AGENT_WORKER-Thread-5 INFO:Local step 259500, global step 4161959: loss 0.0061
[2019-04-04 11:03:45,212] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 259500, global step 4161959: learning rate 0.0000
[2019-04-04 11:03:47,343] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5274947e-12 2.0945297e-10 5.9287180e-30 9.9630757e-29 1.5060413e-23
 1.0000000e+00 3.9697983e-23], sum to 1.0000
[2019-04-04 11:03:47,343] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9094
[2019-04-04 11:03:47,367] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.83333333333334, 115.6666666666667, 817.3333333333334, 26.0, 26.04913715273494, 0.6260270106033236, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3502200.0000, 
sim time next is 3502800.0000, 
raw observation next is [2.0, 52.0, 115.5, 814.5, 26.0, 26.19217129971147, 0.6377806402552485, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 0.52, 0.385, 0.9, 0.6666666666666666, 0.6826809416426226, 0.7125935467517496, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48583242], dtype=float32), -0.70619386]. 
=============================================
[2019-04-04 11:03:52,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9525293e-12 2.7509450e-10 6.8545946e-30 1.6903728e-28 2.5686540e-23
 1.0000000e+00 1.3138389e-22], sum to 1.0000
[2019-04-04 11:03:52,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1594
[2019-04-04 11:03:52,198] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1666666666666667, 60.16666666666666, 105.6666666666667, 736.6666666666666, 26.0, 26.21621358998532, 0.5768771124424367, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3492600.0000, 
sim time next is 3493200.0000, 
raw observation next is [0.3333333333333333, 60.33333333333334, 107.3333333333333, 753.3333333333334, 26.0, 26.26324909136737, 0.576302124206741, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4718374884579871, 0.6033333333333334, 0.3577777777777777, 0.8324125230202579, 0.6666666666666666, 0.6886040909472809, 0.6921007080689137, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9657899], dtype=float32), -0.21941635]. 
=============================================
[2019-04-04 11:03:52,336] A3C_AGENT_WORKER-Thread-17 INFO:Local step 259500, global step 4164977: loss 0.0138
[2019-04-04 11:03:52,336] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 259500, global step 4164977: learning rate 0.0000
[2019-04-04 11:03:53,712] A3C_AGENT_WORKER-Thread-12 INFO:Local step 260500, global step 4165600: loss 0.0048
[2019-04-04 11:03:53,712] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 260500, global step 4165600: learning rate 0.0000
[2019-04-04 11:03:53,931] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5459431e-10 2.1853577e-09 5.6518226e-25 3.1002562e-23 1.6329890e-18
 1.0000000e+00 1.5672943e-18], sum to 1.0000
[2019-04-04 11:03:53,931] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8117
[2019-04-04 11:03:53,963] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.07626644816209, 0.3704569244602664, 0.0, 1.0, 162506.6798828827], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3612600.0000, 
sim time next is 3613200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.12022342478552, 0.3934091482558784, 0.0, 1.0, 87780.208681883], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5933519520654601, 0.6311363827519595, 0.0, 1.0, 0.4180009937232524], 
reward next is 0.5820, 
noisyNet noise sample is [array([-0.75833946], dtype=float32), -0.5125516]. 
=============================================
[2019-04-04 11:03:54,251] A3C_AGENT_WORKER-Thread-18 INFO:Local step 260500, global step 4165823: loss 0.0046
[2019-04-04 11:03:54,251] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 260500, global step 4165823: learning rate 0.0000
[2019-04-04 11:03:54,688] A3C_AGENT_WORKER-Thread-15 INFO:Local step 261000, global step 4166009: loss 0.4353
[2019-04-04 11:03:54,690] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 261000, global step 4166010: learning rate 0.0000
[2019-04-04 11:03:55,302] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6270987e-09 2.2026164e-08 4.6837384e-25 9.9502308e-23 3.2640911e-19
 1.0000000e+00 3.8435332e-18], sum to 1.0000
[2019-04-04 11:03:55,302] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4546
[2019-04-04 11:03:55,332] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 63.66666666666667, 0.0, 0.0, 26.0, 24.79516703732808, 0.2433416131102079, 0.0, 1.0, 42747.96860187576], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3390000.0000, 
sim time next is 3390600.0000, 
raw observation next is [-3.333333333333333, 61.83333333333333, 0.0, 0.0, 26.0, 24.82752876270231, 0.2324705621182172, 0.0, 1.0, 42628.1155029188], 
processed observation next is [1.0, 0.21739130434782608, 0.37026777469990774, 0.6183333333333333, 0.0, 0.0, 0.6666666666666666, 0.5689607302251926, 0.577490187372739, 0.0, 1.0, 0.20299102620437523], 
reward next is 0.7970, 
noisyNet noise sample is [array([0.23718251], dtype=float32), 1.0556116]. 
=============================================
[2019-04-04 11:03:55,611] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.18055053e-11 9.24379018e-10 8.06416431e-28 1.09223755e-26
 8.24419069e-22 1.00000000e+00 1.76253003e-21], sum to 1.0000
[2019-04-04 11:03:55,611] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5240
[2019-04-04 11:03:55,677] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 24.91735521974622, 0.4025858968592665, 0.0, 1.0, 49588.62481319209], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2923200.0000, 
sim time next is 2923800.0000, 
raw observation next is [-1.0, 79.16666666666667, 0.0, 0.0, 26.0, 24.95814174938439, 0.4062593651049748, 1.0, 1.0, 18966.80501071285], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.7916666666666667, 0.0, 0.0, 0.6666666666666666, 0.5798451457820324, 0.6354197883683249, 1.0, 1.0, 0.09031811909863262], 
reward next is 0.9097, 
noisyNet noise sample is [array([1.0917705], dtype=float32), -0.3497004]. 
=============================================
[2019-04-04 11:03:56,300] A3C_AGENT_WORKER-Thread-11 INFO:Local step 260500, global step 4166653: loss 0.0064
[2019-04-04 11:03:56,304] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 260500, global step 4166656: learning rate 0.0000
[2019-04-04 11:03:56,827] A3C_AGENT_WORKER-Thread-16 INFO:Local step 261000, global step 4166890: loss 0.4319
[2019-04-04 11:03:56,828] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 261000, global step 4166890: learning rate 0.0000
[2019-04-04 11:03:59,018] A3C_AGENT_WORKER-Thread-20 INFO:Local step 259500, global step 4167776: loss 0.0151
[2019-04-04 11:03:59,022] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 259500, global step 4167778: learning rate 0.0000
[2019-04-04 11:04:00,402] A3C_AGENT_WORKER-Thread-3 INFO:Local step 261000, global step 4168379: loss 0.4452
[2019-04-04 11:04:00,402] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 261000, global step 4168379: learning rate 0.0000
[2019-04-04 11:04:00,928] A3C_AGENT_WORKER-Thread-4 INFO:Local step 261000, global step 4168608: loss 0.4534
[2019-04-04 11:04:00,929] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 261000, global step 4168608: learning rate 0.0000
[2019-04-04 11:04:01,396] A3C_AGENT_WORKER-Thread-13 INFO:Local step 260500, global step 4168810: loss 0.0043
[2019-04-04 11:04:01,397] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 260500, global step 4168810: learning rate 0.0000
[2019-04-04 11:04:01,807] A3C_AGENT_WORKER-Thread-19 INFO:Local step 260500, global step 4169000: loss 0.0042
[2019-04-04 11:04:01,816] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 260500, global step 4169004: learning rate 0.0000
[2019-04-04 11:04:02,381] A3C_AGENT_WORKER-Thread-6 INFO:Local step 261000, global step 4169297: loss 0.4603
[2019-04-04 11:04:02,381] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 261000, global step 4169297: learning rate 0.0000
[2019-04-04 11:04:02,610] A3C_AGENT_WORKER-Thread-10 INFO:Local step 260500, global step 4169418: loss 0.0032
[2019-04-04 11:04:02,611] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 260500, global step 4169418: learning rate 0.0000
[2019-04-04 11:04:02,644] A3C_AGENT_WORKER-Thread-2 INFO:Local step 261000, global step 4169437: loss 0.4556
[2019-04-04 11:04:02,646] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 261000, global step 4169438: learning rate 0.0000
[2019-04-04 11:04:06,240] A3C_AGENT_WORKER-Thread-5 INFO:Local step 260000, global step 4171025: loss 0.1242
[2019-04-04 11:04:06,242] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 260000, global step 4171025: learning rate 0.0000
[2019-04-04 11:04:06,468] A3C_AGENT_WORKER-Thread-14 INFO:Local step 260000, global step 4171087: loss 0.1287
[2019-04-04 11:04:06,472] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 260000, global step 4171087: learning rate 0.0000
[2019-04-04 11:04:11,497] A3C_AGENT_WORKER-Thread-12 INFO:Local step 261000, global step 4173310: loss 0.4571
[2019-04-04 11:04:11,499] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 261000, global step 4173310: learning rate 0.0000
[2019-04-04 11:04:12,178] A3C_AGENT_WORKER-Thread-18 INFO:Local step 261000, global step 4173642: loss 0.4563
[2019-04-04 11:04:12,180] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 261000, global step 4173642: learning rate 0.0000
[2019-04-04 11:04:12,586] A3C_AGENT_WORKER-Thread-15 INFO:Local step 261500, global step 4173838: loss 0.0857
[2019-04-04 11:04:12,591] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 261500, global step 4173842: learning rate 0.0000
[2019-04-04 11:04:13,750] A3C_AGENT_WORKER-Thread-17 INFO:Local step 260000, global step 4174399: loss 0.1266
[2019-04-04 11:04:13,751] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 260000, global step 4174399: learning rate 0.0000
[2019-04-04 11:04:13,764] A3C_AGENT_WORKER-Thread-11 INFO:Local step 261000, global step 4174403: loss 0.4481
[2019-04-04 11:04:13,765] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 261000, global step 4174403: learning rate 0.0000
[2019-04-04 11:04:14,393] A3C_AGENT_WORKER-Thread-16 INFO:Local step 261500, global step 4174684: loss 0.0765
[2019-04-04 11:04:14,413] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 261500, global step 4174684: learning rate 0.0000
[2019-04-04 11:04:14,562] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2004764e-11 3.0620031e-09 2.5818532e-26 1.5562262e-24 3.0603453e-20
 1.0000000e+00 6.6279165e-19], sum to 1.0000
[2019-04-04 11:04:14,563] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7740
[2019-04-04 11:04:14,618] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666667, 61.33333333333334, 109.1666666666667, 744.3333333333334, 26.0, 25.6379703795471, 0.4900461713826313, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3579600.0000, 
sim time next is 3580200.0000, 
raw observation next is [-4.5, 59.5, 111.0, 758.0, 26.0, 25.57117083339504, 0.4816104547708062, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3379501385041552, 0.595, 0.37, 0.8375690607734807, 0.6666666666666666, 0.6309309027829201, 0.6605368182569354, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06320254], dtype=float32), 0.71891636]. 
=============================================
[2019-04-04 11:04:17,617] A3C_AGENT_WORKER-Thread-3 INFO:Local step 261500, global step 4176025: loss 0.0683
[2019-04-04 11:04:17,629] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 261500, global step 4176027: learning rate 0.0000
[2019-04-04 11:04:18,336] A3C_AGENT_WORKER-Thread-4 INFO:Local step 261500, global step 4176356: loss 0.0656
[2019-04-04 11:04:18,337] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 261500, global step 4176356: learning rate 0.0000
[2019-04-04 11:04:18,552] A3C_AGENT_WORKER-Thread-13 INFO:Local step 261000, global step 4176467: loss 0.4532
[2019-04-04 11:04:18,556] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 261000, global step 4176467: learning rate 0.0000
[2019-04-04 11:04:19,689] A3C_AGENT_WORKER-Thread-19 INFO:Local step 261000, global step 4176992: loss 0.4508
[2019-04-04 11:04:19,689] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 261000, global step 4176993: learning rate 0.0000
[2019-04-04 11:04:19,949] A3C_AGENT_WORKER-Thread-6 INFO:Local step 261500, global step 4177107: loss 0.0745
[2019-04-04 11:04:19,951] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 261500, global step 4177107: learning rate 0.0000
[2019-04-04 11:04:20,347] A3C_AGENT_WORKER-Thread-2 INFO:Local step 261500, global step 4177272: loss 0.0627
[2019-04-04 11:04:20,347] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 261500, global step 4177272: learning rate 0.0000
[2019-04-04 11:04:20,348] A3C_AGENT_WORKER-Thread-10 INFO:Local step 261000, global step 4177273: loss 0.4424
[2019-04-04 11:04:20,350] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 261000, global step 4177273: learning rate 0.0000
[2019-04-04 11:04:20,567] A3C_AGENT_WORKER-Thread-20 INFO:Local step 260000, global step 4177353: loss 0.1270
[2019-04-04 11:04:20,568] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 260000, global step 4177353: learning rate 0.0000
[2019-04-04 11:04:23,474] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0747995e-11 3.2738265e-10 1.1738707e-27 5.8694123e-26 1.5013508e-21
 1.0000000e+00 2.5436747e-21], sum to 1.0000
[2019-04-04 11:04:23,475] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6394
[2019-04-04 11:04:23,526] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 47.0, 282.5, 26.0, 25.25316173021312, 0.301844734431631, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3744000.0000, 
sim time next is 3744600.0000, 
raw observation next is [-4.0, 72.0, 61.00000000000001, 331.3333333333334, 26.0, 25.19161835805996, 0.3035227619881227, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.72, 0.20333333333333337, 0.3661141804788215, 0.6666666666666666, 0.5993015298383298, 0.6011742539960409, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48056698], dtype=float32), -0.0023063698]. 
=============================================
[2019-04-04 11:04:24,098] A3C_AGENT_WORKER-Thread-5 INFO:Local step 260500, global step 4178897: loss 0.0038
[2019-04-04 11:04:24,104] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 260500, global step 4178898: learning rate 0.0000
[2019-04-04 11:04:24,604] A3C_AGENT_WORKER-Thread-14 INFO:Local step 260500, global step 4179153: loss 0.0031
[2019-04-04 11:04:24,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 260500, global step 4179153: learning rate 0.0000
[2019-04-04 11:04:26,250] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7898896e-13 6.9349318e-11 1.0144388e-30 1.5638522e-28 1.5045599e-23
 1.0000000e+00 2.5967523e-23], sum to 1.0000
[2019-04-04 11:04:26,250] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6740
[2019-04-04 11:04:26,286] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.9999999999999999, 52.0, 100.6666666666667, 675.6666666666666, 26.0, 26.42426462805019, 0.5762106971227446, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3404400.0000, 
sim time next is 3405000.0000, 
raw observation next is [1.5, 50.0, 102.3333333333333, 693.3333333333334, 26.0, 26.48708806822923, 0.5838847647411846, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5041551246537397, 0.5, 0.341111111111111, 0.7661141804788214, 0.6666666666666666, 0.7072573390191025, 0.6946282549137281, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4223329], dtype=float32), 1.4888713]. 
=============================================
[2019-04-04 11:04:26,293] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[90.75501 ]
 [90.698235]
 [90.58982 ]
 [90.560234]
 [90.50873 ]], R is [[90.90549469]
 [90.99643707]
 [91.08647156]
 [91.17560577]
 [91.26384735]].
[2019-04-04 11:04:28,737] A3C_AGENT_WORKER-Thread-12 INFO:Local step 261500, global step 4181060: loss 0.0878
[2019-04-04 11:04:28,738] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 261500, global step 4181060: learning rate 0.0000
[2019-04-04 11:04:29,497] A3C_AGENT_WORKER-Thread-18 INFO:Local step 261500, global step 4181438: loss 0.0758
[2019-04-04 11:04:29,498] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 261500, global step 4181439: learning rate 0.0000
[2019-04-04 11:04:30,759] A3C_AGENT_WORKER-Thread-11 INFO:Local step 261500, global step 4182049: loss 0.0757
[2019-04-04 11:04:30,760] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 261500, global step 4182049: learning rate 0.0000
[2019-04-04 11:04:31,018] A3C_AGENT_WORKER-Thread-17 INFO:Local step 260500, global step 4182165: loss 0.0041
[2019-04-04 11:04:31,019] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 260500, global step 4182165: learning rate 0.0000
[2019-04-04 11:04:31,147] A3C_AGENT_WORKER-Thread-15 INFO:Local step 262000, global step 4182220: loss 3.8664
[2019-04-04 11:04:31,149] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 262000, global step 4182220: learning rate 0.0000
[2019-04-04 11:04:32,464] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0130698e-12 3.9892861e-10 1.9640345e-27 1.5003286e-25 7.3529579e-21
 1.0000000e+00 1.9583172e-21], sum to 1.0000
[2019-04-04 11:04:32,468] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0258
[2019-04-04 11:04:32,479] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 37.0, 118.5, 828.5, 26.0, 26.43202087246064, 0.5923108078856235, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4017600.0000, 
sim time next is 4018200.0000, 
raw observation next is [-5.666666666666667, 35.66666666666667, 118.3333333333333, 832.6666666666667, 26.0, 26.45965366709437, 0.5978365093919492, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.30563250230840255, 0.3566666666666667, 0.3944444444444443, 0.9200736648250462, 0.6666666666666666, 0.7049711389245307, 0.699278836463983, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5010242], dtype=float32), -0.29240993]. 
=============================================
[2019-04-04 11:04:32,631] A3C_AGENT_WORKER-Thread-16 INFO:Local step 262000, global step 4182940: loss 3.8930
[2019-04-04 11:04:32,631] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 262000, global step 4182940: learning rate 0.0000
[2019-04-04 11:04:34,978] A3C_AGENT_WORKER-Thread-3 INFO:Local step 262000, global step 4184075: loss 3.8351
[2019-04-04 11:04:34,979] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 262000, global step 4184075: learning rate 0.0000
[2019-04-04 11:04:35,351] A3C_AGENT_WORKER-Thread-13 INFO:Local step 261500, global step 4184262: loss 0.0900
[2019-04-04 11:04:35,353] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 261500, global step 4184262: learning rate 0.0000
[2019-04-04 11:04:35,775] A3C_AGENT_WORKER-Thread-4 INFO:Local step 262000, global step 4184459: loss 3.8312
[2019-04-04 11:04:35,780] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 262000, global step 4184461: learning rate 0.0000
[2019-04-04 11:04:36,719] A3C_AGENT_WORKER-Thread-19 INFO:Local step 261500, global step 4184910: loss 0.0996
[2019-04-04 11:04:36,724] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 261500, global step 4184912: learning rate 0.0000
[2019-04-04 11:04:37,295] A3C_AGENT_WORKER-Thread-10 INFO:Local step 261500, global step 4185195: loss 0.0825
[2019-04-04 11:04:37,303] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 261500, global step 4185195: learning rate 0.0000
[2019-04-04 11:04:37,496] A3C_AGENT_WORKER-Thread-20 INFO:Local step 260500, global step 4185293: loss 0.0018
[2019-04-04 11:04:37,511] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 260500, global step 4185293: learning rate 0.0000
[2019-04-04 11:04:37,541] A3C_AGENT_WORKER-Thread-6 INFO:Local step 262000, global step 4185315: loss 3.8400
[2019-04-04 11:04:37,546] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 262000, global step 4185315: learning rate 0.0000
[2019-04-04 11:04:37,705] A3C_AGENT_WORKER-Thread-2 INFO:Local step 262000, global step 4185398: loss 3.8247
[2019-04-04 11:04:37,707] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 262000, global step 4185398: learning rate 0.0000
[2019-04-04 11:04:38,887] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5227342e-12 1.1120654e-10 3.2837086e-28 1.2789245e-26 9.8603817e-22
 1.0000000e+00 9.3475236e-22], sum to 1.0000
[2019-04-04 11:04:38,890] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6005
[2019-04-04 11:04:38,919] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.166666666666667, 26.5, 110.6666666666667, 818.0, 26.0, 26.86703002556503, 0.7363013120690867, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4024200.0000, 
sim time next is 4024800.0000, 
raw observation next is [-3.0, 26.0, 109.0, 812.0, 26.0, 27.03802574200939, 0.7538693042481698, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.26, 0.36333333333333334, 0.8972375690607735, 0.6666666666666666, 0.7531688118341157, 0.7512897680827232, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0843613], dtype=float32), 0.8186948]. 
=============================================
[2019-04-04 11:04:40,145] A3C_AGENT_WORKER-Thread-5 INFO:Local step 261000, global step 4186698: loss 0.4846
[2019-04-04 11:04:40,146] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 261000, global step 4186698: learning rate 0.0000
[2019-04-04 11:04:40,744] A3C_AGENT_WORKER-Thread-14 INFO:Local step 261000, global step 4187029: loss 0.4946
[2019-04-04 11:04:40,746] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 261000, global step 4187030: learning rate 0.0000
[2019-04-04 11:04:40,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6937572e-10 5.3760640e-09 4.1558967e-26 2.6744937e-24 8.7424906e-20
 1.0000000e+00 6.3682211e-19], sum to 1.0000
[2019-04-04 11:04:40,790] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4721
[2019-04-04 11:04:40,816] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.1666666666666667, 42.83333333333334, 0.0, 0.0, 26.0, 25.37255664681719, 0.4670707856012007, 0.0, 1.0, 45593.85455563886], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4144200.0000, 
sim time next is 4144800.0000, 
raw observation next is [-0.3333333333333333, 42.66666666666667, 0.0, 0.0, 26.0, 25.41774336420115, 0.4677914801669683, 0.0, 1.0, 18764.68103538777], 
processed observation next is [1.0, 1.0, 0.4533702677747, 0.4266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6181452803500959, 0.6559304933889895, 0.0, 1.0, 0.089355623978037], 
reward next is 0.9106, 
noisyNet noise sample is [array([-0.8601286], dtype=float32), -0.121818535]. 
=============================================
[2019-04-04 11:04:45,261] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0717371e-11 9.6241605e-11 2.7050543e-28 6.1687382e-26 3.3938166e-21
 1.0000000e+00 1.7444765e-21], sum to 1.0000
[2019-04-04 11:04:45,264] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2086
[2019-04-04 11:04:45,295] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.06445669086798, 0.411272909428869, 1.0, 1.0, 75164.03752551002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4560600.0000, 
sim time next is 4561200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.06698795635407, 0.4193171312771608, 1.0, 1.0, 46489.65818153325], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5889156630295057, 0.639772377092387, 1.0, 1.0, 0.22137932467396784], 
reward next is 0.7786, 
noisyNet noise sample is [array([-0.6559384], dtype=float32), -0.2561971]. 
=============================================
[2019-04-04 11:04:45,296] A3C_AGENT_WORKER-Thread-12 INFO:Local step 262000, global step 4189370: loss 3.8723
[2019-04-04 11:04:45,297] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 262000, global step 4189370: learning rate 0.0000
[2019-04-04 11:04:45,936] A3C_AGENT_WORKER-Thread-15 INFO:Local step 262500, global step 4189714: loss 0.2348
[2019-04-04 11:04:45,939] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 262500, global step 4189714: learning rate 0.0000
[2019-04-04 11:04:45,981] A3C_AGENT_WORKER-Thread-18 INFO:Local step 262000, global step 4189738: loss 3.8374
[2019-04-04 11:04:45,982] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 262000, global step 4189739: learning rate 0.0000
[2019-04-04 11:04:47,205] A3C_AGENT_WORKER-Thread-16 INFO:Local step 262500, global step 4190386: loss 0.2442
[2019-04-04 11:04:47,208] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 262500, global step 4190386: learning rate 0.0000
[2019-04-04 11:04:47,289] A3C_AGENT_WORKER-Thread-17 INFO:Local step 261000, global step 4190431: loss 0.4913
[2019-04-04 11:04:47,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 261000, global step 4190431: learning rate 0.0000
[2019-04-04 11:04:47,405] A3C_AGENT_WORKER-Thread-11 INFO:Local step 262000, global step 4190491: loss 3.8672
[2019-04-04 11:04:47,405] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 262000, global step 4190491: learning rate 0.0000
[2019-04-04 11:04:47,619] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2958153e-13 2.4312871e-10 8.8479291e-30 5.5765369e-28 1.0266829e-22
 1.0000000e+00 9.9784726e-23], sum to 1.0000
[2019-04-04 11:04:47,620] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0481
[2019-04-04 11:04:47,635] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 58.33333333333334, 203.8333333333333, 15.0, 26.0, 26.25419692827732, 0.5506987213324801, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4531200.0000, 
sim time next is 4531800.0000, 
raw observation next is [1.833333333333333, 57.66666666666666, 186.6666666666667, 12.0, 26.0, 26.2538978505418, 0.5505817969110299, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5133887349953832, 0.5766666666666665, 0.6222222222222223, 0.013259668508287293, 0.6666666666666666, 0.6878248208784834, 0.6835272656370099, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9579715], dtype=float32), 0.36122552]. 
=============================================
[2019-04-04 11:04:49,505] A3C_AGENT_WORKER-Thread-3 INFO:Local step 262500, global step 4191505: loss 0.1476
[2019-04-04 11:04:49,506] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 262500, global step 4191506: learning rate 0.0000
[2019-04-04 11:04:50,844] A3C_AGENT_WORKER-Thread-4 INFO:Local step 262500, global step 4192244: loss 0.1376
[2019-04-04 11:04:50,848] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 262500, global step 4192245: learning rate 0.0000
[2019-04-04 11:04:51,921] A3C_AGENT_WORKER-Thread-6 INFO:Local step 262500, global step 4192837: loss 0.2422
[2019-04-04 11:04:51,923] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 262500, global step 4192838: learning rate 0.0000
[2019-04-04 11:04:51,972] A3C_AGENT_WORKER-Thread-13 INFO:Local step 262000, global step 4192866: loss 3.8515
[2019-04-04 11:04:51,974] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 262000, global step 4192867: learning rate 0.0000
[2019-04-04 11:04:52,326] A3C_AGENT_WORKER-Thread-2 INFO:Local step 262500, global step 4193069: loss 0.1364
[2019-04-04 11:04:52,326] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 262500, global step 4193069: learning rate 0.0000
[2019-04-04 11:04:53,028] A3C_AGENT_WORKER-Thread-20 INFO:Local step 261000, global step 4193451: loss 0.5018
[2019-04-04 11:04:53,029] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 261000, global step 4193451: learning rate 0.0000
[2019-04-04 11:04:53,730] A3C_AGENT_WORKER-Thread-19 INFO:Local step 262000, global step 4193796: loss 3.8344
[2019-04-04 11:04:53,733] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 262000, global step 4193796: learning rate 0.0000
[2019-04-04 11:04:54,287] A3C_AGENT_WORKER-Thread-10 INFO:Local step 262000, global step 4194071: loss 3.8255
[2019-04-04 11:04:54,287] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 262000, global step 4194072: learning rate 0.0000
[2019-04-04 11:04:56,280] A3C_AGENT_WORKER-Thread-5 INFO:Local step 261500, global step 4195120: loss 0.0964
[2019-04-04 11:04:56,282] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 261500, global step 4195120: learning rate 0.0000
[2019-04-04 11:04:56,949] A3C_AGENT_WORKER-Thread-14 INFO:Local step 261500, global step 4195480: loss 0.0940
[2019-04-04 11:04:56,949] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 261500, global step 4195480: learning rate 0.0000
[2019-04-04 11:04:58,039] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8358126e-14 3.7031389e-12 6.3998768e-35 9.6087236e-32 9.1716044e-26
 1.0000000e+00 6.0320496e-26], sum to 1.0000
[2019-04-04 11:04:58,042] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2705
[2019-04-04 11:04:58,048] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.53333333333333, 30.33333333333333, 128.3333333333333, 806.5, 26.0, 28.19614052572739, 1.078567858899413, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4369200.0000, 
sim time next is 4369800.0000, 
raw observation next is [14.51666666666667, 30.66666666666667, 141.6666666666667, 771.0, 26.0, 28.37360881888149, 1.107201483455023, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8647276084949217, 0.3066666666666667, 0.4722222222222224, 0.8519337016574585, 0.6666666666666666, 0.8644674015734575, 0.8690671611516744, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5803502], dtype=float32), 0.90013963]. 
=============================================
[2019-04-04 11:04:59,498] A3C_AGENT_WORKER-Thread-12 INFO:Local step 262500, global step 4196802: loss 0.2583
[2019-04-04 11:04:59,500] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 262500, global step 4196803: learning rate 0.0000
[2019-04-04 11:04:59,803] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9709639e-10 3.9034229e-09 7.5750399e-26 9.2753374e-24 5.0652883e-20
 1.0000000e+00 8.2035141e-19], sum to 1.0000
[2019-04-04 11:04:59,803] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7049
[2019-04-04 11:04:59,823] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.97928141553391, 0.3092569595354092, 0.0, 1.0, 43818.65271024382], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3816600.0000, 
sim time next is 3817200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.96308679781025, 0.3005873065762337, 0.0, 1.0, 43882.08580443809], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5802572331508541, 0.6001957688587446, 0.0, 1.0, 0.20896231335446708], 
reward next is 0.7910, 
noisyNet noise sample is [array([-0.12224159], dtype=float32), -1.3269829]. 
=============================================
[2019-04-04 11:05:00,100] A3C_AGENT_WORKER-Thread-18 INFO:Local step 262500, global step 4197131: loss 0.1499
[2019-04-04 11:05:00,105] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 262500, global step 4197131: learning rate 0.0000
[2019-04-04 11:05:01,583] A3C_AGENT_WORKER-Thread-11 INFO:Local step 262500, global step 4197898: loss 0.1619
[2019-04-04 11:05:01,584] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 262500, global step 4197898: learning rate 0.0000
[2019-04-04 11:05:01,729] A3C_AGENT_WORKER-Thread-15 INFO:Local step 263000, global step 4197973: loss 0.3855
[2019-04-04 11:05:01,730] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 263000, global step 4197973: learning rate 0.0000
[2019-04-04 11:05:03,999] A3C_AGENT_WORKER-Thread-16 INFO:Local step 263000, global step 4198697: loss 0.3832
[2019-04-04 11:05:04,000] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 263000, global step 4198697: learning rate 0.0000
[2019-04-04 11:05:04,044] A3C_AGENT_WORKER-Thread-17 INFO:Local step 261500, global step 4198703: loss 0.0886
[2019-04-04 11:05:04,044] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 261500, global step 4198703: learning rate 0.0000
[2019-04-04 11:05:04,691] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0153638e-12 1.4239195e-10 5.6185774e-29 1.5497665e-27 1.4545645e-22
 1.0000000e+00 3.8160907e-21], sum to 1.0000
[2019-04-04 11:05:04,695] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0630
[2019-04-04 11:05:04,731] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.666666666666667, 50.33333333333334, 0.0, 0.0, 26.0, 26.38614780080945, 0.7075970380197042, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4645200.0000, 
sim time next is 4645800.0000, 
raw observation next is [3.5, 51.0, 0.0, 0.0, 26.0, 26.4034021026214, 0.703426409010727, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5595567867036012, 0.51, 0.0, 0.0, 0.6666666666666666, 0.7002835085517832, 0.7344754696702424, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.854108], dtype=float32), -1.6775589]. 
=============================================
[2019-04-04 11:05:08,493] A3C_AGENT_WORKER-Thread-3 INFO:Local step 263000, global step 4199758: loss 0.3811
[2019-04-04 11:05:08,493] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 263000, global step 4199758: learning rate 0.0000
[2019-04-04 11:05:09,462] A3C_AGENT_WORKER-Thread-13 INFO:Local step 262500, global step 4199990: loss 0.2574
[2019-04-04 11:05:09,463] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 262500, global step 4199990: learning rate 0.0000
[2019-04-04 11:05:09,501] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 11:05:09,503] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 11:05:09,504] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 11:05:09,505] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:05:09,504] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 11:05:09,508] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:05:09,507] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run43
[2019-04-04 11:05:09,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:05:09,547] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run43
[2019-04-04 11:05:09,582] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run43
[2019-04-04 11:06:04,761] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.2734687], dtype=float32), 0.32132906]
[2019-04-04 11:06:04,761] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.616666666666667, 66.5, 0.0, 0.0, 26.0, 24.59085421402424, 0.2137156663884776, 0.0, 1.0, 42783.72000486489]
[2019-04-04 11:06:04,761] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 11:06:04,762] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.03297934e-09 9.70098490e-09 5.26597076e-24 2.49264799e-22
 2.47170831e-18 1.00000000e+00 1.33039974e-17], sampled 0.889824000766148
[2019-04-04 11:08:13,150] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 11:08:42,940] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 11:08:52,290] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 11:08:53,328] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 4200000, evaluation results [4200000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 11:08:55,081] A3C_AGENT_WORKER-Thread-4 INFO:Local step 263000, global step 4200344: loss 0.3764
[2019-04-04 11:08:55,082] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 263000, global step 4200345: learning rate 0.0000
[2019-04-04 11:08:57,459] A3C_AGENT_WORKER-Thread-6 INFO:Local step 263000, global step 4200889: loss 0.3779
[2019-04-04 11:08:57,459] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 263000, global step 4200889: learning rate 0.0000
[2019-04-04 11:08:57,580] A3C_AGENT_WORKER-Thread-19 INFO:Local step 262500, global step 4200913: loss 0.2646
[2019-04-04 11:08:57,626] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 262500, global step 4200913: learning rate 0.0000
[2019-04-04 11:08:58,095] A3C_AGENT_WORKER-Thread-2 INFO:Local step 263000, global step 4201026: loss 0.3858
[2019-04-04 11:08:58,098] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 263000, global step 4201026: learning rate 0.0000
[2019-04-04 11:08:58,677] A3C_AGENT_WORKER-Thread-10 INFO:Local step 262500, global step 4201131: loss 0.1563
[2019-04-04 11:08:58,695] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 262500, global step 4201131: learning rate 0.0000
[2019-04-04 11:08:58,998] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0294905e-11 2.1963088e-10 8.3295523e-27 4.3191767e-25 9.8407343e-21
 1.0000000e+00 1.6906635e-20], sum to 1.0000
[2019-04-04 11:08:58,998] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0680
[2019-04-04 11:08:59,079] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.666666666666667, 24.0, 0.0, 0.0, 26.0, 25.98631214262592, 0.5710022152829031, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4997400.0000, 
sim time next is 4998000.0000, 
raw observation next is [5.333333333333333, 25.0, 0.0, 0.0, 26.0, 25.91604813049745, 0.5552030826128559, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6103416435826409, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6596706775414543, 0.6850676942042853, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6396782], dtype=float32), -0.30514157]. 
=============================================
[2019-04-04 11:08:59,201] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[79.813484]
 [80.46965 ]
 [81.52457 ]
 [83.06319 ]
 [82.300896]], R is [[79.56434631]
 [79.76870728]
 [79.97102356]
 [80.17131042]
 [80.36959839]].
[2019-04-04 11:08:59,613] A3C_AGENT_WORKER-Thread-20 INFO:Local step 261500, global step 4201337: loss 0.0944
[2019-04-04 11:08:59,614] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 261500, global step 4201337: learning rate 0.0000
[2019-04-04 11:09:08,979] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6777039e-10 2.5243332e-09 1.5092425e-26 1.5025108e-25 1.3719957e-20
 1.0000000e+00 4.7430485e-20], sum to 1.0000
[2019-04-04 11:09:08,980] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0547
[2019-04-04 11:09:09,043] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 25.80009581152848, 0.5672165665247344, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5004000.0000, 
sim time next is 5004600.0000, 
raw observation next is [3.0, 36.5, 0.0, 0.0, 26.0, 25.79380883789021, 0.5085169282003934, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.365, 0.0, 0.0, 0.6666666666666666, 0.6494840698241843, 0.6695056427334644, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21971592], dtype=float32), 0.38320515]. 
=============================================
[2019-04-04 11:09:09,236] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4968539e-11 4.3222119e-09 1.4168019e-26 1.2037564e-24 7.7788727e-21
 1.0000000e+00 1.9283041e-19], sum to 1.0000
[2019-04-04 11:09:09,236] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9570
[2019-04-04 11:09:09,371] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.66666666666667, 73.33333333333334, 35.33333333333334, 26.0, 25.32568304764206, 0.3313919515164682, 0.0, 1.0, 38913.27951270073], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4263000.0000, 
sim time next is 4263600.0000, 
raw observation next is [3.0, 50.33333333333334, 91.66666666666667, 44.16666666666667, 26.0, 25.33272984384686, 0.3674328254692976, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.5033333333333334, 0.3055555555555556, 0.04880294659300185, 0.6666666666666666, 0.6110608203205716, 0.6224776084897659, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.096589], dtype=float32), -1.7514234]. 
=============================================
[2019-04-04 11:09:10,723] A3C_AGENT_WORKER-Thread-5 INFO:Local step 262000, global step 4203784: loss 3.9288
[2019-04-04 11:09:10,724] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 262000, global step 4203784: learning rate 0.0000
[2019-04-04 11:09:11,003] A3C_AGENT_WORKER-Thread-14 INFO:Local step 262000, global step 4203838: loss 3.9078
[2019-04-04 11:09:11,004] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 262000, global step 4203838: learning rate 0.0000
[2019-04-04 11:09:12,052] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:09:12,052] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:09:12,056] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run32
[2019-04-04 11:09:15,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:09:15,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:09:15,160] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run32
[2019-04-04 11:09:15,993] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2559870e-09 2.6546640e-08 5.5790344e-25 5.2005945e-23 3.3118399e-19
 1.0000000e+00 5.4186795e-18], sum to 1.0000
[2019-04-04 11:09:15,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5474
[2019-04-04 11:09:16,066] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 65.5, 0.0, 0.0, 26.0, 24.74757376950879, 0.2093918252579983, 0.0, 1.0, 39537.28030513781], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4858200.0000, 
sim time next is 4858800.0000, 
raw observation next is [-3.333333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 24.70932914879218, 0.2034684142389032, 0.0, 1.0, 39562.01077771688], 
processed observation next is [0.0, 0.21739130434782608, 0.37026777469990774, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5591107623993482, 0.5678228047463011, 0.0, 1.0, 0.18839052751293753], 
reward next is 0.8116, 
noisyNet noise sample is [array([1.1870757], dtype=float32), 0.5170183]. 
=============================================
[2019-04-04 11:09:16,953] A3C_AGENT_WORKER-Thread-12 INFO:Local step 263000, global step 4204897: loss 0.3828
[2019-04-04 11:09:16,954] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 263000, global step 4204897: learning rate 0.0000
[2019-04-04 11:09:18,288] A3C_AGENT_WORKER-Thread-18 INFO:Local step 263000, global step 4205084: loss 0.3837
[2019-04-04 11:09:18,336] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 263000, global step 4205084: learning rate 0.0000
[2019-04-04 11:09:22,343] A3C_AGENT_WORKER-Thread-11 INFO:Local step 263000, global step 4205728: loss 0.3858
[2019-04-04 11:09:22,347] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 263000, global step 4205728: learning rate 0.0000
[2019-04-04 11:09:23,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:09:23,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:09:23,592] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run32
[2019-04-04 11:09:25,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:09:25,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:09:25,172] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run32
[2019-04-04 11:09:26,394] A3C_AGENT_WORKER-Thread-17 INFO:Local step 262000, global step 4206543: loss 3.9425
[2019-04-04 11:09:26,417] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 262000, global step 4206543: learning rate 0.0000
[2019-04-04 11:09:27,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:09:27,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:09:27,602] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run32
[2019-04-04 11:09:27,771] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:09:27,772] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:09:27,811] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run32
[2019-04-04 11:09:32,614] A3C_AGENT_WORKER-Thread-13 INFO:Local step 263000, global step 4207401: loss 0.3922
[2019-04-04 11:09:32,672] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 263000, global step 4207401: learning rate 0.0000
[2019-04-04 11:09:37,006] A3C_AGENT_WORKER-Thread-19 INFO:Local step 263000, global step 4208250: loss 0.3804
[2019-04-04 11:09:37,011] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 263000, global step 4208250: learning rate 0.0000
[2019-04-04 11:09:39,864] A3C_AGENT_WORKER-Thread-10 INFO:Local step 263000, global step 4208753: loss 0.3894
[2019-04-04 11:09:39,865] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 263000, global step 4208753: learning rate 0.0000
[2019-04-04 11:09:40,120] A3C_AGENT_WORKER-Thread-20 INFO:Local step 262000, global step 4208800: loss 3.9700
[2019-04-04 11:09:40,138] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 262000, global step 4208800: learning rate 0.0000
[2019-04-04 11:09:47,697] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1646258e-11 2.7783917e-10 2.7228718e-28 1.9472725e-26 8.0264090e-22
 1.0000000e+00 3.8686664e-21], sum to 1.0000
[2019-04-04 11:09:47,701] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7005
[2019-04-04 11:09:47,707] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.4, 20.0, 0.0, 0.0, 26.0, 26.18448605790664, 0.6094310384770645, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5094000.0000, 
sim time next is 5094600.0000, 
raw observation next is [8.350000000000001, 22.5, 0.0, 0.0, 26.0, 26.09861760941078, 0.5907181241925109, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6939058171745154, 0.225, 0.0, 0.0, 0.6666666666666666, 0.6748848007842317, 0.6969060413975036, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6307329], dtype=float32), -0.060324408]. 
=============================================
[2019-04-04 11:09:48,927] A3C_AGENT_WORKER-Thread-5 INFO:Local step 262500, global step 4210576: loss 0.2531
[2019-04-04 11:09:48,996] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 262500, global step 4210576: learning rate 0.0000
[2019-04-04 11:09:49,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:09:49,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:09:49,023] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run32
[2019-04-04 11:09:49,467] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:09:49,468] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:09:49,491] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run32
[2019-04-04 11:09:50,443] A3C_AGENT_WORKER-Thread-14 INFO:Local step 262500, global step 4210831: loss 0.2531
[2019-04-04 11:09:50,479] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 262500, global step 4210831: learning rate 0.0000
[2019-04-04 11:09:51,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:09:51,027] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:09:51,030] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run32
[2019-04-04 11:10:00,316] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:10:00,316] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:10:00,320] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run32
[2019-04-04 11:10:00,551] A3C_AGENT_WORKER-Thread-17 INFO:Local step 262500, global step 4213167: loss 0.2333
[2019-04-04 11:10:00,551] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 262500, global step 4213167: learning rate 0.0000
[2019-04-04 11:10:01,066] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4316159e-13 1.0413261e-11 3.2039803e-31 3.1004123e-29 2.2505521e-24
 1.0000000e+00 2.0397076e-23], sum to 1.0000
[2019-04-04 11:10:01,066] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8983
[2019-04-04 11:10:01,073] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.83333333333333, 17.33333333333334, 0.0, 0.0, 26.0, 27.95629908040073, 1.013296757121352, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5080200.0000, 
sim time next is 5080800.0000, 
raw observation next is [10.66666666666667, 17.66666666666667, 0.0, 0.0, 26.0, 27.86694085280913, 0.9950934045529004, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7580794090489382, 0.17666666666666672, 0.0, 0.0, 0.6666666666666666, 0.8222450710674275, 0.8316978015176334, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4854755], dtype=float32), 1.7763885]. 
=============================================
[2019-04-04 11:10:01,772] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:10:01,772] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:10:01,779] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run32
[2019-04-04 11:10:02,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:10:02,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:10:02,970] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run32
[2019-04-04 11:10:07,923] A3C_AGENT_WORKER-Thread-20 INFO:Local step 262500, global step 4214928: loss 0.2495
[2019-04-04 11:10:07,924] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 262500, global step 4214928: learning rate 0.0000
[2019-04-04 11:10:10,067] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.06175909 0.08958893 0.00086278 0.00124184 0.00335391 0.8352881
 0.00790531], sum to 1.0000
[2019-04-04 11:10:10,067] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8337
[2019-04-04 11:10:10,094] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.2, 95.16666666666667, 0.0, 0.0, 20.0, 18.7323782676697, -1.092149175812832, 0.0, 1.0, 198377.5275261486], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 600.0000, 
sim time next is 1200.0000, 
raw observation next is [2.4, 95.33333333333334, 0.0, 0.0, 21.0, 18.73354350556502, -1.043026692398193, 0.0, 1.0, 185716.4674550301], 
processed observation next is [0.0, 0.0, 0.5290858725761773, 0.9533333333333335, 0.0, 0.0, 0.25, 0.061128625463751675, 0.152324435867269, 0.0, 1.0, 0.8843641307382386], 
reward next is 0.1156, 
noisyNet noise sample is [array([-1.2092991], dtype=float32), -0.67214996]. 
=============================================
[2019-04-04 11:10:13,589] A3C_AGENT_WORKER-Thread-5 INFO:Local step 263000, global step 4216393: loss 0.3824
[2019-04-04 11:10:13,591] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 263000, global step 4216393: learning rate 0.0000
[2019-04-04 11:10:15,317] A3C_AGENT_WORKER-Thread-14 INFO:Local step 263000, global step 4216919: loss 0.3826
[2019-04-04 11:10:15,318] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 263000, global step 4216919: learning rate 0.0000
[2019-04-04 11:10:17,031] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.2653330e-11 3.3303280e-09 9.6748108e-26 4.7117506e-24 4.7326021e-20
 1.0000000e+00 1.4538068e-19], sum to 1.0000
[2019-04-04 11:10:17,034] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0293
[2019-04-04 11:10:17,147] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 26.0, 23.38515303824006, 0.04955687046180005, 1.0, 1.0, 203503.946585177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 113400.0000, 
sim time next is 114000.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 26.0, 24.11149430782881, 0.1403185668260233, 0.0, 1.0, 158987.191564885], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5092911923190675, 0.5467728556086745, 0.0, 1.0, 0.7570818645946905], 
reward next is 0.2429, 
noisyNet noise sample is [array([-0.53451395], dtype=float32), -0.3103223]. 
=============================================
[2019-04-04 11:10:17,151] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[77.11515 ]
 [74.409775]
 [71.415474]
 [71.75997 ]
 [72.09608 ]], R is [[76.48210144]
 [75.74821472]
 [75.02722931]
 [75.05848694]
 [75.08968353]].
[2019-04-04 11:10:22,593] A3C_AGENT_WORKER-Thread-17 INFO:Local step 263000, global step 4219047: loss 0.3698
[2019-04-04 11:10:22,593] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 263000, global step 4219047: learning rate 0.0000
[2019-04-04 11:10:23,786] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.01626769e-10 8.39452230e-10 1.73217785e-26 2.94214352e-25
 1.25016015e-20 1.00000000e+00 2.70984724e-20], sum to 1.0000
[2019-04-04 11:10:23,787] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0796
[2019-04-04 11:10:23,820] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.966666666666667, 94.66666666666666, 0.0, 0.0, 26.0, 24.8668013281803, 0.2344234469347535, 0.0, 1.0, 41641.4519835266], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 508800.0000, 
sim time next is 509400.0000, 
raw observation next is [2.15, 94.0, 0.0, 0.0, 26.0, 24.86016816604192, 0.2339122120778888, 0.0, 1.0, 41478.78439377346], 
processed observation next is [1.0, 0.9130434782608695, 0.5221606648199446, 0.94, 0.0, 0.0, 0.6666666666666666, 0.5716806805034933, 0.5779707373592963, 0.0, 1.0, 0.19751802092273074], 
reward next is 0.8025, 
noisyNet noise sample is [array([-1.1195339], dtype=float32), 0.40212807]. 
=============================================
[2019-04-04 11:10:29,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:10:29,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:10:29,404] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run32
[2019-04-04 11:10:29,994] A3C_AGENT_WORKER-Thread-20 INFO:Local step 263000, global step 4221266: loss 0.3655
[2019-04-04 11:10:29,995] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 263000, global step 4221266: learning rate 0.0000
[2019-04-04 11:10:31,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:10:31,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:10:31,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run32
[2019-04-04 11:10:32,026] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5625891e-08 6.8705793e-07 9.3234128e-21 1.0875632e-19 1.5317187e-16
 9.9999928e-01 1.3247384e-15], sum to 1.0000
[2019-04-04 11:10:32,028] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1330
[2019-04-04 11:10:32,082] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.2, 70.33333333333334, 0.0, 0.0, 26.0, 22.63755588034809, -0.2684941272091597, 0.0, 1.0, 49318.87435764341], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 357600.0000, 
sim time next is 358200.0000, 
raw observation next is [-15.3, 71.0, 0.0, 0.0, 26.0, 22.58875130518184, -0.2735284855423374, 0.0, 1.0, 49324.73093906156], 
processed observation next is [1.0, 0.13043478260869565, 0.03878116343490302, 0.71, 0.0, 0.0, 0.6666666666666666, 0.38239594209848676, 0.4088238381525542, 0.0, 1.0, 0.2348796711383884], 
reward next is 0.7651, 
noisyNet noise sample is [array([-1.2365478], dtype=float32), -1.3579143]. 
=============================================
[2019-04-04 11:10:38,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:10:38,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:10:38,900] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run32
[2019-04-04 11:10:42,893] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5739160e-08 3.7950952e-07 1.2005161e-20 5.7178048e-19 1.5111167e-15
 9.9999952e-01 5.8480984e-15], sum to 1.0000
[2019-04-04 11:10:42,893] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5757
[2019-04-04 11:10:42,914] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.9, 75.5, 0.0, 0.0, 26.0, 22.09260767470404, -0.3987692275161987, 0.0, 1.0, 48616.22180920339], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 365400.0000, 
sim time next is 366000.0000, 
raw observation next is [-16.0, 76.33333333333334, 0.0, 0.0, 26.0, 22.080181400827, -0.4147464994913093, 0.0, 1.0, 48610.83076893476], 
processed observation next is [1.0, 0.21739130434782608, 0.01939058171745151, 0.7633333333333334, 0.0, 0.0, 0.6666666666666666, 0.34001511673558343, 0.36175116683623026, 0.0, 1.0, 0.23148014651873694], 
reward next is 0.7685, 
noisyNet noise sample is [array([1.0107827], dtype=float32), 1.0823113]. 
=============================================
[2019-04-04 11:10:42,944] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[61.566296]
 [61.713642]
 [61.851013]
 [62.009296]
 [62.15397 ]], R is [[61.58295441]
 [61.73561859]
 [61.88666534]
 [62.03603745]
 [62.18212509]].
[2019-04-04 11:10:47,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:10:47,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:10:47,196] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run32
[2019-04-04 11:10:58,130] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.61831445e-10 7.20396454e-09 6.13833994e-26 1.82196956e-23
 1.02752686e-19 1.00000000e+00 2.48209233e-18], sum to 1.0000
[2019-04-04 11:10:58,130] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1718
[2019-04-04 11:10:58,160] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.2, 75.0, 0.0, 0.0, 26.0, 24.18031419951986, 0.0777531429696246, 0.0, 1.0, 43321.34628324317], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 617400.0000, 
sim time next is 618000.0000, 
raw observation next is [-4.3, 75.0, 0.0, 0.0, 26.0, 24.13657741827112, 0.06734081222012471, 0.0, 1.0, 43531.62785442648], 
processed observation next is [0.0, 0.13043478260869565, 0.34349030470914127, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5113814515225933, 0.5224469374067082, 0.0, 1.0, 0.20729346597345943], 
reward next is 0.7927, 
noisyNet noise sample is [array([-0.280114], dtype=float32), -0.89473397]. 
=============================================
[2019-04-04 11:10:58,191] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.40815]
 [77.56483]
 [77.72167]
 [77.82195]
 [77.95764]], R is [[77.26889038]
 [77.28990936]
 [77.31157684]
 [77.3337326 ]
 [77.35639954]].
[2019-04-04 11:10:59,734] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.95793470e-10 1.01391056e-08 1.74627723e-25 1.38119384e-23
 1.62354707e-19 1.00000000e+00 1.51105756e-18], sum to 1.0000
[2019-04-04 11:10:59,737] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3706
[2019-04-04 11:10:59,783] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.1, 75.0, 0.0, 0.0, 26.0, 24.18661211152649, 0.08734234614710512, 0.0, 1.0, 43142.31765798671], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 616800.0000, 
sim time next is 617400.0000, 
raw observation next is [-4.2, 75.0, 0.0, 0.0, 26.0, 24.18030674665559, 0.07775236672230579, 0.0, 1.0, 43321.34206487755], 
processed observation next is [0.0, 0.13043478260869565, 0.34626038781163443, 0.75, 0.0, 0.0, 0.6666666666666666, 0.515025562221299, 0.5259174555741019, 0.0, 1.0, 0.20629210507084547], 
reward next is 0.7937, 
noisyNet noise sample is [array([1.0317885], dtype=float32), 0.48796287]. 
=============================================
[2019-04-04 11:11:04,204] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.8822015e-10 4.6325037e-09 5.5519868e-25 9.8065800e-24 2.0594769e-19
 1.0000000e+00 1.7097136e-18], sum to 1.0000
[2019-04-04 11:11:04,204] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0451
[2019-04-04 11:11:04,223] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 24.71867711647788, 0.1558109152499356, 0.0, 1.0, 41699.50070312083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 686400.0000, 
sim time next is 687000.0000, 
raw observation next is [-3.816666666666666, 70.66666666666667, 0.0, 0.0, 26.0, 24.68520115785773, 0.1480110850181366, 0.0, 1.0, 41630.89186927224], 
processed observation next is [0.0, 0.9565217391304348, 0.3568790397045245, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.5571000964881442, 0.5493370283393789, 0.0, 1.0, 0.1982423422346297], 
reward next is 0.8018, 
noisyNet noise sample is [array([1.0088408], dtype=float32), 0.34672752]. 
=============================================
[2019-04-04 11:11:04,230] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.022675]
 [77.1343  ]
 [77.19876 ]
 [77.2792  ]
 [77.359604]], R is [[76.96741486]
 [76.9991684 ]
 [77.03022003]
 [77.06064606]
 [77.09049988]].
[2019-04-04 11:11:05,740] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1091331e-11 3.4204667e-10 2.5499320e-28 5.7791210e-26 4.1410790e-21
 1.0000000e+00 6.2678343e-21], sum to 1.0000
[2019-04-04 11:11:05,741] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9455
[2019-04-04 11:11:05,807] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 80.0, 132.5, 531.0, 26.0, 25.00245289473588, 0.349198396560372, 0.0, 1.0, 18723.87818829856], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 568800.0000, 
sim time next is 569400.0000, 
raw observation next is [-1.2, 80.5, 130.6666666666667, 509.6666666666666, 26.0, 24.999815484291, 0.3477009824114687, 0.0, 1.0, 25931.66447797677], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.805, 0.4355555555555557, 0.5631675874769796, 0.6666666666666666, 0.5833179570242498, 0.6159003274704896, 0.0, 1.0, 0.12348411656179414], 
reward next is 0.8765, 
noisyNet noise sample is [array([-1.4886668], dtype=float32), -0.96734583]. 
=============================================
[2019-04-04 11:11:18,531] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.4162646e-13 1.7926082e-11 5.0643470e-33 6.6601434e-30 3.2942922e-24
 1.0000000e+00 2.2548240e-23], sum to 1.0000
[2019-04-04 11:11:18,537] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6963
[2019-04-04 11:11:18,556] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.03387725505881, 0.412729189198129, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1022400.0000, 
sim time next is 1023000.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 24.95350508454042, 0.402919547548357, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.579458757045035, 0.6343065158494524, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6627633], dtype=float32), -0.21959288]. 
=============================================
[2019-04-04 11:11:18,569] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[ 98.1194  ]
 [ 99.991776]
 [ 99.24762 ]
 [ 98.3702  ]
 [100.45278 ]], R is [[98.97705078]
 [98.9872818 ]
 [98.99741364]
 [99.00743866]
 [99.0173645 ]].
[2019-04-04 11:11:25,210] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7760098e-09 1.3204132e-07 5.9803497e-22 1.4554928e-20 1.0587310e-16
 9.9999988e-01 1.8079557e-16], sum to 1.0000
[2019-04-04 11:11:25,211] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5688
[2019-04-04 11:11:25,245] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.20465457082566, -0.1682197647735297, 0.0, 1.0, 48611.32968001892], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 353400.0000, 
sim time next is 354000.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.08023406681285, -0.1852696629006809, 0.0, 1.0, 48829.27456300599], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4233528389010708, 0.438243445699773, 0.0, 1.0, 0.23252035506193328], 
reward next is 0.7675, 
noisyNet noise sample is [array([-0.17841126], dtype=float32), -0.8317569]. 
=============================================
[2019-04-04 11:11:25,258] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[66.44647]
 [66.5461 ]
 [66.70743]
 [66.863  ]
 [67.00882]], R is [[66.35348511]
 [66.45846558]
 [66.56345367]
 [66.66828918]
 [66.7728653 ]].
[2019-04-04 11:11:26,481] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7713853e-09 5.8677618e-09 1.3782188e-25 7.4920526e-24 4.4033538e-19
 1.0000000e+00 1.2468086e-18], sum to 1.0000
[2019-04-04 11:11:26,481] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8303
[2019-04-04 11:11:26,484] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.35457843481877, 0.324250079432387, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1205400.0000, 
sim time next is 1206000.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 26.0, 24.33004370516696, 0.3182688865108116, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 1.0, 0.922437673130194, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5275036420972468, 0.6060896288369372, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49313524], dtype=float32), -0.069606505]. 
=============================================
[2019-04-04 11:11:26,490] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[79.72908 ]
 [79.78085 ]
 [79.83424 ]
 [79.891495]
 [79.93724 ]], R is [[79.87238312]
 [80.0736618 ]
 [80.27292633]
 [80.47019958]
 [80.66549683]].
[2019-04-04 11:11:29,228] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5034324e-12 9.0732630e-11 2.2948532e-31 3.4980359e-29 1.8793668e-24
 1.0000000e+00 4.7742638e-23], sum to 1.0000
[2019-04-04 11:11:29,231] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5721
[2019-04-04 11:11:29,252] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 76.33333333333334, 0.0, 0.0, 26.0, 25.73072201518756, 0.619269484806908, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1027200.0000, 
sim time next is 1027800.0000, 
raw observation next is [14.4, 76.0, 0.0, 0.0, 26.0, 25.81706221988414, 0.6229881632690072, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6514218516570116, 0.707662721089669, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9769679], dtype=float32), -0.96249294]. 
=============================================
[2019-04-04 11:11:30,303] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6530551e-10 5.5164593e-09 3.1785092e-26 2.5951368e-24 1.4567316e-20
 1.0000000e+00 6.3346609e-20], sum to 1.0000
[2019-04-04 11:11:30,308] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7505
[2019-04-04 11:11:30,328] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.7, 72.66666666666666, 0.0, 0.0, 26.0, 24.54763094769162, 0.1655216159606005, 0.0, 1.0, 39041.28548773803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 881400.0000, 
sim time next is 882000.0000, 
raw observation next is [-0.6, 72.0, 0.0, 0.0, 26.0, 24.54187230738459, 0.1743000245596491, 0.0, 1.0, 39027.96900325678], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5451560256153826, 0.5581000081865497, 0.0, 1.0, 0.1858474714440799], 
reward next is 0.8142, 
noisyNet noise sample is [array([0.06492967], dtype=float32), 0.07367799]. 
=============================================
[2019-04-04 11:11:30,351] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[81.21812 ]
 [81.23474 ]
 [81.255325]
 [81.27077 ]
 [81.26435 ]], R is [[81.19913483]
 [81.20123291]
 [81.20332336]
 [81.20542908]
 [81.20761871]].
[2019-04-04 11:11:44,228] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2820322e-12 1.1979701e-10 1.0796880e-29 4.7096406e-27 4.6298824e-23
 1.0000000e+00 2.2691489e-22], sum to 1.0000
[2019-04-04 11:11:44,228] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6661
[2019-04-04 11:11:44,249] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.38783439109621, 0.4861293562702327, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1362000.0000, 
sim time next is 1362600.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.25149062243251, 0.4487308803844172, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6042908852027091, 0.649576960128139, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.53134096], dtype=float32), -0.44366708]. 
=============================================
[2019-04-04 11:11:45,333] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1281120e-13 3.5610449e-11 1.5456467e-33 2.2687331e-30 6.6206187e-25
 1.0000000e+00 6.1286596e-24], sum to 1.0000
[2019-04-04 11:11:45,333] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3353
[2019-04-04 11:11:45,380] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.56666666666667, 53.0, 41.83333333333334, 30.83333333333334, 26.0, 27.51160435628253, 0.7542803724095822, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1615200.0000, 
sim time next is 1615800.0000, 
raw observation next is [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 26.0, 27.00002913858884, 0.764495754323134, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8056325023084026, 0.535, 0.11222222222222224, 0.027255985267034995, 0.6666666666666666, 0.7500024282157366, 0.7548319181077113, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.68771744], dtype=float32), 0.9940848]. 
=============================================
[2019-04-04 11:11:48,381] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8826701e-10 1.6082490e-10 1.9510188e-27 1.7456911e-25 2.1185123e-21
 1.0000000e+00 1.9382443e-20], sum to 1.0000
[2019-04-04 11:11:48,381] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3343
[2019-04-04 11:11:48,399] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.51165837469216, 0.5407112855218006, 0.0, 1.0, 42907.38029109663], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1720200.0000, 
sim time next is 1720800.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.48564855080937, 0.5398911094557138, 0.0, 1.0, 53866.41614994144], 
processed observation next is [1.0, 0.9565217391304348, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6238040459007808, 0.6799637031519046, 0.0, 1.0, 0.2565067435711497], 
reward next is 0.7435, 
noisyNet noise sample is [array([1.1148006], dtype=float32), 1.0571402]. 
=============================================
[2019-04-04 11:11:50,087] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1315441e-11 1.6358552e-09 7.1861130e-27 4.3693063e-25 1.1676313e-20
 1.0000000e+00 7.2922349e-20], sum to 1.0000
[2019-04-04 11:11:50,087] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2605
[2019-04-04 11:11:50,103] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.97014485344152, 0.3563164558789707, 0.0, 1.0, 43686.58880230947], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1748400.0000, 
sim time next is 1749000.0000, 
raw observation next is [-1.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.94396699624154, 0.3499636160940953, 0.0, 1.0, 43751.74241924233], 
processed observation next is [0.0, 0.21739130434782608, 0.4321329639889197, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5786639163534616, 0.6166545386980318, 0.0, 1.0, 0.2083416305678206], 
reward next is 0.7917, 
noisyNet noise sample is [array([-2.030198], dtype=float32), 0.18247138]. 
=============================================
[2019-04-04 11:11:50,115] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.398285]
 [82.50499 ]
 [82.617386]
 [82.73401 ]
 [82.879364]], R is [[82.26249695]
 [82.23184204]
 [82.20172119]
 [82.17204285]
 [82.14286804]].
[2019-04-04 11:11:53,322] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4154777e-11 9.8728387e-11 1.9262513e-28 2.0117851e-26 1.0237224e-22
 1.0000000e+00 5.2505140e-21], sum to 1.0000
[2019-04-04 11:11:53,323] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4262
[2019-04-04 11:11:53,383] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 24.95749673234162, 0.4094366662074354, 1.0, 1.0, 97941.51345196718], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1453800.0000, 
sim time next is 1454400.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 24.87646602060681, 0.4274823782725212, 1.0, 1.0, 111018.756998163], 
processed observation next is [1.0, 0.8695652173913043, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5730388350505674, 0.6424941260908404, 1.0, 1.0, 0.5286607476103], 
reward next is 0.4713, 
noisyNet noise sample is [array([-0.29332256], dtype=float32), -0.8644982]. 
=============================================
[2019-04-04 11:12:12,397] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8035717e-12 2.2174169e-11 3.5237081e-30 5.7165164e-28 2.4898717e-23
 1.0000000e+00 8.6361097e-23], sum to 1.0000
[2019-04-04 11:12:12,400] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0880
[2019-04-04 11:12:12,437] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.7, 100.0, 0.0, 0.0, 26.0, 24.94678858588296, 0.300564533491323, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 934200.0000, 
sim time next is 934800.0000, 
raw observation next is [4.8, 100.0, 0.0, 0.0, 26.0, 24.80298063268252, 0.279504609684836, 0.0, 1.0, 18711.16008389565], 
processed observation next is [1.0, 0.8260869565217391, 0.5955678670360112, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5669150527235433, 0.5931682032282787, 0.0, 1.0, 0.089100762304265], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.3878368], dtype=float32), 1.7756269]. 
=============================================
[2019-04-04 11:12:18,533] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2188057e-10 3.7039287e-09 5.0337214e-26 2.5051808e-24 1.2006265e-20
 1.0000000e+00 2.0716112e-19], sum to 1.0000
[2019-04-04 11:12:18,533] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0125
[2019-04-04 11:12:18,552] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.3, 65.0, 153.8333333333333, 0.0, 26.0, 25.04139162581322, 0.4976438229751518, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1172400.0000, 
sim time next is 1173000.0000, 
raw observation next is [18.3, 65.0, 148.6666666666667, 0.0, 26.0, 25.04541450978305, 0.4970647284405432, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.4955555555555557, 0.0, 0.6666666666666666, 0.5871178758152542, 0.6656882428135144, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.5385242], dtype=float32), -0.5184027]. 
=============================================
[2019-04-04 11:12:18,565] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[79.92377]
 [80.15263]
 [80.3661 ]
 [80.58421]
 [80.82857]], R is [[79.89056396]
 [80.09165955]
 [80.29074097]
 [80.48783112]
 [80.68295288]].
[2019-04-04 11:12:29,259] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4978288e-11 4.2133470e-11 2.9677578e-30 5.7434678e-28 8.0728952e-23
 1.0000000e+00 4.6582830e-22], sum to 1.0000
[2019-04-04 11:12:29,259] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4434
[2019-04-04 11:12:29,312] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.82650109826463, 0.4746019686469252, 1.0, 1.0, 173374.3180651607], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1363800.0000, 
sim time next is 1364400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.95394460829484, 0.4906962046783849, 1.0, 1.0, 25845.48085264178], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.57949538402457, 0.6635654015594616, 1.0, 1.0, 0.12307371834591324], 
reward next is 0.8769, 
noisyNet noise sample is [array([-0.12542588], dtype=float32), 1.1450971]. 
=============================================
[2019-04-04 11:12:35,970] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9496722e-11 1.4920649e-09 6.1556924e-26 3.5094304e-25 6.9395275e-21
 1.0000000e+00 1.6216026e-20], sum to 1.0000
[2019-04-04 11:12:35,970] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9058
[2019-04-04 11:12:36,041] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 62.0, 124.5, 0.0, 26.0, 25.74436302516578, 0.3451961188619516, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1951200.0000, 
sim time next is 1951800.0000, 
raw observation next is [-3.3, 62.0, 120.3333333333333, 0.0, 26.0, 25.74158055138917, 0.3446914087737483, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.37119113573407203, 0.62, 0.401111111111111, 0.0, 0.6666666666666666, 0.6451317126157642, 0.6148971362579161, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17372279], dtype=float32), -0.4441223]. 
=============================================
[2019-04-04 11:12:43,146] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2748616e-11 1.6007634e-10 1.0263984e-27 5.9676218e-26 3.1569594e-21
 1.0000000e+00 1.4081958e-20], sum to 1.0000
[2019-04-04 11:12:43,149] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2161
[2019-04-04 11:12:43,168] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.516666666666667, 89.5, 0.0, 0.0, 26.0, 25.54640666472951, 0.5594023834784344, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1458600.0000, 
sim time next is 1459200.0000, 
raw observation next is [1.433333333333334, 90.0, 0.0, 0.0, 26.0, 25.57926635995268, 0.5604650887250195, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.502308402585411, 0.9, 0.0, 0.0, 0.6666666666666666, 0.6316055299960567, 0.6868216962416732, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24991496], dtype=float32), -0.03034047]. 
=============================================
[2019-04-04 11:13:14,131] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.56391193e-11 3.94132088e-10 1.73981250e-27 1.01095305e-25
 1.19331567e-21 1.00000000e+00 5.72037601e-21], sum to 1.0000
[2019-04-04 11:13:14,132] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4854
[2019-04-04 11:13:14,192] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.00000000000001, 12.0, 0.0, 26.0, 25.47149085049789, 0.3547105723004126, 1.0, 1.0, 45744.98602468558], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2049000.0000, 
sim time next is 2049600.0000, 
raw observation next is [-3.9, 82.0, 6.999999999999999, 0.0, 26.0, 25.59352812684543, 0.2497789076089486, 1.0, 1.0, 36179.08616501695], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.02333333333333333, 0.0, 0.6666666666666666, 0.6327940105704526, 0.5832596358696495, 1.0, 1.0, 0.1722813626905569], 
reward next is 0.8277, 
noisyNet noise sample is [array([-0.62382936], dtype=float32), -1.3245085]. 
=============================================
[2019-04-04 11:13:20,544] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7625702e-10 6.9262409e-09 1.2808495e-24 2.0215736e-23 5.6360370e-19
 1.0000000e+00 6.4467541e-19], sum to 1.0000
[2019-04-04 11:13:20,544] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2796
[2019-04-04 11:13:20,580] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.533333333333333, 79.66666666666667, 0.0, 0.0, 26.0, 23.98950775395158, 0.04690790270484943, 0.0, 1.0, 43602.18021898931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2612400.0000, 
sim time next is 2613000.0000, 
raw observation next is [-6.616666666666667, 78.83333333333333, 0.0, 0.0, 26.0, 23.93241796145075, 0.03548468543848229, 0.0, 1.0, 43792.99852731366], 
processed observation next is [1.0, 0.21739130434782608, 0.2793167128347184, 0.7883333333333333, 0.0, 0.0, 0.6666666666666666, 0.4943681634542291, 0.511828228479494, 0.0, 1.0, 0.20853808822530315], 
reward next is 0.7915, 
noisyNet noise sample is [array([-0.13703875], dtype=float32), 0.008984998]. 
=============================================
[2019-04-04 11:13:20,602] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[75.74363 ]
 [75.91361 ]
 [76.070656]
 [76.20335 ]
 [76.32235 ]], R is [[75.59912872]
 [75.63551331]
 [75.67236328]
 [75.70960236]
 [75.74715424]].
[2019-04-04 11:13:45,356] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.6459506e-14 3.5695774e-12 3.9858148e-34 1.1054511e-31 1.0965375e-26
 1.0000000e+00 2.1029119e-25], sum to 1.0000
[2019-04-04 11:13:45,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6470
[2019-04-04 11:13:45,370] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 102.0, 680.0, 26.0, 26.6100672705708, 0.608934673504962, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3145800.0000, 
sim time next is 3146400.0000, 
raw observation next is [7.0, 100.0, 103.5, 696.5, 26.0, 26.64464284657288, 0.6339534915247851, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.345, 0.7696132596685082, 0.6666666666666666, 0.7203869038810732, 0.7113178305082618, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.171234], dtype=float32), 1.3330075]. 
=============================================
[2019-04-04 11:14:17,659] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.3500787e-10 4.1516861e-09 1.3418477e-24 2.6136069e-23 1.0802991e-19
 1.0000000e+00 6.2491474e-19], sum to 1.0000
[2019-04-04 11:14:17,659] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2068
[2019-04-04 11:14:17,715] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.79770385879045, 0.2880369297648992, 0.0, 1.0, 41126.51101256716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3379200.0000, 
sim time next is 3379800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.83107725053876, 0.2872756989043827, 0.0, 1.0, 41104.6626575145], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5692564375448965, 0.595758566301461, 0.0, 1.0, 0.19573648884530714], 
reward next is 0.8043, 
noisyNet noise sample is [array([0.31864384], dtype=float32), 1.1138939]. 
=============================================
[2019-04-04 11:14:25,786] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-04 11:14:25,813] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 11:14:25,829] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 11:14:25,829] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:14:25,832] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run44
[2019-04-04 11:14:25,879] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:14:25,882] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run44
[2019-04-04 11:14:25,933] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 11:14:25,933] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:14:25,937] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run44
[2019-04-04 11:17:10,735] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.2725548], dtype=float32), 0.3251401]
[2019-04-04 11:17:10,735] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.0, 55.0, 0.0, 0.0, 26.0, 25.45725081547984, 0.5416469245365692, 1.0, 1.0, 46894.60568729196]
[2019-04-04 11:17:10,735] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 11:17:10,736] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.9565457e-11 5.8717559e-10 1.6366805e-26 7.1634856e-25 3.5126538e-20
 1.0000000e+00 8.1277862e-20], sampled 0.7260830988609908
[2019-04-04 11:17:31,633] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 11:18:03,774] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 11:18:07,739] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 11:18:08,778] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 4300000, evaluation results [4300000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 11:18:12,479] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7393331e-10 1.0521105e-08 1.5999941e-25 2.1910933e-23 1.3611747e-18
 1.0000000e+00 8.4030859e-19], sum to 1.0000
[2019-04-04 11:18:12,479] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0151
[2019-04-04 11:18:12,495] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.166666666666667, 65.83333333333334, 0.0, 0.0, 26.0, 24.85050407171833, 0.3042177953676138, 0.0, 1.0, 40876.20876319417], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3561000.0000, 
sim time next is 3561600.0000, 
raw observation next is [-5.333333333333334, 66.66666666666667, 0.0, 0.0, 26.0, 24.79760695692762, 0.2940366779131138, 0.0, 1.0, 40881.88954816741], 
processed observation next is [0.0, 0.21739130434782608, 0.31486611265004616, 0.6666666666666667, 0.0, 0.0, 0.6666666666666666, 0.566467246410635, 0.598012225971038, 0.0, 1.0, 0.19467566451508292], 
reward next is 0.8053, 
noisyNet noise sample is [array([-1.7539216], dtype=float32), -0.50864434]. 
=============================================
[2019-04-04 11:18:25,227] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5715161e-09 2.4366607e-08 1.1603027e-24 3.2604132e-23 3.1412740e-19
 1.0000000e+00 1.7618269e-18], sum to 1.0000
[2019-04-04 11:18:25,232] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8071
[2019-04-04 11:18:25,247] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 60.83333333333333, 0.0, 0.0, 26.0, 24.7220160433251, 0.2177831447910835, 0.0, 1.0, 42843.31372042868], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3391800.0000, 
sim time next is 3392400.0000, 
raw observation next is [-3.0, 61.66666666666667, 0.0, 0.0, 26.0, 24.6823504006484, 0.216654337809406, 0.0, 1.0, 42859.26255919712], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5568625333873666, 0.5722181126031353, 0.0, 1.0, 0.20409172647236726], 
reward next is 0.7959, 
noisyNet noise sample is [array([-1.1834258], dtype=float32), 1.0722951]. 
=============================================
[2019-04-04 11:18:29,597] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0486615e-10 6.4252764e-10 1.5319924e-26 1.1405982e-24 2.5234767e-20
 1.0000000e+00 1.6170571e-19], sum to 1.0000
[2019-04-04 11:18:29,598] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9444
[2019-04-04 11:18:29,633] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 54.0, 112.5, 787.0, 26.0, 25.36327918478344, 0.4530128455845986, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3582000.0000, 
sim time next is 3582600.0000, 
raw observation next is [-3.833333333333333, 54.16666666666666, 113.0, 796.6666666666667, 26.0, 25.29799294424988, 0.4452642629049441, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.3564173591874424, 0.5416666666666665, 0.37666666666666665, 0.8802946593001842, 0.6666666666666666, 0.6081660786874901, 0.6484214209683147, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.50452346], dtype=float32), -0.19374461]. 
=============================================
[2019-04-04 11:18:37,626] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.6531289e-11 2.9903930e-09 2.7167944e-26 2.8332026e-24 1.6010369e-19
 1.0000000e+00 2.3206787e-19], sum to 1.0000
[2019-04-04 11:18:37,626] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8687
[2019-04-04 11:18:37,710] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 54.0, 112.5, 787.0, 26.0, 25.36320656070966, 0.4529625221767962, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3582000.0000, 
sim time next is 3582600.0000, 
raw observation next is [-3.833333333333333, 54.16666666666666, 113.0, 796.6666666666667, 26.0, 25.29792198713378, 0.4452134249670008, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.3564173591874424, 0.5416666666666665, 0.37666666666666665, 0.8802946593001842, 0.6666666666666666, 0.6081601655944816, 0.6484044749890002, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08263443], dtype=float32), -0.43368328]. 
=============================================
[2019-04-04 11:18:39,910] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.10173301e-10 1.18606169e-09 4.78114839e-25 1.11835414e-23
 1.58552623e-19 1.00000000e+00 3.07421209e-19], sum to 1.0000
[2019-04-04 11:18:39,910] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9089
[2019-04-04 11:18:39,919] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 28.5, 0.0, 0.0, 26.0, 25.76516500908979, 0.4927298755243791, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4041000.0000, 
sim time next is 4041600.0000, 
raw observation next is [-3.666666666666667, 29.33333333333333, 0.0, 0.0, 26.0, 25.61510017148385, 0.4877587912873596, 1.0, 1.0, 165282.8789156084], 
processed observation next is [1.0, 0.782608695652174, 0.3610341643582641, 0.2933333333333333, 0.0, 0.0, 0.6666666666666666, 0.6345916809569875, 0.6625862637624532, 1.0, 1.0, 0.7870613281695638], 
reward next is 0.2129, 
noisyNet noise sample is [array([2.7898886], dtype=float32), -0.77073234]. 
=============================================
[2019-04-04 11:18:54,231] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.3453655e-09 4.2266986e-08 5.4705295e-22 1.2364205e-20 4.3948166e-17
 1.0000000e+00 1.2321713e-16], sum to 1.0000
[2019-04-04 11:18:54,235] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5113
[2019-04-04 11:18:54,247] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.16610976312301, 0.1110729622839737, 0.0, 1.0, 43648.97673982619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3988800.0000, 
sim time next is 3989400.0000, 
raw observation next is [-12.16666666666667, 64.0, 0.0, 0.0, 26.0, 24.09998530977439, 0.1023588581028433, 0.0, 1.0, 43691.85250447007], 
processed observation next is [1.0, 0.17391304347826086, 0.12557710064635264, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5083321091478658, 0.5341196193676144, 0.0, 1.0, 0.20805644049747654], 
reward next is 0.7919, 
noisyNet noise sample is [array([0.8961829], dtype=float32), -0.18919855]. 
=============================================
[2019-04-04 11:18:54,505] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9685649e-09 2.8833808e-08 2.5189634e-22 2.7921459e-21 2.6094508e-17
 1.0000000e+00 9.1106930e-17], sum to 1.0000
[2019-04-04 11:18:54,507] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3196
[2019-04-04 11:18:54,534] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.0, 67.0, 0.0, 0.0, 26.0, 23.72295404440339, 0.01967444336171912, 0.0, 1.0, 43821.18513515228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3993600.0000, 
sim time next is 3994200.0000, 
raw observation next is [-13.0, 66.0, 0.0, 0.0, 26.0, 23.68647104575859, 0.005360513786127984, 0.0, 1.0, 43795.13356183017], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.66, 0.0, 0.0, 0.6666666666666666, 0.47387258714654923, 0.5017868379287093, 0.0, 1.0, 0.20854825505633412], 
reward next is 0.7915, 
noisyNet noise sample is [array([0.431777], dtype=float32), -1.2430936]. 
=============================================
[2019-04-04 11:19:01,628] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5574104e-12 2.5706068e-10 1.3987132e-29 1.6360742e-27 3.1090099e-22
 1.0000000e+00 1.1898520e-21], sum to 1.0000
[2019-04-04 11:19:01,631] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1775
[2019-04-04 11:19:01,656] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 35.0, 93.83333333333334, 652.0, 26.0, 26.39599797082931, 0.7121658950505388, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4117200.0000, 
sim time next is 4117800.0000, 
raw observation next is [4.0, 35.0, 93.66666666666666, 609.0, 26.0, 26.76420128072959, 0.7429284752185271, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.3122222222222222, 0.6729281767955801, 0.6666666666666666, 0.7303501067274659, 0.7476428250728424, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05126759], dtype=float32), -0.7852112]. 
=============================================
[2019-04-04 11:19:11,141] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2025360e-10 6.2379493e-09 3.8163867e-26 7.2064623e-25 8.4586882e-20
 1.0000000e+00 2.3306581e-20], sum to 1.0000
[2019-04-04 11:19:11,143] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2988
[2019-04-04 11:19:11,156] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.75, 70.0, 0.0, 0.0, 26.0, 25.26916054921151, 0.3992736704193492, 0.0, 1.0, 40973.00385203881], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4595400.0000, 
sim time next is 4596000.0000, 
raw observation next is [-1.833333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 25.29566456900705, 0.3971516571523019, 0.0, 1.0, 37655.96869173537], 
processed observation next is [1.0, 0.17391304347826086, 0.41181902123730385, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.6079720474172543, 0.632383885717434, 0.0, 1.0, 0.17931413662731127], 
reward next is 0.8207, 
noisyNet noise sample is [array([0.49593872], dtype=float32), -1.1993725]. 
=============================================
[2019-04-04 11:19:11,185] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[80.63313]
 [80.69336]
 [80.70615]
 [80.60291]
 [80.5874 ]], R is [[80.57303619]
 [80.57219696]
 [80.52787781]
 [80.40349579]
 [80.3525238 ]].
[2019-04-04 11:19:13,603] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.5342928e-13 2.0565138e-11 4.3347121e-31 1.7991592e-28 2.6984959e-23
 1.0000000e+00 4.6081745e-23], sum to 1.0000
[2019-04-04 11:19:13,604] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6590
[2019-04-04 11:19:13,618] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 59.66666666666667, 204.6666666666667, 15.0, 26.0, 26.17621292804095, 0.5427284043271279, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4530000.0000, 
sim time next is 4530600.0000, 
raw observation next is [1.5, 59.0, 221.0, 18.0, 26.0, 26.2050105790231, 0.5507811084114114, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5041551246537397, 0.59, 0.7366666666666667, 0.019889502762430938, 0.6666666666666666, 0.6837508815852583, 0.6835937028038037, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19929938], dtype=float32), -1.0600512]. 
=============================================
[2019-04-04 11:19:15,748] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6005985e-10 1.3940452e-09 1.2798176e-26 1.5243341e-24 2.0604032e-20
 1.0000000e+00 2.1350244e-19], sum to 1.0000
[2019-04-04 11:19:15,749] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6269
[2019-04-04 11:19:15,759] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 63.66666666666667, 0.0, 0.0, 26.0, 25.6633808680146, 0.4234522742889, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3705000.0000, 
sim time next is 3705600.0000, 
raw observation next is [1.333333333333333, 65.33333333333334, 0.0, 0.0, 26.0, 25.59533956026232, 0.4086950042858339, 0.0, 1.0, 48168.58852606288], 
processed observation next is [0.0, 0.9130434782608695, 0.4995383194829178, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6329449633551935, 0.636231668095278, 0.0, 1.0, 0.2293742310764899], 
reward next is 0.7706, 
noisyNet noise sample is [array([2.2455623], dtype=float32), 0.17725497]. 
=============================================
[2019-04-04 11:19:19,410] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.03462655e-11 2.94867741e-10 5.66986698e-28 2.68371828e-27
 8.89775425e-22 1.00000000e+00 1.29185296e-21], sum to 1.0000
[2019-04-04 11:19:19,414] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1430
[2019-04-04 11:19:19,445] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 56.16666666666666, 201.6666666666667, 606.3333333333334, 26.0, 25.37189141583549, 0.4429090001489131, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4283400.0000, 
sim time next is 4284000.0000, 
raw observation next is [7.0, 57.0, 208.5, 551.0, 26.0, 25.40400240155363, 0.4433923069560957, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6565096952908588, 0.57, 0.695, 0.6088397790055249, 0.6666666666666666, 0.6170002001294691, 0.6477974356520318, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0691939], dtype=float32), -1.2139207]. 
=============================================
[2019-04-04 11:19:19,458] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[86.91536 ]
 [86.85437 ]
 [86.70742 ]
 [86.41    ]
 [86.156715]], R is [[86.98666382]
 [87.1167984 ]
 [87.24562836]
 [87.37317657]
 [87.49944305]].
[2019-04-04 11:19:25,634] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8188706e-10 5.3369720e-10 3.1720168e-26 1.6797700e-24 6.6393768e-20
 1.0000000e+00 1.6070151e-19], sum to 1.0000
[2019-04-04 11:19:25,634] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6909
[2019-04-04 11:19:25,653] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 77.0, 0.0, 0.0, 26.0, 25.20485588250902, 0.4320462410759723, 0.0, 1.0, 42786.29812309823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4748400.0000, 
sim time next is 4749000.0000, 
raw observation next is [-3.166666666666667, 78.16666666666667, 0.0, 0.0, 26.0, 25.19181364323104, 0.4256181048448116, 0.0, 1.0, 42147.43165358886], 
processed observation next is [1.0, 1.0, 0.3748845798707295, 0.7816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5993178036025867, 0.6418727016149371, 0.0, 1.0, 0.20070205549328027], 
reward next is 0.7993, 
noisyNet noise sample is [array([-0.75619954], dtype=float32), -0.19459043]. 
=============================================
[2019-04-04 11:19:25,661] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[79.856834]
 [79.90553 ]
 [79.96228 ]
 [80.00177 ]
 [79.969864]], R is [[79.68704224]
 [79.68643188]
 [79.6775589 ]
 [79.64839935]
 [79.59102631]].
[2019-04-04 11:19:27,734] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0180109e-11 1.6704207e-10 1.3641099e-28 1.1254182e-26 2.1766666e-21
 1.0000000e+00 4.7587951e-21], sum to 1.0000
[2019-04-04 11:19:27,742] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5069
[2019-04-04 11:19:27,802] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.06175902812878, 0.4644170265468352, 0.0, 1.0, 18706.2261540012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4477800.0000, 
sim time next is 4478400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.06249717222639, 0.4575665358630409, 1.0, 1.0, 23591.84888744531], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5885414310188658, 0.6525221786210137, 1.0, 1.0, 0.11234213755926338], 
reward next is 0.8877, 
noisyNet noise sample is [array([-0.61660945], dtype=float32), 1.0477527]. 
=============================================
[2019-04-04 11:19:30,854] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.6729118e-10 1.9433155e-09 1.4869001e-24 1.6169796e-23 5.4766977e-19
 1.0000000e+00 1.2041320e-18], sum to 1.0000
[2019-04-04 11:19:30,856] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6083
[2019-04-04 11:19:30,883] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 30.0, 0.0, 0.0, 26.0, 25.30282787078413, 0.4869596878968483, 0.0, 1.0, 169364.2736412829], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4048200.0000, 
sim time next is 4048800.0000, 
raw observation next is [-4.0, 29.66666666666666, 0.0, 0.0, 26.0, 25.34791845533588, 0.5092822256503513, 0.0, 1.0, 88507.20079971795], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.29666666666666663, 0.0, 0.0, 0.6666666666666666, 0.6123265379446566, 0.6697607418834505, 0.0, 1.0, 0.42146286095103785], 
reward next is 0.5785, 
noisyNet noise sample is [array([0.22162396], dtype=float32), -0.27981618]. 
=============================================
[2019-04-04 11:19:35,331] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:19:35,331] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:19:35,335] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run33
[2019-04-04 11:19:37,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:19:37,531] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:19:37,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run33
[2019-04-04 11:19:39,317] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2048073e-11 1.2901765e-09 3.5332045e-26 2.0971210e-24 3.7382450e-20
 1.0000000e+00 1.7889770e-19], sum to 1.0000
[2019-04-04 11:19:39,318] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0112
[2019-04-04 11:19:39,339] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 30.0, 118.5, 834.5, 26.0, 25.11174259382972, 0.3942277463157997, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4190400.0000, 
sim time next is 4191000.0000, 
raw observation next is [1.166666666666667, 30.66666666666666, 118.3333333333333, 838.6666666666667, 26.0, 25.14240252721122, 0.3964594335961941, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.49492151431209613, 0.3066666666666666, 0.3944444444444443, 0.9267034990791898, 0.6666666666666666, 0.5952002106009351, 0.6321531445320647, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36381882], dtype=float32), 1.7272643]. 
=============================================
[2019-04-04 11:19:39,355] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[79.27649]
 [79.26415]
 [79.09148]
 [79.06026]
 [78.98474]], R is [[79.48461151]
 [79.68976593]
 [79.78660583]
 [79.89676666]
 [80.05327606]].
[2019-04-04 11:19:40,912] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:19:40,912] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:19:40,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run33
[2019-04-04 11:19:43,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:19:43,074] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:19:43,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run33
[2019-04-04 11:19:44,600] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:19:44,601] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:19:44,606] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run33
[2019-04-04 11:19:47,703] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:19:47,703] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:19:47,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run33
[2019-04-04 11:19:55,361] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3241687e-11 3.0587152e-10 6.3248526e-29 2.4014437e-26 4.7745396e-22
 1.0000000e+00 4.4900069e-22], sum to 1.0000
[2019-04-04 11:19:55,361] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4482
[2019-04-04 11:19:55,429] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 75.0, 25.0, 55.0, 26.0, 24.74145233429975, 0.3840590767989109, 1.0, 1.0, 196385.5632187004], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4469400.0000, 
sim time next is 4470000.0000, 
raw observation next is [0.0, 74.0, 20.83333333333334, 45.83333333333334, 26.0, 24.84611154670926, 0.4688920475085362, 1.0, 1.0, 177134.3390227219], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.74, 0.06944444444444446, 0.050644567219152864, 0.6666666666666666, 0.570509295559105, 0.656297349169512, 1.0, 1.0, 0.8434968524891518], 
reward next is 0.1565, 
noisyNet noise sample is [array([-0.8130385], dtype=float32), -0.92210656]. 
=============================================
[2019-04-04 11:19:55,438] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[87.64435]
 [86.66292]
 [86.76926]
 [86.70265]
 [86.63233]], R is [[87.62746429]
 [86.81601715]
 [86.94786072]
 [87.0783844 ]
 [87.20760345]].
[2019-04-04 11:19:58,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:19:58,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:19:58,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run33
[2019-04-04 11:19:59,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:19:59,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:19:59,463] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run33
[2019-04-04 11:20:00,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:20:00,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:20:00,035] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run33
[2019-04-04 11:20:03,983] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6145404e-11 1.8222953e-09 1.6968481e-26 7.9860516e-25 1.0810736e-20
 1.0000000e+00 3.2821192e-20], sum to 1.0000
[2019-04-04 11:20:03,983] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7751
[2019-04-04 11:20:04,068] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.716666666666667, 62.16666666666666, 39.66666666666666, 6.000000000000001, 26.0, 25.373363911383, 0.2700325866177501, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 118200.0000, 
sim time next is 118800.0000, 
raw observation next is [-7.8, 61.0, 41.0, 4.5, 26.0, 25.37777015165775, 0.2564031532495906, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.61, 0.13666666666666666, 0.004972375690607734, 0.6666666666666666, 0.6148141793048124, 0.5854677177498635, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04566713], dtype=float32), -0.122711234]. 
=============================================
[2019-04-04 11:20:05,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:20:05,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:20:05,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run33
[2019-04-04 11:20:07,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:20:07,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:20:07,698] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run33
[2019-04-04 11:20:08,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:20:08,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:20:08,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run33
[2019-04-04 11:20:12,998] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.2987406e-12 5.6917058e-11 7.3229767e-29 5.5918983e-27 2.3076804e-22
 1.0000000e+00 7.9802209e-21], sum to 1.0000
[2019-04-04 11:20:12,999] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2789
[2019-04-04 11:20:13,023] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666667, 74.0, 40.83333333333333, 25.16666666666667, 26.0, 26.02938068735053, 0.4996726737835093, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4728000.0000, 
sim time next is 4728600.0000, 
raw observation next is [0.5, 75.0, 29.0, 28.0, 26.0, 25.86689946061792, 0.467951493049017, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.75, 0.09666666666666666, 0.030939226519337018, 0.6666666666666666, 0.6555749550514932, 0.655983831016339, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.79152435], dtype=float32), -2.0374112]. 
=============================================
[2019-04-04 11:20:23,496] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9148774e-09 6.1056395e-09 8.8527501e-24 1.7624150e-22 2.8640943e-18
 1.0000000e+00 3.1327686e-17], sum to 1.0000
[2019-04-04 11:20:23,498] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2243
[2019-04-04 11:20:23,521] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.13294418838927, 0.09929121173859172, 0.0, 1.0, 44962.67839370439], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 168600.0000, 
sim time next is 169200.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.08626046875003, 0.08904929802124084, 0.0, 1.0, 44914.4591407251], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5071883723958358, 0.5296830993404136, 0.0, 1.0, 0.21387837686059571], 
reward next is 0.7861, 
noisyNet noise sample is [array([-0.8396491], dtype=float32), -0.42952234]. 
=============================================
[2019-04-04 11:20:32,703] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.42670675e-09 3.20035305e-08 1.03344925e-22 9.56255321e-22
 1.08856960e-17 1.00000000e+00 6.48655052e-17], sum to 1.0000
[2019-04-04 11:20:32,704] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7239
[2019-04-04 11:20:32,800] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.54738901152137, -0.2980058871274366, 0.0, 1.0, 45023.61995836028], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 198600.0000, 
sim time next is 199200.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.4833243543006, -0.2223548512970458, 1.0, 1.0, 202343.6265148717], 
processed observation next is [1.0, 0.30434782608695654, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3736103628583833, 0.42588171623431803, 1.0, 1.0, 0.9635410786422461], 
reward next is 0.0365, 
noisyNet noise sample is [array([0.06870906], dtype=float32), 0.34744528]. 
=============================================
[2019-04-04 11:20:33,230] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1389948e-11 5.3722377e-10 3.1911167e-28 6.0087387e-26 8.2991928e-21
 1.0000000e+00 4.3414696e-21], sum to 1.0000
[2019-04-04 11:20:33,230] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2268
[2019-04-04 11:20:33,275] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.583333333333333, 62.5, 138.3333333333333, 0.0, 26.0, 25.84358939137391, 0.402306786289494, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 222600.0000, 
sim time next is 223200.0000, 
raw observation next is [-3.4, 62.0, 133.0, 0.0, 26.0, 26.04141918022083, 0.4182891444552633, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.368421052631579, 0.62, 0.44333333333333336, 0.0, 0.6666666666666666, 0.6701182650184027, 0.639429714818421, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05111304], dtype=float32), 0.60431147]. 
=============================================
[2019-04-04 11:20:34,671] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6945712e-11 4.4719661e-10 8.0051023e-28 3.1235659e-26 1.3739276e-21
 1.0000000e+00 2.7053942e-21], sum to 1.0000
[2019-04-04 11:20:34,672] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9484
[2019-04-04 11:20:34,725] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.966666666666667, 84.0, 0.0, 0.0, 26.0, 24.61228698035394, 0.1916546671522021, 0.0, 1.0, 40381.5434141131], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 535200.0000, 
sim time next is 535800.0000, 
raw observation next is [1.783333333333333, 84.5, 0.0, 0.0, 26.0, 24.58432731840342, 0.1867008508719897, 0.0, 1.0, 40464.9196910838], 
processed observation next is [0.0, 0.17391304347826086, 0.5120036934441367, 0.845, 0.0, 0.0, 0.6666666666666666, 0.5486939432002851, 0.5622336169573299, 0.0, 1.0, 0.19269009376706572], 
reward next is 0.8073, 
noisyNet noise sample is [array([0.80408233], dtype=float32), -0.15478154]. 
=============================================
[2019-04-04 11:20:39,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:20:39,874] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:20:39,887] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run33
[2019-04-04 11:20:40,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:20:40,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:20:40,008] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run33
[2019-04-04 11:20:44,012] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.417002e-12 5.221957e-11 1.830113e-30 5.839703e-28 2.564982e-23
 1.000000e+00 4.524480e-22], sum to 1.0000
[2019-04-04 11:20:44,012] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0690
[2019-04-04 11:20:44,054] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 24.66666666666667, 0.0, 0.0, 26.0, 26.44951793019192, 0.6998700036166247, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4990200.0000, 
sim time next is 4990800.0000, 
raw observation next is [6.0, 24.33333333333334, 0.0, 0.0, 26.0, 25.59247627288587, 0.6030075510698097, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6327063560738226, 0.7010025170232699, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3576212], dtype=float32), -0.93635786]. 
=============================================
[2019-04-04 11:20:45,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:20:45,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:20:45,104] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run33
[2019-04-04 11:20:52,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:20:52,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:20:52,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run33
[2019-04-04 11:20:57,833] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4025685e-10 3.1707842e-10 1.3661415e-27 1.1013290e-26 1.4405732e-20
 1.0000000e+00 1.8562498e-20], sum to 1.0000
[2019-04-04 11:20:57,833] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7753
[2019-04-04 11:20:57,879] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 95.66666666666667, 0.0, 0.0, 26.0, 24.87386079032729, 0.2379245941091981, 0.0, 1.0, 39750.0811302185], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 519000.0000, 
sim time next is 519600.0000, 
raw observation next is [4.2, 94.33333333333334, 0.0, 0.0, 26.0, 24.86380392905065, 0.2371564804152493, 0.0, 1.0, 39712.57051100291], 
processed observation next is [0.0, 0.0, 0.5789473684210527, 0.9433333333333335, 0.0, 0.0, 0.6666666666666666, 0.5719836607542209, 0.5790521601384164, 0.0, 1.0, 0.18910747862382338], 
reward next is 0.8109, 
noisyNet noise sample is [array([-0.2645573], dtype=float32), -1.0939531]. 
=============================================
[2019-04-04 11:21:05,691] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.0185168e-13 3.2763361e-11 1.9553224e-30 7.7026870e-29 4.0969049e-24
 1.0000000e+00 2.9298938e-23], sum to 1.0000
[2019-04-04 11:21:05,693] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3522
[2019-04-04 11:21:05,708] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 93.0, 48.0, 0.0, 26.0, 25.7881080539575, 0.4090477925161853, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 920400.0000, 
sim time next is 921000.0000, 
raw observation next is [4.4, 93.0, 42.00000000000001, 0.0, 26.0, 25.80081973589078, 0.4093380305903723, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5844875346260389, 0.93, 0.14, 0.0, 0.6666666666666666, 0.6500683113242317, 0.6364460101967908, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9928996], dtype=float32), 1.0203199]. 
=============================================
[2019-04-04 11:21:05,725] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[92.02099 ]
 [92.251274]
 [92.45829 ]
 [92.69411 ]
 [92.954796]], R is [[91.95792389]
 [92.03834534]
 [92.1179657 ]
 [92.19678497]
 [92.27481842]].
[2019-04-04 11:21:07,456] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0371022e-11 1.4676856e-10 9.8191697e-29 1.2638124e-26 1.4445456e-21
 1.0000000e+00 1.7597890e-21], sum to 1.0000
[2019-04-04 11:21:07,461] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3129
[2019-04-04 11:21:07,500] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 84.0, 49.0, 0.0, 26.0, 25.42519541811563, 0.3791701366839011, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 833400.0000, 
sim time next is 834000.0000, 
raw observation next is [-3.899999999999999, 83.33333333333334, 45.66666666666667, 0.0, 26.0, 25.76830698133907, 0.4094998772813001, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.35457063711911363, 0.8333333333333335, 0.15222222222222223, 0.0, 0.6666666666666666, 0.6473589151115892, 0.6364999590937667, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19352071], dtype=float32), -0.031596947]. 
=============================================
[2019-04-04 11:21:07,506] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.59251 ]
 [85.657295]
 [84.60342 ]
 [84.60217 ]
 [84.67333 ]], R is [[85.67902374]
 [85.82223511]
 [85.04411316]
 [85.08888245]
 [85.23799133]].
[2019-04-04 11:21:15,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1227397e-12 1.4144852e-11 1.4618965e-29 1.5915849e-28 1.1893606e-23
 1.0000000e+00 9.7788296e-23], sum to 1.0000
[2019-04-04 11:21:15,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7271
[2019-04-04 11:21:15,232] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.9, 70.66666666666666, 107.3333333333333, 52.16666666666666, 26.0, 25.95379680919671, 0.3406526970576313, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 726000.0000, 
sim time next is 726600.0000, 
raw observation next is [-1.8, 69.33333333333334, 113.6666666666667, 55.33333333333333, 26.0, 25.97619268134144, 0.3374777383847841, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.41274238227146814, 0.6933333333333335, 0.378888888888889, 0.06114180478821362, 0.6666666666666666, 0.6646827234451201, 0.6124925794615947, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3720881], dtype=float32), -0.7942966]. 
=============================================
[2019-04-04 11:21:23,849] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.8055231e-11 1.2128319e-09 5.4183472e-26 4.6374277e-24 1.1208764e-19
 1.0000000e+00 1.5279299e-19], sum to 1.0000
[2019-04-04 11:21:23,849] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3911
[2019-04-04 11:21:23,927] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.63333333333333, 67.66666666666667, 0.0, 0.0, 26.0, 25.17890626959665, 0.329312829066072, 1.0, 1.0, 88154.67692641674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 330000.0000, 
sim time next is 330600.0000, 
raw observation next is [-12.71666666666667, 68.83333333333333, 0.0, 0.0, 26.0, 25.17548648077241, 0.3336481615253, 1.0, 1.0, 78524.94347607948], 
processed observation next is [1.0, 0.8260869565217391, 0.1103416435826407, 0.6883333333333332, 0.0, 0.0, 0.6666666666666666, 0.5979572067310341, 0.6112160538417667, 1.0, 1.0, 0.37392830226704515], 
reward next is 0.6261, 
noisyNet noise sample is [array([1.1233655], dtype=float32), 0.57635427]. 
=============================================
[2019-04-04 11:21:24,190] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.4979376e-12 1.4411687e-10 5.5273539e-30 3.2197047e-28 1.3214538e-22
 1.0000000e+00 2.8872125e-22], sum to 1.0000
[2019-04-04 11:21:24,194] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3368
[2019-04-04 11:21:24,214] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.83333333333333, 49.33333333333334, 44.5, 0.0, 26.0, 27.80882887835012, 1.007307955631099, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1095600.0000, 
sim time next is 1096200.0000, 
raw observation next is [18.55, 49.5, 35.0, 0.0, 26.0, 27.8585004786326, 1.014135547135516, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.976454293628809, 0.495, 0.11666666666666667, 0.0, 0.6666666666666666, 0.8215417065527166, 0.8380451823785053, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.69435227], dtype=float32), -0.39659077]. 
=============================================
[2019-04-04 11:21:29,570] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.7039232e-10 1.6118804e-08 1.5692748e-25 1.1926955e-23 7.9249215e-20
 1.0000000e+00 4.7946418e-19], sum to 1.0000
[2019-04-04 11:21:29,572] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7718
[2019-04-04 11:21:29,586] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.41666666666667, 93.5, 0.0, 0.0, 26.0, 23.78208569258443, 0.2042341899506296, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1224600.0000, 
sim time next is 1225200.0000, 
raw observation next is [15.33333333333333, 94.0, 0.0, 0.0, 26.0, 23.75696728847519, 0.2010198322615512, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8873499538319484, 0.94, 0.0, 0.0, 0.6666666666666666, 0.4797472740395993, 0.5670066107538504, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7755834], dtype=float32), -0.17375627]. 
=============================================
[2019-04-04 11:21:31,817] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0617939e-12 2.4660815e-10 5.9247586e-31 4.3754379e-28 1.6422054e-23
 1.0000000e+00 1.3247294e-22], sum to 1.0000
[2019-04-04 11:21:31,818] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3924
[2019-04-04 11:21:31,844] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.33333333333333, 81.0, 0.0, 0.0, 26.0, 25.64418178748026, 0.613825425930015, 0.0, 1.0, 18727.70130938056], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1147200.0000, 
sim time next is 1147800.0000, 
raw observation next is [12.51666666666667, 80.5, 0.0, 0.0, 26.0, 25.65641263715488, 0.6139218740931556, 0.0, 1.0, 18727.07530330078], 
processed observation next is [0.0, 0.2608695652173913, 0.8093259464450602, 0.805, 0.0, 0.0, 0.6666666666666666, 0.6380343864295733, 0.7046406246977185, 0.0, 1.0, 0.08917654906333704], 
reward next is 0.9108, 
noisyNet noise sample is [array([2.102568], dtype=float32), -2.3347566]. 
=============================================
[2019-04-04 11:21:35,727] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.1346181e-14 1.0955168e-12 1.4189387e-33 1.7924772e-31 8.5222693e-27
 1.0000000e+00 1.0794107e-24], sum to 1.0000
[2019-04-04 11:21:35,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5342
[2019-04-04 11:21:35,740] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.95, 79.5, 0.0, 0.0, 26.0, 26.55537339159071, 0.6500191628964427, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1013400.0000, 
sim time next is 1014000.0000, 
raw observation next is [14.76666666666667, 80.0, 0.0, 0.0, 26.0, 25.86486321200488, 0.5870051415170545, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8716528162511544, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6554052676670734, 0.6956683805056848, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0454979], dtype=float32), 1.1245304]. 
=============================================
[2019-04-04 11:21:35,764] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[102.57178]
 [102.60307]
 [102.68621]
 [102.66017]
 [102.64839]], R is [[102.52645874]
 [102.50119781]
 [102.47618866]
 [102.45143127]
 [102.42691803]].
[2019-04-04 11:21:41,167] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.1468228e-13 3.1535573e-11 3.8741256e-31 5.9842309e-29 3.3619857e-23
 1.0000000e+00 1.1673406e-22], sum to 1.0000
[2019-04-04 11:21:41,167] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6265
[2019-04-04 11:21:41,215] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 81.0, 0.0, 26.0, 26.24436994495242, 0.4799594551701546, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1432800.0000, 
sim time next is 1433400.0000, 
raw observation next is [1.1, 92.0, 78.0, 0.0, 26.0, 25.70369721346077, 0.5094847271639827, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.26, 0.0, 0.6666666666666666, 0.6419747677883976, 0.6698282423879943, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.60006833], dtype=float32), -0.6157061]. 
=============================================
[2019-04-04 11:21:53,080] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.7895104e-12 1.2348077e-10 4.8032742e-30 7.7264992e-28 1.6198143e-22
 1.0000000e+00 5.9750142e-22], sum to 1.0000
[2019-04-04 11:21:53,081] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2572
[2019-04-04 11:21:53,096] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.7, 98.66666666666666, 0.0, 0.0, 26.0, 25.50047278851907, 0.6086856166769475, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1284000.0000, 
sim time next is 1284600.0000, 
raw observation next is [5.600000000000001, 99.33333333333334, 0.0, 0.0, 26.0, 25.52826701024134, 0.602905717538565, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.6177285318559558, 0.9933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6273555841867783, 0.700968572512855, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6340985], dtype=float32), -0.09058172]. 
=============================================
[2019-04-04 11:21:53,649] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3583769e-12 7.3692122e-11 5.8360895e-30 4.5315480e-28 6.1865640e-23
 1.0000000e+00 5.9431892e-22], sum to 1.0000
[2019-04-04 11:21:53,649] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8976
[2019-04-04 11:21:53,692] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 22.66666666666666, 0.0, 26.0, 25.73793930900343, 0.4959952201928673, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1413600.0000, 
sim time next is 1414200.0000, 
raw observation next is [-0.6, 100.0, 27.33333333333333, 0.0, 26.0, 25.82972112315166, 0.5077200513711831, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.0911111111111111, 0.0, 0.6666666666666666, 0.6524767602626383, 0.6692400171237277, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.25546244], dtype=float32), 1.4823347]. 
=============================================
[2019-04-04 11:21:56,245] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6208001e-12 4.2409375e-11 2.7972611e-30 1.1522418e-28 1.7759455e-23
 1.0000000e+00 8.9005630e-24], sum to 1.0000
[2019-04-04 11:21:56,257] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0043
[2019-04-04 11:21:56,264] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 122.3333333333333, 0.0, 26.0, 26.05895303016255, 0.5791894115800746, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1338600.0000, 
sim time next is 1339200.0000, 
raw observation next is [1.1, 92.0, 120.0, 0.0, 26.0, 26.048639064212, 0.575460476070961, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.4, 0.0, 0.6666666666666666, 0.6707199220176667, 0.6918201586903203, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2772089], dtype=float32), -0.3996846]. 
=============================================
[2019-04-04 11:22:02,213] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6681656e-11 1.3304299e-10 1.0200666e-28 2.1052053e-27 1.2530618e-22
 1.0000000e+00 3.6583653e-22], sum to 1.0000
[2019-04-04 11:22:02,214] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7292
[2019-04-04 11:22:02,290] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 36.66666666666667, 0.0, 26.0, 25.39024002057411, 0.4281415773321284, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1439400.0000, 
sim time next is 1440000.0000, 
raw observation next is [1.1, 92.0, 32.0, 0.0, 26.0, 24.68832477527604, 0.4062036407325927, 1.0, 1.0, 196601.4990281954], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.10666666666666667, 0.0, 0.6666666666666666, 0.5573603979396701, 0.6354012135775309, 1.0, 1.0, 0.9361976144199781], 
reward next is 0.0638, 
noisyNet noise sample is [array([0.48997903], dtype=float32), 0.8493099]. 
=============================================
[2019-04-04 11:22:02,300] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[87.278564]
 [87.500786]
 [87.625496]
 [87.789986]
 [87.90418 ]], R is [[87.41639709]
 [87.54223633]
 [87.66681671]
 [87.79014587]
 [87.9122467 ]].
[2019-04-04 11:22:06,138] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.40397480e-10 1.22419515e-08 1.85833517e-25 2.11769410e-23
 1.72312216e-19 1.00000000e+00 5.15108684e-19], sum to 1.0000
[2019-04-04 11:22:06,138] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5145
[2019-04-04 11:22:06,190] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8, 73.33333333333334, 0.0, 0.0, 26.0, 24.60567493492772, 0.1680386179949351, 0.0, 1.0, 39038.54867455531], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 880800.0000, 
sim time next is 881400.0000, 
raw observation next is [-0.7, 72.66666666666666, 0.0, 0.0, 26.0, 24.54763094769162, 0.1655216159606005, 0.0, 1.0, 39041.28548773803], 
processed observation next is [1.0, 0.17391304347826086, 0.443213296398892, 0.7266666666666666, 0.0, 0.0, 0.6666666666666666, 0.545635912307635, 0.5551738719868669, 0.0, 1.0, 0.185910883274943], 
reward next is 0.8141, 
noisyNet noise sample is [array([0.24992134], dtype=float32), 0.22148557]. 
=============================================
[2019-04-04 11:22:10,073] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8143112e-10 3.3127918e-09 1.7598333e-24 7.1098885e-23 5.3257620e-19
 1.0000000e+00 1.9318539e-18], sum to 1.0000
[2019-04-04 11:22:10,077] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4527
[2019-04-04 11:22:10,157] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 80.33333333333334, 91.0, 14.0, 26.0, 25.06343793188842, 0.2678158673928557, 0.0, 1.0, 38997.46135465421], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1869600.0000, 
sim time next is 1870200.0000, 
raw observation next is [-4.5, 79.0, 72.0, 0.0, 26.0, 25.0439242619696, 0.2652103950132584, 0.0, 1.0, 51357.32033449913], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.79, 0.24, 0.0, 0.6666666666666666, 0.5869936884974667, 0.5884034650044194, 0.0, 1.0, 0.24455866825951966], 
reward next is 0.7554, 
noisyNet noise sample is [array([0.9538733], dtype=float32), -0.65876096]. 
=============================================
[2019-04-04 11:22:10,217] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2034210e-13 9.2811635e-12 1.7565934e-32 9.6995079e-31 6.2786859e-25
 1.0000000e+00 5.7151545e-24], sum to 1.0000
[2019-04-04 11:22:10,223] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3253
[2019-04-04 11:22:10,236] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 79.66666666666667, 0.0, 0.0, 26.0, 25.49085621194359, 0.4781583396824387, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1020000.0000, 
sim time next is 1020600.0000, 
raw observation next is [14.4, 79.0, 0.0, 0.0, 26.0, 25.33872862070843, 0.4565958604569877, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6115607183923691, 0.6521986201523292, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.38892463], dtype=float32), -0.3982704]. 
=============================================
[2019-04-04 11:22:16,231] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1798067e-10 1.3557915e-09 1.9135967e-27 5.7051721e-26 9.4138677e-22
 1.0000000e+00 6.7607224e-21], sum to 1.0000
[2019-04-04 11:22:16,232] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6611
[2019-04-04 11:22:16,310] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.199999999999999, 68.33333333333333, 231.1666666666667, 9.0, 26.0, 25.63237955723024, 0.3084926465638654, 1.0, 1.0, 90610.89497292304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1942800.0000, 
sim time next is 1943400.0000, 
raw observation next is [-5.1, 66.66666666666667, 230.3333333333333, 8.0, 26.0, 25.63711891563104, 0.3284381995988674, 1.0, 1.0, 60062.03207506326], 
processed observation next is [1.0, 0.4782608695652174, 0.3213296398891967, 0.6666666666666667, 0.7677777777777777, 0.008839779005524863, 0.6666666666666666, 0.6364265763025866, 0.6094793998662892, 1.0, 1.0, 0.2860096765479203], 
reward next is 0.7140, 
noisyNet noise sample is [array([1.9573078], dtype=float32), 0.9265704]. 
=============================================
[2019-04-04 11:22:18,941] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5574844e-11 5.9996264e-10 7.1786057e-27 2.5455235e-25 3.8032440e-21
 1.0000000e+00 1.0993554e-20], sum to 1.0000
[2019-04-04 11:22:18,941] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6016
[2019-04-04 11:22:19,011] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 62.0, 52.0, 0.0, 26.0, 25.55532408247431, 0.3160022003522833, 1.0, 1.0, 26354.08982037136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1958400.0000, 
sim time next is 1959000.0000, 
raw observation next is [-2.983333333333333, 64.16666666666667, 44.66666666666666, 0.0, 26.0, 25.53946175851087, 0.3109184634532439, 1.0, 1.0, 30873.87871335168], 
processed observation next is [1.0, 0.6956521739130435, 0.37996306555863346, 0.6416666666666667, 0.14888888888888885, 0.0, 0.6666666666666666, 0.628288479875906, 0.6036394878177479, 1.0, 1.0, 0.14701847006357943], 
reward next is 0.8530, 
noisyNet noise sample is [array([-0.5409925], dtype=float32), 0.43632174]. 
=============================================
[2019-04-04 11:22:19,016] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.79624 ]
 [79.68133 ]
 [79.53248 ]
 [79.368645]
 [79.15071 ]], R is [[79.93799591]
 [80.01312256]
 [80.0842514 ]
 [80.14358521]
 [80.16933441]].
[2019-04-04 11:22:20,551] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.8498996e-12 1.4198975e-10 2.0915473e-29 1.1645490e-27 5.0346703e-23
 1.0000000e+00 4.7669867e-22], sum to 1.0000
[2019-04-04 11:22:20,552] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5134
[2019-04-04 11:22:20,584] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 84.0, 95.0, 0.0, 26.0, 25.86053518993326, 0.5222310383418596, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1684800.0000, 
sim time next is 1685400.0000, 
raw observation next is [1.1, 84.66666666666667, 99.0, 0.0, 26.0, 25.85255911825676, 0.5212899245428887, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.8466666666666667, 0.33, 0.0, 0.6666666666666666, 0.6543799265213966, 0.673763308180963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0715538], dtype=float32), -0.43623176]. 
=============================================
[2019-04-04 11:22:40,702] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-04 11:22:40,703] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 11:22:40,704] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:22:40,706] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run45
[2019-04-04 11:22:40,745] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 11:22:40,746] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:22:40,748] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run45
[2019-04-04 11:22:40,780] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 11:22:40,781] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:22:40,785] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run45
[2019-04-04 11:25:24,667] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.27212265], dtype=float32), 0.32801816]
[2019-04-04 11:25:24,668] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-0.08333333333333329, 60.83333333333333, 75.33333333333333, 559.0, 26.0, 25.33069659526994, 0.3994155227634204, 1.0, 1.0, 0.0]
[2019-04-04 11:25:24,668] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 11:25:24,670] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.2758247e-12 2.0733679e-10 3.6034194e-30 5.2726338e-28 4.8743371e-23
 1.0000000e+00 2.2785691e-22], sampled 0.821289888118269
[2019-04-04 11:25:41,112] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.27212265], dtype=float32), 0.32801816]
[2019-04-04 11:25:41,113] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [9.350000000000001, 43.0, 0.0, 0.0, 26.0, 25.74268921438362, 0.6548463787705086, 0.0, 1.0, 0.0]
[2019-04-04 11:25:41,113] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 11:25:41,115] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.4772328e-12 5.0870891e-10 2.8807886e-29 2.4054676e-27 2.2638748e-22
 1.0000000e+00 8.3850775e-22], sampled 0.6254846217977533
[2019-04-04 11:25:56,495] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 11:26:20,186] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.27212265], dtype=float32), 0.32801816]
[2019-04-04 11:26:20,186] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.0, 39.33333333333334, 0.0, 0.0, 26.0, 25.47633888491412, 0.4026847363193207, 0.0, 1.0, 0.0]
[2019-04-04 11:26:20,186] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 11:26:20,187] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.2784381e-10 1.8809554e-09 1.0084148e-25 5.1974779e-24 1.4894187e-19
 1.0000000e+00 5.9916166e-19], sampled 0.22809025468052024
[2019-04-04 11:26:25,313] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 11:26:32,096] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7307 275786560.0346 1233.1784
[2019-04-04 11:26:33,135] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 4400000, evaluation results [4400000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.730666501954, 275786560.0345853, 1233.1784155230887]
[2019-04-04 11:26:44,195] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6199625e-10 5.8665535e-09 2.2043550e-24 5.9265006e-23 4.8050787e-19
 1.0000000e+00 2.6126766e-18], sum to 1.0000
[2019-04-04 11:26:44,195] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6674
[2019-04-04 11:26:44,211] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.55, 84.0, 0.0, 0.0, 26.0, 24.28488139358335, 0.1386221362243832, 0.0, 1.0, 43990.70731239899], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2255400.0000, 
sim time next is 2256000.0000, 
raw observation next is [-7.633333333333333, 84.66666666666666, 0.0, 0.0, 26.0, 24.34793433194572, 0.1185629239413342, 0.0, 1.0, 44097.4823603585], 
processed observation next is [1.0, 0.08695652173913043, 0.2511542012927055, 0.8466666666666666, 0.0, 0.0, 0.6666666666666666, 0.5289945276621433, 0.5395209746471114, 0.0, 1.0, 0.2099880112398024], 
reward next is 0.7900, 
noisyNet noise sample is [array([-0.61023057], dtype=float32), -1.0048274]. 
=============================================
[2019-04-04 11:26:44,217] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[74.14441 ]
 [74.127594]
 [74.2087  ]
 [74.217926]
 [74.29173 ]], R is [[74.15506744]
 [74.20404053]
 [74.25108337]
 [74.29910278]
 [74.34664917]].
[2019-04-04 11:26:52,757] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2440025e-09 4.1013791e-08 4.1172568e-24 1.2774400e-22 1.3514890e-18
 1.0000000e+00 6.8924196e-18], sum to 1.0000
[2019-04-04 11:26:52,767] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9768
[2019-04-04 11:26:52,780] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.1, 86.5, 0.0, 0.0, 26.0, 24.1736016960221, 0.0845978233380756, 0.0, 1.0, 43643.5409863981], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2259000.0000, 
sim time next is 2259600.0000, 
raw observation next is [-8.2, 86.66666666666667, 0.0, 0.0, 26.0, 24.09607275936872, 0.07463405150591912, 0.0, 1.0, 43646.29879314041], 
processed observation next is [1.0, 0.13043478260869565, 0.23545706371191139, 0.8666666666666667, 0.0, 0.0, 0.6666666666666666, 0.5080060632807267, 0.5248780171686397, 0.0, 1.0, 0.2078395180625734], 
reward next is 0.7922, 
noisyNet noise sample is [array([1.7023808], dtype=float32), -0.5602077]. 
=============================================
[2019-04-04 11:26:58,760] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1752878e-10 2.5961764e-09 1.1419827e-24 4.8187625e-23 1.1854857e-19
 1.0000000e+00 2.2587296e-18], sum to 1.0000
[2019-04-04 11:26:58,760] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7875
[2019-04-04 11:26:58,828] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.6451222175307, 0.2108474121950538, 0.0, 1.0, 39421.29274448982], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2343000.0000, 
sim time next is 2343600.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.60546978435909, 0.2032494041091371, 0.0, 1.0, 39533.57890769543], 
processed observation next is [0.0, 0.13043478260869565, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5504558153632576, 0.5677498013697123, 0.0, 1.0, 0.18825513765569254], 
reward next is 0.8117, 
noisyNet noise sample is [array([2.1420882], dtype=float32), -0.5112755]. 
=============================================
[2019-04-04 11:27:05,493] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9297832e-11 4.5248874e-10 1.7624306e-27 1.4148313e-25 4.6224929e-21
 1.0000000e+00 1.3691201e-20], sum to 1.0000
[2019-04-04 11:27:05,493] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2994
[2019-04-04 11:27:05,566] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 72.0, 0.0, 0.0, 26.0, 26.08795524618441, 0.4952905603834183, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2139600.0000, 
sim time next is 2140200.0000, 
raw observation next is [-5.0, 72.5, 0.0, 0.0, 26.0, 25.91512715509437, 0.4684587866086209, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.725, 0.0, 0.0, 0.6666666666666666, 0.6595939295911976, 0.6561529288695404, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1540446], dtype=float32), 0.70712405]. 
=============================================
[2019-04-04 11:27:19,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.2384098e-10 5.7381797e-09 1.8998299e-25 6.3222875e-24 1.7136761e-19
 1.0000000e+00 4.3864039e-19], sum to 1.0000
[2019-04-04 11:27:19,557] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1237
[2019-04-04 11:27:19,566] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.05, 69.0, 0.0, 0.0, 26.0, 25.2205773731791, 0.3592662584155696, 0.0, 1.0, 51160.90417837909], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2673000.0000, 
sim time next is 2673600.0000, 
raw observation next is [-4.366666666666666, 69.0, 0.0, 0.0, 26.0, 25.11251687038878, 0.3445563361379402, 0.0, 1.0, 46700.46233575704], 
processed observation next is [1.0, 0.9565217391304348, 0.34164358264081257, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5927097391990651, 0.6148521120459801, 0.0, 1.0, 0.2223831539797954], 
reward next is 0.7776, 
noisyNet noise sample is [array([2.672392], dtype=float32), -0.13037558]. 
=============================================
[2019-04-04 11:27:23,869] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0905126e-12 3.6107559e-10 2.2737084e-28 6.9734734e-27 3.5168469e-22
 1.0000000e+00 5.9181673e-22], sum to 1.0000
[2019-04-04 11:27:23,882] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8659
[2019-04-04 11:27:23,892] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.3, 29.0, 129.0, 325.6666666666667, 26.0, 25.76516898859947, 0.3884350091757718, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2560200.0000, 
sim time next is 2560800.0000, 
raw observation next is [3.3, 29.0, 121.5, 338.3333333333333, 26.0, 25.81912827399764, 0.3897185795513065, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.554016620498615, 0.29, 0.405, 0.3738489871086556, 0.6666666666666666, 0.6515940228331368, 0.6299061931837688, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19181183], dtype=float32), 1.0795985]. 
=============================================
[2019-04-04 11:27:27,340] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.2544825e-10 1.0088852e-08 3.2013755e-25 9.5994575e-24 1.7159853e-19
 1.0000000e+00 6.0388858e-19], sum to 1.0000
[2019-04-04 11:27:27,342] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0156
[2019-04-04 11:27:27,389] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.83358049413074, 0.2654516646944937, 0.0, 1.0, 38530.42486251141], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2337000.0000, 
sim time next is 2337600.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.81583128456752, 0.2568578157259589, 0.0, 1.0, 38572.73113414392], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5679859403806266, 0.5856192719086529, 0.0, 1.0, 0.183679672067352], 
reward next is 0.8163, 
noisyNet noise sample is [array([0.26673266], dtype=float32), 0.90874153]. 
=============================================
[2019-04-04 11:27:29,638] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.1851983e-11 1.2895220e-09 9.1959480e-27 5.5629258e-25 4.0571610e-20
 1.0000000e+00 1.3589223e-19], sum to 1.0000
[2019-04-04 11:27:29,639] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1343
[2019-04-04 11:27:29,711] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.2, 67.66666666666667, 107.0, 300.0, 26.0, 25.30114488438176, 0.319978095239228, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2366400.0000, 
sim time next is 2367000.0000, 
raw observation next is [-3.1, 67.0, 121.0, 360.0, 26.0, 25.26066344426867, 0.3198570493999373, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.37673130193905824, 0.67, 0.4033333333333333, 0.39779005524861877, 0.6666666666666666, 0.6050552870223891, 0.6066190164666457, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09196513], dtype=float32), 0.50679094]. 
=============================================
[2019-04-04 11:27:29,742] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[80.47024]
 [79.70385]
 [78.92029]
 [78.2266 ]
 [77.67146]], R is [[81.33132935]
 [81.51802063]
 [81.70284271]
 [81.88581848]
 [82.0669632 ]].
[2019-04-04 11:27:32,385] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.5034851e-09 9.7093931e-08 6.5710141e-21 3.4086740e-20 1.3952319e-16
 9.9999988e-01 3.4274566e-16], sum to 1.0000
[2019-04-04 11:27:32,386] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7625
[2019-04-04 11:27:32,403] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.7802822135417, -0.1963134636433009, 0.0, 1.0, 43241.98676737951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2703600.0000, 
sim time next is 2704200.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.69845057493624, -0.2064646904075822, 0.0, 1.0, 43299.03822466784], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.6666666666666666, 0.3915375479113532, 0.43117843653080595, 0.0, 1.0, 0.2061858963079421], 
reward next is 0.7938, 
noisyNet noise sample is [array([-1.2057447], dtype=float32), 0.9324846]. 
=============================================
[2019-04-04 11:27:49,120] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3811196e-12 5.0931181e-10 1.2898547e-27 4.0030343e-26 1.0932449e-21
 1.0000000e+00 4.2387647e-21], sum to 1.0000
[2019-04-04 11:27:49,122] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9438
[2019-04-04 11:27:49,152] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.166666666666667, 98.83333333333334, 0.0, 0.0, 26.0, 24.82343801853745, 0.2270150480086178, 0.0, 1.0, 55439.05687490878], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2873400.0000, 
sim time next is 2874000.0000, 
raw observation next is [1.333333333333333, 97.66666666666667, 0.0, 0.0, 26.0, 24.77493697744571, 0.2199079602462973, 0.0, 1.0, 55431.75990371163], 
processed observation next is [1.0, 0.2608695652173913, 0.4995383194829178, 0.9766666666666667, 0.0, 0.0, 0.6666666666666666, 0.564578081453809, 0.5733026534154324, 0.0, 1.0, 0.26396076144624586], 
reward next is 0.7360, 
noisyNet noise sample is [array([1.0833833], dtype=float32), 0.33704546]. 
=============================================
[2019-04-04 11:27:49,197] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.07488 ]
 [85.04864 ]
 [85.01113 ]
 [84.965416]
 [84.919655]], R is [[84.98406219]
 [84.87023163]
 [84.75765991]
 [84.64480591]
 [84.53204346]].
[2019-04-04 11:28:05,253] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1433247e-10 4.3932397e-10 5.0741860e-27 2.6138948e-24 4.9063194e-20
 1.0000000e+00 1.0975960e-19], sum to 1.0000
[2019-04-04 11:28:05,255] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1760
[2019-04-04 11:28:05,278] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.40205022673267, 0.3440329288001454, 0.0, 1.0, 33692.6171179601], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3102000.0000, 
sim time next is 3102600.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.37144233290747, 0.343703597766754, 0.0, 1.0, 53067.47752237694], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6142868610756226, 0.6145678659222513, 0.0, 1.0, 0.2527022739160807], 
reward next is 0.7473, 
noisyNet noise sample is [array([-0.8217742], dtype=float32), 1.3062495]. 
=============================================
[2019-04-04 11:28:09,691] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0982795e-10 5.9534366e-10 7.7572804e-26 8.9515991e-25 3.3089464e-20
 1.0000000e+00 2.2732764e-19], sum to 1.0000
[2019-04-04 11:28:09,694] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7883
[2019-04-04 11:28:09,735] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 69.0, 0.0, 0.0, 26.0, 25.17153057552495, 0.4084202566652033, 0.0, 1.0, 61583.06842120261], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2234400.0000, 
sim time next is 2235000.0000, 
raw observation next is [-5.0, 68.5, 0.0, 0.0, 26.0, 25.21487650436499, 0.4107252625219421, 0.0, 1.0, 50524.99562805834], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.685, 0.0, 0.0, 0.6666666666666666, 0.6012397086970825, 0.6369084208406474, 0.0, 1.0, 0.2405952172764683], 
reward next is 0.7594, 
noisyNet noise sample is [array([-1.7823553], dtype=float32), -1.3155929]. 
=============================================
[2019-04-04 11:28:09,743] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.44721 ]
 [78.186676]
 [79.18947 ]
 [80.12594 ]
 [82.35881 ]], R is [[76.74913788]
 [76.68839264]
 [76.48786163]
 [76.04432678]
 [76.28388214]].
[2019-04-04 11:28:31,375] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.33731762e-12 1.04160874e-10 7.00663786e-29 4.18689235e-27
 3.65136627e-22 1.00000000e+00 3.20271348e-21], sum to 1.0000
[2019-04-04 11:28:31,376] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5608
[2019-04-04 11:28:31,393] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 54.0, 118.0, 811.0, 26.0, 26.24678252315418, 0.5773457359469171, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3328200.0000, 
sim time next is 3328800.0000, 
raw observation next is [-5.333333333333333, 54.0, 117.3333333333333, 809.1666666666667, 26.0, 26.11295285546427, 0.5635301399598469, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3148661126500462, 0.54, 0.391111111111111, 0.8941068139963169, 0.6666666666666666, 0.6760794046220223, 0.6878433799866156, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17170352], dtype=float32), -1.7158922]. 
=============================================
[2019-04-04 11:28:42,474] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2769341e-12 8.8004173e-11 2.4390817e-28 1.1964714e-26 5.2266910e-22
 1.0000000e+00 3.1877198e-22], sum to 1.0000
[2019-04-04 11:28:42,478] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6040
[2019-04-04 11:28:42,539] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 61.0, 144.6666666666667, 228.6666666666667, 26.0, 25.85411381444265, 0.3890323068528931, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2798400.0000, 
sim time next is 2799000.0000, 
raw observation next is [-4.5, 59.5, 152.0, 233.0, 26.0, 25.87468273285439, 0.3960684279679203, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3379501385041552, 0.595, 0.5066666666666667, 0.2574585635359116, 0.6666666666666666, 0.6562235610711991, 0.6320228093226401, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8404298], dtype=float32), -1.7801336]. 
=============================================
[2019-04-04 11:28:42,549] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[85.8455  ]
 [86.12484 ]
 [86.44192 ]
 [86.73026 ]
 [86.882706]], R is [[85.77235413]
 [85.9146347 ]
 [86.05548859]
 [86.19493103]
 [86.33298492]].
[2019-04-04 11:28:43,080] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7405777e-10 8.6366936e-10 4.3347069e-26 2.2754397e-24 1.3213191e-20
 1.0000000e+00 3.9340927e-19], sum to 1.0000
[2019-04-04 11:28:43,081] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9602
[2019-04-04 11:28:43,098] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.166666666666667, 72.0, 0.0, 0.0, 26.0, 25.25938388240499, 0.412472435389551, 0.0, 1.0, 43740.93417869663], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3802200.0000, 
sim time next is 3802800.0000, 
raw observation next is [-3.333333333333333, 73.0, 0.0, 0.0, 26.0, 25.23497691360583, 0.4072991511857142, 0.0, 1.0, 43790.7245562101], 
processed observation next is [1.0, 0.0, 0.37026777469990774, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6029147428004858, 0.6357663837285714, 0.0, 1.0, 0.20852725979147668], 
reward next is 0.7915, 
noisyNet noise sample is [array([0.8463372], dtype=float32), -1.1820433]. 
=============================================
[2019-04-04 11:28:43,499] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1747658e-10 3.2344960e-09 3.9049826e-26 1.7815017e-24 4.2519491e-20
 1.0000000e+00 1.2408769e-19], sum to 1.0000
[2019-04-04 11:28:43,500] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5586
[2019-04-04 11:28:43,513] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.333333333333334, 65.0, 0.0, 0.0, 26.0, 25.06203077331512, 0.313472700635249, 0.0, 1.0, 41412.17350021089], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3908400.0000, 
sim time next is 3909000.0000, 
raw observation next is [-5.666666666666666, 62.0, 0.0, 0.0, 26.0, 25.00477269481232, 0.2996106019262002, 0.0, 1.0, 41580.87458622423], 
processed observation next is [1.0, 0.21739130434782608, 0.3056325023084026, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5837310579010268, 0.5998702006420668, 0.0, 1.0, 0.19800416469630586], 
reward next is 0.8020, 
noisyNet noise sample is [array([0.19948581], dtype=float32), 1.0165211]. 
=============================================
[2019-04-04 11:28:43,522] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.28224 ]
 [78.56684 ]
 [78.776184]
 [78.930214]
 [79.06877 ]], R is [[77.98774719]
 [78.01066589]
 [78.03418732]
 [78.05815125]
 [78.08229065]].
[2019-04-04 11:28:53,899] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.2596044e-10 6.9526029e-10 1.9333499e-26 9.1983785e-25 2.0483969e-20
 1.0000000e+00 2.9122477e-20], sum to 1.0000
[2019-04-04 11:28:53,899] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4885
[2019-04-04 11:28:53,917] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.333333333333333, 64.0, 0.0, 0.0, 26.0, 25.34009577773127, 0.4811048179392997, 0.0, 1.0, 44480.6189971811], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3540000.0000, 
sim time next is 3540600.0000, 
raw observation next is [-1.5, 63.0, 0.0, 0.0, 26.0, 25.37034572430062, 0.479889004063579, 0.0, 1.0, 43547.41441090048], 
processed observation next is [1.0, 1.0, 0.4210526315789474, 0.63, 0.0, 0.0, 0.6666666666666666, 0.6141954770250516, 0.6599630013545263, 0.0, 1.0, 0.20736864005190703], 
reward next is 0.7926, 
noisyNet noise sample is [array([-1.1630982], dtype=float32), 0.2385737]. 
=============================================
[2019-04-04 11:28:56,174] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.01003800e-09 1.30387745e-08 9.77423747e-24 1.70761779e-22
 3.74065962e-18 1.00000000e+00 1.30834066e-17], sum to 1.0000
[2019-04-04 11:28:56,174] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6845
[2019-04-04 11:28:56,218] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 66.16666666666667, 28.33333333333333, 166.3333333333333, 26.0, 23.57482058342619, -0.03758021454588736, 0.0, 1.0, 40758.50613142909], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3052200.0000, 
sim time next is 3052800.0000, 
raw observation next is [-6.0, 64.0, 42.0, 214.5, 26.0, 23.5599079640424, -0.02591098440141355, 0.0, 1.0, 40645.80537820391], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.14, 0.23701657458563535, 0.6666666666666666, 0.4633256636701999, 0.4913630051995288, 0.0, 1.0, 0.1935514541819234], 
reward next is 0.8064, 
noisyNet noise sample is [array([-0.502032], dtype=float32), 0.5471301]. 
=============================================
[2019-04-04 11:28:58,197] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2050762e-11 1.7056165e-10 1.5554701e-27 9.6426813e-26 6.2068992e-21
 1.0000000e+00 2.9388758e-20], sum to 1.0000
[2019-04-04 11:28:58,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2768
[2019-04-04 11:28:58,215] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 34.0, 0.0, 0.0, 26.0, 26.45828893681026, 0.6533698313930195, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4125600.0000, 
sim time next is 4126200.0000, 
raw observation next is [3.0, 34.5, 0.0, 0.0, 26.0, 26.50092248027403, 0.6573329830919717, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.345, 0.0, 0.0, 0.6666666666666666, 0.7084102066895026, 0.7191109943639905, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24975017], dtype=float32), -0.32928875]. 
=============================================
[2019-04-04 11:29:00,823] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.2842080e-10 4.5146997e-09 4.7247251e-25 5.1099765e-23 2.1621629e-18
 1.0000000e+00 1.6586543e-18], sum to 1.0000
[2019-04-04 11:29:00,826] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5998
[2019-04-04 11:29:00,841] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93795231173934, 0.2698984712044595, 0.0, 1.0, 38075.94099414045], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3022200.0000, 
sim time next is 3022800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.91041726148305, 0.2614990530043943, 0.0, 1.0, 38017.31901579564], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5758681051235875, 0.5871663510014647, 0.0, 1.0, 0.1810348524561697], 
reward next is 0.8190, 
noisyNet noise sample is [array([-1.945335], dtype=float32), 0.58603865]. 
=============================================
[2019-04-04 11:29:03,811] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.33807082e-12 1.10988274e-10 1.24012758e-29 3.49946551e-28
 1.05351068e-22 1.00000000e+00 4.97501125e-22], sum to 1.0000
[2019-04-04 11:29:03,813] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9584
[2019-04-04 11:29:03,882] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 67.33333333333334, 99.33333333333333, 641.1666666666667, 26.0, 26.16052108004094, 0.5356941577802207, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3835200.0000, 
sim time next is 3835800.0000, 
raw observation next is [-3.0, 65.5, 101.0, 680.0, 26.0, 26.23515037954987, 0.5550730775399934, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.655, 0.33666666666666667, 0.7513812154696132, 0.6666666666666666, 0.686262531629156, 0.6850243591799977, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40520507], dtype=float32), 1.1239725]. 
=============================================
[2019-04-04 11:29:04,484] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.4847281e-12 2.0349414e-10 5.1324323e-27 6.1697976e-26 2.4126706e-21
 1.0000000e+00 3.1214418e-21], sum to 1.0000
[2019-04-04 11:29:04,488] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6564
[2019-04-04 11:29:04,531] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.33333333333333, 50.0, 99.66666666666667, 655.6666666666667, 26.0, 26.42914941968986, 0.533546210770827, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4008000.0000, 
sim time next is 4008600.0000, 
raw observation next is [-10.0, 48.5, 101.0, 698.0, 26.0, 26.49120392159066, 0.5455615340710921, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.18559556786703602, 0.485, 0.33666666666666667, 0.7712707182320442, 0.6666666666666666, 0.7076003267992217, 0.681853844690364, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18120019], dtype=float32), 0.44343153]. 
=============================================
[2019-04-04 11:29:09,966] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9393350e-11 6.4215400e-10 5.1872889e-27 1.8254810e-25 5.4322894e-21
 1.0000000e+00 1.3343724e-20], sum to 1.0000
[2019-04-04 11:29:09,971] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1257
[2019-04-04 11:29:10,014] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 38.33333333333334, 0.0, 0.0, 26.0, 25.34893530513629, 0.5398815339567836, 0.0, 1.0, 183988.4949726874], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4135200.0000, 
sim time next is 4135800.0000, 
raw observation next is [1.0, 37.16666666666666, 0.0, 0.0, 26.0, 25.46941667626601, 0.5698821604222565, 0.0, 1.0, 49009.22146215361], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.3716666666666666, 0.0, 0.0, 0.6666666666666666, 0.6224513896888343, 0.6899607201407522, 0.0, 1.0, 0.23337724505787433], 
reward next is 0.7666, 
noisyNet noise sample is [array([-0.45614645], dtype=float32), 0.38558537]. 
=============================================
[2019-04-04 11:29:12,518] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.2409782e-10 6.7474781e-10 1.0768720e-26 1.9860143e-24 6.6012929e-20
 1.0000000e+00 1.5020799e-19], sum to 1.0000
[2019-04-04 11:29:12,518] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4886
[2019-04-04 11:29:12,548] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 97.33333333333333, 0.0, 0.0, 26.0, 25.41118017747926, 0.3436850024097794, 0.0, 1.0, 70377.84756188584], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3098400.0000, 
sim time next is 3099000.0000, 
raw observation next is [-1.0, 98.66666666666667, 0.0, 0.0, 26.0, 25.38806933190931, 0.3437842159676589, 0.0, 1.0, 66698.93486652574], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9866666666666667, 0.0, 0.0, 0.6666666666666666, 0.6156724443257758, 0.6145947386558863, 0.0, 1.0, 0.31761397555488446], 
reward next is 0.6824, 
noisyNet noise sample is [array([0.37732136], dtype=float32), 0.97701436]. 
=============================================
[2019-04-04 11:29:12,558] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.54483 ]
 [80.284515]
 [80.16938 ]
 [80.16953 ]
 [80.1811  ]], R is [[80.606987  ]
 [80.46578979]
 [80.46307373]
 [80.56915283]
 [80.76346588]].
[2019-04-04 11:29:18,286] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5482567e-12 4.0970044e-10 8.0364866e-30 1.3444690e-27 1.1916410e-22
 1.0000000e+00 4.9278099e-22], sum to 1.0000
[2019-04-04 11:29:18,291] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5899
[2019-04-04 11:29:18,325] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 54.5, 204.0, 604.0, 26.0, 25.42339339725174, 0.4055284255218445, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4271400.0000, 
sim time next is 4272000.0000, 
raw observation next is [4.666666666666666, 54.66666666666667, 190.1666666666667, 640.3333333333334, 26.0, 25.35528728753123, 0.4006238815210876, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.5918744228993538, 0.5466666666666667, 0.6338888888888891, 0.7075506445672192, 0.6666666666666666, 0.6129406072942691, 0.6335412938403625, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6235337], dtype=float32), -0.116287746]. 
=============================================
[2019-04-04 11:29:18,333] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[88.82314 ]
 [88.31982 ]
 [87.75997 ]
 [87.220894]
 [86.70639 ]], R is [[89.01277924]
 [89.12265015]
 [89.23142242]
 [89.33911133]
 [89.44572449]].
[2019-04-04 11:29:21,832] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5663869e-09 5.0055618e-08 7.5847954e-23 3.0578115e-21 1.8293713e-17
 1.0000000e+00 7.0722831e-17], sum to 1.0000
[2019-04-04 11:29:21,832] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9742
[2019-04-04 11:29:21,851] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.18022256450548, 0.1137711891745468, 0.0, 1.0, 43643.89971858619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3988800.0000, 
sim time next is 3989400.0000, 
raw observation next is [-12.16666666666667, 64.0, 0.0, 0.0, 26.0, 24.11401463913138, 0.1050442692036519, 0.0, 1.0, 43686.3789886024], 
processed observation next is [1.0, 0.17391304347826086, 0.12557710064635264, 0.64, 0.0, 0.0, 0.6666666666666666, 0.509501219927615, 0.5350147564012173, 0.0, 1.0, 0.20803037613620193], 
reward next is 0.7920, 
noisyNet noise sample is [array([2.154801], dtype=float32), 0.13037893]. 
=============================================
[2019-04-04 11:29:21,853] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6960019e-11 7.3008689e-11 7.7432844e-29 5.1572001e-27 7.5788953e-23
 1.0000000e+00 1.6805032e-21], sum to 1.0000
[2019-04-04 11:29:21,855] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0490
[2019-04-04 11:29:21,874] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7848084e-12 4.9031074e-10 1.5438297e-28 9.8146861e-27 1.5340900e-21
 1.0000000e+00 1.5995945e-21], sum to 1.0000
[2019-04-04 11:29:21,874] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5565
[2019-04-04 11:29:21,901] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 78.0, 52.66666666666666, 0.0, 26.0, 25.924378118461, 0.5648200182356042, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4465200.0000, 
sim time next is 4465800.0000, 
raw observation next is [0.0, 78.0, 49.0, 0.0, 26.0, 26.02861276588749, 0.5772205059684009, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.16333333333333333, 0.0, 0.6666666666666666, 0.6690510638239576, 0.6924068353228003, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36172482], dtype=float32), 1.4313085]. 
=============================================
[2019-04-04 11:29:21,902] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 32.0, 117.5, 792.5, 26.0, 26.68424559518957, 0.6305488985029839, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4100400.0000, 
sim time next is 4101000.0000, 
raw observation next is [-0.6666666666666667, 31.33333333333334, 118.6666666666667, 800.3333333333334, 26.0, 26.71212494697625, 0.6381713159718022, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44413665743305636, 0.3133333333333334, 0.39555555555555566, 0.8843462246777164, 0.6666666666666666, 0.7260104122480208, 0.7127237719906008, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5198814], dtype=float32), -1.4569949]. 
=============================================
[2019-04-04 11:29:21,923] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[86.85925 ]
 [86.90804 ]
 [86.96785 ]
 [87.219185]
 [87.242744]], R is [[87.00843048]
 [87.13834381]
 [87.26696014]
 [87.39429474]
 [87.52035522]].
[2019-04-04 11:29:22,244] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.9348571e-09 3.4206661e-08 2.9585235e-22 1.3042724e-20 3.1558701e-17
 1.0000000e+00 2.3360024e-17], sum to 1.0000
[2019-04-04 11:29:22,244] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9004
[2019-04-04 11:29:22,273] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.16666666666667, 64.0, 0.0, 0.0, 26.0, 23.49558687623473, -0.02468455120159401, 0.0, 1.0, 43529.10778945599], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3996600.0000, 
sim time next is 3997200.0000, 
raw observation next is [-13.33333333333333, 65.0, 0.0, 0.0, 26.0, 23.49873166921427, -0.02987417522773134, 0.0, 1.0, 43412.5577909652], 
processed observation next is [1.0, 0.2608695652173913, 0.09325946445060027, 0.65, 0.0, 0.0, 0.6666666666666666, 0.4582276391011891, 0.4900419415907562, 0.0, 1.0, 0.20672646567126285], 
reward next is 0.7933, 
noisyNet noise sample is [array([0.09461904], dtype=float32), -1.10587]. 
=============================================
[2019-04-04 11:29:29,732] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5776562e-11 2.3243928e-10 6.3775514e-28 8.9897803e-27 2.9339105e-22
 1.0000000e+00 2.0303629e-21], sum to 1.0000
[2019-04-04 11:29:29,732] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2920
[2019-04-04 11:29:29,778] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.95144199302497, 0.4612113924438981, 1.0, 1.0, 111846.8401112559], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4475400.0000, 
sim time next is 4476000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.98648938545873, 0.4709814744012324, 1.0, 1.0, 46649.74135286372], 
processed observation next is [1.0, 0.8260869565217391, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5822074487882274, 0.6569938248004108, 1.0, 1.0, 0.22214162548982724], 
reward next is 0.7779, 
noisyNet noise sample is [array([-0.5401608], dtype=float32), -0.067321114]. 
=============================================
[2019-04-04 11:29:29,784] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.269196]
 [82.972145]
 [81.61456 ]
 [83.50819 ]
 [86.07067 ]], R is [[85.31416321]
 [84.92842102]
 [84.47831726]
 [84.35918427]
 [84.42014313]].
[2019-04-04 11:29:38,904] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.8813794e-12 2.3910063e-10 3.0870159e-30 4.9139977e-28 2.6355128e-23
 1.0000000e+00 1.0981980e-21], sum to 1.0000
[2019-04-04 11:29:38,904] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3403
[2019-04-04 11:29:38,947] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 80.33333333333333, 0.0, 26.0, 26.17418300560234, 0.5224846795107138, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4697400.0000, 
sim time next is 4698000.0000, 
raw observation next is [0.0, 92.0, 89.0, 0.0, 26.0, 26.14700200139416, 0.5245164217494978, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.2966666666666667, 0.0, 0.6666666666666666, 0.6789168334495134, 0.6748388072498326, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.353016], dtype=float32), -1.4613477]. 
=============================================
[2019-04-04 11:29:38,972] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[91.447876]
 [91.02249 ]
 [90.63865 ]
 [90.320724]
 [89.70822 ]], R is [[91.98493958]
 [92.06509399]
 [92.14444733]
 [92.2230072 ]
 [92.30078125]].
[2019-04-04 11:29:40,127] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6619432e-13 1.4439579e-11 7.6451531e-32 9.2899691e-30 5.6499934e-24
 1.0000000e+00 4.8483908e-24], sum to 1.0000
[2019-04-04 11:29:40,128] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7613
[2019-04-04 11:29:40,147] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.833333333333333, 49.0, 126.3333333333333, 843.6666666666667, 26.0, 26.30920247871842, 0.6880108274211353, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4625400.0000, 
sim time next is 4626000.0000, 
raw observation next is [4.0, 49.0, 129.5, 836.0, 26.0, 26.4785787075856, 0.726668175023676, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5734072022160666, 0.49, 0.43166666666666664, 0.9237569060773481, 0.6666666666666666, 0.7065482256321335, 0.742222725007892, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5016638], dtype=float32), -1.9004973]. 
=============================================
[2019-04-04 11:29:40,170] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[92.15139 ]
 [91.92247 ]
 [91.79    ]
 [91.80097 ]
 [91.643295]], R is [[92.45129395]
 [92.52677917]
 [92.60150909]
 [92.67549133]
 [92.74873352]].
[2019-04-04 11:29:43,355] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2886586e-14 3.7875232e-12 2.4677136e-33 2.7535454e-31 5.1930161e-25
 1.0000000e+00 9.7776627e-25], sum to 1.0000
[2019-04-04 11:29:43,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2223
[2019-04-04 11:29:43,373] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.75, 32.5, 133.6666666666667, 209.6666666666666, 26.0, 28.307050700926, 1.118632405013594, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4374600.0000, 
sim time next is 4375200.0000, 
raw observation next is [13.6, 33.0, 118.3333333333333, 104.8333333333333, 26.0, 28.50922685497763, 1.137282067736551, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8393351800554018, 0.33, 0.3944444444444443, 0.11583793738489867, 0.6666666666666666, 0.8757689045814692, 0.8790940225788503, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1420637], dtype=float32), -1.8715788]. 
=============================================
[2019-04-04 11:29:49,538] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0092920e-10 1.6301460e-09 1.8395518e-26 9.2659257e-25 2.7850415e-20
 1.0000000e+00 9.1926488e-20], sum to 1.0000
[2019-04-04 11:29:49,540] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5420
[2019-04-04 11:29:49,569] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.55, 73.0, 0.0, 0.0, 26.0, 25.37101066150809, 0.4939353317380669, 0.0, 1.0, 46975.42924133546], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4494600.0000, 
sim time next is 4495200.0000, 
raw observation next is [-0.5666666666666667, 73.0, 0.0, 0.0, 26.0, 25.50618524269612, 0.4903724470802154, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.44690674053554946, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6255154368913433, 0.6634574823600717, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.56711465], dtype=float32), 1.207322]. 
=============================================
[2019-04-04 11:29:50,297] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.7357227e-11 1.6896284e-09 1.0785576e-26 1.4454253e-24 2.8219260e-20
 1.0000000e+00 4.4159674e-20], sum to 1.0000
[2019-04-04 11:29:50,299] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3691
[2019-04-04 11:29:50,326] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8833333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.32279035684804, 0.416206362027577, 0.0, 1.0, 44344.71105015046], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4507800.0000, 
sim time next is 4508400.0000, 
raw observation next is [-0.8666666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.26675100463027, 0.4081145380886584, 0.0, 1.0, 42838.2286768827], 
processed observation next is [1.0, 0.17391304347826086, 0.4385964912280702, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.605562583719189, 0.6360381793628861, 0.0, 1.0, 0.20399156512801286], 
reward next is 0.7960, 
noisyNet noise sample is [array([1.4986206], dtype=float32), 0.16970065]. 
=============================================
[2019-04-04 11:29:53,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:29:53,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:29:53,939] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run34
[2019-04-04 11:29:57,750] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.8397534e-10 2.2303491e-08 4.0591671e-23 1.3351449e-22 6.3807716e-18
 1.0000000e+00 1.7276390e-17], sum to 1.0000
[2019-04-04 11:29:57,752] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0146
[2019-04-04 11:29:57,766] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 36.66666666666666, 0.0, 0.0, 26.0, 24.81842458596958, 0.2128651160840288, 0.0, 1.0, 40176.57983001737], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4081200.0000, 
sim time next is 4081800.0000, 
raw observation next is [-4.0, 37.33333333333334, 0.0, 0.0, 26.0, 24.77224608224579, 0.2012292781225057, 0.0, 1.0, 40166.93253667688], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.3733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5643538401871492, 0.5670764260408353, 0.0, 1.0, 0.19127110731750896], 
reward next is 0.8087, 
noisyNet noise sample is [array([0.61793315], dtype=float32), -1.9454985]. 
=============================================
[2019-04-04 11:29:58,791] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.5930568e-11 3.6251180e-09 1.2965651e-25 4.6412505e-24 1.7414662e-19
 1.0000000e+00 3.9325622e-19], sum to 1.0000
[2019-04-04 11:29:58,791] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2601
[2019-04-04 11:29:58,826] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 48.66666666666667, 0.0, 0.0, 26.0, 25.14852297642786, 0.272285558324682, 0.0, 1.0, 38383.20992265364], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4940400.0000, 
sim time next is 4941000.0000, 
raw observation next is [-2.0, 48.0, 0.0, 0.0, 26.0, 25.18929333323468, 0.2753284304380846, 0.0, 1.0, 38397.31055922278], 
processed observation next is [1.0, 0.17391304347826086, 0.40720221606648205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.5991077777695567, 0.5917761434793615, 0.0, 1.0, 0.18284433599629896], 
reward next is 0.8172, 
noisyNet noise sample is [array([-0.44356897], dtype=float32), 0.5895994]. 
=============================================
[2019-04-04 11:29:58,841] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[77.367355]
 [77.50805 ]
 [77.65079 ]
 [77.774765]
 [77.896805]], R is [[77.2862854 ]
 [77.3306427 ]
 [77.37471008]
 [77.41821289]
 [77.46118164]].
[2019-04-04 11:29:59,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:29:59,950] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:29:59,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run34
[2019-04-04 11:30:02,786] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7215985e-10 1.0230460e-09 7.9386418e-24 1.2129628e-22 1.1687689e-18
 1.0000000e+00 1.7490434e-18], sum to 1.0000
[2019-04-04 11:30:02,786] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8467
[2019-04-04 11:30:02,819] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.53236120526876, 0.5208821526933461, 0.0, 1.0, 56223.23232356439], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3962400.0000, 
sim time next is 3963000.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.48926668654565, 0.517566816126643, 0.0, 1.0, 67548.63549555514], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6241055572121376, 0.6725222720422144, 0.0, 1.0, 0.321660169026453], 
reward next is 0.6783, 
noisyNet noise sample is [array([1.9635289], dtype=float32), -0.3106963]. 
=============================================
[2019-04-04 11:30:02,826] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[71.272514]
 [71.425804]
 [72.14072 ]
 [73.359604]
 [75.03175 ]], R is [[71.38025665]
 [71.39872742]
 [71.41313934]
 [71.45415497]
 [71.54775238]].
[2019-04-04 11:30:02,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:30:02,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:30:02,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run34
[2019-04-04 11:30:05,389] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:30:05,389] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:30:05,423] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run34
[2019-04-04 11:30:05,478] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.7142112e-11 2.2652071e-09 8.6858575e-25 1.5335675e-23 6.2449768e-19
 1.0000000e+00 5.7166297e-19], sum to 1.0000
[2019-04-04 11:30:05,479] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7060
[2019-04-04 11:30:05,493] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.833333333333333, 44.83333333333334, 56.0, 230.3333333333333, 26.0, 25.10243750769981, 0.3307600945307186, 0.0, 1.0, 29399.82616714299], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4900200.0000, 
sim time next is 4900800.0000, 
raw observation next is [2.666666666666667, 44.66666666666667, 44.5, 208.6666666666667, 26.0, 25.06372713275656, 0.324412614376681, 0.0, 1.0, 43149.92221541941], 
processed observation next is [0.0, 0.7391304347826086, 0.5364727608494922, 0.4466666666666667, 0.14833333333333334, 0.23057090239410685, 0.6666666666666666, 0.5886439277297134, 0.6081375381255604, 0.0, 1.0, 0.20547582007342574], 
reward next is 0.7945, 
noisyNet noise sample is [array([2.0857344], dtype=float32), 1.8923074]. 
=============================================
[2019-04-04 11:30:07,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:30:07,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:30:07,355] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run34
[2019-04-04 11:30:08,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:30:08,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:30:08,572] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run34
[2019-04-04 11:30:12,156] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.9501980e-11 1.1628194e-09 2.3965362e-28 5.8749233e-26 1.0460467e-21
 1.0000000e+00 1.0218077e-20], sum to 1.0000
[2019-04-04 11:30:12,156] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1640
[2019-04-04 11:30:12,167] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.45, 72.5, 0.0, 0.0, 26.0, 25.75764765881978, 0.416628831264259, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4326600.0000, 
sim time next is 4327200.0000, 
raw observation next is [4.5, 72.0, 0.0, 0.0, 26.0, 25.75996148426364, 0.4042384615017142, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5872576177285319, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6466634570219701, 0.6347461538339048, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7914055], dtype=float32), 0.8377884]. 
=============================================
[2019-04-04 11:30:13,645] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5404310e-11 8.8856655e-10 7.4539760e-29 2.1750425e-26 3.3659014e-22
 1.0000000e+00 3.6143253e-21], sum to 1.0000
[2019-04-04 11:30:13,645] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9355
[2019-04-04 11:30:13,708] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.8, 86.0, 84.33333333333334, 0.0, 26.0, 24.39858521547687, 0.1468418743969436, 0.0, 1.0, 33099.6289802833], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 49800.0000, 
sim time next is 50400.0000, 
raw observation next is [7.7, 86.0, 83.0, 0.0, 26.0, 24.41568714068518, 0.1505742066655177, 0.0, 1.0, 29195.53775923952], 
processed observation next is [0.0, 0.6086956521739131, 0.6759002770083103, 0.86, 0.27666666666666667, 0.0, 0.6666666666666666, 0.5346405950570983, 0.5501914022218393, 0.0, 1.0, 0.13902637028209294], 
reward next is 0.8610, 
noisyNet noise sample is [array([-1.2571967], dtype=float32), 1.5188301]. 
=============================================
[2019-04-04 11:30:16,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:30:16,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:30:16,551] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run34
[2019-04-04 11:30:18,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:30:18,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:30:18,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run34
[2019-04-04 11:30:19,601] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:30:19,601] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:30:19,604] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run34
[2019-04-04 11:30:26,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0921620e-12 3.6652723e-11 2.4635964e-31 4.4377485e-29 2.9059496e-24
 1.0000000e+00 1.2379571e-23], sum to 1.0000
[2019-04-04 11:30:26,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3297
[2019-04-04 11:30:26,126] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.5, 18.0, 0.0, 0.0, 26.0, 27.76012808678516, 0.9772095793829733, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5081400.0000, 
sim time next is 5082000.0000, 
raw observation next is [10.33333333333333, 18.33333333333333, 0.0, 0.0, 26.0, 27.66467744440866, 0.9606290719494811, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7488457987072946, 0.1833333333333333, 0.0, 0.0, 0.6666666666666666, 0.8053897870340551, 0.820209690649827, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9534411], dtype=float32), -0.18072073]. 
=============================================
[2019-04-04 11:30:26,137] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[92.43784 ]
 [91.82346 ]
 [93.97669 ]
 [93.657814]
 [93.29151 ]], R is [[90.53275299]
 [90.62742615]
 [90.72115326]
 [90.81394196]
 [90.90579987]].
[2019-04-04 11:30:27,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:30:27,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:30:27,871] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run34
[2019-04-04 11:30:27,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:30:27,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:30:27,962] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run34
[2019-04-04 11:30:27,995] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:30:27,998] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:30:28,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run34
[2019-04-04 11:30:36,549] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0109800e-11 1.3166799e-09 7.0963024e-26 1.3418682e-24 2.1662720e-20
 1.0000000e+00 5.4232443e-20], sum to 1.0000
[2019-04-04 11:30:36,549] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4529
[2019-04-04 11:30:36,595] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.2, 39.0, 37.0, 707.0, 26.0, 26.33535168422122, 0.3556833378959235, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 401400.0000, 
sim time next is 402000.0000, 
raw observation next is [-9.100000000000001, 38.66666666666667, 34.33333333333334, 656.3333333333334, 26.0, 26.20778545057186, 0.4764444261269161, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.21052631578947364, 0.3866666666666667, 0.11444444444444447, 0.7252302025782689, 0.6666666666666666, 0.6839821208809882, 0.658814808708972, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31256884], dtype=float32), -0.519891]. 
=============================================
[2019-04-04 11:30:36,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[77.43117 ]
 [77.75778 ]
 [77.92595 ]
 [78.290764]
 [78.66103 ]], R is [[77.3221283 ]
 [77.54890442]
 [77.77341461]
 [77.99568176]
 [78.21572876]].
[2019-04-04 11:30:42,234] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0041008e-09 2.1094179e-08 1.8590320e-23 8.8397335e-22 6.5995621e-18
 1.0000000e+00 4.1279022e-17], sum to 1.0000
[2019-04-04 11:30:42,234] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0895
[2019-04-04 11:30:42,257] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 77.33333333333333, 0.0, 0.0, 26.0, 23.44735283116438, -0.08220963518279119, 0.0, 1.0, 44239.1953753111], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 183000.0000, 
sim time next is 183600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.40492987506879, -0.0921209691309605, 0.0, 1.0, 44036.07026832593], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4504108229223993, 0.4692930102896798, 0.0, 1.0, 0.20969557270631395], 
reward next is 0.7903, 
noisyNet noise sample is [array([0.25104174], dtype=float32), 0.29161796]. 
=============================================
[2019-04-04 11:30:47,137] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2674625e-11 3.9450587e-10 3.2863545e-27 1.9302348e-24 4.0391087e-20
 1.0000000e+00 6.6833886e-20], sum to 1.0000
[2019-04-04 11:30:47,137] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5183
[2019-04-04 11:30:47,146] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 122.5, 734.5, 26.0, 25.12872383741856, 0.4430403473779829, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4806000.0000, 
sim time next is 4806600.0000, 
raw observation next is [3.0, 37.0, 114.0, 732.0, 26.0, 25.17971161781472, 0.4452597211595135, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.38, 0.8088397790055248, 0.6666666666666666, 0.5983093014845601, 0.6484199070531712, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7420958], dtype=float32), 2.2691567]. 
=============================================
[2019-04-04 11:30:48,172] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.6946227e-11 1.0751819e-09 3.3883729e-27 7.1359933e-25 4.7327284e-20
 1.0000000e+00 2.8853989e-20], sum to 1.0000
[2019-04-04 11:30:48,172] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3542
[2019-04-04 11:30:48,288] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.666666666666666, 87.0, 103.3333333333333, 349.1666666666667, 26.0, 23.88127858048322, 0.228493704498034, 0.0, 1.0, 202434.7080353583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4782000.0000, 
sim time next is 4782600.0000, 
raw observation next is [-5.5, 84.5, 124.0, 419.0, 26.0, 24.31781378082753, 0.3489413017274095, 0.0, 1.0, 202145.2252079068], 
processed observation next is [0.0, 0.34782608695652173, 0.3102493074792244, 0.845, 0.41333333333333333, 0.46298342541436466, 0.6666666666666666, 0.5264844817356277, 0.6163137672424698, 0.0, 1.0, 0.9625963105138419], 
reward next is 0.0374, 
noisyNet noise sample is [array([-0.93060917], dtype=float32), 1.0371915]. 
=============================================
[2019-04-04 11:30:52,963] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8445770e-10 1.0704225e-09 1.7016708e-26 5.0605437e-25 4.7886077e-20
 1.0000000e+00 4.0874866e-19], sum to 1.0000
[2019-04-04 11:30:52,963] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4728
[2019-04-04 11:30:53,027] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.9759257626995, 0.3012140155362809, 0.0, 1.0, 79442.26236746274], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 588600.0000, 
sim time next is 589200.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 25.022078484022, 0.3059000529350694, 0.0, 1.0, 55790.47119571063], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5851732070018333, 0.6019666843116899, 0.0, 1.0, 0.2656689104557649], 
reward next is 0.7343, 
noisyNet noise sample is [array([0.13274309], dtype=float32), -0.38925222]. 
=============================================
[2019-04-04 11:30:57,226] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2170002e-13 7.2431963e-11 3.4000597e-30 6.9937970e-28 5.8495058e-23
 1.0000000e+00 3.5505042e-23], sum to 1.0000
[2019-04-04 11:30:57,229] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2444
[2019-04-04 11:30:57,244] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 29.0, 119.5, 824.0, 26.0, 26.67921111624005, 0.612651287952226, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4964400.0000, 
sim time next is 4965000.0000, 
raw observation next is [3.5, 28.33333333333334, 120.3333333333333, 831.0, 26.0, 26.66793071984434, 0.6278460351468115, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5595567867036012, 0.2833333333333334, 0.401111111111111, 0.918232044198895, 0.6666666666666666, 0.7223275599870282, 0.7092820117156039, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7999822], dtype=float32), 1.4918982]. 
=============================================
[2019-04-04 11:30:57,250] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[89.957466]
 [89.96105 ]
 [90.00364 ]
 [89.96668 ]
 [90.07833 ]], R is [[90.14360809]
 [90.24217224]
 [90.3397522 ]
 [90.43635559]
 [90.53199005]].
[2019-04-04 11:30:59,161] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:30:59,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:30:59,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run34
[2019-04-04 11:31:01,454] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.7882601e-10 1.6824419e-08 2.1130128e-24 8.9695201e-23 1.4057337e-18
 1.0000000e+00 4.9537353e-18], sum to 1.0000
[2019-04-04 11:31:01,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2963
[2019-04-04 11:31:01,483] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 23.8018985236793, 0.006287705157940707, 0.0, 1.0, 44395.75500752063], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 622800.0000, 
sim time next is 623400.0000, 
raw observation next is [-4.5, 67.5, 0.0, 0.0, 26.0, 23.77561630599963, 0.004009124604336047, 0.0, 1.0, 44320.82313431671], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.675, 0.0, 0.0, 0.6666666666666666, 0.48130135883330255, 0.501336374868112, 0.0, 1.0, 0.2110515387348415], 
reward next is 0.7889, 
noisyNet noise sample is [array([0.51929724], dtype=float32), 0.6041002]. 
=============================================
[2019-04-04 11:31:02,176] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.4735830e-10 6.5266250e-09 2.6723711e-25 1.1057847e-23 3.2587910e-19
 1.0000000e+00 1.1316133e-18], sum to 1.0000
[2019-04-04 11:31:02,176] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6725
[2019-04-04 11:31:02,205] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 38.0, 0.0, 0.0, 26.0, 25.54334300161063, 0.3896246464866833, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4912200.0000, 
sim time next is 4912800.0000, 
raw observation next is [1.0, 37.33333333333334, 0.0, 0.0, 26.0, 25.52819252163204, 0.3825882321933296, 0.0, 1.0, 18748.09603033972], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.3733333333333334, 0.0, 0.0, 0.6666666666666666, 0.62734937680267, 0.6275294107311099, 0.0, 1.0, 0.08927664776352247], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.4812507], dtype=float32), 0.7499278]. 
=============================================
[2019-04-04 11:31:04,360] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8563866e-13 2.7285205e-11 3.2445578e-30 2.0944535e-29 8.6121223e-24
 1.0000000e+00 5.2627179e-24], sum to 1.0000
[2019-04-04 11:31:04,360] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2582
[2019-04-04 11:31:04,430] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.333333333333333, 43.0, 112.6666666666667, 716.5, 26.0, 26.72798572962902, 0.6791556930360327, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5046000.0000, 
sim time next is 5046600.0000, 
raw observation next is [2.666666666666667, 42.0, 113.3333333333333, 735.0, 26.0, 26.91678768272547, 0.6924787923291279, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5364727608494922, 0.42, 0.37777777777777766, 0.8121546961325967, 0.6666666666666666, 0.7430656402271225, 0.7308262641097093, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.92170274], dtype=float32), 0.6364547]. 
=============================================
[2019-04-04 11:31:05,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:31:05,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:31:05,187] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run34
[2019-04-04 11:31:11,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:31:11,474] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:31:11,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run34
[2019-04-04 11:31:21,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:31:21,272] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:31:21,275] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run34
[2019-04-04 11:31:26,497] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-04 11:31:26,498] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 11:31:26,498] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:31:26,500] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run46
[2019-04-04 11:31:26,573] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 11:31:26,574] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:31:26,575] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 11:31:26,576] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:31:26,579] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run46
[2019-04-04 11:31:26,681] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/5/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run46
